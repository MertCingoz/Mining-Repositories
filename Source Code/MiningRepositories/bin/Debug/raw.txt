commit 4442f3da03856dafa88767404bb0575db37bf7b8
Author: tedchoc <tedchoc@chromium.org>
Date:   Fri Sep 2 12:18:04 2016 -0700

    Update .classpath to include the location component src
    
    Also, this includes leakcanary src.
    
    BUG=
    
    Review-Url: https://codereview.chromium.org/2308693002
    Cr-Original-Commit-Position: refs/heads/master@{#416319}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c53b42a1bfb20f485686ee0deca17b0508416699

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 4c075c2..393bd67 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -52,6 +52,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="components/invalidation/impl/android/java/src"/>
     <classpathentry kind="src" path="components/invalidation/impl/android/javatests/src"/>
     <classpathentry kind="src" path="components/invalidation/impl/android/junit/src"/>
+    <classpathentry kind="src" path="components/location/android/java/src"/>
     <classpathentry kind="src" path="components/navigation_interception/android/java/src"/>
     <classpathentry kind="src" path="components/ntp_tiles/android/java/src"/>
     <classpathentry kind="src" path="components/policy/android/java/src"/>
@@ -108,6 +109,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="third_party/gif_player/src"/>
     <classpathentry kind="src" path="third_party/jmake/src"/>
     <classpathentry kind="src" path="third_party/junit/src/src/main/java"/>
+    <classpathentry kind="src" path="third_party/leakcanary/src/leakcanary-android/src/main/java"/>
+    <classpathentry kind="src" path="third_party/leakcanary/src/leakcanary-watcher/src/main/java"/>
     <classpathentry kind="src" path="third_party/mockito/src/src/main/java"/>
     <classpathentry kind="src" path="third_party/robolectric/robolectric/robolectric/src/main/java"/>
     <classpathentry kind="src" path="third_party/robolectric/robolectric/robolectric/src/main/java"/>

commit 51c8685136948a99d4b54c77ad9e42ab34569788
Author: changwan <changwan@chromium.org>
Date:   Fri Sep 2 08:45:50 2016 -0700

    Add some class paths for mockito and robolectric
    
    -annotations is needed for @Config,
    -shadows & -utils are needed for Shadow use cases
    
    BUG=
    
    Review-Url: https://codereview.chromium.org/2292083007
    Cr-Original-Commit-Position: refs/heads/master@{#416273}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a186d8771a155cf8a2c09eac22aceae9000a7f89

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 7f91523..4c075c2 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -108,7 +108,12 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="third_party/gif_player/src"/>
     <classpathentry kind="src" path="third_party/jmake/src"/>
     <classpathentry kind="src" path="third_party/junit/src/src/main/java"/>
-    <classpathentry kind="src" path="third_party/mockito/src/src"/>
+    <classpathentry kind="src" path="third_party/mockito/src/src/main/java"/>
+    <classpathentry kind="src" path="third_party/robolectric/robolectric/robolectric/src/main/java"/>
+    <classpathentry kind="src" path="third_party/robolectric/robolectric/robolectric/src/main/java"/>
+    <classpathentry kind="src" path="third_party/robolectric/robolectric/robolectric-annotations/src/main/java"/>
+    <classpathentry kind="src" path="third_party/robolectric/robolectric/robolectric-shadows/shadows-core/src/main/java"/>
+    <classpathentry kind="src" path="third_party/robolectric/robolectric/robolectric-utils/src/main/java"/>
     <classpathentry kind="src" path="tools/android/findbugs_plugin/src"/>
     <classpathentry kind="src" path="tools/android/findbugs_plugin/test/java/src"/>
     <classpathentry kind="src" path="tools/android/memconsumer/java/src"/>
@@ -217,7 +222,6 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="lib" path="third_party/findbugs/lib/findbugs.jar"/>
     <classpathentry kind="lib" path="third_party/junit/src/lib/hamcrest-core-1.1.jar"/>
     <classpathentry kind="lib" path="third_party/mockito/src/lib/repackaged/cglib-and-asm-1.0.jar" sourcepath="third_party/mockito/src/lib/sources/cglib-and-asm-1.0-sources.jar"/>
-    <classpathentry kind="lib" path="third_party/robolectric/lib/robolectric-2.4-jar-with-dependencies.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/cacheinvalidation_proto_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/content_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/jsr_305_javalib.jar"/>

commit b2dde16bf947d24e6acffc8f8450e9edc15966c3
Author: tedchoc <tedchoc@chromium.org>
Date:   Tue Aug 30 14:46:49 2016 -0700

    Fix NFC crasher from accessing a destroyed activity.
    
    While this works, it would be better to have a clearer signal that
    the Activity died (or is in the process of shutting down) so we
    could make a better timed decision to disable NFC reading.
    
    Also note that Activity#isDestroyed() is only available in
    JELLY_BEAN_MR1, so this is not the best solution for all device
    implementations that need to determine this.
    
    BUG=640890
    
    Review-Url: https://codereview.chromium.org/2292353002
    Cr-Original-Commit-Position: refs/heads/master@{#415441}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1ff2cc181eb07396527b76ba2e15d48684c27d15

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 416ac89..7f91523 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -76,6 +76,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="device/battery/android/java/src"/>
     <classpathentry kind="src" path="device/bluetooth/android/java/src"/>
     <classpathentry kind="src" path="device/gamepad/android/java/src"/>
+    <classpathentry kind="src" path="device/nfc/android/java/src"/>
     <classpathentry kind="src" path="device/usb/android/java/src"/>
     <classpathentry kind="src" path="device/vibration/android/java/src"/>
     <classpathentry kind="src" path="media/base/android/java/src"/>
@@ -250,6 +251,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="lib" path="out/Debug/lib.java/device/battery/android/battery_monitor_android.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/device/battery/mojo_bindings_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/device/bluetooth/java.jar"/>
+    <classpathentry kind="lib" path="out/Debug/lib.java/device/nfc/mojo_bindings_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/device/vibration/android/vibration_manager_android.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/device/vibration/mojo_bindings_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/media/base/android/media_java.jar"/>

commit 912dd3c55535639b6887f88c9bbfaf9c8604158a
Author: mdjones <mdjones@chromium.org>
Date:   Tue Aug 30 12:49:50 2016 -0700

    Remove dom_distiller core dependency on content
    
    This change removes the top level android directory from dom_distiller
    and splits it into content/browser/android and core/android. The two
    directories now each have their own JNI registrar that allows them to
    depend on appropriate parts of the component.
    
    BUG=562769
    TBR=yfriedman@chromium.org
    
    Review-Url: https://codereview.chromium.org/2252963004
    Cr-Original-Commit-Position: refs/heads/master@{#415400}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fd194b338274f4a2eba70021f739c3b93e07be99

diff --git a/eclipse/.classpath b/eclipse/.classpath
index ebe5be3..416ac89 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -231,8 +231,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="lib" path="out/Debug/lib.java/chrome/android/chrome_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/chrome/android/document_tab_model_info_proto_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/bookmarks/common/android/bookmarks_java.jar"/>
-    <classpathentry kind="lib" path="out/Debug/lib.java/components/dom_distiller/android/dom_distiller_content_java.jar"/>
-    <classpathentry kind="lib" path="out/Debug/lib.java/components/dom_distiller/android/dom_distiller_core_java.jar"/>
+    <classpathentry kind="lib" path="out/Debug/lib.java/components/dom_distiller/content/browser/android/dom_distiller_content_java.jar"/>
+    <classpathentry kind="lib" path="out/Debug/lib.java/components/dom_distiller/core/android/dom_distiller_core_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/external_video_surface/java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/gcm_driver/android/gcm_driver_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/gcm_driver/instance_id/android/instance_id_driver_java.jar"/>

commit 81548f53cb4a6c56846a1e7265b254aa1a3e1366
Author: fdoray <fdoray@chromium.org>
Date:   Fri Aug 26 06:51:18 2016 -0700

    Remove calls to deprecated MessageLoop methods from tools/.
    
    This CL removes calls to these methods from tools/ files:
    - MessageLoop::PostTask
    - MessageLoop::PostDelayedTask
    - MessageLoop::ReleaseSoon
    - MessageLoop::DeleteSoon
    - MessageLoop::Run
    - MessageLoop::RunUntilIdle
    
    BUG=616447
    
    Review-Url: https://codereview.chromium.org/2278993002
    Cr-Original-Commit-Position: refs/heads/master@{#414701}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ee7bd139a3b585f2adad420252bd82578caec4a3

diff --git a/forwarder2/device_forwarder_main.cc b/forwarder2/device_forwarder_main.cc
index af796b3..0efd61b 100644
--- a/forwarder2/device_forwarder_main.cc
+++ b/forwarder2/device_forwarder_main.cc
@@ -13,6 +13,7 @@
 #include "base/command_line.h"
 #include "base/compiler_specific.h"
 #include "base/logging.h"
+#include "base/single_thread_task_runner.h"
 #include "base/strings/string_piece.h"
 #include "base/strings/stringprintf.h"
 #include "base/threading/thread.h"
@@ -72,7 +73,7 @@ class ServerDelegate : public Daemon::ServerDelegate {
       client_socket->WriteString("OK");
       return;
     }
-    controller_thread_->message_loop()->PostTask(
+    controller_thread_->task_runner()->PostTask(
         FROM_HERE,
         base::Bind(&ServerDelegate::StartController, base::Unretained(this),
                    GetExitNotifierFD(), base::Passed(&client_socket)));

commit 0548f80fb2ba1103860cccbe09a0fe436200ecd8
Author: changwan <changwan@chromium.org>
Date:   Tue Aug 16 23:15:45 2016 -0700

    Add android_webview glue to the classpath
    
    BUG=
    
    Review-Url: https://codereview.chromium.org/2253543002
    Cr-Original-Commit-Position: refs/heads/master@{#412458}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1e96829a25975bfd82d410d657403e3556c0fe3e

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 222e1cf..ebe5be3 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -16,6 +16,7 @@ to the classpath for downstream development. See "additional_entries" below.
 
 {% endblock %} -->
 <classpath>
+    <classpathentry kind="src" path="android_webview/glue/java/src"/>
     <classpathentry kind="src" path="android_webview/java/generated_src"/>
     <classpathentry kind="src" path="android_webview/java/src"/>
     <classpathentry kind="src" path="android_webview/javatests/src"/>

commit 32d1807e8a9fdafc9fb760e442863f519a1a929f
Author: ianwen <ianwen@chromium.org>
Date:   Fri Aug 12 11:22:40 2016 -0700

    Fix an Eclipse display error for gamepad
    
    This CL makes device/gamepad a java source for Eclipse, so that
    ContentViewCore will have no error displayed in Eclipse.
    
    BUG=NONE
    
    Review-Url: https://codereview.chromium.org/2240693002
    Cr-Original-Commit-Position: refs/heads/master@{#411702}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fef4145788e7477d77972346e6cb525c968d947b

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 024b788..222e1cf 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -74,6 +74,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="content/shell/android/shell_apk/src"/>
     <classpathentry kind="src" path="device/battery/android/java/src"/>
     <classpathentry kind="src" path="device/bluetooth/android/java/src"/>
+    <classpathentry kind="src" path="device/gamepad/android/java/src"/>
     <classpathentry kind="src" path="device/usb/android/java/src"/>
     <classpathentry kind="src" path="device/vibration/android/java/src"/>
     <classpathentry kind="src" path="media/base/android/java/src"/>

commit 7fd491fc732667f09915eaeb5b7af513d16c321d
Author: bauerb <bauerb@chromium.org>
Date:   Thu Aug 11 11:16:51 2016 -0700

    Update Eclipse .classpath.
    
    * Add components/sync/.
    * Change cglib-and-asm entry to use the jar file that is actually used for building.
      The src/ directory in the repository contains changes that are incompatible with
      robolectric.
    
    Review-Url: https://codereview.chromium.org/2235893002
    Cr-Original-Commit-Position: refs/heads/master@{#411381}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b6629ab895e81f2585d7c2f31843b216f0d95beb

diff --git a/eclipse/.classpath b/eclipse/.classpath
index a15386f..024b788 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -58,6 +58,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="components/precache/android/javatests/src"/>
     <classpathentry kind="src" path="components/safe_json/android/java/src"/>
     <classpathentry kind="src" path="components/service_tab_launcher/android/java/src"/>
+    <classpathentry kind="src" path="components/sync/android/java/src"/>
+    <classpathentry kind="src" path="components/sync/android/javatests/src"/>
     <classpathentry kind="src" path="components/variations/android/java/src"/>
     <classpathentry kind="src" path="components/web_contents_delegate_android/android/java/src"/>
     <classpathentry kind="src" path="content/public/android/java/src"/>
@@ -103,7 +105,6 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="third_party/gif_player/src"/>
     <classpathentry kind="src" path="third_party/jmake/src"/>
     <classpathentry kind="src" path="third_party/junit/src/src/main/java"/>
-    <classpathentry kind="src" path="third_party/mockito/src/cglib-and-asm/src"/>
     <classpathentry kind="src" path="third_party/mockito/src/src"/>
     <classpathentry kind="src" path="tools/android/findbugs_plugin/src"/>
     <classpathentry kind="src" path="tools/android/findbugs_plugin/test/java/src"/>
@@ -212,6 +213,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="lib" path="third_party/android_tools/sdk/extras/android/support/v7/appcompat/libs/android-support-v7-appcompat.jar" sourcepath="third_party/android_tools/sdk/sources"/>
     <classpathentry kind="lib" path="third_party/findbugs/lib/findbugs.jar"/>
     <classpathentry kind="lib" path="third_party/junit/src/lib/hamcrest-core-1.1.jar"/>
+    <classpathentry kind="lib" path="third_party/mockito/src/lib/repackaged/cglib-and-asm-1.0.jar" sourcepath="third_party/mockito/src/lib/sources/cglib-and-asm-1.0-sources.jar"/>
     <classpathentry kind="lib" path="third_party/robolectric/lib/robolectric-2.4-jar-with-dependencies.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/cacheinvalidation_proto_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/content_java.jar"/>

commit e96f0b62bd35d0df8f6e79dc69c47643d27c4ff1
Author: smaier <smaier@chromium.org>
Date:   Fri Aug 5 14:18:16 2016 -0700

    Allowing dexdiffer to work with empty mapping files
    
    BUG=None
    
    Review-Url: https://codereview.chromium.org/2219013002
    Cr-Original-Commit-Position: refs/heads/master@{#410165}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 58d015044f009cedf4cffbf56c8714a82c1cc202

diff --git a/dexdiffer/dexdiffer.py b/dexdiffer/dexdiffer.py
index d505c24..cbe6451 100644
--- a/dexdiffer/dexdiffer.py
+++ b/dexdiffer/dexdiffer.py
@@ -57,6 +57,7 @@ def _ReadMappingDict(mapping_file):
   mapping = {}
   renamed_class_name = ''
   original_class_name = ''
+  current_entry = []
   for line in mapping_file:
     line = line.strip()
     if _IsNewClass(line):
@@ -70,7 +71,8 @@ def _ReadMappingDict(mapping_file):
       original_member_name, renamed_member_name = _ParseMappingLine(line)
       member_mappings[renamed_member_name] = original_member_name
 
-  mapping[renamed_class_name] = current_entry
+  if current_entry and renamed_class_name:
+    mapping[renamed_class_name] = current_entry
   return mapping
 
 
@@ -127,8 +129,9 @@ def _TypeLookup(renamed_type, mapping_dict):
 def _GetMemberIdentifier(line_tokens, mapping_dict, renamed_class_name,
                          is_function):
   assert len(line_tokens) > 1
-  assert renamed_class_name in mapping_dict
-  mapping_entry = mapping_dict[renamed_class_name][1]
+  if mapping_dict:
+    assert renamed_class_name in mapping_dict
+    mapping_entry = mapping_dict[renamed_class_name][1]
 
   renamed_type = line_tokens[0]
   real_type = _TypeLookup(renamed_type, mapping_dict)
@@ -147,6 +150,10 @@ def _GetMemberIdentifier(line_tokens, mapping_dict, renamed_class_name,
 
   renamed_member_identifier = (real_type + ' ' + renamed_name_token
                                + function_args)
+
+  if not mapping_dict:
+    return renamed_member_identifier
+
   if renamed_member_identifier not in mapping_entry:
     print 'Proguarded class which caused the issue:', renamed_class_name
     print 'Key supposed to be in this dict:',  mapping_entry
@@ -159,6 +166,8 @@ def _GetMemberIdentifier(line_tokens, mapping_dict, renamed_class_name,
 
 def _GetClassNames(line_tokens, mapping_dict):
   assert len(line_tokens) > 1
+  if not mapping_dict:
+    return line_tokens[1], line_tokens[1]
   assert line_tokens[1] in mapping_dict
   return line_tokens[1], mapping_dict[line_tokens[1]][0]
 
@@ -174,7 +183,8 @@ def _IsLineFunctionDefinition(line):
 def _BuildMappedDexDict(dextra_file, mapping_dict):
   # Have to add 'bool' -> 'boolean' mapping in dictionary, since for some reason
   # dextra shortens boolean to bool.
-  mapping_dict['bool'] = ['boolean', {}]
+  if mapping_dict:
+    mapping_dict['bool'] = ['boolean', {}]
   dex_dict = {}
   current_entry = []
   reading_class_header = True

commit 96dd8fd3b05ad8f90075d11ec61e14c34c441031
Author: bashi <bashi@chromium.org>
Date:   Thu Aug 4 18:20:20 2016 -0700

    Fix a syntax error in mempressure.py
    
    Lacks of closing parenthesis.
    
    Review-Url: https://codereview.chromium.org/2212193002
    Cr-Original-Commit-Position: refs/heads/master@{#409959}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a1533f15b8236414147ad2a00580538defa630ef

diff --git a/mempressure.py b/mempressure.py
index ffa7c12..053f0a1 100755
--- a/mempressure.py
+++ b/mempressure.py
@@ -12,7 +12,7 @@ import sys
 _SRC_PATH = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..'))
 
-sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil')
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil'))
 from devil.android import device_errors
 from devil.android import device_utils
 from devil.android import flag_changer

commit 448340c658e366036c6e80686c54bcfb1ecba560
Author: shaktisahu <shaktisahu@chromium.org>
Date:   Wed Aug 3 18:07:08 2016 -0700

    Tied up BlimpNavigationController to NavigationFeature
    
    Added some necessary navigation methods to BlimpNavigationController
    Java interface and hooked it up all the way to navigation_feature.cc.
    Added C++ unit tests for the same.
    
    BUG=611100
    
    Review-Url: https://codereview.chromium.org/2058263002
    Cr-Original-Commit-Position: refs/heads/master@{#409689}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 07de74f02ced93bc52625ee8f50f9d9a3f40a841

diff --git a/eclipse/.classpath b/eclipse/.classpath
index c21cc98..a15386f 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -27,6 +27,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="blimp/client/app/android/java/src"/>
     <classpathentry kind="src" path="blimp/client/app/android/javatests/src"/>
     <classpathentry kind="src" path="blimp/client/core/android/java/src"/>
+    <classpathentry kind="src" path="blimp/client/core/contents/android/java/src"/>
     <classpathentry kind="src" path="blimp/client/public/android/java/src"/>
     <classpathentry kind="src" path="chrome/android/java/src"/>
     <classpathentry kind="src" path="chrome/android/javatests/src"/>

commit 58972a22f87a1a0331c2cc61b41310e213e64d30
Author: ianwen <ianwen@chromium.org>
Date:   Tue Aug 2 12:38:41 2016 -0700

    Add a Python script to update the support library
    
    Support libraries are now shipped in m2repository, which contains very
    old versions that Clank does not need. This CL makes it easy to download
    the updated library and remove the unwanted folders.
    
    BUG=611171
    
    Review-Url: https://codereview.chromium.org/2188003002
    Cr-Original-Commit-Position: refs/heads/master@{#409277}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e0c54931e1b7b6ab4f82d207575c8d99c2d49f1a

diff --git a/roll/update_support_library.py b/roll/update_support_library.py
new file mode 100755
index 0000000..b81ad10
--- /dev/null
+++ b/roll/update_support_library.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python
+#
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""
+Updates the Android support repository (m2repository).
+"""
+
+import argparse
+import fnmatch
+import os
+import subprocess
+import shutil
+import sys
+
+DIR_SOURCE_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),
+                                               '..', '..', '..'))
+ANDROID_SDK_PATH = os.path.abspath(os.path.join(DIR_SOURCE_ROOT, 'third_party',
+                                                'android_tools', 'sdk'))
+TARGET_NAME = 'extra-android-m2repository'
+# The first version we included was 23.2.1. Any folders that are older than
+# that should not be included by Chrome's git repo. Unstable versions should
+# also be excluded.
+REMOVE_LIST = ['databinding', '13.*', '18.*', '19.*', '20.*', '21.*', '22.*',
+               '23.0.*', '23.1.*', '23.2.0', '*-alpha*', '*-beta*']
+
+
+def main():
+  parser = argparse.ArgumentParser(description='Updates the Android support '
+                                   'repository in third_party/android_tools')
+  parser.add_argument('--sdk-dir',
+                      help='Directory for the Android SDK.')
+  args = parser.parse_args()
+
+  sdk_path = ANDROID_SDK_PATH
+  if args.sdk_dir is not None:
+    sdk_path = os.path.abspath(os.path.join(DIR_SOURCE_ROOT, args.sdk_dir))
+
+  sdk_tool = os.path.abspath(os.path.join(sdk_path, 'tools', 'android'))
+  if not os.path.exists(sdk_tool):
+    print 'SDK tool not found at %s' % sdk_tool
+    return 1
+
+  # Run the android sdk update tool in command line.
+  subprocess.check_call([sdk_tool, 'update', 'sdk' , '--no-ui',
+                         '--filter', TARGET_NAME])
+
+  m2repo = os.path.abspath(os.path.join(sdk_path, 'extras', 'android',
+                                        'm2repository'))
+  # Remove obsolete folders and unused folders according to REMOVE_LIST.
+  count = 0
+  for folder, _, _ in os.walk(m2repo):
+    for pattern in REMOVE_LIST:
+      if fnmatch.fnmatch(os.path.basename(folder), pattern):
+        count += 1
+        print 'Removing %s' % os.path.relpath(folder, sdk_path)
+        shutil.rmtree(folder)
+  if count == 0:
+    print ('No files were removed from the updated support library. '
+           'Did you update it successfully?')
+    return 1
+
+
+if __name__ == '__main__':
+  sys.exit(main())

commit 338842964cfa23a662136911599ad3f3987fcd13
Author: wangxianzhu <wangxianzhu@chromium.org>
Date:   Tue Aug 2 12:17:50 2016 -0700

    Replace me with jbudorick as owner of tools/android
    
    NOTRY=true
    
    Review-Url: https://codereview.chromium.org/2200243002
    Cr-Original-Commit-Position: refs/heads/master@{#409265}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1a3a5eb7ad36ee1f7dd9d0504df33cdce262098d

diff --git a/OWNERS b/OWNERS
index f8370f6..49d9078 100644
--- a/OWNERS
+++ b/OWNERS
@@ -1,4 +1,4 @@
 digit@chromium.org
+jbudorick@chromium.org
 michaelbai@chromium.org
-wangxianzhu@chromium.org
 yfriedman@chromium.org

commit a47abca1a89bccf9f6bce4405a5c8a62915ddd96
Author: xingliu <xingliu@chromium.org>
Date:   Sat Jul 30 14:15:19 2016 -0700

    Add eclipse classpath for blimp.
    
    So after run gclient runhooks, we don't need to manually add java build path in eclipse.
    
    Review-Url: https://codereview.chromium.org/2193133003
    Cr-Original-Commit-Position: refs/heads/master@{#408876}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e61bd7b8d628632ed1b34548e64340f6620f2783

diff --git a/eclipse/.classpath b/eclipse/.classpath
index f88e99f..c21cc98 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -26,6 +26,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="base/test/android/javatests/src"/>
     <classpathentry kind="src" path="blimp/client/app/android/java/src"/>
     <classpathentry kind="src" path="blimp/client/app/android/javatests/src"/>
+    <classpathentry kind="src" path="blimp/client/core/android/java/src"/>
+    <classpathentry kind="src" path="blimp/client/public/android/java/src"/>
     <classpathentry kind="src" path="chrome/android/java/src"/>
     <classpathentry kind="src" path="chrome/android/javatests/src"/>
     <classpathentry kind="src" path="chrome/android/sync_shell/javatests/src"/>

commit 9dbc8384a9f29d80b4bd67cb028736c8072f0383
Author: bauerb <bauerb@chromium.org>
Date:   Wed Jul 27 05:15:20 2016 -0700

    Enable Popular Sites in Android demo mode.
    
    BUG=625033
    
    Review-Url: https://codereview.chromium.org/2172203002
    Cr-Original-Commit-Position: refs/heads/master@{#408100}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 65f0f9bcaabb68512ce76b923bc9f3305b235b8d

diff --git a/eclipse/.classpath b/eclipse/.classpath
index dd53ed3..f88e99f 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -49,6 +49,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="components/invalidation/impl/android/javatests/src"/>
     <classpathentry kind="src" path="components/invalidation/impl/android/junit/src"/>
     <classpathentry kind="src" path="components/navigation_interception/android/java/src"/>
+    <classpathentry kind="src" path="components/ntp_tiles/android/java/src"/>
     <classpathentry kind="src" path="components/policy/android/java/src"/>
     <classpathentry kind="src" path="components/precache/android/java/src"/>
     <classpathentry kind="src" path="components/precache/android/javatests/src"/>

commit 3eecef1fcc18ca2e57cde5286820f2143a93b9a6
Author: yolandyan <yolandyan@chromium.org>
Date:   Wed Jul 20 12:18:56 2016 -0700

    Fix error in find annotated tests script
    
    Review-Url: https://codereview.chromium.org/2164073002
    Cr-Original-Commit-Position: refs/heads/master@{#406635}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 36d37849ca31c69c073df3778ae1c04af8db22d5

diff --git a/find_annotated_tests.py b/find_annotated_tests.py
index c6b9662..a32df0a 100755
--- a/find_annotated_tests.py
+++ b/find_annotated_tests.py
@@ -162,7 +162,7 @@ def main():
     script_runtime = datetime.datetime.utcnow()
     script_runtime_string = script_runtime.strftime(_EXPORT_TIME_FORMAT)
   else:
-    script_runtime = arguments.timestamp_string
+    script_runtime_string = arguments.timestamp_string
   logging.info('Build time is %s', script_runtime_string)
   apk_output_dir = os.path.abspath(os.path.join(
       constants.DIR_SOURCE_ROOT, arguments.apk_output_dir))

commit e8def23a0ded70e0b491422672a25c85849ae58f
Author: ianwen <ianwen@chromium.org>
Date:   Wed Jul 20 10:39:57 2016 -0700

    Add AAR support to Chrome and convert support libraries to using AARs
    
    Chrome has been using Jar and resources from support library for years.
    In Q1 2016, Android team stops shipping jars/res for support libray.
    Instead, AAR is promoted, which is a zip that wraps jars and resources.
    
    This CL introduces a utility python script that processes an AAR file.
    In GN gen time, it lists all files in the AAR, yet it does not extract
    it. Actual unpacking is postponed until compilation.
    
    Two other things to notice:
    1. In the old jar we depended on, support-v13 contains support-v4 and
    support-annotations. After converting to AAR, these two libraries are no
    longer part of support-v13. Thus this change needs to be reflected.
    2. In the new AAR format, support-v4 now contains two jars instead of
    one. All public classes are in classes.jar, and all hidden classes are
    in libs/internal_impl-$VERSION.jar.
    
    This work is not possible without bajones@'s pioneering work in
    https://chromiumcodereview.appspot.com/2069273002/
    
    BUG=611171
    
    Review-Url: https://codereview.chromium.org/2156453002
    Cr-Original-Commit-Position: refs/heads/master@{#406603}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c84b9756d482be2561734a19138b41ca630f2701

diff --git a/audio_focus_grabber/BUILD.gn b/audio_focus_grabber/BUILD.gn
index 94ea751..83cb20d 100644
--- a/audio_focus_grabber/BUILD.gn
+++ b/audio_focus_grabber/BUILD.gn
@@ -13,7 +13,7 @@ android_apk("audio_focus_grabber_apk") {
   deps = [
     ":audio_focus_grabber_apk_resources",
     "//base:base_java",
-    "//third_party/android_tools:android_support_v13_java",
+    "//third_party/android_tools:android_support_v4_java",
   ]
 
   java_files = [

commit 4ea4373160a38edf72327993d84afcb222533196
Author: tedchoc <tedchoc@chromium.org>
Date:   Tue Jul 19 15:58:59 2016 -0700

    Update eclipse .classpath file to include custom tab files.
    
    BUG=
    
    Review-Url: https://codereview.chromium.org/2164603003
    Cr-Original-Commit-Position: refs/heads/master@{#406405}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 837f5c15e7fd91aa6e0a67c01781f9b9a417c59e

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 035a6a5..dd53ed3 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -93,6 +93,9 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="third_party/android_protobuf/src/java/src/main/java" including="com/google/protobuf/nano/*"/>
     <classpathentry kind="src" path="third_party/android_swipe_refresh/java/src"/>
     <classpathentry kind="src" path="third_party/cacheinvalidation/src/java"/>
+    <classpathentry kind="src" path="third_party/custom_tabs_client/src/customtabs/src"/>
+    <classpathentry kind="src" path="third_party/custom_tabs_client/src/Application/src/main/java"/>
+    <classpathentry kind="src" path="third_party/custom_tabs_client/src/shared/src/main/java"/>
     <classpathentry kind="src" path="third_party/gif_player/src"/>
     <classpathentry kind="src" path="third_party/jmake/src"/>
     <classpathentry kind="src" path="third_party/junit/src/src/main/java"/>

commit 552909848d3ec75d36505b8f426030cb3a5d5c86
Author: thakis <thakis@chromium.org>
Date:   Mon Jul 18 12:23:19 2016 -0700

    Improve grammar in some comments.
    
    git grep'd for "more then", "less then", "greater then".
    (Interestingly, no hits for "fewer then".)
    
    BUG=none
    NOPRESUBMIT=true
    CQ_INCLUDE_TRYBOTS=tryserver.chromium.linux:closure_compilation;master.tryserver.chromium.linux:closure_compilation
    
    Review-Url: https://codereview.chromium.org/2157963002
    Cr-Original-Commit-Position: refs/heads/master@{#406059}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e5b5786ad5d329fbfc39a32896ad4686a8eda112

diff --git a/forwarder2/socket.h b/forwarder2/socket.h
index acea0ec..5e68f16 100644
--- a/forwarder2/socket.h
+++ b/forwarder2/socket.h
@@ -46,7 +46,7 @@ class Socket {
   int GetPort();
 
   // Just a wrapper around unix read() function.
-  // Reads up to buffer_size, but may read less then buffer_size.
+  // Reads up to buffer_size, but may read less than buffer_size.
   // Returns the number of bytes read.
   int Read(void* buffer, size_t buffer_size);
 

commit e4669bc06708f2126d4349f6d2c28638a0f72412
Author: thestig <thestig@chromium.org>
Date:   Wed Jul 13 15:36:34 2016 -0700

    Use base::StartWith() in more places when appropriate.
    
    TBR=thakis@chromium.org
    
    Review-Url: https://codereview.chromium.org/2127373006
    Cr-Original-Commit-Position: refs/heads/master@{#405308}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 30d94e4734a194c1524403c1f2edb222ffe35531

diff --git a/forwarder2/host_forwarder_main.cc b/forwarder2/host_forwarder_main.cc
index 2fae616..9613660 100644
--- a/forwarder2/host_forwarder_main.cc
+++ b/forwarder2/host_forwarder_main.cc
@@ -240,7 +240,7 @@ class HostControllersManager {
     const std::string prefix = base::StringPrintf("%d:", port);
     for (HostControllerMap::const_iterator others = controllers_->begin();
          others != controllers_->end(); ++others) {
-      if (others->first.find(prefix) == 0U)
+      if (base::StartsWith(others->first, prefix, base::CompareCase::SENSITIVE))
         return;
     }
     // No other port is being forwarded to this device:
@@ -318,7 +318,7 @@ class HostControllersManager {
 
 class ServerDelegate : public Daemon::ServerDelegate {
  public:
-  ServerDelegate(const std::string& adb_path)
+  explicit ServerDelegate(const std::string& adb_path)
       : adb_path_(adb_path), has_failed_(false) {}
 
   bool has_failed() const {
@@ -370,7 +370,7 @@ class ServerDelegate : public Daemon::ServerDelegate {
 
 class ClientDelegate : public Daemon::ClientDelegate {
  public:
-  ClientDelegate(const base::Pickle& command_pickle)
+  explicit ClientDelegate(const base::Pickle& command_pickle)
       : command_pickle_(command_pickle), has_failed_(false) {}
 
   bool has_failed() const { return has_failed_; }

commit 8e7230933bd4b5326fc8ff05dbe5d54a27f0cc2e
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jul 13 01:33:56 2016 -0700

    sandwich: Uses original WPR archive for prefetch benchmark runs
    
    Before, Sandwich was using the patched WPR archive with headers
    ensure the resource get cached. But that could lead to non realistic
    page load time in theory if multiple requests are issued on uncachable
    resources.
    
    This CL fixes this issue by using the original WPR archive when running
    the benchmarks for NoState-Prefetch.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2144633002
    Cr-Original-Commit-Position: refs/heads/master@{#405081}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4f7630de82bf4b4bd0c6b13a6196c9ae57c61052

diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index 22c9b00..0f53263 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -651,7 +651,7 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
       runner = self._common_builder.CreateSandwichRunner()
       for transformer in transformer_list:
         transformer(runner)
-      runner.wpr_archive_path = self._wpr_archive_path
+      runner.wpr_archive_path = self._common_builder.original_wpr_task.path
       runner.wpr_out_log_path = os.path.join(
           RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
       runner.cache_archive_path = BuildBenchmarkCacheArchive.path

commit d75322fb44a694817c638bbd3cc826471b36eab2
Author: gabadie <gabadie@chromium.org>
Date:   Mon Jul 11 06:06:21 2016 -0700

    sandwich: Fix some trivial failures on SWR benchmark runs
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2138713002
    Cr-Original-Commit-Position: refs/heads/master@{#404645}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 81bf8788e08a8bdb1d0b68203ae2acb1d3d4fd3d

diff --git a/loading/request_track.py b/loading/request_track.py
index 454cbe0..a0542dc 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -341,7 +341,7 @@ class Request(object):
     directives = [s.strip() for s in cache_control_str.split(',')]
     for directive in directives:
       parts = directive.split('=')
-      if len(parts) == 1:
+      if len(parts) != 2:
         continue
       (name, value) = parts
       if name == directive_name:
@@ -385,10 +385,12 @@ class Request(object):
     net::HttpResponseHeaders's constructor.
     """
     assert not self.IsDataRequest()
-    headers = '{} {} {}\x00'.format(
-        self.protocol.upper(), self.status, self.status_text)
+    assert self.HasReceivedResponse()
+    headers = bytes('{} {} {}\x00'.format(
+        self.protocol.upper(), self.status, self.status_text))
     for key in sorted(self.response_headers.keys()):
-      headers += '{}: {}\x00'.format(key, self.response_headers[key])
+      headers += (bytes(key.encode('latin-1')) + b': ' +
+          bytes(self.response_headers[key].encode('latin-1')) + b'\x00')
     return headers
 
   def __eq__(self, o):
diff --git a/loading/sandwich_swr.py b/loading/sandwich_swr.py
index a902f4d..e6b3736 100644
--- a/loading/sandwich_swr.py
+++ b/loading/sandwich_swr.py
@@ -96,6 +96,8 @@ def _BuildBenchmarkCache(
         cache_backend.DeleteKey(request.url)
         delete_count += 1
         continue
+      if not request.HasReceivedResponse():
+        continue
       if request.url in urls_to_enable_swr:
         request.SetHTTPResponseHeader(
             'cache-control', 'max-age=0,stale-while-revalidate=315360000')
diff --git a/loading/sandwich_utils.py b/loading/sandwich_utils.py
index b5ec497..eda55e0 100644
--- a/loading/sandwich_utils.py
+++ b/loading/sandwich_utils.py
@@ -35,6 +35,8 @@ def FilterOutDataAndIncompleteRequests(requests):
     if request.protocol is None:
       assert not request.HasReceivedResponse()
       continue
+    if request.protocol in {'about'}:
+      continue
     if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
       raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
     yield request

commit e804469272ddeb9dc6652f3467ac8c8da973e1cb
Author: gabadie <gabadie@chromium.org>
Date:   Mon Jul 11 02:50:20 2016 -0700

    sandwich: Fixes a bug when served from cache request are before.
    
    Before, prefetch experiment's CSV collection was maintaining a dict
    of transfered bytes per URLs as it was iterating over all resource
    requests and served from memory cache URLs request would just grabe
    the size from this dictionary. But we may have requests served from
    memory cache before original request that filled the memory cache,
    leading to a KeyError.
    
    This CL fixes this issue by first building the dictionary by going
    over resources requests, and then actually sum the bytes using this
    dictionary, avoiding the KeyError.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2134763002
    Cr-Original-Commit-Position: refs/heads/master@{#404626}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 866025e8432bd4a23c221189be36696113e87c32

diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index 456c697..22c9b00 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -414,16 +414,14 @@ def _ProcessRunOutputDir(
     run_output_verifier.VerifyTrace(trace)
 
     logging.info('extracting metrics from trace: %s', trace_path)
-    served_from_network_bytes = 0
-    served_from_cache_bytes = 0
-    urls_hitting_network = set()
+
+    # Gather response size per URLs.
     response_sizes = {}
     for request in sandwich_utils.FilterOutDataAndIncompleteRequests(
         trace.request_track.GetEvents()):
       # Ignore requests served from the blink's cache.
       if request.served_from_cache:
         continue
-      urls_hitting_network.add(request.url)
       if request.from_disk_cache:
         if request.url in cached_encoded_data_lengths:
           response_size = cached_encoded_data_lengths[request.url]
@@ -433,13 +431,27 @@ def _ProcessRunOutputDir(
           # load.
           logging.warning('Looks like could be served from memory cache: %s',
               request.url)
-          response_size = response_sizes[request.url]
-        served_from_cache_bytes += response_size
+          if request.url in response_sizes:
+            response_size = response_sizes[request.url]
       else:
         response_size = request.GetResponseTransportLength()
-        served_from_network_bytes += response_size
       response_sizes[request.url] = response_size
 
+    # Sums the served from cache/network bytes.
+    served_from_network_bytes = 0
+    served_from_cache_bytes = 0
+    urls_hitting_network = set()
+    for request in sandwich_utils.FilterOutDataAndIncompleteRequests(
+        trace.request_track.GetEvents()):
+      # Ignore requests served from the blink's cache.
+      if request.served_from_cache:
+        continue
+      urls_hitting_network.add(request.url)
+      if request.from_disk_cache:
+        served_from_cache_bytes += response_sizes[request.url]
+      else:
+        served_from_network_bytes += response_sizes[request.url]
+
     # Make sure the served from blink's cache requests have at least one
     # corresponding request that was not served from the blink's cache.
     for request in sandwich_utils.FilterOutDataAndIncompleteRequests(

commit a0a914a516ee12e86b780d30fb2d62016af73f5a
Author: gabadie <gabadie@chromium.org>
Date:   Fri Jul 8 09:48:47 2016 -0700

    sandwich: Handle 200 responses with no data.
    
    Before sandwich was asserting that the request encoded_data_length > 0.
    But it turn out that encoded_data_length does not include the response
    headers' length. As a result, this assertion was failing on some website
    doing XHR requests that were returning 200 response.
    
    This CL fixes this issue by correctly computing the total number of
    byte transfered back to chrome with GetResponseTransportLength(),
    including the response headers size.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2131973002
    Cr-Original-Commit-Position: refs/heads/master@{#404415}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2dad1bfcf999e58230aabcba1e75b5e994b31fcc

diff --git a/loading/request_track.py b/loading/request_track.py
index 89b4f35..454cbe0 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -209,6 +209,7 @@ class Request(object):
     self.timing = None
     self.status = None
     self.status_text = None
+    self.response_headers_length = 0
     self.encoded_data_length = 0
     self.data_chunks = []
     self.failed = False
@@ -253,24 +254,20 @@ class Request(object):
       result.timing = Timing(request_time=result.timestamp)
     return result
 
-  def GetEncodedDataLength(self):
+  def GetResponseTransportLength(self):
     """Get the total amount of encoded data no matter whether load has finished
     or not.
     """
     assert self.HasReceivedResponse()
     assert not self.from_disk_cache and not self.served_from_cache
-    assert self.protocol != 'about'
-    if self.failed:
-      # TODO(gabadie): Once crbug.com/622018 is fixed, remove this branch.
-      return 0
+    assert self.protocol not in {'about', 'blob', 'data'}
     if self.timing.loading_finished != Timing.UNVAILABLE:
       encoded_data_length = self.encoded_data_length
-      assert encoded_data_length > 0
     else:
       encoded_data_length = sum(
           [chunk_size for _, chunk_size in self.data_chunks])
       assert encoded_data_length > 0 or len(self.data_chunks) == 0
-    return encoded_data_length
+    return encoded_data_length + self.response_headers_length
 
   def GetHTTPResponseHeader(self, header_name):
     """Gets the value of a HTTP response header.
@@ -736,7 +733,7 @@ class RequestTrack(devtools_monitor.Track):
 
     _CopyFromDictToObject(redirect_response, r,
                           (('headers', 'response_headers'),
-                           ('encodedDataLength', 'encoded_data_length'),
+                           ('encodedDataLength', 'response_headers_length'),
                            ('fromDiskCache', 'from_disk_cache'),
                            ('protocol', 'protocol'), ('status', 'status'),
                            ('statusText', 'status_text')))
@@ -792,6 +789,7 @@ class RequestTrack(devtools_monitor.Track):
                       # Actual request headers are not known before reaching the
                       # network stack.
                       ('requestHeaders', 'request_headers'),
+                      ('encodedDataLength', 'response_headers_length'),
                       ('headers', 'response_headers')))
     timing_dict = {}
     # Some URLs don't have a timing dict (e.g. data URLs), and timings for
@@ -822,8 +820,6 @@ class RequestTrack(devtools_monitor.Track):
     assert (status == RequestTrack._STATUS_RESPONSE
             or status == RequestTrack._STATUS_DATA)
     r.encoded_data_length = params['encodedDataLength']
-    assert (r.encoded_data_length > 0 or r.protocol in {'about', 'data'} or
-            r.from_disk_cache or r.served_from_cache)
     r.timing.loading_finished = r._TimestampOffsetFromStartMs(
         params['timestamp'])
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index 0201d8b..456c697 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -354,10 +354,10 @@ def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
     if request.url in effective_encoded_data_lengths:
       effective_encoded_data_lengths[request.url] = max(
           effective_encoded_data_lengths[request.url],
-          request.GetEncodedDataLength())
+          request.GetResponseTransportLength())
     else:
       effective_encoded_data_lengths[request.url] = (
-          request.GetEncodedDataLength())
+          request.GetResponseTransportLength())
 
   upload_data_stream_cache_entry_keys = set()
   upload_data_stream_requests = set()
@@ -436,7 +436,7 @@ def _ProcessRunOutputDir(
           response_size = response_sizes[request.url]
         served_from_cache_bytes += response_size
       else:
-        response_size = request.GetEncodedDataLength()
+        response_size = request.GetResponseTransportLength()
         served_from_network_bytes += response_size
       response_sizes[request.url] = response_size
 

commit 4ed69a608899391defd9061663ce12937df57d0f
Author: gabadie <gabadie@chromium.org>
Date:   Fri Jul 8 08:02:30 2016 -0700

    sandwich: Update SWR benchmarks to enable SWR on subsets of resources
    
    Before, the Stale-While-Revalidate experiment was just making sure
    this feature could have a performance improvement on page loading.
    
    This CL implement the more detailed to see how would the page loads
    would be improved if a list of domains know third_parties would
    have a Stale-While-Revalidate on all their resources.
    
    This CL achieve this by doing the following:
     - Patch WPR so that all resource are going to the cache
     - Record original cache
     - Setup the benchmark producing the list of URL to enable SWR
     - Create the benchmark cache by:
        - Remove No-Store resources
        - Adding the SWR header on URL that requiere
        - And restore all other headers so that response headers such as
          Set-Cookie are still in the cache to avoid entropy caused by
          different cookie values.
     - Run the benchmark
     - Extract metrics
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2092023003
    Cr-Original-Commit-Position: refs/heads/master@{#404389}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b4e0bbf41da723ecd135ac467649561603b3efe0

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 4041943..ffd7b00 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -10,9 +10,11 @@ WepPageReplay serving all connections.
 """
 
 import argparse
+import csv
 import json
 import logging
 import os
+import re
 import sys
 from urlparse import urlparse
 import yaml
@@ -115,8 +117,14 @@ def _ArgumentParser():
       help='Setup all NoState-Prefetch benchmarks.')
 
   # Setup Stale-While-Revalidate benchmarks subcommand.
-  subparsers.add_parser('setup-swr', parents=[sandwich_setup_parser],
+  swr_setup_parser = subparsers.add_parser('setup-swr',
+      parents=[sandwich_setup_parser],
       help='Setup all Stale-While-Revalidate benchmarks.')
+  swr_setup_parser.add_argument('-d', '--domains-csv',
+      type=argparse.FileType('r'), required=True,
+      help='Path of the CSV containing the pattern of domains in a '
+           '`domain-patterns` column and a `usage` column in percent in how '
+           'likely they are in a page load.')
 
   # Run benchmarks subcommand (used in _RunBenchmarkMain).
   subparsers.add_parser('run', parents=[task_manager.CommandLineParser()],
@@ -164,25 +172,46 @@ def _GenerateNoStatePrefetchBenchmarkTasks(
 
 
 def _SetupStaleWhileRevalidateBenchmark(args):
-  del args # unused.
+  domain_regexes = []
+  for row in csv.DictReader(args.domains_csv):
+    domain_patterns = json.loads('[{}]'.format(row['domain-patterns']))
+    for domain_pattern in domain_patterns:
+      domain_pattern_escaped = r'(\.|^){}$'.format(re.escape(domain_pattern))
+      domain_regexes.append({
+          'usage': float(row['usage']),
+          'domain_regex': domain_pattern_escaped.replace(r'\?', r'\w*')})
   return {
-    'network_conditions': ['Regular3G', 'Regular2G']
+    'domain_regexes': domain_regexes,
+    'network_conditions': ['Regular3G', 'Regular2G'],
+    'usage_thresholds': [1, 3, 5, 10]
   }
 
 
 def _GenerateStaleWhileRevalidateBenchmarkTasks(
     common_builder, main_transformer, benchmark_setup):
+  # Compile domain regexes.
+  domain_regexes = []
+  for e in benchmark_setup['domain_regexes']:
+     domain_regexes.append({
+        'usage': e['usage'],
+        'domain_regex': re.compile(e['domain_regex'])})
+
+  # Build tasks.
   builder = sandwich_swr.StaleWhileRevalidateBenchmarkBuilder(common_builder)
-  for enable_swr in [False, True]:
-    builder.PopulateBenchmark(enable_swr, _MAIN_TRANSFORMER_LIST_NAME,
-                              transformer_list=[main_transformer])
-    for network_condition in benchmark_setup['network_conditions']:
-      transformer_list_name = network_condition.lower()
-      network_transformer = \
-          sandwich_utils.NetworkSimulationTransformer(network_condition)
-      transformer_list = [main_transformer, network_transformer]
+  for network_condition in benchmark_setup['network_conditions']:
+    transformer_list_name = network_condition.lower()
+    network_transformer = \
+        sandwich_utils.NetworkSimulationTransformer(network_condition)
+    transformer_list = [main_transformer, network_transformer]
+    builder.PopulateBenchmark(
+        'no-swr', [], transformer_list_name, transformer_list)
+    for usage_threshold in benchmark_setup['usage_thresholds']:
+      benchmark_name = 'threshold{}'.format(usage_threshold)
+      selected_domain_regexes = [e['domain_regex'] for e in domain_regexes
+          if e['usage'] > usage_threshold]
       builder.PopulateBenchmark(
-          enable_swr, transformer_list_name, transformer_list)
+          benchmark_name, selected_domain_regexes,
+          transformer_list_name, transformer_list)
 
 
 _TASK_GENERATORS = {
diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index 0d261b8..0201d8b 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -77,60 +77,6 @@ def _NormalizeUrl(url):
   return urlparse.urlunparse(parsed_url)
 
 
-def _PatchWpr(wpr_archive):
-  """Patches a WPR archive to get all resources into the HTTP cache and avoid
-  invalidation and revalidations.
-
-  Args:
-    wpr_archive: wpr_backend.WprArchiveBackend WPR archive to patch.
-  """
-  # Sets the resources cache max-age to 10 years.
-  MAX_AGE = 10 * 365 * 24 * 60 * 60
-  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
-
-  logging.info('number of entries: %d', len(wpr_archive.ListUrlEntries()))
-  patched_entry_count = 0
-  for url_entry in wpr_archive.ListUrlEntries():
-    response_headers = url_entry.GetResponseHeadersDict()
-    if 'cache-control' in response_headers and \
-        response_headers['cache-control'] == CACHE_CONTROL:
-      continue
-    # Override the cache-control header to set the resources max age to MAX_AGE.
-    #
-    # Important note: Some resources holding sensitive information might have
-    # cache-control set to no-store which allow the resource to be cached but
-    # not cached in the file system. NoState-Prefetch is going to take care of
-    # this case. But in here, to simulate NoState-Prefetch, we don't have other
-    # choices but save absolutely all cached resources on disk so they survive
-    # after killing chrome for cache save, modification and push.
-    url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
-
-    # TODO(gabadie): May need to extend Vary blacklist (referer?)
-    #
-    # All of these Vary and Pragma possibilities need to be removed from
-    # response headers in order for Chrome to store a resource in HTTP cache and
-    # not to invalidate it.
-    url_entry.RemoveResponseHeaderDirectives('vary', {'*', 'cookie'})
-    url_entry.RemoveResponseHeaderDirectives('pragma', {'no-cache'})
-    patched_entry_count += 1
-  logging.info('number of entries patched: %d', patched_entry_count)
-
-
-def _FilterOutDataAndIncompleteRequests(requests):
-  for request in filter(lambda r: not r.IsDataRequest(), requests):
-    # The protocol is only known once the response has been received. But the
-    # trace recording might have been stopped with still some JavaScript
-    # originated requests that have not received any responses yet.
-    if request.protocol is None:
-      assert not request.HasReceivedResponse()
-      continue
-    if request.protocol == 'about':
-      continue
-    if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
-      raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
-    yield request
-
-
 def _PatchCacheArchive(cache_archive_path, loading_trace_path,
                        cache_archive_dest_path):
   """Patch the cache archive.
@@ -153,7 +99,7 @@ def _PatchCacheArchive(cache_archive_path, loading_trace_path,
     cache_backend = chrome_cache.CacheBackend(cache_path, 'simple')
     cache_entries = set(cache_backend.ListKeys())
     logging.info('Original cache size: %d bytes' % cache_backend.GetSize())
-    for request in _FilterOutDataAndIncompleteRequests(
+    for request in sandwich_utils.FilterOutDataAndIncompleteRequests(
         trace.request_track.GetEvents()):
       # On requests having an upload data stream such as POST requests,
       # net::HttpCache::GenerateCacheKey() prefixes the cache entry's key with
@@ -257,7 +203,8 @@ def _ExtractDiscoverableUrls(
         dependencies_lens, subresource_discoverer)
 
   whitelisted_urls = set()
-  for request in _FilterOutDataAndIncompleteRequests(discovered_requests):
+  for request in sandwich_utils.FilterOutDataAndIncompleteRequests(
+      discovered_requests):
     logging.debug('white-listing %s', request.url)
     whitelisted_urls.add(request.url)
   logging.info('number of white-listed resources: %d', len(whitelisted_urls))
@@ -289,37 +236,6 @@ def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
     logging.error('+     ' + url)
 
 
-class _RequestOutcome:
-  All, ServedFromCache, NotServedFromCache, Post = range(4)
-
-
-def _ListUrlRequests(trace, request_kind):
-  """Lists requested URLs from a trace.
-
-  Args:
-    trace: (loading_trace.LoadingTrace) loading trace.
-    request_kind: _RequestOutcome indicating the subset of requests to output.
-
-  Returns:
-    set([str])
-  """
-  urls = set()
-  for request_event in _FilterOutDataAndIncompleteRequests(
-      trace.request_track.GetEvents()):
-    if (request_kind == _RequestOutcome.ServedFromCache and
-        request_event.from_disk_cache):
-      urls.add(request_event.url)
-    elif (request_kind == _RequestOutcome.Post and
-        request_event.method.upper().strip() == 'POST'):
-      urls.add(request_event.url)
-    elif (request_kind == _RequestOutcome.NotServedFromCache and
-        not request_event.from_disk_cache):
-      urls.add(request_event.url)
-    elif request_kind == _RequestOutcome.All:
-      urls.add(request_event.url)
-  return urls
-
-
 class _RunOutputVerifier(object):
   """Object to verify benchmark run from traces and WPR log stored in the
   runner output directory.
@@ -347,12 +263,14 @@ class _RunOutputVerifier(object):
     """Verifies a trace with the cache validation result and the benchmark
     setup.
     """
-    effective_requests = _ListUrlRequests(trace, _RequestOutcome.All)
-    effective_post_requests = _ListUrlRequests(trace, _RequestOutcome.Post)
-    effective_cached_requests = \
-        _ListUrlRequests(trace, _RequestOutcome.ServedFromCache)
-    effective_uncached_requests = \
-        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache)
+    effective_requests = sandwich_utils.ListUrlRequests(
+        trace, sandwich_utils.RequestOutcome.All)
+    effective_post_requests = sandwich_utils.ListUrlRequests(
+        trace, sandwich_utils.RequestOutcome.Post)
+    effective_cached_requests = sandwich_utils.ListUrlRequests(
+        trace, sandwich_utils.RequestOutcome.ServedFromCache)
+    effective_uncached_requests = sandwich_utils.ListUrlRequests(
+        trace, sandwich_utils.RequestOutcome.NotServedFromCache)
 
     missing_requests = self._original_requests.difference(effective_requests)
     unexpected_requests = effective_requests.difference(self._original_requests)
@@ -422,10 +340,12 @@ def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
     cache_keys = set(
         chrome_cache.CacheBackend(cache_directory, 'simple').ListKeys())
   trace = loading_trace.LoadingTrace.FromJsonFile(cache_build_trace_path)
-  effective_requests = _ListUrlRequests(trace, _RequestOutcome.All)
-  effective_post_requests = _ListUrlRequests(trace, _RequestOutcome.Post)
+  effective_requests = sandwich_utils.ListUrlRequests(
+      trace, sandwich_utils.RequestOutcome.All)
+  effective_post_requests = sandwich_utils.ListUrlRequests(
+      trace, sandwich_utils.RequestOutcome.Post)
   effective_encoded_data_lengths = {}
-  for request in _FilterOutDataAndIncompleteRequests(
+  for request in sandwich_utils.FilterOutDataAndIncompleteRequests(
       trace.request_track.GetEvents()):
     if request.from_disk_cache or request.served_from_cache:
       # At cache archive creation time, a request might be loaded several times,
@@ -498,7 +418,7 @@ def _ProcessRunOutputDir(
     served_from_cache_bytes = 0
     urls_hitting_network = set()
     response_sizes = {}
-    for request in _FilterOutDataAndIncompleteRequests(
+    for request in sandwich_utils.FilterOutDataAndIncompleteRequests(
         trace.request_track.GetEvents()):
       # Ignore requests served from the blink's cache.
       if request.served_from_cache:
@@ -522,7 +442,7 @@ def _ProcessRunOutputDir(
 
     # Make sure the served from blink's cache requests have at least one
     # corresponding request that was not served from the blink's cache.
-    for request in _FilterOutDataAndIncompleteRequests(
+    for request in sandwich_utils.FilterOutDataAndIncompleteRequests(
         trace.request_track.GetEvents()):
       assert (request.url in urls_hitting_network or
               not request.served_from_cache)
@@ -537,12 +457,12 @@ def _ProcessRunOutputDir(
             len(cache_validation_result['successfully_cached_resources']),
         'cache_recording.cached_subresource_count':
             len(cache_validation_result['expected_cached_resources']),
-        'benchmark.subresource_count': len(_ListUrlRequests(
-            trace, _RequestOutcome.All)),
+        'benchmark.subresource_count': len(sandwich_utils.ListUrlRequests(
+            trace, sandwich_utils.RequestOutcome.All)),
         'benchmark.served_from_cache_count_theoretic':
             len(benchmark_setup['cache_whitelist']),
-        'benchmark.served_from_cache_count': len(_ListUrlRequests(
-            trace, _RequestOutcome.ServedFromCache)),
+        'benchmark.served_from_cache_count': len(sandwich_utils.ListUrlRequests(
+            trace, sandwich_utils.RequestOutcome.ServedFromCache)),
         'benchmark.served_from_network_bytes': served_from_network_bytes,
         'benchmark.served_from_cache_bytes': served_from_cache_bytes
     }
@@ -594,7 +514,6 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
     @self.RegisterTask('common/webpages-patched.wpr',
                        dependencies=[self._common_builder.original_wpr_task])
     def BuildPatchedWpr():
-      common_util.EnsureParentDirectoryExists(BuildPatchedWpr.path)
       shutil.copyfile(
           self._common_builder.original_wpr_task.path, BuildPatchedWpr.path)
       wpr_archive = wpr_backend.WprArchiveBackend(BuildPatchedWpr.path)
@@ -615,7 +534,10 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
         json.dump(original_response_headers, file_output)
 
       # Patch WPR.
-      _PatchWpr(wpr_archive)
+      wpr_url_entries = wpr_archive.ListUrlEntries()
+      for wpr_url_entry in wpr_url_entries:
+        sandwich_utils.PatchWprEntryToBeCached(wpr_url_entry)
+      logging.info('number of patched entries: %d', len(wpr_url_entries))
       wpr_archive.Persist()
 
     @self.RegisterTask('common/original-cache.zip', [BuildPatchedWpr])
diff --git a/loading/sandwich_swr.py b/loading/sandwich_swr.py
index 46910c4..a902f4d 100644
--- a/loading/sandwich_swr.py
+++ b/loading/sandwich_swr.py
@@ -2,10 +2,33 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+""" This module implements the Stale-While-Revalidate performance improvement
+experiment on third parties' resources.
+
+The top level operations of the experiment are:
+  1. Record WPR archive;
+  2. Create a patched WPR archive so that all resource are getting cached;
+  3. Record original cache using the patched WPR archive;
+  4. Setup the benchmark producing the list of URL to enable SWR in a JSON file;
+  5. Create the benchmark cache by:
+     - Remove No-Store resources;
+     - Adding the SWR header on resources that are experimentally required to
+       have it;
+     - Patch SWR header on resources that already had it to make sure the
+       the SWR freshness is not out of date;
+     - And restore all other headers so that response headers such as
+       Set-Cookie are still in the cache to avoid entropy caused by
+       different cookie values.
+  6. Run the benchmark;
+  7. Extract metrics into CSV files.
+"""
+
 import csv
+import json
 import logging
 import os
 import shutil
+from urlparse import urlparse
 
 import chrome_cache
 import common_util
@@ -13,16 +36,54 @@ import loading_trace
 import request_track
 import sandwich_metrics
 import sandwich_runner
+import sandwich_utils
 import task_manager
+import wpr_backend
+
+
+def _ExtractRegexMatchingUrls(urls, domain_regexes):
+  urls_to_enable = set()
+  for url in urls:
+    if url in urls_to_enable:
+      continue
+    parsed_url = urlparse(url)
+    for domain_regex in domain_regexes:
+      if domain_regex.search(parsed_url.netloc):
+        urls_to_enable.add(url)
+        break
+  return urls_to_enable
+
+
+def _BuildBenchmarkCache(
+    original_wpr_trace_path, urls_to_enable_swr,
+    original_cache_trace_path, original_cache_archive_path,
+    cache_archive_dest_path):
+  # Load trace that was generated at original cache creation.
+  logging.info('loading %s', original_wpr_trace_path)
+  trace = loading_trace.LoadingTrace.FromJsonFile(original_wpr_trace_path)
 
+  # Lists URLs that should not be in the cache or already have SWR headers.
+  urls_should_not_be_cached = set()
+  urls_already_with_swr = set()
+  for request in trace.request_track.GetEvents():
+    caching_policy = request_track.CachingPolicy(request)
+    if not caching_policy.IsCacheable():
+      urls_should_not_be_cached.add(request.url)
+    elif caching_policy.GetFreshnessLifetimes()[1] > 0:
+      urls_already_with_swr.add(request.url)
+  # Trace are fat, kill this one to save up memory for the next one to load in
+  # this scope.
+  del trace
 
-def _BuildPatchedCache(original_cache_run_path, original_cache_archive_path,
-      cache_archive_dest_path):
-  CACHE_CONTROL_VALUE = 'max-age=0,stale-while-revalidate=315360000'
-  trace_path = os.path.join(
-      original_cache_run_path, '0', sandwich_runner.TRACE_FILENAME)
-  trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
-  patch_count = 0
+  # Load trace that was generated at original cache creation.
+  logging.info('loading %s', original_cache_trace_path)
+  trace = loading_trace.LoadingTrace.FromJsonFile(original_cache_trace_path)
+
+  # Create cache contents.
+  delete_count = 0
+  swr_patch_count = 0
+  originaly_swr_patch_count = 0
+  noswr_patch_count = 0
   with common_util.TemporaryDirectory(prefix='sandwich_tmp') as tmp_path:
     cache_path = os.path.join(tmp_path, 'cache')
     chrome_cache.UnzipDirectoryContent(original_cache_archive_path, cache_path)
@@ -31,18 +92,77 @@ def _BuildPatchedCache(original_cache_run_path, original_cache_archive_path,
     for request in trace.request_track.GetEvents():
       if request.url not in cache_keys:
         continue
-      caching_policy = request_track.CachingPolicy(request)
-      assert caching_policy.IsCacheable()
-      freshness = caching_policy.GetFreshnessLifetimes()
-      if freshness[0] == 0:
+      if request.url in urls_should_not_be_cached:
+        cache_backend.DeleteKey(request.url)
+        delete_count += 1
         continue
-      request.SetHTTPResponseHeader('cache-control', CACHE_CONTROL_VALUE)
+      if request.url in urls_to_enable_swr:
+        request.SetHTTPResponseHeader(
+            'cache-control', 'max-age=0,stale-while-revalidate=315360000')
+        request.SetHTTPResponseHeader(
+            'last-modified', 'Thu, 23 Jun 2016 11:30:00 GMT')
+        swr_patch_count += 1
+      elif request.url in urls_already_with_swr:
+        # Force to use SWR on resources that originally attempted to use it.
+        request.SetHTTPResponseHeader(
+            'cache-control', 'max-age=0,stale-while-revalidate=315360000')
+        # The resource originally had SWR enabled therefore we don't
+        # Last-Modified to repro exactly the performance impact in case these
+        # headers were not set properly causing an invalidation instead of a
+        # revalidation.
+        originaly_swr_patch_count += 1
+      else:
+        # Force synchronous revalidation.
+        request.SetHTTPResponseHeader('cache-control', 'max-age=0')
+        noswr_patch_count += 1
       raw_headers = request.GetRawResponseHeaders()
       cache_backend.UpdateRawResponseHeaders(request.url, raw_headers)
-      patch_count += 1
     chrome_cache.ZipDirectoryContent(cache_path, cache_archive_dest_path)
-  logging.info('Patched %d cached resources out of %d' % (
-      patch_count, len(cache_keys)))
+  logging.info('patched %d cached resources with forced SWR', swr_patch_count)
+  logging.info('patched %d cached resources with original SWR',
+      originaly_swr_patch_count)
+  logging.info('patched %d cached resources without SWR', noswr_patch_count)
+  logging.info('deleted %d cached resources', delete_count)
+
+
+def _ProcessRunOutputDir(benchmark_setup, runner_output_dir):
+  """Process benchmark's run output directory.
+
+  Args:
+    cache_validation_result: Same as for _RunOutputVerifier
+    benchmark_setup: Same as for _RunOutputVerifier
+    runner_output_dir: Same as for SandwichRunner.output_dir
+
+  Returns:
+    List of dictionary.
+  """
+  run_metrics_list = []
+  for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
+      runner_output_dir):
+    trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
+    logging.info('processing trace: %s', trace_path)
+    trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
+    served_from_cache_urls = sandwich_utils.ListUrlRequests(
+        trace, sandwich_utils.RequestOutcome.ServedFromCache)
+    matching_subresource_count_used_from_cache = (
+        served_from_cache_urls.intersection(
+            set(benchmark_setup['urls_to_enable_swr'])))
+    run_metrics = {
+        'url': trace.url,
+        'repeat_id': repeat_id,
+        'benchmark_name': benchmark_setup['benchmark_name'],
+        'cache_recording.subresource_count':
+            len(benchmark_setup['effective_subresource_urls']),
+        'cache_recording.matching_subresource_count':
+            len(benchmark_setup['urls_to_enable_swr']),
+        'benchmark.matching_subresource_count_used_from_cache':
+            len(matching_subresource_count_used_from_cache)
+    }
+    run_metrics.update(
+        sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
+            repeat_dir, trace))
+    run_metrics_list.append(run_metrics)
+  return run_metrics_list
 
 
 class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
@@ -54,99 +174,148 @@ class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
                                   common_builder.output_directory,
                                   common_builder.output_subdirectory)
     self._common_builder = common_builder
-    self._patched_cache_task = None
+    self._patched_wpr_path = None
+    self._original_cache_task = None
+    self._original_cache_trace_path = None
     self._PopulateCommonPipelines()
 
   def _PopulateCommonPipelines(self):
     """Creates necessary tasks to produce initial cache archives.
 
     Here is the full dependency tree for the returned task:
-    common/patched-cache.zip
-      depends on: common/original-cache.zip
+    depends on: common/original-cache.zip
+      depends on: common/webpages-patched.wpr
         depends on: common/webpages.wpr
     """
-    @self.RegisterTask('common/original-cache.zip',
+    @self.RegisterTask('common/webpages-patched.wpr',
                        dependencies=[self._common_builder.original_wpr_task])
+    def BuildPatchedWpr():
+      shutil.copyfile(
+          self._common_builder.original_wpr_task.path, BuildPatchedWpr.path)
+      wpr_archive = wpr_backend.WprArchiveBackend(BuildPatchedWpr.path)
+      wpr_url_entries = wpr_archive.ListUrlEntries()
+      for wpr_url_entry in wpr_url_entries:
+        sandwich_utils.PatchWprEntryToBeCached(wpr_url_entry)
+      logging.info('number of patched entries: %d', len(wpr_url_entries))
+      wpr_archive.Persist()
+
+    @self.RegisterTask('common/original-cache.zip',
+                       dependencies=[BuildPatchedWpr])
     def BuildOriginalCache():
       runner = self._common_builder.CreateSandwichRunner()
-      runner.wpr_archive_path = self._common_builder.original_wpr_task.path
+      runner.wpr_archive_path = BuildPatchedWpr.path
       runner.cache_archive_path = BuildOriginalCache.path
       runner.cache_operation = sandwich_runner.CacheOperation.SAVE
       runner.output_dir = BuildOriginalCache.run_path
       runner.Run()
     BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
 
-    @self.RegisterTask('common/patched-cache.zip',
-                       dependencies=[BuildOriginalCache])
-    def BuildPatchedCache():
-      _BuildPatchedCache(BuildOriginalCache.run_path, BuildOriginalCache.path,
-                         BuildPatchedCache.path)
-
-    self._patched_cache_task = BuildPatchedCache
+    self._original_cache_trace_path = os.path.join(
+        BuildOriginalCache.run_path, '0', sandwich_runner.TRACE_FILENAME)
+    self._patched_wpr_path = BuildPatchedWpr.path
+    self._original_cache_task = BuildOriginalCache
 
-  def PopulateBenchmark(self, enable_swr, transformer_list_name,
-                        transformer_list):
+  def PopulateBenchmark(self, benchmark_name, domain_regexes,
+                        transformer_list_name, transformer_list):
     """Populate benchmarking tasks.
 
     Args:
-      enable_swr: Enable SWR patching or not.
+      benchmark_name: Name of the benchmark.
+      domain_regexes: Compiled regexes of domains to enable SWR.
       transformer_list_name: A string describing the transformers, will be used
           in Task names (prefer names without spaces and special characters).
       transformer_list: An ordered list of function that takes an instance of
           SandwichRunner as parameter, would be applied immediately before
           SandwichRunner.Run() in the given order.
 
+
     Here is the full dependency of the added tree for the returned task:
-    <transformer_list_name>/{swr,worstcase}-metrics.csv
-      depends on: <transformer_list_name>/{swr,worstcase}-run/
-        depends on: some tasks saved by PopulateCommonPipelines()
+    <transformer_list_name>/<benchmark_name>-metrics.csv
+      depends on: <transformer_list_name>/<benchmark_name>-run/
+        depends on: common/<benchmark_name>-cache.zip
+          depends on: common/<benchmark_name>-setup.json
+            depends on: common/patched-cache.zip
     """
-    additional_column_names = ['url', 'repeat_id']
+    additional_column_names = [
+        'url',
+        'repeat_id',
+        'benchmark_name',
+
+        # Number of resources of the page.
+        'cache_recording.subresource_count',
+
+        # Number of resources matching at least one domain regex, to give an
+        # idea in the CSV how much the threshold influence additional SWR uses.
+        'cache_recording.matching_subresource_count',
+
+        # Number of resources fetched from cache matching at least one domain
+        # regex, to give an actual idea if it is possible to have performance
+        # improvement on the web page (or not because only XHR), but also tells
+        # if the page loading time should see a performance improvement or not
+        # compared with a different thresholds.
+        'benchmark.matching_subresource_count_used_from_cache']
+    shared_task_prefix = os.path.join('common', benchmark_name)
+    task_prefix = os.path.join(transformer_list_name, benchmark_name)
+
+    @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
+                       dependencies=[self._original_cache_task])
+    def SetupBenchmark():
+      logging.info('loading %s', self._original_cache_trace_path)
+      trace = loading_trace.LoadingTrace.FromJsonFile(
+          self._original_cache_trace_path)
+      logging.info('generating %s', SetupBenchmark.path)
+      effective_subresource_urls = sandwich_utils.ListUrlRequests(
+          trace, sandwich_utils.RequestOutcome.All)
+      urls_to_enable_swr = _ExtractRegexMatchingUrls(
+          effective_subresource_urls, domain_regexes)
+      logging.info(
+          'count of urls to enable SWR: %s', len(urls_to_enable_swr))
+      with open(SetupBenchmark.path, 'w') as output:
+        json.dump({
+            'benchmark_name': benchmark_name,
+            'urls_to_enable_swr': [url for url in urls_to_enable_swr],
+            'effective_subresource_urls':
+                [url for url in effective_subresource_urls]
+          }, output)
 
-    task_prefix = os.path.join(transformer_list_name, '')
-    if enable_swr:
-      task_prefix += 'swr'
-    else:
-      task_prefix += 'worstcase'
+    @self.RegisterTask(shared_task_prefix + '-cache.zip', merge=True,
+                       dependencies=[SetupBenchmark])
+    def BuildBenchmarkCacheArchive():
+      benchmark_setup = json.load(open(SetupBenchmark.path))
+      _BuildBenchmarkCache(
+          original_wpr_trace_path=(
+              self._common_builder.original_wpr_recording_trace_path),
+          urls_to_enable_swr=set(benchmark_setup['urls_to_enable_swr']),
+          original_cache_trace_path=self._original_cache_trace_path,
+          original_cache_archive_path=self._original_cache_task.path,
+          cache_archive_dest_path=BuildBenchmarkCacheArchive.path)
 
-    @self.RegisterTask(task_prefix + '-run/', [self._patched_cache_task])
+    @self.RegisterTask(task_prefix + '-run/', [BuildBenchmarkCacheArchive])
     def RunBenchmark():
       runner = self._common_builder.CreateSandwichRunner()
       for transformer in transformer_list:
         transformer(runner)
-      runner.wpr_archive_path = self._common_builder.original_wpr_task.path
+      runner.wpr_archive_path = self._patched_wpr_path
       runner.wpr_out_log_path = os.path.join(
           RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
-      runner.cache_archive_path = self._patched_cache_task.path
+      runner.cache_archive_path = BuildBenchmarkCacheArchive.path
       runner.cache_operation = sandwich_runner.CacheOperation.PUSH
       runner.output_dir = RunBenchmark.path
-      if enable_swr:
-        runner.chrome_args.append('--enable-features=StaleWhileRevalidate2')
+      runner.chrome_args.append('--enable-features=StaleWhileRevalidate2')
       runner.Run()
 
     @self.RegisterTask(task_prefix + '-metrics.csv', [RunBenchmark])
     def ExtractMetrics():
-      run_metrics_list = []
-      for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
-          RunBenchmark.path):
-        trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
-        logging.info('processing trace: %s', trace_path)
-        trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
-        run_metrics = {
-            'url': trace.url,
-            'repeat_id': repeat_id,
-        }
-        run_metrics.update(
-            sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
-                repeat_dir, trace))
-        run_metrics_list.append(run_metrics)
+      benchmark_setup = json.load(open(SetupBenchmark.path))
+      run_metrics_list = _ProcessRunOutputDir(
+          benchmark_setup, RunBenchmark.path)
 
       run_metrics_list.sort(key=lambda e: e['repeat_id'])
       with open(ExtractMetrics.path, 'w') as csv_file:
         writer = csv.DictWriter(csv_file, fieldnames=(additional_column_names +
                                     sandwich_metrics.COMMON_CSV_COLUMN_NAMES))
         writer.writeheader()
-        for trace_metrics in run_metrics_list:
-          writer.writerow(trace_metrics)
+        for run_metrics in run_metrics_list:
+          writer.writerow(run_metrics)
 
     self._common_builder.default_final_tasks.append(ExtractMetrics)
diff --git a/loading/sandwich_utils.py b/loading/sandwich_utils.py
index bbc2d9d..b5ec497 100644
--- a/loading/sandwich_utils.py
+++ b/loading/sandwich_utils.py
@@ -2,6 +2,8 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import os
+
 import common_util
 import emulation
 import sandwich_runner
@@ -25,6 +27,83 @@ def NetworkSimulationTransformer(network_condition):
   return Transformer
 
 
+def FilterOutDataAndIncompleteRequests(requests):
+  for request in filter(lambda r: not r.IsDataRequest(), requests):
+    # The protocol is only known once the response has been received. But the
+    # trace recording might have been stopped with still some JavaScript
+    # originated requests that have not received any responses yet.
+    if request.protocol is None:
+      assert not request.HasReceivedResponse()
+      continue
+    if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
+      raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
+    yield request
+
+
+class RequestOutcome:
+  All, ServedFromCache, NotServedFromCache, Post = range(4)
+
+
+def ListUrlRequests(trace, request_kind):
+  """Lists requested URLs from a trace.
+
+  Args:
+    trace: (loading_trace.LoadingTrace) loading trace.
+    request_kind: RequestOutcome.* indicating the subset of requests to output.
+
+  Returns:
+    set([str])
+  """
+  urls = set()
+  for request_event in FilterOutDataAndIncompleteRequests(
+      trace.request_track.GetEvents()):
+    if (request_kind == RequestOutcome.ServedFromCache and
+        request_event.from_disk_cache):
+      urls.add(request_event.url)
+    elif (request_kind == RequestOutcome.Post and
+        request_event.method.upper().strip() == 'POST'):
+      urls.add(request_event.url)
+    elif (request_kind == RequestOutcome.NotServedFromCache and
+        not request_event.from_disk_cache):
+      urls.add(request_event.url)
+    elif request_kind == RequestOutcome.All:
+      urls.add(request_event.url)
+  return urls
+
+
+def PatchWprEntryToBeCached(wpr_url_entry):
+  """Patches a WprUrlEntry to ensure the resources to go into the HTTP cache and
+  avoid invalidation and revalidations.
+
+  Args:
+    wpr_url_entry: Wpr url entry of the resource to put into the cache.
+  """
+  MAX_AGE = 10 * 365 * 24 * 60 * 60
+  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
+
+  # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
+  # TODO(gabadie): may need to delete ETag.
+  # TODO(gabadie): may need to take care of x-cache.
+  #
+  # Override the cache-control header to set the resources max age to MAX_AGE.
+  #
+  # Important note: Some resources holding sensitive information might have
+  # cache-control set to no-store which allow the resource to be cached but
+  # not cached in the file system. NoState-Prefetch is going to take care of
+  # this case. But in here, to simulate NoState-Prefetch, we don't have other
+  # choices but save absolutely all cached resources on disk so they survive
+  # after killing chrome for cache save, modification and push.
+  wpr_url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
+
+  # TODO(gabadie): May need to extend Vary blacklist (referer?)
+  #
+  # All of these Vary and Pragma possibilities need to be removed from
+  # response headers in order for Chrome to store a resource in HTTP cache and
+  # not to invalidate it.
+  wpr_url_entry.RemoveResponseHeaderDirectives('vary', {'*', 'cookie'})
+  wpr_url_entry.RemoveResponseHeaderDirectives('pragma', {'no-cache'})
+
+
 class SandwichCommonBuilder(task_manager.Builder):
   """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
   """
@@ -46,6 +125,7 @@ class SandwichCommonBuilder(task_manager.Builder):
     self.default_final_tasks = []
 
     self.original_wpr_task = None
+    self.original_wpr_recording_trace_path = None
 
   def CreateSandwichRunner(self):
     """Create a runner for non benchmark purposes."""
@@ -62,7 +142,10 @@ class SandwichCommonBuilder(task_manager.Builder):
       runner = self.CreateSandwichRunner()
       runner.wpr_archive_path = BuildOriginalWpr.path
       runner.wpr_record = True
-      runner.output_dir = BuildOriginalWpr.path[:-4] + '-run'
+      runner.output_dir = BuildOriginalWpr.run_path
       runner.Run()
+    BuildOriginalWpr.run_path = BuildOriginalWpr.path[:-4] + '-run'
 
     self.original_wpr_task = BuildOriginalWpr
+    self.original_wpr_recording_trace_path = os.path.join(
+        BuildOriginalWpr.run_path, '0', sandwich_runner.TRACE_FILENAME)

commit a611f683832e8dc939696038f9cb46bdae21d45a
Author: gabadie <gabadie@chromium.org>
Date:   Thu Jul 7 12:15:57 2016 -0700

    tools/android/loading: Move task_manager's logs into a subdirectory
    
    On Sandwich, when iterating on the command line, the number of
    distinct log file is growing in the output directory, making less
    convenient to browse this output directory.
    
    This CL fixes this issue by outputing all task logs into a
    sub-directory titled `logs` within the output directory.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2128203002
    Cr-Original-Commit-Position: refs/heads/master@{#404205}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 791010c3106a7f070a664545c473b8c71325cd35

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 7cb0834..4041943 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -218,6 +218,7 @@ def _RunBenchmarkMain(args):
   setup_path = os.path.join(args.output, _SANDWICH_SETUP_FILENAME)
   with open(setup_path) as file_input:
     setup = yaml.load(file_input)
+  android_device = None
   if setup['sandwich_runner']['android_device_serial']:
     android_device = device_setup.GetDeviceFromSerial(
         setup['sandwich_runner']['android_device_serial'])
diff --git a/loading/task_manager.py b/loading/task_manager.py
index 0ae1f7a..f132eb1 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -64,6 +64,7 @@ import sys
 import common_util
 
 
+_TASK_LOGS_DIR_NAME = 'logs'
 _TASK_GRAPH_DOTFILE_NAME = 'tasks_graph.dot'
 _TASK_GRAPH_PNG_NAME = 'tasks_graph.png'
 _TASK_RESUME_ARGUMENTS_FILE = 'resume.txt'
@@ -467,9 +468,11 @@ def ExecuteWithCommandLine(args, default_final_tasks):
 
   log_filename = datetime.datetime.now().strftime(
       _TASK_EXECUTION_LOG_NAME_FORMAT)
+  log_path = os.path.join(args.output, _TASK_LOGS_DIR_NAME, log_filename)
+  if not os.path.isdir(os.path.dirname(log_path)):
+    os.makedirs(os.path.dirname(log_path))
   formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
-  handler = logging.FileHandler(
-      os.path.join(args.output, log_filename), mode='a')
+  handler = logging.FileHandler(log_path, mode='a')
   handler.setFormatter(formatter)
   logging.getLogger().addHandler(handler)
   logging.info(

commit 8e6c7fb637894296336839c8259a5a31f4e7b04f
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jul 6 09:36:36 2016 -0700

    sandwich: Fixes two sources of KeyError task failures
    
    BUG=623966
    
    Review-Url: https://codereview.chromium.org/2112483002
    Cr-Original-Commit-Position: refs/heads/master@{#403916}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 66cf3abbaa67cfa8e9f6707b4da1d95d79bd7879

diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index be8f609..0d261b8 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -21,7 +21,7 @@ import json
 import os
 import re
 import shutil
-from urlparse import urlparse
+import urlparse
 
 import chrome_cache
 import common_util
@@ -30,6 +30,7 @@ from prefetch_view import PrefetchSimulationView
 from request_dependencies_lens import RequestDependencyLens
 import sandwich_metrics
 import sandwich_runner
+import sandwich_utils
 import task_manager
 import wpr_backend
 
@@ -69,6 +70,13 @@ SUBRESOURCE_DISCOVERERS = set([
 _UPLOAD_DATA_STREAM_REQUESTS_REGEX = re.compile(r'^\d+/(?P<url>.*)$')
 
 
+def _NormalizeUrl(url):
+  """Returns normalized URL such as removing trailing slashes."""
+  parsed_url = list(urlparse.urlparse(url))
+  parsed_url[2] = re.sub(r'/{2,}', r'/', parsed_url[2])
+  return urlparse.urlunparse(parsed_url)
+
+
 def _PatchWpr(wpr_archive):
   """Patches a WPR archive to get all resources into the HTTP cache and avoid
   invalidation and revalidations.
@@ -201,7 +209,15 @@ def _PruneOutOriginalNoStoreRequests(original_headers_path, requests):
     original_headers = json.load(file_input)
   pruned_requests = set()
   for request in requests:
-    request_original_headers = original_headers[request.url]
+    url = _NormalizeUrl(request.url)
+    if url not in original_headers:
+      # TODO(gabadie): Investigate why these requests were not in WPR.
+      assert request.failed
+      logging.warning(
+          'could not find original headers for: %s (failure: %s)',
+          url, request.error_text)
+      continue
+    request_original_headers = original_headers[url]
     if ('cache-control' in request_original_headers and
         'no-store' in request_original_headers['cache-control'].lower()):
       pruned_requests.add(request)
@@ -369,7 +385,7 @@ class _RunOutputVerifier(object):
     for request in all_wpr_requests:
       if request.is_wpr_host:
         continue
-      if urlparse(request.url).path.startswith('/web-page-replay'):
+      if urlparse.urlparse(request.url).path.startswith('/web-page-replay'):
         wpr_command_colliding_urls.add(request.url)
       elif request.is_served is False:
         unserved_wpr_urls.add(request.url)
@@ -481,6 +497,7 @@ def _ProcessRunOutputDir(
     served_from_network_bytes = 0
     served_from_cache_bytes = 0
     urls_hitting_network = set()
+    response_sizes = {}
     for request in _FilterOutDataAndIncompleteRequests(
         trace.request_track.GetEvents()):
       # Ignore requests served from the blink's cache.
@@ -488,9 +505,20 @@ def _ProcessRunOutputDir(
         continue
       urls_hitting_network.add(request.url)
       if request.from_disk_cache:
-        served_from_cache_bytes += cached_encoded_data_lengths[request.url]
+        if request.url in cached_encoded_data_lengths:
+          response_size = cached_encoded_data_lengths[request.url]
+        else:
+          # Some fat webpages may overflow the Memory cache, and so some
+          # requests might be served from disk cache couple of times per page
+          # load.
+          logging.warning('Looks like could be served from memory cache: %s',
+              request.url)
+          response_size = response_sizes[request.url]
+        served_from_cache_bytes += response_size
       else:
-        served_from_network_bytes += request.GetEncodedDataLength()
+        response_size = request.GetEncodedDataLength()
+        served_from_network_bytes += response_size
+      response_sizes[request.url] = response_size
 
     # Make sure the served from blink's cache requests have at least one
     # corresponding request that was not served from the blink's cache.
@@ -574,6 +602,15 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
       # Save up original response headers.
       original_response_headers = {e.url: e.GetResponseHeadersDict() \
           for e in wpr_archive.ListUrlEntries()}
+      logging.info('save up response headers for %d resources',
+                   len(original_response_headers))
+      if not original_response_headers:
+        # TODO(gabadie): How is it possible to not even have the main resource
+        # in the WPR archive? Example URL can be found in:
+        # http://crbug.com/623966#c5
+        raise Exception(
+            'Looks like no resources were recorded in WPR during: {}'.format(
+                self._common_builder.original_wpr_task.name))
       with open(self._original_headers_path, 'w') as file_output:
         json.dump(original_response_headers, file_output)
 

commit 3e86377df2c0cc96858bac573e84dee2e4e81657
Author: ioanap <ioanap@google.com>
Date:   Wed Jul 6 04:09:39 2016 -0700

    Moved BrowsingDataCounter and part of BrowsingDataCounterUtils to components.
    
    Changes:
       - BrowsingDataCounter now resides in components/browsing_data/counters/
       - Moved enums and TimePeriod utils to components/browsing_data/browsing_data_utils[.h/.cc]
       - Changes to the counters that still reside in chrome/browser/browsing_data/ are temporary, meant to allow this partial change.
         They will be moved to components in a subsequent CL
    
    TBR=dbeam@chromium.org
    BUG=620317
    
    Review-Url: https://codereview.chromium.org/2084903002
    Cr-Original-Commit-Position: refs/heads/master@{#403882}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c776d1065425025fdcded9885505ea4637158929

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 4f9a467..035a6a5 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -118,8 +118,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_memory_pressure_level"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/bitmap_format_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/bookmark_type_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/browsing_data_time_period_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/browsing_data_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/browsing_data_utils_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/certificate_mime_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/cert_verify_status_android_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/chromium_url_request_java"/>

commit 6d2b6941fb92e76494236fa5fc1b3d50131ba220
Author: gabadie <gabadie@chromium.org>
Date:   Fri Jul 1 06:08:44 2016 -0700

    tools/android/loading: Use ujson to parse trace JSON files if available
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2109393004
    Cr-Original-Commit-Position: refs/heads/master@{#403441}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7545f1ba23ae5d5e3bdb5bc62272ebe45ff21d94

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index beb6785..8a4b334 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -5,7 +5,10 @@
 """Represents the trace of a page load."""
 
 import datetime
-import json
+try:
+  import ujson as json
+except ImportError:
+  import json
 import time
 
 import devtools_monitor

commit 7e7182e6cb0f0f647aaff04669e834b49a91efa6
Author: gabadie <gabadie@chromium.org>
Date:   Fri Jul 1 03:02:52 2016 -0700

    sandwich: Fixes some failures in request_track.py
    
    This CL fixes the following failures in request_track.py:
     - Data protocol are known to have encodedDataLength == 0
     - Some request_id might be duplicated and this CL just ignore the
       duplicated errors.
    
    BUG=623966
    
    Review-Url: https://codereview.chromium.org/2115553002
    Cr-Original-Commit-Position: refs/heads/master@{#403432}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 08ade9ccbfd03bcef642262243542f83bb6776e8

diff --git a/loading/request_track.py b/loading/request_track.py
index f14bc65..89b4f35 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -695,10 +695,12 @@ class RequestTrack(devtools_monitor.Track):
     # Several "requestWillBeSent" events can be dispatched in a row in the case
     # of redirects.
     redirect_initiator = None
+    if request_id in self._completed_requests_by_id:
+      assert request_id not in self._requests_in_flight
+      return
     if request_id in self._requests_in_flight:
       redirect_initiator = self._HandleRedirect(request_id, params)
-    assert (request_id not in self._requests_in_flight
-            and request_id not in self._completed_requests_by_id)
+    assert (request_id not in self._requests_in_flight)
     r = Request()
     r.request_id = request_id
     _CopyFromDictToObject(
@@ -759,6 +761,9 @@ class RequestTrack(devtools_monitor.Track):
     request.served_from_cache = True
 
   def _ResponseReceived(self, request_id, params):
+    if request_id in self._completed_requests_by_id:
+      assert request_id not in self._requests_in_flight
+      return
     assert request_id in self._requests_in_flight
     (r, status) = self._requests_in_flight[request_id]
     if status == RequestTrack._STATUS_RESPONSE:
@@ -771,7 +776,8 @@ class RequestTrack(devtools_monitor.Track):
       self.duplicates_count += 1
       return
     assert status == RequestTrack._STATUS_SENT
-    assert r.frame_id == params['frameId']
+    assert (r.frame_id == params['frameId'] or
+            params['response']['protocol'] == 'data')
     assert r.timestamp <= params['timestamp']
     if r.resource_type == 'Other':
       r.resource_type = params.get('type', 'Other')
@@ -816,7 +822,7 @@ class RequestTrack(devtools_monitor.Track):
     assert (status == RequestTrack._STATUS_RESPONSE
             or status == RequestTrack._STATUS_DATA)
     r.encoded_data_length = params['encodedDataLength']
-    assert (r.encoded_data_length > 0 or r.protocol == 'about' or
+    assert (r.encoded_data_length > 0 or r.protocol in {'about', 'data'} or
             r.from_disk_cache or r.served_from_cache)
     r.timing.loading_finished = r._TimestampOffsetFromStartMs(
         params['timestamp'])
@@ -834,8 +840,6 @@ class RequestTrack(devtools_monitor.Track):
     self._FinalizeRequest(request_id)
 
   def _FinalizeRequest(self, request_id):
-    if request_id not in self._requests_in_flight:
-      return
     (request, status) = self._requests_in_flight[request_id]
     assert status == RequestTrack._STATUS_FINISHED
     del self._requests_in_flight[request_id]
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 64d3e77..3f0baea 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -492,6 +492,17 @@ class RequestTrackTestCase(unittest.TestCase):
     with self.assertRaises(AssertionError):
       self.request_track.Handle('Network.requestWillBeSent', msg)
 
+  def testIgnoreCompletedDuplicates(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    self.request_track.Handle('Network.responseReceived',
+                              RequestTrackTestCase._RESPONSE_RECEIVED)
+    self.request_track.Handle('Network.loadingFinished',
+                              RequestTrackTestCase._LOADING_FINISHED)
+    # Should not raise an AssertionError.
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+
   def testSequenceOfGeneratedResponse(self):
     self.request_track.Handle('Network.requestServedFromCache',
                               RequestTrackTestCase._SERVED_FROM_CACHE)

commit 337cfe21e0ec0eae978ee07da2d45333411f1cdf
Author: yfriedman <yfriedman@chromium.org>
Date:   Thu Jun 30 12:09:23 2016 -0700

    Add some webapk source directories to eclise .classpath
    
    Review-Url: https://codereview.chromium.org/2115493005
    Cr-Original-Commit-Position: refs/heads/master@{#403241}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 06abe9e0f846d32f5f5bcc6a670c494f9ca81eb6

diff --git a/eclipse/.classpath b/eclipse/.classpath
index f9bb406..4f9a467 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -30,6 +30,14 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="chrome/android/javatests/src"/>
     <classpathentry kind="src" path="chrome/android/sync_shell/javatests/src"/>
     <classpathentry kind="src" path="chrome/android/junit/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/libs/client/junit/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/libs/client/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/libs/common/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/libs/runtime_library/junit/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/libs/runtime_library/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/shell_apk/javatests/dex_optimizer/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/shell_apk/javatests/src"/>
+    <classpathentry kind="src" path="chrome/android/webapk/shell_apk/src"/>
     <classpathentry kind="src" path="chrome/test/android/javatests/src"/>
     <classpathentry kind="src" path="chrome/test/chromedriver/test/webview_shell/java/src"/>
     <classpathentry kind="src" path="components/bookmarks/common/android/java/src"/>

commit 0e5e83e3349f7ccc91e52823e9fa4ac22e31e42d
Author: primiano <primiano@chromium.org>
Date:   Thu Jun 30 08:13:47 2016 -0700

    Remove tools/android/heap_profiler
    
    It is a standalone .so library meant to be LD_PRELOAD-ed
    using awkward tricks into the Android zygote to get heap dumps.
    Never worked too well in Chrome as it intercepted only malloc
    calls, without being able to hook allocations in PartitionAlloc
    or Blink GC.
    This is subsumed by the work of crbug.com/602701 which
    integrates native heap profiling with chrome://tracing and
    supports all the major chrome allocators.
    For future heap profiling adventures use
    //components/tracing/docs/heap_profiler.md
    
    This CL also removes //third_party/bsdtrees that was introduced
    as a dependency to libheap_profiler. Nothing else seems to
    depend on it.
    
    BUG=382489
    
    Review-Url: https://codereview.chromium.org/2105873005
    Cr-Original-Commit-Position: refs/heads/master@{#403179}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cf04606c3af6634f3bcbd8d39007e6061372a3c0

diff --git a/BUILD.gn b/BUILD.gn
index 99f9a01..55d9416 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -21,14 +21,6 @@ group("android_tools") {
   ]
 }
 
-# GYP: //tools/android/android_tools.gyp:heap_profiler
-group("heap_profiler") {
-  deps = [
-    "//tools/android/heap_profiler:heap_dump",
-    "//tools/android/heap_profiler:heap_profiler",
-  ]
-}
-
 # GYP: //tools/android/android_tools.gyp:memdump
 group("memdump") {
   deps = [
diff --git a/android_tools.gyp b/android_tools.gyp
index 3e1589f..83ff642 100644
--- a/android_tools.gyp
+++ b/android_tools.gyp
@@ -21,15 +21,6 @@
       ],
     },
     {
-      # GN: //tools/android:heap_profiler
-      'target_name': 'heap_profiler',
-      'type': 'none',
-      'dependencies': [
-        'heap_profiler/heap_profiler.gyp:heap_dump',
-        'heap_profiler/heap_profiler.gyp:heap_profiler',
-      ],
-    },
-    {
       # GN: //tools/android:memdump
       'target_name': 'memdump',
       'type': 'none',
diff --git a/heap_profiler/BUILD.gn b/heap_profiler/BUILD.gn
deleted file mode 100644
index 41366e5..0000000
--- a/heap_profiler/BUILD.gn
+++ /dev/null
@@ -1,63 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import("//testing/test.gni")
-
-shared_library("heap_profiler") {
-  sources = [
-    "heap_profiler_hooks_android.c",
-  ]
-  public_deps = [
-    ":heap_profiler_core",
-  ]
-}
-
-source_set("heap_profiler_core") {
-  sources = [
-    "heap_profiler.c",
-    "heap_profiler.h",
-  ]
-
-  configs -= [ "//build/config/compiler:chromium_code" ]
-  configs += [ "//build/config/compiler:no_chromium_code" ]
-}
-
-executable("heap_dump") {
-  sources = [
-    "heap_dump.c",
-  ]
-
-  deps = [
-    ":heap_profiler_core",
-  ]
-
-  configs -= [ "//build/config/compiler:chromium_code" ]
-  configs += [ "//build/config/compiler:no_chromium_code" ]
-}
-
-test("heap_profiler_unittests") {
-  sources = [
-    "heap_profiler_unittest.cc",
-  ]
-  deps = [
-    ":heap_profiler_core",
-    "//testing/gtest",
-    "//testing/gtest:gtest_main",
-  ]
-}
-
-executable("heap_profiler_integrationtest") {
-  testonly = true
-
-  sources = [
-    "heap_profiler_integrationtest.cc",
-  ]
-  deps = [
-    ":heap_profiler",
-    "//testing/gtest",
-  ]
-
-  configs -= [ "//build/config/compiler:chromium_code" ]
-  configs += [ "//build/config/compiler:no_chromium_code" ]
-}
diff --git a/heap_profiler/DEPS b/heap_profiler/DEPS
deleted file mode 100644
index 458e541..0000000
--- a/heap_profiler/DEPS
+++ /dev/null
@@ -1,3 +0,0 @@
-include_rules = [
-  "+third_party/bsdtrees",
-]
diff --git a/heap_profiler/heap_dump.c b/heap_profiler/heap_dump.c
deleted file mode 100644
index e7bfbee..0000000
--- a/heap_profiler/heap_dump.c
+++ /dev/null
@@ -1,350 +0,0 @@
-// Copyright 2014 The Chromium Authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-// The client dump tool for libheap_profiler. It attaches to a process (given
-// its pid) and dumps all the libheap_profiler tracking information in JSON.
-// The JSON output looks like this:
-// {
-//   "total_allocated": 908748493,   # Total bytes allocated and not freed.
-//   "num_allocs":      37542,       # Number of allocations.
-//   "num_stacks":      3723,        # Number of allocation call-sites.
-//   "allocs":                       # Optional. Printed only with the -x arg.
-//   {
-//     "beef1234": {"l": 17, "f": 1, "s": "1a"},
-//      ^            ^        ^       ^ Index of the corresponding entry in the
-//      |            |        |         next "stacks" section. Essentially a ref
-//      |            |        |         to the call site that created the alloc.
-//      |            |        |
-//      |            |        +-------> Flags (last arg of heap_profiler_alloc).
-//      |            +----------------> Length of the Alloc.
-//      +-----------------------------> Start address of the Alloc (hex).
-//   },
-//   "stacks":
-//   {
-//      "1a": {"l": 17, "f": [1074792772, 1100849864, 1100850688, ...]},
-//       ^      ^        ^
-//       |      |        +-----> Stack frames (absolute virtual addresses).
-//       |      +--------------> Bytes allocated and not freed by the call site.
-//       +---------------------> Index of the entry (as for "allocs" xref).
-//                               Indexes are hex and might not be monotonic.
-
-#include <errno.h>
-#include <fcntl.h>
-#include <inttypes.h>
-#include <stdbool.h>
-#include <stddef.h>
-#include <stdint.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <sys/ptrace.h>
-#include <sys/stat.h>
-#include <sys/wait.h>
-#include <time.h>
-#include <unistd.h>
-
-#include "tools/android/heap_profiler/heap_profiler.h"
-
-static void lseek_abs(int fd, size_t off);
-static void read_proc_cmdline(char* cmdline, int size);
-static ssize_t read_safe(int fd, void* buf, size_t count);
-
-static int pid;
-
-
-static int dump_process_heap(
-    int mem_fd,
-    FILE* fmaps,
-    bool dump_also_allocs,
-    bool pedantic,  // Enable pedantic consistency checks on memory counters.
-    char* comment) {
-  HeapStats stats;
-  time_t tm;
-  char cmdline[512];
-
-  tm = time(NULL);
-  read_proc_cmdline(cmdline, sizeof(cmdline));
-
-  // Look for the mmap which contains the HeapStats in the target process vmem.
-  // On Linux/Android, the libheap_profiler mmaps explicitly /dev/zero. The
-  // region furthermore starts with a magic marker to disambiguate.
-  bool stats_mmap_found = false;
-  for (;;) {
-    char line[1024];
-    if (fgets(line, sizeof(line), fmaps) == NULL)
-      break;
-
-    uintptr_t start;
-    uintptr_t end;
-    char map_file[32];
-    int ret = sscanf(line, "%"SCNxPTR"-%"SCNxPTR" rw-p %*s %*s %*s %31s",
-                     &start, &end, map_file);
-    const size_t size = end - start + 1;
-    if (ret != 3 || strcmp(map_file, "/dev/zero") != 0 || size < sizeof(stats))
-      continue;
-
-    // The mmap looks promising. Let's check for the magic marker.
-    lseek_abs(mem_fd, start);
-    ssize_t rsize = read_safe(mem_fd, &stats, sizeof(stats));
-
-    if (rsize == -1) {
-      perror("read");
-      return -1;
-    }
-
-    if (rsize < sizeof(stats))
-      continue;
-
-    if (stats.magic_start == HEAP_PROFILER_MAGIC_MARKER) {
-      stats_mmap_found = true;
-      break;
-    }
-  }
-
-  if (!stats_mmap_found) {
-    fprintf(stderr, "Could not find the HeapStats area. "
-                    "It looks like libheap_profiler is not loaded.\n");
-    return -1;
-  }
-
-  // Print JSON-formatted output.
-  printf("{\n");
-  printf("  \"pid\":             %d,\n", pid);
-  printf("  \"time\":            %ld,\n", tm);
-  printf("  \"comment\":         \"%s\",\n", comment);
-  printf("  \"cmdline\":         \"%s\",\n", cmdline);
-  printf("  \"pagesize\":        %d,\n", getpagesize());
-  printf("  \"total_allocated\": %zu,\n", stats.total_alloc_bytes);
-  printf("  \"num_allocs\":      %"PRIu32",\n", stats.num_allocs);
-  printf("  \"num_stacks\":      %"PRIu32",\n", stats.num_stack_traces);
-
-  uint32_t dbg_counted_allocs = 0;
-  size_t dbg_counted_total_alloc_bytes = 0;
-  bool prepend_trailing_comma = false;  // JSON syntax, I hate you.
-  uint32_t i;
-
-  // Dump the optional allocation table.
-  if (dump_also_allocs) {
-    printf("  \"allocs\": {");
-    lseek_abs(mem_fd, (uintptr_t) stats.allocs);
-    for (i = 0; i < stats.max_allocs; ++i) {
-      Alloc alloc;
-      if (read_safe(mem_fd, &alloc, sizeof(alloc)) != sizeof(alloc)) {
-        fprintf(stderr, "ERROR: cannot read allocation table\n");
-        perror("read");
-        return -1;
-      }
-
-      // Skip empty (i.e. freed) entries.
-      if (alloc.start == 0 && alloc.end == 0)
-        continue;
-
-      if (alloc.end < alloc.start) {
-        fprintf(stderr, "ERROR: found inconsistent alloc.\n");
-        return -1;
-      }
-
-      size_t alloc_size = alloc.end - alloc.start + 1;
-      size_t stack_idx = (
-          (uintptr_t) alloc.st - (uintptr_t) stats.stack_traces) /
-          sizeof(StacktraceEntry);
-      dbg_counted_total_alloc_bytes += alloc_size;
-      ++dbg_counted_allocs;
-
-      if (prepend_trailing_comma)
-        printf(",");
-      prepend_trailing_comma = true;
-      printf("\"%"PRIxPTR"\": {\"l\": %zu, \"f\": %"PRIu32", \"s\": \"%zx\"}",
-             alloc.start, alloc_size, alloc.flags, stack_idx);
-    }
-    printf("},\n");
-
-    if (pedantic && dbg_counted_allocs != stats.num_allocs) {
-      fprintf(stderr,
-              "ERROR: inconsistent alloc count (%"PRIu32" vs %"PRIu32").\n",
-              dbg_counted_allocs, stats.num_allocs);
-      return -1;
-    }
-
-    if (pedantic && dbg_counted_total_alloc_bytes != stats.total_alloc_bytes) {
-      fprintf(stderr, "ERROR: inconsistent alloc totals (%zu vs %zu).\n",
-              dbg_counted_total_alloc_bytes, stats.total_alloc_bytes);
-      return -1;
-    }
-  }
-
-  // Dump the distinct stack traces.
-  printf("  \"stacks\": {");
-  prepend_trailing_comma = false;
-  dbg_counted_total_alloc_bytes = 0;
-  lseek_abs(mem_fd, (uintptr_t) stats.stack_traces);
-  for (i = 0; i < stats.max_stack_traces; ++i) {
-    StacktraceEntry st;
-    if (read_safe(mem_fd, &st, sizeof(st)) != sizeof(st)) {
-      fprintf(stderr, "ERROR: cannot read stack trace table\n");
-      perror("read");
-      return -1;
-    }
-
-    // Skip empty (i.e. freed) entries.
-    if (st.alloc_bytes == 0)
-      continue;
-
-    dbg_counted_total_alloc_bytes += st.alloc_bytes;
-
-    if (prepend_trailing_comma)
-      printf(",");
-    prepend_trailing_comma = true;
-
-    printf("\"%"PRIx32"\":{\"l\": %zu, \"f\": [", i, st.alloc_bytes);
-    size_t n = 0;
-    for (;;) {
-      printf("%" PRIuPTR, st.frames[n]);
-      ++n;
-      if (n == HEAP_PROFILER_MAX_DEPTH || st.frames[n] == 0)
-        break;
-      else
-        printf(",");
-    }
-    printf("]}");
-  }
-  printf("}\n}\n");
-
-  if (pedantic && dbg_counted_total_alloc_bytes != stats.total_alloc_bytes) {
-    fprintf(stderr, "ERROR: inconsistent stacks totals (%zu vs %zu).\n",
-            dbg_counted_total_alloc_bytes, stats.total_alloc_bytes);
-    return -1;
-  }
-
-  fflush(stdout);
-  return 0;
-}
-
-// Unfortunately lseek takes a *signed* offset, which is unsuitable for large
-// files like /proc/X/mem on 64-bit.
-static void lseek_abs(int fd, size_t off) {
-#define OFF_T_MAX ((off_t) ~(((uint64_t) 1) << (8 * sizeof(off_t) - 1)))
-  if (off <= OFF_T_MAX) {
-    lseek(fd, (off_t) off, SEEK_SET);
-    return;
-  }
-  lseek(fd, (off_t) OFF_T_MAX, SEEK_SET);
-  lseek(fd, (off_t) (off - OFF_T_MAX), SEEK_CUR);
-}
-
-static ssize_t read_safe(int fd, void* buf, size_t count) {
-  ssize_t res;
-  size_t bytes_read = 0;
-  do {
-    do {
-      res = read(fd, buf + bytes_read, count - bytes_read);
-    } while (res == -1  && errno == EINTR);
-    if (res <= 0)
-      break;
-    bytes_read += res;
-  } while (bytes_read < count);
-  return bytes_read ? bytes_read : res;
-}
-
-static int open_proc_mem_fd() {
-  char path[64];
-  snprintf(path, sizeof(path), "/proc/%d/mem", pid);
-  int mem_fd = open(path, O_RDONLY);
-  if (mem_fd < 0) {
-    fprintf(stderr, "Could not attach to target process virtual memory.\n");
-    perror("open");
-  }
-  return mem_fd;
-}
-
-static FILE* open_proc_maps() {
-  char path[64];
-  snprintf(path, sizeof(path), "/proc/%d/maps", pid);
-  FILE* fmaps = fopen(path, "r");
-  if (fmaps == NULL) {
-    fprintf(stderr, "Could not open %s.\n", path);
-    perror("fopen");
-  }
-  return fmaps;
-}
-
-static void read_proc_cmdline(char* cmdline, int size) {
-  char path[64];
-  snprintf(path, sizeof(path), "/proc/%d/cmdline", pid);
-  int cmdline_fd = open(path, O_RDONLY);
-  if (cmdline_fd < 0) {
-    fprintf(stderr, "Could not open %s.\n", path);
-    perror("open");
-    cmdline[0] = '\0';
-    return;
-  }
-  int length = read_safe(cmdline_fd, cmdline, size);
-  if (length < 0) {
-    fprintf(stderr, "Could not read %s.\n", path);
-    perror("read");
-    length = 0;
-  }
-  close(cmdline_fd);
-  cmdline[length] = '\0';
-}
-
-int main(int argc, char** argv) {
-  char c;
-  int ret = 0;
-  bool dump_also_allocs = false;
-  bool pedantic = true;
-  char comment[1024] = { '\0' };
-
-  while (((c = getopt(argc, argv, "xnc:")) & 0x80) == 0) {
-   switch (c) {
-      case 'x':
-        dump_also_allocs = true;
-        break;
-      case 'n':
-        pedantic = false;
-        break;
-      case 'c':
-        strlcpy(comment, optarg, sizeof(comment));
-        break;
-     }
-  }
-
-  if (optind >= argc) {
-    printf("Usage: %s [-n] [-x] [-c comment] pid\n"
-           "  -n: Skip pedantic checks on dump consistency.\n"
-           "  -x: Extended dump, includes individual allocations.\n"
-           "  -c: Appends the given comment to the JSON dump.\n",
-           argv[0]);
-    return -1;
-  }
-
-  pid = atoi(argv[optind]);
-
-  if (ptrace(PTRACE_ATTACH, pid, NULL, NULL) == -1) {
-    perror("ptrace");
-    return -1;
-  }
-
-  // Wait for the process to actually freeze.
-  waitpid(pid, NULL, 0);
-
-  int mem_fd = open_proc_mem_fd();
-  if (mem_fd < 0)
-    ret = -1;
-
-  FILE* fmaps = open_proc_maps();
-  if (fmaps == NULL)
-    ret = -1;
-
-  if (ret == 0)
-    ret = dump_process_heap(mem_fd, fmaps, dump_also_allocs, pedantic, comment);
-
-  ptrace(PTRACE_DETACH, pid, NULL, NULL);
-
-  // Cleanup.
-  fflush(stdout);
-  close(mem_fd);
-  fclose(fmaps);
-  return ret;
-}
diff --git a/heap_profiler/heap_profiler.c b/heap_profiler/heap_profiler.c
deleted file mode 100644
index aef63ba..0000000
--- a/heap_profiler/heap_profiler.c
+++ /dev/null
@@ -1,397 +0,0 @@
-// Copyright 2014 The Chromium Authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-// This is a OS-independent* module which purpose is tracking allocations and
-// their call sites (stack traces). It is able to deal with hole punching
-// (read: munmap). Also, it has low overhead and its presence in the system its
-// barely noticeable, even if tracing *all* the processes.
-// This module does NOT know how to deal with stack unwinding. The caller must
-// do that and pass the addresses of the unwound stack.
-// * (Modulo three lines for mutexes.)
-//
-// Exposed API:
-//   void heap_profiler_init(HeapStats*);
-//   void heap_profiler_alloc(addr, size, stack_frames, depth, flags);
-//   void heap_profiler_free(addr, size);  (size == 0 means free entire region).
-//
-// The profiling information is tracked into two data structures:
-// 1) A RB-Tree of non-overlapping VM regions (allocs) sorted by their start
-//    addr. Each entry tracks the start-end addresses and points to the stack
-//    trace which created that allocation (see below).
-// 2) A (hash) table of stack traces. In general the #allocations >> #call sites
-//    which create those allocations. In order to avoid duplicating the latter,
-//    they are stored distinctly in this hash table and used by reference.
-//
-//   /  Process virtual address space  \
-//   +------+      +------+      +------+
-//   |Alloc1|      |Alloc2|      |Alloc3|    <- Allocs (a RB-Tree underneath)
-//   +------+      +------+      +------+
-//    Len: 12       Len: 4        Len: 4
-//       |            |             |                     stack_traces
-//       |            |             |              +-----------+--------------+
-//       |            |             |              | Alloc tot | stack frames +
-//       |            |             |              +-----------+--------------+
-//       +------------|-------------+------------> |    16     | 0x1234 ....  |
-//                    |                            +-----------+--------------+
-//                    +--------------------------> |     4     | 0x5678 ....  |
-//                                                 +-----------+--------------+
-//                                                   (A hash-table underneath)
-//
-// Final note: the memory for both 1) and 2) entries is carved out from two
-// static pools (i.e. stack_traces and allocs). The pools are treated as
-// a sbrk essentially, and are kept compact by reusing freed elements (hence
-// having a freelist for each of them).
-//
-// All the internal (static) functions here assume that the |lock| is held.
-
-#include <assert.h>
-#include <string.h>
-
-// Platform-dependent mutex boilerplate.
-#if defined(__linux__) || defined(__ANDROID__)
-#include <pthread.h>
-#define DEFINE_MUTEX(x) pthread_mutex_t x = PTHREAD_MUTEX_INITIALIZER
-#define LOCK_MUTEX(x) pthread_mutex_lock(&x)
-#define UNLOCK_MUTEX(x) pthread_mutex_unlock(&x)
-#else
-#error OS not supported.
-#endif
-
-#include "tools/android/heap_profiler/heap_profiler.h"
-
-
-static DEFINE_MUTEX(lock);
-
-// |stats| contains the global tracking metadata and is the entry point which
-// is read by the heap_dump tool.
-static HeapStats* stats;
-
-// +---------------------------------------------------------------------------+
-// + Stack traces hash-table                                                   +
-// +---------------------------------------------------------------------------+
-#define ST_ENTRIES_MAX (64 * 1024)
-#define ST_HASHTABLE_BUCKETS (64 * 1024) /* Must be a power of 2. */
-
-static StacktraceEntry stack_traces[ST_ENTRIES_MAX];
-static StacktraceEntry* stack_traces_freelist;
-static StacktraceEntry* stack_traces_ht[ST_HASHTABLE_BUCKETS];
-
-// Looks up a stack trace from the stack frames. Creates a new one if necessary.
-static StacktraceEntry* record_stacktrace(uintptr_t* frames, uint32_t depth) {
-  if (depth == 0)
-    return NULL;
-
-  if (depth > HEAP_PROFILER_MAX_DEPTH)
-    depth = HEAP_PROFILER_MAX_DEPTH;
-
-  uint32_t i;
-  uintptr_t hash = 0;
-  for (i = 0; i < depth; ++i)
-    hash = (hash << 1) ^ (frames[i]);
-  const uint32_t slot = hash & (ST_HASHTABLE_BUCKETS - 1);
-  StacktraceEntry* st = stack_traces_ht[slot];
-
-  // Look for an existing entry in the hash-table.
-  const size_t frames_length = depth * sizeof(uintptr_t);
-  while (st != NULL && st->hash != hash &&
-         memcmp(frames, st->frames, frames_length) != 0) {
-    st = st->next;
-  }
-
-  // If not found, create a new one from the stack_traces array and add it to
-  // the hash-table.
-  if (st == NULL) {
-    // Get a free element either from the freelist or from the pool.
-    if (stack_traces_freelist != NULL) {
-      st = stack_traces_freelist;
-      stack_traces_freelist = stack_traces_freelist->next;
-    } else if (stats->max_stack_traces < ST_ENTRIES_MAX) {
-      st = &stack_traces[stats->max_stack_traces];
-      ++stats->max_stack_traces;
-    } else {
-      return NULL;
-    }
-
-    memset(st, 0, sizeof(*st));
-    memcpy(st->frames, frames, frames_length);
-    st->hash = hash;
-    st->next = stack_traces_ht[slot];
-    stack_traces_ht[slot] = st;
-    ++stats->num_stack_traces;
-  }
-
-  return st;
-}
-
-// Frees up a stack trace and appends it to the corresponding freelist.
-static void free_stacktrace(StacktraceEntry* st) {
-  assert(st->alloc_bytes == 0);
-  const uint32_t slot = st->hash & (ST_HASHTABLE_BUCKETS - 1);
-
-  // The expected load factor of the hash-table is very low. Frees should be
-  // pretty rare. Hence don't bother with a doubly linked list, might cost more.
-  StacktraceEntry** prev = &stack_traces_ht[slot];
-  while (*prev != st)
-    prev = &((*prev)->next);
-
-  // Remove from the hash-table bucket.
-  assert(*prev == st);
-  *prev = st->next;
-
-  // Add to the freelist.
-  st->next = stack_traces_freelist;
-  stack_traces_freelist = st;
-  --stats->num_stack_traces;
-}
-
-// +---------------------------------------------------------------------------+
-// + Allocs RB-tree                                                            +
-// +---------------------------------------------------------------------------+
-#define ALLOCS_ENTRIES_MAX (256 * 1024)
-
-static Alloc allocs[ALLOCS_ENTRIES_MAX];
-static Alloc* allocs_freelist;
-static RB_HEAD(HeapEntriesTree, Alloc) allocs_tree =
-    RB_INITIALIZER(&allocs_tree);
-
-// Comparator used by the RB-Tree (mind the overflow, avoid arith on addresses).
-static int allocs_tree_cmp(Alloc *alloc_1, Alloc *alloc_2) {
-  if (alloc_1->start < alloc_2->start)
-    return -1;
-  if (alloc_1->start > alloc_2->start)
-    return 1;
-  return 0;
-}
-
-RB_PROTOTYPE(HeapEntriesTree, Alloc, rb_node, allocs_tree_cmp);
-RB_GENERATE(HeapEntriesTree, Alloc, rb_node, allocs_tree_cmp);
-
-// Allocates a new Alloc and inserts it in the tree.
-static Alloc* insert_alloc(
-    uintptr_t start, uintptr_t end, StacktraceEntry* st, uint32_t flags) {
-  Alloc* alloc = NULL;
-
-  // First of all, get a free element either from the freelist or from the pool.
-  if (allocs_freelist != NULL) {
-    alloc = allocs_freelist;
-    allocs_freelist = alloc->next_free;
-  } else if (stats->max_allocs < ALLOCS_ENTRIES_MAX) {
-    alloc = &allocs[stats->max_allocs];
-    ++stats->max_allocs;
-  } else {
-    return NULL;  // OOM.
-  }
-
-  alloc->start = start;
-  alloc->end = end;
-  alloc->st = st;
-  alloc->flags = flags;
-  alloc->next_free = NULL;
-  RB_INSERT(HeapEntriesTree, &allocs_tree, alloc);
-  ++stats->num_allocs;
-  return alloc;
-}
-
-// Deletes all the allocs in the range [addr, addr+size[ dealing with partial
-// frees and hole punching. Note that in the general case this function might
-// need to deal with very unfortunate cases, as below:
-//
-// Alloc tree begin: [Alloc 1]----[Alloc 2]-------[Alloc 3][Alloc 4]---[Alloc 5]
-// Deletion range:                      [xxxxxxxxxxxxxxxxxxxx]
-// Alloc tree end:   [Alloc 1]----[Al.2]----------------------[Al.4]---[Alloc 5]
-//                   Alloc3 has to be deleted and Alloc 2,4 shrunk.
-static uint32_t delete_allocs_in_range(void* addr, size_t size) {
-  uintptr_t del_start = (uintptr_t) addr;
-  uintptr_t del_end = del_start + size - 1;
-  uint32_t flags = 0;
-
-  Alloc* alloc = NULL;
-  Alloc* next_alloc = RB_ROOT(&allocs_tree);
-
-  // Lookup the first (by address) relevant Alloc to initiate the deletion walk.
-  // At the end of the loop next_alloc is either:
-  // - the closest alloc starting before (or exactly at) the start of the
-  //   deletion range (i.e. addr == del_start).
-  // - the first alloc inside the deletion range.
-  // - the first alloc after the deletion range iff the range was already empty
-  //   (in this case the next loop will just bail out doing nothing).
-  // - NULL: iff the entire tree is empty (as above).
-  while (next_alloc != NULL) {
-    alloc = next_alloc;
-    if (alloc->start > del_start) {
-      next_alloc = RB_LEFT(alloc, rb_node);
-    } else if (alloc->end < del_start) {
-      next_alloc = RB_RIGHT(alloc, rb_node);
-    } else {  // alloc->start <= del_start && alloc->end >= del_start
-      break;
-    }
-  }
-
-  // Now scan the allocs linearly deleting chunks (or eventually whole allocs)
-  // until passing the end of the deleting region.
-  next_alloc = alloc;
-  while (next_alloc != NULL) {
-    alloc = next_alloc;
-    next_alloc = RB_NEXT(HeapEntriesTree, &allocs_tree, alloc);
-
-    if (size != 0) {
-      // In the general case we stop passed the end of the deletion range.
-      if (alloc->start > del_end)
-        break;
-
-      // This deals with the case of the first Alloc laying before the range.
-      if (alloc->end < del_start)
-        continue;
-    } else {
-      // size == 0 is a special case. It means deleting only the alloc which
-      // starts exactly at |del_start| if any (for dealing with free(ptr)).
-      if (alloc->start > del_start)
-        break;
-      if (alloc->start < del_start)
-        continue;
-      del_end = alloc->end;
-    }
-
-    // Reached this point the Alloc must overlap (partially or completely) with
-    // the deletion range.
-    assert(!(alloc->start > del_end || alloc->end < del_start));
-
-    StacktraceEntry* st = alloc->st;
-    flags |= alloc->flags;
-    uintptr_t freed_bytes = 0;  // Bytes freed in this cycle.
-
-    if (del_start <= alloc->start) {
-      if (del_end >= alloc->end) {
-        // Complete overlap. Delete full Alloc. Note: the range might might
-        // still overlap with the next allocs.
-        // Begin:      ------[alloc.start    alloc.end]-[next alloc]
-        // Del range:      [xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]
-        // Result:     ---------------------------------[next alloc]
-        //             [next alloc] will be shrinked on the next iteration.
-        freed_bytes = alloc->end - alloc->start + 1;
-        RB_REMOVE(HeapEntriesTree, &allocs_tree, alloc);
-
-        // Clean-up, so heap_dump can tell this is a free entry and skip it.
-        alloc->start = alloc->end = 0;
-        alloc->st = NULL;
-
-        // Put in the freelist.
-        alloc->next_free = allocs_freelist;
-        allocs_freelist = alloc;
-        --stats->num_allocs;
-      } else {
-        // Partial overlap at beginning. Cut first part and shrink the alloc.
-        // Begin:      ------[alloc.start  alloc.end]-[next alloc]
-        // Del range:      [xxxxxx]
-        // Result:     ------------[start  alloc.end]-[next alloc]
-        freed_bytes = del_end - alloc->start + 1;
-        alloc->start = del_end + 1;
-        // No need to update the tree even if we changed the key. The keys are
-        // still monotonic (because the ranges are guaranteed to not overlap).
-      }
-    } else {
-      if (del_end >= alloc->end) {
-        // Partial overlap at end. Cut last part and shrink the alloc left.
-        // Begin:      ------[alloc.start     alloc.end]-[next alloc]
-        // Del range:                               [xxxxxxxx]
-        // Result:     ------[alloc.start alloc.end]-----[next alloc]
-        //             [next alloc] will be shrinked on the next iteration.
-        freed_bytes = alloc->end - del_start + 1;
-        alloc->end = del_start - 1;
-      } else {
-        // Hole punching. Requires creating an extra alloc.
-        // Begin:      ------[alloc.start     alloc.end]-[next alloc]
-        // Del range:                   [xxx]
-        // Result:     ------[ alloc 1 ]-----[ alloc 2 ]-[next alloc]
-        freed_bytes = del_end - del_start + 1;
-        const uintptr_t old_end = alloc->end;
-        alloc->end = del_start - 1;
-
-        // In case of OOM, don't count the 2nd alloc we failed to allocate.
-        if (insert_alloc(del_end + 1, old_end, st, alloc->flags) == NULL)
-          freed_bytes += (old_end - del_end);
-      }
-    }
-    // Now update the StackTraceEntry the Alloc was pointing to, eventually
-    // freeing it up.
-    assert(st->alloc_bytes >= freed_bytes);
-    st->alloc_bytes -= freed_bytes;
-    if (st->alloc_bytes == 0)
-      free_stacktrace(st);
-    stats->total_alloc_bytes -= freed_bytes;
-  }
-  return flags;
-}
-
-// +---------------------------------------------------------------------------+
-// + Library entry points (refer to heap_profiler.h for API doc).              +
-// +---------------------------------------------------------------------------+
-void heap_profiler_free(void* addr, size_t size, uint32_t* old_flags) {
-  assert(size == 0 || ((uintptr_t) addr + (size - 1)) >= (uintptr_t) addr);
-
-  LOCK_MUTEX(lock);
-  uint32_t flags = delete_allocs_in_range(addr, size);
-  UNLOCK_MUTEX(lock);
-
-  if (old_flags != NULL)
-    *old_flags = flags;
-}
-
-void heap_profiler_alloc(void* addr, size_t size, uintptr_t* frames,
-                         uint32_t depth, uint32_t flags) {
-  if (depth > HEAP_PROFILER_MAX_DEPTH)
-    depth = HEAP_PROFILER_MAX_DEPTH;
-
-  if (size == 0)  // Apps calling malloc(0), sometimes it happens.
-    return;
-
-  const uintptr_t start = (uintptr_t) addr;
-  const uintptr_t end = start + (size - 1);
-  assert(start <= end);
-
-  LOCK_MUTEX(lock);
-
-  delete_allocs_in_range(addr, size);
-
-  StacktraceEntry* st = record_stacktrace(frames, depth);
-  if (st != NULL) {
-    Alloc* alloc = insert_alloc(start, end, st, flags);
-    if (alloc != NULL) {
-      st->alloc_bytes += size;
-      stats->total_alloc_bytes += size;
-    }
-  }
-
-  UNLOCK_MUTEX(lock);
-}
-
-void heap_profiler_init(HeapStats* heap_stats) {
-  LOCK_MUTEX(lock);
-
-  assert(stats == NULL);
-  stats = heap_stats;
-  memset(stats, 0, sizeof(HeapStats));
-  stats->magic_start = HEAP_PROFILER_MAGIC_MARKER;
-  stats->allocs = &allocs[0];
-  stats->stack_traces = &stack_traces[0];
-
-  UNLOCK_MUTEX(lock);
-}
-
-void heap_profiler_cleanup(void) {
-  LOCK_MUTEX(lock);
-
-  assert(stats != NULL);
-  memset(stack_traces, 0, sizeof(StacktraceEntry) * stats->max_stack_traces);
-  memset(stack_traces_ht, 0, sizeof(stack_traces_ht));
-  stack_traces_freelist = NULL;
-
-  memset(allocs, 0, sizeof(Alloc) * stats->max_allocs);
-  allocs_freelist = NULL;
-  RB_INIT(&allocs_tree);
-
-  stats = NULL;
-
-  UNLOCK_MUTEX(lock);
-}
diff --git a/heap_profiler/heap_profiler.gyp b/heap_profiler/heap_profiler.gyp
deleted file mode 100644
index 50e6797..0000000
--- a/heap_profiler/heap_profiler.gyp
+++ /dev/null
@@ -1,75 +0,0 @@
-# Copyright 2014 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-{
-  'targets': [
-    {
-      # libheap_profiler is the library that will be preloaded in the Android
-      # Zygote and contains the black magic to hook malloc/mmap calls.
-      'target_name': 'heap_profiler',
-      'type': 'shared_library',
-      'include_dirs': [ '../../..' ],
-      'sources': [ 'heap_profiler_hooks_android.c' ],
-      'dependencies': [ 'heap_profiler_core' ],
-    },
-    {
-      # heap_profiler_core contains only the tracking metadata code without any
-      # hooks. It is required by both the hprof library itself and the unittest.
-      'target_name': 'heap_profiler_core',
-      'type': 'static_library',
-      'sources': [
-        'heap_profiler.c',
-        'heap_profiler.h',
-      ],
-      'include_dirs': [ '../../..' ],
-    },
-    {
-      'target_name': 'heap_dump',
-      'type': 'executable',
-      'sources': [ 'heap_dump.c' ],
-      'include_dirs': [ '../../..' ],
-    },
-    {
-      'target_name': 'heap_profiler_unittests',
-      'type': '<(gtest_target_type)',
-      'sources': [ 'heap_profiler_unittest.cc' ],
-      'dependencies': [
-        'heap_profiler_core',
-        '../../../testing/android/native_test.gyp:native_test_native_code',
-        '../../../testing/gtest.gyp:gtest',
-        '../../../testing/gtest.gyp:gtest_main',
-      ],
-      'include_dirs': [ '../../..' ],
-    },
-    {
-      'target_name': 'heap_profiler_unittests_apk',
-      'type': 'none',
-      'dependencies': [
-        'heap_profiler_unittests',
-      ],
-      'variables': {
-        'test_suite_name': 'heap_profiler_unittests',
-      },
-      'includes': [ '../../../build/apk_test.gypi' ],
-    },
-    {
-      'target_name': 'heap_profiler_integrationtest',
-      'type': 'executable',
-      'sources': [ 'heap_profiler_integrationtest.cc' ],
-      'dependencies': [ '../../../testing/gtest.gyp:gtest' ],
-      'include_dirs': [ '../../..' ],
-    },
-    {
-      'target_name': 'heap_profiler_integrationtest_stripped',
-      'type': 'none',
-      'dependencies': [ 'heap_profiler_integrationtest' ],
-      'actions': [{
-        'action_name': 'strip heap_profiler_integrationtest',
-        'inputs': [ '<(PRODUCT_DIR)/heap_profiler_integrationtest' ],
-        'outputs': [ '<(PRODUCT_DIR)/heap_profiler_integrationtest_stripped' ],
-        'action': [ '<(android_strip)', '<@(_inputs)', '-o', '<@(_outputs)' ],
-      }],
-    },
-  ],
-}
diff --git a/heap_profiler/heap_profiler.h b/heap_profiler/heap_profiler.h
deleted file mode 100644
index 4711f47..0000000
--- a/heap_profiler/heap_profiler.h
+++ /dev/null
@@ -1,92 +0,0 @@
-// Copyright 2014 The Chromium Authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#ifndef TOOLS_ANDROID_HEAP_PROFILER_HEAP_PROFILER_H_
-#define TOOLS_ANDROID_HEAP_PROFILER_HEAP_PROFILER_H_
-
-#include <stddef.h>
-#include <stdint.h>
-
-#include "third_party/bsdtrees/tree.h"
-
-#define HEAP_PROFILER_MAGIC_MARKER 0x42beef42L
-#define HEAP_PROFILER_MAX_DEPTH 12
-
-// The allocation is a result of a system malloc() invocation.
-#define HEAP_PROFILER_FLAGS_MALLOC 1
-
-// The allocation is a result of a mmap() invocation.
-#define HEAP_PROFILER_FLAGS_MMAP 2  // Allocation performed through mmap.
-
-// Only in the case of FLAGS_MMAP: The mmap is not anonymous (i.e. file backed).
-#define HEAP_PROFILER_FLAGS_MMAP_FILE 4
-
-// Android only: allocation made by the Zygote (before forking).
-#define HEAP_PROFILER_FLAGS_IN_ZYGOTE 8
-
-#ifdef __cplusplus
-extern "C" {
-#endif
-
-typedef struct StacktraceEntry {
-  uintptr_t frames[HEAP_PROFILER_MAX_DEPTH];  // Absolute addrs of stack frames.
-  uint32_t hash;  // H(frames), used to keep these entries in a hashtable.
-
-  // Total number of bytes allocated through this code path. It is equal to the
-  // sum of Alloc instances' length which .bt == this.
-  size_t alloc_bytes;
-
-  // |next| has a dual purpose. When the entry is used (hence in the hashtable),
-  // this is a ptr to the next item in the same bucket. When the entry is free,
-  // this is a ptr to the next entry in the freelist.
-  struct StacktraceEntry* next;
-} StacktraceEntry;
-
-// Represents a contiguous range of virtual memory which has been allocated by
-// a give code path (identified by the corresponding StacktraceEntry).
-typedef struct Alloc {
-  RB_ENTRY(Alloc) rb_node;  // Anchor for the RB-tree;
-  uintptr_t start;
-  uintptr_t end;
-  uint32_t flags;       // See HEAP_PROFILER_FLAGS_*.
-  StacktraceEntry* st;  // NULL == free entry.
-  struct Alloc* next_free;
-} Alloc;
-
-typedef struct {
-  uint32_t magic_start;       // The magic marker used to locate the stats mmap.
-  uint32_t num_allocs;        // The total number of allocation entries present.
-  uint32_t max_allocs;        // The max number of items in |allocs|.
-  uint32_t num_stack_traces;  // The total number of stack traces present.
-  uint32_t max_stack_traces;  // The max number of items in |stack_traces|.
-  size_t total_alloc_bytes;   // Total allocation bytes tracked.
-  Alloc* allocs;              // Start of the the Alloc pool.
-  StacktraceEntry* stack_traces;  // Start of the StacktraceEntry pool.
-} HeapStats;
-
-// Initialize the heap_profiler. The caller has to allocate the HeapStats
-// "superblock", since the way it is mapped is platform-specific.
-void heap_profiler_init(HeapStats* heap_stats);
-
-// Records and allocation. The caller must unwind the stack and pass the
-// frames array. Flags are optionals and don't affect the behavior of the
-// library (they're just kept along and dumped).
-void heap_profiler_alloc(void* addr,
-                         size_t size,
-                         uintptr_t* frames,
-                         uint32_t depth,
-                         uint32_t flags);
-
-// Frees any allocation (even partial) overlapping with the given range.
-// If old_flags != NULL, it will be filled with the flags of the deleted allocs.
-void heap_profiler_free(void* addr, size_t size, uint32_t* old_flags);
-
-// Cleans up the HeapStats and all the internal data structures.
-void heap_profiler_cleanup(void);
-
-#ifdef __cplusplus
-}
-#endif
-
-#endif  // TOOLS_ANDROID_HEAP_PROFILER_HEAP_PROFILER_H_
diff --git a/heap_profiler/heap_profiler_hooks_android.c b/heap_profiler/heap_profiler_hooks_android.c
deleted file mode 100644
index af102ec..0000000
--- a/heap_profiler/heap_profiler_hooks_android.c
+++ /dev/null
@@ -1,211 +0,0 @@
-// Copyright 2014 The Chromium Authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#include <dlfcn.h>
-#include <errno.h>
-#include <fcntl.h>
-#include <stdbool.h>
-#include <stddef.h>
-#include <stdint.h>
-#include <stdlib.h>
-#include <string.h>
-#include <sys/mman.h>
-#include <unistd.h>
-#include <unwind.h>
-
-#include "tools/android/heap_profiler/heap_profiler.h"
-
-#define HEAP_PROFILER_EXPORT __attribute__((visibility("default")))
-
-
-static inline __attribute__((always_inline))
-uint32_t get_backtrace(uintptr_t* frames, uint32_t max_depth);
-
-// Function pointers typedefs for the hooked symbols.
-typedef void* (*mmap_t)(void*, size_t, int, int, int, off_t);
-typedef void* (*mmap2_t)(void*, size_t, int, int, int, off_t);
-typedef void* (*mmap64_t)(void*, size_t, int, int, int, off64_t);
-typedef void* (*mremap_t)(void*, size_t, size_t, unsigned long);
-typedef int (*munmap_t)(void*, size_t);
-typedef void* (*malloc_t)(size_t);
-typedef void* (*calloc_t)(size_t, size_t);
-typedef void* (*realloc_t)(void*, size_t);
-typedef void (*free_t)(void*);
-
-// And their actual definitions.
-static mmap_t real_mmap;
-static mmap2_t real_mmap2;
-static mmap64_t real_mmap64;
-static mremap_t real_mremap;
-static munmap_t real_munmap;
-static malloc_t real_malloc;
-static calloc_t real_calloc;
-static realloc_t real_realloc;
-static free_t real_free;
-static int* has_forked_off_zygote;
-
-HEAP_PROFILER_EXPORT const HeapStats* heap_profiler_stats_for_tests;
-
-// +---------------------------------------------------------------------------+
-// + Initialization of heap_profiler and lookup of hooks' addresses            +
-// +---------------------------------------------------------------------------+
-__attribute__((constructor))
-static void initialize() {
-  real_mmap = (mmap_t) dlsym(RTLD_NEXT, "mmap");
-  real_mmap2 = (mmap_t) dlsym(RTLD_NEXT, "mmap2");
-  real_mmap64 = (mmap64_t) dlsym(RTLD_NEXT, "mmap64");
-  real_mremap = (mremap_t) dlsym(RTLD_NEXT, "mremap");
-  real_munmap = (munmap_t) dlsym(RTLD_NEXT, "munmap");
-  real_malloc = (malloc_t) dlsym(RTLD_NEXT, "malloc");
-  real_calloc = (calloc_t) dlsym(RTLD_NEXT, "calloc");
-  real_realloc = (realloc_t) dlsym(RTLD_NEXT, "realloc");
-  real_free = (free_t) dlsym(RTLD_NEXT, "free");
-
-  // gMallocLeakZygoteChild is an extra useful piece of information to have.
-  // When available, it tells whether we're in the zygote (=0) or forked (=1)
-  // a child off it. In the worst case it will be NULL and we'll just ignore it.
-  has_forked_off_zygote = (int*) dlsym(RTLD_NEXT, "gMallocLeakZygoteChild");
-
-  // Allocate room for the HeapStats area and initialize the heap profiler.
-  // Make an explicit map of /dev/zero (instead of MAP_ANONYMOUS), so that the
-  // heap_dump tool can easily spot the mapping in the target process.
-  int fd = open("/dev/zero", O_RDONLY);
-  if (fd < 0) {
-    abort();  // This world has gone wrong. Good night Vienna.
-  }
-
-  HeapStats* stats = (HeapStats*) real_mmap(
-      0, sizeof(HeapStats), PROT_READ | PROT_WRITE, MAP_PRIVATE, fd, 0);
-  heap_profiler_stats_for_tests = stats;
-  heap_profiler_init(stats);
-}
-
-static inline __attribute__((always_inline)) void unwind_and_record_alloc(
-    void* start, size_t size, uint32_t flags) {
-  const int errno_save = errno;
-  uintptr_t frames[HEAP_PROFILER_MAX_DEPTH];
-  const uint32_t depth = get_backtrace(frames, HEAP_PROFILER_MAX_DEPTH);
-  if (has_forked_off_zygote != NULL && *has_forked_off_zygote == 0)
-    flags |= HEAP_PROFILER_FLAGS_IN_ZYGOTE;
-  heap_profiler_alloc(start, size, frames, depth, flags);
-  errno = errno_save;
-}
-
-static inline __attribute__((always_inline)) void discard_alloc(
-    void* start, size_t size, uint32_t* old_flags) {
-  const int errno_save = errno;
-  heap_profiler_free(start, size, old_flags);
-  errno = errno_save;
-}
-
-// Flags are non-functional extra decorators that are made available to the
-// final heap_dump tool, to get more details about the source of the allocation.
-static uint32_t get_flags_for_mmap(int fd) {
-  return HEAP_PROFILER_FLAGS_MMAP | (fd ? HEAP_PROFILER_FLAGS_MMAP_FILE : 0);
-}
-
-// +---------------------------------------------------------------------------+
-// + Actual mmap/malloc hooks                                                  +
-// +---------------------------------------------------------------------------+
-HEAP_PROFILER_EXPORT void* mmap(
-    void* addr, size_t size, int prot, int flags, int fd, off_t offset)  {
-  void* ret = real_mmap(addr, size, prot, flags, fd, offset);
-  if (ret != MAP_FAILED)
-    unwind_and_record_alloc(ret, size, get_flags_for_mmap(fd));
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT void* mmap2(
-    void* addr, size_t size, int prot, int flags, int fd, off_t pgoffset)  {
-  void* ret = real_mmap2(addr, size, prot, flags, fd, pgoffset);
-  if (ret != MAP_FAILED)
-    unwind_and_record_alloc(ret, size, get_flags_for_mmap(fd));
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT void* mmap64(
-    void* addr, size_t size, int prot, int flags, int fd, off64_t offset) {
-  void* ret = real_mmap64(addr, size, prot, flags, fd, offset);
-  if (ret != MAP_FAILED)
-    unwind_and_record_alloc(ret, size, get_flags_for_mmap(fd));
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT void* mremap(
-    void* addr, size_t oldlen, size_t newlen, unsigned long flags) {
-  void* ret = real_mremap(addr, oldlen, newlen, flags);
-  if (ret != MAP_FAILED) {
-    uint32_t flags = 0;
-    if (addr)
-      discard_alloc(addr, oldlen, &flags);
-    if (newlen > 0)
-      unwind_and_record_alloc(ret, newlen, flags);
-  }
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT int munmap(void* ptr, size_t size) {
-  int ret = real_munmap(ptr, size);
-  discard_alloc(ptr, size, /*old_flags=*/NULL);
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT void* malloc(size_t byte_count) {
-  void* ret = real_malloc(byte_count);
-  if (ret != NULL)
-    unwind_and_record_alloc(ret, byte_count, HEAP_PROFILER_FLAGS_MALLOC);
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT void* calloc(size_t nmemb, size_t size) {
-  void* ret = real_calloc(nmemb, size);
-  if (ret != NULL)
-    unwind_and_record_alloc(ret, nmemb * size, HEAP_PROFILER_FLAGS_MALLOC);
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT void* realloc(void* ptr, size_t size) {
-  void* ret = real_realloc(ptr, size);
-  uint32_t flags = 0;
-  if (ptr)
-    discard_alloc(ptr, 0, &flags);
-  if (ret != NULL)
-    unwind_and_record_alloc(ret, size, flags | HEAP_PROFILER_FLAGS_MALLOC);
-  return ret;
-}
-
-HEAP_PROFILER_EXPORT void free(void* ptr) {
-  real_free(ptr);
-  discard_alloc(ptr, 0, /*old_flags=*/NULL);
-}
-
-// +---------------------------------------------------------------------------+
-// + Stack unwinder                                                            +
-// +---------------------------------------------------------------------------+
-typedef struct {
-  uintptr_t* frames;
-  uint32_t frame_count;
-  uint32_t max_depth;
-  bool have_skipped_self;
-} stack_crawl_state_t;
-
-static _Unwind_Reason_Code unwind_fn(struct _Unwind_Context* ctx, void* arg) {
-  stack_crawl_state_t* state = (stack_crawl_state_t*) arg;
-  uintptr_t ip = _Unwind_GetIP(ctx);
-
-  if (ip != 0 && !state->have_skipped_self) {
-    state->have_skipped_self = true;
-    return _URC_NO_REASON;
-  }
-
-  state->frames[state->frame_count++] = ip;
-  return (state->frame_count >= state->max_depth) ?
-          _URC_END_OF_STACK : _URC_NO_REASON;
-}
-
-static uint32_t get_backtrace(uintptr_t* frames, uint32_t max_depth) {
-  stack_crawl_state_t state = {.frames = frames, .max_depth = max_depth};
-  _Unwind_Backtrace(unwind_fn, &state);
-  return state.frame_count;
-}
diff --git a/heap_profiler/heap_profiler_integrationtest.cc b/heap_profiler/heap_profiler_integrationtest.cc
deleted file mode 100644
index f1909b1..0000000
--- a/heap_profiler/heap_profiler_integrationtest.cc
+++ /dev/null
@@ -1,179 +0,0 @@
-// Copyright 2014 The Chromium Authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#include <dlfcn.h>
-#include <fcntl.h>
-#include <stddef.h>
-#include <stdlib.h>
-#include <string.h>
-#include <sys/mman.h>
-#include <unistd.h>
-#include <map>
-
-#include "testing/gtest/include/gtest/gtest.h"
-#include "tools/android/heap_profiler/heap_profiler.h"
-
-namespace {
-
-typedef void* (*AllocatorFn)(size_t);
-typedef int (*FreeFn)(void*, size_t);
-
-const size_t kSize1 = 499 * PAGE_SIZE;
-const size_t kSize2 = 503 * PAGE_SIZE;
-const size_t kSize3 = 509 * PAGE_SIZE;
-
-// The purpose of the four functions below is to create watermarked allocations,
-// so the test fixture can ascertain that the hooks work end-to-end.
-__attribute__((noinline)) void* MallocInner(size_t size) {
-  void* ptr = malloc(size);
-  // The memset below is to avoid tail-call elimination optimizations and ensure
-  // that this function will be part of the stack trace.
-  memset(ptr, 0, size);
-  return ptr;
-}
-
-__attribute__((noinline)) void* MallocOuter(size_t size) {
-  void* ptr = MallocInner(size);
-  memset(ptr, 0, size);
-  return ptr;
-}
-
-__attribute__((noinline)) void* DoMmap(size_t size) {
-  return mmap(
-      0, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
-}
-
-__attribute__((noinline)) void* MmapInner(size_t size) {
-  void* ptr = DoMmap(size);
-  memset(ptr, 0, size);
-  return ptr;
-}
-
-__attribute__((noinline)) void* MmapOuter(size_t size) {
-  void* ptr = MmapInner(size);
-  memset(ptr, 0, size);
-  return ptr;
-}
-
-const HeapStats* GetHeapStats() {
-  HeapStats* const* stats_ptr = reinterpret_cast<HeapStats* const*>(
-      dlsym(RTLD_DEFAULT, "heap_profiler_stats_for_tests"));
-  EXPECT_TRUE(stats_ptr != NULL);
-  const HeapStats* stats = *stats_ptr;
-  EXPECT_TRUE(stats != NULL);
-  EXPECT_EQ(HEAP_PROFILER_MAGIC_MARKER, stats->magic_start);
-  return stats;
-}
-
-bool StackTraceContains(const StacktraceEntry* s, AllocatorFn fn) {
-  // kExpectedFnLen is a gross estimation of the watermark functions' size.
-  // It tries to address the following problem: the addrs in the unwound stack
-  // stack frames will NOT point to the beginning of the functions, but to the
-  // PC after the call to malloc/mmap.
-  const size_t kExpectedFnLen = 16;
-  const uintptr_t fn_addr = reinterpret_cast<uintptr_t>(fn);
-  for (size_t i = 0; i < HEAP_PROFILER_MAX_DEPTH; ++i) {
-    if (s->frames[i] >= fn_addr && s->frames[i] <= fn_addr + kExpectedFnLen)
-      return true;
-  }
-  return false;
-}
-
-const StacktraceEntry* LookupStackTrace(size_t size, AllocatorFn fn) {
-  const HeapStats* stats = GetHeapStats();
-  for (size_t i = 0; i < stats->max_stack_traces; ++i) {
-    const StacktraceEntry* st = &stats->stack_traces[i];
-    if (st->alloc_bytes == size && StackTraceContains(st, fn))
-      return st;
-  }
-  return NULL;
-}
-
-int DoFree(void* addr, size_t /*size, ignored.*/) {
-  free(addr);
-  return 0;
-}
-
-void TestStackTracesWithParams(AllocatorFn outer_fn,
-                               AllocatorFn inner_fn,
-                               FreeFn free_fn) {
-  const HeapStats* stats = GetHeapStats();
-
-  void* m1 = outer_fn(kSize1);
-  void* m2 = inner_fn(kSize2);
-  void* m3 = inner_fn(kSize3);
-  free_fn(m3, kSize3);
-
-  const StacktraceEntry* st1 = LookupStackTrace(kSize1, inner_fn);
-  const StacktraceEntry* st2 = LookupStackTrace(kSize2, inner_fn);
-  const StacktraceEntry* st3 = LookupStackTrace(kSize3, inner_fn);
-
-  EXPECT_TRUE(st1 != NULL);
-  EXPECT_TRUE(StackTraceContains(st1, outer_fn));
-  EXPECT_TRUE(StackTraceContains(st1, inner_fn));
-
-  EXPECT_TRUE(st2 != NULL);
-  EXPECT_FALSE(StackTraceContains(st2, outer_fn));
-  EXPECT_TRUE(StackTraceContains(st2, inner_fn));
-
-  EXPECT_EQ(NULL, st3);
-
-  const size_t total_alloc_start = stats->total_alloc_bytes;
-  const size_t num_stack_traces_start = stats->num_stack_traces;
-
-  free_fn(m1, kSize1);
-  free_fn(m2, kSize2);
-
-  const size_t total_alloc_end = stats->total_alloc_bytes;
-  const size_t num_stack_traces_end = stats->num_stack_traces;
-
-  EXPECT_EQ(kSize1 + kSize2, total_alloc_start - total_alloc_end);
-  EXPECT_EQ(2, num_stack_traces_start - num_stack_traces_end);
-  EXPECT_EQ(NULL, LookupStackTrace(kSize1, inner_fn));
-  EXPECT_EQ(NULL, LookupStackTrace(kSize2, inner_fn));
-  EXPECT_EQ(NULL, LookupStackTrace(kSize3, inner_fn));
-}
-
-TEST(HeapProfilerIntegrationTest, TestMallocStackTraces) {
-  TestStackTracesWithParams(&MallocOuter, &MallocInner, &DoFree);
-}
-
-TEST(HeapProfilerIntegrationTest, TestMmapStackTraces) {
-  TestStackTracesWithParams(&MmapOuter, &MmapInner, &munmap);
-}
-
-// Returns the path of the directory containing the current executable.
-std::string GetExePath() {
-  char buf[1024];
-  ssize_t len = readlink("/proc/self/exe", buf, sizeof(buf) - 1);
-  if (len == -1)
-    return std::string();
-  std::string path(buf, len);
-  size_t sep = path.find_last_of('/');
-  if (sep == std::string::npos)
-    return std::string();
-  path.erase(sep);
-  return path;
-}
-
-}  // namespace
-
-int main(int argc, char** argv) {
-  // Re-launch the process itself forcing the preload of the libheap_profiler.
-  char* ld_preload = getenv("LD_PRELOAD");
-  if (ld_preload == NULL || strstr(ld_preload, "libheap_profiler.so") == NULL) {
-    char env_ld_lib_path[256];
-    strlcpy(env_ld_lib_path, "LD_LIBRARY_PATH=", sizeof(env_ld_lib_path));
-    strlcat(env_ld_lib_path, GetExePath().c_str(), sizeof(env_ld_lib_path));
-    char env_ld_preload[] = "LD_PRELOAD=libheap_profiler.so";
-    char* const env[] = {env_ld_preload, env_ld_lib_path, 0};
-    execve("/proc/self/exe", argv, env);
-    // execve() never returns, unless something goes wrong.
-    perror("execve");
-    assert(false);
-  }
-
-  testing::InitGoogleTest(&argc, argv);
-  return RUN_ALL_TESTS();
-}
diff --git a/heap_profiler/heap_profiler_unittest.cc b/heap_profiler/heap_profiler_unittest.cc
deleted file mode 100644
index 893f214..0000000
--- a/heap_profiler/heap_profiler_unittest.cc
+++ /dev/null
@@ -1,458 +0,0 @@
-// Copyright 2014 The Chromium Authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#include <stddef.h>
-#include <stdint.h>
-#include <string.h>
-#include <map>
-
-#include "testing/gtest/include/gtest/gtest.h"
-#include "tools/android/heap_profiler/heap_profiler.h"
-
-namespace {
-
-class HeapProfilerTest : public testing::Test {
- public:
-  void SetUp() override { heap_profiler_init(&stats_); }
-
-  void TearDown() override {
-    CheckAllocVsStacktaceConsistency();
-    heap_profiler_cleanup();
-  }
-
- protected:
-  struct StackTrace {
-    uintptr_t frames[HEAP_PROFILER_MAX_DEPTH];
-    size_t depth;
-  };
-
-  StackTrace GenStackTrace(size_t depth, uintptr_t base) {
-    assert(depth <= HEAP_PROFILER_MAX_DEPTH);
-    StackTrace st;
-    for (size_t i = 0; i < depth; ++i)
-      st.frames[i] = base + i * 0x10UL;
-    st.depth = depth;
-    return st;
-  }
-
-  void ExpectAlloc(uintptr_t start,
-                   uintptr_t end,
-                   const StackTrace& st,
-                   uint32_t flags) {
-    for (uint32_t i = 0; i < stats_.max_allocs; ++i) {
-      const Alloc& alloc = stats_.allocs[i];
-      if (alloc.start != start || alloc.end != end)
-        continue;
-      // Check that the stack trace match.
-      for (uint32_t j = 0; j < st.depth; ++j) {
-        EXPECT_EQ(st.frames[j], alloc.st->frames[j])
-            << "Stacktrace not matching @ depth " << j;
-      }
-      EXPECT_EQ(flags, alloc.flags);
-      return;
-    }
-
-    FAIL() << "Alloc not found [" << std::hex << start << "," << end << "]";
-  }
-
-  void CheckAllocVsStacktaceConsistency() {
-    uint32_t allocs_seen = 0;
-    uint32_t stack_traces_seen = 0;
-    std::map<StacktraceEntry*, uintptr_t> stacktrace_bytes_by_alloc;
-
-    for (uint32_t i = 0; i < stats_.max_allocs; ++i) {
-      Alloc* alloc = &stats_.allocs[i];
-      if (alloc->start == 0 && alloc->end == 0)
-        continue;
-      ++allocs_seen;
-      stacktrace_bytes_by_alloc[alloc->st] += alloc->end - alloc->start + 1;
-    }
-
-    for (uint32_t i = 0; i < stats_.max_stack_traces; ++i) {
-      StacktraceEntry* st = &stats_.stack_traces[i];
-      if (st->alloc_bytes == 0)
-        continue;
-      ++stack_traces_seen;
-      EXPECT_EQ(1u, stacktrace_bytes_by_alloc.count(st));
-      EXPECT_EQ(stacktrace_bytes_by_alloc[st], st->alloc_bytes);
-    }
-
-    EXPECT_EQ(allocs_seen, stats_.num_allocs);
-    EXPECT_EQ(stack_traces_seen, stats_.num_stack_traces);
-  }
-
-  HeapStats stats_;
-};
-
-TEST_F(HeapProfilerTest, SimpleAlloc) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 2048, st1.frames, st1.depth, 0);
-
-  EXPECT_EQ(2u, stats_.num_allocs);
-  EXPECT_EQ(1u, stats_.num_stack_traces);
-  EXPECT_EQ(1024u + 2048, stats_.total_alloc_bytes);
-  ExpectAlloc(0x1000, 0x13ff, st1, 0);
-  ExpectAlloc(0x2000, 0x27ff, st1, 0);
-}
-
-TEST_F(HeapProfilerTest, AllocMultipleStacks) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(4, 0x1000);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 2048, st2.frames, st2.depth, 0);
-  heap_profiler_alloc((void*)0x3000, 32, st1.frames, st1.depth, 0);
-
-  EXPECT_EQ(3u, stats_.num_allocs);
-  EXPECT_EQ(2u, stats_.num_stack_traces);
-  EXPECT_EQ(1024u + 2048 + 32, stats_.total_alloc_bytes);
-  ExpectAlloc(0x1000, 0x13ff, st1, 0);
-  ExpectAlloc(0x2000, 0x27ff, st2, 0);
-  ExpectAlloc(0x3000, 0x301f, st1, 0);
-}
-
-TEST_F(HeapProfilerTest, SimpleAllocAndFree) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  heap_profiler_free((void*)0x1000, 1024, NULL);
-
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, Realloc) {
-  StackTrace st1 = GenStackTrace(8, 0);
-  heap_profiler_alloc((void*)0, 32, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0, 32, st1.frames, st1.depth, 0);
-}
-
-TEST_F(HeapProfilerTest, AllocAndFreeMultipleStacks) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 2048, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x3000, 32, st2.frames, st2.depth, 0);
-  heap_profiler_alloc((void*)0x4000, 64, st2.frames, st2.depth, 0);
-
-  heap_profiler_free((void*)0x1000, 1024, NULL);
-  heap_profiler_free((void*)0x3000, 32, NULL);
-
-  EXPECT_EQ(2u, stats_.num_allocs);
-  EXPECT_EQ(2u, stats_.num_stack_traces);
-  EXPECT_EQ(2048u + 64, stats_.total_alloc_bytes);
-  ExpectAlloc(0x2000, 0x27ff, st1, 0);
-  ExpectAlloc(0x4000, 0x403f, st2, 0);
-}
-
-TEST_F(HeapProfilerTest, AllocAndFreeAll) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 2048, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x3000, 32, st2.frames, st2.depth, 0);
-  heap_profiler_alloc((void*)0x4000, 64, st2.frames, st2.depth, 0);
-
-  heap_profiler_free((void*)0x1000, 1024, NULL);
-  heap_profiler_free((void*)0x2000, 2048, NULL);
-  heap_profiler_free((void*)0x3000, 32, NULL);
-  heap_profiler_free((void*)0x4000, 64, NULL);
-
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, AllocAndFreeWithZeroSize) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 2048, st2.frames, st2.depth, 0);
-
-  heap_profiler_free((void*)0x1000, 0, NULL);
-
-  EXPECT_EQ(1u, stats_.num_allocs);
-  EXPECT_EQ(1u, stats_.num_stack_traces);
-  EXPECT_EQ(2048u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, AllocAndFreeContiguous) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  heap_profiler_alloc((void*)0x1000, 4096, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 4096, st2.frames, st2.depth, 0);
-
-  heap_profiler_free((void*)0x1000, 8192, NULL);
-
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, SparseAllocsOneLargeOuterFree) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-
-  heap_profiler_alloc((void*)0x1010, 1, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x1400, 2, st2.frames, st2.depth, 0);
-  heap_profiler_alloc((void*)0x1600, 5, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x9000, 4096, st2.frames, st2.depth, 0);
-
-  heap_profiler_free((void*)0x0800, 8192, NULL);
-  EXPECT_EQ(1u, stats_.num_allocs);
-  EXPECT_EQ(1u, stats_.num_stack_traces);
-  EXPECT_EQ(4096u, stats_.total_alloc_bytes);
-  ExpectAlloc(0x9000, 0x9fff, st2, 0);
-}
-
-TEST_F(HeapProfilerTest, SparseAllocsOneLargePartiallyOverlappingFree) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  StackTrace st3 = GenStackTrace(4, 0x2000);
-
-  // This will be untouched.
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-
-  // These will be partially freed in one shot (% 64 a bytes "margin").
-  heap_profiler_alloc((void*)0x2000, 128, st2.frames, st2.depth, 0);
-  heap_profiler_alloc((void*)0x2400, 128, st2.frames, st2.depth, 0);
-  heap_profiler_alloc((void*)0x2f80, 128, st2.frames, st2.depth, 0);
-
-  // This will be untouched.
-  heap_profiler_alloc((void*)0x3000, 1024, st3.frames, st3.depth, 0);
-
-  heap_profiler_free((void*)0x2040, 4096 - 64 - 64, NULL);
-  EXPECT_EQ(4u, stats_.num_allocs);
-  EXPECT_EQ(3u, stats_.num_stack_traces);
-  EXPECT_EQ(1024u + 64 + 64 + 1024, stats_.total_alloc_bytes);
-
-  ExpectAlloc(0x1000, 0x13ff, st1, 0);
-  ExpectAlloc(0x2000, 0x203f, st2, 0);
-  ExpectAlloc(0x2fc0, 0x2fff, st2, 0);
-  ExpectAlloc(0x3000, 0x33ff, st3, 0);
-}
-
-TEST_F(HeapProfilerTest, AllocAndFreeScattered) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  heap_profiler_alloc((void*)0x1000, 4096, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 4096, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x3000, 4096, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x4000, 4096, st1.frames, st1.depth, 0);
-
-  heap_profiler_free((void*)0x800, 4096, NULL);
-  EXPECT_EQ(4u, stats_.num_allocs);
-  EXPECT_EQ(2048u + 4096 + 4096 + 4096, stats_.total_alloc_bytes);
-
-  heap_profiler_free((void*)0x1800, 4096, NULL);
-  EXPECT_EQ(3u, stats_.num_allocs);
-  EXPECT_EQ(2048u + 4096 + 4096, stats_.total_alloc_bytes);
-
-  heap_profiler_free((void*)0x2800, 4096, NULL);
-  EXPECT_EQ(2u, stats_.num_allocs);
-  EXPECT_EQ(2048u + 4096, stats_.total_alloc_bytes);
-
-  heap_profiler_free((void*)0x3800, 4096, NULL);
-  EXPECT_EQ(1u, stats_.num_allocs);
-  EXPECT_EQ(2048u, stats_.total_alloc_bytes);
-
-  heap_profiler_free((void*)0x4800, 4096, NULL);
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, AllocAndOverFreeContiguous) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  heap_profiler_alloc((void*)0x1000, 4096, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 4096, st2.frames, st2.depth, 0);
-
-  heap_profiler_free((void*)0, 16834, NULL);
-
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, AllocContiguousAndPunchHole) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  heap_profiler_alloc((void*)0x1000, 4096, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 4096, st2.frames, st2.depth, 0);
-
-  // Punch a 4k hole in the middle of the two contiguous 4k allocs.
-  heap_profiler_free((void*)0x1800, 4096, NULL);
-
-  EXPECT_EQ(2u, stats_.num_allocs);
-  EXPECT_EQ(2u, stats_.num_stack_traces);
-  EXPECT_EQ(4096u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, AllocAndPartialFree) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(6, 0x1000);
-  StackTrace st3 = GenStackTrace(7, 0x2000);
-  StackTrace st4 = GenStackTrace(9, 0x3000);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  heap_profiler_alloc((void*)0x2000, 1024, st2.frames, st2.depth, 0);
-  heap_profiler_alloc((void*)0x3000, 1024, st3.frames, st3.depth, 0);
-  heap_profiler_alloc((void*)0x4000, 1024, st4.frames, st4.depth, 0);
-
-  heap_profiler_free((void*)0x1000, 128, NULL);  // Shrink left by 128B.
-  heap_profiler_free((void*)0x2380, 128, NULL);  // Shrink right by 128B.
-  heap_profiler_free((void*)0x3100, 512, NULL);  // 512B hole in the middle.
-  heap_profiler_free((void*)0x4000, 512, NULL);  // Free up the 4th alloc...
-  heap_profiler_free((void*)0x4200, 512, NULL);  // ...but do it in two halves.
-
-  EXPECT_EQ(4u, stats_.num_allocs);  // 1 + 2 + two sides around the hole 3.
-  EXPECT_EQ(3u, stats_.num_stack_traces);  // st4 should be gone.
-  EXPECT_EQ(896u + 896 + 512, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, RandomIndividualAllocAndFrees) {
-  static const size_t NUM_ST = 128;
-  static const size_t NUM_OPS = 1000;
-
-  StackTrace st[NUM_ST];
-  for (uint32_t i = 0; i < NUM_ST; ++i)
-    st[i] = GenStackTrace((i % 10) + 2, i * 128);
-
-  for (size_t i = 0; i < NUM_OPS; ++i) {
-    uintptr_t start = ((i + 7) << 8) & (0xffffff);
-    size_t size = (start >> 16) & 0x0fff;
-    if (i & 1) {
-      StackTrace* s = &st[start % NUM_ST];
-      heap_profiler_alloc((void*)start, size, s->frames, s->depth, 0);
-    } else {
-      heap_profiler_free((void*)start, size, NULL);
-    }
-    CheckAllocVsStacktaceConsistency();
-  }
-}
-
-TEST_F(HeapProfilerTest, RandomAllocAndFreesBatches) {
-  static const size_t NUM_ST = 128;
-  static const size_t NUM_ALLOCS = 100;
-
-  StackTrace st[NUM_ST];
-  for (size_t i = 0; i < NUM_ST; ++i)
-    st[i] = GenStackTrace((i % 10) + 2, i * NUM_ST);
-
-  for (int repeat = 0; repeat < 5; ++repeat) {
-    for (size_t i = 0; i < NUM_ALLOCS; ++i) {
-      StackTrace* s = &st[i % NUM_ST];
-      heap_profiler_alloc(
-          (void*)(i * 4096), ((i + 1) * 32) % 4097, s->frames, s->depth, 0);
-      CheckAllocVsStacktaceConsistency();
-    }
-
-    for (size_t i = 0; i < NUM_ALLOCS; ++i) {
-      heap_profiler_free((void*)(i * 1024), ((i + 1) * 64) % 16000, NULL);
-      CheckAllocVsStacktaceConsistency();
-    }
-  }
-}
-
-TEST_F(HeapProfilerTest, UnwindStackTooLargeShouldSaturate) {
-  StackTrace st1 = GenStackTrace(HEAP_PROFILER_MAX_DEPTH, 0x0);
-  uintptr_t many_frames[100] = {};
-  memcpy(many_frames, st1.frames, sizeof(uintptr_t) * st1.depth);
-  heap_profiler_alloc((void*)0x1000, 1024, many_frames, 100, 0);
-  ExpectAlloc(0x1000, 0x13ff, st1, 0);
-}
-
-TEST_F(HeapProfilerTest, NoUnwindShouldNotCrashButNoop) {
-  heap_profiler_alloc((void*)0x1000, 1024, NULL, 0, 0);
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, FreeNonExisting) {
-  StackTrace st1 = GenStackTrace(5, 0x0);
-  heap_profiler_free((void*)0x1000, 1024, NULL);
-  heap_profiler_free((void*)0x1400, 1024, NULL);
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-  heap_profiler_alloc((void*)0x1000, 1024, st1.frames, st1.depth, 0);
-  EXPECT_EQ(1u, stats_.num_allocs);
-  EXPECT_EQ(1024u, stats_.total_alloc_bytes);
-}
-
-TEST_F(HeapProfilerTest, FlagsConsistency) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  uint32_t flags = 0;
-  heap_profiler_alloc((void*)0x1000, 4096, st1.frames, st1.depth, 42);
-  heap_profiler_alloc((void*)0x2000, 4096, st1.frames, st1.depth, 142);
-
-  ExpectAlloc(0x1000, 0x1fff, st1, 42);
-  ExpectAlloc(0x2000, 0x2fff, st1, 142);
-
-  // Punch a 4k hole in the middle of the two contiguous 4k allocs.
-  heap_profiler_free((void*)0x1800, 4096, NULL);
-
-  ExpectAlloc(0x1000, 0x17ff, st1, 42);
-  heap_profiler_free((void*)0x1000, 2048, &flags);
-  EXPECT_EQ(42u, flags);
-
-  ExpectAlloc(0x2800, 0x2fff, st1, 142);
-  heap_profiler_free((void*)0x2800, 2048, &flags);
-  EXPECT_EQ(142u, flags);
-}
-
-TEST_F(HeapProfilerTest, BeConsistentOnOOM) {
-  static const size_t NUM_ALLOCS = 512 * 1024;
-  uintptr_t frames[1];
-
-  for (uintptr_t i = 0; i < NUM_ALLOCS; ++i) {
-    frames[0] = i;
-    heap_profiler_alloc((void*)(i * 32), 32, frames, 1, 0);
-  }
-
-  CheckAllocVsStacktaceConsistency();
-  // Check that we're saturating, otherwise this entire test is pointless.
-  EXPECT_LT(stats_.num_allocs, NUM_ALLOCS);
-  EXPECT_LT(stats_.num_stack_traces, NUM_ALLOCS);
-
-  for (uintptr_t i = 0; i < NUM_ALLOCS; ++i)
-    heap_profiler_free((void*)(i * 32), 32, NULL);
-
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-}
-
-#ifdef __LP64__
-TEST_F(HeapProfilerTest, Test64Bit) {
-  StackTrace st1 = GenStackTrace(8, 0x0);
-  StackTrace st2 = GenStackTrace(10, 0x7fffffff70000000L);
-  StackTrace st3 = GenStackTrace(10, 0xffffffff70000000L);
-  heap_profiler_alloc((void*)0x1000, 4096, st1.frames, st1.depth, 0);
-  heap_profiler_alloc(
-      (void*)0x7ffffffffffff000L, 4096, st2.frames, st2.depth, 0);
-  heap_profiler_alloc(
-      (void*)0xfffffffffffff000L, 4096, st3.frames, st3.depth, 0);
-  EXPECT_EQ(3u, stats_.num_allocs);
-  EXPECT_EQ(3u, stats_.num_stack_traces);
-  EXPECT_EQ(4096u + 4096 + 4096, stats_.total_alloc_bytes);
-
-  heap_profiler_free((void*)0x1000, 4096, NULL);
-  EXPECT_EQ(2u, stats_.num_allocs);
-  EXPECT_EQ(2u, stats_.num_stack_traces);
-  EXPECT_EQ(4096u + 4096, stats_.total_alloc_bytes);
-
-  heap_profiler_free((void*)0x7ffffffffffff000L, 4096, NULL);
-  EXPECT_EQ(1u, stats_.num_allocs);
-  EXPECT_EQ(1u, stats_.num_stack_traces);
-  EXPECT_EQ(4096u, stats_.total_alloc_bytes);
-
-  heap_profiler_free((void*)0xfffffffffffff000L, 4096, NULL);
-  EXPECT_EQ(0u, stats_.num_allocs);
-  EXPECT_EQ(0u, stats_.num_stack_traces);
-  EXPECT_EQ(0u, stats_.total_alloc_bytes);
-}
-#endif
-
-}  // namespace

commit d1897d77f4553df3bcd102736fae3bd564f226a8
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 29 04:39:40 2016 -0700

    sandwich: Filter-out about protocol requests
    
    Some website are just crazy enought to request about:blank, causing
    an assertion failure in Sandwich because of an unexpected protocol.
    This CL white list the about protocol.
    
    BUG=623966
    
    Review-Url: https://codereview.chromium.org/2108713002
    Cr-Original-Commit-Position: refs/heads/master@{#402780}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: dcc6dde78e2c75518cca6a775794e193ac5f4757

diff --git a/loading/request_track.py b/loading/request_track.py
index 04933a4..f14bc65 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -259,6 +259,7 @@ class Request(object):
     """
     assert self.HasReceivedResponse()
     assert not self.from_disk_cache and not self.served_from_cache
+    assert self.protocol != 'about'
     if self.failed:
       # TODO(gabadie): Once crbug.com/622018 is fixed, remove this branch.
       return 0
@@ -815,6 +816,8 @@ class RequestTrack(devtools_monitor.Track):
     assert (status == RequestTrack._STATUS_RESPONSE
             or status == RequestTrack._STATUS_DATA)
     r.encoded_data_length = params['encodedDataLength']
+    assert (r.encoded_data_length > 0 or r.protocol == 'about' or
+            r.from_disk_cache or r.served_from_cache)
     r.timing.loading_finished = r._TimestampOffsetFromStartMs(
         params['timestamp'])
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index d82b7fc..be8f609 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -116,6 +116,8 @@ def _FilterOutDataAndIncompleteRequests(requests):
     if request.protocol is None:
       assert not request.HasReceivedResponse()
       continue
+    if request.protocol == 'about':
+      continue
     if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
       raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
     yield request

commit ee32649a1c31c293cb62208b998e3dcdf92b4144
Author: yolandyan <yolandyan@google.com>
Date:   Tue Jun 28 20:30:23 2016 -0700

    Find annotated tests by exposing API in instrumentation_test_instance
    
    BUG=601909
    
    Review-Url: https://codereview.chromium.org/1851143002
    Cr-Original-Commit-Position: refs/heads/master@{#402699}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 97dc1912066a4312ffac05dbad24880897be80e9

diff --git a/find_annotated_tests.py b/find_annotated_tests.py
new file mode 100755
index 0000000..c6b9662
--- /dev/null
+++ b/find_annotated_tests.py
@@ -0,0 +1,181 @@
+#!/usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Finds all the annotated tests from proguard dump"""
+
+import argparse
+import datetime
+import json
+import linecache
+import logging
+import os
+import pprint
+import re
+import sys
+import time
+
+_SRC_DIR = os.path.abspath(os.path.join(
+      os.path.dirname(__file__), '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.utils import cmd_helper
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
+from pylib.instrumentation import instrumentation_test_instance
+
+
+_CRBUG_ID_PATTERN = re.compile(r'crbug(?:.com)?/(\d+)')
+_EXPORT_TIME_FORMAT = '%Y%m%dT%H%M%S'
+_GIT_LOG_TIME_PATTERN = re.compile(r'\d+')
+_GIT_LOG_MESSAGE_PATTERN = r'Cr-Commit-Position: refs/heads/master@{#(\d+)}'
+_GIT_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S'
+
+
+def _GetBugId(test_annotations):
+  """Find and return the test bug id from its annoation message elements"""
+  # TODO(yolandyan): currently the script only supports on bug id per method,
+  # add support for multiple bug id
+  for content in test_annotations.itervalues():
+    if content and content.get('message'):
+      search_result = re.search(_CRBUG_ID_PATTERN, content.get('message'))
+      if search_result is not None:
+        return int(search_result.group(1))
+  return None
+
+
+def _GetTests(test_apks, apk_output_dir):
+  """Return a list of all annotated tests and total test count"""
+  result = []
+  total_test_count = 0
+  for test_apk in test_apks:
+    logging.info('Current test apk: %s', test_apk)
+    test_jar = os.path.join(
+        apk_output_dir, constants.SDK_BUILD_TEST_JAVALIB_DIR,
+        '%s.jar' % test_apk)
+    all_tests = instrumentation_test_instance.GetAllTests(test_jar=test_jar)
+    for test_class in all_tests:
+      class_path = test_class['class']
+      class_name = test_class['class'].split('.')[-1]
+
+      class_annotations = test_class['annotations']
+      class_bug_id = _GetBugId(class_annotations)
+      for test_method in test_class['methods']:
+        total_test_count += 1
+        # getting annotation of each test case
+        test_annotations = test_method['annotations']
+        test_bug_id = _GetBugId(test_annotations)
+        test_bug_id = test_bug_id if test_bug_id else class_bug_id
+        test_annotations.update(class_annotations)
+        # getting test method name of each test
+        test_name = test_method['method']
+        test_dict = {
+            'bug_id': test_bug_id,
+            'annotations': test_annotations,
+            'test_name': test_name,
+            'test_apk_name': test_apk,
+            'class_name': class_name,
+            'class_path': class_path
+        }
+        result.append(test_dict)
+
+  logging.info('Total count of tests in all test apks: %d', total_test_count)
+  return result, total_test_count
+
+
+def _GetReportMeta(utc_script_runtime_string, total_test_count):
+  """Returns a dictionary of the report's metadata"""
+  revision = cmd_helper.GetCmdOutput(['git', 'rev-parse', 'HEAD']).strip()
+  raw_string = cmd_helper.GetCmdOutput(
+      ['git', 'log', '--pretty=format:%at', '--max-count=1', 'HEAD'])
+  time_string_search = re.search(_GIT_LOG_TIME_PATTERN, raw_string)
+  if time_string_search is None:
+    raise Exception('Timestamp format incorrect, expected all digits, got %s'
+                    % raw_string)
+
+  raw_string = cmd_helper.GetCmdOutput(
+      ['git', 'log', '--pretty=format:%b', '--max-count=1', 'HEAD'])
+  commit_pos_search = re.search(_GIT_LOG_MESSAGE_PATTERN, raw_string)
+  if commit_pos_search is None:
+    raise Exception('Cr-Commit-Position is not found, potentially running with '
+                    'uncommited HEAD')
+  commit_pos = int(commit_pos_search.group(1))
+
+  utc_revision_time = datetime.datetime.utcfromtimestamp(
+      int(time_string_search.group(0)))
+  utc_revision_time = utc_revision_time.strftime(_EXPORT_TIME_FORMAT)
+  logging.info(
+      'revision is %s, revision time is %s', revision, utc_revision_time)
+
+  return {
+      'revision': revision,
+      'commit_pos': commit_pos,
+      'script_runtime': utc_script_runtime_string,
+      'revision_time': utc_revision_time,
+      'platform': 'android',
+      'total_test_count': total_test_count
+  }
+
+
+def _GetReport(test_apks, script_runtime_string, apk_output_dir):
+  """Generate the dictionary of report data
+
+  Args:
+    test_apks: a list of apks for search for tests
+    script_runtime_string: the time when the script is run at
+                           format: '%Y%m%dT%H%M%S'
+  """
+
+  test_data, total_test_count = _GetTests(test_apks, apk_output_dir)
+  report_meta = _GetReportMeta(script_runtime_string, total_test_count)
+  report_data = {
+      'metadata': report_meta,
+      'tests': test_data
+  }
+  return report_data
+
+
+def main():
+  parser = argparse.ArgumentParser()
+  parser.add_argument('-t', '--test-apks', nargs='+', dest='test_apks',
+                      required=True,
+                      help='List all test apks file name that the script uses '
+                           'to fetch tracked tests from')
+  parser.add_argument('--json-output-dir', required=True,
+                      help='JSON file output dir')
+  parser.add_argument('--apk-output-dir', required=True,
+                      help='The output directory of test apks')
+  parser.add_argument('--timestamp-string',
+                      help='The time when this script is run, passed in by the '
+                           'recipe that runs this script so both the recipe '
+                           'and this script use it to format output json name')
+  parser.add_argument('-v', '--verbose', action='store_true', default=False,
+                      help='INFO verbosity')
+
+  arguments = parser.parse_args(sys.argv[1:])
+  logging.basicConfig(
+      level=logging.INFO if arguments.verbose else logging.WARNING)
+
+  if arguments.timestamp_string is None:
+    script_runtime = datetime.datetime.utcnow()
+    script_runtime_string = script_runtime.strftime(_EXPORT_TIME_FORMAT)
+  else:
+    script_runtime = arguments.timestamp_string
+  logging.info('Build time is %s', script_runtime_string)
+  apk_output_dir = os.path.abspath(os.path.join(
+      constants.DIR_SOURCE_ROOT, arguments.apk_output_dir))
+  report_data = _GetReport(
+      arguments.test_apks, script_runtime_string, apk_output_dir)
+
+  json_output_path = os.path.join(
+      arguments.json_output_dir,
+      '%s-android-chrome.json' % script_runtime_string)
+  with open(json_output_path, 'w') as f:
+    json.dump(report_data, f, sort_keys=True, separators=(',',': '))
+    logging.info('Saved json output file to %s', json_output_path)
+
+
+if __name__ == '__main__':
+  sys.exit(main())

commit 0d753b40937a388b460fa2939dfdb30d1a3caf20
Author: gabadie <gabadie@chromium.org>
Date:   Tue Jun 28 10:34:16 2016 -0700

    sandwich: Properly detect page load failures
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2102053002
    Cr-Original-Commit-Position: refs/heads/master@{#402499}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5ca99b2df9e139cc5a67648462b9610183025fdb

diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 640b068..5c3bb94 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -47,6 +47,7 @@ _TRACING_CATEGORIES = [
   'blink.user_timing',
   'devtools.timeline',
   'java',
+  'navigation',
   'toplevel',
   'v8',
   '-cc',  # A lot of unnecessary events are enabled by default in "cc".
@@ -79,6 +80,10 @@ class CacheOperation(object):
   CLEAR, SAVE, PUSH = range(3)
 
 
+class SandwichRunnerError(Exception):
+  pass
+
+
 class SandwichRunner(object):
   """Sandwich runner.
 
@@ -209,6 +214,12 @@ class SandwichRunner(object):
             trace = RecordTrace()
         else:
           trace = RecordTrace()
+        for event in trace.request_track.GetEvents():
+          if event.failed:
+            logging.warning(
+                'request to %s failed: %s', event.url, event.error_text)
+        if not trace.tracing_track.HasLoadingSucceeded():
+          raise SandwichRunnerError('Page load has failed.')
     if run_path is not None:
       trace_path = os.path.join(run_path, TRACE_FILENAME)
       trace.ToJsonFile(trace_path)

commit 73e7842cd78d0b55f24ed5e387ee0f2ba3bbe739
Author: gabadie <gabadie@chromium.org>
Date:   Tue Jun 28 08:48:49 2016 -0700

    tools/android/loading: Properly detect page loading failures
    
    Before, one page failures, Blink sided generated response were
    triggering assertion in request_track.RequestTrack.
    
    This CL fixes this issue by removing the asserts and instead
    implements loading_trace.LoadingTrace.HasLoadingSucceed()
    that is based on navigation trace events.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2103673003
    Cr-Original-Commit-Position: refs/heads/master@{#402481}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c44e7bf15904d461676839524c3a850dc6d05da7

diff --git a/loading/request_track.py b/loading/request_track.py
index 744f973..04933a4 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -175,6 +175,7 @@ class Request(object):
                  chunks received, with their offset in ms relative to
                  Timing.requestTime.
     failed: (bool) Whether the request failed.
+    error_text: (str) User friendly error message when request failed.
     start_msec: (float) Request start time, in milliseconds from chrome start.
     end_msec: (float) Request end time, in milliseconds from chrome start.
       start_msec.
@@ -211,6 +212,7 @@ class Request(object):
     self.encoded_data_length = 0
     self.data_chunks = []
     self.failed = False
+    self.error_text = None
 
   @property
   def start_msec(self):
@@ -749,7 +751,8 @@ class RequestTrack(devtools_monitor.Track):
     return initiator
 
   def _RequestServedFromCache(self, request_id, _):
-    assert request_id in self._requests_in_flight
+    if request_id not in self._requests_in_flight:
+      return
     (request, status) = self._requests_in_flight[request_id]
     assert status == RequestTrack._STATUS_SENT
     request.served_from_cache = True
@@ -796,6 +799,8 @@ class RequestTrack(devtools_monitor.Track):
     self._request_id_to_response_received[request_id] = params
 
   def _DataReceived(self, request_id, params):
+    if request_id not in self._requests_in_flight:
+      return
     (r, status) = self._requests_in_flight[request_id]
     assert (status == RequestTrack._STATUS_RESPONSE
             or status == RequestTrack._STATUS_DATA)
@@ -804,7 +809,8 @@ class RequestTrack(devtools_monitor.Track):
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_DATA)
 
   def _LoadingFinished(self, request_id, params):
-    assert request_id in self._requests_in_flight
+    if request_id not in self._requests_in_flight:
+      return
     (r, status) = self._requests_in_flight[request_id]
     assert (status == RequestTrack._STATUS_RESPONSE
             or status == RequestTrack._STATUS_DATA)
@@ -814,17 +820,19 @@ class RequestTrack(devtools_monitor.Track):
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
     self._FinalizeRequest(request_id)
 
-  def _LoadingFailed(self, request_id, _):
+  def _LoadingFailed(self, request_id, params):
     if request_id not in self._requests_in_flight:
       logging.warning('An unknown request failed: %s' % request_id)
       return
     (r, _) = self._requests_in_flight[request_id]
     r.failed = True
+    r.error_text = params['errorText']
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
     self._FinalizeRequest(request_id)
 
   def _FinalizeRequest(self, request_id):
-    assert request_id in self._requests_in_flight
+    if request_id not in self._requests_in_flight:
+      return
     (request, status) = self._requests_in_flight[request_id]
     assert status == RequestTrack._STATUS_FINISHED
     del self._requests_in_flight[request_id]
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index e96fbf2..64d3e77 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -400,11 +400,24 @@ class RequestTrackTestCase(unittest.TestCase):
           "encodedDataLength": 32768,
           "requestId": "32493.1",
           "timestamp": 5571441.893121}}
+  _SERVED_FROM_CACHE = {
+      "method": "Network.requestServedFromCache",
+      "params": {
+          "requestId": "32493.1"}}
   _LOADING_FINISHED = {'method': 'Network.loadingFinished',
                        'params': {
                            'encodedDataLength': 101829,
                            'requestId': '32493.1',
                            'timestamp': 5571441.891189}}
+  _LOADING_FAILED = {'method': 'Network.loadingFailed',
+                     'params': {
+                         'canceled': False,
+                         'blockedReason': None,
+                         'encodedDataLength': 101829,
+                         'errorText': 'net::ERR_TOO_MANY_REDIRECTS',
+                         'requestId': '32493.1',
+                         'timestamp': 5571441.891189,
+                         'type': 'Document'}}
 
   def setUp(self):
     self.request_track = RequestTrack(None)
@@ -479,6 +492,13 @@ class RequestTrackTestCase(unittest.TestCase):
     with self.assertRaises(AssertionError):
       self.request_track.Handle('Network.requestWillBeSent', msg)
 
+  def testSequenceOfGeneratedResponse(self):
+    self.request_track.Handle('Network.requestServedFromCache',
+                              RequestTrackTestCase._SERVED_FROM_CACHE)
+    self.request_track.Handle('Network.loadingFinished',
+                              RequestTrackTestCase._LOADING_FINISHED)
+    self.assertEquals(0, len(self.request_track.GetEvents()))
+
   def testInvalidSequence(self):
     msg1 = RequestTrackTestCase._REQUEST_WILL_BE_SENT
     msg2 = RequestTrackTestCase._LOADING_FINISHED
@@ -549,6 +569,17 @@ class RequestTrackTestCase(unittest.TestCase):
     with self.assertRaises(AssertionError):
       self.request_track.Handle('Network.responseReceived', msg2_different)
 
+  def testLoadingFailed(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    self.request_track.Handle('Network.responseReceived',
+                              RequestTrackTestCase._RESPONSE_RECEIVED)
+    self.request_track.Handle('Network.loadingFailed',
+                              RequestTrackTestCase._LOADING_FAILED)
+    r = self.request_track.GetEvents()[0]
+    self.assertTrue(r.failed)
+    self.assertEquals('net::ERR_TOO_MANY_REDIRECTS', r.error_text)
+
   def testCanSerialize(self):
     self._ValidSequence(self.request_track)
     json_dict = self.request_track.ToJsonDict()
diff --git a/loading/tracing.py b/loading/tracing.py
index 3bf5246..f69392d 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -86,6 +86,13 @@ class TracingTrack(devtools_monitor.Track):
     return [e for e in matching_events
         if 'frame' in e.args and e.args['frame'] == self.GetMainFrameID()]
 
+  def GetMainFrameRoutingID(self):
+    """Returns the main frame routing ID."""
+    for event in self.GetMatchingEvents(
+        'navigation', 'RenderFrameImpl::OnNavigate'):
+      return event.args['id']
+    assert False
+
   def GetMainFrameID(self):
     """Returns the main frame ID."""
     if not self._main_frame_id:
@@ -225,6 +232,19 @@ class TracingTrack(devtools_monitor.Track):
     self._IndexEvents()
     return self._interval_tree.GetEvents()
 
+  def HasLoadingSucceeded(self):
+    """Returns whether the loading has succeed at recording time."""
+    main_frame_id = self.GetMainFrameRoutingID()
+    for event in self.GetMatchingEvents(
+        'navigation', 'RenderFrameImpl::didFailProvisionalLoad'):
+      if event.args['id'] == main_frame_id:
+        return False
+    for event in self.GetMatchingEvents(
+        'navigation', 'RenderFrameImpl::didFailLoad'):
+      if event.args['id'] == main_frame_id:
+        return False
+    return True
+
   class _SpanningEvents(object):
     def __init__(self):
       self._duration_stack = []
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index f621c1f..6b6cd6c 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -360,6 +360,51 @@ class TracingTrackTestCase(unittest.TestCase):
     self.assertSetEqual(
         set('A'), self.track.Filter(categories=set('A')).Categories())
 
+  def testHasLoadingSucceeded(self):
+    cat = 'navigation'
+    on_navigate = 'RenderFrameImpl::OnNavigate'
+    fail_provisional = 'RenderFrameImpl::didFailProvisionalLoad'
+    fail_load = 'RenderFrameImpl::didFailLoad'
+
+    track = TracingTrack.FromJsonDict({'categories': [cat], 'events': []})
+    with self.assertRaises(AssertionError):
+      track.HasLoadingSucceeded()
+
+    track = TracingTrack.FromJsonDict({'categories': [cat], 'events': [
+        {'cat': cat, 'name': on_navigate, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1}]})
+    self.assertTrue(track.HasLoadingSucceeded())
+
+    track = TracingTrack.FromJsonDict({'categories': [cat], 'events': [
+        {'cat': cat, 'name': on_navigate, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1},
+        {'cat': cat, 'name': on_navigate, 'args': {'id': 2},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1},
+        {'cat': cat, 'name': fail_provisional, 'args': {'id': 2},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1}]})
+    self.assertTrue(track.HasLoadingSucceeded())
+
+    track = TracingTrack.FromJsonDict({'categories': [cat], 'events': [
+        {'cat': cat, 'name': on_navigate, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1},
+        {'cat': cat, 'name': fail_provisional, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1}]})
+    self.assertFalse(track.HasLoadingSucceeded())
+
+    track = TracingTrack.FromJsonDict({'categories': [cat], 'events': [
+        {'cat': cat, 'name': on_navigate, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1},
+        {'cat': cat, 'name': fail_load, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1}]})
+    self.assertFalse(track.HasLoadingSucceeded())
+
+    track = TracingTrack.FromJsonDict({'categories': [cat], 'events': [
+        {'cat': cat, 'name': on_navigate, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1},
+        {'cat': cat, 'name': fail_load, 'args': {'id': 1},
+            'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 1, 'tid': 1}]})
+    self.assertFalse(track.HasLoadingSucceeded())
+
   def _HandleEvents(self, events):
     self.track.Handle('Tracing.dataCollected', {'params': {'value': [
         self.EventToMicroseconds(e) for e in events]}})

commit 7efeb10d5a344b9f33e1712abfa454f60801bb1a
Author: gabadie <gabadie@chromium.org>
Date:   Tue Jun 28 03:39:23 2016 -0700

    tools/android/loading: Remove static tasks from task_manager.
    
    Before, Sandwich was using static task to populate WPR archive
    task with command line given path of the WPR archive to use. However
    this command line flag has been removed. As a result no code use
    the static tasks anymore. This CL removes them.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2100763002
    Cr-Original-Commit-Position: refs/heads/master@{#402440}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5128502ba2961041d370b925f840097b57b50cda

diff --git a/loading/task_manager.py b/loading/task_manager.py
index 1e5ca48..0ae1f7a 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -6,38 +6,30 @@
 
 A Task consists of a 'recipe' (a closure to be executed) and a list of refs to
 tasks that should be executed prior to executing this Task (i.e. dependencies).
+The responsibility of the recipe of a task is to produce the file with the name
+assigned at task creation.
 
-A Task can be either 'static' or 'dynamic'. A static tasks only represents an
-existing file on the filesystem, its recipe is a no-op. The responsibility of
-the recipe of a dynamic task is to produce the file with the name assigned at
-task creation.
-
-A scenario is a ordered list of dynamic tasks to execute such that the
-dependencies of a given task are execute before the said task. The scenario is
-built from a list of final tasks and a list of frozen tasks:
+A scenario is a ordered list of tasks to execute such that the dependencies of a
+given task are execute before the said task. The scenario is built from a list
+of final tasks and a list of frozen tasks:
   - A final task is a task to execute ultimately. Therefore the scenario is
     composed of final tasks and their required intermediary tasks.
-  - A frozen task is dynamic task to not execute. This is a mechanism to morph a
-    dynamic task that may have dependencies to a static task with no dependency
-    at scenario generation time, injecting what the dynamic task have already
-    produced before as an input of the smaller tasks dependency graph covered
-    by the scenario.
+  - A frozen task is task to not execute. This is a mechanism to morph a task
+    that may have dependencies to a task with no dependency at scenario
+    generation time, injecting what the task have already produced before as an
+    input of the smaller tasks dependency graph covered by the scenario.
 
 Example:
   # -------------------------------------------------- Build my dependency graph
   builder = Builder('my/output/dir')
-  input0 = builder.CreateStaticTask('input0', 'path/to/input/file0')
-  input1 = builder.CreateStaticTask('input1', 'path/to/input/file1')
-  input2 = builder.CreateStaticTask('input2', 'path/to/input/file2')
-  input3 = builder.CreateStaticTask('input3', 'path/to/input/file3')
 
-  @builder.RegisterTask('out0', dependencies=[input0, input2])
+  @builder.RegisterTask('out0')
   def BuildOut0():
-    DoStuff(input0.path, input2.path, out=BuildOut0.path)
+    Produce(out=BuildOut0.path)
 
-  @builder.RegisterTask('out1', dependencies=[input1, input3])
+  @builder.RegisterTask('out1')
   def BuildOut1():
-    DoStuff(input1.path, input3.path, out=BuildOut1.path)
+    Produce(out=BuildOut1.path)
 
   @builder.RegisterTask('out2', dependencies=[BuildOut0, BuildOut1])
   def BuildOut2():
@@ -85,7 +77,7 @@ class TaskError(Exception):
 
 
 class Task(object):
-  """Task that can be either a static task or dynamic with a recipe."""
+  """Task with a recipe."""
 
   def __init__(self, name, path, dependencies, recipe):
     """Constructor.
@@ -94,7 +86,7 @@ class Task(object):
       name: The name of the  task.
       path: Path to the file or directory that this task produces.
       dependencies: List of parent task to execute before.
-      recipe: Function to execute if a dynamic task or None if a static task.
+      recipe: Function to execute.
     """
     self.name = name
     self.path = path
@@ -104,16 +96,10 @@ class Task(object):
 
   def Execute(self):
     """Executes this task."""
-    if self.IsStatic():
-      raise TaskError('Task {} is static.'.format(self.name))
     if not self._is_done:
       self._recipe()
     self._is_done = True
 
-  def IsStatic(self):
-    """Returns whether this task is a static task."""
-    return self._recipe == None
-
 
 class Builder(object):
   """Utilities for creating sub-graphs of tasks with dependencies."""
@@ -122,31 +108,18 @@ class Builder(object):
     """Constructor.
 
     Args:
-      output_directory: Output directory where the dynamic tasks work.
+      output_directory: Output directory where the tasks work.
       output_subdirectory: Subdirectory to put all created tasks in or None.
     """
     self.output_directory = output_directory
     self.output_subdirectory = output_subdirectory
     self._tasks = {}
 
-  def CreateStaticTask(self, task_name, path):
-    """Creates and returns a new static task."""
-    task_name = self._RebaseTaskName(task_name)
-    if not os.path.exists(path):
-      raise TaskError('Error while creating task {}: File not found: {}'.format(
-          task_name, path))
-    if task_name in self._tasks:
-      raise TaskError('Task {} already exists.'.format(task_name))
-    task = Task(task_name, path, [], None)
-    self._tasks[task_name] = task
-    return task
-
   # Caution:
-  #   This decorator may not create a dynamic task in the case where
-  #   merge=True and another dynamic target having the same name have already
-  #   been created. In this case, it will just reuse the former task. This is at
-  #   the user responsibility to ensure that merged tasks would do the exact
-  #   same thing.
+  #   This decorator may not create a task in the case where merge=True and
+  #   another task having the same name have already been created. In this case,
+  #   it will just reuse the former task. This is at the user responsibility to
+  #   ensure that merged tasks would do the exact same thing.
   #
   #     @builder.RegisterTask('hello')
   #     def TaskA():
@@ -160,7 +133,7 @@ class Builder(object):
   #     assert TaskA == TaskB
   #     TaskB.Execute() # Sets set my_object.a == 1
   def RegisterTask(self, task_name, dependencies=None, merge=False):
-    """Decorator that wraps a function into a dynamic task.
+    """Decorator that wraps a function into a task.
 
     Args:
       task_name: The name of this new task to register.
@@ -179,9 +152,6 @@ class Builder(object):
         if not merge:
           raise TaskError('Task {} already exists.'.format(rebased_task_name))
         task = self._tasks[rebased_task_name]
-        if task.IsStatic():
-          raise TaskError('Should not merge dynamic task {} with the already '
-                          'existing static one.'.format(rebased_task_name))
         return task
       task_path = self.RebaseOutputPath(task_name)
       task = Task(rebased_task_name, task_path, dependencies, recipe)
@@ -213,8 +183,6 @@ def GenerateScenario(final_tasks, frozen_tasks):
   scenario = []
   task_paths = {}
   def InternalAppendTarget(task):
-    if task.IsStatic():
-      return
     if task in frozen_tasks:
       if not os.path.exists(task.path):
         raise TaskError('Frozen target `{}`\'s path doesn\'t exist.'.format(
@@ -276,7 +244,7 @@ def ListResumingTasksToFreeze(scenario, final_tasks, skipped_tasks):
   walked_tasks = set()
 
   def InternalWalk(task):
-    if task.IsStatic() or task in walked_tasks:
+    if task in walked_tasks:
       return
     walked_tasks.add(task)
     if task not in scenario_tasks or task not in skipped_tasks:
@@ -301,10 +269,9 @@ def OutputGraphViz(scenario, final_tasks, output):
     output: A file-like output stream to receive the dot file.
 
   Graph interpretations:
-    - Static tasks are shape less.
     - Final tasks (the one that where directly appended) are box shaped.
-    - Non final dynamic tasks are ellipse shaped.
-    - Frozen dynamic tasks have a blue shape.
+    - Non final tasks are ellipse shaped.
+    - Frozen tasks have a blue shape.
   """
   task_execution_ids = {t: i for i, t in enumerate(scenario)}
   tasks_node_ids = dict()
@@ -316,9 +283,7 @@ def OutputGraphViz(scenario, final_tasks, output):
     node_label = task.name
     node_color = 'blue'
     node_shape = 'ellipse'
-    if task.IsStatic():
-      node_shape = 'plaintext'
-    elif task in task_execution_ids:
+    if task in task_execution_ids:
       node_color = 'black'
       node_label = str(task_execution_ids[task]) + ': ' + node_label
     if task in final_tasks:
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index bdcb809..f6e9788 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -19,13 +19,13 @@ import task_manager
 
 _GOLDEN_GRAPHVIZ = """digraph graphname {
   n0 [label="0: b", color=black, shape=ellipse];
-  n1 [label="1: c", color=black, shape=ellipse];
-  n0 -> n1;
-  n2 [label="a", color=blue, shape=plaintext];
-  n2 -> n1;
-  n3 [label="2: d", color=black, shape=ellipse];
-  n1 -> n3;
-  n4 [label="3: f", color=black, shape=box];
+  n1 [label="1: a", color=black, shape=ellipse];
+  n2 [label="2: c", color=black, shape=ellipse];
+  n0 -> n2;
+  n1 -> n2;
+  n3 [label="3: d", color=black, shape=ellipse];
+  n2 -> n3;
+  n4 [label="4: f", color=black, shape=box];
   n3 -> n4;
   n5 [label="e", color=blue, shape=ellipse];
   n5 -> n4;
@@ -68,19 +68,11 @@ class TaskManagerTestCase(unittest.TestCase):
 
 
 class TaskTest(TaskManagerTestCase):
-  def testStaticTask(self):
-    task = task_manager.Task('hello.json', 'what/ever/hello.json', [], None)
-    self.assertTrue(task.IsStatic())
-    self.assertTrue(task._is_done)
-    with self.assertRaises(task_manager.TaskError):
-      task.Execute()
-
-  def testDynamicTask(self):
+  def testTaskExecution(self):
     def Recipe():
       Recipe.counter += 1
     Recipe.counter = 0
     task = task_manager.Task('hello.json', 'what/ever/hello.json', [], Recipe)
-    self.assertFalse(task.IsStatic())
     self.assertFalse(task._is_done)
     self.assertEqual(0, Recipe.counter)
     task.Execute()
@@ -88,7 +80,7 @@ class TaskTest(TaskManagerTestCase):
     task.Execute()
     self.assertEqual(1, Recipe.counter)
 
-  def testDynamicTaskWithUnexecutedDeps(self):
+  def testTaskExecutionWithUnexecutedDeps(self):
     def RecipeA():
       self.fail()
 
@@ -104,29 +96,12 @@ class TaskTest(TaskManagerTestCase):
 
 
 class BuilderTest(TaskManagerTestCase):
-  def testCreateUnexistingStaticTask(self):
-    builder = task_manager.Builder(self.output_directory, None)
-    with self.assertRaises(task_manager.TaskError):
-      builder.CreateStaticTask('hello.txt', '/__unexisting/file/path')
-
-  def testCreateStaticTask(self):
-    builder = task_manager.Builder(self.output_directory, None)
-    task = builder.CreateStaticTask('hello.py', __file__)
-    self.assertTrue(task.IsStatic())
-
-  def testDuplicateStaticTask(self):
-    builder = task_manager.Builder(self.output_directory, None)
-    builder.CreateStaticTask('hello.py', __file__)
-    with self.assertRaises(task_manager.TaskError):
-      builder.CreateStaticTask('hello.py', __file__)
-
   def testRegisterTask(self):
     builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('hello.txt')
     def TaskA():
       TaskA.executed = True
     TaskA.executed = False
-    self.assertFalse(TaskA.IsStatic())
     self.assertEqual(os.path.join(self.output_directory, 'hello.txt'),
                      TaskA.path)
     self.assertFalse(TaskA.executed)
@@ -155,37 +130,24 @@ class BuilderTest(TaskManagerTestCase):
       pass
     self.assertEqual(TaskA, TaskB)
 
-  def testStaticTaskMergingError(self):
-    builder = task_manager.Builder(self.output_directory, None)
-    builder.CreateStaticTask('hello.py', __file__)
-    with self.assertRaises(task_manager.TaskError):
-      @builder.RegisterTask('hello.py', merge=True)
-      def TaskA():
-        pass
-      del TaskA # unused
-
   def testOutputSubdirectory(self):
     builder = task_manager.Builder(self.output_directory, 'subdir')
 
-    builder.CreateStaticTask('hello.py', __file__)
-    self.assertIn('subdir/hello.py', builder._tasks)
-    self.assertNotIn('hello.py', builder._tasks)
-
-    builder.CreateStaticTask('subdir/hello.py', __file__)
-    self.assertIn('subdir/subdir/hello.py', builder._tasks)
-
     @builder.RegisterTask('world.txt')
     def TaskA():
       pass
     del TaskA # unused
+
     self.assertIn('subdir/world.txt', builder._tasks)
-    self.assertNotIn('hello.py', builder._tasks)
+    self.assertNotIn('subdir/subdir/world.txt', builder._tasks)
+    self.assertNotIn('world.txt', builder._tasks)
 
     @builder.RegisterTask('subdir/world.txt')
     def TaskB():
       pass
     del TaskB # unused
     self.assertIn('subdir/subdir/world.txt', builder._tasks)
+    self.assertNotIn('world.txt', builder._tasks)
 
 
 class GenerateScenarioTest(TaskManagerTestCase):
@@ -298,11 +260,13 @@ class GenerateScenarioTest(TaskManagerTestCase):
 
   def testGraphVizOutput(self):
     builder = task_manager.Builder(self.output_directory, None)
-    static_task = builder.CreateStaticTask('a', __file__)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
     @builder.RegisterTask('b')
     def TaskB():
       pass
-    @builder.RegisterTask('c', dependencies=[TaskB, static_task])
+    @builder.RegisterTask('c', dependencies=[TaskB, TaskA])
     def TaskC():
       pass
     @builder.RegisterTask('d', dependencies=[TaskC])
@@ -323,11 +287,10 @@ class GenerateScenarioTest(TaskManagerTestCase):
   def testListResumingTasksToFreeze(self):
     TaskManagerTestCase.setUp(self)
     builder = task_manager.Builder(self.output_directory, None)
-    static_task = builder.CreateStaticTask('static', __file__)
     @builder.RegisterTask('a')
     def TaskA():
       pass
-    @builder.RegisterTask('b', dependencies=[static_task])
+    @builder.RegisterTask('b')
     def TaskB():
       pass
     @builder.RegisterTask('c', dependencies=[TaskA, TaskB])

commit 69a1c52a94937a465ee6229265f37e84b2444f2f
Author: gabadie <gabadie@chromium.org>
Date:   Tue Jun 28 01:40:33 2016 -0700

    sandwich: Implement HTMLPreloadScanner subresource discoverer respecting no-store
    
    To study if it is worth storing resources having Cache-Control: No-Store,
    this CL implement a new subresource discoverer that prune down resource
    that originally had this response header.
    
    To achieve that, this CL changes the WPR patching task to also output
    a json file having original response header per resource.
    
    This CL also take the oportunity to move subresource discoverer enum
    consts in a sandwich_prefetch.Discoverer.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2092433002
    Cr-Original-Commit-Position: refs/heads/master@{#402426}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b1b1fc14fbf8e3dc00998dbea1e34cb0c3cce6db

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 40a0f78..7cb0834 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -140,17 +140,17 @@ def _SetupNoStatePrefetchBenchmark(args):
     'network_conditions': ['Regular4G', 'Regular3G', 'Regular2G'],
     'subresource_discoverers': [
         e for e in sandwich_prefetch.SUBRESOURCE_DISCOVERERS
-            if e != sandwich_prefetch.FULL_CACHE_DISCOVERER]
+            if e != sandwich_prefetch.Discoverer.FullCache]
   }
 
 
 def _GenerateNoStatePrefetchBenchmarkTasks(
     common_builder, main_transformer, benchmark_setup):
   builder = sandwich_prefetch.PrefetchBenchmarkBuilder(common_builder)
-  builder.PopulateLoadBenchmark(sandwich_prefetch.EMPTY_CACHE_DISCOVERER,
+  builder.PopulateLoadBenchmark(sandwich_prefetch.Discoverer.EmptyCache,
                                 _MAIN_TRANSFORMER_LIST_NAME,
                                 transformer_list=[main_transformer])
-  builder.PopulateLoadBenchmark(sandwich_prefetch.FULL_CACHE_DISCOVERER,
+  builder.PopulateLoadBenchmark(sandwich_prefetch.Discoverer.FullCache,
                                 _MAIN_TRANSFORMER_LIST_NAME,
                                 transformer_list=[main_transformer])
   for network_condition in benchmark_setup['network_conditions']:
diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index 8429305..d82b7fc 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -34,54 +34,59 @@ import task_manager
 import wpr_backend
 
 
-# Do not prefetch anything.
-EMPTY_CACHE_DISCOVERER = 'empty-cache'
+class Discoverer(object):
+  # Do not prefetch anything.
+  EmptyCache = 'empty-cache'
 
-# Prefetches everything to load fully from cache (impossible in practice).
-FULL_CACHE_DISCOVERER = 'full-cache'
+  # Prefetches everything to load fully from cache (impossible in practice).
+  FullCache = 'full-cache'
 
-# Prefetches the first resource following the redirection chain.
-REDIRECTED_MAIN_DISCOVERER = 'redirected-main'
+  # Prefetches the first resource following the redirection chain.
+  MainDocument = 'main-document'
 
-# All resources which are fetched from the main document and their redirections.
-PARSER_DISCOVERER = 'parser'
+  # All resources which are fetched from the main document and their
+  # redirections.
+  Parser = 'parser'
 
-# Simulation of HTMLPreloadScanner on the main document and their redirections.
-HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner'
+  # Simulation of HTMLPreloadScanner on the main document and their
+  # redirections and subsets:
+  #   Store: only resources that don't have Cache-Control: No-Store.
+  HTMLPreloadScanner = 'html-scanner'
+  HTMLPreloadScannerStore = 'html-scanner-store'
 
+
+# List of all available sub-resource discoverers.
 SUBRESOURCE_DISCOVERERS = set([
-  EMPTY_CACHE_DISCOVERER,
-  FULL_CACHE_DISCOVERER,
-  REDIRECTED_MAIN_DISCOVERER,
-  PARSER_DISCOVERER,
-  HTML_PRELOAD_SCANNER_DISCOVERER
+  Discoverer.EmptyCache,
+  Discoverer.FullCache,
+  Discoverer.MainDocument,
+  Discoverer.Parser,
+  Discoverer.HTMLPreloadScanner,
+  Discoverer.HTMLPreloadScannerStore,
 ])
 
+
 _UPLOAD_DATA_STREAM_REQUESTS_REGEX = re.compile(r'^\d+/(?P<url>.*)$')
 
 
-def _PatchWpr(wpr_archive_path):
+def _PatchWpr(wpr_archive):
   """Patches a WPR archive to get all resources into the HTTP cache and avoid
   invalidation and revalidations.
 
   Args:
-    wpr_archive_path: Path of the WPR archive to patch.
+    wpr_archive: wpr_backend.WprArchiveBackend WPR archive to patch.
   """
   # Sets the resources cache max-age to 10 years.
   MAX_AGE = 10 * 365 * 24 * 60 * 60
   CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
 
-  wpr_archive = wpr_backend.WprArchiveBackend(wpr_archive_path)
+  logging.info('number of entries: %d', len(wpr_archive.ListUrlEntries()))
+  patched_entry_count = 0
   for url_entry in wpr_archive.ListUrlEntries():
     response_headers = url_entry.GetResponseHeadersDict()
     if 'cache-control' in response_headers and \
         response_headers['cache-control'] == CACHE_CONTROL:
       continue
-    logging.info('patching %s' % url_entry.url)
-    # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
-    # TODO(gabadie): may need to delete ETag.
-    # TODO(gabadie): may need to take care of x-cache.
-    #
     # Override the cache-control header to set the resources max age to MAX_AGE.
     #
     # Important note: Some resources holding sensitive information might have
@@ -97,14 +102,10 @@ def _PatchWpr(wpr_archive_path):
     # All of these Vary and Pragma possibilities need to be removed from
     # response headers in order for Chrome to store a resource in HTTP cache and
     # not to invalidate it.
-    #
-    # Note: HttpVaryData::Init() in Chrome adds an implicit 'Vary: cookie'
-    # header to any redirect.
-    # TODO(gabadie): Find a way to work around this issue.
     url_entry.RemoveResponseHeaderDirectives('vary', {'*', 'cookie'})
     url_entry.RemoveResponseHeaderDirectives('pragma', {'no-cache'})
-
-  wpr_archive.Persist()
+    patched_entry_count += 1
+  logging.info('number of entries patched: %d', patched_entry_count)
 
 
 def _FilterOutDataAndIncompleteRequests(requests):
@@ -170,12 +171,50 @@ def _PatchCacheArchive(cache_archive_path, loading_trace_path,
     logging.info('Patched cache size: %d bytes' % cache_backend.GetSize())
 
 
-def _ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
+def _DiscoverRequests(dependencies_lens, subresource_discoverer):
+  trace = dependencies_lens.loading_trace
+  first_resource_request = trace.request_track.GetFirstResourceRequest()
+
+  if subresource_discoverer == Discoverer.EmptyCache:
+    requests = []
+  elif subresource_discoverer == Discoverer.FullCache:
+    requests = dependencies_lens.loading_trace.request_track.GetEvents()
+  elif subresource_discoverer == Discoverer.MainDocument:
+    requests = [dependencies_lens.GetRedirectChain(first_resource_request)[-1]]
+  elif subresource_discoverer == Discoverer.Parser:
+    requests = PrefetchSimulationView.ParserDiscoverableRequests(
+        first_resource_request, dependencies_lens)
+  elif subresource_discoverer == Discoverer.HTMLPreloadScanner:
+    requests = PrefetchSimulationView.PreloadedRequests(
+        first_resource_request, dependencies_lens, trace)
+  else:
+    assert False
+  logging.info('number of requests discovered by %s: %d',
+      subresource_discoverer, len(requests))
+  return requests
+
+
+def _PruneOutOriginalNoStoreRequests(original_headers_path, requests):
+  with open(original_headers_path) as file_input:
+    original_headers = json.load(file_input)
+  pruned_requests = set()
+  for request in requests:
+    request_original_headers = original_headers[request.url]
+    if ('cache-control' in request_original_headers and
+        'no-store' in request_original_headers['cache-control'].lower()):
+      pruned_requests.add(request)
+  return [r for r in requests if r not in pruned_requests]
+
+
+def _ExtractDiscoverableUrls(
+    original_headers_path, loading_trace_path, subresource_discoverer):
   """Extracts discoverable resource urls from a loading trace according to a
   sub-resource discoverer.
 
   Args:
-    loading_trace_path: The loading trace's path.
+    original_headers_path: Path of JSON containing the original headers.
+    loading_trace_path: Path of the loading trace recorded at original cache
+      creation.
     subresource_discoverer: The sub-resources discoverer that should white-list
       the resources to keep in cache for the NoState-Prefetch benchmarks.
 
@@ -184,30 +223,20 @@ def _ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   """
   assert subresource_discoverer in SUBRESOURCE_DISCOVERERS, \
       'unknown prefetch simulation {}'.format(subresource_discoverer)
-
-  # Load trace and related infos.
-  logging.info('loading %s' % loading_trace_path)
+  logging.info('loading %s', loading_trace_path)
   trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
   dependencies_lens = RequestDependencyLens(trace)
-  first_resource_request = trace.request_track.GetFirstResourceRequest()
 
   # Build the list of discovered requests according to the desired simulation.
   discovered_requests = []
-  if subresource_discoverer == EMPTY_CACHE_DISCOVERER:
-    pass
-  elif subresource_discoverer == FULL_CACHE_DISCOVERER:
-    discovered_requests = trace.request_track.GetEvents()
-  elif subresource_discoverer == REDIRECTED_MAIN_DISCOVERER:
-    discovered_requests = \
-        [dependencies_lens.GetRedirectChain(first_resource_request)[-1]]
-  elif subresource_discoverer == PARSER_DISCOVERER:
-    discovered_requests = PrefetchSimulationView.ParserDiscoverableRequests(
-        first_resource_request, dependencies_lens)
-  elif subresource_discoverer == HTML_PRELOAD_SCANNER_DISCOVERER:
-    discovered_requests = PrefetchSimulationView.PreloadedRequests(
-        first_resource_request, dependencies_lens, trace)
+  if subresource_discoverer == Discoverer.HTMLPreloadScannerStore:
+    requests = _DiscoverRequests(
+        dependencies_lens, Discoverer.HTMLPreloadScanner)
+    discovered_requests = _PruneOutOriginalNoStoreRequests(
+        original_headers_path, requests)
   else:
-    assert False
+    discovered_requests = _DiscoverRequests(
+        dependencies_lens, subresource_discoverer)
 
   whitelisted_urls = set()
   for request in _FilterOutDataAndIncompleteRequests(discovered_requests):
@@ -509,6 +538,7 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
                                   common_builder.output_subdirectory)
     self._common_builder = common_builder
 
+    self._original_headers_path = None
     self._wpr_archive_path = None
     self._cache_path = None
     self._trace_from_grabbing_reference_cache = None
@@ -528,13 +558,26 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
           depends on: common/webpages-patched.wpr
             depends on: common/webpages.wpr
     """
+    self._original_headers_path = self.RebaseOutputPath(
+        'common/response-headers.json')
+
     @self.RegisterTask('common/webpages-patched.wpr',
                        dependencies=[self._common_builder.original_wpr_task])
     def BuildPatchedWpr():
       common_util.EnsureParentDirectoryExists(BuildPatchedWpr.path)
       shutil.copyfile(
           self._common_builder.original_wpr_task.path, BuildPatchedWpr.path)
-      _PatchWpr(BuildPatchedWpr.path)
+      wpr_archive = wpr_backend.WprArchiveBackend(BuildPatchedWpr.path)
+
+      # Save up original response headers.
+      original_response_headers = {e.url: e.GetResponseHeadersDict() \
+          for e in wpr_archive.ListUrlEntries()}
+      with open(self._original_headers_path, 'w') as file_output:
+        json.dump(original_response_headers, file_output)
+
+      # Patch WPR.
+      _PatchWpr(wpr_archive)
+      wpr_archive.Persist()
 
     @self.RegisterTask('common/original-cache.zip', [BuildPatchedWpr])
     def BuildOriginalCache():
@@ -609,7 +652,9 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
                        dependencies=[self._cache_validation_task])
     def SetupBenchmark():
       whitelisted_urls = _ExtractDiscoverableUrls(
-          self._trace_from_grabbing_reference_cache, subresource_discoverer)
+          original_headers_path=self._original_headers_path,
+          loading_trace_path=self._trace_from_grabbing_reference_cache,
+          subresource_discoverer=subresource_discoverer)
 
       common_util.EnsureParentDirectoryExists(SetupBenchmark.path)
       with open(SetupBenchmark.path, 'w') as output:
diff --git a/loading/sandwich_prefetch_unittest.py b/loading/sandwich_prefetch_unittest.py
index 3c54dd5..0d69a47 100644
--- a/loading/sandwich_prefetch_unittest.py
+++ b/loading/sandwich_prefetch_unittest.py
@@ -2,7 +2,10 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import json
 import os
+import shutil
+import tempfile
 import unittest
 import urlparse
 
@@ -16,44 +19,93 @@ TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
 class SandwichPrefetchTestCase(unittest.TestCase):
   _TRACE_PATH = os.path.join(TEST_DATA_DIR, 'scanner_vs_parser.trace')
 
+  def setUp(self):
+    self._tmp_dir = tempfile.mkdtemp()
+
+  def tearDown(self):
+    shutil.rmtree(self._tmp_dir)
+
+  def GetTmpPath(self, file_name):
+    return os.path.join(self._tmp_dir, file_name)
+
   def GetResourceUrl(self, path):
     return urlparse.urljoin('http://l/', path)
 
-  def testNoDiscovererWhitelisting(self):
-    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_prefetch.EMPTY_CACHE_DISCOVERER)
+  def testEmptyCacheWhitelisting(self):
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(None,
+        self._TRACE_PATH, sandwich_prefetch.Discoverer.EmptyCache)
     self.assertEquals(set(), url_set)
 
   def testFullCacheWhitelisting(self):
     reference_url_set = set([self.GetResourceUrl('./'),
                              self.GetResourceUrl('0.png'),
                              self.GetResourceUrl('1.png'),
+                             self.GetResourceUrl('0.css'),
                              self.GetResourceUrl('favicon.ico')])
-    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_prefetch.FULL_CACHE_DISCOVERER)
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(None,
+        self._TRACE_PATH, sandwich_prefetch.Discoverer.FullCache)
     self.assertEquals(reference_url_set, url_set)
 
-  def testRedirectedMainWhitelisting(self):
+  def testMainDocumentWhitelisting(self):
     reference_url_set = set([self.GetResourceUrl('./')])
-    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_prefetch.REDIRECTED_MAIN_DISCOVERER)
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(None,
+        self._TRACE_PATH, sandwich_prefetch.Discoverer.MainDocument)
     self.assertEquals(reference_url_set, url_set)
 
   def testParserDiscoverableWhitelisting(self):
     reference_url_set = set([self.GetResourceUrl('./'),
                              self.GetResourceUrl('0.png'),
-                             self.GetResourceUrl('1.png')])
-    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_prefetch.PARSER_DISCOVERER)
+                             self.GetResourceUrl('1.png'),
+                             self.GetResourceUrl('0.css')])
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(None,
+        self._TRACE_PATH, sandwich_prefetch.Discoverer.Parser)
     self.assertEquals(reference_url_set, url_set)
 
   def testHTMLPreloadScannerWhitelisting(self):
     reference_url_set = set([self.GetResourceUrl('./'),
-                             self.GetResourceUrl('0.png')])
-    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_prefetch.HTML_PRELOAD_SCANNER_DISCOVERER)
+                             self.GetResourceUrl('0.png'),
+                             self.GetResourceUrl('0.css')])
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(None,
+        self._TRACE_PATH, sandwich_prefetch.Discoverer.HTMLPreloadScanner)
     self.assertEquals(reference_url_set, url_set)
 
+  def testHTMLPreloadScannerStoreWhitelisting(self):
+    original_headers_path = self.GetTmpPath('original_headers.json')
+
+    def RunTest(reference_urls):
+      url_set = sandwich_prefetch._ExtractDiscoverableUrls(
+          original_headers_path, self._TRACE_PATH,
+          sandwich_prefetch.Discoverer.HTMLPreloadScannerStore)
+      self.assertEquals(set(reference_urls), url_set)
+
+    with open(original_headers_path, 'w') as output_file:
+      json.dump({
+          self.GetResourceUrl('./'): {},
+          self.GetResourceUrl('0.png'): {'cache-control': 'max-age=0'},
+          self.GetResourceUrl('0.css'): {}
+        }, output_file)
+    RunTest([self.GetResourceUrl('./'),
+             self.GetResourceUrl('0.png'),
+             self.GetResourceUrl('0.css')])
+
+    with open(original_headers_path, 'w') as output_file:
+      json.dump({
+          self.GetResourceUrl('./'): {},
+          self.GetResourceUrl('0.png'): {'cache-control': 'private, no-store'},
+          self.GetResourceUrl('0.css'): {}
+        }, output_file)
+    RunTest([self.GetResourceUrl('./'),
+             self.GetResourceUrl('0.css')])
+
+    with open(original_headers_path, 'w') as output_file:
+      json.dump({
+          self.GetResourceUrl('./'): {'cache-control': 'private, no-store'},
+          self.GetResourceUrl('0.png'): {},
+          self.GetResourceUrl('0.css'): {}
+        }, output_file)
+    RunTest([self.GetResourceUrl('0.png'),
+             self.GetResourceUrl('0.css')])
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/task_manager.py b/loading/task_manager.py
index a82becc..1e5ca48 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -172,23 +172,28 @@ class Builder(object):
       A Task that was created by wrapping the function or an existing registered
       wrapper (that have wrapped a different function).
     """
-    task_name = self._RebaseTaskName(task_name)
+    rebased_task_name = self._RebaseTaskName(task_name)
     dependencies = dependencies or []
     def InnerAddTaskWithNewPath(recipe):
-      if task_name in self._tasks:
+      if rebased_task_name in self._tasks:
         if not merge:
-          raise TaskError('Task {} already exists.'.format(task_name))
-        task = self._tasks[task_name]
+          raise TaskError('Task {} already exists.'.format(rebased_task_name))
+        task = self._tasks[rebased_task_name]
         if task.IsStatic():
           raise TaskError('Should not merge dynamic task {} with the already '
-                          'existing static one.'.format(task_name))
+                          'existing static one.'.format(rebased_task_name))
         return task
-      task_path = os.path.join(self.output_directory, task_name)
-      task = Task(task_name, task_path, dependencies, recipe)
-      self._tasks[task_name] = task
+      task_path = self.RebaseOutputPath(task_name)
+      task = Task(rebased_task_name, task_path, dependencies, recipe)
+      self._tasks[rebased_task_name] = task
       return task
     return InnerAddTaskWithNewPath
 
+  def RebaseOutputPath(self, builder_relative_path):
+    """Rebases buider relative path."""
+    return os.path.join(
+        self.output_directory, self._RebaseTaskName(builder_relative_path))
+
   def _RebaseTaskName(self,  task_name):
     if self.output_subdirectory:
       return os.path.join(self.output_subdirectory, task_name)
diff --git a/loading/testdata/scanner_vs_parser.trace b/loading/testdata/scanner_vs_parser.trace
index 75d10a2..991fe8b 100644
--- a/loading/testdata/scanner_vs_parser.trace
+++ b/loading/testdata/scanner_vs_parser.trace
@@ -40,6 +40,15 @@
       },
       {
         "initiator": {
+          "lineNumber": 22,
+          "type": "parser",
+          "url": "http://l/"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/0.css"
+      },
+      {
+        "initiator": {
           "type": "other"
         },
         "protocol": "http/1.0",
@@ -65,7 +74,8 @@
         "name": "Resource",
         "ph": "S",
         "pid": 3,
-        "ts": 1213697828839
+        "ts": 1213697828839,
+        "id": 0
       },
       {
         "args": {},
@@ -73,7 +83,8 @@
         "name": "Resource",
         "ph": "F",
         "pid": 3,
-        "ts": 1213697889955
+        "ts": 1213697889955,
+        "id": 0
       },
       {
         "args": {
@@ -86,7 +97,33 @@
         "name": "Resource",
         "ph": "S",
         "pid": 3,
-        "ts": 1213697891911
+        "ts": 1213697891911,
+        "id": 1
+      },
+      {
+        "args": {
+          "step": "Preload"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697892658,
+        "id": 1
+      },
+      {
+        "args": {
+          "data": {
+            "priority": 1,
+            "url": "http://l/0.css"
+          }
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "S",
+        "pid": 3,
+        "ts": 1213697892660,
+        "id": 2
       },
       {
         "args": {
@@ -96,7 +133,8 @@
         "name": "Resource",
         "ph": "T",
         "pid": 3,
-        "ts": 1213697892658
+        "ts": 1213697892661,
+        "id": 2
       },
       {
         "args": {
@@ -109,7 +147,20 @@
         "name": "Resource",
         "ph": "S",
         "pid": 3,
-        "ts": 1213697934273
+        "ts": 1213697934273,
+        "id": 3
+      },
+      {
+        "args": {
+          "priority": 3,
+          "step": "ChangePriority"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697984606,
+        "id": 0
       },
       {
         "args": {},
@@ -117,7 +168,8 @@
         "name": "Resource",
         "ph": "F",
         "pid": 3,
-        "ts": 1213697943810
+        "ts": 1213697943810,
+        "id": 1
       },
       {
         "args": {
@@ -128,7 +180,8 @@
         "name": "Resource",
         "ph": "T",
         "pid": 3,
-        "ts": 1213697984606
+        "ts": 1213697984875,
+        "id": 1
       },
       {
         "args": {
@@ -139,7 +192,8 @@
         "name": "Resource",
         "ph": "T",
         "pid": 3,
-        "ts": 1213697984875
+        "ts": 1213697985346,
+        "id": 2
       },
       {
         "args": {
@@ -150,7 +204,17 @@
         "name": "Resource",
         "ph": "T",
         "pid": 3,
-        "ts": 1213697985346
+        "ts": 1213697985346,
+        "id": 3
+      },
+      {
+        "args": {},
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "F",
+        "pid": 3,
+        "ts": 1213698035637,
+        "id": 2
       },
       {
         "args": {},
@@ -158,7 +222,8 @@
         "name": "Resource",
         "ph": "F",
         "pid": 3,
-        "ts": 1213698035637
+        "ts": 1213698035637,
+        "id": 3
       }
     ]
   },

commit 8c3b28aacb91e83774d6b4723cd1148090d2481a
Author: gabadie <gabadie@chromium.org>
Date:   Thu Jun 23 10:42:48 2016 -0700

    sandwich: Implement setup-{prefetch,swr} generating a sandwich_setup.yaml
    
    Before, sandwich had only run commands. But the state could depend
    on the state of outside files such as the corpus. This could
    lead to non trivial behavior such as when removing an URL, the
    directory names of other URLs could change.
    
    This CL fixes that issue by introducing the setup sub-command concept
    that prepare the output directory with a sandwich_setup.yaml. Then
    the run sub-command just generate the tasks only according to that
    file.
    
    Among the changes, the --wpr-archive path is also removed because
    now that sandwich support multiple URLs, a single WPR archive
    would not scale in size with 100 URLs runs, and also to make sure the
    state of the sandwich runs only depends on the state of all the files
    stored in the sandwich's output directory.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2098533002
    Cr-Original-Commit-Position: refs/heads/master@{#401646}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6ba17619277f1941a870deb850c4581603b7511b

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 2760347..40a0f78 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -15,6 +15,7 @@ import logging
 import os
 import sys
 from urlparse import urlparse
+import yaml
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -42,6 +43,7 @@ _SPEED_INDEX_MEASUREMENT = 'speed-index'
 _MEMORY_MEASUREMENT = 'memory'
 _TTFMP_MEASUREMENT = 'ttfmp'
 _CORPUS_DIR = 'sandwich_corpuses'
+_SANDWICH_SETUP_FILENAME = 'sandwich_setup.yaml'
 
 _MAIN_TRANSFORMER_LIST_NAME = 'no-network-emulation'
 
@@ -64,43 +66,40 @@ def ReadUrlsFromCorpus(corpus_path):
   if json_data and key in json_data:
     url_list = json_data[key]
     if isinstance(url_list, list) and len(url_list) > 0:
-      return url_list
+      return [str(u) for u in url_list]
   raise Exception(
       'File {} does not define a list named "urls"'.format(json_file_name))
 
 
-def _GenerateUrlDirectoryNames(urls):
+def _GenerateUrlDirectoryMap(urls):
   domain_times_encountered_per_domain = {}
+  url_directories = {}
   for url in urls:
     domain = '.'.join(urlparse(url).netloc.split('.')[-2:])
     domain_times_encountered = domain_times_encountered_per_domain.get(
         domain, 0)
     output_subdirectory = '{}.{}'.format(domain, domain_times_encountered)
     domain_times_encountered_per_domain[domain] = domain_times_encountered + 1
-    yield url, output_subdirectory
+    url_directories[output_subdirectory] = url
+  return url_directories
 
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
-  task_parser = task_manager.CommandLineParser()
-
-  # Command parser when dealing with _RunBenchmarkMain.
-  sandwich_runner_parser = argparse.ArgumentParser(
-      add_help=False, parents=[task_parser])
-  sandwich_runner_parser.add_argument('--android', default=None, type=str,
-                                      dest='android_device_serial',
-                                      help='Android device\'s serial to use.')
-  sandwich_runner_parser.add_argument('-c', '--corpus', required=True,
+  # Command line parser when dealing with _SetupBenchmarkMain.
+  sandwich_setup_parser = argparse.ArgumentParser(add_help=False)
+  sandwich_setup_parser.add_argument('--android', default=None, type=str,
+      dest='android_device_serial', help='Android device\'s serial to use.')
+  sandwich_setup_parser.add_argument('-c', '--corpus', required=True,
       help='Path to a JSON file with a corpus such as in %s/.' % _CORPUS_DIR)
-  sandwich_runner_parser.add_argument('-m', '--measure', default=[], nargs='+',
+  sandwich_setup_parser.add_argument('-m', '--measure', default=[], nargs='+',
       choices=[_SPEED_INDEX_MEASUREMENT,
                _MEMORY_MEASUREMENT,
                _TTFMP_MEASUREMENT],
       dest='optional_measures', help='Enable optional measurements.')
-  sandwich_runner_parser.add_argument('--wpr-archive', default=None, type=str,
-      dest='wpr_archive_path',
-      help='WebPageReplay archive to use, instead of generating one.')
-  sandwich_runner_parser.add_argument('-r', '--url-repeat', default=1, type=int,
+  sandwich_setup_parser.add_argument('-o', '--output', type=str, required=True,
+      help='Path of the output directory to setup.')
+  sandwich_setup_parser.add_argument('-r', '--url-repeat', default=1, type=int,
       help='How many times to repeat the urls.')
 
   # Plumbing parser to configure OPTIONS.
@@ -111,18 +110,17 @@ def _ArgumentParser():
       fromfile_prefix_chars=task_manager.FROMFILE_PREFIX_CHARS)
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
-  # Run NoState-Prefetch benchmarks subcommand.
-  run_parser = subparsers.add_parser('run',
-      parents=[sandwich_runner_parser],
-      help='Run all NoState-Prefetch benchmarks steps using the task '
-           'manager infrastructure.')
-  run_parser.add_argument('-g', '--gen-full', action='store_true',
-      help='Generate the full graph with all possible benchmarks.')
+  # Setup NoState-Prefetch benchmarks subcommand.
+  subparsers.add_parser('setup-prefetch', parents=[sandwich_setup_parser],
+      help='Setup all NoState-Prefetch benchmarks.')
 
-  # Run Stale-While-Revalidate benchmarks subcommand.
-  subparsers.add_parser('run-swr', parents=[sandwich_runner_parser],
-      help='Run all Stale-While-Revalidate benchmarks steps using the task '
-           'manager infrastructure.')
+  # Setup Stale-While-Revalidate benchmarks subcommand.
+  subparsers.add_parser('setup-swr', parents=[sandwich_setup_parser],
+      help='Setup all Stale-While-Revalidate benchmarks.')
+
+  # Run benchmarks subcommand (used in _RunBenchmarkMain).
+  subparsers.add_parser('run', parents=[task_manager.CommandLineParser()],
+      help='Run benchmarks steps using the task manager infrastructure.')
 
   # Collect subcommand.
   collect_csv_parser = subparsers.add_parser('collect-csv',
@@ -136,8 +134,18 @@ def _ArgumentParser():
   return parser
 
 
+def _SetupNoStatePrefetchBenchmark(args):
+  del args # unused.
+  return {
+    'network_conditions': ['Regular4G', 'Regular3G', 'Regular2G'],
+    'subresource_discoverers': [
+        e for e in sandwich_prefetch.SUBRESOURCE_DISCOVERERS
+            if e != sandwich_prefetch.FULL_CACHE_DISCOVERER]
+  }
+
+
 def _GenerateNoStatePrefetchBenchmarkTasks(
-    args, common_builder, main_transformer):
+    common_builder, main_transformer, benchmark_setup):
   builder = sandwich_prefetch.PrefetchBenchmarkBuilder(common_builder)
   builder.PopulateLoadBenchmark(sandwich_prefetch.EMPTY_CACHE_DISCOVERER,
                                 _MAIN_TRANSFORMER_LIST_NAME,
@@ -145,28 +153,30 @@ def _GenerateNoStatePrefetchBenchmarkTasks(
   builder.PopulateLoadBenchmark(sandwich_prefetch.FULL_CACHE_DISCOVERER,
                                 _MAIN_TRANSFORMER_LIST_NAME,
                                 transformer_list=[main_transformer])
-  if not args.gen_full:
-    return
-  for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
+  for network_condition in benchmark_setup['network_conditions']:
     transformer_list_name = network_condition.lower()
     network_transformer = \
         sandwich_utils.NetworkSimulationTransformer(network_condition)
     transformer_list = [main_transformer, network_transformer]
-    for subresource_discoverer in sandwich_prefetch.SUBRESOURCE_DISCOVERERS:
-      if subresource_discoverer == sandwich_prefetch.FULL_CACHE_DISCOVERER:
-        continue
+    for subresource_discoverer in benchmark_setup['subresource_discoverers']:
       builder.PopulateLoadBenchmark(
           subresource_discoverer, transformer_list_name, transformer_list)
 
 
-def _GenerateStaleWhileRevalidateBenchmarkTasks(
-    args, common_builder, main_transformer):
+def _SetupStaleWhileRevalidateBenchmark(args):
   del args # unused.
+  return {
+    'network_conditions': ['Regular3G', 'Regular2G']
+  }
+
+
+def _GenerateStaleWhileRevalidateBenchmarkTasks(
+    common_builder, main_transformer, benchmark_setup):
   builder = sandwich_swr.StaleWhileRevalidateBenchmarkBuilder(common_builder)
   for enable_swr in [False, True]:
     builder.PopulateBenchmark(enable_swr, _MAIN_TRANSFORMER_LIST_NAME,
                               transformer_list=[main_transformer])
-    for network_condition in ['Regular3G', 'Regular2G']:
+    for network_condition in benchmark_setup['network_conditions']:
       transformer_list_name = network_condition.lower()
       network_transformer = \
           sandwich_utils.NetworkSimulationTransformer(network_condition)
@@ -175,32 +185,60 @@ def _GenerateStaleWhileRevalidateBenchmarkTasks(
           enable_swr, transformer_list_name, transformer_list)
 
 
-def _RunBenchmarkMain(args, task_generator):
+_TASK_GENERATORS = {
+  'prefetch': _GenerateNoStatePrefetchBenchmarkTasks,
+  'swr': _GenerateStaleWhileRevalidateBenchmarkTasks
+}
+
+
+def _SetupBenchmarkMain(args, benchmark_type, benchmark_specific_handler):
+  assert benchmark_type in _TASK_GENERATORS
   urls = ReadUrlsFromCorpus(args.corpus)
-  android_device = None
-  if args.android_device_serial:
+  setup = {
+    'benchmark_type': benchmark_type,
+    'benchmark_setup': benchmark_specific_handler(args),
+    'sandwich_runner': {
+      'record_video': _SPEED_INDEX_MEASUREMENT in args.optional_measures,
+      'record_memory_dumps': _MEMORY_MEASUREMENT in args.optional_measures,
+      'record_first_meaningful_paint': (
+          _TTFMP_MEASUREMENT in args.optional_measures),
+      'repeat': args.url_repeat,
+      'android_device_serial': args.android_device_serial
+    },
+    'urls': _GenerateUrlDirectoryMap(urls)
+  }
+  if not os.path.isdir(args.output):
+    os.makedirs(args.output)
+  setup_path = os.path.join(args.output, _SANDWICH_SETUP_FILENAME)
+  with open(setup_path, 'w') as file_output:
+    yaml.dump(setup, file_output, default_flow_style=False)
+
+
+def _RunBenchmarkMain(args):
+  setup_path = os.path.join(args.output, _SANDWICH_SETUP_FILENAME)
+  with open(setup_path) as file_input:
+    setup = yaml.load(file_input)
+  if setup['sandwich_runner']['android_device_serial']:
     android_device = device_setup.GetDeviceFromSerial(
-        args.android_device_serial)
+        setup['sandwich_runner']['android_device_serial'])
+  task_generator = _TASK_GENERATORS[setup['benchmark_type']]
 
   def MainTransformer(runner):
-    runner.record_video = _SPEED_INDEX_MEASUREMENT in args.optional_measures
-    runner.record_memory_dumps = _MEMORY_MEASUREMENT in args.optional_measures
+    runner.record_video = setup['sandwich_runner']['record_video']
+    runner.record_memory_dumps = setup['sandwich_runner']['record_memory_dumps']
     runner.record_first_meaningful_paint = (
-        _TTFMP_MEASUREMENT in args.optional_measures)
-    runner.repeat = args.url_repeat
+        setup['sandwich_runner']['record_first_meaningful_paint'])
+    runner.repeat = setup['sandwich_runner']['repeat']
 
   default_final_tasks = []
-  for url, output_subdirectory in _GenerateUrlDirectoryNames(urls):
+  for output_subdirectory, url in setup['urls'].iteritems():
     common_builder = sandwich_utils.SandwichCommonBuilder(
         android_device=android_device,
         url=url,
         output_directory=args.output,
         output_subdirectory=output_subdirectory)
-    if args.wpr_archive_path:
-      common_builder.OverridePathToWprArchive(args.wpr_archive_path)
-    else:
-      common_builder.PopulateWprRecordingTask()
-    task_generator(args, common_builder, MainTransformer)
+    common_builder.PopulateWprRecordingTask()
+    task_generator(common_builder, MainTransformer, setup['benchmark_setup'])
     default_final_tasks.extend(common_builder.default_final_tasks)
   return task_manager.ExecuteWithCommandLine(args, default_final_tasks)
 
@@ -212,10 +250,14 @@ def main(command_line_args):
   args = _ArgumentParser().parse_args(command_line_args)
   OPTIONS.SetParsedArgs(args)
 
+  if args.subcommand == 'setup-prefetch':
+    return _SetupBenchmarkMain(
+        args, 'prefetch', _SetupNoStatePrefetchBenchmark)
+  if args.subcommand == 'setup-swr':
+    return _SetupBenchmarkMain(
+        args, 'swr', _SetupStaleWhileRevalidateBenchmark)
   if args.subcommand == 'run':
-    return _RunBenchmarkMain(args, _GenerateNoStatePrefetchBenchmarkTasks)
-  if args.subcommand == 'run-swr':
-    return _RunBenchmarkMain(args, _GenerateStaleWhileRevalidateBenchmarkTasks)
+    return _RunBenchmarkMain(args)
   if args.subcommand == 'collect-csv':
     with args.output_csv as output_file:
       if not csv_util.CollectCSVsFromDirectory(args.output_dir, output_file):
diff --git a/loading/sandwich_utils.py b/loading/sandwich_utils.py
index 791c3ec..bbc2d9d 100644
--- a/loading/sandwich_utils.py
+++ b/loading/sandwich_utils.py
@@ -54,15 +54,6 @@ class SandwichCommonBuilder(task_manager.Builder):
     runner.android_device = self._android_device
     return runner
 
-  def OverridePathToWprArchive(self, original_wpr_path):
-    """Sets the original WPR archive path's to be used.
-
-    Args:
-      original_wpr_path: Path of the original WPR archive to be used.
-    """
-    self.original_wpr_task = \
-        self.CreateStaticTask('common/webpages.wpr', original_wpr_path)
-
   def PopulateWprRecordingTask(self):
     """Records the original WPR archive."""
     @self.RegisterTask('common/webpages.wpr')

commit f4e944f7236fb045f15e671a75a999e03d05cca5
Author: gabadie <gabadie@chromium.org>
Date:   Thu Jun 23 04:45:46 2016 -0700

    sandwich: Replace --swr-benchmark with run-swr subcommand.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2093433002
    Cr-Original-Commit-Position: refs/heads/master@{#401582}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a8b44506aab0a1a2b17bbf26d8362b80380b5663

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 853a531..2760347 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -13,7 +13,6 @@ import argparse
 import json
 import logging
 import os
-import shutil
 import sys
 from urlparse import urlparse
 
@@ -27,19 +26,13 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 import devil_chromium
 
-import chrome_cache
-import common_util
 import csv_util
 import device_setup
-import emulation
 import options
-import sandwich_metrics
 import sandwich_prefetch
-import sandwich_runner
 import sandwich_swr
 import sandwich_utils
 import task_manager
-from trace_test.webserver_test import WebServer
 
 
 # Use options layer to access constants.
@@ -50,6 +43,8 @@ _MEMORY_MEASUREMENT = 'memory'
 _TTFMP_MEASUREMENT = 'ttfmp'
 _CORPUS_DIR = 'sandwich_corpuses'
 
+_MAIN_TRANSFORMER_LIST_NAME = 'no-network-emulation'
+
 
 def ReadUrlsFromCorpus(corpus_path):
   """Retrieves the list of URLs associated with the corpus name."""
@@ -74,15 +69,39 @@ def ReadUrlsFromCorpus(corpus_path):
       'File {} does not define a list named "urls"'.format(json_file_name))
 
 
+def _GenerateUrlDirectoryNames(urls):
+  domain_times_encountered_per_domain = {}
+  for url in urls:
+    domain = '.'.join(urlparse(url).netloc.split('.')[-2:])
+    domain_times_encountered = domain_times_encountered_per_domain.get(
+        domain, 0)
+    output_subdirectory = '{}.{}'.format(domain, domain_times_encountered)
+    domain_times_encountered_per_domain[domain] = domain_times_encountered + 1
+    yield url, output_subdirectory
+
+
 def _ArgumentParser():
   """Build a command line argument's parser."""
   task_parser = task_manager.CommandLineParser()
 
-  # Command parser when dealing with SandwichRunner.
-  sandwich_runner_parser = argparse.ArgumentParser(add_help=False)
+  # Command parser when dealing with _RunBenchmarkMain.
+  sandwich_runner_parser = argparse.ArgumentParser(
+      add_help=False, parents=[task_parser])
   sandwich_runner_parser.add_argument('--android', default=None, type=str,
                                       dest='android_device_serial',
                                       help='Android device\'s serial to use.')
+  sandwich_runner_parser.add_argument('-c', '--corpus', required=True,
+      help='Path to a JSON file with a corpus such as in %s/.' % _CORPUS_DIR)
+  sandwich_runner_parser.add_argument('-m', '--measure', default=[], nargs='+',
+      choices=[_SPEED_INDEX_MEASUREMENT,
+               _MEMORY_MEASUREMENT,
+               _TTFMP_MEASUREMENT],
+      dest='optional_measures', help='Enable optional measurements.')
+  sandwich_runner_parser.add_argument('--wpr-archive', default=None, type=str,
+      dest='wpr_archive_path',
+      help='WebPageReplay archive to use, instead of generating one.')
+  sandwich_runner_parser.add_argument('-r', '--url-repeat', default=1, type=int,
+      help='How many times to repeat the urls.')
 
   # Plumbing parser to configure OPTIONS.
   plumbing_parser = OPTIONS.GetParentParser('plumbing options')
@@ -92,41 +111,18 @@ def _ArgumentParser():
       fromfile_prefix_chars=task_manager.FROMFILE_PREFIX_CHARS)
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
-  # Record test trace subcommand.
-  record_trace_parser = subparsers.add_parser('record-test-trace',
-      parents=[sandwich_runner_parser],
-      help='Record a test trace using the trace_test.webserver_test.')
-  record_trace_parser.add_argument('--source-dir', type=str, required=True,
-                                   help='Base path where the files are opened '
-                                        'by the web server.')
-  record_trace_parser.add_argument('--page', type=str, required=True,
-                                   help='Source page in source-dir to navigate '
-                                        'to.')
-  record_trace_parser.add_argument('-o', '--output', type=str, required=True,
-                                   help='Output path of the generated trace.')
-
-  # Run subcommand.
+  # Run NoState-Prefetch benchmarks subcommand.
   run_parser = subparsers.add_parser('run',
-      parents=[sandwich_runner_parser, task_parser],
-      help='Run all steps using the task manager infrastructure.')
+      parents=[sandwich_runner_parser],
+      help='Run all NoState-Prefetch benchmarks steps using the task '
+           'manager infrastructure.')
   run_parser.add_argument('-g', '--gen-full', action='store_true',
-                          help='Generate the full graph with all possible '
-                               'benchmarks.')
-  run_parser.add_argument('-c', '--corpus', required=True,
-      help='Path to a JSON file with a corpus such as in %s/.' % _CORPUS_DIR)
-  run_parser.add_argument('-m', '--measure', default=[], nargs='+',
-      choices=[_SPEED_INDEX_MEASUREMENT,
-               _MEMORY_MEASUREMENT,
-               _TTFMP_MEASUREMENT],
-      dest='optional_measures', help='Enable optional measurements.')
-  run_parser.add_argument('--wpr-archive', default=None, type=str,
-                          dest='wpr_archive_path',
-                          help='WebPageReplay archive to use, instead of '
-                               'generating one.')
-  run_parser.add_argument('-r', '--url-repeat', default=1, type=int,
-                          help='How many times to repeat the urls.')
-  run_parser.add_argument('--swr-benchmark', action='store_true',
-                          help='Run the Stale-While-Revalidate benchmarks.')
+      help='Generate the full graph with all possible benchmarks.')
+
+  # Run Stale-While-Revalidate benchmarks subcommand.
+  subparsers.add_parser('run-swr', parents=[sandwich_runner_parser],
+      help='Run all Stale-While-Revalidate benchmarks steps using the task '
+           'manager infrastructure.')
 
   # Collect subcommand.
   collect_csv_parser = subparsers.add_parser('collect-csv',
@@ -140,42 +136,51 @@ def _ArgumentParser():
   return parser
 
 
-def _GetAndroidDeviceFromArgs(args):
+def _GenerateNoStatePrefetchBenchmarkTasks(
+    args, common_builder, main_transformer):
+  builder = sandwich_prefetch.PrefetchBenchmarkBuilder(common_builder)
+  builder.PopulateLoadBenchmark(sandwich_prefetch.EMPTY_CACHE_DISCOVERER,
+                                _MAIN_TRANSFORMER_LIST_NAME,
+                                transformer_list=[main_transformer])
+  builder.PopulateLoadBenchmark(sandwich_prefetch.FULL_CACHE_DISCOVERER,
+                                _MAIN_TRANSFORMER_LIST_NAME,
+                                transformer_list=[main_transformer])
+  if not args.gen_full:
+    return
+  for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
+    transformer_list_name = network_condition.lower()
+    network_transformer = \
+        sandwich_utils.NetworkSimulationTransformer(network_condition)
+    transformer_list = [main_transformer, network_transformer]
+    for subresource_discoverer in sandwich_prefetch.SUBRESOURCE_DISCOVERERS:
+      if subresource_discoverer == sandwich_prefetch.FULL_CACHE_DISCOVERER:
+        continue
+      builder.PopulateLoadBenchmark(
+          subresource_discoverer, transformer_list_name, transformer_list)
+
+
+def _GenerateStaleWhileRevalidateBenchmarkTasks(
+    args, common_builder, main_transformer):
+  del args # unused.
+  builder = sandwich_swr.StaleWhileRevalidateBenchmarkBuilder(common_builder)
+  for enable_swr in [False, True]:
+    builder.PopulateBenchmark(enable_swr, _MAIN_TRANSFORMER_LIST_NAME,
+                              transformer_list=[main_transformer])
+    for network_condition in ['Regular3G', 'Regular2G']:
+      transformer_list_name = network_condition.lower()
+      network_transformer = \
+          sandwich_utils.NetworkSimulationTransformer(network_condition)
+      transformer_list = [main_transformer, network_transformer]
+      builder.PopulateBenchmark(
+          enable_swr, transformer_list_name, transformer_list)
+
+
+def _RunBenchmarkMain(args, task_generator):
+  urls = ReadUrlsFromCorpus(args.corpus)
+  android_device = None
   if args.android_device_serial:
-    return device_setup.GetDeviceFromSerial(args.android_device_serial)
-  return None
-
-
-def _RecordWebServerTestTrace(args):
-  with common_util.TemporaryDirectory() as out_path:
-    runner = sandwich_runner.SandwichRunner()
-    runner.android_device = _GetAndroidDeviceFromArgs(args)
-    # Reuse the WPR's forwarding to access the webpage from Android.
-    runner.wpr_record = True
-    runner.wpr_archive_path = os.path.join(out_path, 'wpr')
-    runner.output_dir = os.path.join(out_path, 'run')
-    with WebServer.Context(
-        source_dir=args.source_dir, communication_dir=out_path) as server:
-      address = server.Address()
-      runner.url = 'http://%s/%s' % (address, args.page)
-      runner.Run()
-    trace_path = os.path.join(
-        out_path, 'run', '0', sandwich_runner.TRACE_FILENAME)
-    shutil.copy(trace_path, args.output)
-  return 0
-
-
-def _GenerateBenchmarkTasks(args, android_device, url, output_subdirectory):
-  MAIN_TRANSFORMER_LIST_NAME = 'no-network-emulation'
-  common_builder = sandwich_utils.SandwichCommonBuilder(
-      android_device=android_device,
-      url=url,
-      output_directory=args.output,
-      output_subdirectory=output_subdirectory)
-  if args.wpr_archive_path:
-    common_builder.OverridePathToWprArchive(args.wpr_archive_path)
-  else:
-    common_builder.PopulateWprRecordingTask()
+    android_device = device_setup.GetDeviceFromSerial(
+        args.android_device_serial)
 
   def MainTransformer(runner):
     runner.record_video = _SPEED_INDEX_MEASUREMENT in args.optional_measures
@@ -184,55 +189,19 @@ def _GenerateBenchmarkTasks(args, android_device, url, output_subdirectory):
         _TTFMP_MEASUREMENT in args.optional_measures)
     runner.repeat = args.url_repeat
 
-  if not args.swr_benchmark:
-    builder = sandwich_prefetch.PrefetchBenchmarkBuilder(common_builder)
-    builder.PopulateLoadBenchmark(sandwich_prefetch.EMPTY_CACHE_DISCOVERER,
-                                  MAIN_TRANSFORMER_LIST_NAME,
-                                  transformer_list=[MainTransformer])
-    builder.PopulateLoadBenchmark(sandwich_prefetch.FULL_CACHE_DISCOVERER,
-                                  MAIN_TRANSFORMER_LIST_NAME,
-                                  transformer_list=[MainTransformer])
-    if args.gen_full:
-      for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
-        transformer_list_name = network_condition.lower()
-        network_transformer = \
-            sandwich_utils.NetworkSimulationTransformer(
-                network_condition)
-        transformer_list = [MainTransformer, network_transformer]
-        for subresource_discoverer in sandwich_prefetch.SUBRESOURCE_DISCOVERERS:
-          if subresource_discoverer == sandwich_prefetch.FULL_CACHE_DISCOVERER:
-            continue
-          builder.PopulateLoadBenchmark(subresource_discoverer,
-              transformer_list_name, transformer_list)
-  else:
-    builder = sandwich_swr.StaleWhileRevalidateBenchmarkBuilder(common_builder)
-    for enable_swr in [False, True]:
-      builder.PopulateBenchmark(enable_swr, MAIN_TRANSFORMER_LIST_NAME,
-                                transformer_list=[MainTransformer])
-      for network_condition in ['Regular3G', 'Regular2G']:
-        transformer_list_name = network_condition.lower()
-        network_transformer = \
-            sandwich_utils.NetworkSimulationTransformer(
-                network_condition)
-        transformer_list = [MainTransformer, network_transformer]
-        builder.PopulateBenchmark(enable_swr, transformer_list_name,
-                                  transformer_list)
-  return common_builder.default_final_tasks
-
-
-def _RunAllMain(args):
-  urls = ReadUrlsFromCorpus(args.corpus)
-  domain_times_encountered_per_domain = {}
   default_final_tasks = []
-  android_device = _GetAndroidDeviceFromArgs(args)
-  for url in urls:
-    domain = '.'.join(urlparse(url).netloc.split('.')[-2:])
-    domain_times_encountered = domain_times_encountered_per_domain.get(
-        domain, 0)
-    output_subdirectory = '{}.{}'.format(domain, domain_times_encountered)
-    domain_times_encountered_per_domain[domain] = domain_times_encountered + 1
-    default_final_tasks.extend(
-        _GenerateBenchmarkTasks(args, android_device, url, output_subdirectory))
+  for url, output_subdirectory in _GenerateUrlDirectoryNames(urls):
+    common_builder = sandwich_utils.SandwichCommonBuilder(
+        android_device=android_device,
+        url=url,
+        output_directory=args.output,
+        output_subdirectory=output_subdirectory)
+    if args.wpr_archive_path:
+      common_builder.OverridePathToWprArchive(args.wpr_archive_path)
+    else:
+      common_builder.PopulateWprRecordingTask()
+    task_generator(args, common_builder, MainTransformer)
+    default_final_tasks.extend(common_builder.default_final_tasks)
   return task_manager.ExecuteWithCommandLine(args, default_final_tasks)
 
 
@@ -243,10 +212,10 @@ def main(command_line_args):
   args = _ArgumentParser().parse_args(command_line_args)
   OPTIONS.SetParsedArgs(args)
 
-  if args.subcommand == 'record-test-trace':
-    return _RecordWebServerTestTrace(args)
   if args.subcommand == 'run':
-    return _RunAllMain(args)
+    return _RunBenchmarkMain(args, _GenerateNoStatePrefetchBenchmarkTasks)
+  if args.subcommand == 'run-swr':
+    return _RunBenchmarkMain(args, _GenerateStaleWhileRevalidateBenchmarkTasks)
   if args.subcommand == 'collect-csv':
     with args.output_csv as output_file:
       if not csv_util.CollectCSVsFromDirectory(args.output_dir, output_file):

commit 2f4f5e71aa679417fb58c93caa33347a0d614f8f
Author: agrieve <agrieve@chromium.org>
Date:   Wed Jun 22 14:04:07 2016 -0700

    Use GN's dependency info for native libraries in write_build_config.py
    
    This is in place of readelf / write_ordered_libaries.py.
    
    Main motivation is to be able to write .build_config files quickly, so
    that they can be used for generating build.gradle files for Android
    Studio without first needing to build a bunch of native libraries.
    
    BUG=620034
    CQ_INCLUDE_TRYBOTS=tryserver.blink:linux_blink_rel
    
    Review-Url: https://codereview.chromium.org/2082453003
    Cr-Original-Commit-Position: refs/heads/master@{#401398}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 48bd27ea3fd3b86e352d6a7a8e4e70b0c711ae78

diff --git a/memconsumer/BUILD.gn b/memconsumer/BUILD.gn
index 90b7fcf..70a4447 100644
--- a/memconsumer/BUILD.gn
+++ b/memconsumer/BUILD.gn
@@ -16,10 +16,9 @@ android_apk("memconsumer_apk") {
     "java/src/org/chromium/memconsumer/MemConsumer.java",
     "java/src/org/chromium/memconsumer/ResidentService.java",
   ]
-  native_libs = [ "libmemconsumer.so" ]
+  shared_libraries = [ ":libmemconsumer" ]
 
   deps = [
-    ":libmemconsumer",
     ":memconsumer_apk_resources",
     "//base:base_java",
   ]

commit 94cf5e0886418f5abee7760404355d7f9804645b
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 22 08:44:38 2016 -0700

    sandwich: Add total and served from cache resources size columns in CSV
    
    The NoState-Prefetch study also need to monitor the amount of bytes
    prefetched compared to the nativgation's performance improvement.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2079323002
    Cr-Original-Commit-Position: refs/heads/master@{#401292}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d8501768822fd748922c0a40782a6f1b485a8643

diff --git a/loading/csv_util.py b/loading/csv_util.py
index e27d74a..ccdfb92 100644
--- a/loading/csv_util.py
+++ b/loading/csv_util.py
@@ -40,7 +40,8 @@ def CollectCSVsFromDirectory(directory_path, file_output):
       if csv_field_names is None:
         csv_field_names = reader.fieldnames
       else:
-        assert reader.fieldnames == csv_field_names
+        assert reader.fieldnames == csv_field_names, (
+            'Different field names in: {}'.format(csv_file))
       for row in reader:
         csv_rows.append(row)
 
diff --git a/loading/request_track.py b/loading/request_track.py
index ffd1f43..744f973 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -24,6 +24,7 @@ import devtools_monitor
 
 class Timing(object):
   """Collects the timing data for a request."""
+  UNVAILABLE = -1
   _TIMING_NAMES = (
       ('connectEnd', 'connect_end'), ('connectStart', 'connect_start'),
       ('dnsEnd', 'dns_end'), ('dnsStart', 'dns_start'),
@@ -44,7 +45,7 @@ class Timing(object):
     Initialize with keywords arguments from __slots__.
     """
     for slot in self.__slots__:
-      setattr(self, slot, -1)
+      setattr(self, slot, self.UNVAILABLE)
     for (attr, value) in kwargs.items():
       setattr(self, attr, value)
 
@@ -250,6 +251,24 @@ class Request(object):
       result.timing = Timing(request_time=result.timestamp)
     return result
 
+  def GetEncodedDataLength(self):
+    """Get the total amount of encoded data no matter whether load has finished
+    or not.
+    """
+    assert self.HasReceivedResponse()
+    assert not self.from_disk_cache and not self.served_from_cache
+    if self.failed:
+      # TODO(gabadie): Once crbug.com/622018 is fixed, remove this branch.
+      return 0
+    if self.timing.loading_finished != Timing.UNVAILABLE:
+      encoded_data_length = self.encoded_data_length
+      assert encoded_data_length > 0
+    else:
+      encoded_data_length = sum(
+          [chunk_size for _, chunk_size in self.data_chunks])
+      assert encoded_data_length > 0 or len(self.data_chunks) == 0
+    return encoded_data_length
+
   def GetHTTPResponseHeader(self, header_name):
     """Gets the value of a HTTP response header.
 
diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index 8290e49..8429305 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -286,7 +286,8 @@ class _RunOutputVerifier(object):
       benchmark_setup: JSON of the benchmark setup.
     """
     self._cache_whitelist = set(benchmark_setup['cache_whitelist'])
-    self._original_requests = set(cache_validation_result['effective_requests'])
+    self._original_requests = set(
+        cache_validation_result['effective_encoded_data_lengths'].keys())
     self._original_post_requests = set(
         cache_validation_result['effective_post_requests'])
     self._original_cached_requests = self._original_requests.intersection(
@@ -360,7 +361,8 @@ def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
 
   Returns:
     {
-      'effective_requests': [URLs of all requests],
+      'effective_encoded_data_lengths':
+        {URL of all requests: encoded_data_length},
       'effective_post_requests': [URLs of POST requests],
       'expected_cached_resources': [URLs of resources expected to be cached],
       'successfully_cached': [URLs of cached sub-resources]
@@ -375,6 +377,20 @@ def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
   trace = loading_trace.LoadingTrace.FromJsonFile(cache_build_trace_path)
   effective_requests = _ListUrlRequests(trace, _RequestOutcome.All)
   effective_post_requests = _ListUrlRequests(trace, _RequestOutcome.Post)
+  effective_encoded_data_lengths = {}
+  for request in _FilterOutDataAndIncompleteRequests(
+      trace.request_track.GetEvents()):
+    if request.from_disk_cache or request.served_from_cache:
+      # At cache archive creation time, a request might be loaded several times,
+      # but avoid the request.encoded_data_length == 0 if loaded from cache.
+      continue
+    if request.url in effective_encoded_data_lengths:
+      effective_encoded_data_lengths[request.url] = max(
+          effective_encoded_data_lengths[request.url],
+          request.GetEncodedDataLength())
+    else:
+      effective_encoded_data_lengths[request.url] = (
+          request.GetEncodedDataLength())
 
   upload_data_stream_cache_entry_keys = set()
   upload_data_stream_requests = set()
@@ -396,7 +412,7 @@ def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
                          'Cached resources')
 
   return {
-      'effective_requests': [url for url in effective_requests],
+      'effective_encoded_data_lengths': effective_encoded_data_lengths,
       'effective_post_requests': [url for url in effective_post_requests],
       'expected_cached_resources': [url for url in expected_cached_requests],
       'successfully_cached_resources': [url for url in effective_cache_keys]
@@ -418,6 +434,8 @@ def _ProcessRunOutputDir(
   run_metrics_list = []
   run_output_verifier = _RunOutputVerifier(
       cache_validation_result, benchmark_setup)
+  cached_encoded_data_lengths = (
+      cache_validation_result['effective_encoded_data_lengths'])
   for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
       runner_output_dir):
     trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
@@ -429,12 +447,33 @@ def _ProcessRunOutputDir(
     run_output_verifier.VerifyTrace(trace)
 
     logging.info('extracting metrics from trace: %s', trace_path)
+    served_from_network_bytes = 0
+    served_from_cache_bytes = 0
+    urls_hitting_network = set()
+    for request in _FilterOutDataAndIncompleteRequests(
+        trace.request_track.GetEvents()):
+      # Ignore requests served from the blink's cache.
+      if request.served_from_cache:
+        continue
+      urls_hitting_network.add(request.url)
+      if request.from_disk_cache:
+        served_from_cache_bytes += cached_encoded_data_lengths[request.url]
+      else:
+        served_from_network_bytes += request.GetEncodedDataLength()
+
+    # Make sure the served from blink's cache requests have at least one
+    # corresponding request that was not served from the blink's cache.
+    for request in _FilterOutDataAndIncompleteRequests(
+        trace.request_track.GetEvents()):
+      assert (request.url in urls_hitting_network or
+              not request.served_from_cache)
+
     run_metrics = {
         'url': trace.url,
         'repeat_id': repeat_id,
         'subresource_discoverer': benchmark_setup['subresource_discoverer'],
         'cache_recording.subresource_count':
-            len(cache_validation_result['effective_requests']),
+            len(cache_validation_result['effective_encoded_data_lengths']),
         'cache_recording.cached_subresource_count_theoretic':
             len(cache_validation_result['successfully_cached_resources']),
         'cache_recording.cached_subresource_count':
@@ -445,6 +484,8 @@ def _ProcessRunOutputDir(
             len(benchmark_setup['cache_whitelist']),
         'benchmark.served_from_cache_count': len(_ListUrlRequests(
             trace, _RequestOutcome.ServedFromCache)),
+        'benchmark.served_from_network_bytes': served_from_network_bytes,
+        'benchmark.served_from_cache_bytes': served_from_cache_bytes
     }
     run_metrics.update(
         sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
@@ -555,7 +596,9 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
         'cache_recording.cached_subresource_count',
         'benchmark.subresource_count',
         'benchmark.served_from_cache_count_theoretic',
-        'benchmark.served_from_cache_count']
+        'benchmark.served_from_cache_count',
+        'benchmark.served_from_network_bytes',
+        'benchmark.served_from_cache_bytes']
 
     assert subresource_discoverer in SUBRESOURCE_DISCOVERERS
     assert 'common' not in SUBRESOURCE_DISCOVERERS

commit f28a1e080196cacad40ef460926d16ea6c51fdce
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 22 03:52:30 2016 -0700

    sandwich: Add a timeout of 30 minutes on run.
    
    Before, sandwich was not timing out Chrome run, and could lead to
    different source of hanging when recording the trace or tearing
    down chrome.
    
    This CL addresses this issue in the SandwichRunner by arming a 30
    minutes SIGALARM for each run to fail the task and let the
    task_manager carry on on other URLs with the --keep-going flag.
    
    This CL also take the oportunity to let the task_manager to
    re-raise ENOSPC errors to avoid corrupting the resuming file.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2084513004
    Cr-Original-Commit-Position: refs/heads/master@{#401247}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 55009cbbecf103b24224ebd9b3a7a1ad13cb0fa5

diff --git a/loading/common_util.py b/loading/common_util.py
index c4eac2c..899062c 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -8,6 +8,7 @@ import logging
 import os
 import re
 import shutil
+import signal
 import subprocess
 import sys
 import tempfile
@@ -114,3 +115,36 @@ def GetCommandLineForLogging(cmd, env_diff=None):
     for key, value in env_diff.iteritems():
       cmd_str += '{}={} '.format(key, value)
   return cmd_str + subprocess.list2cmdline(cmd)
+
+
+# TimeoutError inherit from BaseException to pass through DevUtils' retries
+# decorator that catches only exceptions inheriting from Exception.
+class TimeoutError(BaseException):
+  pass
+
+
+# If this exception is ever raised, then might be better to replace this
+# implementation with Thread.join(timeout=XXX).
+class TimeoutCollisionError(Exception):
+  pass
+
+
+@contextlib.contextmanager
+def TimeoutScope(seconds, error_name):
+  """Raises TimeoutError if the with statement is finished within |seconds|."""
+  assert seconds > 0
+  def _signal_callback(signum, frame):
+    del signum, frame # unused.
+    raise TimeoutError(error_name)
+
+  try:
+    signal.signal(signal.SIGALRM, _signal_callback)
+    if signal.alarm(seconds) != 0:
+      raise TimeoutCollisionError(
+          'Discarding an alarm that was scheduled before.')
+    yield
+  finally:
+    signal.alarm(0)
+    if signal.getsignal(signal.SIGALRM) != _signal_callback:
+      raise TimeoutCollisionError('Looks like there is a signal.signal(signal.'
+          'SIGALRM) made within the with statement.')
diff --git a/loading/common_util_unittest.py b/loading/common_util_unittest.py
index 8984881..62ddaee 100644
--- a/loading/common_util_unittest.py
+++ b/loading/common_util_unittest.py
@@ -2,6 +2,8 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import signal
+import time
 import unittest
 
 import common_util
@@ -47,5 +49,50 @@ class SerializeAttributesTestCase(unittest.TestCase):
           json_dict, foo_fighters, ['foo_fighters', 'whisky_bar', 'baz'])
 
 
+class TimeoutScopeTestCase(unittest.TestCase):
+  def testTimeoutRaise(self):
+    self.assertEquals(0, signal.alarm(0))
+
+    with self.assertRaisesRegexp(common_util.TimeoutError, 'hello'):
+      with common_util.TimeoutScope(seconds=1, error_name='hello'):
+        signal.pause()
+        self.fail()
+    self.assertEquals(0, signal.alarm(0))
+
+    with self.assertRaisesRegexp(common_util.TimeoutError, 'world'):
+      with common_util.TimeoutScope(seconds=1, error_name='world'):
+        time.sleep(2)
+    self.assertEquals(0, signal.alarm(0))
+
+  def testCollisionDetection(self):
+    ONE_YEAR = 365 * 24 * 60 * 60
+
+    def _mock_callback(signum, frame):
+      del signum, frame # unused.
+
+    flag = False
+    with self.assertRaises(common_util.TimeoutCollisionError):
+      with common_util.TimeoutScope(seconds=ONE_YEAR, error_name=''):
+        flag = True
+        signal.signal(signal.SIGALRM, _mock_callback)
+    self.assertTrue(flag)
+    self.assertEquals(0, signal.alarm(0))
+
+    flag = False
+    with self.assertRaises(common_util.TimeoutCollisionError):
+      with common_util.TimeoutScope(seconds=ONE_YEAR, error_name=''):
+        flag = True
+        with common_util.TimeoutScope(seconds=ONE_YEAR, error_name=''):
+          self.fail()
+    self.assertTrue(flag)
+    self.assertEquals(0, signal.alarm(0))
+
+    signal.alarm(ONE_YEAR)
+    with self.assertRaises(common_util.TimeoutCollisionError):
+      with common_util.TimeoutScope(seconds=ONE_YEAR, error_name=''):
+        self.fail()
+    self.assertEquals(0, signal.alarm(0))
+
+
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 815ccc0..640b068 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -19,6 +19,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'telemetry',
 import websocket
 
 import chrome_cache
+import common_util
 import controller
 import devtools_monitor
 import device_setup
@@ -86,6 +87,7 @@ class SandwichRunner(object):
   """
   _ATTEMPT_COUNT = 3
   _STOP_DELAY_MULTIPLIER = 2
+  _ABORT_RUN_TIMEOUT_SECONDS = 30 * 60
 
   def __init__(self):
     """Configures a sandwich runner out of the box.
@@ -181,30 +183,32 @@ class SandwichRunner(object):
     if self.wpr_record or self.cache_operation == CacheOperation.SAVE:
       stop_delay_multiplier = self._STOP_DELAY_MULTIPLIER
     # TODO(gabadie): add a way to avoid recording a trace.
-    with self._chrome_ctl.Open() as connection:
-      if clear_cache:
-        connection.ClearCache()
-
-      # Binds all parameters of RecordUrlNavigation() to avoid repetition.
-      def RecordTrace():
-        return loading_trace.LoadingTrace.RecordUrlNavigation(
-            url=self.url,
-            connection=connection,
-            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            categories=categories,
-            timeout_seconds=_DEVTOOLS_TIMEOUT,
-            stop_delay_multiplier=stop_delay_multiplier)
-
-      if run_path is not None and self.record_video:
-        device = self._chrome_ctl.GetDevice()
-        if device is None:
-          raise RuntimeError('Can only record video on a remote device.')
-        video_recording_path = os.path.join(run_path, VIDEO_FILENAME)
-        with device_setup.RemoteSpeedIndexRecorder(device, connection,
-                                                   video_recording_path):
+    with common_util.TimeoutScope(
+        self._ABORT_RUN_TIMEOUT_SECONDS, 'Sandwich run overdue.'):
+      with self._chrome_ctl.Open() as connection:
+        if clear_cache:
+          connection.ClearCache()
+
+        # Binds all parameters of RecordUrlNavigation() to avoid repetition.
+        def RecordTrace():
+          return loading_trace.LoadingTrace.RecordUrlNavigation(
+              url=self.url,
+              connection=connection,
+              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+              categories=categories,
+              timeout_seconds=_DEVTOOLS_TIMEOUT,
+              stop_delay_multiplier=stop_delay_multiplier)
+
+        if run_path is not None and self.record_video:
+          device = self._chrome_ctl.GetDevice()
+          if device is None:
+            raise RuntimeError('Can only record video on a remote device.')
+          video_recording_path = os.path.join(run_path, VIDEO_FILENAME)
+          with device_setup.RemoteSpeedIndexRecorder(device, connection,
+                                                     video_recording_path):
+            trace = RecordTrace()
+        else:
           trace = RecordTrace()
-      else:
-        trace = RecordTrace()
     if run_path is not None:
       trace_path = os.path.join(run_path, TRACE_FILENAME)
       trace.ToJsonFile(trace_path)
diff --git a/loading/task_manager.py b/loading/task_manager.py
index eb3fec5..a82becc 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -62,6 +62,7 @@ Example:
 import argparse
 import collections
 import datetime
+import errno
 import logging
 import os
 import re
@@ -424,7 +425,9 @@ class _ResumingFileBuilder(object):
     # Log the succeed tasks so that they are ensured to be frozen in case
     # of a sudden death.
     self._resume_output.write('-f\n^{}$\n'.format(re.escape(task.name)))
+    # Makes sure the task freezing command line make it to the disk.
     self._resume_output.flush()
+    os.fsync(self._resume_output.fileno())
 
   def OnScenarioFinish(
       self, scenario, final_tasks, failed_tasks, skipped_tasks):
@@ -511,7 +514,16 @@ def ExecuteWithCommandLine(args, default_final_tasks):
           task.Execute()
         except (MemoryError, SyntaxError):
           raise
-        except (Exception, KeyboardInterrupt):
+        except BaseException:
+          # The resuming file being incrementally generated by
+          # resume_file_builder.OnTaskSuccess() is automatically fsynced().
+          # But resume_file_builder.OnScenarioFinish() completely rewrite
+          # this file with the mininal subset of task to freeze, and in case
+          # of an ENOSPC, we don't want to touch the resuming file at all so
+          # that it remains uncorrupted.
+          if (sys.exc_info()[0] == IOError and
+              sys.exc_info()[1].errno == errno.ENOSPC):
+            raise
           logging.exception('%s %s failed', '-' * 60, task.name)
           failed_tasks.append(task)
           if args.keep_going and sys.exc_info()[0] != KeyboardInterrupt:
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index 87d18ad..bdcb809 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -4,6 +4,7 @@
 
 import argparse
 import contextlib
+import errno
 import os
 import re
 import shutil
@@ -12,6 +13,7 @@ import sys
 import tempfile
 import unittest
 
+import common_util
 import task_manager
 
 
@@ -405,12 +407,28 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     def SimulateKillTask():
       self.task_execution_history.append(SimulateKillTask.name)
       raise MemoryError
+    @builder.RegisterTask('timeout_error', dependencies=[TaskD])
+    def SimulateTimeoutError():
+      self.task_execution_history.append(SimulateTimeoutError.name)
+      raise common_util.TimeoutError
+    @builder.RegisterTask('errno_ENOSPC', dependencies=[TaskD])
+    def SimulateENOSPC():
+      self.task_execution_history.append(SimulateENOSPC.name)
+      raise IOError(errno.ENOSPC, os.strerror(errno.ENOSPC))
+    @builder.RegisterTask('errno_EPERM', dependencies=[TaskD])
+    def SimulateEPERM():
+      self.task_execution_history.append(SimulateEPERM.name)
+      raise IOError(errno.EPERM, os.strerror(errno.EPERM))
 
     default_final_tasks = [TaskD, TaskE]
     if self.with_raise_exception_tasks:
-      default_final_tasks.append(RaiseExceptionTask)
-      default_final_tasks.append(RaiseKeyboardInterruptTask)
-      default_final_tasks.append(SimulateKillTask)
+      default_final_tasks.extend([
+          RaiseExceptionTask,
+          RaiseKeyboardInterruptTask,
+          SimulateKillTask,
+          SimulateTimeoutError,
+          SimulateENOSPC,
+          SimulateEPERM])
     task_parser = task_manager.CommandLineParser()
     parser = argparse.ArgumentParser(parents=[task_parser],
         fromfile_prefix_chars=task_manager.FROMFILE_PREFIX_CHARS)
@@ -522,6 +540,31 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     with open(self.ResumeFilePath()) as resume_input:
       self.assertEqual(EXPECTED_RESUME_FILE_CONTENT, resume_input.read())
 
+  def testTimeoutError(self):
+    self.with_raise_exception_tasks = True
+    self.Execute(['-k', '-e', 'timeout_error', '-e', r'^b$'])
+    self.assertListEqual(['a', 'd', 'timeout_error', 'b'],
+                         self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual('-f\n^d$\n-f\n^b$', resume_input.read())
+
+  def testENOSPC(self):
+    self.with_raise_exception_tasks = True
+    with self.assertRaises(IOError):
+      self.Execute(['-k', '-e', 'errno_ENOSPC', '-e', r'^a$'])
+    self.assertListEqual(
+        ['a', 'd', 'errno_ENOSPC'], self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual('-f\n^a$\n-f\n^d$\n', resume_input.read())
+
+  def testEPERM(self):
+    self.with_raise_exception_tasks = True
+    self.Execute(['-k', '-e', 'errno_EPERM', '-e', r'^b$'])
+    self.assertListEqual(['a', 'd', 'errno_EPERM', 'b'],
+                         self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual('-f\n^d$\n-f\n^b$', resume_input.read())
+
   def testImpossibleTasks(self):
     self.assertEqual(1, self.Execute(['-f', r'^a$', '-e', r'^c$']))
     self.assertListEqual([], self.task_execution_history)

commit 977f4214f04eaa699f37e85be997f82f1a6a99f3
Author: gabadie <gabadie@chromium.org>
Date:   Tue Jun 21 10:54:35 2016 -0700

    sandwich: Add support for time to first meaningful paint.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2083593002
    Cr-Original-Commit-Position: refs/heads/master@{#401039}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e7ffde8f31410db12752d3326a56dcc393266b74

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 57ef0cc..853a531 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -47,6 +47,7 @@ OPTIONS = options.OPTIONS
 
 _SPEED_INDEX_MEASUREMENT = 'speed-index'
 _MEMORY_MEASUREMENT = 'memory'
+_TTFMP_MEASUREMENT = 'ttfmp'
 _CORPUS_DIR = 'sandwich_corpuses'
 
 
@@ -114,7 +115,9 @@ def _ArgumentParser():
   run_parser.add_argument('-c', '--corpus', required=True,
       help='Path to a JSON file with a corpus such as in %s/.' % _CORPUS_DIR)
   run_parser.add_argument('-m', '--measure', default=[], nargs='+',
-      choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
+      choices=[_SPEED_INDEX_MEASUREMENT,
+               _MEMORY_MEASUREMENT,
+               _TTFMP_MEASUREMENT],
       dest='optional_measures', help='Enable optional measurements.')
   run_parser.add_argument('--wpr-archive', default=None, type=str,
                           dest='wpr_archive_path',
@@ -177,6 +180,8 @@ def _GenerateBenchmarkTasks(args, android_device, url, output_subdirectory):
   def MainTransformer(runner):
     runner.record_video = _SPEED_INDEX_MEASUREMENT in args.optional_measures
     runner.record_memory_dumps = _MEMORY_MEASUREMENT in args.optional_measures
+    runner.record_first_meaningful_paint = (
+        _TTFMP_MEASUREMENT in args.optional_measures)
     runner.repeat = args.url_repeat
 
   if not args.swr_benchmark:
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index f46e79a..8e5544e 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -12,6 +12,7 @@ import json
 import logging
 import os
 import shutil
+import subprocess
 import sys
 import tempfile
 
@@ -26,6 +27,7 @@ from telemetry.internal.image_processing import video
 from telemetry.util import image_util
 from telemetry.util import rgba_color
 
+import common_util
 import loading_trace as loading_trace_module
 import sandwich_runner
 import tracing
@@ -36,6 +38,7 @@ COMMON_CSV_COLUMN_NAMES = [
     'platform',
     'first_layout',
     'first_contentful_paint',
+    'first_meaningful_paint',
     'total_load',
     'js_onload_event',
     'browser_malloc_avg',
@@ -181,6 +184,36 @@ def _ExtractDefaultMetrics(loading_trace):
   return metrics
 
 
+def _ExtractTimeToFirstMeaningfulPaint(loading_trace):
+  """Extracts the time to first meaningful paint from a given trace.
+
+  Args:
+    loading_trace: loading_trace_module.LoadingTrace.
+
+  Returns:
+    Time to first meaningful paint in milliseconds.
+  """
+  required_categories = set(sandwich_runner.TTFMP_ADDITIONAL_CATEGORIES)
+  if not required_categories.issubset(loading_trace.tracing_track.Categories()):
+    return _UNAVAILABLE_CSV_VALUE
+  logging.info('  Extracting first_meaningful_paint')
+  events = [e.ToJsonDict() for e in loading_trace.tracing_track.GetEvents()]
+  with common_util.TemporaryDirectory(prefix='sandwich_tmp_') as tmp_dir:
+    chrome_trace_path = os.path.join(tmp_dir, 'chrome_trace.json')
+    with open(chrome_trace_path, 'w') as output_file:
+      json.dump({'traceEvents': events, 'metadata': {}}, output_file)
+    catapult_run_metric_bin_path = os.path.join(
+        _SRC_DIR, 'third_party', 'catapult', 'tracing', 'bin', 'run_metric')
+    output = subprocess.check_output(
+        [catapult_run_metric_bin_path, 'firstPaintMetric', chrome_trace_path])
+  json_output = json.loads(output)
+  for metric in json_output[chrome_trace_path]['pairs']['values']:
+    if metric['name'] == 'firstMeaningfulPaint_avg':
+      return metric['numeric']['value']
+  logging.info('  Extracting first_meaningful_paint: failed')
+  return _FAILED_CSV_VALUE
+
+
 def _ExtractMemoryMetrics(loading_trace):
   """Extracts all the memory metrics from a given trace.
 
@@ -292,6 +325,8 @@ def ExtractCommonMetricsFromRepeatDirectory(repeat_dir, trace):
   }
   run_metrics.update(_ExtractDefaultMetrics(trace))
   run_metrics.update(_ExtractMemoryMetrics(trace))
+  run_metrics['first_meaningful_paint'] = _ExtractTimeToFirstMeaningfulPaint(
+      trace)
   video_path = os.path.join(repeat_dir, sandwich_runner.VIDEO_FILENAME)
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index eea8a45..815ccc0 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -51,6 +51,11 @@ _TRACING_CATEGORIES = [
   '-cc',  # A lot of unnecessary events are enabled by default in "cc".
 ]
 
+TTFMP_ADDITIONAL_CATEGORIES = [
+  'loading',
+  'disabled-by-default-blink.debug.layout',
+]
+
 def _CleanArtefactsFromPastRuns(output_directories_path):
   """Cleans artifacts generated from past run in the output directory.
 
@@ -120,6 +125,9 @@ class SandwichRunner(object):
     # Configures whether to record memory dumps.
     self.record_memory_dumps = False
 
+    # Configures whether to record tracing categories needed for TTFMP.
+    self.record_first_meaningful_paint = False
+
     # Path to the WPR archive to load or save. Is str or None.
     self.wpr_archive_path = None
 
@@ -167,6 +175,8 @@ class SandwichRunner(object):
     categories = _TRACING_CATEGORIES
     if self.record_memory_dumps:
       categories += [MEMORY_DUMP_CATEGORY]
+    if self.record_first_meaningful_paint:
+      categories += TTFMP_ADDITIONAL_CATEGORIES
     stop_delay_multiplier = 0
     if self.wpr_record or self.cache_operation == CacheOperation.SAVE:
       stop_delay_multiplier = self._STOP_DELAY_MULTIPLIER

commit df1fefb956574ecc87d0c0735bda2b36a98257a1
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 15 04:39:06 2016 -0700

    tools/android/loading: Let the task manager accept to freeze unexecuted tasks
    
    Before, the task_manager would raise an exception if trying to freeze
    from the command line a task that have not executed before.
    
    This can be anoying when doing post processing Sandwich command line runs
    on an output directory that has runned with --keep-going and had some
    task failures.
    
    This CL fixes this issue by marking the tasks to freeze and all their
    direct and indirect depending tasks as impossible to run, and then
    removing these impossible to run tasks from the list of final tasks.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2061393003
    Cr-Original-Commit-Position: refs/heads/master@{#399884}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e3b2ed27145af6ef38014c8b3a9f0bb0e1cb2d8b

diff --git a/loading/task_manager.py b/loading/task_manager.py
index 37466e9..eb3fec5 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -343,7 +343,7 @@ def CommandLineParser():
   """
   parser = argparse.ArgumentParser(add_help=False)
   parser.add_argument('-d', '--dry-run', action='store_true',
-                      help='Only prints the deps of tasks to build.')
+                      help='Only prints the tasks to build.')
   parser.add_argument('-e', '--to-execute', metavar='REGEX', type=str,
                       action='append', dest='run_regexes', default=[],
                       help='Regex selecting tasks to execute.')
@@ -381,13 +381,27 @@ def _SelectTasksFromCommandLineRegexes(args, default_final_tasks):
 
   # Lists parents of |final_tasks| to freeze.
   frozen_tasks = set()
+  impossible_tasks = set()
   if frozen_regexes:
-    for task in GenerateScenario(final_tasks, frozen_tasks=set()):
+    complete_scenario = GenerateScenario(final_tasks, frozen_tasks=set())
+    dependents_per_task = GenerateDependentSetPerTask(complete_scenario)
+    def MarkTaskAsImpossible(task):
+      if task in impossible_tasks:
+        return
+      impossible_tasks.add(task)
+      for dependent in dependents_per_task[task]:
+        MarkTaskAsImpossible(dependent)
+
+    for task in complete_scenario:
       for regex in frozen_regexes:
         if regex.search(task.name):
-          frozen_tasks.add(task)
+          if os.path.exists(task.path):
+            frozen_tasks.add(task)
+          else:
+            MarkTaskAsImpossible(task)
           break
-  return final_tasks, frozen_tasks
+
+  return [t for t in final_tasks if t not in impossible_tasks], frozen_tasks
 
 
 class _ResumingFileBuilder(object):
@@ -463,8 +477,7 @@ def ExecuteWithCommandLine(args, default_final_tasks):
   # Use the build scenario.
   if args.dry_run:
     for task in scenario:
-      print '{}:{}'.format(
-          task.name, ' '.join([' \\\n  ' + d.name for d in task._dependencies]))
+      print task.name
     return 0
 
   # Run the Scenario while saving intermediate state to be able to resume later.
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index b4b8742..87d18ad 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -522,6 +522,14 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     with open(self.ResumeFilePath()) as resume_input:
       self.assertEqual(EXPECTED_RESUME_FILE_CONTENT, resume_input.read())
 
+  def testImpossibleTasks(self):
+    self.assertEqual(1, self.Execute(['-f', r'^a$', '-e', r'^c$']))
+    self.assertListEqual([], self.task_execution_history)
+
+    self.assertEqual(0, self.Execute(
+        ['-f', r'^a$', '-e', r'^c$', '-e', r'^b$']))
+    self.assertListEqual(['b'], self.task_execution_history)
+
 
 if __name__ == '__main__':
   unittest.main()

commit 0081d45ca958e6aee2fa5177e17957efa4d424a1
Author: smaier <smaier@chromium.org>
Date:   Mon Jun 13 15:14:44 2016 -0700

    Dexdiffer scripts
    
    Tools to allow us to diff between similiar dex files (using dextra to put them
    into a readable form). This helps when checking proguard configs to see what
    they are actually doing.
    
    BUG=619124
    
    Review-Url: https://codereview.chromium.org/2057323002
    Cr-Original-Commit-Position: refs/heads/master@{#399568}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b6dc58c6ce55c6691e21cd176983823842f70fef

diff --git a/dexdiffer/OWNERS b/dexdiffer/OWNERS
new file mode 100644
index 0000000..29e1a00
--- /dev/null
+++ b/dexdiffer/OWNERS
@@ -0,0 +1,2 @@
+smaier@chromium.org
+agrieve@chromium.org
diff --git a/dexdiffer/dexdiffer.py b/dexdiffer/dexdiffer.py
new file mode 100644
index 0000000..d505c24
--- /dev/null
+++ b/dexdiffer/dexdiffer.py
@@ -0,0 +1,276 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+"""Tool to diff 2 dex files that have been proguarded.
+
+To use this tool, first get dextra. http://newandroidbook.com/tools/dextra.html
+Then use the dextra binary on a classes.dex file like so:
+  dextra_binary -j -f -m classes.dex > output.dextra
+Do this for both the dex files you want to compare. Then, take the appropriate
+proguard mapping files uesd to generate those dex files, and use this script:
+  python dexdiffer.py mappingfile1 output1.dextra mappingfile2 output2.dextra
+"""
+
+import argparse
+import re
+import sys
+
+
+_QUALIFIERS = set(['public', 'protected', 'private', 'final', 'static',
+                   'abstract', 'volatile', 'native', 'enum'])
+
+
+def _IsNewClass(line):
+  return line.endswith(':')
+
+
+# Expects lines like one of these 3:
+# 'android.support.v8.MenuPopupHelper -> android.support.v8.v:'
+# '    android.view.LayoutInflater mInflater -> d'
+# '    117:118:void setForceShowIcon(boolean) -> b'
+# Those three examples would return
+# 'android.support.v8.MenuPopupHelper', 'android.support.v8.v'
+# 'android.view.LayoutInflater mInflater', 'android.view.LayoutInflater d'
+# 'void setForceShowIcon(boolean)', 'void b(boolean)'
+def _ParseMappingLine(line):
+  line = line.rstrip(':')
+
+  # Stripping any line number denotations
+  line = re.sub(r'\d+:\d+:', '', line)
+  line = re.sub(r'\):\d+', ')', line)
+
+  original_name, new_name = line.split(' -> ')
+
+  type_string = ''
+  if ' ' in original_name:
+    type_string = original_name[:original_name.find(' ') + 1]
+
+  arguments_string = ''
+  match = re.search(r'(\(.*?\))', original_name)
+  if match:
+    arguments_string = match.group(1)
+
+  return original_name, type_string + new_name + arguments_string
+
+
+def _ReadMappingDict(mapping_file):
+  mapping = {}
+  renamed_class_name = ''
+  original_class_name = ''
+  for line in mapping_file:
+    line = line.strip()
+    if _IsNewClass(line):
+      if renamed_class_name:
+        mapping[renamed_class_name] = current_entry
+
+      member_mappings = {}
+      original_class_name, renamed_class_name = _ParseMappingLine(line)
+      current_entry = [original_class_name, member_mappings]
+    else:
+      original_member_name, renamed_member_name = _ParseMappingLine(line)
+      member_mappings[renamed_member_name] = original_member_name
+
+  mapping[renamed_class_name] = current_entry
+  return mapping
+
+
+def _StripComments(string):
+  # Remove all occurances of multiline comments (/*COMMENT*/)
+  string = re.sub(r'/\*.*?\*/', "", string, flags=re.DOTALL)
+  # Remove all occurances of single line comments (//COMMENT)
+  string = re.sub(r'//.*?$', "", string)
+  return string
+
+
+def _StripQuotes(string):
+  return re.sub(r'([\'"]).*?\1', '', string)
+
+
+def _RemoveQualifiers(string_tokens):
+  while string_tokens and string_tokens[0] in _QUALIFIERS:
+    string_tokens = string_tokens[1:]
+  return string_tokens
+
+
+def _GetLineTokens(line):
+  line = _StripComments(line)
+  # Match all alphanumeric + underscore with \w then cases for:
+  # '$', '<', '>', '{', '}', '[', ']', and '.'
+  tokens = re.findall(r'[\w\$\.<>\{\}\[\]]+', line)
+  return _RemoveQualifiers(tokens)
+
+
+def _IsClassDefinition(line_tokens):
+  return line_tokens and line_tokens[0] == 'class'
+
+
+def _IsEndOfClass_definition(line_tokens):
+  return line_tokens and line_tokens[-1] == '{'
+
+
+def _IsEndOfClass(line_tokens):
+  return line_tokens and line_tokens[-1] == '}'
+
+
+def _TypeLookup(renamed_type, mapping_dict):
+  renamed_type_stripped = renamed_type.strip('[]')
+  postfix = renamed_type.replace(renamed_type_stripped, '')
+
+  if renamed_type_stripped in mapping_dict:
+    real_type = mapping_dict[renamed_type_stripped][0]
+  else:
+    real_type = renamed_type_stripped
+
+  return real_type + postfix
+
+
+def _GetMemberIdentifier(line_tokens, mapping_dict, renamed_class_name,
+                         is_function):
+  assert len(line_tokens) > 1
+  assert renamed_class_name in mapping_dict
+  mapping_entry = mapping_dict[renamed_class_name][1]
+
+  renamed_type = line_tokens[0]
+  real_type = _TypeLookup(renamed_type, mapping_dict)
+
+  renamed_name_token = line_tokens[1]
+  renamed_name_token, _, _ = renamed_name_token.partition('=')
+
+  function_args = ''
+  if is_function:
+    function_args += '('
+    for token in line_tokens[2:]:
+      function_args += _TypeLookup(token, mapping_dict) + ','
+    # Remove trailing ','
+    function_args = function_args.rstrip(',')
+    function_args += ')'
+
+  renamed_member_identifier = (real_type + ' ' + renamed_name_token
+                               + function_args)
+  if renamed_member_identifier not in mapping_entry:
+    print 'Proguarded class which caused the issue:', renamed_class_name
+    print 'Key supposed to be in this dict:',  mapping_entry
+    print 'Definition line tokens:', line_tokens
+
+  # This will be the real type + real_identifier + any real function args (if
+  # applicable)
+  return mapping_entry[renamed_member_identifier]
+
+
+def _GetClassNames(line_tokens, mapping_dict):
+  assert len(line_tokens) > 1
+  assert line_tokens[1] in mapping_dict
+  return line_tokens[1], mapping_dict[line_tokens[1]][0]
+
+
+def _IsLineFunctionDefinition(line):
+  line = _StripComments(line)
+  line = _StripQuotes(line)
+  return line.find('(') > 0 and line.find(')') > 0
+
+
+# Expects data from dextra -j -m -f
+# Returns dictionary mapping class name to list of members
+def _BuildMappedDexDict(dextra_file, mapping_dict):
+  # Have to add 'bool' -> 'boolean' mapping in dictionary, since for some reason
+  # dextra shortens boolean to bool.
+  mapping_dict['bool'] = ['boolean', {}]
+  dex_dict = {}
+  current_entry = []
+  reading_class_header = True
+  unmatched_string = False
+
+  for line in dextra_file:
+    # Accounting for multi line strings
+    if line.count('"') % 2:
+      unmatched_string = not unmatched_string
+      continue
+    if unmatched_string:
+      continue
+
+    line_tokens = _GetLineTokens(line)
+    if _IsClassDefinition(line_tokens):
+      reading_class_header = True
+      renamed_class_name, real_class_name = _GetClassNames(line_tokens,
+                                                           mapping_dict)
+    if _IsEndOfClass_definition(line_tokens):
+      reading_class_header = False
+      continue
+    if _IsEndOfClass(line_tokens):
+      dex_dict[real_class_name] = current_entry
+      current_entry = []
+      continue
+
+    if not reading_class_header and line_tokens:
+      is_function = _IsLineFunctionDefinition(line)
+      member = _GetMemberIdentifier(line_tokens, mapping_dict,
+                                    renamed_class_name, is_function)
+      current_entry.append(member)
+
+  return dex_dict
+
+
+def _DiffDexDicts(dex_base, dex_new):
+  diffs = []
+  for key, base_class_members in dex_base.iteritems():
+    if key in dex_new:
+      # Class in both
+      base_class_members_set = set(base_class_members)
+      # Removing from dex_new to have just those which only appear in dex_new
+      # left over.
+      new_class_members_set = set(dex_new.pop(key))
+      if base_class_members_set == new_class_members_set:
+        continue
+      else:
+        # They are not equal
+        diff_string = key
+        for diff in base_class_members_set.difference(new_class_members_set):
+          # Base has stuff the new one doesn't
+          diff_string += '\n' + '-  ' + diff
+        for diff in new_class_members_set.difference(base_class_members_set):
+          # New has stuff the base one doesn't
+          diff_string += '\n' + '+  ' + diff
+        diffs.append(diff_string)
+    else:
+      # Class not found in new
+      diff_string = '-class ' + key
+      diffs.append(diff_string)
+  if dex_new:
+    # Classes in new that have yet to be hit by base
+    for key in dex_new:
+      diff_string = '+class ' + key
+      diffs.append(diff_string)
+
+  return diffs
+
+
+def main():
+  parser = argparse.ArgumentParser()
+  parser.add_argument('base_mapping_file',
+                      help='Mapping file from proguard output for base dex')
+  parser.add_argument('base_dextra_output',
+                      help='dextra -j -f -m output for base dex')
+  parser.add_argument('new_mapping_file',
+                      help='Mapping file from proguard output for new dex')
+  parser.add_argument('new_dextra_output',
+                      help='dextra -j -f -m output for new dex')
+  args = parser.parse_args()
+
+  with open(args.base_mapping_file) as f:
+    mapping_base = _ReadMappingDict(f)
+  with open(args.base_dextra_output) as f:
+    dex_base = _BuildMappedDexDict(f, mapping_base)
+  with open(args.new_mapping_file) as f:
+    mapping_new = _ReadMappingDict(f)
+  with open(args.new_dextra_output) as f:
+    dex_new = _BuildMappedDexDict(f, mapping_new)
+
+  diffs = _DiffDexDicts(dex_base, dex_new)
+  if diffs:
+    for diff in diffs:
+      print diff
+
+
+if __name__ == '__main__':
+  main()
+
diff --git a/dexdiffer/dexdiffer_unittest.py b/dexdiffer/dexdiffer_unittest.py
new file mode 100644
index 0000000..4fd635d
--- /dev/null
+++ b/dexdiffer/dexdiffer_unittest.py
@@ -0,0 +1,186 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+"""Tests for dexdiffer."""
+
+import dexdiffer
+import json
+import unittest
+
+
+class DexdifferTest(unittest.TestCase):
+
+  def testReadDict(self):
+    mapping_file = [
+        'package.ClassName -> package.qq:\n',
+        'android.support.v8.MenuPopupHelper -> android.support.v8.v:',
+        '    android.view.LayoutInflater mInflater -> d\n',
+        '    117:118:void setForceShowIcon(boolean) -> b',
+        '    1:1:package.ClassName <init>(int,int) -> <init>',
+    ]
+    expected = {
+        'package.qq': ['package.ClassName', {}],
+        'android.support.v8.v':
+        ['android.support.v8.MenuPopupHelper',
+         {'android.view.LayoutInflater d':
+              'android.view.LayoutInflater mInflater',
+          'void b(boolean)': 'void setForceShowIcon(boolean)',
+          'package.ClassName <init>(int,int)':
+              'package.ClassName <init>(int,int)'}],
+    }
+
+    actual = dexdiffer._ReadMappingDict(mapping_file)
+    self.assertDeepEqual(actual, expected)
+
+  def testGetLineTokens1(self):
+    line = '        public  void <init> (android.content.Context,float); // Cxr'
+    expected = ['void', '<init>', 'android.content.Context', 'float']
+    actual = dexdiffer._GetLineTokens(line)
+    self.assertDeepEqual(actual, expected)
+
+  def testGetLineTokens2(self):
+    line = '/** asdfasdf /**/ private final static   /*asdf*/int varname$1;'
+    expected = ['int', 'varname$1']
+    actual = dexdiffer._GetLineTokens(line)
+    self.assertDeepEqual(actual, expected)
+
+  def testGetLineTokens3(self):
+    line = 'int[] varname_1;'
+    expected = ['int[]', 'varname_1']
+    actual = dexdiffer._GetLineTokens(line)
+    self.assertDeepEqual(actual, expected)
+
+  def testGetLineTokensEmpty(self):
+    line = '/***/   /*asdf*/ //comment;'
+    expected = []
+    actual = dexdiffer._GetLineTokens(line)
+    self.assertDeepEqual(actual, expected)
+
+  def testGetMemberIdentifier(self):
+    line_tokens = ['void', 'b', 'boolean', 'rnmd[]']
+    expected = 'void foo(boolean,renamed.type[])'
+    renamed_class_name = 'renamed.class'
+    mapping_dict = { renamed_class_name : [ 'actual.class.name', {
+        'void b': 'void variable_name',
+        'boolean a': 'boolean variable_name2',
+        'void b(boolean,renamed.type)':
+            'void wrong_function(boolean,renamed.type)',
+        'void b(boolean,renamed.type[])': expected,
+    }], 'rnmd': ['renamed.type', {}]}
+    actual = dexdiffer._GetMemberIdentifier(line_tokens, mapping_dict,
+                                             renamed_class_name, True)
+    self.assertEqual(expected, actual)
+
+  def testIsLineFunctionDefinition(self):
+    line = 'java.lang.String  CONSOLE_ELISION= "[(0)]"'
+    self.assertFalse(dexdiffer._IsLineFunctionDefinition(line))
+
+  def testStripQuotes(self):
+    string = 'abc\'123\'"456"def\'"7"\''
+    self.assertEqual('abcdef', dexdiffer._StripQuotes(string))
+
+  def testBuildMappedDexDict(self):
+    dextra_output = [
+'/* 3396 */ public class   org.chromium.chrome.browser.widget.a',
+'           extends android.view.View    {',
+'         /** 2 Instance Fields  **/',
+'          private  org.chromium.chrome.browser.widget.b$1      x;',
+'          private  int  mPosition;',
+'         /** 1 Direct Methods  **/',
+'         public  void <init> (android.content.Context, android.util.Attribute'
+              'Set); // Constructor',
+'         /** 2 Virtual Methods  **/',
+'         public  void init (int, int);',
+'         protected  void onDraw (android.graphics.Canvas);',
+'        }  // end class org.chromium.chrome.browser.widget.a',
+'/* 3397 */ class   org.chromium.chrome.browser.widget.b$1',
+'           implements android.text.TextWatcher  {',
+'         /** 1 Instance Fields  **/',
+'          final  org.chromium.chrome.browser.widget.FloatLabelLayout'
+              '    this$0;',
+'         /** 1 Direct Methods  **/',
+'          bool <init> (); // Constructor',
+'         /** 1 Virtual Methods  **/',
+'         public  void x (org.chromium.chrome.browser.widget.a, int, int,'
+              ' int);',
+'        }  // end class org.chromium.chrome.browser.widget.b$1']
+    mapping_dict = {
+        'org.chromium.chrome.browser.widget.a': ['class_name_a', {
+            'class_name_b$1 x': 'class_name_b$1 full_member_variable',
+            'int mPosition': 'int mPosition',
+            'void <init>(android.content.Context,android.util.AttributeSet)':
+              'void <init>(android.content.Context,android.util.AttributeSet)',
+            'void init(int,int)': 'void init(int,int)',
+            'void onDraw(android.graphics.Canvas)':
+                'void onDraw(android.graphics.Canvas)',
+        }],
+        'org.chromium.chrome.browser.widget.b$1': ['class_name_b$1', {
+            'org.chromium.chrome.browser.widget.FloatLabelLayout this$0':
+              'org.chromium.chrome.browser.widget.FloatLabelLayout this$0',
+            'boolean <init>()': 'boolean <init>()',
+            'void x(class_name_a,int,int,int)':
+              'void full_function_name(class_name_a,int,int,int)'
+          }],
+    }
+    actual = dexdiffer._BuildMappedDexDict(dextra_output, mapping_dict)
+    expected = { 'class_name_a': [
+        'class_name_b$1 full_member_variable',
+        'int mPosition',
+        'void <init>(android.content.Context,android.util.AttributeSet)',
+        'void init(int,int)',
+        'void onDraw(android.graphics.Canvas)'
+    ], 'class_name_b$1': [
+        'org.chromium.chrome.browser.widget.FloatLabelLayout this$0',
+        'boolean <init>()',
+        'void full_function_name(class_name_a,int,int,int)'
+    ]}
+    self.assertDeepEqual(actual, expected)
+
+  def testParseMappingLine(self):
+    orig_name = "abc.q12$1"
+    new_name = "a$1"
+    line = orig_name + " -> " + new_name
+    actual_orig_name, actual_new_name = dexdiffer._ParseMappingLine(line)
+    self.assertEqual(orig_name, actual_orig_name)
+    self.assertEqual(new_name, actual_new_name)
+
+  def testDiffDexDicts(self):
+    base_dict = { 'class_name_a': [
+        'class_name_b$1 full_member_variable',
+        'int mPosition',
+        'void <init>(android.content.Context,android.util.AttributeSet)',
+        'void init(int,int)',
+        'void onDraw(android.graphics.Canvas)'
+    ], 'class_name_b$1': [
+        'org.chromium.chrome.browser.widget.FloatLabelLayout this$0',
+        'void <init>()',
+        'void full_function_name(class_name_a,int,int)'
+    ], 'class_name_deleted': []}
+    new_dict = { 'class_name_a': [
+        'class_name_b$1 full_member_variable',
+        'int mPosition',
+        'void <init>(android.content.Context,android.util.AttributeSet)',
+        'void init(int,int)',
+        'void onDraw(android.graphics.Canvas)'
+    ], 'class_name_b$1': [
+        'org.chromium.chrome.browser.widget.FloatLabelLayout this$0',
+        'void <init>()',
+        'void full_function_name(int,int,int)'
+    ], 'class_name_new': []}
+    actual = dexdiffer._DiffDexDicts(base_dict, new_dict)
+    expected = [('class_name_b$1\n'
+                 '-  void full_function_name(class_name_a,int,int)\n'
+                 '+  void full_function_name(int,int,int)'),
+                '-class class_name_deleted',
+                '+class class_name_new']
+
+    self.assertDeepEqual(actual, expected)
+
+  def assertDeepEqual(self, actual, expected):
+    # Only designed to work for json-able types
+    a = json.dumps(actual, sort_keys=True)
+    e = json.dumps(expected, sort_keys=True)
+    self.assertEqual(a, e)
+
+if __name__ == '__main__':
+  unittest.main()

commit 6656bb048abdc42de5bf33dfc90ff91de9f55bc1
Author: droger <droger@chromium.org>
Date:   Mon Jun 13 05:28:16 2016 -0700

    tools/android/loading Add delay before deleting worker instances
    
    Add a random delay before destroying the worker instances. This is done
    to avoid a ComputeEngine issue that happens when too many instances are
    destroyed at the same time.
    
    Review-Url: https://codereview.chromium.org/2065573002
    Cr-Original-Commit-Position: refs/heads/master@{#399438}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 02f9011efb86d2a63e731d114a1bf84ec1f987c9

diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index b27ef35..6b3a369 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -6,6 +6,7 @@ import argparse
 import json
 import logging
 import os
+import random
 import sys
 import time
 
@@ -188,6 +189,12 @@ class Worker(object):
                                                remote_log_path)
     # Self destruct.
     if self._self_destruct:
+      # Workaround for ComputeEngine internal bug b/28760288.
+      random_delay = random.random() * 600.0  # Up to 10 minutes.
+      self._logger.info(
+          'Wait %.0fs to avoid load spikes on compute engine.' % random_delay)
+      time.sleep(random_delay)
+
       self._logger.info('Starting instance destruction: ' + self._instance_name)
       google_instance_helper = GoogleInstanceHelper(
           self._credentials, self._project_name, self._logger)

commit b30d5867ad2d9adeada80e655f07fb7b608208c5
Author: droger <droger@chromium.org>
Date:   Sat Jun 11 09:08:23 2016 -0700

    tools/android/loading Add link to dashboard and a way to cancel tasks.
    
    Review-Url: https://codereview.chromium.org/2053363002
    Cr-Original-Commit-Position: refs/heads/master@{#399372}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2b296fbd6a2e7426ce347b9af6fbeb8b21f3f301

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 468f35d..675e2af 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -38,7 +38,7 @@ instance_helper = common.google_instance_helper.GoogleInstanceHelper(
 app = flask.Flask(__name__)
 
 
-def Render(message, memory_logs=None):
+def RenderJobCreationPage(message, memory_logs=None):
   """Renders the log.html template.
 
   Args:
@@ -48,7 +48,8 @@ def Render(message, memory_logs=None):
   log = None
   if memory_logs:
     log = memory_logs.Flush().split('\n')
-  return flask.render_template('log.html', body=message, log=log)
+  return flask.render_template('log.html', body=message, log=log,
+                               title='Job Creation Status')
 
 
 def PollWorkers(tag, start_time, timeout_hours, email_address, task_url):
@@ -325,7 +326,8 @@ def StartFromJsonString(http_body_str):
   task = ClovisTask.FromJsonString(http_body_str)
   if not task:
     clovis_logger.error('Invalid JSON task.')
-    return Render('Invalid JSON task:\n' + http_body_str, memory_logs)
+    return RenderJobCreationPage(
+        'Invalid JSON task:\n' + http_body_str, memory_logs)
 
   task_tag = task.BackendParams()['tag']
   clovis_logger.info('Start processing %s task with tag %s.' % (task.Action(),
@@ -342,7 +344,7 @@ def StartFromJsonString(http_body_str):
   # Process the job on the queue, to avoid timeout issues.
   deferred.defer(SpawnTasksOnBackgroundQueue, task_tag)
 
-  return Render(
+  return RenderJobCreationPage(
       flask.Markup(
           '<a href="%s">See progress.</a>' % FrontendJob.GetJobURL(task_tag)),
       memory_logs)
@@ -438,7 +440,7 @@ def SpawnTasks(frontend_job):
   if max_instances == -1:
     frontend_job.status = 'instance_count_error'
     return
-  elif task.BackendParams()['instance_count'] == 0:
+  elif max_instances == 0 and task.BackendParams()['instance_count'] > 0:
     frontend_job.status = 'no_instance_available_error'
     return
   elif max_instances < task.BackendParams()['instance_count']:
@@ -523,46 +525,71 @@ def StartFromForm():
   """HTML form endpoint."""
   data_stream = flask.request.files.get('json_task')
   if not data_stream:
-    return Render('Failed, no content.')
+    return RenderJobCreationPage('Failed, no content.')
   http_body_str = data_stream.read()
   return StartFromJsonString(http_body_str)
 
 
+@app.route('/kill_job')
+def KillJob():
+  tag = flask.request.args.get('tag')
+  page_title = 'Kill Job'
+  if not tag:
+    return flask.render_template('log.html', body='Failed: Invalid tag.',
+                                 title=page_title)
+
+  frontend_job = FrontendJob.GetFromTag(tag)
+
+  if not frontend_job:
+    return flask.render_template('log.html', body='Job not found.',
+                                 title=page_title)
+  Finalize(tag, frontend_job.email, 'CANCELED', frontend_job.task_url)
+
+  body = 'Killed job %s.' % tag
+  return flask.render_template('log.html', body=body, title=page_title)
+
+
 @app.route('/list_jobs')
-def ShowTaskList():
+def ShowJobList():
   """Shows a list of all active jobs."""
   tags = FrontendJob.ListJobs()
+  page_title = 'Active Jobs'
 
   if not tags:
-    Render('No active jobs.')
+    return flask.render_template('log.html', body='No active job.',
+                                 title=page_title)
 
-  html = flask.Markup('Active tasks:<ul>')
+  html = ''
   for tag in tags:
     html += flask.Markup(
         '<li><a href="%s">%s</a></li>') % (FrontendJob.GetJobURL(tag), tag)
   html += flask.Markup('</ul>')
-  return Render(html)
+  return flask.render_template('log.html', body=html, title=page_title)
 
 
 @app.route(FrontendJob.SHOW_JOB_URL)
-def ShowTask():
+def ShowJob():
   """Shows basic information abour a job."""
   tag = flask.request.args.get('tag')
+  page_title = 'Job Information'
   if not tag:
-    return Render('Invalid task tag.')
+    return flask.render_template('log.html', body='Invalid tag.',
+                                 title=page_title)
 
   frontend_job = FrontendJob.GetFromTag(tag)
 
   if not frontend_job:
-    return Render('Task not found.')
-
-  message = flask.Markup('Task details:' + frontend_job.RenderAsHtml())
+    return flask.render_template('log.html', body='Job not found.',
+                                 title=page_title)
 
   log = None
   if frontend_job.log:
     log = frontend_job.log.split('\n')
 
-  return flask.render_template('log.html', body=message, log=log)
+  body = flask.Markup(frontend_job.RenderAsHtml())
+  body += flask.Markup('<a href="/kill_job?tag=%s">Kill</a>' % tag)
+  return flask.render_template('log.html', log=log, title=page_title,
+                               body=body)
 
 
 @app.errorhandler(404)
diff --git a/loading/cloud/frontend/templates/base.html b/loading/cloud/frontend/templates/base.html
index 611f2e7..72d732c 100644
--- a/loading/cloud/frontend/templates/base.html
+++ b/loading/cloud/frontend/templates/base.html
@@ -18,7 +18,8 @@
   <div class="menu">
     <ul style="border: 1px solid #e7e7e7; background-color: #f3f3f3;">
       <li> <div> Clovis </div>
-      <li> <a href="/">New Task</a>
+      <li> <a href="/">New Job</a>
+      <li> <a href="/list_jobs">Active Jobs</a>
       <li> <a href="https://chromium.googlesource.com/chromium/src/+/master/tools/android/loading/cloud/frontend/README.md">
              Documentation
            </a>
diff --git a/loading/cloud/frontend/templates/log.html b/loading/cloud/frontend/templates/log.html
index d6b6dcf..9b43f87 100644
--- a/loading/cloud/frontend/templates/log.html
+++ b/loading/cloud/frontend/templates/log.html
@@ -10,7 +10,7 @@
 
 {% block content %}
 
-<h2> Task Creation Status </h2>
+<h2> {{ title }} </h2>
 
 {{ body }}
 

commit 2087aa1d8d0f23a18cc62fa8809fd26ab8b617e9
Author: droger <droger@chromium.org>
Date:   Fri Jun 10 07:36:55 2016 -0700

    tools/android/loading Reduce taskqueue access rate.
    
    This CL increases the number of URLs per ClovisTask, making them longer
    to process, and thus reducing the rate at which workers pull tasks from
    the queue.
    
    Review-Url: https://codereview.chromium.org/2059683002
    Cr-Original-Commit-Position: refs/heads/master@{#399178}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: de7d806a321ac7bf620c6de6d8d4cbe83f4a63fb

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index c0b142c..468f35d 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -210,7 +210,9 @@ def SplitClovisTask(task):
     task.ActionParams()['traces'] = traces
 
   # Compute the split key.
-  split_params_for_action = {'trace': ('urls', 1), 'report': ('traces', 5)}
+  # Keep the tasks small, but large enough to avoid "rate exceeded" errors when
+  # pulling them from the TaskQueue.
+  split_params_for_action = {'trace': ('urls', 3), 'report': ('traces', 5)}
   (split_key, slice_size) = split_params_for_action.get(task.Action(),
                                                         (None, 0))
   if not split_key:

commit dff8e44493805ce4d34145559a20b69d551918e9
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jun 10 07:33:41 2016 -0700

    clovis: Allow customization of the output table name in the report generation.
    
    Review-Url: https://codereview.chromium.org/2057073002
    Cr-Original-Commit-Position: refs/heads/master@{#399176}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 71272bfa3bfa51ee1760f6468832bf0b3f1c5e47

diff --git a/loading/cloud/common/google_bigquery_helper.py b/loading/cloud/common/google_bigquery_helper.py
index f1a7880..547c434 100644
--- a/loading/cloud/common/google_bigquery_helper.py
+++ b/loading/cloud/common/google_bigquery_helper.py
@@ -34,6 +34,9 @@ def GetBigQueryTableID(clovis_report_task):
   # Name the table after the last path component of the trace bucket.
   trace_bucket = clovis_report_task.ActionParams()['trace_bucket']
   table_id = os.path.basename(os.path.normpath(trace_bucket))
+  task_name = clovis_report_task.BackendParams().get('task_name')
+  if task_name is not None:
+    table_id += '_' + task_name
   # BigQuery table names can contain only alpha numeric characters and
   # underscores.
   return ''.join(c for c in table_id if c.isalnum() or c == '_')

commit f3f1df7f55398adedce067121d7e638357001f6f
Author: droger <droger@chromium.org>
Date:   Fri Jun 10 07:15:44 2016 -0700

    tools/android/loading Move task creation to background and add dashboard
    
    This CL moves the tasks creation to the background queue,
    to avoid AppEngine timeouts.
    
    It also adds a way to list the current jobs and track them.
    
    The list of jobs is persisted in the datastore.
    
    There is more work to be done in follow up CLs:
    - improving the UI for the job dashboard
    - possibly replacing the per-job polling by a global
      polling that would poll all active jobs at once, in order
      to simplify the code.
    
    Review-Url: https://codereview.chromium.org/2041193008
    Cr-Original-Commit-Position: refs/heads/master@{#399175}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4953d75f1ebef626ca9dfcc20630193dedcfb8ea

diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
index 739bb9c..97c88a5 100644
--- a/loading/cloud/common/clovis_task.py
+++ b/loading/cloud/common/clovis_task.py
@@ -30,16 +30,15 @@ class ClovisTask(object):
       self._backend_params.update({'tag': str(uuid.uuid1())})
 
   @classmethod
-  def FromJsonString(cls, json_dict):
-    """Loads a ClovisTask from a JSON string.
+  def FromJsonDict(cls, json_dict):
+    """Loads a ClovisTask from a JSON dictionary.
 
     Returns:
       ClovisTask: The task, or None if the string is invalid.
     """
     try:
-      data = json.loads(json_dict)
-      action = data['action']
-      action_params = data['action_params']
+      action = json_dict['action']
+      action_params = json_dict['action_params']
       # Vaidate the format.
       if action == 'trace':
         urls = action_params['urls']
@@ -51,7 +50,19 @@ class ClovisTask(object):
       else:
         # When more actions are supported, check that they are valid here.
         return None
-      return cls(action, action_params, data.get('backend_params'))
+      return cls(action, action_params, json_dict.get('backend_params'))
+    except Exception:
+      return None
+
+  @classmethod
+  def FromJsonString(cls, json_string):
+    """Loads a ClovisTask from a JSON string.
+
+    Returns:
+      ClovisTask: The task, or None if the string is invalid.
+    """
+    try:
+      return cls.FromJsonDict(json.loads(json_string))
     except Exception:
       return None
 
@@ -60,11 +71,14 @@ class ClovisTask(object):
     """Loads a ClovisTask from a base 64 string."""
     return ClovisTask.FromJsonString(base64.b64decode(base64_string))
 
+  def ToJsonDict(self):
+    """Returns the JSON representation of the task as a dictionary."""
+    return {'action': self._action, 'action_params': self._action_params,
+            'backend_params': self._backend_params}
+
   def ToJsonString(self):
-    """Returns the JSON representation of the task."""
-    task_dict = {'action': self._action, 'action_params': self._action_params,
-                 'backend_params': self._backend_params}
-    return json.dumps(task_dict)
+    """Returns the JSON representation of the task as a string."""
+    return json.dumps(self.ToJsonDict())
 
   def Action(self):
     return self._action
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 2543372..c0b142c 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -3,10 +3,12 @@
 # found in the LICENSE file.
 
 import logging
+import datetime
 import math
 import os
 import sys
 import time
+import traceback
 
 import cloudstorage
 import flask
@@ -20,6 +22,7 @@ import common.google_bigquery_helper
 import common.google_instance_helper
 from common.loading_trace_database import LoadingTraceDatabase
 import email_helper
+from frontend_job import FrontendJob
 from memory_logs import MemoryLogs
 
 
@@ -99,6 +102,7 @@ def Finalize(tag, email_address, status, task_url):
       logger=clovis_logger)
   clovis_logger.info('Scheduling instance group destruction for tag: ' + tag)
   deferred.defer(DeleteInstanceGroup, tag)
+  FrontendJob.DeleteForTag(tag)
 
 
 def GetEstimatedTaskDurationInSeconds(task):
@@ -324,9 +328,61 @@ def StartFromJsonString(http_body_str):
   task_tag = task.BackendParams()['tag']
   clovis_logger.info('Start processing %s task with tag %s.' % (task.Action(),
                                                                 task_tag))
+  user_email = email_helper.GetUserEmail()
+
+  # Write the job to the datastore.
+  frontend_job = FrontendJob.CreateForTag(task_tag)
+  frontend_job.email = user_email
+  frontend_job.status = 'not_started'
+  frontend_job.clovis_task = task.ToJsonString()
+  frontend_job.put()
+
+  # Process the job on the queue, to avoid timeout issues.
+  deferred.defer(SpawnTasksOnBackgroundQueue, task_tag)
+
+  return Render(
+      flask.Markup(
+          '<a href="%s">See progress.</a>' % FrontendJob.GetJobURL(task_tag)),
+      memory_logs)
+
+
+def SpawnTasksOnBackgroundQueue(task_tag):
+  """Spawns Clovis tasks associated with task_tag from the backgound queue.
+
+  This function is mostly a wrapper around SpawnTasks() that catches exceptions.
+  It is assumed that a FrontendJob for task_tag exists.
+  """
+  memory_logs = MemoryLogs(clovis_logger)
+  memory_logs.Start()
+  clovis_logger.info('Spawning tasks on background queue.')
+
+  try:
+    frontend_job = FrontendJob.GetFromTag(task_tag)
+    frontend_job.status = 'will_start'
+    SpawnTasks(frontend_job)
+  except Exception as e:
+    clovis_logger.error('Exception spawning tasks: ' + str(e))
+    clovis_logger.error(traceback.print_exc())
+
+  # Update the task.
+  if frontend_job:
+    frontend_job.log = memory_logs.Flush()
+    frontend_job.put()
+
+
+def SpawnTasks(frontend_job):
+  """ Spawns Clovis tasks associated with the frontend job."""
+  user_email = frontend_job.email
+  task = ClovisTask.FromJsonString(frontend_job.clovis_task)
+  task_tag = task.BackendParams()['tag']
+
+  # Delete the clovis task from the FrontendJob because it can make the object
+  # very heavy and it is no longer needed.
+  frontend_job.clovis_task = None
+
   # Compute the task directory.
+  frontend_job.status = 'building_task_dir'
   task_dir_components = []
-  user_email = email_helper.GetUserEmail()
   user_name = None
   if user_email:
     user_name = user_email[:user_email.find('@')]
@@ -339,26 +395,35 @@ def StartFromJsonString(http_body_str):
   task_dir = os.path.join(task.Action(), '_'.join(task_dir_components))
 
   # Build the URL where the result will live.
+  frontend_job.status = 'building_task_url'
   task_url = GetTaskURL(task, task_dir)
   if task_url:
     clovis_logger.info('Task result URL: ' + task_url)
+    frontend_job.task_url = task_url
   else:
-    return Render('Could not build the task URL.', memory_logs)
+    frontend_job.status = 'task_url_error'
+    return
 
   # Split the task in smaller tasks.
+  frontend_job.status = 'splitting_task'
+  frontend_job.put()
   sub_tasks = SplitClovisTask(task)
   if not sub_tasks:
-    return Render('Task split failed.', memory_logs)
+    frontend_job.status = 'task_split_error'
+    return
 
   # Compute estimates for the work duration, in order to compute the instance
   # count and the timeout.
+  frontend_job.status = 'estimating_duration'
   sequential_duration_s = \
       GetEstimatedTaskDurationInSeconds(sub_tasks[0]) * len(sub_tasks)
   if sequential_duration_s <= 0:
-    return Render('Time estimation failed.', memory_logs)
+    frontend_job.status = 'time_estimation_error'
+    return
 
   # Compute the number of required instances if not specified.
   if task.BackendParams().get('instance_count') is None:
+    frontend_job.status = 'estimating_instance_count'
     target_parallel_duration_s = 1800.0 # 30 minutes.
     task.BackendParams()['instance_count'] = math.ceil(
         sequential_duration_s / target_parallel_duration_s)
@@ -366,11 +431,14 @@ def StartFromJsonString(http_body_str):
   # Check the instance quotas.
   clovis_logger.info(
       'Requesting %i instances.' % task.BackendParams()['instance_count'])
+  frontend_job.status = 'checking_instance_quotas'
   max_instances = instance_helper.GetAvailableInstanceCount()
   if max_instances == -1:
-    return Render('Failed to count the available instances.', memory_logs)
+    frontend_job.status = 'instance_count_error'
+    return
   elif task.BackendParams()['instance_count'] == 0:
-    return Render('Cannot create instances, quota exceeded.', memory_logs)
+    frontend_job.status = 'no_instance_available_error'
+    return
   elif max_instances < task.BackendParams()['instance_count']:
     clovis_logger.warning(
         'Instance count limited by quota: %i available / %i requested.' % (
@@ -378,16 +446,21 @@ def StartFromJsonString(http_body_str):
     task.BackendParams()['instance_count'] = max_instances
 
   # Compute the timeout if there is none specified.
-  expected_duration_h = sequential_duration_s / (
-      task.BackendParams()['instance_count'] * 3600.0)
+  expected_duration_s = sequential_duration_s / (
+      task.BackendParams()['instance_count'])
+  frontend_job.eta = datetime.datetime.now() + datetime.timedelta(
+      seconds=expected_duration_s)
   if not task.BackendParams().get('timeout_hours'):
     # Timeout is at least 1 hour.
-    task.BackendParams()['timeout_hours'] = max(1, 5 * expected_duration_h)
+    task.BackendParams()['timeout_hours'] = max(
+        1, 5 * expected_duration_s / 3600.0)
   clovis_logger.info(
       'Timeout delay: %.1f hours. ' % task.BackendParams()['timeout_hours'])
 
+  frontend_job.status = 'queueing_tasks'
   if not EnqueueTasks(sub_tasks, task_tag):
-    return Render('Task creation failed.', memory_logs)
+    frontend_job.status = 'task_creation_error'
+    return
 
   # Start polling the progress.
   clovis_logger.info('Creating worker polling task.')
@@ -397,18 +470,16 @@ def StartFromJsonString(http_body_str):
                  task_url, _countdown=(60 * first_poll_delay_minutes))
 
   # Start the instances if required.
+  frontend_job.status = 'creating_instances'
+  frontend_job.put()
   if not CreateInstanceTemplate(task, task_dir):
-    return Render('Instance template creation failed.', memory_logs)
+    frontend_job.status = 'instance_template_error'
+    return
   if not CreateInstances(task):
-    return Render('Instance creation failed.', memory_logs)
-
-  return Render(flask.Markup(
-      'Success!<br>Your task %s has started.<br>'
-      'Expected duration: %.1f hours.<br>'
-      'You will be notified at %s when completed.') % (
-          task_tag, expected_duration_h, user_email),
-      memory_logs)
+    frontend_job.status = 'instance_creation_error'
+    return
 
+  frontend_job.status = 'started'
 
 def EnqueueTasks(tasks, task_tag):
   """Enqueues a list of tasks in the Google Cloud task queue, for consumption by
@@ -455,6 +526,43 @@ def StartFromForm():
   return StartFromJsonString(http_body_str)
 
 
+@app.route('/list_jobs')
+def ShowTaskList():
+  """Shows a list of all active jobs."""
+  tags = FrontendJob.ListJobs()
+
+  if not tags:
+    Render('No active jobs.')
+
+  html = flask.Markup('Active tasks:<ul>')
+  for tag in tags:
+    html += flask.Markup(
+        '<li><a href="%s">%s</a></li>') % (FrontendJob.GetJobURL(tag), tag)
+  html += flask.Markup('</ul>')
+  return Render(html)
+
+
+@app.route(FrontendJob.SHOW_JOB_URL)
+def ShowTask():
+  """Shows basic information abour a job."""
+  tag = flask.request.args.get('tag')
+  if not tag:
+    return Render('Invalid task tag.')
+
+  frontend_job = FrontendJob.GetFromTag(tag)
+
+  if not frontend_job:
+    return Render('Task not found.')
+
+  message = flask.Markup('Task details:' + frontend_job.RenderAsHtml())
+
+  log = None
+  if frontend_job.log:
+    log = frontend_job.log.split('\n')
+
+  return flask.render_template('log.html', body=message, log=log)
+
+
 @app.errorhandler(404)
 def PageNotFound(e):  # pylint: disable=unused-argument
   """Return a custom 404 error."""
diff --git a/loading/cloud/frontend/frontend_job.py b/loading/cloud/frontend/frontend_job.py
new file mode 100644
index 0000000..0c5783e
--- /dev/null
+++ b/loading/cloud/frontend/frontend_job.py
@@ -0,0 +1,90 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from google.appengine.ext import ndb
+
+
+class FrontendJob(ndb.Model):
+  """Class representing a frontend job.
+
+  A frontend job is a Clovis task sent by the user, and associated metadata
+  (such as the username, the start time...).
+  It is persisted in the Google Cloud datastore.
+
+  All frontend jobs are ancestors of a single entity called 'FrontendJobList'.
+  This allows to benefit from strong consistency when querying the job
+  associated to a tag.
+  """
+  # Base URL path to get information about a job.
+  SHOW_JOB_URL = '/show_job'
+
+  # ndb properties persisted in the datastore. Indexing is not needed.
+  email = ndb.StringProperty(indexed=False)
+  status = ndb.StringProperty(indexed=False)
+  task_url = ndb.StringProperty(indexed=False)
+  eta = ndb.DateTimeProperty(indexed=False)
+  start_time = ndb.DateTimeProperty(auto_now_add=True, indexed=False)
+  # Not indexed by default.
+  clovis_task = ndb.TextProperty(compressed=True, indexed=False)
+  log = ndb.TextProperty(indexed=False)
+
+  @classmethod
+  def _GetParentKeyFromTag(cls, tag):
+    """Gets the key that can be used to retrieve a frontend job from the job
+    list.
+    """
+    return ndb.Key('FrontendJobList', tag)
+
+  @classmethod
+  def CreateForTag(cls, tag):
+    """Creates a frontend job associated with tag."""
+    parent_key = cls._GetParentKeyFromTag(tag)
+    return cls(parent=parent_key)
+
+  @classmethod
+  def GetFromTag(cls, tag):
+    """Gets the frontend job associated with tag."""
+    parent_key = cls._GetParentKeyFromTag(tag)
+    return cls.query(ancestor=parent_key).get()
+
+  @classmethod
+  def DeleteForTag(cls, tag):
+    """Deletes the frontend job assowiated with tag."""
+    parent_key = cls._GetParentKeyFromTag(tag)
+    frontend_job = cls.query(ancestor=parent_key).get(keys_only=True)
+    if frontend_job:
+      frontend_job.delete()
+
+  @classmethod
+  def ListJobs(cls):
+    """Lists all the frontend jobs.
+
+    Returns:
+      list of strings: The list of tags corresponding to existing frontend jobs.
+    """
+    return [key.parent().string_id() for key in cls.query().fetch(
+        100, keys_only=True)]
+
+  @classmethod
+  def GetJobURL(cls, tag):
+    """Gets the URL that can be used to get information about a specific job."""
+    return cls.SHOW_JOB_URL + '?tag=' + tag
+
+  def RenderAsHtml(self):
+    """Render a short job description as a HTML table.
+
+    The log and ClovisTask are not included, because they are potentially very
+    large.
+    """
+    html = '<table>'
+
+    for p in FrontendJob._properties:
+      if p == 'log' or p == 'clovis_task':
+        continue
+      value = getattr(self, p)
+      if value:
+        html += '<tr><td>' + p + '</td><td>' + str(value) + '</td></tr>'
+
+    html += '</table>'
+    return html

commit 238f3b83037059f62aae352cc58b541660c60425
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jun 10 05:16:21 2016 -0700

    clovis: Fix report generation for queuing-related metrics.
    
    Patch from mattcary@.
    
    Review-Url: https://codereview.chromium.org/2054063002
    Cr-Original-Commit-Position: refs/heads/master@{#399161}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 87adb1eb067b221c84ef0c5dfa18b297d6cca875

diff --git a/loading/queuing_lens.py b/loading/queuing_lens.py
index dd7c898..e29ce7a 100644
--- a/loading/queuing_lens.py
+++ b/loading/queuing_lens.py
@@ -83,9 +83,9 @@ class QueuingLens(object):
       matching_source_ids = set(
           source_id for source_id, url in self._source_id_to_url.iteritems()
           if url == request_url)
-      # TODO(mattcary): I think this assert will fail exactly when there is more
-      # than one request for the same URL.
-      assert len(matching_source_ids) <= 1, requests
+      if len(matching_source_ids) > 1:
+        logging.warning('Multiple matching source ids, probably duplicated'
+                        'urls: %s', [rq.url for rq in requests])
       # Get first source id.
       sid = next(s for s in matching_source_ids) \
           if matching_source_ids else None
@@ -98,7 +98,8 @@ class QueuingLens(object):
         if (flight_start_msec < throttle_start_msec and
             flight_end_msec > throttle_start_msec and
             flight_end_msec < throttle_end_msec):
-          blocking_requests.extend(url_to_requests[self._source_id_to_url[sid]])
+          blocking_requests.extend(
+              url_to_requests.get(self._source_id_to_url[sid], []))
 
       info = collections.namedtuple(
           'QueueInfo', ['start_msec', 'end_msec', 'ready_msec', 'blocking'
diff --git a/loading/report.py b/loading/report.py
index 56b6c18..23a39a4 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -85,7 +85,7 @@ class PerUserLensReport(object):
     report['predicted_no_state_prefetch_ms'] = self._no_state_prefetch_ms
 
     # Take the first (earliest) inversion.
-    report['inversion'] = self._inversions[0].url if self._inversions else None
+    report['inversion'] = self._inversions[0].url if self._inversions else ''
 
     report.update(self._cpu_busyness)
     return report
@@ -212,10 +212,13 @@ class LoadingReport(object):
     total_loading_msec = 0
     num_blocking_requests = []
     for queue_info in queuing_info.itervalues():
-      total_blocked_msec += max(0, queue_info.ready_msec -
-                                queue_info.start_msec)
-      total_loading_msec += max(0, queue_info.end_msec -
-                                queue_info.start_msec)
+      try:
+        total_blocked_msec += max(0, queue_info.ready_msec -
+                                  queue_info.start_msec)
+        total_loading_msec += max(0, queue_info.end_msec -
+                                  queue_info.start_msec)
+      except TypeError:
+        pass  # Invalid queue info timings.
       num_blocking_requests.append(len(queue_info.blocking))
     if num_blocking_requests:
       num_blocking_requests.sort()
@@ -231,8 +234,8 @@ class LoadingReport(object):
       avg_blocking = 0
       median_blocking = 0
     return {
-        'total_queuing_blocked_msec': total_blocked_msec,
-        'total_queuing_load_msec': total_loading_msec,
+        'total_queuing_blocked_msec': int(total_blocked_msec),
+        'total_queuing_load_msec': int(total_loading_msec),
         'average_blocking_request_count': avg_blocking,
         'median_blocking_request_count': median_blocking,
     }
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index c67cee2..c9f10a7 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -169,8 +169,8 @@ class LoadingReportTestCase(unittest.TestCase):
         loading_report['contentful_predicted_no_state_prefetch_ms'])
     self.assertEqual(74,
         loading_report['significant_predicted_no_state_prefetch_ms'])
-    self.assertIsNone(loading_report['contentful_inversion'])
-    self.assertIsNone(loading_report['significant_inversion'])
+    self.assertEqual('', loading_report['contentful_inversion'])
+    self.assertEqual('', loading_report['significant_inversion'])
     self.assertIsNone(loading_report['ad_requests'])
     self.assertIsNone(loading_report['ad_or_tracking_requests'])
     self.assertIsNone(loading_report['ad_or_tracking_initiated_requests'])
@@ -200,7 +200,7 @@ class LoadingReportTestCase(unittest.TestCase):
     loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
     self.assertEqual(self.requests[0].url,
                      loading_report['contentful_inversion'])
-    self.assertEqual(None, loading_report['significant_inversion'])
+    self.assertEqual('', loading_report['significant_inversion'])
 
   def testPltNoLoadEvents(self):
     trace = self._MakeTrace()

commit 3dd603c2e645e9d39de7f48481864710e3d27b57
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jun 10 03:15:20 2016 -0700

    clovis: Report the total request count.
    
    Review-Url: https://codereview.chromium.org/2051333003
    Cr-Original-Commit-Position: refs/heads/master@{#399146}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b7347e05aa1d2c68fb15e768250f3bfab16209c6

diff --git a/loading/cloud/frontend/bigquery_schema.json b/loading/cloud/frontend/bigquery_schema.json
index 4706819..254e475 100644
--- a/loading/cloud/frontend/bigquery_schema.json
+++ b/loading/cloud/frontend/bigquery_schema.json
@@ -267,5 +267,9 @@
     {
       "name": "median_blocking_request_count",
       "type": "FLOAT"
+    },
+    {
+      "name": "total_requests",
+      "type": "INTEGER"
     }
 ]
diff --git a/loading/report.py b/loading/report.py
index 63128f2..56b6c18 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -133,6 +133,7 @@ class LoadingReport(object):
           user_lens, activity, network_lens, self._navigation_start_msec)
 
     self._transfer_size = metrics.TotalTransferSize(trace)[1]
+    self._request_count = len(trace.request_track.GetEvents())
 
     content_lens = ContentClassificationLens(
         trace, ad_rules or [], tracking_rules or [])
@@ -157,7 +158,8 @@ class LoadingReport(object):
         'url': self.trace.url,
         'transfer_size': self._transfer_size,
         'dns_requests': self._dns_requests,
-        'dns_cost_ms': self._dns_cost_msec}
+        'dns_cost_ms': self._dns_cost_msec,
+        'total_requests': self._request_count}
 
     for user_lens_type, user_lens_report in self._user_lens_reports.iteritems():
       for key, value in user_lens_report.GenerateReport().iteritems():
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 0762664..c67cee2 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -143,6 +143,7 @@ class LoadingReportTestCase(unittest.TestCase):
                      loading_report['contentful_ms'])
     self.assertAlmostEqual(self._LOAD_END_TIME - self._NAVIGATION_START_TIME,
                            loading_report['plt_ms'])
+    self.assertEqual(2, loading_report['total_requests'])
     self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'], 2)
     self.assertAlmostEqual(0.1844, loading_report['significant_byte_frac'], 2)
     self.assertEqual(2, loading_report['plt_requests'])

commit 3b4750299998f855683279186164a7072484346f
Author: droger <droger@chromium.org>
Date:   Thu Jun 9 13:47:34 2016 -0700

    tools/android/loading Allow specifying 0 instances
    
    As instance count of 0 was considered like a missing count, and leading
    to automatic computation of the instance count.
    
    This was not intended, as it can be useful to request 0 instances (for
    testing).
    
    Review-Url: https://codereview.chromium.org/2052903002
    Cr-Original-Commit-Position: refs/heads/master@{#399004}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f2cc2047e82fe01b5aee6cd216b320952791a023

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 4a63385..2543372 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -358,7 +358,7 @@ def StartFromJsonString(http_body_str):
     return Render('Time estimation failed.', memory_logs)
 
   # Compute the number of required instances if not specified.
-  if not task.BackendParams().get('instance_count'):
+  if task.BackendParams().get('instance_count') is None:
     target_parallel_duration_s = 1800.0 # 30 minutes.
     task.BackendParams()['instance_count'] = math.ceil(
         sequential_duration_s / target_parallel_duration_s)

commit 3464604bce4199a823b46e68f95313e5ac721497
Author: gabadie <gabadie@chromium.org>
Date:   Thu Jun 9 11:43:45 2016 -0700

    sandwich: Reboot device on first WebSocketConnectionClosedException
    
    On some devices, RemoteChromeController start to fail running
    devices because of memory pressure. This CL address this issue
    by letting SandwichRunner to reboot the android device.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2051843002
    Cr-Original-Commit-Position: refs/heads/master@{#398962}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6f9e9b30b3ee77613a144f20cbf0866f67cccb4d

diff --git a/loading/controller.py b/loading/controller.py
index e74044c..97f87ba 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -322,11 +322,11 @@ class RemoteChromeController(ChromeControllerBase):
     assert device is not None, 'Should you be using LocalController instead?'
     super(RemoteChromeController, self).__init__()
     self._device = device
-    self._device.EnableRoot()
     self._metadata['platform'] = {
         'os': 'A-' + device.build_id,
         'product_model': device.product_model
     }
+    self._InitDevice()
 
   def GetDevice(self):
     """Overridden android device."""
@@ -415,6 +415,13 @@ class RemoteChromeController(ChromeControllerBase):
       cmd = ['rm', '-rf', '/data/data/{}/{}'.format(package, directory)]
       self._device.adb.Shell(subprocess.list2cmdline(cmd))
 
+  def RebootDevice(self):
+    """Reboot the remote device."""
+    assert self._wpr_attributes is None, 'WPR should be closed before rebooting'
+    logging.warning('Rebooting the device')
+    device_setup.Reboot(self._device)
+    self._InitDevice()
+
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
     logging.info('Push cache from %s' % cache_path)
@@ -447,6 +454,9 @@ class RemoteChromeController(ChromeControllerBase):
       if not self._device.DismissCrashDialogIfNeeded():
         break
 
+  def _InitDevice(self):
+    self._device.EnableRoot()
+
 
 class LocalChromeController(ChromeControllerBase):
   """Controller for a local (desktop) chrome instance."""
diff --git a/loading/device_setup.py b/loading/device_setup.py
index c1b17d9..9a44652 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -78,6 +78,22 @@ def GetDeviceFromSerial(android_device_serial):
       'Device {} not found'.format(android_device_serial))
 
 
+def Reboot(device):
+  """Reboot the device.
+
+  Args:
+    device: Device to reboot, from DeviceUtils.
+  """
+  # Kills the device -> host forwarder running on the device so that
+  # forwarder.Forwarder have correct state tracking after having rebooted.
+  forwarder.Forwarder.UnmapAllDevicePorts(device)
+  # Reboot the device.
+  device.Reboot()
+  # Pass through the lock screen.
+  time.sleep(3)
+  device.RunShellCommand(['input', 'keyevent', '82'])
+
+
 def DeviceSubmitShellCommandQueue(device, command_queue):
   """Executes on the device a command queue.
 
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 6fb39ab..eea8a45 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -14,6 +14,10 @@ _SRC_DIR = os.path.abspath(os.path.join(
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
 from devil.android import device_utils
 
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'telemetry',
+    'third_party', 'websocket-client'))
+import websocket
+
 import chrome_cache
 import controller
 import devtools_monitor
@@ -145,17 +149,17 @@ class SandwichRunner(object):
       return self.network_condition
     return None
 
-  def _RunNavigation(self, clear_cache, run_id=None):
+  def _RunNavigation(self, clear_cache, repeat_id=None):
     """Run a page navigation to the given URL.
 
     Args:
       clear_cache: Whether if the cache should be cleared before navigation.
-      run_id: Id of the run in the output directory. If it is None, then no
+      repeat_id: Id of the run in the output directory. If it is None, then no
         trace or video will be saved.
     """
     run_path = None
-    if run_id is not None:
-      run_path = os.path.join(self.output_dir, str(run_id))
+    if repeat_id is not None:
+      run_path = os.path.join(self.output_dir, str(repeat_id))
       if not os.path.isdir(run_path):
         os.makedirs(run_path)
     self._chrome_ctl.SetNetworkEmulation(
@@ -195,9 +199,27 @@ class SandwichRunner(object):
       trace_path = os.path.join(run_path, TRACE_FILENAME)
       trace.ToJsonFile(trace_path)
 
-  def _RunUrl(self, run_id):
-    for attempt_id in xrange(self._ATTEMPT_COUNT):
+  def _RunInRetryLoop(self, repeat_id, perform_dry_run_before):
+    """Attempts to run monitoring navigation.
+
+    Args:
+      repeat_id: Id of the run in the output directory.
+      perform_dry_run_before: Whether it should do a dry run attempt before the
+        actual monitoring run.
+
+    Returns:
+      Whether the device should be rebooted to continue attempting for that
+      given |repeat_id|.
+    """
+    resume_attempt_id = 0
+    if perform_dry_run_before:
+      resume_attempt_id = 1
+    for attempt_id in xrange(resume_attempt_id, self._ATTEMPT_COUNT):
       try:
+        if perform_dry_run_before:
+          logging.info('Do sandwich dry run attempt %d', attempt_id)
+        else:
+          logging.info('Do sandwich run attempt %d', attempt_id)
         self._chrome_ctl.ResetBrowserState()
         clear_cache = False
         if self.cache_operation == CacheOperation.CLEAR:
@@ -205,23 +227,64 @@ class SandwichRunner(object):
         elif self.cache_operation == CacheOperation.PUSH:
           self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
         elif self.cache_operation == CacheOperation.SAVE:
-          clear_cache = run_id == 0
-        self._RunNavigation(clear_cache=clear_cache, run_id=run_id)
-        break
+          clear_cache = repeat_id == 0
+        self._RunNavigation(clear_cache=clear_cache, repeat_id=repeat_id)
+        if not perform_dry_run_before or attempt_id > resume_attempt_id:
+          break
       except controller.ChromeControllerError as error:
-        if error.IsIntermittent() and attempt_id + 1 != self._ATTEMPT_COUNT:
-          dump_filename = 'intermittent_failure{}'.format(attempt_id)
-          dump_path = os.path.join(self.output_dir, str(run_id), dump_filename)
+        request_reboot = False
+        is_intermittent = error.IsIntermittent()
+        if (self.android_device and
+            attempt_id == 0 and
+            error.error_type is websocket.WebSocketConnectionClosedException):
+          assert not perform_dry_run_before
+          # On Android, the first socket connection closure is likely caused by
+          # memory pressure on the device and therefore considered intermittent,
+          # and therefore request a reboot of the device to the caller.
+          request_reboot = True
+          is_intermittent = True
+        if is_intermittent and attempt_id + 1 != self._ATTEMPT_COUNT:
+          dump_filename = '{}_intermittent_failure'.format(attempt_id)
+          dump_path = os.path.join(
+              self.output_dir, str(repeat_id), dump_filename)
         else:
           dump_path = os.path.join(self.output_dir, ERROR_FILENAME)
         with open(dump_path, 'w') as dump_output:
           error.Dump(dump_output)
-        if not error.IsIntermittent():
+        if not is_intermittent:
           error.RaiseOriginal()
+        if request_reboot:
+          assert resume_attempt_id is 0
+          return True
     else:
       logging.error('Failed to navigate to %s after %d attemps' % \
                     (self.url, self._ATTEMPT_COUNT))
       error.RaiseOriginal()
+    return False
+
+  def _RunWithWpr(self, resume_repeat_id, perform_dry_run_before):
+    """Opens WPR and attempts to run repeated monitoring navigation.
+
+    Args:
+      resume_repeat_id: Id of the run to resume.
+      perform_dry_run_before: Whether the repeated run to resume should first do
+        a dry run navigation attempt.
+
+    Returns:
+      Number of repeat performed. If < self.repeat, then it means that the
+        device should be rebooted.
+    """
+    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
+        record=self.wpr_record,
+        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
+        disable_script_injection=self.disable_wpr_script_injection,
+        out_log_path=os.path.join(self.output_dir, WPR_LOG_FILENAME)):
+      for repeat_id in xrange(resume_repeat_id, self.repeat):
+        reboot_requested = self._RunInRetryLoop(
+            repeat_id, perform_dry_run_before)
+        if reboot_requested:
+          return repeat_id
+    return self.repeat
 
   def _PullCacheFromDevice(self):
     assert self.cache_operation == CacheOperation.SAVE
@@ -253,14 +316,10 @@ class SandwichRunner(object):
         self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
         chrome_cache.UnzipDirectoryContent(
             self.cache_archive_path, self._local_cache_directory_path)
-
-      with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
-          record=self.wpr_record,
-          network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
-          disable_script_injection=self.disable_wpr_script_injection,
-          out_log_path=os.path.join(self.output_dir, WPR_LOG_FILENAME)):
-        for repeat_id in xrange(self.repeat):
-          self._RunUrl(run_id=repeat_id)
+      times_repeated = self._RunWithWpr(0, False)
+      if times_repeated < self.repeat:
+        self._chrome_ctl.RebootDevice()
+        self._RunWithWpr(times_repeated, True)
     finally:
       if self._local_cache_directory_path:
         shutil.rmtree(self._local_cache_directory_path)

commit 7a42b25435d668811d15e862e3cc839a7c1b0cf6
Author: gabadie <gabadie@chromium.org>
Date:   Thu Jun 9 09:49:07 2016 -0700

    tools/android/loading: Add possibility to delay url monitoring tear down
    
    On website finishing loading resources through javascript such as
    amazon.com, the WPR archive were not recording all these resources
    because were stoping at devtools' load page finish event. These
    miss recording resources were leading to unprecise loading times
    on benchmark using the WPR archive, because all the requests for
    these resources were getting light weight 404 back.
    
    This CL fixes the issue by implementing the stop_delay_multiplier
    in the loading_trace.LoadingTrace.RecordUrlNavigation().
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2049673002
    Cr-Original-Commit-Position: refs/heads/master@{#398918}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d9de2f64da3fceab4200cf0f254a0e8c1a94efaf

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index f81ea4e..cd52dfb 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -5,6 +5,7 @@
 """Library handling DevTools websocket interaction.
 """
 
+import datetime
 import httplib
 import json
 import logging
@@ -109,9 +110,11 @@ class DevToolsConnection(object):
     self._scoped_states = {}
     self._domains_to_enable = set()
     self._tearing_down_tracing = False
-    self._please_stop = False
     self._ws = None
     self._target_descriptor = None
+    self._stop_delay_multiplier = 0
+    self._monitoring_start_timestamp = None
+    self._monitoring_stop_timestamp = None
 
     self._Connect()
     self.RegisterListener('Inspector.targetCrashed', self)
@@ -222,15 +225,19 @@ class DevToolsConnection(object):
     assert res['result'], 'Cache clearing is not supported by this browser.'
     self.SyncRequest('Network.clearBrowserCache')
 
-  def MonitorUrl(self, url, timeout_seconds=DEFAULT_TIMEOUT_SECONDS):
+  def MonitorUrl(self, url, timeout_seconds=DEFAULT_TIMEOUT_SECONDS,
+                 stop_delay_multiplier=0):
     """Navigate to url and dispatch monitoring loop.
 
     Unless you have registered a listener that will call StopMonitoring, this
     will run until timeout from chrome.
 
     Args:
-      url: (str) a URL to navigate to before starting monitoring loop.\
+      url: (str) a URL to navigate to before starting monitoring loop.
       timeout_seconds: timeout in seconds for monitoring loop.
+      stop_delay_multiplier: How long to wait after page load completed before
+        tearing down, relative to the time it took to reach the page load to
+        complete.
     """
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
@@ -245,13 +252,30 @@ class DevToolsConnection(object):
 
     logging.info('Navigate to %s' % url)
     self.SendAndIgnoreResponse('Page.navigate', {'url': url})
-
-    self._Dispatch(timeout=timeout_seconds)
+    self._monitoring_start_timestamp = datetime.datetime.now()
+    self._Dispatch(timeout=timeout_seconds,
+                   stop_delay_multiplier=stop_delay_multiplier)
+    self._monitoring_start_timestamp = None
+    logging.info('Tearing down monitoring.')
     self._TearDownMonitoring()
 
   def StopMonitoring(self):
-    """Stops the monitoring."""
-    self._please_stop = True
+    """Sets the timestamp when to stop monitoring.
+
+    Args:
+      address_delayed_stop: Whether the MonitorUrl()'s stop_delay_multiplier
+        should be addressed or not.
+    """
+    if self._stop_delay_multiplier == 0:
+      self._StopMonitoringImmediately()
+    elif self._monitoring_stop_timestamp is None:
+      assert self._monitoring_start_timestamp is not None
+      current_time = datetime.datetime.now()
+      stop_delay_duration = self._stop_delay_multiplier * (
+          current_time - self._monitoring_start_timestamp)
+      logging.info('Delaying monitoring stop for %ds',
+                   stop_delay_duration.total_seconds())
+      self._monitoring_stop_timestamp = current_time + stop_delay_duration
 
   def ExecuteJavaScript(self, expression):
     """Run JavaScript expression.
@@ -291,15 +315,26 @@ class DevToolsConnection(object):
     assert response == 'Target is closing'
     self._ws = None
 
-  def _Dispatch(self, timeout, kind='Monitoring'):
-    self._please_stop = False
-    while not self._please_stop:
+  def _StopMonitoringImmediately(self):
+    self._monitoring_stop_timestamp = datetime.datetime.now()
+
+  def _Dispatch(self, timeout, kind='Monitoring', stop_delay_multiplier=0):
+    self._monitoring_stop_timestamp = None
+    self._stop_delay_multiplier = stop_delay_multiplier
+    while True:
       try:
         self._ws.DispatchNotifications(timeout=timeout)
       except websocket.WebSocketTimeoutException:
-        break
-    if not self._please_stop:
-      logging.warning('%s stopped on a timeout.' % kind)
+        if self._monitoring_stop_timestamp is None:
+          logging.warning('%s stopped on a timeout.' % kind)
+          break
+      if self._monitoring_stop_timestamp:
+        # After the first timeout reduce the timeout to check when to stop
+        # monitoring more often, because the page at this moment can already be
+        # loaded and not many events would be arriving from it.
+        timeout = 1
+        if datetime.datetime.now() >= self._monitoring_stop_timestamp:
+          break
 
   def Handle(self, method, event):
     del event # unused
@@ -334,7 +369,7 @@ class DevToolsConnection(object):
       stream_handle = msg.get('params', {}).get('stream')
       if not stream_handle:
         self._tearing_down_tracing = False
-        self.StopMonitoring()
+        self._StopMonitoringImmediately()
         # Fall through to regular dispatching.
       else:
         _StreamReader(self._ws, stream_handle).Read(self._TracingStreamDone)
@@ -350,7 +385,7 @@ class DevToolsConnection(object):
       self._domain_listeners[domain].Handle(method, msg)
     if self._tearing_down_tracing and method == self.TRACING_DONE_EVENT:
       self._tearing_down_tracing = False
-      self.StopMonitoring()
+      self._StopMonitoringImmediately()
 
   def _TracingStreamDone(self, data):
     tracing_events = json.loads(data)
@@ -360,7 +395,7 @@ class DevToolsConnection(object):
       if self._please_stop:
         break
     self._tearing_down_tracing = False
-    self.StopMonitoring()
+    self._StopMonitoringImmediately()
 
   def _HttpRequest(self, path):
     assert path[0] == '/'
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index c170e6b..beb6785 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -77,7 +77,8 @@ class LoadingTrace(object):
   @classmethod
   def RecordUrlNavigation(
       cls, url, connection, chrome_metadata, categories,
-      timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
+      timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS,
+      stop_delay_multiplier=0):
     """Create a loading trace by using controller to fetch url.
 
     Args:
@@ -86,6 +87,9 @@ class LoadingTrace(object):
       chrome_metadata: Dictionary of chrome metadata.
       categories: as in tracing.TracingTrack
       timeout_seconds: monitoring connection timeout in seconds.
+      stop_delay_multiplier: How long to wait after page load completed before
+        tearing down, relative to the time it took to reach the page load to
+        complete.
 
     Returns:
       LoadingTrace instance.
@@ -95,7 +99,9 @@ class LoadingTrace(object):
     trace = tracing.TracingTrack(connection, categories)
     start_date_str = datetime.datetime.utcnow().isoformat()
     seconds_since_epoch=time.time()
-    connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
+    connection.MonitorUrl(url,
+                          timeout_seconds=timeout_seconds,
+                          stop_delay_multiplier=stop_delay_multiplier)
     trace = cls(url, chrome_metadata, page, request, trace)
     trace.metadata.update(date=start_date_str,
                           seconds_since_epoch=seconds_since_epoch)
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 7e1d8ce..6fb39ab 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -76,6 +76,7 @@ class SandwichRunner(object):
   method.
   """
   _ATTEMPT_COUNT = 3
+  _STOP_DELAY_MULTIPLIER = 2
 
   def __init__(self):
     """Configures a sandwich runner out of the box.
@@ -162,6 +163,9 @@ class SandwichRunner(object):
     categories = _TRACING_CATEGORIES
     if self.record_memory_dumps:
       categories += [MEMORY_DUMP_CATEGORY]
+    stop_delay_multiplier = 0
+    if self.wpr_record or self.cache_operation == CacheOperation.SAVE:
+      stop_delay_multiplier = self._STOP_DELAY_MULTIPLIER
     # TODO(gabadie): add a way to avoid recording a trace.
     with self._chrome_ctl.Open() as connection:
       if clear_cache:
@@ -174,7 +178,8 @@ class SandwichRunner(object):
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
             categories=categories,
-            timeout_seconds=_DEVTOOLS_TIMEOUT)
+            timeout_seconds=_DEVTOOLS_TIMEOUT,
+            stop_delay_multiplier=stop_delay_multiplier)
 
       if run_path is not None and self.record_video:
         device = self._chrome_ctl.GetDevice()

commit 2ee9e3a2c71988d8350005a1222d839dfe2d6e09
Author: perezju <perezju@chromium.org>
Date:   Thu Jun 9 09:35:04 2016 -0700

    Migrate some DeviceUtils clients to List/StatDirectory
    
    Migrate some previous uses of the (now deprecated) Ls method, and
    instances of hand rolled shell 'ls' commands to the new available
    methods.
    
    Also a bit of cleanup on RunShellCommand uses.
    
    BUG=552376
    
    Review-Url: https://codereview.chromium.org/2045833004
    Cr-Original-Commit-Position: refs/heads/master@{#398907}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5ad37228088780af64b5a8a5d8f2ece618f4ae3e

diff --git a/appstats.py b/appstats.py
index 8bb93a5..032e847 100755
--- a/appstats.py
+++ b/appstats.py
@@ -83,7 +83,7 @@ class DeviceHelper(object):
     try:
       process_name = process_name.split(':')[0]
       cmd = ['dumpsys', 'package', process_name]
-      user_id_lines = adb.RunShellCommand(' '.join(cmd), large_output=True)
+      user_id_lines = adb.RunShellCommand(cmd, large_output=True)
       user_id_lines = Utils.FindLines(user_id_lines, 'userId=')
 
       if not user_id_lines:
@@ -126,8 +126,7 @@ class DeviceHelper(object):
     intersect the two.  The returned result is sorted based on userid."""
     pids = []
     try:
-      cmd = ['ps']
-      pid_lines = adb.RunShellCommand(' '.join(cmd), large_output=True)
+      pid_lines = adb.RunShellCommand(['ps'], large_output=True)
       if default_pid:
         pid_lines = Utils.FindLines(pid_lines, str(default_pid))
       if process_filter:
@@ -177,8 +176,7 @@ class NetworkHelper(object):
       # The number of bytes sent.
       tx_idx = 7
 
-      cmd = ['cat', '/proc/net/xt_qtaguid/stats']
-      net_lines = adb.RunShellCommand(' '.join(cmd), large_output=True)
+      net_lines = adb.ReadFile('/proc/net/xt_qtaguid/stats').splitlines()
       net_lines = Utils.FindLines(net_lines, userid)
       for line in net_lines:
         data = re.split('\s+', line.strip())
@@ -211,7 +209,7 @@ class MemoryHelper(object):
     found it will return [ 0, 0, 0 ]."""
     results = [0, 0, 0]
 
-    mem_lines = adb.RunShellCommand(' '.join(['dumpsys', 'meminfo', pid]))
+    mem_lines = adb.RunShellCommand(['dumpsys', 'meminfo', pid])
     for line in mem_lines:
       match = re.split('\s+', line.strip())
 
@@ -265,7 +263,7 @@ class GraphicsHelper(object):
     represents the graphics memory usage.  Will return this as a single entry
     array of [ Graphics ].  If not found, will return [ 0 ]."""
     try:
-      mem_lines = adb.RunShellCommand(' '.join(['showmap', '-t', pid]))
+      mem_lines = adb.RunShellCommand(['showmap', '-t', pid])
       for line in mem_lines:
         match = re.split('[ ]+', line.strip())
         if match[-1] in GraphicsHelper.__SHOWMAP_KEY_MATCHES:
@@ -280,8 +278,7 @@ class GraphicsHelper(object):
     file in |self.__NV_MAP_FILE_LOCATIONS| and see if one exists.  If so, it
     will return it."""
     for nv_file in GraphicsHelper.__NV_MAP_FILE_LOCATIONS:
-      exists = adb.RunShellCommand(' '.join(['ls', nv_file]))
-      if exists[0] == nv_file.split('/')[-1]:
+      if adb.PathExists(nv_file):
         return nv_file
     return None
 
@@ -293,7 +290,7 @@ class GraphicsHelper(object):
     [ Graphics ].  If not found, will return [ 0 ]."""
     nv_file = GraphicsHelper.__NvMapPath(adb)
     if nv_file:
-      mem_lines = adb.RunShellCommand(' '.join(['cat', nv_file]))
+      mem_lines = adb.ReadFile(nv_file).splitlines()
       for line in mem_lines:
         match = re.split(' +', line.strip())
         if match[2] == pid:

commit 82b47613808c4a0ec5f3ca77b65b6e9d361fea4a
Author: gabadie <gabadie@chromium.org>
Date:   Thu Jun 9 08:43:24 2016 -0700

    tools/android/loading: Fix TCP listening port collision in RemoteChromeController
    
    Before when using the RemoteChromeController, the TCP host to
    device forwarder was set up to listen on OPTIONS.devtools_port.
    
    The issue was that Sandwich was then failing when recording traces
    on several devices from the same host.
    
    This CL fixes the issue as Telemetry did by allocating a port number
    using a socket getting its listening port number assigned by the OS.
    This solution is OS level racy, but in case of failure, the
    exception is caught and the devtool connection is retried
    automatically within RemoteChromeController.Open().
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2044703005
    Cr-Original-Commit-Position: refs/heads/master@{#398894}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c2373ae91ca6719e2cfcf7639c767c80d1eb5c57

diff --git a/loading/controller.py b/loading/controller.py
index e6ac299..e74044c 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -38,6 +38,7 @@ _SRC_DIR = os.path.abspath(os.path.join(
 _CATAPULT_DIR = os.path.join(_SRC_DIR, 'third_party', 'catapult')
 
 sys.path.append(os.path.join(_CATAPULT_DIR, 'devil'))
+from devil.android import device_errors
 from devil.android.sdk import intent
 
 sys.path.append(
@@ -73,6 +74,21 @@ class ChromeControllerInternalError(Exception):
   pass
 
 
+def _AllocateTcpListeningPort():
+  """Allocates a TCP listening port.
+
+  Note: The use of this function is inherently OS level racy because the
+    port returned by this function might be re-used by another running process.
+  """
+  temp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+  try:
+    temp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+    temp_socket.bind(('', 0))
+    return temp_socket.getsockname()[1]
+  finally:
+    temp_socket.close()
+
+
 class ChromeControllerError(Exception):
   """Chrome error with detailed log.
 
@@ -339,23 +355,41 @@ class RemoteChromeController(ChromeControllerBase):
       try:
         for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
           logging.info('Devtools connection attempt %d' % attempt_id)
-          with device_setup.ForwardPort(
-              self._device, 'tcp:%d' % OPTIONS.devtools_port,
-              'localabstract:chrome_devtools_remote'):
-            try:
-              connection = devtools_monitor.DevToolsConnection(
-                  OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-              self._StartConnection(connection)
-            except socket.error as e:
-              if e.errno != errno.ECONNRESET:
-                raise
-              time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
-              continue
-            yield connection
-            if self._slow_death:
-              self._device.adb.Shell('am start com.google.android.launcher')
-              time.sleep(self.TIME_TO_IDLE_SECONDS)
-            break
+          # Adb forwarding does not provide a way to print the port number if
+          # it is allocated atomically by the OS by passing port=0, but we need
+          # dynamically allocated listening port here to handle parallel run on
+          # different devices.
+          host_side_port = _AllocateTcpListeningPort()
+          logging.info('Allocated host sided listening port for devtools '
+              'connection: %d', host_side_port)
+          try:
+            with device_setup.ForwardPort(
+                self._device, 'tcp:%d' % host_side_port,
+                'localabstract:chrome_devtools_remote'):
+              try:
+                connection = devtools_monitor.DevToolsConnection(
+                    OPTIONS.devtools_hostname, host_side_port)
+                self._StartConnection(connection)
+              except socket.error as e:
+                if e.errno != errno.ECONNRESET:
+                  raise
+                time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
+                continue
+              yield connection
+              if self._slow_death:
+                self._device.adb.Shell('am start com.google.android.launcher')
+                time.sleep(self.TIME_TO_IDLE_SECONDS)
+              break
+          except device_errors.AdbCommandFailedError as error:
+            _KNOWN_ADB_FORWARDER_FAILURES = [
+              'cannot bind to socket: Address already in use',
+              'cannot rebind existing socket: Resource temporarily unavailable']
+            for message in _KNOWN_ADB_FORWARDER_FAILURES:
+              if message in error.message:
+                break
+            else:
+              raise
+            continue
         else:
           raise ChromeControllerInternalError(
               'Failed to connect to Chrome devtools after {} '
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 23df7cf..7e1d8ce 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -216,7 +216,7 @@ class SandwichRunner(object):
     else:
       logging.error('Failed to navigate to %s after %d attemps' % \
                     (self.url, self._ATTEMPT_COUNT))
-      raise
+      error.RaiseOriginal()
 
   def _PullCacheFromDevice(self):
     assert self.cache_operation == CacheOperation.SAVE

commit 2ad6308ff2c1e2323177860e06940993f313dfaf
Author: lizeb <lizeb@chromium.org>
Date:   Thu Jun 9 04:54:06 2016 -0700

    clovis: Fix report generation.
    
    The user lenses report an offset, not an absolute number, making the
    report generation fail. Add back the base value.
    
    Review-Url: https://codereview.chromium.org/2048253003
    Cr-Original-Commit-Position: refs/heads/master@{#398845}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 99d96be1a2875d55950a23376fd1742fd3a6f2cd

diff --git a/loading/report.py b/loading/report.py
index a182518..63128f2 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -142,8 +142,9 @@ class LoadingReport(object):
         trace, content_lens, has_ad_rules, has_tracking_rules)
     self._ads_cost = self._AdsAndTrackingCpuCost(
         self._navigation_start_msec,
-        self._user_lens_reports['plt'].GenerateReport()['ms'], content_lens,
-        activity, has_tracking_rules or has_ad_rules)
+        (self._navigation_start_msec
+         + self._user_lens_reports['plt'].GenerateReport()['ms']),
+        content_lens, activity, has_tracking_rules or has_ad_rules)
 
     self._queue_stats = self._ComputeQueueStats(QueuingLens(trace))
 

commit 9e196a6f08344bd8894415ac84907f50673fe98f
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 8 10:46:10 2016 -0700

    sandwich: Fix an assertion failure cause by bad initiator inference
    
    Devtools' inference of request initiatior is not that reliable,
    loading to some javascript originated resources marked as
    initiated from the parser.
    
    This CL removes an assertion in sandwich_prefetch.py to not
    unecessarly fail on some URLs where it happens more often.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2043413002
    Cr-Original-Commit-Position: refs/heads/master@{#398611}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 884aff41f559593f66c9aa6a53a2f83ae118263a

diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index ab093c2..8290e49 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -114,7 +114,6 @@ def _FilterOutDataAndIncompleteRequests(requests):
     # originated requests that have not received any responses yet.
     if request.protocol is None:
       assert not request.HasReceivedResponse()
-      assert request.initiator['type'] == 'script'
       continue
     if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
       raise RuntimeError('Unknown request protocol {}'.format(request.protocol))

commit 9b3bcca0a67ae36aaa72f07d8a9e1cb5b8bc83b7
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jun 8 09:18:42 2016 -0700

    clovis: Add default tracing categories to trace_task_handler.py.
    
    This is needed following https://codereview.chromium.org/2047463002.
    
    Review-Url: https://codereview.chromium.org/2050703002
    Cr-Original-Commit-Position: refs/heads/master@{#398587}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b368cb36ce5975cdc67fe98b7c40fe9bf261a641

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index b9fdd6b..e031636 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -7,6 +7,7 @@ import re
 import sys
 import traceback
 
+import clovis_constants
 import common.clovis_paths
 from common.clovis_task import ClovisTask
 from common.loading_trace_database import LoadingTraceDatabase
@@ -63,7 +64,8 @@ def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
       with chrome_ctl.Open() as connection:
         connection.ClearCache()
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-            url, connection, chrome_ctl.ChromeMetadata())
+            url, connection, chrome_ctl.ChromeMetadata(),
+            clovis_constants.DEFAULT_CATEGORIES)
         trace_metadata['succeeded'] = True
         trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
         sys.stdout.write('Trace generation success.\n')

commit 9eefd46f67c66e50d52b4ed91d3536eb527ed31c
Author: mattcary <mattcary@chromium.org>
Date:   Wed Jun 8 07:10:47 2016 -0700

    Clovis: add queueing information to report.
    
    Adds queuing metrics and updates the bigquery schema appropriately. Slight
    change in queuing_lens to make debugging easier (too many generators are
    annoying...).
    
    Review-Url: https://codereview.chromium.org/2048023002
    Cr-Original-Commit-Position: refs/heads/master@{#398541}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 546a64679f06867e5e2ef1f5f979e9727ace13ee

diff --git a/loading/cloud/frontend/bigquery_schema.json b/loading/cloud/frontend/bigquery_schema.json
index b892d8f..4706819 100644
--- a/loading/cloud/frontend/bigquery_schema.json
+++ b/loading/cloud/frontend/bigquery_schema.json
@@ -251,5 +251,21 @@
     {
         "name": "transfer_size",
         "type": "INTEGER"
+    },
+    {
+      "name": "total_queuing_blocked_msec",
+      "type": "FLOAT"
+    },
+    {
+      "name": "total_queuing_load_msec",
+      "type": "FLOAT"
+    },
+    {
+      "name": "average_blocking_request_count",
+      "type": "FLOAT"
+    },
+    {
+      "name": "median_blocking_request_count",
+      "type": "FLOAT"
     }
 ]
diff --git a/loading/queuing_lens.py b/loading/queuing_lens.py
index 97f87dc..dd7c898 100644
--- a/loading/queuing_lens.py
+++ b/loading/queuing_lens.py
@@ -55,8 +55,8 @@ class QueuingLens(object):
          (start_msec: throttle start, end_msec: throttle end,
           ready_msec: ready,
           blocking: [blocking requests],
-          source_ids: [source ids of the request])}, which the map values are
-      anonymous objects with the specified fields.
+          source_ids: [source ids of the request])}, where the map values are
+      a named tuple with the specified fields.
     """
     url_to_requests = collections.defaultdict(list)
     for rq in self._request_track.GetEvents():
@@ -92,14 +92,13 @@ class QueuingLens(object):
       (throttle_start_msec, throttle_end_msec, ready_msec) = \
          timing_by_source_id[sid] if matching_source_ids else (-1, -1, -1)
 
-      blocking_requests = itertools.chain.from_iterable(
-          url_to_requests[self._source_id_to_url[sid]]
-          for sid, (flight_start_msec,
-                    flight_end_msec, _) in timing_by_source_id.iteritems()
-          if (flight_start_msec < throttle_start_msec and
-              flight_end_msec > throttle_start_msec and
-              flight_end_msec < throttle_end_msec))
-      blocking_requests = [b for b in blocking_requests]
+      blocking_requests = []
+      for sid, (flight_start_msec,
+                flight_end_msec, _)  in timing_by_source_id.iteritems():
+        if (flight_start_msec < throttle_start_msec and
+            flight_end_msec > throttle_start_msec and
+            flight_end_msec < throttle_end_msec):
+          blocking_requests.extend(url_to_requests[self._source_id_to_url[sid]])
 
       info = collections.namedtuple(
           'QueueInfo', ['start_msec', 'end_msec', 'ready_msec', 'blocking'
@@ -129,7 +128,7 @@ class QueuingLens(object):
     for e in events:
       if 'request_url' in e.args['data']:
         urls.add(e.args['data']['request_url'])
-    assert len(urls) == 1
+    assert len(urls) == 1, urls
     return urls.pop()
 
   def _GetEventsForRequest(self, request):
diff --git a/loading/report.py b/loading/report.py
index 9df11b7..a182518 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -14,6 +14,7 @@ import loading_trace
 import metrics
 from network_activity_lens import NetworkActivityLens
 from prefetch_view import PrefetchSimulationView
+from queuing_lens import QueuingLens
 import request_dependencies_lens
 from user_satisfied_lens import (
     FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens,
@@ -144,6 +145,8 @@ class LoadingReport(object):
         self._user_lens_reports['plt'].GenerateReport()['ms'], content_lens,
         activity, has_tracking_rules or has_ad_rules)
 
+    self._queue_stats = self._ComputeQueueStats(QueuingLens(trace))
+
   def GenerateReport(self):
     """Returns a report as a dict."""
     # NOTE: When changing the return value here, also update the schema
@@ -162,6 +165,7 @@ class LoadingReport(object):
     report.update(self._ad_report)
     report.update(self._ads_cost)
     report.update(self._connection_stats)
+    report.update(self._queue_stats)
     return report
 
   @classmethod
@@ -199,6 +203,38 @@ class LoadingReport(object):
     return result
 
   @classmethod
+  def _ComputeQueueStats(cls, queue_lens):
+    queuing_info = queue_lens.GenerateRequestQueuing()
+    total_blocked_msec = 0
+    total_loading_msec = 0
+    num_blocking_requests = []
+    for queue_info in queuing_info.itervalues():
+      total_blocked_msec += max(0, queue_info.ready_msec -
+                                queue_info.start_msec)
+      total_loading_msec += max(0, queue_info.end_msec -
+                                queue_info.start_msec)
+      num_blocking_requests.append(len(queue_info.blocking))
+    if num_blocking_requests:
+      num_blocking_requests.sort()
+      avg_blocking = (float(sum(num_blocking_requests)) /
+                      len(num_blocking_requests))
+      mid = len(num_blocking_requests) / 2
+      if len(num_blocking_requests) & 1:
+        median_blocking = num_blocking_requests[mid]
+      else:
+        median_blocking = (num_blocking_requests[mid-1] +
+                           num_blocking_requests[mid]) / 2
+    else:
+      avg_blocking = 0
+      median_blocking = 0
+    return {
+        'total_queuing_blocked_msec': total_blocked_msec,
+        'total_queuing_load_msec': total_loading_msec,
+        'average_blocking_request_count': avg_blocking,
+        'median_blocking_request_count': median_blocking,
+    }
+
+  @classmethod
   def _AdsAndTrackingCpuCost(
       cls, start_msec, end_msec, content_lens, activity, has_rules):
     """Returns the CPU cost associated with Ads and tracking between timestamps.
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index cff6de3..0762664 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -6,6 +6,7 @@ import unittest
 
 import metrics
 import report
+from queuing_lens import QueuingLens
 import test_utils
 import user_satisfied_lens_unittest
 
@@ -90,6 +91,46 @@ class LoadingReportTestCase(unittest.TestCase):
         self.requests, self.trace_events, self._MAIN_FRAME_ID)
     return trace
 
+  def _AddQueuingEvents(self, source_id, url, start_msec, ready_msec, end_msec):
+    self.trace_events.extend([
+        {'args': {
+            'data': {
+                'request_url': url,
+                'source_id': source_id
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': source_id,
+         'pid': 1, 'tid': 10,
+         'name': QueuingLens.ASYNC_NAME,
+         'ph': 'b',
+         'ts': start_msec * self.MILLI_TO_MICRO
+        },
+        {'args': {
+            'data': {
+                'source_id': source_id
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': source_id,
+         'pid': 1, 'tid': 10,
+         'name': QueuingLens.READY_NAME,
+         'ph': 'n',
+         'ts': ready_msec * self.MILLI_TO_MICRO
+        },
+        {'args': {
+            'data': {
+                'source_id': source_id
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': source_id,
+         'pid': 1, 'tid': 10,
+         'name': QueuingLens.ASYNC_NAME,
+         'ph': 'e',
+         'ts': end_msec * self.MILLI_TO_MICRO
+        }])
+
   def testGenerateReport(self):
     trace = self._MakeTrace()
     loading_report = report.LoadingReport(trace).GenerateReport()
@@ -139,6 +180,10 @@ class LoadingReportTestCase(unittest.TestCase):
         self._FIRST_REQUEST_DATA_LENGTH + self._SECOND_REQUEST_DATA_LENGTH
         + metrics.HTTP_OK_LENGTH * 2,
         loading_report['transfer_size'])
+    self.assertEqual(0, loading_report['total_queuing_blocked_msec'])
+    self.assertEqual(0, loading_report['total_queuing_load_msec'])
+    self.assertEqual(0, loading_report['average_blocking_request_count'])
+    self.assertEqual(0, loading_report['median_blocking_request_count'])
 
   def testInversion(self):
     self.requests[0].timing.loading_finished = 4 * (
@@ -226,6 +271,27 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertAlmostEqual(.5, loading_report['ad_or_tracking_script_frac'], 2)
     self.assertAlmostEqual(0., loading_report['ad_or_tracking_parsing_frac'])
 
+  def testQueueStats(self):
+    # We use three requests, A, B and C. A is not blocked, B is blocked by A,
+    # and C blocked by A and B.
+    BASE_MSEC = self._FIRST_REQUEST_TIME + 4 * self._DURATION
+    self.requests = []
+    request_A = self.trace_creator.RequestAt(BASE_MSEC, 5)
+    request_B = self.trace_creator.RequestAt(BASE_MSEC + 6, 5)
+    request_C = self.trace_creator.RequestAt(BASE_MSEC + 12, 10)
+    self.requests.extend([request_A, request_B, request_C])
+    self._AddQueuingEvents(10, request_A.url,
+                           BASE_MSEC, BASE_MSEC, BASE_MSEC + 5)
+    self._AddQueuingEvents(20, request_B.url,
+                           BASE_MSEC + 1, BASE_MSEC + 6, BASE_MSEC + 11)
+    self._AddQueuingEvents(30, request_C.url,
+                           BASE_MSEC + 2, BASE_MSEC + 12, BASE_MSEC + 22)
+    loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
+    self.assertEqual(15, loading_report['total_queuing_blocked_msec'])
+    self.assertEqual(35, loading_report['total_queuing_load_msec'])
+    self.assertAlmostEqual(1, loading_report['average_blocking_request_count'])
+    self.assertEqual(1, loading_report['median_blocking_request_count'])
+
 
 if __name__ == '__main__':
   unittest.main()

commit cf622a3212c8d8f86ffb5e8a3e10e521c369fe7a
Author: blundell <blundell@chromium.org>
Date:   Wed Jun 8 05:49:54 2016 -0700

    tools/android/loading: Add predicted NoStatePrefetch ms to report.py
    
    This CL adds the predicted time to user satisfaction of a load with
    NoStatePrefetch enabled to report.py.
    
    Review-Url: https://codereview.chromium.org/2046983002
    Cr-Original-Commit-Position: refs/heads/master@{#398531}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5040a22e96658fbe3e10eadfe6d11619ed6603b5

diff --git a/loading/cloud/frontend/bigquery_schema.json b/loading/cloud/frontend/bigquery_schema.json
index 8887c51..b892d8f 100644
--- a/loading/cloud/frontend/bigquery_schema.json
+++ b/loading/cloud/frontend/bigquery_schema.json
@@ -57,6 +57,10 @@
         "type": "FLOAT"
     },
     {
+        "name": "contentful_predicted_no_state_prefetch_ms",
+        "type": "FLOAT"
+    },
+    {
         "name": "contentful_preloaded_requests",
         "type": "INTEGER"
     },
@@ -113,6 +117,10 @@
         "type": "FLOAT"
     },
     {
+        "name": "first_text_predicted_no_state_prefetch_ms",
+        "type": "FLOAT"
+    },
+    {
         "name": "first_text_preloaded_requests",
         "type": "INTEGER"
     },
@@ -161,6 +169,10 @@
         "type": "FLOAT"
     },
     {
+        "name": "plt_predicted_no_state_prefetch_ms",
+        "type": "FLOAT"
+    },
+    {
         "name": "plt_preloaded_requests",
         "type": "INTEGER"
     },
@@ -201,6 +213,10 @@
         "type": "FLOAT"
     },
     {
+        "name": "significant_predicted_no_state_prefetch_ms",
+        "type": "FLOAT"
+    },
+    {
         "name": "significant_preloaded_requests",
         "type": "INTEGER"
     },
diff --git a/loading/graph.py b/loading/graph.py
index 23c4ef8..684fee6 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -191,6 +191,8 @@ class DirectedGraph(object):
     Returns:
       Cost of the longest path.
     """
+    if not self._nodes:
+     return 0
     costs = {n: 0 for n in self._nodes}
     for node in self.TopologicalSort(roots):
       cost = 0
@@ -201,7 +203,6 @@ class DirectedGraph(object):
     if costs_out is not None:
       del costs_out[:]
       costs_out.extend(costs)
-    assert max_cost > 0
     if path_list is not None:
       del path_list[:]
       node = (i for i in self._nodes if costs[i] == max_cost).next()
diff --git a/loading/report.py b/loading/report.py
index 36c7f39..9df11b7 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -13,7 +13,7 @@ from loading_graph_view import LoadingGraphView
 import loading_trace
 import metrics
 from network_activity_lens import NetworkActivityLens
-import prefetch_view
+from prefetch_view import PrefetchSimulationView
 import request_dependencies_lens
 from user_satisfied_lens import (
     FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens,
@@ -42,7 +42,14 @@ class PerUserLensReport(object):
   """Generates a variety of metrics relative to a passed in user lens."""
 
   def __init__(self, trace, user_lens, activity_lens, network_lens,
-               navigation_start_msec, preloaded_requests):
+               navigation_start_msec):
+    requests = trace.request_track.GetEvents()
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        trace)
+    prefetch_view = PrefetchSimulationView(trace, dependencies_lens, user_lens)
+    preloaded_requests = prefetch_view.PreloadedRequests(
+        requests[0], dependencies_lens, trace)
+
     self._navigation_start_msec = navigation_start_msec
 
     self._satisfied_msec = user_lens.SatisfiedMs()
@@ -59,6 +66,8 @@ class PerUserLensReport(object):
     self._cpu_busyness = _ComputeCpuBusyness(activity_lens,
                                              navigation_start_msec,
                                              self._satisfied_msec)
+    prefetch_view.UpdateNodeCosts(lambda n: 0 if n.preloaded else n.cost)
+    self._no_state_prefetch_ms = prefetch_view.Cost()
 
   def GenerateReport(self):
     report = {}
@@ -72,6 +81,7 @@ class PerUserLensReport(object):
                                      self._requests, 0)
     report['preloaded_requests_cost'] = reduce(lambda x,y: x + y.Cost(),
                                         self._preloaded_requests, 0)
+    report['predicted_no_state_prefetch_ms'] = self._no_state_prefetch_ms
 
     # Take the first (earliest) inversion.
     report['inversion'] = self._inversions[0].url if self._inversions else None
@@ -104,12 +114,6 @@ class LoadingReport(object):
     self._navigation_start_msec = min(
         e.start_msec for e in navigation_start_events)
 
-    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
-        self.trace)
-    requests = self.trace.request_track.GetEvents()
-    preloaded_requests = \
-       prefetch_view.PrefetchSimulationView.PreloadedRequests(
-           requests[0], dependencies_lens, self.trace)
     self._dns_requests, self._dns_cost_msec = metrics.DnsRequestsAndCost(trace)
     self._connection_stats = metrics.ConnectionMetrics(trace)
 
@@ -125,8 +129,7 @@ class LoadingReport(object):
                            ['contentful', first_contentful_paint_lens],
                            ['significant', first_significant_paint_lens]]:
       self._user_lens_reports[key] = PerUserLensReport(self.trace,
-          user_lens, activity, network_lens, self._navigation_start_msec,
-          preloaded_requests)
+          user_lens, activity, network_lens, self._navigation_start_msec)
 
     self._transfer_size = metrics.TotalTransferSize(trace)[1]
 
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index a9f6591..cff6de3 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -120,6 +120,13 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertEqual(1, loading_report['first_text_preloaded_requests_cost'])
     self.assertEqual(1, loading_report['contentful_preloaded_requests_cost'])
     self.assertEqual(1, loading_report['significant_preloaded_requests_cost'])
+    self.assertEqual(400, loading_report['plt_predicted_no_state_prefetch_ms'])
+    self.assertEqual(14,
+        loading_report['first_text_predicted_no_state_prefetch_ms'])
+    self.assertEqual(104,
+        loading_report['contentful_predicted_no_state_prefetch_ms'])
+    self.assertEqual(74,
+        loading_report['significant_predicted_no_state_prefetch_ms'])
     self.assertIsNone(loading_report['contentful_inversion'])
     self.assertIsNone(loading_report['significant_inversion'])
     self.assertIsNone(loading_report['ad_requests'])

commit 6d1fc258602e09425d366dc812bc3ce602ffa107
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 8 05:33:18 2016 -0700

    tools/android/loading: Make task_manager.ListResumingTasksToFreeze returning an ordered list
    
    Before, task_manager.ListResumingTasksToFreeze was returning a set
    of tasks, but that could led to tests failures when testing resuming
    file content in task_manager_unittest.CommandLineControlledExecutionTest.
    
    This CL fixes this issue by ordering returning list of tasks to freeze
    ordered as they were discovered by the depth first browsing.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2047513003
    Cr-Original-Commit-Position: refs/heads/master@{#398527}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cb27f2b8edce8a27f346045eba9ebb9d4397eb9f

diff --git a/loading/task_manager.py b/loading/task_manager.py
index 729a8eb..37466e9 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -261,11 +261,12 @@ def ListResumingTasksToFreeze(scenario, final_tasks, skipped_tasks):
     skipped_tasks: Set of Tasks in the scenario that were skipped.
 
   Returns:
-    set(Task)
+    [Task]
   """
   scenario_tasks = set(scenario)
   assert skipped_tasks.issubset(scenario_tasks)
-  frozen_tasks = set()
+  frozen_tasks = []
+  frozen_task_set = set()
   walked_tasks = set()
 
   def InternalWalk(task):
@@ -273,7 +274,9 @@ def ListResumingTasksToFreeze(scenario, final_tasks, skipped_tasks):
       return
     walked_tasks.add(task)
     if task not in scenario_tasks or task not in skipped_tasks:
-      frozen_tasks.add(task)
+      if task not in frozen_task_set:
+        frozen_task_set.add(task)
+        frozen_tasks.append(task)
     else:
       for dependency in task._dependencies:
         InternalWalk(dependency)
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index 40a564a..b4b8742 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -356,17 +356,17 @@ class GenerateScenarioTest(TaskManagerTestCase):
           task_manager.GenerateScenario(final_tasks, resume_frozen_tasks)
       self.assertEqual(skipped_tasks, set(new_scenario))
 
-    RunSubTest([TaskA], set([]), set([TaskA]), set([]))
-    RunSubTest([TaskD], set([]), set([TaskA, TaskD]), set([]))
-    RunSubTest([TaskD], set([]), set([TaskD]), set([TaskA]))
+    RunSubTest([TaskA], set([]), set([TaskA]), [])
+    RunSubTest([TaskD], set([]), set([TaskA, TaskD]), [])
+    RunSubTest([TaskD], set([]), set([TaskD]), [TaskA])
     RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskB, TaskC, TaskE, TaskF]),
-               set([TaskA]))
+               [TaskA])
     RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskC, TaskE, TaskF]),
-               set([TaskA, TaskB]))
-    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskE, TaskF]), set([TaskC]))
-    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskF]), set([TaskC, TaskE]))
+               [TaskA, TaskB])
+    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskE, TaskF]), [TaskC])
+    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskF]), [TaskE, TaskC])
     RunSubTest([TaskD, TaskE, TaskF], set([]), set([TaskD, TaskF]),
-               set([TaskA, TaskE, TaskC]))
+               [TaskA, TaskE, TaskC])
 
 
 class CommandLineControlledExecutionTest(TaskManagerTestCase):

commit 9a0b4a24616f786ff8f17c5000d5ca114b9bc1db
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 8 05:25:48 2016 -0700

    sandwich: Log to different file at each run.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2044703003
    Cr-Original-Commit-Position: refs/heads/master@{#398526}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7297ccc18a353d75936ccc4f26b2a5eb10602130

diff --git a/loading/task_manager.py b/loading/task_manager.py
index 90c9045..729a8eb 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -61,6 +61,7 @@ Example:
 
 import argparse
 import collections
+import datetime
 import logging
 import os
 import re
@@ -73,7 +74,7 @@ import common_util
 _TASK_GRAPH_DOTFILE_NAME = 'tasks_graph.dot'
 _TASK_GRAPH_PNG_NAME = 'tasks_graph.png'
 _TASK_RESUME_ARGUMENTS_FILE = 'resume.txt'
-_TASK_EXECUTION_LOG_NAME = 'task_execution.log'
+_TASK_EXECUTION_LOG_NAME_FORMAT = 'task-execution-%Y-%m-%d-%H-%M-%S.log'
 
 FROMFILE_PREFIX_CHARS = '@'
 
@@ -475,9 +476,11 @@ def ExecuteWithCommandLine(args, default_final_tasks):
       for dependent in dependents_per_task[task]:
         MarkTaskNotToExecute(dependent)
 
+  log_filename = datetime.datetime.now().strftime(
+      _TASK_EXECUTION_LOG_NAME_FORMAT)
   formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
   handler = logging.FileHandler(
-      os.path.join(args.output, _TASK_EXECUTION_LOG_NAME), mode='a')
+      os.path.join(args.output, log_filename), mode='a')
   handler.setFormatter(formatter)
   logging.getLogger().addHandler(handler)
   logging.info(
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index e6960d8..40a564a 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -429,14 +429,10 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
   def testSimple(self):
     self.assertEqual(0, self.Execute([]))
     self.assertListEqual(['a', 'd', 'b', 'c', 'e'], self.task_execution_history)
-    self.assertTrue(
-        os.path.exists(self.OutputPath(task_manager._TASK_EXECUTION_LOG_NAME)))
 
   def testDryRun(self):
     self.assertEqual(0, self.Execute(['-d']))
     self.assertListEqual([], self.task_execution_history)
-    self.assertFalse(
-        os.path.exists(self.OutputPath(task_manager._TASK_EXECUTION_LOG_NAME)))
 
   def testRegex(self):
     self.assertEqual(0, self.Execute(['-e', 'b', '-e', 'd']))

commit 09268af4b9f28b27f51e780992b85d34de0a5aca
Author: droger <droger@chromium.org>
Date:   Tue Jun 7 06:14:18 2016 -0700

    tools/android/loading Add instructions to deploy to staging frontend.
    
    Review-Url: https://codereview.chromium.org/2041323002
    Cr-Original-Commit-Position: refs/heads/master@{#398285}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 22155739d72aa7c92b7628ccd473a542908f2b8c

diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index 775e500..ec9b8d9 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -124,6 +124,16 @@ pip install -r requirements.txt -t lib
 gcloud preview app deploy app.yaml
 ```
 
+To deploy to a staging/test version of the server, you can do:
+
+```shell
+gcloud preview app deploy --no-promote --version $MY_VERSION
+```
+
+where `MY_VERSION` can be something like `staging` or something more unique to
+ensure there is no name collision. You can then access the application live on
+the web by prefixing the URL of the service with `$MY_VERSION-dot-`.
+
 ### Updating the Database Schema
 
 When a change is made to the dictionary returned by report.py, the BigQuery

commit 719e8afc775eeb97481b1a320d56d22cccd7e6bb
Author: droger <droger@chromium.org>
Date:   Tue Jun 7 05:32:24 2016 -0700

    tools/android/loading Fix precision and formatting issues in frontend
    
    Fix rounding issue:
    int(x + 0.5) does not round up correctly.
    This should be round(x+0.5) or math.ceil(x).
    
    Fix formatting issue: the timeout is a float, and should be printed
    using %f instead of %i.
    
    Review-Url: https://codereview.chromium.org/2037373003
    Cr-Original-Commit-Position: refs/heads/master@{#398277}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 89521b32d117da52d09a28419caf3ca303f3b761

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 6a5877b..4a63385 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import logging
+import math
 import os
 import sys
 import time
@@ -359,8 +360,8 @@ def StartFromJsonString(http_body_str):
   # Compute the number of required instances if not specified.
   if not task.BackendParams().get('instance_count'):
     target_parallel_duration_s = 1800.0 # 30 minutes.
-    task.BackendParams()['instance_count'] = int(
-        sequential_duration_s / target_parallel_duration_s + 0.5)  # Rounded up.
+    task.BackendParams()['instance_count'] = math.ceil(
+        sequential_duration_s / target_parallel_duration_s)
 
   # Check the instance quotas.
   clovis_logger.info(
@@ -383,7 +384,7 @@ def StartFromJsonString(http_body_str):
     # Timeout is at least 1 hour.
     task.BackendParams()['timeout_hours'] = max(1, 5 * expected_duration_h)
   clovis_logger.info(
-      'Timeout delay: %i hours. ' % task.BackendParams()['timeout_hours'])
+      'Timeout delay: %.1f hours. ' % task.BackendParams()['timeout_hours'])
 
   if not EnqueueTasks(sub_tasks, task_tag):
     return Render('Task creation failed.', memory_logs)

commit 7b79af62eef37b67a8ec3b50964c303db13e7bc7
Author: pasko <pasko@chromium.org>
Date:   Tue Jun 7 02:58:05 2016 -0700

    Set tracing categories explicitly in Clovis and Sandwich
    
    Also restrict the categories in Sandwich to reduce the size of traces by 2x.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2047463002
    Cr-Original-Commit-Position: refs/heads/master@{#398262}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ca2594ea175abc0daf257cff17b46b9418dad29a

diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index 839809a..81c5837 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -7,6 +7,7 @@ import copy
 import unittest
 
 from activity_lens import (ActivityLens, _EventsTree)
+import clovis_constants
 import test_utils
 import tracing
 
@@ -14,13 +15,15 @@ import tracing
 class ActivityLensTestCase(unittest.TestCase):
   @classmethod
   def _EventsFromRawEvents(cls, raw_events):
-    tracing_track = tracing.TracingTrack(None)
+    tracing_track = tracing.TracingTrack(None,
+        clovis_constants.DEFAULT_CATEGORIES)
     tracing_track.Handle(
         'Tracing.dataCollected', {'params': {'value': raw_events}})
     return tracing_track.GetEvents()
 
   def setUp(self):
-    self.tracing_track = tracing.TracingTrack(None)
+    self.tracing_track = tracing.TracingTrack(None,
+        clovis_constants.DEFAULT_CATEGORIES)
 
   def testGetRendererMainThread(self):
     first_renderer_tid = 12345
diff --git a/loading/analyze.py b/loading/analyze.py
index 9564acb..91ed008 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -25,6 +25,7 @@ import devil_chromium
 from pylib import constants
 
 import activity_lens
+import clovis_constants
 import content_classification_lens
 import controller
 import device_setup
@@ -121,7 +122,8 @@ def _LogRequests(url, clear_cache_override=None):
       if clear_cache:
         connection.ClearCache()
       trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-          url, connection, chrome_ctl.ChromeMetadata())
+          url, connection, chrome_ctl.ChromeMetadata(),
+          categories=clovis_constants.DEFAULT_CATEGORIES)
   except controller.ChromeControllerError as e:
     e.Dump(sys.stderr)
     raise
diff --git a/loading/clovis_constants.py b/loading/clovis_constants.py
new file mode 100644
index 0000000..e64ddf0
--- /dev/null
+++ b/loading/clovis_constants.py
@@ -0,0 +1,21 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Provides common constants for Clovis."""
+
+QUEUING_CATEGORY = 'disabled-by-default-loading.resource'
+
+# Categories to enable or disable for all traces collected. Disabled categories
+# are prefixed with '-'.
+DEFAULT_CATEGORIES = [
+  QUEUING_CATEGORY,
+  'blink',
+  'blink.net',
+  'blink.user_timing',
+  'devtools.timeline',
+  'disabled-by-default-blink.debug.layout',
+  'toplevel',
+  'v8',
+  '-cc',  # Contains a lot of events, none of which we use.
+]
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 0804e27..c170e6b 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -76,7 +76,7 @@ class LoadingTrace(object):
 
   @classmethod
   def RecordUrlNavigation(
-      cls, url, connection, chrome_metadata, additional_categories=None,
+      cls, url, connection, chrome_metadata, categories,
       timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
     """Create a loading trace by using controller to fetch url.
 
@@ -84,8 +84,7 @@ class LoadingTrace(object):
       url: (str) url to fetch.
       connection: An opened devtools connection.
       chrome_metadata: Dictionary of chrome metadata.
-      additional_categories: ([str] or None) TracingTrack additional categories
-                             to capture.
+      categories: as in tracing.TracingTrack
       timeout_seconds: monitoring connection timeout in seconds.
 
     Returns:
@@ -93,9 +92,7 @@ class LoadingTrace(object):
     """
     page = page_track.PageTrack(connection)
     request = request_track.RequestTrack(connection)
-    trace = tracing.TracingTrack(
-        connection,
-        additional_categories=additional_categories)
+    trace = tracing.TracingTrack(connection, categories)
     start_date_str = datetime.datetime.utcnow().isoformat()
     seconds_since_epoch=time.time()
     connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
diff --git a/loading/queuing_lens.py b/loading/queuing_lens.py
index 268407a..97f87dc 100644
--- a/loading/queuing_lens.py
+++ b/loading/queuing_lens.py
@@ -12,12 +12,12 @@ import collections
 import itertools
 import logging
 
-import tracing
+import clovis_constants
 
 
 class QueuingLens(object):
   """Attaches queuing related trace events to request objects."""
-  QUEUING_CATEGORY = tracing.QUEUING_CATEGORY
+  QUEUING_CATEGORY = clovis_constants.QUEUING_CATEGORY
   ASYNC_NAME = 'ScheduledResourceRequest'
   READY_NAME = 'ScheduledResourceRequest.Ready'
   SET_PRIORITY_NAME = 'ScheduledResourceRequest.SetPriority'
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index c0ce2a4..152e07e 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -53,8 +53,8 @@ _MINIMALIST_TRACE_EVENTS = [
 def TracingTrack(events):
   return tracing.TracingTrack.FromJsonDict({
       'events': events,
-      'categories': (tracing.INITIAL_CATEGORIES +
-          (sandwich_runner.MEMORY_DUMP_CATEGORY,))})
+      'categories': (sandwich_runner._TRACING_CATEGORIES +
+          [sandwich_runner.MEMORY_DUMP_CATEGORY])})
 
 
 def LoadingTrace(events):
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index d2ab1b3..23df7cf 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -34,6 +34,18 @@ MEMORY_DUMP_CATEGORY = 'disabled-by-default-memory-infra'
 # network condition.
 _DEVTOOLS_TIMEOUT = 60
 
+# Categories to enable or disable for all traces collected. Disabled categories
+# are prefixed with '-'.
+_TRACING_CATEGORIES = [
+  'blink',
+  'blink.net',
+  'blink.user_timing',
+  'devtools.timeline',
+  'java',
+  'toplevel',
+  'v8',
+  '-cc',  # A lot of unnecessary events are enabled by default in "cc".
+]
 
 def _CleanArtefactsFromPastRuns(output_directories_path):
   """Cleans artifacts generated from past run in the output directory.
@@ -147,13 +159,23 @@ class SandwichRunner(object):
         os.makedirs(run_path)
     self._chrome_ctl.SetNetworkEmulation(
         self._GetEmulatorNetworkCondition('browser'))
-    additional_categories = []
+    categories = _TRACING_CATEGORIES
     if self.record_memory_dumps:
-      additional_categories = [MEMORY_DUMP_CATEGORY]
+      categories += [MEMORY_DUMP_CATEGORY]
     # TODO(gabadie): add a way to avoid recording a trace.
     with self._chrome_ctl.Open() as connection:
       if clear_cache:
         connection.ClearCache()
+
+      # Binds all parameters of RecordUrlNavigation() to avoid repetition.
+      def RecordTrace():
+        return loading_trace.LoadingTrace.RecordUrlNavigation(
+            url=self.url,
+            connection=connection,
+            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+            categories=categories,
+            timeout_seconds=_DEVTOOLS_TIMEOUT)
+
       if run_path is not None and self.record_video:
         device = self._chrome_ctl.GetDevice()
         if device is None:
@@ -161,19 +183,9 @@ class SandwichRunner(object):
         video_recording_path = os.path.join(run_path, VIDEO_FILENAME)
         with device_setup.RemoteSpeedIndexRecorder(device, connection,
                                                    video_recording_path):
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url=self.url,
-              connection=connection,
-              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              additional_categories=additional_categories,
-              timeout_seconds=_DEVTOOLS_TIMEOUT)
+          trace = RecordTrace()
       else:
-        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-            url=self.url,
-            connection=connection,
-            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            additional_categories=additional_categories,
-            timeout_seconds=_DEVTOOLS_TIMEOUT)
+        trace = RecordTrace()
     if run_path is not None:
       trace_path = os.path.join(run_path, TRACE_FILENAME)
       trace.ToJsonFile(trace_path)
diff --git a/loading/test_utils.py b/loading/test_utils.py
index cb2fd2e..86303f9 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -4,6 +4,7 @@
 
 """Common utilities used in unit tests, within this directory."""
 
+import clovis_constants
 import dependency_graph
 import devtools_monitor
 import loading_trace
@@ -140,7 +141,8 @@ def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   request = FakeRequestTrack(requests)
   page_event_track = FakePageTrack(page_events if page_events else [])
   if trace_events is not None:
-    tracing_track = tracing.TracingTrack(None)
+    tracing_track = tracing.TracingTrack(None,
+        clovis_constants.DEFAULT_CATEGORIES)
     tracing_track.Handle('Tracing.dataCollected',
                          {'params': {'value': [e for e in trace_events]}})
   else:
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index 1db5da7..f6dbf96 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -41,6 +41,7 @@ import urlparse
 
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 
+import clovis_constants
 import controller
 import loading_trace
 import options
@@ -234,7 +235,8 @@ def RunTest(webserver, test_page, expected):
     connection.ClearCache()
     observed_seq = InitiatorSequence(
         loading_trace.LoadingTrace.RecordUrlNavigation(
-            url, connection, chrome_controller.ChromeMetadata()))
+            url, connection, chrome_controller.ChromeMetadata(),
+            categories=clovis_constants.DEFAULT_CATEGORIES))
   if observed_seq == expected:
     sys.stdout.write(' ok\n')
     return True
diff --git a/loading/tracing.py b/loading/tracing.py
index faa03ec..3bf5246 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -4,43 +4,27 @@
 
 """Monitor tracing events on chrome via chrome remote debugging."""
 
-import bisect
 import itertools
 import logging
 import operator
 
+import clovis_constants
 import devtools_monitor
 
 
-QUEUING_CATEGORY = 'disabled-by-default-loading.resource'
-_ENABLED_CATEGORIES = (
-    ('toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
-     'blink.user_timing', 'blink.net', 'disabled-by-default-blink.debug.layout')
-    + (QUEUING_CATEGORY,))
-_DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
-INITIAL_CATEGORIES = (
-    _ENABLED_CATEGORIES
-    + tuple('-' + cat for cat in _DISABLED_CATEGORIES))
-
-
 class TracingTrack(devtools_monitor.Track):
   """Grabs and processes trace event messages.
 
   See https://goo.gl/Qabkqk for details on the protocol.
   """
-  def __init__(self, connection, additional_categories=None,
-               disabled_categories=None,
-               fetch_stream=False):
+  def __init__(self, connection, categories, fetch_stream=False):
     """Initialize this TracingTrack.
 
     Args:
       connection: a DevToolsConnection.
-      additional_categories: ([str] or None) If set, a list of additional
-                             categories to add. This cannot be used to re-enable
-                             a category which is disabled by default (see
-                             _DISABLED_CATEGORIES), nor to disable a category.
-      disabled_categories: If set, a set of categories from _ENABLED_CATEGORIES
-                           to disable.
+      categories: ([str] or None) If set, a list of categories to enable or
+                  disable in Chrome tracing. Categories prefixed with '-' are
+                  disabled.
       fetch_stream: if true, use a websocket stream to fetch tracing data rather
         than dataCollected events. It appears based on very limited testing that
         a stream is slower than the default reporting as dataCollected events.
@@ -49,28 +33,7 @@ class TracingTrack(devtools_monitor.Track):
     if connection:
       connection.RegisterListener('Tracing.dataCollected', self)
 
-    categories_to_enable = _ENABLED_CATEGORIES
-    categories_to_disable = _DISABLED_CATEGORIES
-
-    if disabled_categories:
-      assert not any(cat.startswith('-') for cat in disabled_categories), (
-          'Specify categories to disable without an initial -')
-      assert set(disabled_categories).issubset(set(_ENABLED_CATEGORIES)), (
-          'Can only disable categories that are enabled by default')
-      categories_to_enable = (
-          set(categories_to_enable).difference(set(disabled_categories)))
-      categories_to_disable += disabled_categories
-
-    if additional_categories:
-      assert not any(cat.startswith('-') for cat in additional_categories), (
-          'Use disabled_categories to disable a category')
-      assert not (set(additional_categories) & set(_DISABLED_CATEGORIES)), (
-          'Cannot enable a disabled category')
-      categories_to_enable += tuple(additional_categories)
-
-    self._categories = set(
-        itertools.chain(categories_to_enable,
-                        tuple('-' + cat for cat in categories_to_disable)))
+    self._categories = set(categories)
     params = {}
     params['categories'] = ','.join(self._categories)
     if fetch_stream:
@@ -171,7 +134,7 @@ class TracingTrack(devtools_monitor.Track):
       events = filter(
           lambda e : set(e.category.split(',')).intersection(categories),
           events)
-    tracing_track = TracingTrack(None)
+    tracing_track = TracingTrack(None, clovis_constants.DEFAULT_CATEGORIES)
     tracing_track._events = events
     tracing_track._categories = self._categories
     if categories is not None:
@@ -188,7 +151,7 @@ class TracingTrack(devtools_monitor.Track):
       return None
     assert 'events' in json_dict
     events = [Event(e) for e in json_dict['events']]
-    tracing_track = TracingTrack(None)
+    tracing_track = TracingTrack(None, clovis_constants.DEFAULT_CATEGORIES)
     tracing_track._categories = set(json_dict.get('categories', []))
     tracing_track._events = events
     tracing_track._base_msec = events[0].start_msec if events else 0
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 3ad6617..f621c1f 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -42,7 +42,7 @@ class TracingTrackTestCase(unittest.TestCase):
   def setUp(self):
     self.tree_threshold = _IntervalTree._TRESHOLD
     _IntervalTree._TRESHOLD = 2  # Expose more edge cases in the tree.
-    self.track = TracingTrack(None, additional_categories=('A', 'B', 'C', 'D'))
+    self.track = TracingTrack(None, ['A', 'B', 'C', 'D'])
 
   def tearDown(self):
     _IntervalTree._TRESHOLD = self.tree_threshold
@@ -360,26 +360,6 @@ class TracingTrackTestCase(unittest.TestCase):
     self.assertSetEqual(
         set('A'), self.track.Filter(categories=set('A')).Categories())
 
-  def testAdditionalCategories(self):
-    track = TracingTrack(None, additional_categories=('best-category-ever',))
-    self.assertIn('best-category-ever', track.Categories())
-    # Cannot re-enable a category.
-    with self.assertRaises(AssertionError):
-      TracingTrack(None, additional_categories=('cc',))
-    # Cannot disable categories via |additional_categories|.
-    with self.assertRaises(AssertionError):
-      TracingTrack(None, additional_categories=('-best-category-ever',))
-
-  def testDisabledCategories(self):
-    track = TracingTrack(None, disabled_categories=('toplevel',))
-    self.assertNotIn('toplevel', track.Categories())
-    self.assertIn('-toplevel', track.Categories())
-    # Can only disable categories that are enabled by default.
-    with self.assertRaises(AssertionError):
-      TracingTrack(None, disabled_categories=('best-category-ever',))
-    with self.assertRaises(AssertionError):
-      TracingTrack(None, disabled_categories=('cc',))
-
   def _HandleEvents(self, events):
     self.track.Handle('Tracing.dataCollected', {'params': {'value': [
         self.EventToMicroseconds(e) for e in events]}})

commit 015dcf39f14aa7e9bc0026f211ca85badf4b6b48
Author: droger <droger@chromium.org>
Date:   Mon Jun 6 09:14:05 2016 -0700

    tools/android/loading Add timestamps to backend logs.
    
    Review-Url: https://codereview.chromium.org/2039723003
    Cr-Original-Commit-Position: refs/heads/master@{#398043}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2afff3487e1ea9ac76233418a029587566253a73

diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 11bc744..b27ef35 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -7,6 +7,7 @@ import json
 import logging
 import os
 import sys
+import time
 
 from googleapiclient import discovery
 from oauth2client.client import GoogleCredentials
@@ -205,7 +206,10 @@ if __name__ == '__main__':
   args = parser.parse_args()
 
   # Configure logging.
-  logging.basicConfig(level=logging.WARNING)
+  logging.basicConfig(level=logging.WARNING,
+                      format='[%(asctime)s][%(levelname)s] %(message)s',
+                      datefmt='%y-%m-%d %H:%M:%S')
+  logging.Formatter.converter = time.gmtime
   worker_logger = logging.getLogger('worker')
   worker_logger.setLevel(logging.INFO)
 
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index ecb19ee..6a5877b 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -23,6 +23,7 @@ from memory_logs import MemoryLogs
 
 
 # Global variables.
+logging.Formatter.converter = time.gmtime
 clovis_logger = logging.getLogger('clovis_frontend')
 clovis_logger.setLevel(logging.DEBUG)
 project_name = app_identity.get_application_id()

commit fdc2a28b5506a6082a541654b71ce64aca9a4065
Author: droger <droger@chromium.org>
Date:   Mon Jun 6 09:12:38 2016 -0700

    tools/android/loading Switch from multiprocessing Pool to Process
    
    Pool does not allow to hard kill a process, which can lead
    to process leaks if a process does not die on terminate.
    
    Pool also sometimes crashes the main process in terminate(),
    interrupting trace generation.
    
    Review-Url: https://codereview.chromium.org/2037073002
    Cr-Original-Commit-Position: refs/heads/master@{#398041}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1c8f4c8908c64620cc2becdc971c3b779d9827b2

diff --git a/loading/cloud/backend/multiprocessing_helper.py b/loading/cloud/backend/multiprocessing_helper.py
new file mode 100644
index 0000000..3bd7879
--- /dev/null
+++ b/loading/cloud/backend/multiprocessing_helper.py
@@ -0,0 +1,88 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import multiprocessing
+import os
+import Queue
+import resource
+import signal
+
+import psutil
+
+
+def _LimitMemory(memory_share):
+  """Limits the memory available to this process, to avoid OOM issues.
+
+  Args:
+    memory_share: (float) Share coefficient of the total physical memory that
+                          the process can use.
+  """
+  total_memory = psutil.virtual_memory().total
+  memory_limit = memory_share * total_memory
+  resource.setrlimit(resource.RLIMIT_AS, (memory_limit, -1L))
+
+
+def _MultiprocessingWrapper(queue, memory_share, function, args):
+  """Helper function that sets a memory limit on the current process, then
+  calls |function| on |args| and writes the results to |queue|.
+
+  Args:
+    queue: (multiprocessing.Queue) Queue where the results of the wrapped
+           function are written.
+    memory_share: (float) Share coefficient of the total physical memory that
+                          the process can use.
+    function: The wrapped function.
+    args: (list) Arguments for the wrapped function.
+  """
+  try:
+    if memory_share:
+      _LimitMemory(memory_share)
+
+    queue.put(function(*args))
+  except Exception:
+    queue.put(None)
+
+
+def RunInSeparateProcess(function, args, logger, timeout_seconds,
+                         memory_share=None):
+  """Runs a function in a separate process, and kills it after the timeout is
+  reached.
+
+  Args:
+    function: The function to run.
+    args: (list) Arguments for the wrapped function.
+    timeout_seconds: (float) Timeout in seconds after which the subprocess is
+                     terminated.
+    memory_share: (float) Set this parameter to limit the memory available to
+                  the spawned subprocess. This is a ratio of the total system
+                  memory (between 0 and 1).
+  Returns:
+    The result of the wrapped function, or None if the call failed.
+  """
+  queue = multiprocessing.Queue()
+  process = multiprocessing.Process(target=_MultiprocessingWrapper,
+                                    args=(queue, memory_share, function, args))
+  process.daemon = True
+  process.start()
+
+  result = None
+
+  try:
+    logger.info('Wait for result.')
+    # Note: If the subprocess somehow crashes (e.g. Python crashing), this
+    # process will wait the full timeout. Could be avoided but probably not
+    # worth the extra complexity.
+    result = queue.get(block=True, timeout=timeout_seconds)
+  except Queue.Empty:
+    logger.warning('Subprocess timeout.')
+    process.terminate()
+
+  logger.info('Wait for process to terminate.')
+  process.join(timeout=5)
+
+  if process.is_alive():
+    logger.warning('Process still alive, hard killing now.')
+    os.kill(process.pid, signal.SIGKILL)
+
+  return result
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index b705326..b9fdd6b 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -2,37 +2,22 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-import multiprocessing
 import os
 import re
-import resource
 import sys
 import traceback
 
-import psutil
-
 import common.clovis_paths
 from common.clovis_task import ClovisTask
 from common.loading_trace_database import LoadingTraceDatabase
 import controller
 from failure_database import FailureDatabase
 import loading_trace
+import multiprocessing_helper
 import options
 import xvfb_helper
 
 
-def LimitMemory(memory_share):
-  """Limits the memory available to this process, to avoid OOM issues.
-
-  Args:
-    memory_share: (float) Share coefficient of the total physical memory that
-                          the process can use.
-  """
-  total_memory = psutil.virtual_memory().total
-  memory_limit = memory_share * total_memory
-  resource.setrlimit(resource.RLIMIT_AS, (memory_limit, -1L))
-
-
 def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
   """ Generates a trace.
 
@@ -64,6 +49,8 @@ def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
     try:
       sys.stderr = sys.stdout
 
+      sys.stdout.write('Starting trace generation for: %s.\n' % url)
+
       # Set up the controller.
       chrome_ctl = controller.LocalChromeController()
       chrome_ctl.SetChromeEnvOverride(xvfb_helper.GetChromeEnvironment())
@@ -79,6 +66,7 @@ def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
             url, connection, chrome_ctl.ChromeMetadata())
         trace_metadata['succeeded'] = True
         trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+        sys.stdout.write('Trace generation success.\n')
     except controller.ChromeControllerError as e:
       e.Dump(sys.stderr)
     except Exception as e:
@@ -86,7 +74,12 @@ def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
       traceback.print_exc(file=sys.stderr)
 
     if trace:
+      sys.stdout.write('Dumping trace to file.\n')
       trace.ToJsonFile(filename)
+    else:
+      sys.stderr.write('No trace generated.\n')
+
+    sys.stdout.write('Trace generation finished.\n')
 
   sys.stdout = old_stdout
   sys.stderr = old_stderr
@@ -166,46 +159,19 @@ class TraceTaskHandler(object):
     and return values.
     """
     self._logger.info('Starting external process for trace generation.')
-    failed_metadata = {'succeeded':False, 'url':url}
-    failed = False
-    pool = multiprocessing.Pool(1, initializer=LimitMemory, initargs=(0.9,))
-
-    apply_result = pool.apply_async(
+    result = multiprocessing_helper.RunInSeparateProcess(
         GenerateTrace,
-        (url, emulate_device, emulate_network, filename, log_filename))
-    pool.close()
-    apply_result.wait(timeout=180)
+        (url, emulate_device, emulate_network, filename, log_filename),
+        self._logger, timeout_seconds=180, memory_share=0.9)
 
-    if not apply_result.ready():
-      self._logger.error('Process timeout for trace generation of URL: ' + url)
+    self._logger.info('Cleaning up Chrome processes.')
+    controller.LocalChromeController.KillChromeProcesses()
+
+    if not result:
       self._failure_database.AddFailure('trace_process_timeout', url)
-      # Explicitly kill Chrome now, or pool.terminate() will hang.
-      controller.LocalChromeController.KillChromeProcesses()
-      pool.terminate()
-      failed = True
-    elif not apply_result.successful():
-      # Try to reraise the exception that killed the subprocess and add it to
-      # the error log.
-      try:
-        apply_result.get()
-      except Exception as e:
-        with file(log_filename, 'w+') as error_log:
-          error_log.write('Unhandled exception caught by apply_result: {}'
-                          .format(e))
-          traceback.print_exc(file=error_log)
-      else:
-        with file(log_filename, 'w+') as error_log:
-          error_log.write('No exception found for unsuccessful apply_result')
-      self._logger.error('Process failure for trace generation of URL: ' + url)
-      self._failure_database.AddFailure('trace_process_error', url)
-      failed = True
-
-    self._logger.info('Cleaning up external process.')
-    pool.join()
-
-    if failed:
-      return failed_metadata
-    return apply_result.get()
+      return {'succeeded':False, 'url':url}
+    return result
+
 
   def _HandleTraceGenerationResults(self, local_filename, log_filename,
                                     remote_filename, trace_metadata):

commit d991260d5b0ffb90395a73add4ebb321a501c964
Author: gabadie <gabadie@chromium.org>
Date:   Mon Jun 6 08:42:38 2016 -0700

    sandwich: Uplift for production use.
    
    This CL contains several change to uplift sandwich:
    
    1) Finds Android DeviceUtil object from device serial only once
    per command line to speed up sandwich graph generation.
    
    2) Makes SandwichRunner.output_dir mandatory so it can always dumps
    error files in case of error or intermittent failures.
    
    3) Lets AndroidChromeController dismiss crash dialogs when browser
    or renderer process crashes.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2040993002
    Cr-Original-Commit-Position: refs/heads/master@{#398032}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cb1fb319fa271c43e982f256c9365f987c4f9d3c

diff --git a/loading/controller.py b/loading/controller.py
index 452b548..e6ac299 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -115,6 +115,10 @@ class ChromeControllerError(Exception):
     """Returns whether the error is an known intermittent error."""
     return self.error_type in self._INTERMITTENT_WHITE_LIST
 
+  def RaiseOriginal(self):
+    """Raises the original exception that has caused <self>."""
+    raise self.error_type, self.error_value, self.error_traceback
+
 
 class ChromeControllerBase(object):
   """Base class for all controllers.
@@ -326,6 +330,7 @@ class RemoteChromeController(ChromeControllerBase):
         subprocess.list2cmdline(chrome_args)))
     with device_setup.FlagReplacer(
         self._device, command_line_path, self._GetChromeArguments()):
+      self._DismissCrashDialogIfNeeded()
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
@@ -362,6 +367,7 @@ class RemoteChromeController(ChromeControllerBase):
         raise ChromeControllerError(log=logcat)
       finally:
         self._device.ForceStop(package_info.package)
+        self._DismissCrashDialogIfNeeded()
 
   def ResetBrowserState(self):
     """Override resetting Chrome local state."""
@@ -402,6 +408,11 @@ class RemoteChromeController(ChromeControllerBase):
       yield
     self._wpr_attributes = None
 
+  def _DismissCrashDialogIfNeeded(self):
+    for _ in xrange(10):
+      if not self._device.DismissCrashDialogIfNeeded():
+        break
+
 
 class LocalChromeController(ChromeControllerBase):
   """Controller for a local (desktop) chrome instance."""
diff --git a/loading/sandwich.py b/loading/sandwich.py
index e428372..57ef0cc 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -162,10 +162,10 @@ def _RecordWebServerTestTrace(args):
   return 0
 
 
-def _GenerateBenchmarkTasks(args, url, output_subdirectory):
+def _GenerateBenchmarkTasks(args, android_device, url, output_subdirectory):
   MAIN_TRANSFORMER_LIST_NAME = 'no-network-emulation'
   common_builder = sandwich_utils.SandwichCommonBuilder(
-      android_device=_GetAndroidDeviceFromArgs(args),
+      android_device=android_device,
       url=url,
       output_directory=args.output,
       output_subdirectory=output_subdirectory)
@@ -219,6 +219,7 @@ def _RunAllMain(args):
   urls = ReadUrlsFromCorpus(args.corpus)
   domain_times_encountered_per_domain = {}
   default_final_tasks = []
+  android_device = _GetAndroidDeviceFromArgs(args)
   for url in urls:
     domain = '.'.join(urlparse(url).netloc.split('.')[-2:])
     domain_times_encountered = domain_times_encountered_per_domain.get(
@@ -226,7 +227,7 @@ def _RunAllMain(args):
     output_subdirectory = '{}.{}'.format(domain, domain_times_encountered)
     domain_times_encountered_per_domain[domain] = domain_times_encountered + 1
     default_final_tasks.extend(
-        _GenerateBenchmarkTasks(args, url, output_subdirectory))
+        _GenerateBenchmarkTasks(args, android_device, url, output_subdirectory))
   return task_manager.ExecuteWithCommandLine(args, default_final_tasks)
 
 
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 9ed8258..d2ab1b3 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -22,6 +22,7 @@ import loading_trace
 
 
 # Standard filenames in the sandwich runner's output directory.
+ERROR_FILENAME = 'error'
 TRACE_FILENAME = 'trace.json'
 VIDEO_FILENAME = 'video.mp4'
 WPR_LOG_FILENAME = 'wpr.log'
@@ -140,7 +141,7 @@ class SandwichRunner(object):
         trace or video will be saved.
     """
     run_path = None
-    if self.output_dir is not None and run_id is not None:
+    if run_id is not None:
       run_path = os.path.join(self.output_dir, str(run_id))
       if not os.path.isdir(run_path):
         os.makedirs(run_path)
@@ -191,13 +192,15 @@ class SandwichRunner(object):
         self._RunNavigation(clear_cache=clear_cache, run_id=run_id)
         break
       except controller.ChromeControllerError as error:
+        if error.IsIntermittent() and attempt_id + 1 != self._ATTEMPT_COUNT:
+          dump_filename = 'intermittent_failure{}'.format(attempt_id)
+          dump_path = os.path.join(self.output_dir, str(run_id), dump_filename)
+        else:
+          dump_path = os.path.join(self.output_dir, ERROR_FILENAME)
+        with open(dump_path, 'w') as dump_output:
+          error.Dump(dump_output)
         if not error.IsIntermittent():
-          raise
-        if self.output_dir is not None:
-          dump_path = os.path.join(self.output_dir, str(run_id),
-                                   'error{}'.format(attempt_id))
-          with open(dump_path, 'w') as dump_output:
-            error.Dump(dump_output)
+          error.RaiseOriginal()
     else:
       logging.error('Failed to navigate to %s after %d attemps' % \
                     (self.url, self._ATTEMPT_COUNT))
@@ -214,10 +217,10 @@ class SandwichRunner(object):
 
   def Run(self):
     """SandwichRunner main entry point meant to be called once configured."""
+    assert self.output_dir is not None
     assert self._chrome_ctl == None
     assert self._local_cache_directory_path == None
-    if self.output_dir:
-      self._CleanTraceOutputDirectory()
+    self._CleanTraceOutputDirectory()
 
     if self.android_device:
       self._chrome_ctl = controller.RemoteChromeController(self.android_device)
@@ -227,11 +230,6 @@ class SandwichRunner(object):
     self._chrome_ctl.AddChromeArguments(self.chrome_args)
     if self.cache_operation == CacheOperation.SAVE:
       self._chrome_ctl.SetSlowDeath()
-
-    wpr_log_path = None
-    if self.output_dir:
-      wpr_log_path = os.path.join(self.output_dir, WPR_LOG_FILENAME)
-
     try:
       if self.cache_operation == CacheOperation.PUSH:
         assert os.path.isfile(self.cache_archive_path)
@@ -243,7 +241,7 @@ class SandwichRunner(object):
           record=self.wpr_record,
           network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
           disable_script_injection=self.disable_wpr_script_injection,
-          out_log_path=wpr_log_path):
+          out_log_path=os.path.join(self.output_dir, WPR_LOG_FILENAME)):
         for repeat_id in xrange(self.repeat):
           self._RunUrl(run_id=repeat_id)
     finally:
diff --git a/loading/sandwich_utils.py b/loading/sandwich_utils.py
index 3942eec..791c3ec 100644
--- a/loading/sandwich_utils.py
+++ b/loading/sandwich_utils.py
@@ -71,6 +71,7 @@ class SandwichCommonBuilder(task_manager.Builder):
       runner = self.CreateSandwichRunner()
       runner.wpr_archive_path = BuildOriginalWpr.path
       runner.wpr_record = True
+      runner.output_dir = BuildOriginalWpr.path[:-4] + '-run'
       runner.Run()
 
     self.original_wpr_task = BuildOriginalWpr

commit 5803bbe38f610526656b212de0d7af38d193785d
Author: gabadie <gabadie@chromium.org>
Date:   Mon Jun 6 07:36:12 2016 -0700

    sandwich: Merge cache-validation.json and urls-for-resources.json tasks
    
    Before in the NoState-Prefetch benchmark tasks graph, Sandwich had
    the common/cache-validation.json tasks as a leaf. But it could
    lead to issue when selecting final tasks with -e REGEX, and
    but forgetting to have the regex matching this important task as
    well as the subset of task the user initially wanted to run.
    
    This CL addresses this issue by merging the previous tasks
    common/cache-validation.json and common/urls-for-resources.json
    into a single common/patched-cache-validation.json tasks, and
    have all the benchmark subgraphs depending on them so we are
    sure to always run the cache validation at least once before the
    benchmark runs.
    
    This CL take this oportunity to output further statistics in the
    CSV related to the cache genration such as the number of
    sub-resources of an URL, the number of expected cached resources
    and the actual number of successfully cached resources, to
    identify at scale on many URLs from the CSVs potential issues
    the WPR patcher could still have on some webpages.
    
    Moreover, this CL take this change in the extract metrics task
    change as an opportunity to parse the fat loading trace JSON
    file only once per URL repeat row, in order to make theses tasks
    faster.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2033093002
    Cr-Original-Commit-Position: refs/heads/master@{#398021}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b20272f4bed6bf1775113960a2f2fbe70f74562d

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 351bd83..f46e79a 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -135,7 +135,7 @@ def _GetWebPageTrackedEvents(tracing_track):
       # hence the first navigationStart in the trace registers the correct frame
       # id.
       if event_name == 'navigationStart':
-        logging.info('Found navigationStart at: %f', event.start_msec)
+        logging.info('  Found navigationStart at: %f', event.start_msec)
         main_frame_id = event.args['frame']
       continue
 
@@ -147,7 +147,7 @@ def _GetWebPageTrackedEvents(tracing_track):
     # important events (like requestStart) do not have a frame id attached.
     if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
       tracked_events[event_name] = event
-      logging.info('Event %s first appears at: %f', event_name,
+      logging.info('  Event %s first appears at: %f', event_name,
           event.start_msec)
   return tracked_events
 
diff --git a/loading/sandwich_prefetch.py b/loading/sandwich_prefetch.py
index 173d0bf..ab093c2 100644
--- a/loading/sandwich_prefetch.py
+++ b/loading/sandwich_prefetch.py
@@ -2,6 +2,19 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+"""
+Implements a task builder for benchmarking effects of NoState Prefetch.
+Noticeable steps of the task pipeline:
+  * Save a WPR archive
+  * Process the WPR archive to make all resources cacheable
+  * Process cache archive to patch response headers back to their original
+      values.
+  * Find out which resources are discoverable by NoState Prefetch
+      (HTMLPreloadScanner)
+  * Load pages with empty/full/prefetched cache
+  * Extract most important metrics to a CSV
+"""
+
 import csv
 import logging
 import json
@@ -198,10 +211,10 @@ def _ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
     assert False
 
   whitelisted_urls = set()
-  logging.info('white-listing %s' % first_resource_request.url)
   for request in _FilterOutDataAndIncompleteRequests(discovered_requests):
-    logging.info('white-listing %s' % request.url)
+    logging.debug('white-listing %s', request.url)
     whitelisted_urls.add(request.url)
+  logging.info('number of white-listed resources: %d', len(whitelisted_urls))
   return whitelisted_urls
 
 
@@ -261,37 +274,32 @@ def _ListUrlRequests(trace, request_kind):
   return urls
 
 
-def _VerifyBenchmarkOutputDirectory(benchmark_setup_path,
-                                    benchmark_output_directory_path):
-  """Verifies that all run inside the run_output_directory worked as expected.
-
-  Args:
-    benchmark_setup_path: Path of the JSON of the benchmark setup.
-    benchmark_output_directory_path: Path of the benchmark output directory to
-        verify.
+class _RunOutputVerifier(object):
+  """Object to verify benchmark run from traces and WPR log stored in the
+  runner output directory.
   """
-  # TODO(gabadie): What's the best way of propagating errors happening in here?
-  benchmark_setup = json.load(open(benchmark_setup_path))
-  cache_whitelist = set(benchmark_setup['cache_whitelist'])
-  original_requests = set(benchmark_setup['url_resources'])
-  original_cached_requests = original_requests.intersection(cache_whitelist)
-  original_uncached_requests = original_requests.difference(cache_whitelist)
-  all_sent_url_requests = set()
-
-  # Verify requests from traces.
-  run_id = -1
-  while True:
-    run_id += 1
-    run_path = os.path.join(benchmark_output_directory_path, str(run_id))
-    if not os.path.isdir(run_path):
-      break
-    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
-    if not os.path.isfile(trace_path):
-      logging.error('missing trace %s' % trace_path)
-      continue
-    trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
-    logging.info('verifying %s from %s' % (trace.url, trace_path))
 
+  def __init__(self, cache_validation_result, benchmark_setup):
+    """Constructor.
+
+    Args:
+      cache_validation_result: JSON of the cache validation task.
+      benchmark_setup: JSON of the benchmark setup.
+    """
+    self._cache_whitelist = set(benchmark_setup['cache_whitelist'])
+    self._original_requests = set(cache_validation_result['effective_requests'])
+    self._original_post_requests = set(
+        cache_validation_result['effective_post_requests'])
+    self._original_cached_requests = self._original_requests.intersection(
+        self._cache_whitelist)
+    self._original_uncached_requests = self._original_requests.difference(
+        self._cache_whitelist)
+    self._all_sent_url_requests = set()
+
+  def VerifyTrace(self, trace):
+    """Verifies a trace with the cache validation result and the benchmark
+    setup.
+    """
     effective_requests = _ListUrlRequests(trace, _RequestOutcome.All)
     effective_post_requests = _ListUrlRequests(trace, _RequestOutcome.Post)
     effective_cached_requests = \
@@ -299,74 +307,49 @@ def _VerifyBenchmarkOutputDirectory(benchmark_setup_path,
     effective_uncached_requests = \
         _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache)
 
-    missing_requests = original_requests.difference(effective_requests)
-    unexpected_requests = effective_requests.difference(original_requests)
+    missing_requests = self._original_requests.difference(effective_requests)
+    unexpected_requests = effective_requests.difference(self._original_requests)
     expected_cached_requests = \
-        original_cached_requests.difference(missing_requests)
-    missing_cached_requests = \
-        expected_cached_requests.difference(effective_cached_requests)
-    expected_uncached_requests = original_uncached_requests.union(
-        unexpected_requests).union(missing_cached_requests)
-    all_sent_url_requests.update(effective_uncached_requests)
+        self._original_cached_requests.difference(missing_requests)
+    expected_uncached_requests = self._original_uncached_requests.union(
+        unexpected_requests).difference(missing_requests)
 
     # POST requests are known to be unable to use the cache.
     expected_cached_requests.difference_update(effective_post_requests)
     expected_uncached_requests.update(effective_post_requests)
 
-    _PrintUrlSetComparison(original_requests, effective_requests,
+    _PrintUrlSetComparison(self._original_requests, effective_requests,
                            'All resources')
-    _PrintUrlSetComparison(set(), effective_post_requests,
-                           'POST resources')
+    _PrintUrlSetComparison(set(), effective_post_requests, 'POST resources')
     _PrintUrlSetComparison(expected_cached_requests, effective_cached_requests,
                            'Cached resources')
     _PrintUrlSetComparison(expected_uncached_requests,
                            effective_uncached_requests, 'Non cached resources')
 
-  # Verify requests from WPR.
-  wpr_log_path = os.path.join(
-      benchmark_output_directory_path, sandwich_runner.WPR_LOG_FILENAME)
-  logging.info('verifying requests from %s' % wpr_log_path)
-  all_wpr_requests = wpr_backend.ExtractRequestsFromLog(wpr_log_path)
-  all_wpr_urls = set()
-  unserved_wpr_urls = set()
-  wpr_command_colliding_urls = set()
-
-  for request in all_wpr_requests:
-    if request.is_wpr_host:
-      continue
-    if urlparse(request.url).path.startswith('/web-page-replay'):
-      wpr_command_colliding_urls.add(request.url)
-    elif request.is_served is False:
-      unserved_wpr_urls.add(request.url)
-    all_wpr_urls.add(request.url)
-
-  _PrintUrlSetComparison(set(), unserved_wpr_urls,
-                         'Distinct unserved resources from WPR')
-  _PrintUrlSetComparison(set(), wpr_command_colliding_urls,
-                         'Distinct resources colliding to WPR commands')
-  _PrintUrlSetComparison(all_wpr_urls, all_sent_url_requests,
-                         'Distinct resource requests to WPR')
-
+    self._all_sent_url_requests.update(effective_uncached_requests)
 
-def _ReadSubresourceFromRunnerOutputDir(runner_output_dir):
-  """Extracts a list of subresources in runner output directory.
+  def VerifyWprLog(self, wpr_log_path):
+    """Verifies WPR log with previously verified traces."""
+    all_wpr_requests = wpr_backend.ExtractRequestsFromLog(wpr_log_path)
+    all_wpr_urls = set()
+    unserved_wpr_urls = set()
+    wpr_command_colliding_urls = set()
 
-  Args:
-    runner_output_dir: Path of the runner's output directory.
+    for request in all_wpr_requests:
+      if request.is_wpr_host:
+        continue
+      if urlparse(request.url).path.startswith('/web-page-replay'):
+        wpr_command_colliding_urls.add(request.url)
+      elif request.is_served is False:
+        unserved_wpr_urls.add(request.url)
+      all_wpr_urls.add(request.url)
 
-  Returns:
-    [URLs of sub-resources]
-  """
-  trace_path = os.path.join(
-      runner_output_dir, '0', sandwich_runner.TRACE_FILENAME)
-  trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
-  url_set = set()
-  for request_event in _FilterOutDataAndIncompleteRequests(
-      trace.request_track.GetEvents()):
-    url_set.add(request_event.url)
-  logging.info('lists %s resources of %s from %s' % \
-               (len(url_set), trace.url, trace_path))
-  return [url for url in url_set]
+    _PrintUrlSetComparison(set(), unserved_wpr_urls,
+                           'Distinct unserved resources from WPR')
+    _PrintUrlSetComparison(set(), wpr_command_colliding_urls,
+                           'Distinct resources colliding to WPR commands')
+    _PrintUrlSetComparison(all_wpr_urls, self._all_sent_url_requests,
+                           'Distinct resource requests to WPR')
 
 
 def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
@@ -375,6 +358,14 @@ def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
   Args:
     cache_build_trace_path: Path of the generated trace at the cache build time.
     cache_archive_path: Cache archive's path to validate.
+
+  Returns:
+    {
+      'effective_requests': [URLs of all requests],
+      'effective_post_requests': [URLs of POST requests],
+      'expected_cached_resources': [URLs of resources expected to be cached],
+      'successfully_cached': [URLs of cached sub-resources]
+    }
   """
   # TODO(gabadie): What's the best way of propagating errors happening in here?
   logging.info('lists cached urls from %s' % cache_archive_path)
@@ -405,6 +396,69 @@ def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
   _PrintUrlSetComparison(expected_cached_requests, effective_cache_keys,
                          'Cached resources')
 
+  return {
+      'effective_requests': [url for url in effective_requests],
+      'effective_post_requests': [url for url in effective_post_requests],
+      'expected_cached_resources': [url for url in expected_cached_requests],
+      'successfully_cached_resources': [url for url in effective_cache_keys]
+  }
+
+
+def _ProcessRunOutputDir(
+    cache_validation_result, benchmark_setup, runner_output_dir):
+  """Process benchmark's run output directory.
+
+  Args:
+    cache_validation_result: Same as for _RunOutputVerifier
+    benchmark_setup: Same as for _RunOutputVerifier
+    runner_output_dir: Same as for SandwichRunner.output_dir
+
+  Returns:
+    List of dictionary.
+  """
+  run_metrics_list = []
+  run_output_verifier = _RunOutputVerifier(
+      cache_validation_result, benchmark_setup)
+  for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
+      runner_output_dir):
+    trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
+
+    logging.info('loading trace: %s', trace_path)
+    trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
+
+    logging.info('verifying trace: %s', trace_path)
+    run_output_verifier.VerifyTrace(trace)
+
+    logging.info('extracting metrics from trace: %s', trace_path)
+    run_metrics = {
+        'url': trace.url,
+        'repeat_id': repeat_id,
+        'subresource_discoverer': benchmark_setup['subresource_discoverer'],
+        'cache_recording.subresource_count':
+            len(cache_validation_result['effective_requests']),
+        'cache_recording.cached_subresource_count_theoretic':
+            len(cache_validation_result['successfully_cached_resources']),
+        'cache_recording.cached_subresource_count':
+            len(cache_validation_result['expected_cached_resources']),
+        'benchmark.subresource_count': len(_ListUrlRequests(
+            trace, _RequestOutcome.All)),
+        'benchmark.served_from_cache_count_theoretic':
+            len(benchmark_setup['cache_whitelist']),
+        'benchmark.served_from_cache_count': len(_ListUrlRequests(
+            trace, _RequestOutcome.ServedFromCache)),
+    }
+    run_metrics.update(
+        sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
+            repeat_dir, trace))
+    run_metrics_list.append(run_metrics)
+  run_metrics_list.sort(key=lambda e: e['repeat_id'])
+
+  wpr_log_path = os.path.join(
+      runner_output_dir, sandwich_runner.WPR_LOG_FILENAME)
+  logging.info('verifying wpr log: %s', wpr_log_path)
+  run_output_verifier.VerifyWprLog(wpr_log_path)
+  return run_metrics_list
+
 
 class PrefetchBenchmarkBuilder(task_manager.Builder):
   """A builder for a graph of tasks for NoState-Prefetch emulated benchmarks."""
@@ -415,10 +469,10 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
                                   common_builder.output_subdirectory)
     self._common_builder = common_builder
 
-    self._patched_wpr_task = None
-    self._reference_cache_task = None
+    self._wpr_archive_path = None
+    self._cache_path = None
     self._trace_from_grabbing_reference_cache = None
-    self._subresources_for_urls_task = None
+    self._cache_validation_task = None
     self._PopulateCommonPipelines()
 
   def _PopulateCommonPipelines(self):
@@ -428,13 +482,11 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
     subresources (urls-resources.json).
 
     Here is the full dependency tree for the returned task:
-    common/patched-cache-validation.log
+    common/patched-cache-validation.json
       depends on: common/patched-cache.zip
         depends on: common/original-cache.zip
           depends on: common/webpages-patched.wpr
             depends on: common/webpages.wpr
-      depends on: common/urls-resources.json
-        depends on: common/original-cache.zip
     """
     @self.RegisterTask('common/webpages-patched.wpr',
                        dependencies=[self._common_builder.original_wpr_task])
@@ -461,29 +513,18 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
       _PatchCacheArchive(BuildOriginalCache.path,
           original_cache_trace_path, BuildPatchedCache.path)
 
-    @self.RegisterTask('common/subresources-for-urls.json',
-                       [BuildOriginalCache])
-    def ListUrlsResources():
-      url_resources = _ReadSubresourceFromRunnerOutputDir(
-          BuildOriginalCache.run_path)
-      with open(ListUrlsResources.path, 'w') as output:
-        json.dump(url_resources, output)
-
-    @self.RegisterTask('common/patched-cache-validation.log',
+    @self.RegisterTask('common/patched-cache-validation.json',
                        [BuildPatchedCache])
     def ValidatePatchedCache():
-      handler = logging.FileHandler(ValidatePatchedCache.path)
-      logging.getLogger().addHandler(handler)
-      try:
-        _ValidateCacheArchiveContent(
-            original_cache_trace_path, BuildPatchedCache.path)
-      finally:
-        logging.getLogger().removeHandler(handler)
-
-    self._patched_wpr_task = BuildPatchedWpr
+      cache_validation_result = _ValidateCacheArchiveContent(
+          original_cache_trace_path, BuildPatchedCache.path)
+      with open(ValidatePatchedCache.path, 'w') as output:
+        json.dump(cache_validation_result, output)
+
+    self._wpr_archive_path = BuildPatchedWpr.path
     self._trace_from_grabbing_reference_cache = original_cache_trace_path
-    self._reference_cache_task = BuildPatchedCache
-    self._subresources_for_urls_task = ListUrlsResources
+    self._cache_path = BuildPatchedCache.path
+    self._cache_validation_task = ValidatePatchedCache
 
     self._common_builder.default_final_tasks.append(ValidatePatchedCache)
 
@@ -503,21 +544,19 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
     <transformer_list_name>/<subresource_discoverer>-metrics.csv
       depends on: <transformer_list_name>/<subresource_discoverer>-run/
         depends on: common/<subresource_discoverer>-cache.zip
-          depends on: some tasks saved by PopulateCommonPipelines()
           depends on: common/<subresource_discoverer>-setup.json
-            depends on: some tasks saved by PopulateCommonPipelines()
+            depends on: common/patched-cache-validation.json
     """
     additional_column_names = [
         'url',
         'repeat_id',
         'subresource_discoverer',
-        'subresource_count',
-        # The amount of subresources detected at SetupBenchmark step.
-        'subresource_count_theoretic',
-        # Amount of subresources for caching as suggested by the subresource
-        # discoverer.
-        'cached_subresource_count_theoretic',
-        'cached_subresource_count']
+        'cache_recording.subresource_count',
+        'cache_recording.cached_subresource_count_theoretic',
+        'cache_recording.cached_subresource_count',
+        'benchmark.subresource_count',
+        'benchmark.served_from_cache_count_theoretic',
+        'benchmark.served_from_cache_count']
 
     assert subresource_discoverer in SUBRESOURCE_DISCOVERERS
     assert 'common' not in SUBRESOURCE_DISCOVERERS
@@ -525,28 +564,25 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
     task_prefix = os.path.join(transformer_list_name, subresource_discoverer)
 
     @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
-                       dependencies=[self._subresources_for_urls_task])
+                       dependencies=[self._cache_validation_task])
     def SetupBenchmark():
       whitelisted_urls = _ExtractDiscoverableUrls(
           self._trace_from_grabbing_reference_cache, subresource_discoverer)
 
-      url_resources = json.load(open(self._subresources_for_urls_task.path))
       common_util.EnsureParentDirectoryExists(SetupBenchmark.path)
       with open(SetupBenchmark.path, 'w') as output:
         json.dump({
             'cache_whitelist': [url for url in whitelisted_urls],
             'subresource_discoverer': subresource_discoverer,
-            'url_resources': url_resources,
           }, output)
 
     @self.RegisterTask(shared_task_prefix + '-cache.zip', merge=True,
-                       dependencies=[
-                           SetupBenchmark, self._reference_cache_task])
+                       dependencies=[SetupBenchmark])
     def BuildBenchmarkCacheArchive():
-      setup = json.load(open(SetupBenchmark.path))
+      benchmark_setup = json.load(open(SetupBenchmark.path))
       chrome_cache.ApplyUrlWhitelistToCacheArchive(
-          cache_archive_path=self._reference_cache_task.path,
-          whitelisted_urls=setup['cache_whitelist'],
+          cache_archive_path=self._cache_path,
+          whitelisted_urls=benchmark_setup['cache_whitelist'],
           output_cache_archive_path=BuildBenchmarkCacheArchive.path)
 
     @self.RegisterTask(task_prefix + '-run/',
@@ -555,7 +591,7 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
       runner = self._common_builder.CreateSandwichRunner()
       for transformer in transformer_list:
         transformer(runner)
-      runner.wpr_archive_path = self._patched_wpr_task.path
+      runner.wpr_archive_path = self._wpr_archive_path
       runner.wpr_out_log_path = os.path.join(
           RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
       runner.cache_archive_path = BuildBenchmarkCacheArchive.path
@@ -565,42 +601,18 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
 
     @self.RegisterTask(task_prefix + '-metrics.csv',
                        dependencies=[RunBenchmark])
-    def ExtractMetrics():
-      # TODO(gabadie): Performance improvement: load each trace only once and
-      # use it for validation and extraction of metrics later.
-      _VerifyBenchmarkOutputDirectory(SetupBenchmark.path, RunBenchmark.path)
-
+    def ProcessRunOutputDir():
       benchmark_setup = json.load(open(SetupBenchmark.path))
-      run_metrics_list = []
-      for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
-          RunBenchmark.path):
-        trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
-        logging.info('processing trace: %s', trace_path)
-        trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
-        run_metrics = {
-            'url': trace.url,
-            'repeat_id': repeat_id,
-            'subresource_discoverer': benchmark_setup['subresource_discoverer'],
-            'subresource_count': len(_ListUrlRequests(
-                trace, _RequestOutcome.All)),
-            'subresource_count_theoretic':
-                len(benchmark_setup['url_resources']),
-            'cached_subresource_count': len(_ListUrlRequests(
-                trace, _RequestOutcome.ServedFromCache)),
-            'cached_subresource_count_theoretic':
-                len(benchmark_setup['cache_whitelist']),
-        }
-        run_metrics.update(
-            sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
-                repeat_dir, trace))
-        run_metrics_list.append(run_metrics)
-
-      run_metrics_list.sort(key=lambda e: e['repeat_id'])
-      with open(ExtractMetrics.path, 'w') as csv_file:
+      cache_validation_result = json.load(
+          open(self._cache_validation_task.path))
+
+      run_metrics_list = _ProcessRunOutputDir(
+          cache_validation_result, benchmark_setup, RunBenchmark.path)
+      with open(ProcessRunOutputDir.path, 'w') as csv_file:
         writer = csv.DictWriter(csv_file, fieldnames=(additional_column_names +
                                     sandwich_metrics.COMMON_CSV_COLUMN_NAMES))
         writer.writeheader()
         for trace_metrics in run_metrics_list:
           writer.writerow(trace_metrics)
 
-    self._common_builder.default_final_tasks.append(ExtractMetrics)
+    self._common_builder.default_final_tasks.append(ProcessRunOutputDir)

commit d1031e4de8acd8c4c8a2213fe630bc0eafe58f85
Author: blundell <blundell@chromium.org>
Date:   Mon Jun 6 05:33:10 2016 -0700

    tools/android/loading: Output cost of requests in report.py
    
    This CL adds the tracking of the following per-user-lens metrics in report.py:
    - Cumulative cost of requests
    - Cumulative cost of preloaded requests
    
    This provides extra insight over just having the number of such requests.
    
    Review-Url: https://codereview.chromium.org/2039853003
    Cr-Original-Commit-Position: refs/heads/master@{#398011}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3acc7c023b0b1327f5c8a0c27d8be304c5cf81e0

diff --git a/loading/cloud/frontend/bigquery_schema.json b/loading/cloud/frontend/bigquery_schema.json
index c8ce39b..8887c51 100644
--- a/loading/cloud/frontend/bigquery_schema.json
+++ b/loading/cloud/frontend/bigquery_schema.json
@@ -61,10 +61,18 @@
         "type": "INTEGER"
     },
     {
+        "name": "contentful_preloaded_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "contentful_requests",
         "type": "INTEGER"
     },
     {
+        "name": "contentful_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "contentful_script_frac",
         "type": "FLOAT"
     },
@@ -109,10 +117,18 @@
         "type": "INTEGER"
     },
     {
+        "name": "first_text_preloaded_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "first_text_requests",
         "type": "INTEGER"
     },
     {
+        "name": "first_text_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "first_text_script_frac",
         "type": "FLOAT"
     },
@@ -149,10 +165,18 @@
         "type": "INTEGER"
     },
     {
+        "name": "plt_preloaded_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "plt_requests",
         "type": "INTEGER"
     },
     {
+        "name": "plt_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "plt_script_frac",
         "type": "FLOAT"
     },
@@ -181,10 +205,18 @@
         "type": "INTEGER"
     },
     {
+        "name": "significant_preloaded_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "significant_requests",
         "type": "INTEGER"
     },
     {
+        "name": "significant_requests_cost",
+        "type": "FLOAT"
+    },
+    {
         "name": "significant_script_frac",
         "type": "FLOAT"
     },
diff --git a/loading/report.py b/loading/report.py
index fe752b4..36c7f39 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -52,9 +52,9 @@ class PerUserLensReport(object):
 
     self._byte_frac = self._GenerateByteFrac(network_lens)
 
-    self._requests = len(user_lens.CriticalRequests())
-    self._preloaded_requests = len(set(preloaded_requests) &
-                                   set(user_lens.CriticalRequests()))
+    self._requests = user_lens.CriticalRequests()
+    self._preloaded_requests = (
+        [r for r in preloaded_requests if r in self._requests])
 
     self._cpu_busyness = _ComputeCpuBusyness(activity_lens,
                                              navigation_start_msec,
@@ -65,8 +65,13 @@ class PerUserLensReport(object):
 
     report['ms'] = self._satisfied_msec - self._navigation_start_msec
     report['byte_frac'] = self._byte_frac
-    report['requests'] = self._requests
-    report['preloaded_requests'] = self._preloaded_requests
+
+    report['requests'] = len(self._requests)
+    report['preloaded_requests'] = len(self._preloaded_requests)
+    report['requests_cost'] = reduce(lambda x,y: x + y.Cost(),
+                                     self._requests, 0)
+    report['preloaded_requests_cost'] = reduce(lambda x,y: x + y.Cost(),
+                                        self._preloaded_requests, 0)
 
     # Take the first (earliest) inversion.
     report['inversion'] = self._inversions[0].url if self._inversions else None
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 16b299a..a9f6591 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -112,6 +112,14 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertEqual(1, loading_report['first_text_preloaded_requests'])
     self.assertEqual(1, loading_report['contentful_preloaded_requests'])
     self.assertEqual(1, loading_report['significant_preloaded_requests'])
+    self.assertEqual(401, loading_report['plt_requests_cost'])
+    self.assertEqual(1, loading_report['first_text_requests_cost'])
+    self.assertEqual(1, loading_report['contentful_requests_cost'])
+    self.assertEqual(1, loading_report['significant_requests_cost'])
+    self.assertEqual(1, loading_report['plt_preloaded_requests_cost'])
+    self.assertEqual(1, loading_report['first_text_preloaded_requests_cost'])
+    self.assertEqual(1, loading_report['contentful_preloaded_requests_cost'])
+    self.assertEqual(1, loading_report['significant_preloaded_requests_cost'])
     self.assertIsNone(loading_report['contentful_inversion'])
     self.assertIsNone(loading_report['significant_inversion'])
     self.assertIsNone(loading_report['ad_requests'])

commit d699d992a10d61f3594c7ca9317404a04bf2212d
Author: blundell <blundell@chromium.org>
Date:   Mon Jun 6 02:31:10 2016 -0700

    tools/android/loading: Create UserLens for PLT
    
    This CL creates a UserSatisfiedLens for page load time (PLT) and
    refactors report.py to use this new lens. The motivation is to avoid
    having to duplicate code tracking metrics for PLT and other user
    satisfied events.
    
    This change is not backwards compatible as it changes the names of some
    of the keys in report generation.
    
    Review-Url: https://codereview.chromium.org/2032013002
    Cr-Original-Commit-Position: refs/heads/master@{#397997}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 07779818aa5aa770730c78feea6426c3f29442c2

diff --git a/loading/cloud/frontend/bigquery_schema.json b/loading/cloud/frontend/bigquery_schema.json
index 82b16df..c8ce39b 100644
--- a/loading/cloud/frontend/bigquery_schema.json
+++ b/loading/cloud/frontend/bigquery_schema.json
@@ -5,18 +5,6 @@
         "mode": "REQUIRED"
     },
     {
-        "name": "activity_frac",
-        "type": "FLOAT"
-    },
-    {
-        "name": "parsing_frac",
-        "type": "FLOAT"
-    },
-    {
-        "name": "script_frac",
-        "type": "FLOAT"
-    },
-    {
         "name": "ad_or_tracking_initiated_requests",
         "type": "INTEGER"
     },
@@ -137,18 +125,38 @@
         "type": "INTEGER"
     },
     {
+        "name": "plt_activity_frac",
+        "type": "FLOAT"
+    },
+    {
+        "name": "plt_byte_frac",
+        "type": "FLOAT"
+    },
+    {
+        "name": "plt_inversion",
+        "type": "STRING"
+    },
+    {
         "name": "plt_ms",
         "type": "FLOAT"
     },
     {
-        "name": "preloaded_requests",
+        "name": "plt_parsing_frac",
+        "type": "FLOAT"
+    },
+    {
+        "name": "plt_preloaded_requests",
         "type": "INTEGER"
     },
     {
-        "name": "requests",
+        "name": "plt_requests",
         "type": "INTEGER"
     },
     {
+        "name": "plt_script_frac",
+        "type": "FLOAT"
+    },
+    {
         "name": "significant_activity_frac",
         "type": "FLOAT"
     },
diff --git a/loading/report.py b/loading/report.py
index 3542e99..fe752b4 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -16,7 +16,8 @@ from network_activity_lens import NetworkActivityLens
 import prefetch_view
 import request_dependencies_lens
 from user_satisfied_lens import (
-    FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
+    FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens,
+    PLTLens)
 
 
 def _ComputeCpuBusyness(activity, load_start, satisfied_end):
@@ -97,7 +98,6 @@ class LoadingReport(object):
         'blink.user_timing', 'navigationStart')
     self._navigation_start_msec = min(
         e.start_msec for e in navigation_start_events)
-    self._load_end_msec = self._ComputePlt(trace)
 
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
@@ -105,18 +105,18 @@ class LoadingReport(object):
     preloaded_requests = \
        prefetch_view.PrefetchSimulationView.PreloadedRequests(
            requests[0], dependencies_lens, self.trace)
-    self._requests = len(requests)
-    self._preloaded_requests = len(preloaded_requests)
     self._dns_requests, self._dns_cost_msec = metrics.DnsRequestsAndCost(trace)
     self._connection_stats = metrics.ConnectionMetrics(trace)
 
     self._user_lens_reports = {}
+    plt_lens = PLTLens(self.trace)
     first_text_paint_lens = FirstTextPaintLens(self.trace)
     first_contentful_paint_lens = FirstContentfulPaintLens(self.trace)
     first_significant_paint_lens = FirstSignificantPaintLens(self.trace)
     activity = ActivityLens(trace)
     network_lens = NetworkActivityLens(self.trace)
-    for key, user_lens in [['first_text', first_text_paint_lens],
+    for key, user_lens in [['plt', plt_lens],
+                           ['first_text', first_text_paint_lens],
                            ['contentful', first_contentful_paint_lens],
                            ['significant', first_significant_paint_lens]]:
       self._user_lens_reports[key] = PerUserLensReport(self.trace,
@@ -125,10 +125,6 @@ class LoadingReport(object):
 
     self._transfer_size = metrics.TotalTransferSize(trace)[1]
 
-    self._cpu_busyness = _ComputeCpuBusyness(activity,
-                                             self._navigation_start_msec,
-                                             self._load_end_msec)
-
     content_lens = ContentClassificationLens(
         trace, ad_rules or [], tracking_rules or [])
     has_ad_rules = bool(ad_rules)
@@ -136,7 +132,8 @@ class LoadingReport(object):
     self._ad_report = self._AdRequestsReport(
         trace, content_lens, has_ad_rules, has_tracking_rules)
     self._ads_cost = self._AdsAndTrackingCpuCost(
-        self._navigation_start_msec, self._load_end_msec, content_lens,
+        self._navigation_start_msec,
+        self._user_lens_reports['plt'].GenerateReport()['ms'], content_lens,
         activity, has_tracking_rules or has_ad_rules)
 
   def GenerateReport(self):
@@ -146,9 +143,6 @@ class LoadingReport(object):
     # details.
     report = {
         'url': self.trace.url,
-        'plt_ms': self._load_end_msec - self._navigation_start_msec,
-        'requests': self._requests,
-        'preloaded_requests': self._preloaded_requests,
         'transfer_size': self._transfer_size,
         'dns_requests': self._dns_requests,
         'dns_cost_ms': self._dns_cost_msec}
@@ -157,7 +151,6 @@ class LoadingReport(object):
       for key, value in user_lens_report.GenerateReport().iteritems():
         report[user_lens_type + '_' + key] = value
 
-    report.update(self._cpu_busyness)
     report.update(self._ad_report)
     report.update(self._ads_cost)
     report.update(self._connection_stats)
@@ -198,19 +191,6 @@ class LoadingReport(object):
     return result
 
   @classmethod
-  def _ComputePlt(cls, trace):
-    mark_load_events = trace.tracing_track.GetMatchingEvents(
-        'devtools.timeline', 'MarkLoad')
-    # Some traces contain several load events for the main frame.
-    main_frame_load_events = filter(
-        lambda e: e.args['data']['isMainFrame'], mark_load_events)
-    if main_frame_load_events:
-      return max(e.start_msec for e in main_frame_load_events)
-    # Main frame onLoad() didn't finish. Take the end of the last completed
-    # request.
-    return max(r.end_msec or -1 for r in trace.request_track.GetEvents())
-
-  @classmethod
   def _AdsAndTrackingCpuCost(
       cls, start_msec, end_msec, content_lens, activity, has_rules):
     """Returns the CPU cost associated with Ads and tracking between timestamps.
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 5be6501..16b299a 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -104,11 +104,11 @@ class LoadingReportTestCase(unittest.TestCase):
                            loading_report['plt_ms'])
     self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'], 2)
     self.assertAlmostEqual(0.1844, loading_report['significant_byte_frac'], 2)
-    self.assertEqual(2, loading_report['requests'])
+    self.assertEqual(2, loading_report['plt_requests'])
     self.assertEqual(1, loading_report['first_text_requests'])
     self.assertEqual(1, loading_report['contentful_requests'])
     self.assertEqual(1, loading_report['significant_requests'])
-    self.assertEqual(1, loading_report['preloaded_requests'])
+    self.assertEqual(1, loading_report['plt_preloaded_requests'])
     self.assertEqual(1, loading_report['first_text_preloaded_requests'])
     self.assertEqual(1, loading_report['contentful_preloaded_requests'])
     self.assertEqual(1, loading_report['significant_preloaded_requests'])
@@ -174,7 +174,7 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertAlmostEqual(
         float(self._TOPLEVEL_EVENT_DURATION - self._TOPLEVEL_EVENT_OFFSET)
         / (self._LOAD_END_TIME - self._NAVIGATION_START_TIME),
-        loading_report['activity_frac'])
+        loading_report['plt_activity_frac'])
 
   def testActivityBreakdown(self):
     loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
@@ -183,11 +183,11 @@ class LoadingReportTestCase(unittest.TestCase):
         self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME)
 
     self.assertAlmostEqual(self._SCRIPT_EVENT_DURATION / load_time,
-                           loading_report['script_frac'])
+                           loading_report['plt_script_frac'])
     self.assertAlmostEqual(
         (self._PARSING_EVENT_DURATION - self._SCRIPT_EVENT_DURATION)
         / load_time,
-        loading_report['parsing_frac'])
+        loading_report['plt_parsing_frac'])
 
     self.assertAlmostEqual(1., loading_report['significant_script_frac'])
     self.assertAlmostEqual(0., loading_report['significant_parsing_frac'])
@@ -208,7 +208,7 @@ class LoadingReportTestCase(unittest.TestCase):
         'args': {'data': {'scriptName': self.ad_url}}})
     loading_report = report.LoadingReport(
         self._MakeTrace(), [self.ad_domain]).GenerateReport()
-    self.assertAlmostEqual(.5, loading_report['ad_or_tracking_script_frac'])
+    self.assertAlmostEqual(.5, loading_report['ad_or_tracking_script_frac'], 2)
     self.assertAlmostEqual(0., loading_report['ad_or_tracking_parsing_frac'])
 
 
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 14fdfc3..1e4a485 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -64,6 +64,36 @@ class _UserSatisfiedLens(object):
     """
     return self._satisfied_msec
 
+  @classmethod
+  def RequestsBefore(cls, request_track, time_ms):
+    return [rq for rq in request_track.GetEvents()
+            if rq.end_msec <= time_ms]
+
+
+class PLTLens(_UserSatisfiedLens):
+  """A lens built using page load time (PLT) as the metric of user satisfaction.
+  """
+  def __init__(self, trace):
+    self._satisfied_msec = PLTLens._ComputePlt(trace)
+    self._critical_requests = _UserSatisfiedLens.RequestsBefore(
+        trace.request_track, self._satisfied_msec)
+
+  def CriticalRequests(self):
+    return self._critical_requests
+
+  @classmethod
+  def _ComputePlt(cls, trace):
+    mark_load_events = trace.tracing_track.GetMatchingEvents(
+        'devtools.timeline', 'MarkLoad')
+    # Some traces contain several load events for the main frame.
+    main_frame_load_events = filter(
+        lambda e: e.args['data']['isMainFrame'], mark_load_events)
+    if main_frame_load_events:
+      return max(e.start_msec for e in main_frame_load_events)
+    # Main frame onLoad() didn't finish. Take the end of the last completed
+    # request.
+    return max(r.end_msec or -1 for r in trace.request_track.GetEvents())
+
 
 class RequestFingerprintLens(_UserSatisfiedLens):
   """A lens built using requests in a trace that match a set of fingerprints."""
@@ -95,7 +125,7 @@ class _FirstEventLens(_UserSatisfiedLens):
     if trace is None:
       return
     self._CalculateTimes(trace)
-    self._critical_requests = self._RequestsBefore(
+    self._critical_requests = _UserSatisfiedLens.RequestsBefore(
         trace.request_track, self._satisfied_msec)
     self._critical_request_ids = set(rq.request_id
                                      for rq in self._critical_requests)
@@ -127,11 +157,6 @@ class _FirstEventLens(_UserSatisfiedLens):
     raise NotImplementedError
 
   @classmethod
-  def _RequestsBefore(cls, request_track, time_ms):
-    return [rq for rq in request_track.GetEvents()
-            if rq.end_msec <= time_ms]
-
-  @classmethod
   def _CheckCategory(cls, tracing_track, category):
     assert category in tracing_track.Categories(), (
         'The "%s" category must be enabled.' % category)
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 225548d..a24d1e3 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -43,6 +43,29 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def setUp(self):
     super(UserSatisfiedLensTestCase, self).setUp()
 
+  def testPLTLens(self):
+    MAINFRAME = 1
+    trace_creator = test_utils.TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'devtools.timeline', 'pid': 1, 'tid': 1,
+          'name': 'MarkLoad',
+          'args': {'data': {'isMainFrame': True}}},
+         {'ts': 10 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'devtools.timeline', 'pid': 1, 'tid': 1,
+          'name': 'MarkLoad',
+          'args': {'data': {'isMainFrame': True}}},
+         {'ts': 20 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'devtools.timeline', 'pid': 1, 'tid': 1,
+          'name': 'MarkLoad',
+          'args': {'data': {'isMainFrame': False}}}], MAINFRAME)
+    lens = user_satisfied_lens.PLTLens(loading_trace)
+    self.assertEqual(set(['0.1']), lens.CriticalRequestIds())
+    self.assertEqual(10, lens.SatisfiedMs())
+
   def testFirstContentfulPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2

commit 57408e5a9b48df0b538c74c8cb0956692fd2e3bb
Author: droger <droger@chromium.org>
Date:   Fri Jun 3 05:13:39 2016 -0700

    tools/android/loading Compute instance count and timeout automatically
    
    This CL introduces automatic computation of the instance
    count and the timeout.
    It tries to allocate enough instances to finish the job in
    30 minutes.
    If there are not enough instances available (because of quota
    limits), then it uses all the available instances.
    
    An estimation of the duration of the task is also now
    displayed.
    It should be almost always 30 minutes.
    It can be more if there are not enough available instances,
    and less if the task is very small.
    
    Review-Url: https://codereview.chromium.org/2030033002
    Cr-Original-Commit-Position: refs/heads/master@{#397688}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c839eb29548ea00cdff184bae985d0649424a693

diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 8ff353b..e4199b5 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -19,6 +19,7 @@ class GoogleInstanceHelper(object):
     self._compute_api = discovery.build('compute','v1', credentials=credentials)
     self._project = project
     self._project_api_url = self._COMPUTE_API_ROOT + project
+    self._region = 'europe-west1'
     self._zone = 'europe-west1-c'
     self._logger = logger
 
@@ -188,3 +189,22 @@ class GoogleInstanceHelper(object):
       return -1
     return len(response.get('managedInstances', []))
 
+
+  def GetAvailableInstanceCount(self):
+    """Returns the number of instances that can be created, according to the
+    ComputeEngine quotas, or -1 on failure.
+    """
+    request = self._compute_api.regions().get(project=self._project,
+                                              region=self._region)
+    (success, response) = self._ExecuteApiRequest(request)
+    if not success:
+      self._logger.error('Could not get ComputeEngine region information.')
+      return -1
+    metric_name = 'IN_USE_ADDRESSES'
+    for quota in response.get('quotas', []):
+      if quota['metric'] == metric_name:
+        return quota['limit'] - quota['usage']
+    self._logger.error(
+        metric_name + ' quota not found in ComputeEngine response.')
+    return -1
+
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index e6fd69c..775e500 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -16,11 +16,13 @@ following keys:
 
 ### Parameters for `backend_params`
 
--   `instance_count` (int): Number of Compute Engine instances that will be
-    started for this task.
 -   `storage_bucket` (string): Name of the storage bucket used by the backend
     instances. Backend code and data must have been previously deployed to this
     bucket using the `deploy.sh` [script][4].
+-   `instance_count` (int, optional): Number of Compute Engine instances that
+    will be started for this task. If not specified, the number of instances is
+    determined automatically depending on the size of the task and the number
+    of available instances.
 -   `task_name` (string, opitonal): Name of the task, used to build the name of
     the output directory.
 -   `tag` (string, optional): tag internally used to associate tasks to backend
@@ -29,7 +31,7 @@ following keys:
     specified, a unique tag will be generated.
 -   `timeout_hours` (int, optional): if workers are still alive after this
     delay, they will be forcibly killed, to avoid wasting Compute Engine
-    resources. Defaults to `5`.
+    resources. If not specified, the timeout is determined automatically.
 
 ### Parameters for the `trace` action
 
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 9f00252..ecb19ee 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -99,6 +99,28 @@ def Finalize(tag, email_address, status, task_url):
   deferred.defer(DeleteInstanceGroup, tag)
 
 
+def GetEstimatedTaskDurationInSeconds(task):
+  """Returns an estimation of the time required to run the task.
+
+  Args:
+    task: (ClovisTask) The task.
+
+  Returns:
+    float: Time estimation in seconds, or -1 in case of failure.
+  """
+  action_params = task.ActionParams()
+  if task.Action() == 'trace':
+    estimated_trace_time_s = 40.0
+    return (len(action_params['urls']) * action_params.get('repeat_count', 1) *
+            estimated_trace_time_s)
+  elif task.Action() == 'report':
+    estimated_report_time_s = 20.0
+    return len(action_params['traces']) * estimated_report_time_s
+  else:
+    clovis_logger.error('Unexpected action.')
+    return -1
+
+
 def CreateInstanceTemplate(task, task_dir):
   """Create the Compute Engine instance template that will be used to create the
   instances.
@@ -326,14 +348,50 @@ def StartFromJsonString(http_body_str):
   if not sub_tasks:
     return Render('Task split failed.', memory_logs)
 
+  # Compute estimates for the work duration, in order to compute the instance
+  # count and the timeout.
+  sequential_duration_s = \
+      GetEstimatedTaskDurationInSeconds(sub_tasks[0]) * len(sub_tasks)
+  if sequential_duration_s <= 0:
+    return Render('Time estimation failed.', memory_logs)
+
+  # Compute the number of required instances if not specified.
+  if not task.BackendParams().get('instance_count'):
+    target_parallel_duration_s = 1800.0 # 30 minutes.
+    task.BackendParams()['instance_count'] = int(
+        sequential_duration_s / target_parallel_duration_s + 0.5)  # Rounded up.
+
+  # Check the instance quotas.
+  clovis_logger.info(
+      'Requesting %i instances.' % task.BackendParams()['instance_count'])
+  max_instances = instance_helper.GetAvailableInstanceCount()
+  if max_instances == -1:
+    return Render('Failed to count the available instances.', memory_logs)
+  elif task.BackendParams()['instance_count'] == 0:
+    return Render('Cannot create instances, quota exceeded.', memory_logs)
+  elif max_instances < task.BackendParams()['instance_count']:
+    clovis_logger.warning(
+        'Instance count limited by quota: %i available / %i requested.' % (
+            max_instances, task.BackendParams()['instance_count']))
+    task.BackendParams()['instance_count'] = max_instances
+
+  # Compute the timeout if there is none specified.
+  expected_duration_h = sequential_duration_s / (
+      task.BackendParams()['instance_count'] * 3600.0)
+  if not task.BackendParams().get('timeout_hours'):
+    # Timeout is at least 1 hour.
+    task.BackendParams()['timeout_hours'] = max(1, 5 * expected_duration_h)
+  clovis_logger.info(
+      'Timeout delay: %i hours. ' % task.BackendParams()['timeout_hours'])
+
   if not EnqueueTasks(sub_tasks, task_tag):
     return Render('Task creation failed.', memory_logs)
 
   # Start polling the progress.
   clovis_logger.info('Creating worker polling task.')
   first_poll_delay_minutes = 10
-  timeout_hours = task.BackendParams().get('timeout_hours', 5)
-  deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours, user_email,
+  deferred.defer(PollWorkers, task_tag, time.time(),
+                 task.BackendParams()['timeout_hours'], user_email,
                  task_url, _countdown=(60 * first_poll_delay_minutes))
 
   # Start the instances if required.
@@ -344,7 +402,9 @@ def StartFromJsonString(http_body_str):
 
   return Render(flask.Markup(
       'Success!<br>Your task %s has started.<br>'
-      'You will be notified at %s when completed.') % (task_tag, user_email),
+      'Expected duration: %.1f hours.<br>'
+      'You will be notified at %s when completed.') % (
+          task_tag, expected_duration_h, user_email),
       memory_logs)
 
 

commit 07fe7e13a9b0c140ca1ff13f38a26bf39d1071b3
Author: blundell <blundell@chromium.org>
Date:   Fri Jun 3 03:25:13 2016 -0700

    tools/android/loading: Allow disabling tracing categories
    
    This CL adds the ability to disable categories that are enabled by default.
    There is no reason not to support this now that a trace contains the
    information of the categories that were enabled while producing the trace.
    
    Review-Url: https://codereview.chromium.org/2037993002
    Cr-Original-Commit-Position: refs/heads/master@{#397675}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 60a612310102c1c7f2bb7c448441417ab558c544

diff --git a/loading/tracing.py b/loading/tracing.py
index ca9ebcb..faa03ec 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -12,12 +12,14 @@ import operator
 import devtools_monitor
 
 
-_DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
 QUEUING_CATEGORY = 'disabled-by-default-loading.resource'
-INITIAL_CATEGORIES = (
+_ENABLED_CATEGORIES = (
     ('toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
      'blink.user_timing', 'blink.net', 'disabled-by-default-blink.debug.layout')
-    + (QUEUING_CATEGORY,)
+    + (QUEUING_CATEGORY,))
+_DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
+INITIAL_CATEGORIES = (
+    _ENABLED_CATEGORIES
     + tuple('-' + cat for cat in _DISABLED_CATEGORIES))
 
 
@@ -27,6 +29,7 @@ class TracingTrack(devtools_monitor.Track):
   See https://goo.gl/Qabkqk for details on the protocol.
   """
   def __init__(self, connection, additional_categories=None,
+               disabled_categories=None,
                fetch_stream=False):
     """Initialize this TracingTrack.
 
@@ -35,7 +38,9 @@ class TracingTrack(devtools_monitor.Track):
       additional_categories: ([str] or None) If set, a list of additional
                              categories to add. This cannot be used to re-enable
                              a category which is disabled by default (see
-                             INITIAL_CATEGORIES), nor to disable a category.
+                             _DISABLED_CATEGORIES), nor to disable a category.
+      disabled_categories: If set, a set of categories from _ENABLED_CATEGORIES
+                           to disable.
       fetch_stream: if true, use a websocket stream to fetch tracing data rather
         than dataCollected events. It appears based on very limited testing that
         a stream is slower than the default reporting as dataCollected events.
@@ -43,13 +48,29 @@ class TracingTrack(devtools_monitor.Track):
     super(TracingTrack, self).__init__(connection)
     if connection:
       connection.RegisterListener('Tracing.dataCollected', self)
-    extra_categories = additional_categories or []
-    assert not (set(extra_categories) & set(_DISABLED_CATEGORIES)), (
-        'Cannot enable a disabled category')
-    assert not any(cat.startswith('-') for cat in extra_categories), (
-        'Cannot disable a category')
+
+    categories_to_enable = _ENABLED_CATEGORIES
+    categories_to_disable = _DISABLED_CATEGORIES
+
+    if disabled_categories:
+      assert not any(cat.startswith('-') for cat in disabled_categories), (
+          'Specify categories to disable without an initial -')
+      assert set(disabled_categories).issubset(set(_ENABLED_CATEGORIES)), (
+          'Can only disable categories that are enabled by default')
+      categories_to_enable = (
+          set(categories_to_enable).difference(set(disabled_categories)))
+      categories_to_disable += disabled_categories
+
+    if additional_categories:
+      assert not any(cat.startswith('-') for cat in additional_categories), (
+          'Use disabled_categories to disable a category')
+      assert not (set(additional_categories) & set(_DISABLED_CATEGORIES)), (
+          'Cannot enable a disabled category')
+      categories_to_enable += tuple(additional_categories)
+
     self._categories = set(
-        itertools.chain(INITIAL_CATEGORIES, extra_categories))
+        itertools.chain(categories_to_enable,
+                        tuple('-' + cat for cat in categories_to_disable)))
     params = {}
     params['categories'] = ','.join(self._categories)
     if fetch_stream:
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 95acef8..3ad6617 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -366,10 +366,20 @@ class TracingTrackTestCase(unittest.TestCase):
     # Cannot re-enable a category.
     with self.assertRaises(AssertionError):
       TracingTrack(None, additional_categories=('cc',))
-    # Cannot disable categories.
+    # Cannot disable categories via |additional_categories|.
     with self.assertRaises(AssertionError):
       TracingTrack(None, additional_categories=('-best-category-ever',))
 
+  def testDisabledCategories(self):
+    track = TracingTrack(None, disabled_categories=('toplevel',))
+    self.assertNotIn('toplevel', track.Categories())
+    self.assertIn('-toplevel', track.Categories())
+    # Can only disable categories that are enabled by default.
+    with self.assertRaises(AssertionError):
+      TracingTrack(None, disabled_categories=('best-category-ever',))
+    with self.assertRaises(AssertionError):
+      TracingTrack(None, disabled_categories=('cc',))
+
   def _HandleEvents(self, events):
     self.track.Handle('Tracing.dataCollected', {'params': {'value': [
         self.EventToMicroseconds(e) for e in events]}})

commit dd72808381a0ac620a36e97c5eef3dd28a98ad1c
Author: gabadie <gabadie@chromium.org>
Date:   Thu Jun 2 08:43:32 2016 -0700

    tools/android/loading: Implement task_manager's --keep-going flag
    
    Despite our best effort, we may still have task failures in Sandwich.
    This CL implement the --keep-going command line argument on the task
    manager as Make, to continue executing tasks that are not depending
    of the tasks that have failed.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2027163002
    Cr-Original-Commit-Position: refs/heads/master@{#397421}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 55139a149761859d267f3410dda24f4faa9ec7ba

diff --git a/loading/task_manager.py b/loading/task_manager.py
index c6f5661..90c9045 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -60,6 +60,7 @@ Example:
 
 
 import argparse
+import collections
 import logging
 import os
 import re
@@ -72,6 +73,7 @@ import common_util
 _TASK_GRAPH_DOTFILE_NAME = 'tasks_graph.dot'
 _TASK_GRAPH_PNG_NAME = 'tasks_graph.png'
 _TASK_RESUME_ARGUMENTS_FILE = 'resume.txt'
+_TASK_EXECUTION_LOG_NAME = 'task_execution.log'
 
 FROMFILE_PREFIX_CHARS = '@'
 
@@ -230,20 +232,38 @@ def GenerateScenario(final_tasks, frozen_tasks):
   return scenario
 
 
-def ListResumingTasksToFreeze(scenario, final_tasks, failed_task):
+def GenerateDependentSetPerTask(scenario):
+  """Maps direct dependents per tasks of scenario.
+
+  Args:
+    scenario: The scenario containing the Tasks to map.
+
+  Returns:
+    {Task: set(Task)}
+  """
+  task_set = set(scenario)
+  task_children = collections.defaultdict(set)
+  for task in scenario:
+    for parent in task._dependencies:
+      if parent in task_set:
+        task_children[parent].add(task)
+  return task_children
+
+
+def ListResumingTasksToFreeze(scenario, final_tasks, skipped_tasks):
   """Lists the tasks that one needs to freeze to be able to resume the scenario
   after failure.
 
   Args:
     scenario: The scenario (list of Task) to be resumed.
     final_tasks: The list of final Task used to generate the scenario.
-    failed_task: A Task that have failed in the scenario.
+    skipped_tasks: Set of Tasks in the scenario that were skipped.
 
   Returns:
     set(Task)
   """
-  task_to_id = {t: i for i, t in enumerate(scenario)}
-  assert failed_task in task_to_id
+  scenario_tasks = set(scenario)
+  assert skipped_tasks.issubset(scenario_tasks)
   frozen_tasks = set()
   walked_tasks = set()
 
@@ -251,9 +271,7 @@ def ListResumingTasksToFreeze(scenario, final_tasks, failed_task):
     if task.IsStatic() or task in walked_tasks:
       return
     walked_tasks.add(task)
-    if task not in task_to_id:
-      frozen_tasks.add(task)
-    elif task_to_id[task] < task_to_id[failed_task]:
+    if task not in scenario_tasks or task not in skipped_tasks:
       frozen_tasks.add(task)
     else:
       for dependency in task._dependencies:
@@ -328,6 +346,8 @@ def CommandLineParser():
   parser.add_argument('-f', '--to-freeze', metavar='REGEX', type=str,
                       action='append', dest='frozen_regexes', default=[],
                       help='Regex selecting tasks to not execute.')
+  parser.add_argument('-k', '--keep-going', action='store_true', default=False,
+                      help='Keep going when some targets can\'t be made.')
   parser.add_argument('-o', '--output', type=str, required=True,
                       help='Path of the output directory.')
   parser.add_argument('-v', '--output-graphviz', action='store_true',
@@ -336,17 +356,7 @@ def CommandLineParser():
   return parser
 
 
-def ExecuteWithCommandLine(args, default_final_tasks):
-  """Helper to execute tasks using command line arguments.
-
-  Args:
-    args: Command line argument parsed with CommandLineParser().
-    default_final_tasks: Default final tasks if there is no -r command
-      line arguments.
-
-  Returns:
-    0 if success or 1 otherwise
-  """
+def _SelectTasksFromCommandLineRegexes(args, default_final_tasks):
   frozen_regexes = [common_util.VerboseCompileRegexOrAbort(e)
                       for e in args.frozen_regexes]
   run_regexes = [common_util.VerboseCompileRegexOrAbort(e)
@@ -373,10 +383,64 @@ def ExecuteWithCommandLine(args, default_final_tasks):
         if regex.search(task.name):
           frozen_tasks.add(task)
           break
+  return final_tasks, frozen_tasks
+
+
+class _ResumingFileBuilder(object):
+  def __init__(self, args):
+    resume_path = os.path.join(args.output, _TASK_RESUME_ARGUMENTS_FILE)
+    self._resume_output = open(resume_path, 'w')
+    # List initial freezing regexes not to loose track of final targets to
+    # freeze in case of severals resume attempts caused by sudden death.
+    for regex in args.frozen_regexes:
+      self._resume_output.write('-f\n{}\n'.format(regex))
+
+  def __enter__(self):
+    return self
+
+  def __exit__(self, exc_type, exc_value, exc_traceback):
+    del exc_type, exc_value, exc_traceback # unused
+    self._resume_output.close()
+
+  def OnTaskSuccess(self, task):
+    # Log the succeed tasks so that they are ensured to be frozen in case
+    # of a sudden death.
+    self._resume_output.write('-f\n^{}$\n'.format(re.escape(task.name)))
+    self._resume_output.flush()
+
+  def OnScenarioFinish(
+      self, scenario, final_tasks, failed_tasks, skipped_tasks):
+    resume_additonal_arguments = []
+    for task in ListResumingTasksToFreeze(
+        scenario, final_tasks, skipped_tasks):
+      resume_additonal_arguments.extend(
+          ['-f', '^{}$'.format(re.escape(task.name))])
+    self._resume_output.seek(0)
+    self._resume_output.truncate()
+    self._resume_output.write('\n'.join(resume_additonal_arguments))
+    print '# Looks like something went wrong in tasks:'
+    for failed_task in failed_tasks:
+      print '#   {}'.format(failed_task.name)
+    print '#'
+    print '# To resume, append the following parameter:'
+    print '#   ' + FROMFILE_PREFIX_CHARS + self._resume_output.name
 
-  # Create the scenario.
-  scenario = GenerateScenario(final_tasks, frozen_tasks)
 
+def ExecuteWithCommandLine(args, default_final_tasks):
+  """Helper to execute tasks using command line arguments.
+
+  Args:
+    args: Command line argument parsed with CommandLineParser().
+    default_final_tasks: Default final tasks if there is no -r command
+      line arguments.
+
+  Returns:
+    0 if success or 1 otherwise
+  """
+  # Builds the scenario.
+  final_tasks, frozen_tasks = _SelectTasksFromCommandLineRegexes(
+      args, default_final_tasks)
+  scenario = GenerateScenario(final_tasks, frozen_tasks)
   if len(scenario) == 0:
     logging.error('No tasks to build.')
     return 1
@@ -397,22 +461,55 @@ def ExecuteWithCommandLine(args, default_final_tasks):
     for task in scenario:
       print '{}:{}'.format(
           task.name, ' '.join([' \\\n  ' + d.name for d in task._dependencies]))
-  else:
-    for task in scenario:
-      logging.info('%s %s' % ('-' * 60, task.name))
-      try:
-        task.Execute()
-      except:
-        resume_path = os.path.join(args.output, _TASK_RESUME_ARGUMENTS_FILE)
-        resume_additonal_arguments = []
-        for task in ListResumingTasksToFreeze(
-            scenario, final_tasks, task):
-          resume_additonal_arguments.extend(['-f', re.escape(task.name)])
-        with open(resume_path, 'w') as file_output:
-          file_output.write('\n'.join(resume_additonal_arguments))
-        print '# Looks like something went wrong in \'{}\''.format(task.name)
-        print '#'
-        print '# To resume from this task, append the following parameter:'
-        print '#   ' + FROMFILE_PREFIX_CHARS + resume_path
-        raise
+    return 0
+
+  # Run the Scenario while saving intermediate state to be able to resume later.
+  failed_tasks = []
+  tasks_to_skip = set()
+  dependents_per_task = GenerateDependentSetPerTask(scenario)
+
+  def MarkTaskNotToExecute(task):
+    if task not in tasks_to_skip:
+      logging.warning('can not execute task: %s', task.name)
+      tasks_to_skip.add(task)
+      for dependent in dependents_per_task[task]:
+        MarkTaskNotToExecute(dependent)
+
+  formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
+  handler = logging.FileHandler(
+      os.path.join(args.output, _TASK_EXECUTION_LOG_NAME), mode='a')
+  handler.setFormatter(formatter)
+  logging.getLogger().addHandler(handler)
+  logging.info(
+      '%s %s', '-' * 60, common_util.GetCommandLineForLogging(sys.argv))
+  try:
+    with _ResumingFileBuilder(args) as resume_file_builder:
+      for task_execute_id, task in enumerate(scenario):
+        if task in tasks_to_skip:
+          continue
+        logging.info('%s %s', '-' * 60, task.name)
+        try:
+          task.Execute()
+        except (MemoryError, SyntaxError):
+          raise
+        except (Exception, KeyboardInterrupt):
+          logging.exception('%s %s failed', '-' * 60, task.name)
+          failed_tasks.append(task)
+          if args.keep_going and sys.exc_info()[0] != KeyboardInterrupt:
+            MarkTaskNotToExecute(task)
+          else:
+            tasks_to_skip.update(set(scenario[task_execute_id:]))
+            break
+        else:
+          resume_file_builder.OnTaskSuccess(task)
+      if tasks_to_skip:
+        assert failed_tasks
+        resume_file_builder.OnScenarioFinish(
+            scenario, final_tasks, failed_tasks, tasks_to_skip)
+        if sys.exc_info()[0] == KeyboardInterrupt:
+          raise
+        return 1
+  finally:
+    logging.getLogger().removeHandler(handler)
+  assert not failed_tasks
   return 0
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index b674b2d..e6960d8 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import argparse
 import contextlib
 import os
 import re
@@ -56,8 +57,11 @@ class TaskManagerTestCase(unittest.TestCase):
   def tearDown(self):
     shutil.rmtree(self.output_directory)
 
+  def OutputPath(self, file_path):
+    return os.path.join(self.output_directory, file_path)
+
   def TouchOutputFile(self, file_path):
-    with open(os.path.join(self.output_directory, file_path), 'w') as output:
+    with open(self.OutputPath(file_path), 'w') as output:
       output.write(file_path + '\n')
 
 
@@ -265,6 +269,31 @@ class GenerateScenarioTest(TaskManagerTestCase):
     with self.assertRaises(task_manager.TaskError):
       task_manager.GenerateScenario([TaskA, TaskB], set())
 
+  def testGenerateDependentSetPerTask(self):
+    builder = task_manager.Builder(self.output_directory, None)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b')
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskA, TaskB])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskA])
+    def TaskD():
+      pass
+
+    def RunSubTest(expected, scenario, task):
+      self.assertEqual(
+          expected, task_manager.GenerateDependentSetPerTask(scenario)[task])
+
+    RunSubTest(set([]), [TaskA], TaskA)
+    RunSubTest(set([]), [TaskA, TaskB], TaskA)
+    RunSubTest(set([TaskC]), [TaskA, TaskB, TaskC], TaskA)
+    RunSubTest(set([TaskC, TaskD]), [TaskA, TaskB, TaskC, TaskD], TaskA)
+    RunSubTest(set([]), [TaskA, TaskD], TaskD)
+
   def testGraphVizOutput(self):
     builder = task_manager.Builder(self.output_directory, None)
     static_task = builder.CreateStaticTask('a', __file__)
@@ -315,86 +344,187 @@ class GenerateScenarioTest(TaskManagerTestCase):
     for k in 'abcdef':
       self.TouchOutputFile(k)
 
-    def RunSubTest(final_tasks, initial_frozen_tasks, failed_task, reference):
-      scenario = \
-          task_manager.GenerateScenario(final_tasks, initial_frozen_tasks)
+    def RunSubTest(
+        final_tasks, initial_frozen_tasks, skipped_tasks, reference):
+      scenario = task_manager.GenerateScenario(
+          final_tasks, initial_frozen_tasks)
       resume_frozen_tasks = task_manager.ListResumingTasksToFreeze(
-          scenario, final_tasks, failed_task)
+          scenario, final_tasks, skipped_tasks)
       self.assertEqual(reference, resume_frozen_tasks)
 
-      failed_pos = scenario.index(failed_task)
       new_scenario = \
           task_manager.GenerateScenario(final_tasks, resume_frozen_tasks)
-      self.assertEqual(scenario[failed_pos:], new_scenario)
+      self.assertEqual(skipped_tasks, set(new_scenario))
 
-    RunSubTest([TaskA], set([]), TaskA, set([]))
-    RunSubTest([TaskD], set([]), TaskA, set([]))
-    RunSubTest([TaskD], set([]), TaskD, set([TaskA]))
-    RunSubTest([TaskE, TaskF], set([TaskA]), TaskB, set([TaskA]))
-    RunSubTest([TaskE, TaskF], set([TaskA]), TaskC, set([TaskA, TaskB]))
-    RunSubTest([TaskE, TaskF], set([TaskA]), TaskE, set([TaskC]))
-    RunSubTest([TaskE, TaskF], set([TaskA]), TaskF, set([TaskC, TaskE]))
+    RunSubTest([TaskA], set([]), set([TaskA]), set([]))
+    RunSubTest([TaskD], set([]), set([TaskA, TaskD]), set([]))
+    RunSubTest([TaskD], set([]), set([TaskD]), set([TaskA]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskB, TaskC, TaskE, TaskF]),
+               set([TaskA]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskC, TaskE, TaskF]),
+               set([TaskA, TaskB]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskE, TaskF]), set([TaskC]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), set([TaskF]), set([TaskC, TaskE]))
+    RunSubTest([TaskD, TaskE, TaskF], set([]), set([TaskD, TaskF]),
+               set([TaskA, TaskE, TaskC]))
 
 
 class CommandLineControlledExecutionTest(TaskManagerTestCase):
   def setUp(self):
     TaskManagerTestCase.setUp(self)
-    self.with_raise_exception_task = False
+    self.with_raise_exception_tasks = False
+    self.task_execution_history = None
 
-  def Execute(self, *command_line_args):
+  def Execute(self, command_line_args):
+    self.task_execution_history = []
     builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('a')
     def TaskA():
-      pass
+      self.task_execution_history.append(TaskA.name)
     @builder.RegisterTask('b')
     def TaskB():
-      pass
+      self.task_execution_history.append(TaskB.name)
     @builder.RegisterTask('c', dependencies=[TaskA, TaskB])
     def TaskC():
-      pass
+      self.task_execution_history.append(TaskC.name)
     @builder.RegisterTask('d', dependencies=[TaskA])
     def TaskD():
-      pass
+      self.task_execution_history.append(TaskD.name)
     @builder.RegisterTask('e', dependencies=[TaskC])
     def TaskE():
-      pass
-    @builder.RegisterTask('raise_exception', dependencies=[TaskB])
+      self.task_execution_history.append(TaskE.name)
+    @builder.RegisterTask('raise_exception', dependencies=[TaskD])
     def RaiseExceptionTask():
+      self.task_execution_history.append(RaiseExceptionTask.name)
       raise TestException('Expected error.')
+    @builder.RegisterTask('raise_keyboard_interrupt', dependencies=[TaskD])
+    def RaiseKeyboardInterruptTask():
+      self.task_execution_history.append(RaiseKeyboardInterruptTask.name)
+      raise KeyboardInterrupt
+    @builder.RegisterTask('sudden_death', dependencies=[TaskD])
+    def SimulateKillTask():
+      self.task_execution_history.append(SimulateKillTask.name)
+      raise MemoryError
 
     default_final_tasks = [TaskD, TaskE]
-    if self.with_raise_exception_task:
+    if self.with_raise_exception_tasks:
       default_final_tasks.append(RaiseExceptionTask)
-    parser = task_manager.CommandLineParser()
+      default_final_tasks.append(RaiseKeyboardInterruptTask)
+      default_final_tasks.append(SimulateKillTask)
+    task_parser = task_manager.CommandLineParser()
+    parser = argparse.ArgumentParser(parents=[task_parser],
+        fromfile_prefix_chars=task_manager.FROMFILE_PREFIX_CHARS)
     cmd = ['-o', self.output_directory]
     cmd.extend([i for i in command_line_args])
     args = parser.parse_args(cmd)
     with EatStdoutAndStderr():
       return task_manager.ExecuteWithCommandLine(args, default_final_tasks)
 
+  def ResumeFilePath(self):
+    return self.OutputPath(task_manager._TASK_RESUME_ARGUMENTS_FILE)
+
+  def ResumeCmd(self):
+    return task_manager.FROMFILE_PREFIX_CHARS + self.ResumeFilePath()
+
   def testSimple(self):
-    self.assertEqual(0, self.Execute())
+    self.assertEqual(0, self.Execute([]))
+    self.assertListEqual(['a', 'd', 'b', 'c', 'e'], self.task_execution_history)
+    self.assertTrue(
+        os.path.exists(self.OutputPath(task_manager._TASK_EXECUTION_LOG_NAME)))
 
   def testDryRun(self):
-    self.assertEqual(0, self.Execute('-d'))
+    self.assertEqual(0, self.Execute(['-d']))
+    self.assertListEqual([], self.task_execution_history)
+    self.assertFalse(
+        os.path.exists(self.OutputPath(task_manager._TASK_EXECUTION_LOG_NAME)))
 
   def testRegex(self):
-    self.assertEqual(0, self.Execute('-e', 'b', '-e', 'd'))
-    self.assertEqual(1, self.Execute('-e', r'\d'))
+    self.assertEqual(0, self.Execute(['-e', 'b', '-e', 'd']))
+    self.assertListEqual(['b', 'a', 'd'], self.task_execution_history)
+    self.assertEqual(1, self.Execute(['-e', r'\d']))
+    self.assertListEqual([], self.task_execution_history)
 
   def testFreezing(self):
-    self.assertEqual(0, self.Execute('-f', r'\d'))
+    self.assertEqual(0, self.Execute(['-f', r'\d']))
+    self.assertListEqual(['a', 'd', 'b', 'c', 'e'], self.task_execution_history)
     self.TouchOutputFile('c')
-    self.assertEqual(0, self.Execute('-f', 'c'))
+    self.assertEqual(0, self.Execute(['-f', 'c']))
+    self.assertListEqual(['a', 'd', 'e'], self.task_execution_history)
 
   def testDontFreezeUnreachableTasks(self):
     self.TouchOutputFile('c')
-    self.assertEqual(0, self.Execute('-e', 'e', '-f', 'c', '-f', 'd'))
+    self.assertEqual(0, self.Execute(['-e', 'e', '-f', 'c', '-f', 'd']))
+
+  def testAbortOnFirstError(self):
+    ARGS = ['-e', 'exception', '-e', r'^b$']
+    self.with_raise_exception_tasks = True
+    self.assertEqual(1, self.Execute(ARGS))
+    self.assertListEqual(
+        ['a', 'd', 'raise_exception'], self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual('-f\n^d$', resume_input.read())
+
+    self.TouchOutputFile('d')
+    self.assertEqual(1, self.Execute(ARGS + [self.ResumeCmd()]))
+    self.assertListEqual(['raise_exception'], self.task_execution_history)
+
+    self.assertEqual(1, self.Execute(ARGS + [self.ResumeCmd()]))
+    self.assertListEqual(['raise_exception'], self.task_execution_history)
+
+    self.assertEqual(1, self.Execute(ARGS + [self.ResumeCmd(), '-k']))
+    self.assertListEqual(['raise_exception', 'b'], self.task_execution_history)
+
+  def testKeepGoing(self):
+    ARGS = ['-k', '-e', 'exception', '-e', r'^b$']
+    self.with_raise_exception_tasks = True
+    self.assertEqual(1, self.Execute(ARGS))
+    self.assertListEqual(
+        ['a', 'd', 'raise_exception', 'b'], self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual('-f\n^d$\n-f\n^b$', resume_input.read())
+
+    self.TouchOutputFile('d')
+    self.TouchOutputFile('b')
+    self.assertEqual(1, self.Execute(ARGS + [self.ResumeCmd()]))
+    self.assertListEqual(['raise_exception'], self.task_execution_history)
+
+    self.assertEqual(1, self.Execute(ARGS + [self.ResumeCmd()]))
+    self.assertListEqual(['raise_exception'], self.task_execution_history)
+
+  def testKeyboardInterrupt(self):
+    self.with_raise_exception_tasks = True
+    with self.assertRaises(KeyboardInterrupt):
+      self.Execute(
+          ['-k', '-e', 'raise_keyboard_interrupt', '-e', r'^b$'])
+    self.assertListEqual(['a', 'd', 'raise_keyboard_interrupt'],
+                         self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual('-f\n^d$', resume_input.read())
+
+  def testResumeAfterSuddenDeath(self):
+    EXPECTED_RESUME_FILE_CONTENT = '-f\n^a$\n-f\n^d$\n'
+    ARGS = ['-k', '-e', 'sudden_death', '-e', r'^a$']
+    self.with_raise_exception_tasks = True
+    with self.assertRaises(MemoryError):
+      self.Execute(ARGS)
+    self.assertListEqual(
+        ['a', 'd', 'sudden_death'], self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual(EXPECTED_RESUME_FILE_CONTENT, resume_input.read())
 
-  def testTaskFailure(self):
-    self.with_raise_exception_task = True
-    with self.assertRaisesRegexp(TestException, r'^Expected error\.$'):
-      self.Execute('-e', 'raise_exception')
+    self.TouchOutputFile('a')
+    self.TouchOutputFile('d')
+    with self.assertRaises(MemoryError):
+      self.Execute(ARGS + [self.ResumeCmd()])
+    self.assertListEqual(['sudden_death'], self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual(EXPECTED_RESUME_FILE_CONTENT, resume_input.read())
+
+    with self.assertRaises(MemoryError):
+      self.Execute(ARGS + [self.ResumeCmd()])
+    self.assertListEqual(['sudden_death'], self.task_execution_history)
+    with open(self.ResumeFilePath()) as resume_input:
+      self.assertEqual(EXPECTED_RESUME_FILE_CONTENT, resume_input.read())
 
 
 if __name__ == '__main__':

commit cadb8d2cca5f739714b431bbf9be31ab886db9f8
Author: droger <droger@chromium.org>
Date:   Thu Jun 2 08:37:16 2016 -0700

    tools/android/loading Name the BigQuery table after the data it contains
    
    This CL moves a lot of BigQuery related code into a new file
    google_bigquery_helper.py.
    
    It has 2 functional changes:
    1) BigQuery tables are now named after the directory which contains the
       corresponding traces.
       That way, table names are less opaque, and it's easy to know which
       table corresponds to which traces.
       This change is located in GetBigQueryTableID() in
       google_bigquery_helper.py.
    2) Before starting a report task, we now check that the table does not
       already exist. This is necessary since table names are no longer
       unique. For example, if someone runs twice the same report task,
       the same table name is used, which is most likely not expected and
       would lead to duplicated entries in the table.
       This change is located in GetTaskURL() in clovis_frontend.py.
    
    Review-Url: https://codereview.chromium.org/2022033002
    Cr-Original-Commit-Position: refs/heads/master@{#397417}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d218b8dcf27efab17f6abab9bc92a56224b57199

diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index f0f4cf7..e59d65e 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -4,11 +4,10 @@
 
 import json
 import math
-import uuid
 
 from googleapiclient import errors
 
-import common.clovis_paths
+import common.google_bigquery_helper
 from common.loading_trace_database import LoadingTraceDatabase
 import common.google_error_helper as google_error_helper
 from failure_database import FailureDatabase
@@ -87,18 +86,10 @@ class ReportTaskHandler(object):
       rows: (list of dict) Each dictionary is a row to add to the table.
       table_id: (str) Identifier of the BigQuery table to update.
     """
-    # Assumes that the dataset and the table template already exist.
-    dataset = 'clovis_dataset'
-    template = 'report'
-    rows_data = [{'json': row, 'insertId': str(uuid.uuid4())} for row in rows]
-    body = {'rows': rows_data, 'templateSuffix':'_'+table_id}
-    self._logger.info('BigQuery API request:\n' + str(body))
-
     try:
-      response = self._bigquery_service.tabledata().insertAll(
-          projectId=self._project_name, datasetId=dataset, tableId=template,
-          body=body).execute()
-      self._logger.info('BigQuery API response:\n' + str(response))
+      response = common.google_bigquery_helper.InsertInTemplatedBigQueryTable(
+          self._bigquery_service, self._project_name, table_id, rows,
+          self._logger)
     except errors.HttpError as http_error:
       # Handles HTTP error response codes (such as 404), typically indicating a
       # problem in parameters other than 'body'.
@@ -161,6 +152,5 @@ class ReportTaskHandler(object):
       rows.append(report)
 
     if rows:
-      tag = clovis_task.BackendParams()['tag']
-      table_id = common.clovis_paths.GetBigQueryTableID(tag)
+      table_id = common.google_bigquery_helper.GetBigQueryTableID(clovis_task)
       self._StreamRowsToBigQuery(rows, table_id)
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index bf96347..11bc744 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -22,6 +22,7 @@ sys.path.insert(0, os.path.join(_CLOUD_DIR, os.pardir))
 sys.path.append(_CLOUD_DIR)
 
 from common.clovis_task import ClovisTask
+import common.google_bigquery_helper
 from common.google_instance_helper import GoogleInstanceHelper
 from clovis_task_handler import ClovisTaskHandler
 from failure_database import FailureDatabase
@@ -73,8 +74,8 @@ class Worker(object):
       self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
                                         'failure_database')
 
-    bigquery_service = discovery.build('bigquery', 'v2',
-                                       credentials=self._credentials)
+    bigquery_service = common.google_bigquery_helper.GetBigQueryService(
+        self._credentials)
     self._clovis_task_handler = ClovisTaskHandler(
         self._project_name, self._base_path_in_bucket, self._failure_database,
         self._google_storage_accessor, bigquery_service,
diff --git a/loading/cloud/common/clovis_paths.py b/loading/cloud/common/clovis_paths.py
index 44309bc..1ee10e4 100644
--- a/loading/cloud/common/clovis_paths.py
+++ b/loading/cloud/common/clovis_paths.py
@@ -2,33 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-
-# BigQuery path constants.
-
-# Name of the dataset.
-BIGQUERY_DATASET = 'clovis_dataset'
-# Name of the table used as a template for new tables.
-BIGQUERY_TABLE_TEMPLATE = 'report'
-
-
 # Trace path constants.
 
 # Prefix for the loading trace database files.
 TRACE_DATABASE_PREFIX = 'trace_database'
-
-
-def GetBigQueryTableID(tag):
-  """Returns the ID of the BigQuery table associated with tag. This ID is
-  appended at the end of the table name.
-  """
-  # BigQuery table names can contain only alpha numeric characters and
-  # underscores.
-  return ''.join(c for c in tag if c.isalnum() or c == '_')
-
-
-def GetBigQueryTableURL(project_name, tag):
-  """Returns the full URL for the BigQuery table associated with tag."""
-  table_id = GetBigQueryTableID(tag)
-  table_name = BIGQUERY_DATASET + '.' + BIGQUERY_TABLE_TEMPLATE + '_' + table_id
-  return 'https://bigquery.cloud.google.com/table/%s:%s' % (project_name,
-                                                            table_name)
diff --git a/loading/cloud/common/google_bigquery_helper.py b/loading/cloud/common/google_bigquery_helper.py
new file mode 100644
index 0000000..f1a7880
--- /dev/null
+++ b/loading/cloud/common/google_bigquery_helper.py
@@ -0,0 +1,106 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import uuid
+
+from googleapiclient import (discovery, errors)
+
+import common.google_error_helper as google_error_helper
+
+# Name of the dataset.
+BIGQUERY_DATASET = 'clovis_dataset'
+# Name of the table used as a template for new tables.
+BIGQUERY_TABLE_TEMPLATE = 'report'
+
+
+def GetBigQueryService(credentials):
+  """Returns the BigQuery service."""
+  return discovery.build('bigquery', 'v2', credentials=credentials)
+
+
+def GetBigQueryTableID(clovis_report_task):
+  """Returns the ID of the BigQuery table associated with the task.
+  This ID is appended at the end of the table name.
+
+  Args:
+    clovis_report_task: (ClovisTask) The task, must be a 'report' task.
+
+  Returns:
+    str: The table ID.
+  """
+  assert (clovis_report_task.Action() == 'report')
+  # Name the table after the last path component of the trace bucket.
+  trace_bucket = clovis_report_task.ActionParams()['trace_bucket']
+  table_id = os.path.basename(os.path.normpath(trace_bucket))
+  # BigQuery table names can contain only alpha numeric characters and
+  # underscores.
+  return ''.join(c for c in table_id if c.isalnum() or c == '_')
+
+
+def GetBigQueryTableURL(project_name, table_id):
+  """Returns the full URL for the BigQuery table associated with table_id."""
+  return 'https://bigquery.cloud.google.com/table/%s:%s.%s_%s' % (
+      project_name, BIGQUERY_DATASET, BIGQUERY_TABLE_TEMPLATE, table_id)
+
+
+def InsertInTemplatedBigQueryTable(bigquery_service, project_name, table_id,
+                                   rows, logger):
+  """Inserts rows in the BigQuery table corresponding to table_id.
+  Assumes that the BigQuery dataset and table template already exist.
+
+  Args:
+    bigquery_service: The BigQuery service.
+    project_name: (str) Name of the Google Cloud project.
+    table_id: (str) table_id as returned by GetBigQueryTableID().
+    rows: (list) Rows to insert in the table.
+    logger: (logging.Logger) The logger.
+
+  Returns:
+    dict: The BigQuery service response.
+  """
+  rows_data = [{'json': row, 'insertId': str(uuid.uuid4())} for row in rows]
+  body = {'rows': rows_data, 'templateSuffix':'_'+table_id}
+  logger.info('BigQuery API request:\n' + str(body))
+  response = bigquery_service.tabledata().insertAll(
+      projectId=project_name, datasetId=BIGQUERY_DATASET,
+      tableId=BIGQUERY_TABLE_TEMPLATE, body=body).execute()
+  logger.info('BigQuery API response:\n' + str(response))
+  return response
+
+
+def DoesBigQueryTableExist(bigquery_service, project_name, table_id, logger):
+  """Returns wether the BigQuery table identified by table_id exists.
+
+  Raises a HttpError exception if the call to BigQuery API fails.
+
+  Args:
+    bigquery_service: The BigQuery service.
+    project_name: (str) Name of the Google Cloud project.
+    table_id: (str) table_id as returned by GetBigQueryTableID().
+
+  Returns:
+    bool: True if the table exists.
+  """
+  table_name = BIGQUERY_TABLE_TEMPLATE + '_' + table_id
+  logger.info('Getting table information for %s.' % table_name)
+  try:
+    table = bigquery_service.tables().get(projectId=project_name,
+                                          datasetId=BIGQUERY_DATASET,
+                                          tableId=table_name).execute()
+    return bool(table)
+
+  except errors.HttpError as http_error:
+    error_content = google_error_helper.GetErrorContent(http_error)
+    error_reason = google_error_helper.GetErrorReason(error_content)
+    if error_reason == google_error_helper.REASON_NOT_FOUND:
+      return False
+    else:
+      logger.error('BigQuery API error (reason: "%s"):\n%s' % (
+          error_reason, http_error))
+      if error_content:
+        logger.error('Error details:\n%s' % error_content)
+      raise  # Re-raise the exception.
+
+  return False
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 01e4699..9f00252 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -15,6 +15,7 @@ from oauth2client.client import GoogleCredentials
 
 import common.clovis_paths
 from common.clovis_task import ClovisTask
+import common.google_bigquery_helper
 import common.google_instance_helper
 from common.loading_trace_database import LoadingTraceDatabase
 import email_helper
@@ -241,6 +242,49 @@ def GetTracePaths(bucket):
   return traces
 
 
+def GetTaskURL(task, task_dir):
+  """Returns the URL where the task output are generated, or None.
+
+  Args:
+    task: (ClovisTask) The task.
+    task_dir: (str) Working directory for the backend, it is a subdirectory of
+                    the deployment bucket.
+
+  Returns:
+    str: The URL.
+  """
+  clovis_logger.info('Building task result URL.')
+
+  if task.Action() == 'trace':
+    storage_bucket = task.BackendParams().get('storage_bucket')
+    if not storage_bucket:
+      clovis_logger.error('Missing storage_bucket for trace action.')
+      return None
+    return 'https://console.cloud.google.com/storage/%s/%s' % (storage_bucket,
+                                                               task_dir)
+
+  elif task.Action() == 'report':
+    table_id = common.google_bigquery_helper.GetBigQueryTableID(task)
+    task_url = common.google_bigquery_helper.GetBigQueryTableURL(project_name,
+                                                                 table_id)
+    # Abort if the table already exists.
+    bigquery_service = common.google_bigquery_helper.GetBigQueryService(
+        GoogleCredentials.get_application_default())
+    try:
+      table_exists = common.google_bigquery_helper.DoesBigQueryTableExist(
+          bigquery_service, project_name, table_id, clovis_logger)
+    except Exception:
+      return None
+    if table_exists:
+      clovis_logger.error('BigQuery table %s already exists.' % task_url)
+      return None
+    return task_url
+
+  else:
+    clovis_logger.error('Unsupported action: %s.' % task.Action())
+    return None
+
+
 def StartFromJsonString(http_body_str):
   """Main function handling a JSON task posted by the user."""
   # Set up logging.
@@ -259,7 +303,9 @@ def StartFromJsonString(http_body_str):
   # Compute the task directory.
   task_dir_components = []
   user_email = email_helper.GetUserEmail()
-  user_name = user_email[:user_email.find('@')]
+  user_name = None
+  if user_email:
+    user_name = user_email[:user_email.find('@')]
   if user_name:
     task_dir_components.append(user_name)
   task_name = task.BackendParams().get('task_name')
@@ -269,19 +315,11 @@ def StartFromJsonString(http_body_str):
   task_dir = os.path.join(task.Action(), '_'.join(task_dir_components))
 
   # Build the URL where the result will live.
-  task_url = None
-  if task.Action() == 'trace':
-    bucket = task.BackendParams().get('storage_bucket')
-    if bucket:
-      task_url = 'https://console.cloud.google.com/storage/%s/%s' % (bucket,
-                                                                     task_dir)
-  elif task.Action() == 'report':
-    task_url = common.clovis_paths.GetBigQueryTableURL(project_name, task_tag)
+  task_url = GetTaskURL(task, task_dir)
+  if task_url:
+    clovis_logger.info('Task result URL: ' + task_url)
   else:
-    error_string = 'Unsupported action: %s.' % task.Action()
-    clovis_logger.error(error_string)
-    return Render(error_string, memory_logs)
-  clovis_logger.info('Task result URL: ' + task_url)
+    return Render('Could not build the task URL.', memory_logs)
 
   # Split the task in smaller tasks.
   sub_tasks = SplitClovisTask(task)

commit 1cb2a6eb6efe989711629d15d9f3c9b1b46e2d8b
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 1 08:59:46 2016 -0700

    sandwich: Move all NoState-Prefetch related code in sandwich_prefetch.py
    
    mv sandwich_misc.py -> sandwich_prefetch.py
    mv sandwich_task_builder.py -> sandwich_utils.py
    Move sandwich_utils.py's PrefetchTaskBuilder into sandwich_prefetch.py
    Make sandwich_prefetch.py's functions private.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2023263002
    Cr-Original-Commit-Position: refs/heads/master@{#397154}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bfb85a4a099be52f32cd462220d4608c03251866

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 31fb4a8..e428372 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -34,10 +34,10 @@ import device_setup
 import emulation
 import options
 import sandwich_metrics
-import sandwich_misc
+import sandwich_prefetch
 import sandwich_runner
 import sandwich_swr
-import sandwich_task_builder
+import sandwich_utils
 import task_manager
 from trace_test.webserver_test import WebServer
 
@@ -164,7 +164,7 @@ def _RecordWebServerTestTrace(args):
 
 def _GenerateBenchmarkTasks(args, url, output_subdirectory):
   MAIN_TRANSFORMER_LIST_NAME = 'no-network-emulation'
-  common_builder = sandwich_task_builder.SandwichCommonBuilder(
+  common_builder = sandwich_utils.SandwichCommonBuilder(
       android_device=_GetAndroidDeviceFromArgs(args),
       url=url,
       output_directory=args.output,
@@ -180,22 +180,22 @@ def _GenerateBenchmarkTasks(args, url, output_subdirectory):
     runner.repeat = args.url_repeat
 
   if not args.swr_benchmark:
-    builder = sandwich_task_builder.PrefetchBenchmarkBuilder(common_builder)
-    builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
+    builder = sandwich_prefetch.PrefetchBenchmarkBuilder(common_builder)
+    builder.PopulateLoadBenchmark(sandwich_prefetch.EMPTY_CACHE_DISCOVERER,
                                   MAIN_TRANSFORMER_LIST_NAME,
                                   transformer_list=[MainTransformer])
-    builder.PopulateLoadBenchmark(sandwich_misc.FULL_CACHE_DISCOVERER,
+    builder.PopulateLoadBenchmark(sandwich_prefetch.FULL_CACHE_DISCOVERER,
                                   MAIN_TRANSFORMER_LIST_NAME,
                                   transformer_list=[MainTransformer])
     if args.gen_full:
       for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
         transformer_list_name = network_condition.lower()
         network_transformer = \
-            sandwich_task_builder.NetworkSimulationTransformer(
+            sandwich_utils.NetworkSimulationTransformer(
                 network_condition)
         transformer_list = [MainTransformer, network_transformer]
-        for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
-          if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
+        for subresource_discoverer in sandwich_prefetch.SUBRESOURCE_DISCOVERERS:
+          if subresource_discoverer == sandwich_prefetch.FULL_CACHE_DISCOVERER:
             continue
           builder.PopulateLoadBenchmark(subresource_discoverer,
               transformer_list_name, transformer_list)
@@ -207,7 +207,7 @@ def _GenerateBenchmarkTasks(args, url, output_subdirectory):
       for network_condition in ['Regular3G', 'Regular2G']:
         transformer_list_name = network_condition.lower()
         network_transformer = \
-            sandwich_task_builder.NetworkSimulationTransformer(
+            sandwich_utils.NetworkSimulationTransformer(
                 network_condition)
         transformer_list = [MainTransformer, network_transformer]
         builder.PopulateBenchmark(enable_swr, transformer_list_name,
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index c997963..351bd83 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -28,7 +28,6 @@ from telemetry.util import rgba_color
 
 import loading_trace as loading_trace_module
 import sandwich_runner
-import sandwich_misc
 import tracing
 
 
diff --git a/loading/sandwich_misc.py b/loading/sandwich_prefetch.py
similarity index 58%
rename from loading/sandwich_misc.py
rename to loading/sandwich_prefetch.py
index 172f4e4..173d0bf 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_prefetch.py
@@ -2,18 +2,22 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import csv
 import logging
 import json
 import os
 import re
+import shutil
 from urlparse import urlparse
 
 import chrome_cache
 import common_util
-from loading_trace import LoadingTrace
+import loading_trace
 from prefetch_view import PrefetchSimulationView
 from request_dependencies_lens import RequestDependencyLens
+import sandwich_metrics
 import sandwich_runner
+import task_manager
 import wpr_backend
 
 
@@ -43,7 +47,7 @@ SUBRESOURCE_DISCOVERERS = set([
 _UPLOAD_DATA_STREAM_REQUESTS_REGEX = re.compile(r'^\d+/(?P<url>.*)$')
 
 
-def PatchWpr(wpr_archive_path):
+def _PatchWpr(wpr_archive_path):
   """Patches a WPR archive to get all resources into the HTTP cache and avoid
   invalidation and revalidations.
 
@@ -104,8 +108,8 @@ def _FilterOutDataAndIncompleteRequests(requests):
     yield request
 
 
-def PatchCacheArchive(cache_archive_path, loading_trace_path,
-                      cache_archive_dest_path):
+def _PatchCacheArchive(cache_archive_path, loading_trace_path,
+                       cache_archive_dest_path):
   """Patch the cache archive.
 
   Note: This method update the raw response headers of cache entries' to store
@@ -119,7 +123,7 @@ def PatchCacheArchive(cache_archive_path, loading_trace_path,
         archive <cache_archive_path>.
     cache_archive_dest_path: Archive destination's path.
   """
-  trace = LoadingTrace.FromJsonFile(loading_trace_path)
+  trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
   with common_util.TemporaryDirectory(prefix='sandwich_tmp') as tmp_path:
     cache_path = os.path.join(tmp_path, 'cache')
     chrome_cache.UnzipDirectoryContent(cache_archive_path, cache_path)
@@ -154,7 +158,7 @@ def PatchCacheArchive(cache_archive_path, loading_trace_path,
     logging.info('Patched cache size: %d bytes' % cache_backend.GetSize())
 
 
-def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
+def _ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   """Extracts discoverable resource urls from a loading trace according to a
   sub-resource discoverer.
 
@@ -171,7 +175,7 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
 
   # Load trace and related infos.
   logging.info('loading %s' % loading_trace_path)
-  trace = LoadingTrace.FromJsonFile(loading_trace_path)
+  trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
   dependencies_lens = RequestDependencyLens(trace)
   first_resource_request = trace.request_track.GetFirstResourceRequest()
 
@@ -226,16 +230,16 @@ def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
     logging.error('+     ' + url)
 
 
-class RequestOutcome:
+class _RequestOutcome:
   All, ServedFromCache, NotServedFromCache, Post = range(4)
 
 
-def ListUrlRequests(trace, request_kind):
+def _ListUrlRequests(trace, request_kind):
   """Lists requested URLs from a trace.
 
   Args:
-    trace: (LoadingTrace) loading trace.
-    request_kind: RequestOutcome indicating the subset of requests to output.
+    trace: (loading_trace.LoadingTrace) loading trace.
+    request_kind: _RequestOutcome indicating the subset of requests to output.
 
   Returns:
     set([str])
@@ -243,22 +247,22 @@ def ListUrlRequests(trace, request_kind):
   urls = set()
   for request_event in _FilterOutDataAndIncompleteRequests(
       trace.request_track.GetEvents()):
-    if (request_kind == RequestOutcome.ServedFromCache and
+    if (request_kind == _RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
-    elif (request_kind == RequestOutcome.Post and
+    elif (request_kind == _RequestOutcome.Post and
         request_event.method.upper().strip() == 'POST'):
       urls.add(request_event.url)
-    elif (request_kind == RequestOutcome.NotServedFromCache and
+    elif (request_kind == _RequestOutcome.NotServedFromCache and
         not request_event.from_disk_cache):
       urls.add(request_event.url)
-    elif request_kind == RequestOutcome.All:
+    elif request_kind == _RequestOutcome.All:
       urls.add(request_event.url)
   return urls
 
 
-def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
-                                   benchmark_output_directory_path):
+def _VerifyBenchmarkOutputDirectory(benchmark_setup_path,
+                                    benchmark_output_directory_path):
   """Verifies that all run inside the run_output_directory worked as expected.
 
   Args:
@@ -285,15 +289,15 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
     if not os.path.isfile(trace_path):
       logging.error('missing trace %s' % trace_path)
       continue
-    trace = LoadingTrace.FromJsonFile(trace_path)
+    trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
     logging.info('verifying %s from %s' % (trace.url, trace_path))
 
-    effective_requests = ListUrlRequests(trace, RequestOutcome.All)
-    effective_post_requests = ListUrlRequests(trace, RequestOutcome.Post)
+    effective_requests = _ListUrlRequests(trace, _RequestOutcome.All)
+    effective_post_requests = _ListUrlRequests(trace, _RequestOutcome.Post)
     effective_cached_requests = \
-        ListUrlRequests(trace, RequestOutcome.ServedFromCache)
+        _ListUrlRequests(trace, _RequestOutcome.ServedFromCache)
     effective_uncached_requests = \
-        ListUrlRequests(trace, RequestOutcome.NotServedFromCache)
+        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache)
 
     missing_requests = original_requests.difference(effective_requests)
     unexpected_requests = effective_requests.difference(original_requests)
@@ -344,7 +348,7 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
                          'Distinct resource requests to WPR')
 
 
-def ReadSubresourceFromRunnerOutputDir(runner_output_dir):
+def _ReadSubresourceFromRunnerOutputDir(runner_output_dir):
   """Extracts a list of subresources in runner output directory.
 
   Args:
@@ -355,7 +359,7 @@ def ReadSubresourceFromRunnerOutputDir(runner_output_dir):
   """
   trace_path = os.path.join(
       runner_output_dir, '0', sandwich_runner.TRACE_FILENAME)
-  trace = LoadingTrace.FromJsonFile(trace_path)
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
   url_set = set()
   for request_event in _FilterOutDataAndIncompleteRequests(
       trace.request_track.GetEvents()):
@@ -365,7 +369,7 @@ def ReadSubresourceFromRunnerOutputDir(runner_output_dir):
   return [url for url in url_set]
 
 
-def ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
+def _ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
   """Validates a cache archive content.
 
   Args:
@@ -378,9 +382,9 @@ def ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
     chrome_cache.UnzipDirectoryContent(cache_archive_path, cache_directory)
     cache_keys = set(
         chrome_cache.CacheBackend(cache_directory, 'simple').ListKeys())
-  trace = LoadingTrace.FromJsonFile(cache_build_trace_path)
-  effective_requests = ListUrlRequests(trace, RequestOutcome.All)
-  effective_post_requests = ListUrlRequests(trace, RequestOutcome.Post)
+  trace = loading_trace.LoadingTrace.FromJsonFile(cache_build_trace_path)
+  effective_requests = _ListUrlRequests(trace, _RequestOutcome.All)
+  effective_post_requests = _ListUrlRequests(trace, _RequestOutcome.Post)
 
   upload_data_stream_cache_entry_keys = set()
   upload_data_stream_requests = set()
@@ -400,3 +404,203 @@ def ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
                          'POST resources')
   _PrintUrlSetComparison(expected_cached_requests, effective_cache_keys,
                          'Cached resources')
+
+
+class PrefetchBenchmarkBuilder(task_manager.Builder):
+  """A builder for a graph of tasks for NoState-Prefetch emulated benchmarks."""
+
+  def __init__(self, common_builder):
+    task_manager.Builder.__init__(self,
+                                  common_builder.output_directory,
+                                  common_builder.output_subdirectory)
+    self._common_builder = common_builder
+
+    self._patched_wpr_task = None
+    self._reference_cache_task = None
+    self._trace_from_grabbing_reference_cache = None
+    self._subresources_for_urls_task = None
+    self._PopulateCommonPipelines()
+
+  def _PopulateCommonPipelines(self):
+    """Creates necessary tasks to produce initial cache archive.
+
+    Also creates a task for producing a json file with a mapping of URLs to
+    subresources (urls-resources.json).
+
+    Here is the full dependency tree for the returned task:
+    common/patched-cache-validation.log
+      depends on: common/patched-cache.zip
+        depends on: common/original-cache.zip
+          depends on: common/webpages-patched.wpr
+            depends on: common/webpages.wpr
+      depends on: common/urls-resources.json
+        depends on: common/original-cache.zip
+    """
+    @self.RegisterTask('common/webpages-patched.wpr',
+                       dependencies=[self._common_builder.original_wpr_task])
+    def BuildPatchedWpr():
+      common_util.EnsureParentDirectoryExists(BuildPatchedWpr.path)
+      shutil.copyfile(
+          self._common_builder.original_wpr_task.path, BuildPatchedWpr.path)
+      _PatchWpr(BuildPatchedWpr.path)
+
+    @self.RegisterTask('common/original-cache.zip', [BuildPatchedWpr])
+    def BuildOriginalCache():
+      runner = self._common_builder.CreateSandwichRunner()
+      runner.wpr_archive_path = BuildPatchedWpr.path
+      runner.cache_archive_path = BuildOriginalCache.path
+      runner.cache_operation = sandwich_runner.CacheOperation.SAVE
+      runner.output_dir = BuildOriginalCache.run_path
+      runner.Run()
+    BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
+    original_cache_trace_path = os.path.join(
+        BuildOriginalCache.run_path, '0', sandwich_runner.TRACE_FILENAME)
+
+    @self.RegisterTask('common/patched-cache.zip', [BuildOriginalCache])
+    def BuildPatchedCache():
+      _PatchCacheArchive(BuildOriginalCache.path,
+          original_cache_trace_path, BuildPatchedCache.path)
+
+    @self.RegisterTask('common/subresources-for-urls.json',
+                       [BuildOriginalCache])
+    def ListUrlsResources():
+      url_resources = _ReadSubresourceFromRunnerOutputDir(
+          BuildOriginalCache.run_path)
+      with open(ListUrlsResources.path, 'w') as output:
+        json.dump(url_resources, output)
+
+    @self.RegisterTask('common/patched-cache-validation.log',
+                       [BuildPatchedCache])
+    def ValidatePatchedCache():
+      handler = logging.FileHandler(ValidatePatchedCache.path)
+      logging.getLogger().addHandler(handler)
+      try:
+        _ValidateCacheArchiveContent(
+            original_cache_trace_path, BuildPatchedCache.path)
+      finally:
+        logging.getLogger().removeHandler(handler)
+
+    self._patched_wpr_task = BuildPatchedWpr
+    self._trace_from_grabbing_reference_cache = original_cache_trace_path
+    self._reference_cache_task = BuildPatchedCache
+    self._subresources_for_urls_task = ListUrlsResources
+
+    self._common_builder.default_final_tasks.append(ValidatePatchedCache)
+
+  def PopulateLoadBenchmark(self, subresource_discoverer,
+                            transformer_list_name, transformer_list):
+    """Populate benchmarking tasks from its setup tasks.
+
+    Args:
+      subresource_discoverer: Name of a subresources discoverer.
+      transformer_list_name: A string describing the transformers, will be used
+          in Task names (prefer names without spaces and special characters).
+      transformer_list: An ordered list of function that takes an instance of
+          SandwichRunner as parameter, would be applied immediately before
+          SandwichRunner.Run() in the given order.
+
+    Here is the full dependency of the added tree for the returned task:
+    <transformer_list_name>/<subresource_discoverer>-metrics.csv
+      depends on: <transformer_list_name>/<subresource_discoverer>-run/
+        depends on: common/<subresource_discoverer>-cache.zip
+          depends on: some tasks saved by PopulateCommonPipelines()
+          depends on: common/<subresource_discoverer>-setup.json
+            depends on: some tasks saved by PopulateCommonPipelines()
+    """
+    additional_column_names = [
+        'url',
+        'repeat_id',
+        'subresource_discoverer',
+        'subresource_count',
+        # The amount of subresources detected at SetupBenchmark step.
+        'subresource_count_theoretic',
+        # Amount of subresources for caching as suggested by the subresource
+        # discoverer.
+        'cached_subresource_count_theoretic',
+        'cached_subresource_count']
+
+    assert subresource_discoverer in SUBRESOURCE_DISCOVERERS
+    assert 'common' not in SUBRESOURCE_DISCOVERERS
+    shared_task_prefix = os.path.join('common', subresource_discoverer)
+    task_prefix = os.path.join(transformer_list_name, subresource_discoverer)
+
+    @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
+                       dependencies=[self._subresources_for_urls_task])
+    def SetupBenchmark():
+      whitelisted_urls = _ExtractDiscoverableUrls(
+          self._trace_from_grabbing_reference_cache, subresource_discoverer)
+
+      url_resources = json.load(open(self._subresources_for_urls_task.path))
+      common_util.EnsureParentDirectoryExists(SetupBenchmark.path)
+      with open(SetupBenchmark.path, 'w') as output:
+        json.dump({
+            'cache_whitelist': [url for url in whitelisted_urls],
+            'subresource_discoverer': subresource_discoverer,
+            'url_resources': url_resources,
+          }, output)
+
+    @self.RegisterTask(shared_task_prefix + '-cache.zip', merge=True,
+                       dependencies=[
+                           SetupBenchmark, self._reference_cache_task])
+    def BuildBenchmarkCacheArchive():
+      setup = json.load(open(SetupBenchmark.path))
+      chrome_cache.ApplyUrlWhitelistToCacheArchive(
+          cache_archive_path=self._reference_cache_task.path,
+          whitelisted_urls=setup['cache_whitelist'],
+          output_cache_archive_path=BuildBenchmarkCacheArchive.path)
+
+    @self.RegisterTask(task_prefix + '-run/',
+                       dependencies=[BuildBenchmarkCacheArchive])
+    def RunBenchmark():
+      runner = self._common_builder.CreateSandwichRunner()
+      for transformer in transformer_list:
+        transformer(runner)
+      runner.wpr_archive_path = self._patched_wpr_task.path
+      runner.wpr_out_log_path = os.path.join(
+          RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
+      runner.cache_archive_path = BuildBenchmarkCacheArchive.path
+      runner.cache_operation = sandwich_runner.CacheOperation.PUSH
+      runner.output_dir = RunBenchmark.path
+      runner.Run()
+
+    @self.RegisterTask(task_prefix + '-metrics.csv',
+                       dependencies=[RunBenchmark])
+    def ExtractMetrics():
+      # TODO(gabadie): Performance improvement: load each trace only once and
+      # use it for validation and extraction of metrics later.
+      _VerifyBenchmarkOutputDirectory(SetupBenchmark.path, RunBenchmark.path)
+
+      benchmark_setup = json.load(open(SetupBenchmark.path))
+      run_metrics_list = []
+      for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
+          RunBenchmark.path):
+        trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
+        logging.info('processing trace: %s', trace_path)
+        trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
+        run_metrics = {
+            'url': trace.url,
+            'repeat_id': repeat_id,
+            'subresource_discoverer': benchmark_setup['subresource_discoverer'],
+            'subresource_count': len(_ListUrlRequests(
+                trace, _RequestOutcome.All)),
+            'subresource_count_theoretic':
+                len(benchmark_setup['url_resources']),
+            'cached_subresource_count': len(_ListUrlRequests(
+                trace, _RequestOutcome.ServedFromCache)),
+            'cached_subresource_count_theoretic':
+                len(benchmark_setup['cache_whitelist']),
+        }
+        run_metrics.update(
+            sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
+                repeat_dir, trace))
+        run_metrics_list.append(run_metrics)
+
+      run_metrics_list.sort(key=lambda e: e['repeat_id'])
+      with open(ExtractMetrics.path, 'w') as csv_file:
+        writer = csv.DictWriter(csv_file, fieldnames=(additional_column_names +
+                                    sandwich_metrics.COMMON_CSV_COLUMN_NAMES))
+        writer.writeheader()
+        for trace_metrics in run_metrics_list:
+          writer.writerow(trace_metrics)
+
+    self._common_builder.default_final_tasks.append(ExtractMetrics)
diff --git a/loading/sandwich_misc_unittest.py b/loading/sandwich_prefetch_unittest.py
similarity index 68%
rename from loading/sandwich_misc_unittest.py
rename to loading/sandwich_prefetch_unittest.py
index 9e5e455..3c54dd5 100644
--- a/loading/sandwich_misc_unittest.py
+++ b/loading/sandwich_prefetch_unittest.py
@@ -6,22 +6,22 @@ import os
 import unittest
 import urlparse
 
-import sandwich_misc
+import sandwich_prefetch
 
 
 LOADING_DIR = os.path.dirname(__file__)
 TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
 
 
-class SandwichMiscTest(unittest.TestCase):
+class SandwichPrefetchTestCase(unittest.TestCase):
   _TRACE_PATH = os.path.join(TEST_DATA_DIR, 'scanner_vs_parser.trace')
 
   def GetResourceUrl(self, path):
     return urlparse.urljoin('http://l/', path)
 
   def testNoDiscovererWhitelisting(self):
-    url_set = sandwich_misc.ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_misc.EMPTY_CACHE_DISCOVERER)
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_prefetch.EMPTY_CACHE_DISCOVERER)
     self.assertEquals(set(), url_set)
 
   def testFullCacheWhitelisting(self):
@@ -29,29 +29,29 @@ class SandwichMiscTest(unittest.TestCase):
                              self.GetResourceUrl('0.png'),
                              self.GetResourceUrl('1.png'),
                              self.GetResourceUrl('favicon.ico')])
-    url_set = sandwich_misc.ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_misc.FULL_CACHE_DISCOVERER)
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_prefetch.FULL_CACHE_DISCOVERER)
     self.assertEquals(reference_url_set, url_set)
 
   def testRedirectedMainWhitelisting(self):
     reference_url_set = set([self.GetResourceUrl('./')])
-    url_set = sandwich_misc.ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_misc.REDIRECTED_MAIN_DISCOVERER)
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_prefetch.REDIRECTED_MAIN_DISCOVERER)
     self.assertEquals(reference_url_set, url_set)
 
   def testParserDiscoverableWhitelisting(self):
     reference_url_set = set([self.GetResourceUrl('./'),
                              self.GetResourceUrl('0.png'),
                              self.GetResourceUrl('1.png')])
-    url_set = sandwich_misc.ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_misc.PARSER_DISCOVERER)
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_prefetch.PARSER_DISCOVERER)
     self.assertEquals(reference_url_set, url_set)
 
   def testHTMLPreloadScannerWhitelisting(self):
     reference_url_set = set([self.GetResourceUrl('./'),
                              self.GetResourceUrl('0.png')])
-    url_set = sandwich_misc.ExtractDiscoverableUrls(
-        self._TRACE_PATH, sandwich_misc.HTML_PRELOAD_SCANNER_DISCOVERER)
+    url_set = sandwich_prefetch._ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_prefetch.HTML_PRELOAD_SCANNER_DISCOVERER)
     self.assertEquals(reference_url_set, url_set)
 
 
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
deleted file mode 100644
index 644cc23..0000000
--- a/loading/sandwich_task_builder.py
+++ /dev/null
@@ -1,288 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import csv
-import logging
-import json
-import logging
-import os
-import shutil
-
-import chrome_cache
-import common_util
-import emulation
-import loading_trace
-import sandwich_metrics
-import sandwich_misc
-import sandwich_runner
-import task_manager
-
-
-def NetworkSimulationTransformer(network_condition):
-  """Creates a function that accepts a SandwichRunner as a parameter and sets
-  network emulation options on it.
-
-  Args:
-    network_condition: The network condition to apply to the sandwich runner.
-
-  Returns:
-    A callback transforming the SandwichRunner given in argument accordingly
-  """
-  assert network_condition in emulation.NETWORK_CONDITIONS
-  def Transformer(runner):
-    assert isinstance(runner, sandwich_runner.SandwichRunner)
-    runner.network_condition = network_condition
-  return Transformer
-
-
-class SandwichCommonBuilder(task_manager.Builder):
-  """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
-  """
-
-  def __init__(self, android_device, url, output_directory,
-               output_subdirectory):
-    """Constructor.
-
-    Args:
-      android_device: The android DeviceUtils to run sandwich on or None to run
-        it locally.
-      url: URL to benchmark.
-      output_directory: As in task_manager.Builder.__init__
-      output_subdirectory: As in task_manager.Builder.__init__
-    """
-    task_manager.Builder.__init__(self, output_directory, output_subdirectory)
-    self._android_device = android_device
-    self._url = url
-    self.default_final_tasks = []
-
-    self.original_wpr_task = None
-
-  def CreateSandwichRunner(self):
-    """Create a runner for non benchmark purposes."""
-    runner = sandwich_runner.SandwichRunner()
-    runner.url = self._url
-    runner.android_device = self._android_device
-    return runner
-
-  def OverridePathToWprArchive(self, original_wpr_path):
-    """Sets the original WPR archive path's to be used.
-
-    Args:
-      original_wpr_path: Path of the original WPR archive to be used.
-    """
-    self.original_wpr_task = \
-        self.CreateStaticTask('common/webpages.wpr', original_wpr_path)
-
-  def PopulateWprRecordingTask(self):
-    """Records the original WPR archive."""
-    @self.RegisterTask('common/webpages.wpr')
-    def BuildOriginalWpr():
-      common_util.EnsureParentDirectoryExists(BuildOriginalWpr.path)
-      runner = self.CreateSandwichRunner()
-      runner.wpr_archive_path = BuildOriginalWpr.path
-      runner.wpr_record = True
-      runner.Run()
-
-    self.original_wpr_task = BuildOriginalWpr
-
-
-class PrefetchBenchmarkBuilder(task_manager.Builder):
-  """A builder for a graph of tasks for NoState-Prefetch emulated benchmarks."""
-
-  def __init__(self, common_builder):
-    task_manager.Builder.__init__(self,
-                                  common_builder.output_directory,
-                                  common_builder.output_subdirectory)
-    self._common_builder = common_builder
-
-    self._patched_wpr_task = None
-    self._reference_cache_task = None
-    self._trace_from_grabbing_reference_cache = None
-    self._subresources_for_urls_task = None
-    self._PopulateCommonPipelines()
-
-  def _PopulateCommonPipelines(self):
-    """Creates necessary tasks to produce initial cache archive.
-
-    Also creates a task for producing a json file with a mapping of URLs to
-    subresources (urls-resources.json).
-
-    Here is the full dependency tree for the returned task:
-    common/patched-cache-validation.log
-      depends on: common/patched-cache.zip
-        depends on: common/original-cache.zip
-          depends on: common/webpages-patched.wpr
-            depends on: common/webpages.wpr
-      depends on: common/urls-resources.json
-        depends on: common/original-cache.zip
-    """
-    @self.RegisterTask('common/webpages-patched.wpr',
-                       dependencies=[self._common_builder.original_wpr_task])
-    def BuildPatchedWpr():
-      common_util.EnsureParentDirectoryExists(BuildPatchedWpr.path)
-      shutil.copyfile(
-          self._common_builder.original_wpr_task.path, BuildPatchedWpr.path)
-      sandwich_misc.PatchWpr(BuildPatchedWpr.path)
-
-    @self.RegisterTask('common/original-cache.zip', [BuildPatchedWpr])
-    def BuildOriginalCache():
-      runner = self._common_builder.CreateSandwichRunner()
-      runner.wpr_archive_path = BuildPatchedWpr.path
-      runner.cache_archive_path = BuildOriginalCache.path
-      runner.cache_operation = sandwich_runner.CacheOperation.SAVE
-      runner.output_dir = BuildOriginalCache.run_path
-      runner.Run()
-    BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
-    original_cache_trace_path = os.path.join(
-        BuildOriginalCache.run_path, '0', sandwich_runner.TRACE_FILENAME)
-
-    @self.RegisterTask('common/patched-cache.zip', [BuildOriginalCache])
-    def BuildPatchedCache():
-      sandwich_misc.PatchCacheArchive(BuildOriginalCache.path,
-          original_cache_trace_path, BuildPatchedCache.path)
-
-    @self.RegisterTask('common/subresources-for-urls.json',
-                       [BuildOriginalCache])
-    def ListUrlsResources():
-      url_resources = sandwich_misc.ReadSubresourceFromRunnerOutputDir(
-          BuildOriginalCache.run_path)
-      with open(ListUrlsResources.path, 'w') as output:
-        json.dump(url_resources, output)
-
-    @self.RegisterTask('common/patched-cache-validation.log',
-                       [BuildPatchedCache])
-    def ValidatePatchedCache():
-      handler = logging.FileHandler(ValidatePatchedCache.path)
-      logging.getLogger().addHandler(handler)
-      try:
-        sandwich_misc.ValidateCacheArchiveContent(
-            original_cache_trace_path, BuildPatchedCache.path)
-      finally:
-        logging.getLogger().removeHandler(handler)
-
-    self._patched_wpr_task = BuildPatchedWpr
-    self._trace_from_grabbing_reference_cache = original_cache_trace_path
-    self._reference_cache_task = BuildPatchedCache
-    self._subresources_for_urls_task = ListUrlsResources
-
-    self._common_builder.default_final_tasks.append(ValidatePatchedCache)
-
-  def PopulateLoadBenchmark(self, subresource_discoverer,
-                            transformer_list_name, transformer_list):
-    """Populate benchmarking tasks from its setup tasks.
-
-    Args:
-      subresource_discoverer: Name of a subresources discoverer.
-      transformer_list_name: A string describing the transformers, will be used
-          in Task names (prefer names without spaces and special characters).
-      transformer_list: An ordered list of function that takes an instance of
-          SandwichRunner as parameter, would be applied immediately before
-          SandwichRunner.Run() in the given order.
-
-    Here is the full dependency of the added tree for the returned task:
-    <transformer_list_name>/<subresource_discoverer>-metrics.csv
-      depends on: <transformer_list_name>/<subresource_discoverer>-run/
-        depends on: common/<subresource_discoverer>-cache.zip
-          depends on: some tasks saved by PopulateCommonPipelines()
-          depends on: common/<subresource_discoverer>-setup.json
-            depends on: some tasks saved by PopulateCommonPipelines()
-    """
-    additional_column_names = [
-        'url',
-        'repeat_id',
-        'subresource_discoverer',
-        'subresource_count',
-        # The amount of subresources detected at SetupBenchmark step.
-        'subresource_count_theoretic',
-        # Amount of subresources for caching as suggested by the subresource
-        # discoverer.
-        'cached_subresource_count_theoretic',
-        'cached_subresource_count']
-
-    assert subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS
-    assert 'common' not in sandwich_misc.SUBRESOURCE_DISCOVERERS
-    shared_task_prefix = os.path.join('common', subresource_discoverer)
-    task_prefix = os.path.join(transformer_list_name, subresource_discoverer)
-
-    @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
-                       dependencies=[self._subresources_for_urls_task])
-    def SetupBenchmark():
-      whitelisted_urls = sandwich_misc.ExtractDiscoverableUrls(
-          self._trace_from_grabbing_reference_cache, subresource_discoverer)
-
-      url_resources = json.load(open(self._subresources_for_urls_task.path))
-      common_util.EnsureParentDirectoryExists(SetupBenchmark.path)
-      with open(SetupBenchmark.path, 'w') as output:
-        json.dump({
-            'cache_whitelist': [url for url in whitelisted_urls],
-            'subresource_discoverer': subresource_discoverer,
-            'url_resources': url_resources,
-          }, output)
-
-    @self.RegisterTask(shared_task_prefix + '-cache.zip', merge=True,
-                       dependencies=[
-                           SetupBenchmark, self._reference_cache_task])
-    def BuildBenchmarkCacheArchive():
-      setup = json.load(open(SetupBenchmark.path))
-      chrome_cache.ApplyUrlWhitelistToCacheArchive(
-          cache_archive_path=self._reference_cache_task.path,
-          whitelisted_urls=setup['cache_whitelist'],
-          output_cache_archive_path=BuildBenchmarkCacheArchive.path)
-
-    @self.RegisterTask(task_prefix + '-run/',
-                       dependencies=[BuildBenchmarkCacheArchive])
-    def RunBenchmark():
-      runner = self._common_builder.CreateSandwichRunner()
-      for transformer in transformer_list:
-        transformer(runner)
-      runner.wpr_archive_path = self._patched_wpr_task.path
-      runner.wpr_out_log_path = os.path.join(
-          RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
-      runner.cache_archive_path = BuildBenchmarkCacheArchive.path
-      runner.cache_operation = sandwich_runner.CacheOperation.PUSH
-      runner.output_dir = RunBenchmark.path
-      runner.Run()
-
-    @self.RegisterTask(task_prefix + '-metrics.csv',
-                       dependencies=[RunBenchmark])
-    def ExtractMetrics():
-      # TODO(gabadie): Performance improvement: load each trace only once and
-      # use it for validation and extraction of metrics later.
-      sandwich_misc.VerifyBenchmarkOutputDirectory(
-          SetupBenchmark.path, RunBenchmark.path)
-
-      benchmark_setup = json.load(open(SetupBenchmark.path))
-      run_metrics_list = []
-      for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
-          RunBenchmark.path):
-        trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
-        logging.info('processing trace: %s', trace_path)
-        trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
-        run_metrics = {
-            'url': trace.url,
-            'repeat_id': repeat_id,
-            'subresource_discoverer': benchmark_setup['subresource_discoverer'],
-            'subresource_count': len(sandwich_misc.ListUrlRequests(
-                trace, sandwich_misc.RequestOutcome.All)),
-            'subresource_count_theoretic':
-                len(benchmark_setup['url_resources']),
-            'cached_subresource_count': len(sandwich_misc.ListUrlRequests(
-                trace, sandwich_misc.RequestOutcome.ServedFromCache)),
-            'cached_subresource_count_theoretic':
-                len(benchmark_setup['cache_whitelist']),
-        }
-        run_metrics.update(
-            sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
-                repeat_dir, trace))
-        run_metrics_list.append(run_metrics)
-
-      run_metrics_list.sort(key=lambda e: e['repeat_id'])
-      with open(ExtractMetrics.path, 'w') as csv_file:
-        writer = csv.DictWriter(csv_file, fieldnames=(additional_column_names +
-                                    sandwich_metrics.COMMON_CSV_COLUMN_NAMES))
-        writer.writeheader()
-        for trace_metrics in run_metrics_list:
-          writer.writerow(trace_metrics)
-
-    self._common_builder.default_final_tasks.append(ExtractMetrics)
diff --git a/loading/sandwich_utils.py b/loading/sandwich_utils.py
new file mode 100644
index 0000000..3942eec
--- /dev/null
+++ b/loading/sandwich_utils.py
@@ -0,0 +1,76 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import common_util
+import emulation
+import sandwich_runner
+import task_manager
+
+
+def NetworkSimulationTransformer(network_condition):
+  """Creates a function that accepts a SandwichRunner as a parameter and sets
+  network emulation options on it.
+
+  Args:
+    network_condition: The network condition to apply to the sandwich runner.
+
+  Returns:
+    A callback transforming the SandwichRunner given in argument accordingly
+  """
+  assert network_condition in emulation.NETWORK_CONDITIONS
+  def Transformer(runner):
+    assert isinstance(runner, sandwich_runner.SandwichRunner)
+    runner.network_condition = network_condition
+  return Transformer
+
+
+class SandwichCommonBuilder(task_manager.Builder):
+  """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
+  """
+
+  def __init__(self, android_device, url, output_directory,
+               output_subdirectory):
+    """Constructor.
+
+    Args:
+      android_device: The android DeviceUtils to run sandwich on or None to run
+        it locally.
+      url: URL to benchmark.
+      output_directory: As in task_manager.Builder.__init__
+      output_subdirectory: As in task_manager.Builder.__init__
+    """
+    task_manager.Builder.__init__(self, output_directory, output_subdirectory)
+    self._android_device = android_device
+    self._url = url
+    self.default_final_tasks = []
+
+    self.original_wpr_task = None
+
+  def CreateSandwichRunner(self):
+    """Create a runner for non benchmark purposes."""
+    runner = sandwich_runner.SandwichRunner()
+    runner.url = self._url
+    runner.android_device = self._android_device
+    return runner
+
+  def OverridePathToWprArchive(self, original_wpr_path):
+    """Sets the original WPR archive path's to be used.
+
+    Args:
+      original_wpr_path: Path of the original WPR archive to be used.
+    """
+    self.original_wpr_task = \
+        self.CreateStaticTask('common/webpages.wpr', original_wpr_path)
+
+  def PopulateWprRecordingTask(self):
+    """Records the original WPR archive."""
+    @self.RegisterTask('common/webpages.wpr')
+    def BuildOriginalWpr():
+      common_util.EnsureParentDirectoryExists(BuildOriginalWpr.path)
+      runner = self.CreateSandwichRunner()
+      runner.wpr_archive_path = BuildOriginalWpr.path
+      runner.wpr_record = True
+      runner.Run()
+
+    self.original_wpr_task = BuildOriginalWpr

commit f47200f8a0ee823e12148059e29660a4ebd712dc
Author: droger <droger@chromium.org>
Date:   Wed Jun 1 08:34:51 2016 -0700

    tools/android/loading Improve BigQuery template documentation
    
    Review-Url: https://codereview.chromium.org/2024953006
    Cr-Original-Commit-Position: refs/heads/master@{#397149}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a8a7146d69456b794bef6c9e320ff1220a2058a0

diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index df66330..e6fd69c 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -53,8 +53,7 @@ Finds all the traces in the specified bucket and generates a report in BigQuery.
 This requires an existing `clovis_dataset.report` BigQuery table that will be
 used as a template. The schema of this template is not updated automatically and
 must match the format of the report (as generated by `report.py`).
-To update the schema manually, delete and recreate the `clovis_dataset.report`
-from the BigQuery web interface as an empty table with the new schema.
+See [how to update the schema manually][7].
 
 ## Development
 
@@ -127,7 +126,8 @@ gcloud preview app deploy app.yaml
 
 When a change is made to the dictionary returned by report.py, the BigQuery
 database schema must be updated accordingly.
-Then, to actually update the schema, run:
+
+To update the schema, run:
 
 ```shell
 bq update \
@@ -138,6 +138,16 @@ bq update \
 
 Adding a new field is harmless, but don't modify existing ones.
 
+If the above command does not work, this is probably because you are doing more
+than adding fields.
+In this case, you can delete and recreate the `clovis_dataset.report` table from
+the [BigQuery web interface][8]:
+-   Expand `clovis_dataset` from the left menu, and delete the `report` table.
+-   Create a new table in `clovis_dataset`, and call it `report`.
+-   Set `Location` to `None` in order to create an empty table.
+-   Click `Edit as Text` in the `Schema` section , and  paste the contents of
+    `bigquery_schema.json` there.
+
 
 [1]: https://cloud.google.com/sdk
 [2]: https://cloud.google.com/appengine/docs/python/taskqueue
@@ -145,3 +155,5 @@ Adding a new field is harmless, but don't modify existing ones.
 [4]: ../backend/README.md#Deploy-the-code
 [5]: https://cloud.google.com/appengine/docs/python
 [6]: http://flask.pocoo.org
+[7]: #Updating-the-Database-Schema
+[8]: https://bigquery.cloud.google.com

commit cd27d620b1abae9683e2c3f211cd2e2a253628d6
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 1 07:11:59 2016 -0700

    sandwich: Make metrics extraction more customizable.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2009883002
    Cr-Original-Commit-Position: refs/heads/master@{#397134}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 928ffadce5e22bdddeb231d357468b32dd92936b

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 126a24b..c997963 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -32,19 +32,9 @@ import sandwich_misc
 import tracing
 
 
-CSV_FIELD_NAMES = [
-    'repeat_id',
-    'url',
+COMMON_CSV_COLUMN_NAMES = [
     'chromium_commit',
     'platform',
-    'subresource_discoverer',
-    'subresource_count',
-    # The amount of subresources detected at SetupBenchmark step.
-    'subresource_count_theoretic',
-    # Amount of subresources for caching as suggested by the subresource
-    # discoverer.
-    'cached_subresource_count_theoretic',
-    'cached_subresource_count',
     'first_layout',
     'first_contentful_paint',
     'total_load',
@@ -222,30 +212,6 @@ def _ExtractMemoryMetrics(loading_trace):
   }
 
 
-def _ExtractBenchmarkStatistics(benchmark_setup, loading_trace):
-  """Extracts some useful statistics from a benchmark run.
-
-  Args:
-    benchmark_setup: benchmark_setup: dict representing the benchmark setup
-        JSON. The JSON format is according to:
-            PrefetchBenchmarkBuilder.PopulateLoadBenchmark.SetupBenchmark.
-    loading_trace: loading_trace_module.LoadingTrace.
-
-  Returns:
-    Dictionary with all extracted fields set.
-  """
-  return {
-    'subresource_discoverer': benchmark_setup['subresource_discoverer'],
-    'subresource_count': len(sandwich_misc.ListUrlRequests(
-        loading_trace, sandwich_misc.RequestOutcome.All)),
-    'subresource_count_theoretic': len(benchmark_setup['url_resources']),
-    'cached_subresource_count': len(sandwich_misc.ListUrlRequests(
-        loading_trace, sandwich_misc.RequestOutcome.ServedFromCache)),
-    'cached_subresource_count_theoretic':
-        len(benchmark_setup['cache_whitelist']),
-  }
-
-
 def _ExtractCompletenessRecordFromVideo(video_path):
   """Extracts the completeness record from a video.
 
@@ -283,7 +249,7 @@ def _ExtractCompletenessRecordFromVideo(video_path):
   return [(time, FrameProgress(hist)) for time, hist in histograms]
 
 
-def ComputeSpeedIndex(completeness_record):
+def _ComputeSpeedIndex(completeness_record):
   """Computes the speed-index from a completeness record.
 
   Args:
@@ -305,82 +271,41 @@ def ComputeSpeedIndex(completeness_record):
   return speed_index
 
 
-def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
-  """Extracts all the metrics from traces and video of a sandwich run.
+def ExtractCommonMetricsFromRepeatDirectory(repeat_dir, trace):
+  """Extracts all the metrics from traces and video of a sandwich run repeat
+  directory.
 
   Args:
-    benchmark_setup: benchmark_setup: dict representing the benchmark setup
-        JSON. The JSON format is according to:
-            PrefetchBenchmarkBuilder.PopulateLoadBenchmark.SetupBenchmark.
-    run_directory_path: Path of the run directory.
+    repeat_dir: Path of the repeat directory within a run directory.
+    trace: preloaded LoadingTrace in |repeat_dir|
+
+  Contract:
+    trace == LoadingTrace.FromJsonFile(
+        os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME))
 
   Returns:
     Dictionary of extracted metrics.
   """
-  trace_path = os.path.join(run_directory_path, 'trace.json')
-  logging.info('processing trace \'%s\'' % trace_path)
-  loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
   run_metrics = {
-      'url': loading_trace.url,
-      'chromium_commit': loading_trace.metadata['chromium_commit'],
-      'platform': (loading_trace.metadata['platform']['os'] + '-' +
-          loading_trace.metadata['platform']['product_model'])
+      'chromium_commit': trace.metadata['chromium_commit'],
+      'platform': (trace.metadata['platform']['os'] + '-' +
+          trace.metadata['platform']['product_model'])
   }
-  run_metrics.update(_ExtractDefaultMetrics(loading_trace))
-  run_metrics.update(_ExtractMemoryMetrics(loading_trace))
-  if benchmark_setup:
-    run_metrics.update(
-        _ExtractBenchmarkStatistics(benchmark_setup, loading_trace))
-  video_path = os.path.join(run_directory_path, 'video.mp4')
+  run_metrics.update(_ExtractDefaultMetrics(trace))
+  run_metrics.update(_ExtractMemoryMetrics(trace))
+  video_path = os.path.join(repeat_dir, sandwich_runner.VIDEO_FILENAME)
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
     try:
       completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
-      run_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+      run_metrics['speed_index'] = _ComputeSpeedIndex(completeness_record)
     except video.BoundingBoxNotFoundException:
       # Sometimes the bounding box for the web content area is not present. Skip
       # calculating Speed Index.
       run_metrics['speed_index'] = _FAILED_CSV_VALUE
   else:
     run_metrics['speed_index'] = _UNAVAILABLE_CSV_VALUE
-  for key, value in loading_trace.metadata['network_emulation'].iteritems():
+  for key, value in trace.metadata['network_emulation'].iteritems():
     run_metrics['net_emul.' + key] = value
+  assert set(run_metrics.keys()) == set(COMMON_CSV_COLUMN_NAMES)
   return run_metrics
-
-
-def ExtractMetricsFromRunnerOutputDirectory(benchmark_setup_path,
-                                            output_directory_path):
-  """Extracts all the metrics from all the traces of a sandwich runner output
-  directory.
-
-  Args:
-    benchmark_setup_path: Path of the JSON of the benchmark setup.
-    output_directory_path: The sandwich runner's output directory to extract the
-        metrics from.
-
-  Returns:
-    List of dictionaries.
-  """
-  benchmark_setup = None
-  if benchmark_setup_path:
-    benchmark_setup = json.load(open(benchmark_setup_path))
-  assert os.path.isdir(output_directory_path)
-  metrics = []
-  for node_name in os.listdir(output_directory_path):
-    if not os.path.isdir(os.path.join(output_directory_path, node_name)):
-      continue
-    try:
-      repeat_id = int(node_name)
-    except ValueError:
-      continue
-    run_directory_path = os.path.join(output_directory_path, node_name)
-    run_metrics = _ExtractMetricsFromRunDirectory(
-        benchmark_setup, run_directory_path)
-    run_metrics['repeat_id'] = repeat_id
-    # TODO(gabadie): Make common metrics extraction with benchmark type
-    # specific CSV column.
-    # assert set(run_metrics.keys()) == set(CSV_FIELD_NAMES)
-    metrics.append(run_metrics)
-  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich runner ' +
-                            'output directory.').format(output_directory_path)
-  return metrics
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 7ea183c..c0ce2a4 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -232,7 +232,7 @@ class PageTrackTest(unittest.TestCase):
       point(400, 1.0),
     ]
     self.assertEqual(120 + 70 * 0.6 + 90 * 0.25,
-                     puller.ComputeSpeedIndex(completness_record))
+                     puller._ComputeSpeedIndex(completness_record))
 
     completness_record = [
       point(70, 0.0),
@@ -242,7 +242,7 @@ class PageTrackTest(unittest.TestCase):
       point(240, 1.0),
     ]
     self.assertEqual(80 + 60 * 0.7 + 10 * 0.4 + 20 * 0.1,
-                     puller.ComputeSpeedIndex(completness_record))
+                     puller._ComputeSpeedIndex(completness_record))
 
     completness_record = [
       point(90, 0.0),
@@ -251,7 +251,7 @@ class PageTrackTest(unittest.TestCase):
       point(230, 1.0),
     ]
     with self.assertRaises(ValueError):
-      puller.ComputeSpeedIndex(completness_record)
+      puller._ComputeSpeedIndex(completness_record)
 
 
 if __name__ == '__main__':
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index fb48b7c..9ed8258 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -254,3 +254,24 @@ class SandwichRunner(object):
       self._PullCacheFromDevice()
 
     self._chrome_ctl = None
+
+
+def WalkRepeatedRuns(runner_output_dir):
+  """Yields unordered (repeat id, path of the repeat directory).
+
+  Args:
+    runner_output_dir: Same as for SandwichRunner.output_dir.
+  """
+  repeated_run_count = 0
+  for node_name in os.listdir(runner_output_dir):
+    repeat_dir = os.path.join(runner_output_dir, node_name)
+    if not os.path.isdir(repeat_dir):
+      continue
+    try:
+      repeat_id = int(node_name)
+    except ValueError:
+      continue
+    yield repeat_id, repeat_dir
+    repeated_run_count += 1
+  assert repeated_run_count > 0, ('Error: not a sandwich runner output '
+                                  'directory: {}').format(runner_output_dir)
diff --git a/loading/sandwich_swr.py b/loading/sandwich_swr.py
index ee4831a..46910c4 100644
--- a/loading/sandwich_swr.py
+++ b/loading/sandwich_swr.py
@@ -101,6 +101,8 @@ class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
       depends on: <transformer_list_name>/{swr,worstcase}-run/
         depends on: some tasks saved by PopulateCommonPipelines()
     """
+    additional_column_names = ['url', 'repeat_id']
+
     task_prefix = os.path.join(transformer_list_name, '')
     if enable_swr:
       task_prefix += 'swr'
@@ -124,15 +126,27 @@ class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
 
     @self.RegisterTask(task_prefix + '-metrics.csv', [RunBenchmark])
     def ExtractMetrics():
-      trace_metrics_list = \
-          sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
-              None, RunBenchmark.path)
-      trace_metrics_list.sort(key=lambda e: e['repeat_id'])
+      run_metrics_list = []
+      for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
+          RunBenchmark.path):
+        trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
+        logging.info('processing trace: %s', trace_path)
+        trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
+        run_metrics = {
+            'url': trace.url,
+            'repeat_id': repeat_id,
+        }
+        run_metrics.update(
+            sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
+                repeat_dir, trace))
+        run_metrics_list.append(run_metrics)
+
+      run_metrics_list.sort(key=lambda e: e['repeat_id'])
       with open(ExtractMetrics.path, 'w') as csv_file:
-        writer = csv.DictWriter(csv_file,
-                                fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
+        writer = csv.DictWriter(csv_file, fieldnames=(additional_column_names +
+                                    sandwich_metrics.COMMON_CSV_COLUMN_NAMES))
         writer.writeheader()
-        for trace_metrics in trace_metrics_list:
+        for trace_metrics in run_metrics_list:
           writer.writerow(trace_metrics)
 
     self._common_builder.default_final_tasks.append(ExtractMetrics)
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 61a4828..644cc23 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -5,12 +5,14 @@
 import csv
 import logging
 import json
+import logging
 import os
 import shutil
 
 import chrome_cache
 import common_util
 import emulation
+import loading_trace
 import sandwich_metrics
 import sandwich_misc
 import sandwich_runner
@@ -186,6 +188,18 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
           depends on: common/<subresource_discoverer>-setup.json
             depends on: some tasks saved by PopulateCommonPipelines()
     """
+    additional_column_names = [
+        'url',
+        'repeat_id',
+        'subresource_discoverer',
+        'subresource_count',
+        # The amount of subresources detected at SetupBenchmark step.
+        'subresource_count_theoretic',
+        # Amount of subresources for caching as suggested by the subresource
+        # discoverer.
+        'cached_subresource_count_theoretic',
+        'cached_subresource_count']
+
     assert subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS
     assert 'common' not in sandwich_misc.SUBRESOURCE_DISCOVERERS
     shared_task_prefix = os.path.join('common', subresource_discoverer)
@@ -233,17 +247,42 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
     @self.RegisterTask(task_prefix + '-metrics.csv',
                        dependencies=[RunBenchmark])
     def ExtractMetrics():
+      # TODO(gabadie): Performance improvement: load each trace only once and
+      # use it for validation and extraction of metrics later.
       sandwich_misc.VerifyBenchmarkOutputDirectory(
           SetupBenchmark.path, RunBenchmark.path)
-      trace_metrics_list = \
-          sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
-              SetupBenchmark.path, RunBenchmark.path)
-      trace_metrics_list.sort(key=lambda e: e['repeat_id'])
+
+      benchmark_setup = json.load(open(SetupBenchmark.path))
+      run_metrics_list = []
+      for repeat_id, repeat_dir in sandwich_runner.WalkRepeatedRuns(
+          RunBenchmark.path):
+        trace_path = os.path.join(repeat_dir, sandwich_runner.TRACE_FILENAME)
+        logging.info('processing trace: %s', trace_path)
+        trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
+        run_metrics = {
+            'url': trace.url,
+            'repeat_id': repeat_id,
+            'subresource_discoverer': benchmark_setup['subresource_discoverer'],
+            'subresource_count': len(sandwich_misc.ListUrlRequests(
+                trace, sandwich_misc.RequestOutcome.All)),
+            'subresource_count_theoretic':
+                len(benchmark_setup['url_resources']),
+            'cached_subresource_count': len(sandwich_misc.ListUrlRequests(
+                trace, sandwich_misc.RequestOutcome.ServedFromCache)),
+            'cached_subresource_count_theoretic':
+                len(benchmark_setup['cache_whitelist']),
+        }
+        run_metrics.update(
+            sandwich_metrics.ExtractCommonMetricsFromRepeatDirectory(
+                repeat_dir, trace))
+        run_metrics_list.append(run_metrics)
+
+      run_metrics_list.sort(key=lambda e: e['repeat_id'])
       with open(ExtractMetrics.path, 'w') as csv_file:
-        writer = csv.DictWriter(csv_file,
-                                fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
+        writer = csv.DictWriter(csv_file, fieldnames=(additional_column_names +
+                                    sandwich_metrics.COMMON_CSV_COLUMN_NAMES))
         writer.writeheader()
-        for trace_metrics in trace_metrics_list:
+        for trace_metrics in run_metrics_list:
           writer.writerow(trace_metrics)
 
     self._common_builder.default_final_tasks.append(ExtractMetrics)

commit 199b95e6c0ca9d2a2c86aff51caae82216870a44
Author: droger <droger@chromium.org>
Date:   Wed Jun 1 06:47:07 2016 -0700

    tools/android/loading Prevent gcloud instance leak.
    
    This CL moves the instance creation at the end of the process, in order
    to avoid creating instances if the task is not valid.
    
    It also schedules the polling task responsible for cleaning up the
    instances before actually creating them so that this cleanup task is run
    even if something goes wrong during the instance creation process.
    
    Review-Url: https://codereview.chromium.org/2027843004
    Cr-Original-Commit-Position: refs/heads/master@{#397126}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9081ca13fdb637f8897d99c346c947bd4dd4e31d

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 3303b5b..01e4699 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -268,10 +268,6 @@ def StartFromJsonString(http_body_str):
   task_dir_components.append(task_tag)
   task_dir = os.path.join(task.Action(), '_'.join(task_dir_components))
 
-  # Create the instance template if required.
-  if not CreateInstanceTemplate(task, task_dir):
-    return Render('Template creation failed.', memory_logs)
-
   # Build the URL where the result will live.
   task_url = None
   if task.Action() == 'trace':
@@ -295,10 +291,6 @@ def StartFromJsonString(http_body_str):
   if not EnqueueTasks(sub_tasks, task_tag):
     return Render('Task creation failed.', memory_logs)
 
-  # Start the instances if required.
-  if not CreateInstances(task):
-    return Render('Instance creation failed.', memory_logs)
-
   # Start polling the progress.
   clovis_logger.info('Creating worker polling task.')
   first_poll_delay_minutes = 10
@@ -306,6 +298,12 @@ def StartFromJsonString(http_body_str):
   deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours, user_email,
                  task_url, _countdown=(60 * first_poll_delay_minutes))
 
+  # Start the instances if required.
+  if not CreateInstanceTemplate(task, task_dir):
+    return Render('Instance template creation failed.', memory_logs)
+  if not CreateInstances(task):
+    return Render('Instance creation failed.', memory_logs)
+
   return Render(flask.Markup(
       'Success!<br>Your task %s has started.<br>'
       'You will be notified at %s when completed.') % (task_tag, user_email),

commit ab22a9f9e85cd499dc8221969aae5a8c7d18c5bb
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jun 1 06:42:19 2016 -0700

    tools/android/loading: Don't fail when an unknown request fails.
    
    When fetching a page, sometimes a "LoadingFailed" notification is
    delivered for a request that was never started, with a good network on
    which failed requests are not expected.
    Log a warning instead of failing in this case.
    
    Review-Url: https://codereview.chromium.org/1705693002
    Cr-Original-Commit-Position: refs/heads/master@{#397124}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b3e2ef0b8158748cc123724bad3548a3eb11723b

diff --git a/loading/request_track.py b/loading/request_track.py
index 93028be..ffd1f43 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -796,7 +796,9 @@ class RequestTrack(devtools_monitor.Track):
     self._FinalizeRequest(request_id)
 
   def _LoadingFailed(self, request_id, _):
-    assert request_id in self._requests_in_flight
+    if request_id not in self._requests_in_flight:
+      logging.warning('An unknown request failed: %s' % request_id)
+      return
     (r, _) = self._requests_in_flight[request_id]
     r.failed = True
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)

commit e0b65938a455cc56c9824052e19b2c7602aa79ca
Author: mattcary <mattcary@chromium.org>
Date:   Wed Jun 1 05:39:55 2016 -0700

    Clovis: upgrade instance class so we don't have to optimize memory usage.
    
    Review-Url: https://codereview.chromium.org/2023073003
    Cr-Original-Commit-Position: refs/heads/master@{#397115}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 65c873e2440233e3b12242930190869a5fd90919

diff --git a/loading/cloud/frontend/app.yaml b/loading/cloud/frontend/app.yaml
index 2801e9d..48bf775 100644
--- a/loading/cloud/frontend/app.yaml
+++ b/loading/cloud/frontend/app.yaml
@@ -1,6 +1,7 @@
 runtime: python27
 api_version: 1
 threadsafe: yes
+instance_class: F4_1G
 
 builtins:
 - deferred: on

commit 20ab708f2f9555cae77e8ffa6ba6cf3b1848a630
Author: gabadie <gabadie@chromium.org>
Date:   Wed Jun 1 05:11:52 2016 -0700

    tools/android/loading: Lets some execption pass through chrome controller's Open
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2026193003
    Cr-Original-Commit-Position: refs/heads/master@{#397109}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 79b66d38a29dda10dc6801a5e9d5950d09abb39a

diff --git a/loading/controller.py b/loading/controller.py
index 46e68df..452b548 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -82,6 +82,7 @@ class ChromeControllerError(Exception):
   """
   _INTERMITTENT_WHITE_LIST = {websocket.WebSocketTimeoutException,
                               devtools_monitor.DevToolsConnectionTargetCrashed}
+  _PASSTHROUGH_WHITE_LIST = (MemoryError, SyntaxError)
 
   def __init__(self, log):
     """Constructor
@@ -354,7 +355,9 @@ class RemoteChromeController(ChromeControllerBase):
           raise ChromeControllerInternalError(
               'Failed to connect to Chrome devtools after {} '
               'attempts.'.format(self.DEVTOOLS_CONNECTION_ATTEMPTS))
-      except:
+      except ChromeControllerError._PASSTHROUGH_WHITE_LIST:
+        raise
+      except Exception:
         logcat = ''.join([l + '\n' for l in self._device.adb.Logcat(dump=True)])
         raise ChromeControllerError(log=logcat)
       finally:
@@ -523,7 +526,9 @@ class LocalChromeController(ChromeControllerBase):
         connection.Close()
         chrome_process.wait()
         chrome_process = None
-    except:
+    except ChromeControllerError._PASSTHROUGH_WHITE_LIST:
+      raise
+    except Exception:
       raise ChromeControllerError(log=open(tmp_log.name).read())
     finally:
       if OPTIONS.local_noisy:

commit b707a181903fa4932ec0fe6a7ac79e5dcf02f9f6
Author: droger <droger@chromium.org>
Date:   Tue May 31 11:20:40 2016 -0700

    tools/android/loading Update Clovis report BigQuery schema.
    
    Review-Url: https://codereview.chromium.org/2022193002
    Cr-Original-Commit-Position: refs/heads/master@{#396868}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e5ba1f39fa5df8c2aada2a706b7b69813b59c8f0

diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index c1dbcc6..df66330 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -132,7 +132,7 @@ Then, to actually update the schema, run:
 ```shell
 bq update \
   --schema \
-    $CHROMIUM_SRC/tools/android/loading/cloud/frontend/bigquery_schema.json
+    $CHROMIUM_SRC/tools/android/loading/cloud/frontend/bigquery_schema.json \
   -t clovis_dataset.report
 ```
 
diff --git a/loading/cloud/frontend/bigquery_schema.json b/loading/cloud/frontend/bigquery_schema.json
index d713526..82b16df 100644
--- a/loading/cloud/frontend/bigquery_schema.json
+++ b/loading/cloud/frontend/bigquery_schema.json
@@ -1,137 +1,199 @@
 [
     {
-        "mode": "REQUIRED",
         "name": "url",
-        "type": "STRING"
+        "type": "STRING",
+        "mode": "REQUIRED"
     },
     {
-        "mode": "NULLABLE",
-        "name": "first_text_ms",
+        "name": "activity_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "contentful_paint_ms",
+        "name": "parsing_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "significant_paint_ms",
+        "name": "script_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "plt_ms",
+        "name": "ad_or_tracking_initiated_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "ad_or_tracking_initiated_transfer_size",
+        "type": "INTEGER"
+    },
+    {
+        "name": "ad_or_tracking_parsing_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "contentful_byte_frac",
+        "name": "ad_or_tracking_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "ad_or_tracking_script_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "significant_byte_frac",
+        "name": "ad_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "connection_cost_ms",
+        "type": "FLOAT"
+    },
+    {
+        "name": "connections",
+        "type": "INTEGER"
+    },
+    {
+        "name": "contentful_activity_frac",
+        "type": "FLOAT"
+    },
+    {
+        "name": "contentful_byte_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
         "name": "contentful_inversion",
         "type": "STRING"
     },
     {
-        "mode": "NULLABLE",
-        "name": "significant_inversion",
-        "type": "STRING"
+        "name": "contentful_ms",
+        "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "request_count",
-        "type": "INTEGER"
+        "name": "contentful_parsing_frac",
+        "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "ad_requests",
+        "name": "contentful_preloaded_requests",
         "type": "INTEGER"
     },
     {
-        "mode": "NULLABLE",
-        "name": "tracking_requests",
+        "name": "contentful_requests",
         "type": "INTEGER"
     },
     {
-        "mode": "NULLABLE",
-        "name": "transfer_size",
-        "type": "INTEGER"
+        "name": "contentful_script_frac",
+        "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "ad_or_tracking_initiated_transfer_size",
+        "name": "data_requests",
         "type": "INTEGER"
     },
     {
-        "mode": "NULLABLE",
-        "name": "ad_or_tracking_requests",
+        "name": "dns_cost_ms",
+        "type": "FLOAT"
+    },
+    {
+        "name": "dns_requests",
         "type": "INTEGER"
     },
     {
-        "mode": "NULLABLE",
-        "name": "ad_or_tracking_initiated_requests",
+        "name": "domains",
         "type": "INTEGER"
     },
     {
-        "mode": "NULLABLE",
-        "name": "activity_load_frac",
+        "name": "first_text_activity_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "activity_contentful_paint_frac",
+        "name": "first_text_byte_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "activity_significant_paint_frac",
+        "name": "first_text_inversion",
+        "type": "STRING"
+    },
+    {
+        "name": "first_text_ms",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "parsing_load_frac",
+        "name": "first_text_parsing_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "script_load_frac",
+        "name": "first_text_preloaded_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "first_text_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "first_text_script_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "parsing_contentful_frac",
+        "name": "h2_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "http11_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "plt_ms",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "script_contentful_frac",
+        "name": "preloaded_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "significant_activity_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "parsing_significant_frac",
+        "name": "significant_byte_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "script_significant_frac",
+        "name": "significant_inversion",
+        "type": "STRING"
+    },
+    {
+        "name": "significant_ms",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "dns_cost_ms",
+        "name": "significant_parsing_frac",
         "type": "FLOAT"
     },
     {
-        "mode": "NULLABLE",
-        "name": "dns_requests",
+        "name": "significant_preloaded_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "significant_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "significant_script_frac",
+        "type": "FLOAT"
+    },
+    {
+        "name": "ssl_connections",
+        "type": "INTEGER"
+    },
+    {
+        "name": "ssl_cost_ms",
+        "type": "FLOAT"
+    },
+    {
+        "name": "tracking_requests",
+        "type": "INTEGER"
+    },
+    {
+        "name": "transfer_size",
         "type": "INTEGER"
     }
 ]

commit e4122bb69123b1b18c8d0ad24fb153739cba6f6e
Author: gabadie <gabadie@chromium.org>
Date:   Tue May 31 09:24:05 2016 -0700

    tools/android/loading: Fix a bug in CachingPolicy.PolicyAtDate()
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2026733002
    Cr-Original-Commit-Position: refs/heads/master@{#396844}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b574164cb43200dac2dd931594c996cdc34d31fd

diff --git a/loading/request_track.py b/loading/request_track.py
index 6c3277d..93028be 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -455,7 +455,7 @@ class CachingPolicy(object):
     age = self._GetCurrentAge(timestamp)
     if freshness[0] > age:
       return self.VALIDATION_NONE
-    if freshness[1] > age:
+    if (freshness[0] + freshness[1]) > age:
       return self.VALIDATION_ASYNC
     return self.VALIDATION_SYNC
 
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 336e46c..e96fbf2 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -216,13 +216,16 @@ class CachingPolicyTestCase(unittest.TestCase):
   def testStaleWhileRevalidate(self):
     r = self._MakeRequest()
     r.response_headers['Cache-Control'] = (
-        'whatever,max-age=100,stale-while-revalidate=2000')
+        'whatever,max-age=1000,stale-while-revalidate=2000')
     self.assertEqual(
         CachingPolicy.VALIDATION_ASYNC,
         CachingPolicy(r).PolicyAtDate(r.wall_time + 200))
     self.assertEqual(
-        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy.VALIDATION_ASYNC,
         CachingPolicy(r).PolicyAtDate(r.wall_time + 2000))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 3100))
     # must-revalidate overrides stale-while-revalidate.
     r.response_headers['Cache-Control'] += ',must-revalidate'
     self.assertEqual(

commit 5cfbae816dd1c06434448ed0065ebad88f5c01a9
Author: mattcary <mattcary@chromium.org>
Date:   Tue May 31 08:43:36 2016 -0700

    Clovis (cloud): log errors that raise an exception in the subprocess before our
    GenerateTrace method has a chance to hook everything.
    
    Review-Url: https://codereview.chromium.org/2020283002
    Cr-Original-Commit-Position: refs/heads/master@{#396832}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 30a11fab3e7168c27b2f3714fc1f325cc8b7f761

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index c127fd9..b705326 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -183,8 +183,19 @@ class TraceTaskHandler(object):
       controller.LocalChromeController.KillChromeProcesses()
       pool.terminate()
       failed = True
-
-    if not apply_result.successful():
+    elif not apply_result.successful():
+      # Try to reraise the exception that killed the subprocess and add it to
+      # the error log.
+      try:
+        apply_result.get()
+      except Exception as e:
+        with file(log_filename, 'w+') as error_log:
+          error_log.write('Unhandled exception caught by apply_result: {}'
+                          .format(e))
+          traceback.print_exc(file=error_log)
+      else:
+        with file(log_filename, 'w+') as error_log:
+          error_log.write('No exception found for unsuccessful apply_result')
       self._logger.error('Process failure for trace generation of URL: ' + url)
       self._failure_database.AddFailure('trace_process_error', url)
       failed = True
@@ -233,9 +244,13 @@ class TraceTaskHandler(object):
     else:
       self._logger.warning('No trace found at: ' + local_filename)
 
-    self._logger.debug('Uploading analyze log')
-    remote_log_location = remote_trace_location + '.log'
-    self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
+    if os.path.isfile(log_filename):
+      self._logger.debug('Uploading analyze log')
+      remote_log_location = remote_trace_location + '.log'
+      self._google_storage_accessor.UploadFile(
+          log_filename, remote_log_location)
+    else:
+      self._logger.warning('No log file found at: {}'.format(log_filename))
 
   def Finalize(self):
     """Called once before the handler is destroyed."""
@@ -287,4 +302,3 @@ class TraceTaskHandler(object):
 
     if success_happened:
       self._UploadTraceDatabase()
-

commit 658c9183f345618abfa032d36174f8914d254095
Author: pasko <pasko@chromium.org>
Date:   Tue May 31 08:39:52 2016 -0700

    android/loading: reset more browsing state
    
    Also remove caches and the information about recently open tabs. All probably
    not strictly necessary, but good to have to avoid questions about their
    potential influence on speed.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2027633002
    Cr-Original-Commit-Position: refs/heads/master@{#396831}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d11d1dc3b53b509289792aeaf956a0ed416e582c

diff --git a/loading/controller.py b/loading/controller.py
index 279d0ba..46e68df 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -296,7 +296,7 @@ class RemoteChromeController(ChromeControllerBase):
     Caution: The browser state might need to be manually reseted.
 
     Args:
-      device: an android device.
+      device: (device_utils.DeviceUtils) an android device.
     """
     assert device is not None, 'Should you be using LocalController instead?'
     super(RemoteChromeController, self).__init__()
@@ -361,13 +361,16 @@ class RemoteChromeController(ChromeControllerBase):
         self._device.ForceStop(package_info.package)
 
   def ResetBrowserState(self):
-    """Override for chrome state reseting."""
-    logging.info('Reset chrome\'s profile')
-    package_info = OPTIONS.ChromePackage()
-    # We assume all the browser is in the Default user profile directory.
-    cmd = ['rm', '-rf', '/data/data/{}/app_chrome/Default'.format(
-               package_info.package)]
-    self._device.adb.Shell(subprocess.list2cmdline(cmd))
+    """Override resetting Chrome local state."""
+    logging.info('Resetting Chrome local state')
+    package = OPTIONS.ChromePackage().package
+    # Remove the Chrome Profile and the various disk caches. Other parts
+    # theoretically should not affect loading performance. Also remove the tab
+    # state to prevent it from growing infinitely. [:D]
+    for directory in ['app_chrome/Default', 'cache', 'app_chrome/ShaderCache',
+                      'app_tabs']:
+      cmd = ['rm', '-rf', '/data/data/{}/{}'.format(package, directory)]
+      self._device.adb.Shell(subprocess.list2cmdline(cmd))
 
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""

commit ccb6f61c60488ae825f01763f01debf9b12a87b3
Author: pasko <pasko@chromium.org>
Date:   Tue May 31 07:15:53 2016 -0700

    Sandwich: Capture metrics starting from navigationStart
    
    All metrics are registered according to the first occurrence of the
    corresponding event in the trace.
    
    Previously we only captured events that started after unloadEventEnd for the
    about:blank page. The unload event executes asynchronously, which made us
    occasionally miss the first requestStart.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2023763003
    Cr-Original-Commit-Position: refs/heads/master@{#396824}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 42a76c87b3ab3ea5599e6440ffba7484e938209b

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 0a6b167..126a24b 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -128,28 +128,38 @@ def _GetWebPageTrackedEvents(tracing_track):
     tracing_track: The tracing.TracingTrack.
 
   Returns:
-    Dictionary all tracked events.
+    A dict mapping event.name -> tracing.Event for each first occurrence of a
+        tracked event.
   """
-  main_frame = None
+  main_frame_id = None
   tracked_events = {}
-  for event in tracing_track.GetEvents():
+  sorted_events = sorted(tracing_track.GetEvents(),
+                         key=lambda event: event.start_msec)
+  for event in sorted_events:
     if event.category != 'blink.user_timing':
       continue
     event_name = event.name
-    # Ignore events until about:blank's unloadEventEnd that give the main
-    # frame id.
-    if not main_frame:
-      if event_name == 'unloadEventEnd':
-        main_frame = event.args['frame']
-        logging.info('found about:blank\'s event \'unloadEventEnd\'')
+
+    # Find the id of the main frame. Skip all events until it is found.
+    if not main_frame_id:
+      # Tracing (in Sandwich) is started after about:blank is fully loaded,
+      # hence the first navigationStart in the trace registers the correct frame
+      # id.
+      if event_name == 'navigationStart':
+        logging.info('Found navigationStart at: %f', event.start_msec)
+        main_frame_id = event.args['frame']
       continue
-    # Ignore sub-frames events. requestStart don't have the frame set but it
-    # is fine since tracking the first one after about:blank's unloadEventEnd.
-    if 'frame' in event.args and event.args['frame'] != main_frame:
+
+    # Ignore events with frame id attached, but not being the main frame.
+    if 'frame' in event.args and event.args['frame'] != main_frame_id:
       continue
+
+    # Capture trace events by the first time of their appearance. Note: some
+    # important events (like requestStart) do not have a frame id attached.
     if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
-      logging.info('found url\'s event \'%s\'' % event_name)
       tracked_events[event_name] = event
+      logging.info('Event %s first appears at: %f', event_name,
+          event.start_msec)
   return tracked_events
 
 
@@ -157,7 +167,7 @@ def _ExtractDefaultMetrics(loading_trace):
   """Extracts all the default metrics from a given trace.
 
   Args:
-    loading_trace: loading_trace_module.LoadingTrace.
+    loading_trace: loading_trace.LoadingTrace.
 
   Returns:
     Dictionary with all trace extracted fields set.
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 12ed037..7ea183c 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -23,12 +23,12 @@ _MEM_CAT = sandwich_runner.MEMORY_DUMP_CATEGORY
 _START = 'requestStart'
 _LOADS = 'loadEventStart'
 _LOADE = 'loadEventEnd'
-_UNLOAD = 'unloadEventEnd'
+_NAVIGATION_START = 'navigationStart'
 _PAINT = 'firstContentfulPaint'
 _LAYOUT = 'firstLayout'
 
 _MINIMALIST_TRACE_EVENTS = [
-    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10000,
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _NAVIGATION_START, 'ts': 10000,
         'args': {'frame': '0'}},
     {'ph': 'R', 'cat': _BLINK_CAT, 'name': _START,  'ts': 20000,
         'args': {}},
@@ -145,16 +145,12 @@ class PageTrackTest(unittest.TestCase):
             'name': _LOADS},
         {'ph': 'R', 'ts':  5000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
             'name': _LOADE},
-        {'ph': 'R', 'ts':  6000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _UNLOAD},
         {'ph': 'R', 'ts':  7000, 'args': {},             'cat': _BLINK_CAT,
             'name': _START},
         {'ph': 'R', 'ts':  8000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
             'name': _LOADS},
         {'ph': 'R', 'ts':  9000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
             'name': _LOADE},
-        {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _UNLOAD},
         {'ph': 'R', 'ts': 11000, 'args': {'frame': '0'}, 'cat': 'whatever',
             'name': _START},
         {'ph': 'R', 'ts': 12000, 'args': {'frame': '0'}, 'cat': 'whatever',
@@ -163,6 +159,10 @@ class PageTrackTest(unittest.TestCase):
             'name': _LOADE},
         {'ph': 'R', 'ts': 14000, 'args': {},             'cat': _BLINK_CAT,
             'name': _START},
+        {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _NAVIGATION_START}, # Event out of |start_msec| order.
+        {'ph': 'R', 'ts':  6000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _NAVIGATION_START},
         {'ph': 'R', 'ts': 15000, 'args': {},             'cat': _BLINK_CAT,
             'name': _START},
         {'ph': 'R', 'ts': 16000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
@@ -203,7 +203,7 @@ class PageTrackTest(unittest.TestCase):
   def testExtractDefaultMetricsBestEffort(self):
     metrics = puller._ExtractDefaultMetrics(LoadingTrace([
         {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _UNLOAD},
+            'name': _NAVIGATION_START},
         {'ph': 'R', 'ts': 11000, 'args': {'frame': '0'}, 'cat': 'whatever',
             'name': _START}]))
     self.assertEquals(4, len(metrics))
diff --git a/loading/tracing.py b/loading/tracing.py
index 9a03e22..ca9ebcb 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -86,6 +86,7 @@ class TracingTrack(devtools_monitor.Track):
     return self._base_msec
 
   def GetEvents(self):
+    """Returns a list of tracing.Event. Not sorted."""
     return self._events
 
   def GetMatchingEvents(self, category, name):

commit 0807a8c85ca3e99adc7e2898261640302a3bbe6c
Author: lizeb <lizeb@chromium.org>
Date:   Tue May 31 06:27:03 2016 -0700

    clovis: Report connection stats.
    
    Review-Url: https://codereview.chromium.org/2028553002
    Cr-Original-Commit-Position: refs/heads/master@{#396821}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 87aa7157f4946785443e59e5ec7855976062d9a8

diff --git a/loading/metrics.py b/loading/metrics.py
index 59a3077..3a82d1e 100644
--- a/loading/metrics.py
+++ b/loading/metrics.py
@@ -9,6 +9,9 @@ shows a graph of the amount of data to download for a new visit to the same
 page, with a given time interval.
 """
 
+import collections
+import urlparse
+
 import content_classification_lens
 from request_track import CachingPolicy
 
@@ -116,6 +119,52 @@ def DnsRequestsAndCost(trace):
   return (dns_requests_count, dns_cost)
 
 
+def ConnectionMetrics(trace):
+  """Returns the connection metrics for a given trace.
+
+  Returns:
+  {
+    'connections': int,
+    'connection_cost_ms': float,
+    'ssl_connections': int,
+    'ssl_cost_ms': float,
+    'http11_requests': int,
+    'h2_requests': int,
+    'data_requests': int,
+    'domains': int
+  }
+  """
+  requests = trace.request_track.GetEvents()
+  requests_with_connect = [r for r in requests if r.timing.connect_start != -1]
+  requests_with_connect_count = len(requests_with_connect)
+  connection_cost = sum(r.timing.connect_end - r.timing.connect_start
+                        for r in requests_with_connect)
+  ssl_requests = [r for r in requests if r.timing.ssl_start != -1]
+  ssl_requests_count = len(ssl_requests)
+  ssl_cost = sum(r.timing.ssl_end - r.timing.ssl_start for r in ssl_requests)
+  requests_per_protocol = collections.defaultdict(int)
+  for r in requests:
+    requests_per_protocol[r.protocol] += 1
+
+  domains = set()
+  for r in requests:
+    if r.protocol == 'data':
+      continue
+    domain = urlparse.urlparse(r.url).hostname
+    domains.add(domain)
+
+  return {
+    'connections': requests_with_connect_count,
+    'connection_cost_ms': connection_cost,
+    'ssl_connections': ssl_requests_count,
+    'ssl_cost_ms': ssl_cost,
+    'http11_requests': requests_per_protocol['http/1.1'],
+    'h2_requests': requests_per_protocol['h2'],
+    'data_requests': requests_per_protocol['data'],
+    'domains': len(domains)
+  }
+
+
 def PlotTransferSizeVsTimeBetweenVisits(trace):
   times = [10, 60, 300, 600, 3600, 4 * 3600, 12 * 3600, 24 * 3600]
   labels = ['10s', '1m', '10m', '1h', '4h', '12h', '1d']
diff --git a/loading/metrics_unittest.py b/loading/metrics_unittest.py
index 236b48a..b10a529 100644
--- a/loading/metrics_unittest.py
+++ b/loading/metrics_unittest.py
@@ -84,6 +84,32 @@ class MetricsTestCase(unittest.TestCase):
     self.assertEqual(1, count)
     self.assertEqual(8, cost)
 
+  def testConnectionMetrics(self):
+    requests = [request_track.Request.FromJsonDict(copy.deepcopy(self._REQUEST))
+                for _ in xrange(3)]
+    requests[0].url = 'http://chromium.org/'
+    requests[0].protocol = 'http/1.1'
+    requests[0].timing.connect_start = 12
+    requests[0].timing.connect_end = 42
+    requests[0].timing.ssl_start = 50
+    requests[0].timing.ssl_end = 70
+    requests[1].url = 'https://chromium.org/where-am-i/'
+    requests[1].protocol = 'h2'
+    requests[1].timing.connect_start = 22
+    requests[1].timing.connect_end = 73
+    requests[2].url = 'http://www.chromium.org/here/'
+    requests[2].protocol = 'http/42'
+    trace = test_utils.LoadingTraceFromEvents(requests)
+    stats = metrics.ConnectionMetrics(trace)
+    self.assertEqual(2, stats['connections'])
+    self.assertEqual(81, stats['connection_cost_ms'])
+    self.assertEqual(1, stats['ssl_connections'])
+    self.assertEqual(20, stats['ssl_cost_ms'])
+    self.assertEqual(1, stats['http11_requests'])
+    self.assertEqual(1, stats['h2_requests'])
+    self.assertEqual(0, stats['data_requests'])
+    self.assertEqual(2, stats['domains'])
+
   @classmethod
   def _MakeTrace(cls):
     request = request_track.Request.FromJsonDict(copy.deepcopy(cls._REQUEST))
diff --git a/loading/report.py b/loading/report.py
index bdbf116..3542e99 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -108,6 +108,7 @@ class LoadingReport(object):
     self._requests = len(requests)
     self._preloaded_requests = len(preloaded_requests)
     self._dns_requests, self._dns_cost_msec = metrics.DnsRequestsAndCost(trace)
+    self._connection_stats = metrics.ConnectionMetrics(trace)
 
     self._user_lens_reports = {}
     first_text_paint_lens = FirstTextPaintLens(self.trace)
@@ -159,6 +160,7 @@ class LoadingReport(object):
     report.update(self._cpu_busyness)
     report.update(self._ad_report)
     report.update(self._ads_cost)
+    report.update(self._connection_stats)
     return report
 
   @classmethod

commit a37b221739adc293e4d22fc805b1fc10cb08f0f0
Author: lizeb <lizeb@chromium.org>
Date:   Tue May 31 05:51:52 2016 -0700

    clovis: Add the database schema and instructions to update it.
    
    Review-Url: https://codereview.chromium.org/2022063002
    Cr-Original-Commit-Position: refs/heads/master@{#396820}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c7ae4bedf7fd3865492ffa48d5528916656b4c30

diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index f5fa5f7..c1dbcc6 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -123,6 +123,22 @@ pip install -r requirements.txt -t lib
 gcloud preview app deploy app.yaml
 ```
 
+### Updating the Database Schema
+
+When a change is made to the dictionary returned by report.py, the BigQuery
+database schema must be updated accordingly.
+Then, to actually update the schema, run:
+
+```shell
+bq update \
+  --schema \
+    $CHROMIUM_SRC/tools/android/loading/cloud/frontend/bigquery_schema.json
+  -t clovis_dataset.report
+```
+
+Adding a new field is harmless, but don't modify existing ones.
+
+
 [1]: https://cloud.google.com/sdk
 [2]: https://cloud.google.com/appengine/docs/python/taskqueue
 [3]: https://cloud.google.com/appengine/docs/python/config/queue
diff --git a/loading/cloud/frontend/bigquery_schema.json b/loading/cloud/frontend/bigquery_schema.json
new file mode 100644
index 0000000..d713526
--- /dev/null
+++ b/loading/cloud/frontend/bigquery_schema.json
@@ -0,0 +1,137 @@
+[
+    {
+        "mode": "REQUIRED",
+        "name": "url",
+        "type": "STRING"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "first_text_ms",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "contentful_paint_ms",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "significant_paint_ms",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "plt_ms",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "contentful_byte_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "significant_byte_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "contentful_inversion",
+        "type": "STRING"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "significant_inversion",
+        "type": "STRING"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "request_count",
+        "type": "INTEGER"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "ad_requests",
+        "type": "INTEGER"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "tracking_requests",
+        "type": "INTEGER"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "transfer_size",
+        "type": "INTEGER"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "ad_or_tracking_initiated_transfer_size",
+        "type": "INTEGER"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "ad_or_tracking_requests",
+        "type": "INTEGER"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "ad_or_tracking_initiated_requests",
+        "type": "INTEGER"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "activity_load_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "activity_contentful_paint_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "activity_significant_paint_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "parsing_load_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "script_load_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "parsing_contentful_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "script_contentful_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "parsing_significant_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "script_significant_frac",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "dns_cost_ms",
+        "type": "FLOAT"
+    },
+    {
+        "mode": "NULLABLE",
+        "name": "dns_requests",
+        "type": "INTEGER"
+    }
+]
diff --git a/loading/report.py b/loading/report.py
index d8416eb..bdbf116 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -140,6 +140,9 @@ class LoadingReport(object):
 
   def GenerateReport(self):
     """Returns a report as a dict."""
+    # NOTE: When changing the return value here, also update the schema
+    # (bigquery_schema.json) accordingly. See cloud/frontend/README.md for
+    # details.
     report = {
         'url': self.trace.url,
         'plt_ms': self._load_end_msec - self._navigation_start_msec,

commit 79b16c6a3af8fed37a0c4a155f20e3d09be14955
Author: lizeb <lizeb@chromium.org>
Date:   Tue May 31 01:53:57 2016 -0700

    clovis: Report the number and cost of DNS requests.
    
    Review-Url: https://codereview.chromium.org/2021093002
    Cr-Original-Commit-Position: refs/heads/master@{#396804}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d75decea55c0632853ce530014c5314563c11378

diff --git a/loading/metrics.py b/loading/metrics.py
index deb4619..59a3077 100644
--- a/loading/metrics.py
+++ b/loading/metrics.py
@@ -106,6 +106,16 @@ def AdsAndTrackingTransferSize(trace, ad_rules_filename,
   return TransferSize(requests)
 
 
+def DnsRequestsAndCost(trace):
+  """Returns the number and cost of DNS requests for a trace."""
+  requests = trace.request_track.GetEvents()
+  requests_with_dns = [r for r in requests if r.timing.dns_start != -1]
+  dns_requests_count = len(requests_with_dns)
+  dns_cost = sum(r.timing.dns_end - r.timing.dns_start
+                 for r in requests_with_dns)
+  return (dns_requests_count, dns_cost)
+
+
 def PlotTransferSizeVsTimeBetweenVisits(trace):
   times = [10, 60, 300, 600, 3600, 4 * 3600, 12 * 3600, 24 * 3600]
   labels = ['10s', '1m', '10m', '1h', '4h', '12h', '1d']
diff --git a/loading/metrics_unittest.py b/loading/metrics_unittest.py
index 56153bf..236b48a 100644
--- a/loading/metrics_unittest.py
+++ b/loading/metrics_unittest.py
@@ -72,6 +72,18 @@ class MetricsTestCase(unittest.TestCase):
     self.assertEqual(self._BODY_SIZE + self._RESPONSE_HEADERS_SIZE,
                      downloaded)
 
+  def testDnsRequestsAndCost(self):
+    trace = self._MakeTrace()
+    (count, cost) = metrics.DnsRequestsAndCost(trace)
+    self.assertEqual(0, count)
+    self.assertEqual(0, cost)
+    r = trace.request_track.GetEvents()[0]
+    r.timing.dns_end = 12
+    r.timing.dns_start = 4
+    (count, cost) = metrics.DnsRequestsAndCost(trace)
+    self.assertEqual(1, count)
+    self.assertEqual(8, cost)
+
   @classmethod
   def _MakeTrace(cls):
     request = request_track.Request.FromJsonDict(copy.deepcopy(cls._REQUEST))
diff --git a/loading/report.py b/loading/report.py
index 21ca1e8..d8416eb 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -107,6 +107,7 @@ class LoadingReport(object):
            requests[0], dependencies_lens, self.trace)
     self._requests = len(requests)
     self._preloaded_requests = len(preloaded_requests)
+    self._dns_requests, self._dns_cost_msec = metrics.DnsRequestsAndCost(trace)
 
     self._user_lens_reports = {}
     first_text_paint_lens = FirstTextPaintLens(self.trace)
@@ -144,7 +145,9 @@ class LoadingReport(object):
         'plt_ms': self._load_end_msec - self._navigation_start_msec,
         'requests': self._requests,
         'preloaded_requests': self._preloaded_requests,
-        'transfer_size': self._transfer_size}
+        'transfer_size': self._transfer_size,
+        'dns_requests': self._dns_requests,
+        'dns_cost_ms': self._dns_cost_msec}
 
     for user_lens_type, user_lens_report in self._user_lens_reports.iteritems():
       for key, value in user_lens_report.GenerateReport().iteritems():

commit 731ad5221f2845419dff155b42a227e72b43c054
Author: mattcary <mattcary@chromium.org>
Date:   Tue May 31 01:09:50 2016 -0700

    Clovis: tweak test-util comment to make it accurate.
    
    Review-Url: https://codereview.chromium.org/2018373002
    Cr-Original-Commit-Position: refs/heads/master@{#396802}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 378adfe12b9ed629a8843cc661eb12b445bf3ac7

diff --git a/loading/test_utils.py b/loading/test_utils.py
index 8858da2..cb2fd2e 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -136,7 +136,7 @@ def MakeRequest(
 
 
 def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
-  """Returns a LoadingTrace instance from a list of requests and page events."""
+  """Returns a LoadingTrace instance from various events."""
   request = FakeRequestTrack(requests)
   page_event_track = FakePageTrack(page_events if page_events else [])
   if trace_events is not None:

commit 14a03b8f6f7b4be0d523204707181525e2bbb64f
Author: mattcary <mattcary@chromium.org>
Date:   Mon May 30 07:18:35 2016 -0700

    Clovis: resource queuing analysis lens.
    
    Uses the queuing lifecycle events added in cl/1994943002.
    
    Review-Url: https://codereview.chromium.org/2018353003
    Cr-Original-Commit-Position: refs/heads/master@{#396723}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 739701d5197b50ad34893addf51979fd938887f5

diff --git a/loading/queuing_lens.py b/loading/queuing_lens.py
new file mode 100644
index 0000000..268407a
--- /dev/null
+++ b/loading/queuing_lens.py
@@ -0,0 +1,161 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Lens for resource load queuing.
+
+When executed as a script, takes a loading trace and prints queuing information
+for each request.
+"""
+
+import collections
+import itertools
+import logging
+
+import tracing
+
+
+class QueuingLens(object):
+  """Attaches queuing related trace events to request objects."""
+  QUEUING_CATEGORY = tracing.QUEUING_CATEGORY
+  ASYNC_NAME = 'ScheduledResourceRequest'
+  READY_NAME = 'ScheduledResourceRequest.Ready'
+  SET_PRIORITY_NAME = 'ScheduledResourceRequest.SetPriority'
+  QUEUING_NAMES = set([ASYNC_NAME,
+                       READY_NAME,
+                       SET_PRIORITY_NAME])
+
+  IN_FLIGHT_NAME = 'ResourceScheduler::Client.InFlightRequests'
+  SHOULD_START_NAME = 'ResourceScheduler::Client::ShouldStartRequestInfo'
+
+  def __init__(self, trace):
+    self._request_track = trace.request_track
+    self._queuing_events_by_id = self._GetQueuingEvents(trace.tracing_track)
+    self._source_id_to_url = {}
+    for source_id, events in self._queuing_events_by_id.iteritems():
+      self._source_id_to_url[source_id] = self._GetQueuingEventUrl(events)
+
+  def GenerateRequestQueuing(self):
+    """Computes queuing information for each request.
+
+    We determine blocking requests by looking at which urls are in-flight
+    (created but not yet destroyed) at the time of the creation of each
+    request. This means that a request that we list as blocking may just be
+    queued (throttled) at the same time as our request, and not actually
+    blocking.
+
+    The lifetime of the queuing events extends from when a resource is first
+    slotted into the sytem until the request is complete. The main interesting
+    queuing events are begin, end (which define the lifetime) and ready, an
+    instant event that is usually within a millisecond after the request_time of
+    the Request.
+
+    Returns:
+      {request_track.Request:
+         (start_msec: throttle start, end_msec: throttle end,
+          ready_msec: ready,
+          blocking: [blocking requests],
+          source_ids: [source ids of the request])}, which the map values are
+      anonymous objects with the specified fields.
+    """
+    url_to_requests = collections.defaultdict(list)
+    for rq in self._request_track.GetEvents():
+      url_to_requests[rq.url].append(rq)
+    # Queuing events are organized by source id, which corresponds to a load of
+    # a url. First collect timing information for each source id, then associate
+    # with each request.
+    timing_by_source_id = {}
+    for source_id, events in self._queuing_events_by_id.iteritems():
+      assert all(e.end_msec is None for e in events), \
+          'Unexpected end_msec for nested async queuing events'
+      ready_times = [e.start_msec for e in events if e.name == self.READY_NAME]
+      if not ready_times:
+        ready_msec = None
+      else:
+        assert len(ready_times) == 1, events
+        ready_msec = ready_times[0]
+      timing_by_source_id[source_id] = (
+          min(e.start_msec for e in events),
+          max(e.start_msec for e in events),
+          ready_msec)
+    queue_info = {}
+    for request_url, requests in url_to_requests.iteritems():
+      matching_source_ids = set(
+          source_id for source_id, url in self._source_id_to_url.iteritems()
+          if url == request_url)
+      # TODO(mattcary): I think this assert will fail exactly when there is more
+      # than one request for the same URL.
+      assert len(matching_source_ids) <= 1, requests
+      # Get first source id.
+      sid = next(s for s in matching_source_ids) \
+          if matching_source_ids else None
+      (throttle_start_msec, throttle_end_msec, ready_msec) = \
+         timing_by_source_id[sid] if matching_source_ids else (-1, -1, -1)
+
+      blocking_requests = itertools.chain.from_iterable(
+          url_to_requests[self._source_id_to_url[sid]]
+          for sid, (flight_start_msec,
+                    flight_end_msec, _) in timing_by_source_id.iteritems()
+          if (flight_start_msec < throttle_start_msec and
+              flight_end_msec > throttle_start_msec and
+              flight_end_msec < throttle_end_msec))
+      blocking_requests = [b for b in blocking_requests]
+
+      info = collections.namedtuple(
+          'QueueInfo', ['start_msec', 'end_msec', 'ready_msec', 'blocking'
+                        'source_ids'])
+      info.start_msec = throttle_start_msec
+      info.end_msec = throttle_end_msec
+      info.ready_msec = ready_msec
+      current_request_ids = set(rq.request_id for rq in requests)
+      info.blocking = [b for b in blocking_requests
+                       if b is not None and
+                       b.request_id not in current_request_ids]
+      info.source_ids = matching_source_ids
+      for rq in requests:
+        queue_info[rq] = info
+    return queue_info
+
+  def _GetQueuingEvents(self, tracing_track):
+    events = collections.defaultdict(list)
+    for e in tracing_track.GetEvents():
+      if (e.category == self.QUEUING_CATEGORY and
+          e.name in self.QUEUING_NAMES):
+        events[e.args['data']['source_id']].append(e)
+    return events
+
+  def _GetQueuingEventUrl(self, events):
+    urls = set()
+    for e in events:
+      if 'request_url' in e.args['data']:
+        urls.add(e.args['data']['request_url'])
+    assert len(urls) == 1
+    return urls.pop()
+
+  def _GetEventsForRequest(self, request):
+    request_events = []
+    for source_id, url in self._source_id_to_url:
+      if url == request.url:
+        request_events.extend(self._queuing_events_by_id[source_id])
+    return request_events
+
+
+def _Main(trace_file):
+  import loading_trace
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_file)
+  lens = QueuingLens(trace)
+  queue_info = lens.GenerateRequestQueuing()
+  base_msec = trace.request_track.GetFirstRequestMillis()
+  mkmsec = lambda ms: ms - base_msec if ms > 0 else -1
+  for rq, info in queue_info.iteritems():
+    print '{fp} ({ts}->{te})[{rs}->{re}] {ids} {url}'.format(
+        fp=rq.fingerprint,
+        ts=mkmsec(info.start_msec), te=mkmsec(info.end_msec),
+        rs=mkmsec(rq.start_msec), re=mkmsec(rq.end_msec),
+        ids=info.source_ids, url=rq.url)
+    for blocking_request in info.blocking:
+      print '  {} {}'.format(blocking_request.fingerprint, blocking_request.url)
+
+if __name__ == '__main__':
+  import sys
+  _Main(sys.argv[1])
diff --git a/loading/queuing_lens_unittest.py b/loading/queuing_lens_unittest.py
new file mode 100644
index 0000000..2b0a8d7
--- /dev/null
+++ b/loading/queuing_lens_unittest.py
@@ -0,0 +1,114 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from queuing_lens import QueuingLens
+import request_track
+import test_utils
+
+class QueuingLensTestCase(unittest.TestCase):
+  MILLIS_TO_MICROS = 1000
+  MILLIS_TO_SECONDS = 0.001
+  URL_1 = 'http://1'
+  URL_2 = 'http://2'
+
+  def testRequestQueuing(self):
+    # http://1: queued at 5ms, request start at 10ms, ready at 11ms,
+    #           done at 12ms; blocked by 2.
+    # http://2: queued at 4ms, request start at 4ms, ready at 5 ms, done at 9ms.
+    trace_events = [
+        {'args': {
+            'data': {
+                'request_url': self.URL_1,
+                'source_id': 1
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': 1,
+         'name': QueuingLens.ASYNC_NAME,
+         'ph': 'b',
+         'ts': 5 * self.MILLIS_TO_MICROS
+        },
+        {'args': {
+            'data': {
+                'source_id': 1
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': 1,
+         'name': QueuingLens.READY_NAME,
+         'ph': 'n',
+         'ts': 10 * self.MILLIS_TO_MICROS
+        },
+        {'args': {
+            'data': {
+                'source_id': 1
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': 1,
+         'name': QueuingLens.ASYNC_NAME,
+         'ph': 'e',
+         'ts': 12 * self.MILLIS_TO_MICROS
+        },
+
+        {'args': {
+            'data': {
+                'request_url': self.URL_2,
+                'source_id': 2
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': 2,
+         'name': QueuingLens.ASYNC_NAME,
+         'ph': 'b',
+         'ts': 4 * self.MILLIS_TO_MICROS
+        },
+        {'args': {
+            'data': {
+                'source_id': 2
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': 2,
+         'name': QueuingLens.READY_NAME,
+         'ph': 'n',
+         'ts': 5 * self.MILLIS_TO_MICROS
+        },
+        {'args': {
+            'data': {
+                'source_id': 2
+            }
+          },
+         'cat': QueuingLens.QUEUING_CATEGORY,
+         'id': 2,
+         'name': QueuingLens.ASYNC_NAME,
+         'ph': 'e',
+         'ts': 9 * self.MILLIS_TO_MICROS
+        }]
+    requests = [
+        request_track.Request.FromJsonDict(
+            {'url': self.URL_1,
+             'request_id': '0.1',
+             'timing': {'request_time': 10 * self.MILLIS_TO_SECONDS,
+                        'loading_finished': 2}}),
+        request_track.Request.FromJsonDict(
+            {'url': self.URL_2,
+             'request_id': '0.2',
+             'timing': {'request_time': 4 * self.MILLIS_TO_SECONDS,
+                        'loading_finished': 5}})]
+    trace = test_utils.LoadingTraceFromEvents(
+        requests=requests, trace_events=trace_events)
+    queue_info = QueuingLens(trace).GenerateRequestQueuing()
+    self.assertEqual(set(['0.2']),
+                     set(rq.request_id
+                         for rq in queue_info[requests[0]].blocking))
+    self.assertEqual((5., 10., 12.), (queue_info[requests[0]].start_msec,
+                                      queue_info[requests[0]].ready_msec,
+                                      queue_info[requests[0]].end_msec))
+    self.assertEqual(0, len(queue_info[requests[1]].blocking))
+    self.assertEqual((4., 5., 9.), (queue_info[requests[1]].start_msec,
+                                    queue_info[requests[1]].ready_msec,
+                                    queue_info[requests[1]].end_msec))
diff --git a/loading/tracing.py b/loading/tracing.py
index b04787c..9a03e22 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -13,9 +13,11 @@ import devtools_monitor
 
 
 _DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
+QUEUING_CATEGORY = 'disabled-by-default-loading.resource'
 INITIAL_CATEGORIES = (
     ('toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
      'blink.user_timing', 'blink.net', 'disabled-by-default-blink.debug.layout')
+    + (QUEUING_CATEGORY,)
     + tuple('-' + cat for cat in _DISABLED_CATEGORIES))
 
 

commit f1fb885db058708c293dc17ea03324bbc7db3bfa
Author: droger <droger@chromium.org>
Date:   Mon May 30 00:47:52 2016 -0700

    tools/android/loading Allow changing the output directory
    
    Task output is now generated in a unique directory, avoiding
    overriding the results from previous runs.
    
    The name of the unique directory is composed of the name of the
    user, a unique ID, and optionally a task name.
    
    The 'report' task now needs an extra parameter, since the trace
    directory is no longer hardcoded.
    
    Review-Url: https://codereview.chromium.org/2008033003
    Cr-Original-Commit-Position: refs/heads/master@{#396692}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 06e4163835bf23a8d5eb7d9ddce33024dec98316

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 3df4f60..3376457 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -60,9 +60,10 @@ gcloud compute instances create $INSTANCE_NAME \
  --image ubuntu-14-04 \
  --zone europe-west1-c \
  --scopes cloud-platform,https://www.googleapis.com/auth/cloud-taskqueue \
- --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,taskqueue-tag=some_tag \
+ --metadata \
+    cloud-storage-path=$CLOUD_STORAGE_PATH,task-dir=dir,taskqueue-tag=tag \
  --metadata-from-file \
-     startup-script=$CHROMIUM_SRC/tools/android/loading/cloud/backend/startup-script.sh
+    startup-script=$CHROMIUM_SRC/tools/android/loading/cloud/backend/startup-script.sh
 ```
 
 If you are debbugging, you probably want to set additional metadata:
@@ -98,8 +99,8 @@ gcloud compute instances get-serial-port-output $INSTANCE_NAME
 dictionary with the keys:
 
 -   `project_name` (string): Name of the Google Cloud project
--   `cloud_storage_path` (string): Path in Google Storage where generated traces
-    will be stored.
+-   `task_storage_path` (string): Path in Google Storage where task output is
+    generated.
 -   `binaries_path` (string): Path to the executables (Containing chrome).
 -   `src_path` (string): Path to the Chromium source directory.
 -   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
@@ -108,7 +109,7 @@ dictionary with the keys:
      and tracking filtering rules.
 -   `instance_name` (string, optional): Name of the Compute Engine instance this
     script is running on.
--   `worker_log_file` (string, optional): Path to the log file capturing the
+-   `worker_log_path` (string, optional): Path to the log file capturing the
     output of `worker.py`, to be uploaded to Cloud Storage.
 -   `self_destruct` (boolean, optional): Whether the worker will destroy the
     Compute Engine instance when there are no remaining tasks to process. This
diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index 92499e8..6e05ee9 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -31,11 +31,10 @@ class ClovisTaskHandler(object):
       instance_name(str, optional): Name of the ComputeEngine instance.
     """
     self._failure_database = failure_database
-    trace_path = os.path.join(base_path, 'trace')
     self._handlers = {
         'trace': TraceTaskHandler(
-            trace_path, failure_database, google_storage_accessor,
-            binaries_path, logger, instance_name),
+            base_path, failure_database, google_storage_accessor, binaries_path,
+            logger, instance_name),
         'report': ReportTaskHandler(
             project_name, failure_database, google_storage_accessor,
             bigquery_service, logger, ad_rules_filename,
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index c74e56e..9be41d2 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -78,6 +78,8 @@ chown -R pythonapp:pythonapp /opt/app
 # Create the configuration file for this deployment.
 DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
 TASKQUEUE_TAG=`get_instance_metadata taskqueue-tag`
+TASK_DIR=`get_instance_metadata task-dir`
+TASK_STORAGE_PATH=$CLOUD_STORAGE_PATH/$TASK_DIR
 if [ "$(get_instance_metadata self-destruct)" == "false" ]; then
   SELF_DESTRUCT="False"
 else
@@ -89,7 +91,7 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
   "instance_name" : "$INSTANCE_NAME",
   "project_name" : "$PROJECTID",
-  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
+  "task_storage_path" : "$TASK_STORAGE_PATH",
   "binaries_path" : "/opt/app/clovis/binaries",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index bc25bf9..bf96347 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -42,9 +42,9 @@ class Worker(object):
     if self._self_destruct and not self._instance_name:
       self._logger.error('Self destruction requires an instance name.')
 
-    # Separate the cloud storage path into the bucket and the base path under
+    # Separate the task storage path into the bucket and the base path under
     # the bucket.
-    storage_path_components = config['cloud_storage_path'].split('/')
+    storage_path_components = config['task_storage_path'].split('/')
     self._bucket_name = storage_path_components[0]
     self._base_path_in_bucket = ''
     if len(storage_path_components) > 1:
diff --git a/loading/cloud/common/clovis_paths.py b/loading/cloud/common/clovis_paths.py
index 03c1b33..44309bc 100644
--- a/loading/cloud/common/clovis_paths.py
+++ b/loading/cloud/common/clovis_paths.py
@@ -15,8 +15,6 @@ BIGQUERY_TABLE_TEMPLATE = 'report'
 
 # Prefix for the loading trace database files.
 TRACE_DATABASE_PREFIX = 'trace_database'
-# Name of the directory where traces are located.
-TRACE_DIR = 'trace'
 
 
 def GetBigQueryTableID(tag):
diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
index cf470e7..739bb9c 100644
--- a/loading/cloud/common/clovis_task.py
+++ b/loading/cloud/common/clovis_task.py
@@ -39,14 +39,15 @@ class ClovisTask(object):
     try:
       data = json.loads(json_dict)
       action = data['action']
+      action_params = data['action_params']
       # Vaidate the format.
       if action == 'trace':
-        action_params = data['action_params']
         urls = action_params['urls']
         if (type(urls) is not list) or (len(urls) == 0):
           return None
       elif action == 'report':
-        action_params = data.get('action_params')
+        if not action_params.get('trace_bucket'):
+          return None
       else:
         # When more actions are supported, check that they are valid here.
         return None
diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 02c4121..8ff353b 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -58,9 +58,17 @@ class GoogleInstanceHelper(object):
     """Returns the name of the instance group associated with tag."""
     return 'group-' + tag
 
-  def CreateTemplate(self, tag, bucket):
-    """Creates an instance template for instances identified by tag and using
-    bucket for deployment. Returns True if successful.
+  def CreateTemplate(self, tag, bucket, task_dir):
+    """Creates an instance template for instances identified by tag.
+
+    Args:
+      tag: (string) Tag associated to a task.
+      bucket: (string) Root bucket where the deployment is located.
+      task_dir: (string) Subdirectory of |bucket| where task data is read and
+                         written.
+
+    Returns:
+      boolean: True if successful.
     """
     image_url = self._COMPUTE_API_ROOT + \
                 'ubuntu-os-cloud/global/images/ubuntu-1404-trusty-v20160406'
@@ -93,6 +101,8 @@ class GoogleInstanceHelper(object):
             'metadata': { 'items': [
                 {'key': 'cloud-storage-path',
                  'value': bucket},
+                {'key': 'task-dir',
+                 'value': task_dir},
                 {'key': 'startup-script-url',
                  'value': 'gs://%s/deployment/startup-script.sh' % bucket},
                 {'key': 'taskqueue-tag', 'value': tag}]}}}
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index 0b564e1..f5fa5f7 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -21,6 +21,8 @@ following keys:
 -   `storage_bucket` (string): Name of the storage bucket used by the backend
     instances. Backend code and data must have been previously deployed to this
     bucket using the `deploy.sh` [script][4].
+-   `task_name` (string, opitonal): Name of the task, used to build the name of
+    the output directory.
 -   `tag` (string, optional): tag internally used to associate tasks to backend
     ComputeEngine instances. This parameter should not be set in general, as it
     is mostly exposed for development purposes. If this parameter is not
@@ -42,10 +44,11 @@ running Chrome.
 
 ### Parameters for the `report` action
 
-Finds all the traces in the bucket (specified in the backend parameters) and
-generates a report in BigQuery.
+Finds all the traces in the specified bucket and generates a report in BigQuery.
 
-This action has no parameters.
+- `trace_bucket` (string): Name of the storage bucket where trace databases can
+  be found. It can be either absolute or relative to the `storage_bucket`
+  specified in the backend parameters.
 
 This requires an existing `clovis_dataset.report` BigQuery table that will be
 used as a template. The schema of this template is not updated automatically and
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 2a23aa1..3303b5b 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -98,7 +98,7 @@ def Finalize(tag, email_address, status, task_url):
   deferred.defer(DeleteInstanceGroup, tag)
 
 
-def CreateInstanceTemplate(task):
+def CreateInstanceTemplate(task, task_dir):
   """Create the Compute Engine instance template that will be used to create the
   instances.
   """
@@ -111,7 +111,8 @@ def CreateInstanceTemplate(task):
   if not bucket:
     clovis_logger.error('Missing bucket in backend_params.')
     return False
-  return instance_helper.CreateTemplate(task.BackendParams()['tag'], bucket)
+  return instance_helper.CreateTemplate(task.BackendParams()['tag'], bucket,
+                                        task_dir)
 
 
 def CreateInstances(task):
@@ -163,13 +164,19 @@ def SplitClovisTask(task):
   """
   # For report task, need to find the traces first.
   if task.Action() == 'report':
-    bucket = task.BackendParams().get('storage_bucket')
-    if not bucket:
-      clovis_logger.error('Missing storage bucket for report task.')
+    trace_bucket = task.ActionParams().get('trace_bucket')
+    if not trace_bucket:
+      clovis_logger.error('Missing trace bucket for report task.')
       return None
-    traces = GetTracePaths(bucket)
+
+    # Allow passing the trace bucket as absolute or relative to the base bucket.
+    base_bucket = task.BackendParams().get('storage_bucket', '')
+    if not trace_bucket.startswith(base_bucket):
+      trace_bucket = os.path.join(base_bucket, trace_bucket)
+
+    traces = GetTracePaths(trace_bucket)
     if not traces:
-      clovis_logger.error('No traces found in bucket: ' + bucket)
+      clovis_logger.error('No traces found in bucket: ' + trace_bucket)
       return None
     task.ActionParams()['traces'] = traces
 
@@ -202,7 +209,7 @@ def GetTracePaths(bucket):
 
   This function assumes a specific structure for the files in the bucket. These
   assumptions must match the behavior of the backend:
-  - The trace databases are located under the TRACE_DIR directory in the bucket.
+  - The trace databases are located in the bucket.
   - The trace databases files are the only objects with the
     TRACE_DATABASE_PREFIX prefix in their name.
 
@@ -210,8 +217,7 @@ def GetTracePaths(bucket):
     list: The list of paths to traces, as strings.
   """
   traces = []
-  prefix = os.path.join('/', bucket, common.clovis_paths.TRACE_DIR,
-                        common.clovis_paths.TRACE_DATABASE_PREFIX)
+  prefix = os.path.join('/', bucket, common.clovis_paths.TRACE_DATABASE_PREFIX)
   file_stats = cloudstorage.listbucket(prefix)
 
   for file_stat in file_stats:
@@ -250,9 +256,20 @@ def StartFromJsonString(http_body_str):
   task_tag = task.BackendParams()['tag']
   clovis_logger.info('Start processing %s task with tag %s.' % (task.Action(),
                                                                 task_tag))
+  # Compute the task directory.
+  task_dir_components = []
+  user_email = email_helper.GetUserEmail()
+  user_name = user_email[:user_email.find('@')]
+  if user_name:
+    task_dir_components.append(user_name)
+  task_name = task.BackendParams().get('task_name')
+  if task_name:
+    task_dir_components.append(task_name)
+  task_dir_components.append(task_tag)
+  task_dir = os.path.join(task.Action(), '_'.join(task_dir_components))
 
   # Create the instance template if required.
-  if not CreateInstanceTemplate(task):
+  if not CreateInstanceTemplate(task, task_dir):
     return Render('Template creation failed.', memory_logs)
 
   # Build the URL where the result will live.
@@ -260,7 +277,8 @@ def StartFromJsonString(http_body_str):
   if task.Action() == 'trace':
     bucket = task.BackendParams().get('storage_bucket')
     if bucket:
-      task_url = 'https://console.cloud.google.com/storage/' + bucket
+      task_url = 'https://console.cloud.google.com/storage/%s/%s' % (bucket,
+                                                                     task_dir)
   elif task.Action() == 'report':
     task_url = common.clovis_paths.GetBigQueryTableURL(project_name, task_tag)
   else:
@@ -285,7 +303,6 @@ def StartFromJsonString(http_body_str):
   clovis_logger.info('Creating worker polling task.')
   first_poll_delay_minutes = 10
   timeout_hours = task.BackendParams().get('timeout_hours', 5)
-  user_email = email_helper.GetUserEmail()
   deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours, user_email,
                  task_url, _countdown=(60 * first_poll_delay_minutes))
 

commit 8400dc6dbfcee5cb133d56c6fee5ee5f5377b93d
Author: blundell <blundell@chromium.org>
Date:   Fri May 27 08:45:07 2016 -0700

    tools/android/loading: Track requests per UserLens in report.py
    
    This change is not backwards compatible is it changes the name of
    |request_count| to |requests| for consistency.
    
    Review-Url: https://codereview.chromium.org/2020643002
    Cr-Original-Commit-Position: refs/heads/master@{#396474}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9740722c3a076d9cf1e99f09d27a90b3fbcb33ea

diff --git a/loading/report.py b/loading/report.py
index d489eaf..21ca1e8 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -51,6 +51,7 @@ class PerUserLensReport(object):
 
     self._byte_frac = self._GenerateByteFrac(network_lens)
 
+    self._requests = len(user_lens.CriticalRequests())
     self._preloaded_requests = len(set(preloaded_requests) &
                                    set(user_lens.CriticalRequests()))
 
@@ -63,6 +64,7 @@ class PerUserLensReport(object):
 
     report['ms'] = self._satisfied_msec - self._navigation_start_msec
     report['byte_frac'] = self._byte_frac
+    report['requests'] = self._requests
     report['preloaded_requests'] = self._preloaded_requests
 
     # Take the first (earliest) inversion.
@@ -103,6 +105,7 @@ class LoadingReport(object):
     preloaded_requests = \
        prefetch_view.PrefetchSimulationView.PreloadedRequests(
            requests[0], dependencies_lens, self.trace)
+    self._requests = len(requests)
     self._preloaded_requests = len(preloaded_requests)
 
     self._user_lens_reports = {}
@@ -139,6 +142,7 @@ class LoadingReport(object):
     report = {
         'url': self.trace.url,
         'plt_ms': self._load_end_msec - self._navigation_start_msec,
+        'requests': self._requests,
         'preloaded_requests': self._preloaded_requests,
         'transfer_size': self._transfer_size}
 
@@ -164,7 +168,6 @@ class LoadingReport(object):
     requests = trace.request_track.GetEvents()
     has_rules = has_ad_rules or has_tracking_rules
     result = {
-        'request_count': len(requests),
         'ad_requests': 0 if has_ad_rules else None,
         'tracking_requests': 0 if has_tracking_rules else None,
         'ad_or_tracking_requests': 0 if has_rules else None,
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 4299ed4..5be6501 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -104,6 +104,10 @@ class LoadingReportTestCase(unittest.TestCase):
                            loading_report['plt_ms'])
     self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'], 2)
     self.assertAlmostEqual(0.1844, loading_report['significant_byte_frac'], 2)
+    self.assertEqual(2, loading_report['requests'])
+    self.assertEqual(1, loading_report['first_text_requests'])
+    self.assertEqual(1, loading_report['contentful_requests'])
+    self.assertEqual(1, loading_report['significant_requests'])
     self.assertEqual(1, loading_report['preloaded_requests'])
     self.assertEqual(1, loading_report['first_text_preloaded_requests'])
     self.assertEqual(1, loading_report['contentful_preloaded_requests'])

commit 5317af0d3a91117453da6646c79a41b8517c69b0
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 27 06:42:55 2016 -0700

    tools/android/loading: Fixes LocalChromeController when using old psutil
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2015203002
    Cr-Original-Commit-Position: refs/heads/master@{#396455}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f7b11f583ecf4833c2af374f819ae99ca01be293

diff --git a/loading/controller.py b/loading/controller.py
index 00e5c8b..279d0ba 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -412,7 +412,7 @@ class LocalChromeController(ChromeControllerBase):
     self._using_temp_profile_dir = self._profile_dir is None
     if self._using_temp_profile_dir:
       self._profile_dir = tempfile.mkdtemp(suffix='.profile')
-    self._chrome_env_override = None
+    self._chrome_env_override = {}
     self._metadata['platform'] = {
         'os': platform.system()[0] + '-' + platform.release(),
         'product_model': 'unknown'
@@ -432,7 +432,14 @@ class LocalChromeController(ChromeControllerBase):
     chrome_path = OPTIONS.LocalBinary('chrome')
     for process in psutil.process_iter():
       try:
-        if process.exe() == chrome_path:
+        process_bin_path = None
+        # In old versions of psutil, process.exe is a member, in newer ones it's
+        # a method.
+        if type(process.exe) == str:
+          process_bin_path = process.exe
+        else:
+          process_bin_path = process.exe()
+        if os.path.abspath(process_bin_path) == os.path.abspath(chrome_path):
           process.terminate()
           killed_count += 1
           try:
@@ -475,7 +482,7 @@ class LocalChromeController(ChromeControllerBase):
         tempfile.NamedTemporaryFile(prefix="chrome_controller_", suffix='.log')
     chrome_process = None
     try:
-      chrome_env_override = self._chrome_env_override.copy() or {}
+      chrome_env_override = self._chrome_env_override.copy()
       if self._wpr_attributes:
         chrome_env_override.update(self._wpr_attributes.chrome_env_override)
 

commit 423a813335d406505315bcd580a2c35fb1e8a469
Author: blundell <blundell@chromium.org>
Date:   Fri May 27 06:40:44 2016 -0700

    tools/android/loading: Consolidate code tracking per-user-lens metrics
    
    This CL refactors report.py in order to abstract the notion of tracking
    a bunch of metrics for a variety of different user lenses (e.g., first
    text/first contentful paint/first significant paint). It introduces
    a new PerUserLensReport class that takes in a user lens instance and
    tracks a variety of data for that user lens. LoadingReport is changed to
    use one instance of this class per desired user lens rather than
    computing each individual metric manually per user lens.
    
    The intent is to simplify the addition of either future user lenses or
    future per-user-lens metrics: Each of those additions can now occur at
    just one place in the code.
    
    This change is not backwards compatible, as it changes the names of some
    of the data generated in a loading report.
    
    Review-Url: https://codereview.chromium.org/2016333002
    Cr-Original-Commit-Position: refs/heads/master@{#396454}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f0066d5542c87bc9d7a0fbefbed54a19a4f0ebdf

diff --git a/loading/report.py b/loading/report.py
index 6830170..d489eaf 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -19,6 +19,66 @@ from user_satisfied_lens import (
     FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
 
 
+def _ComputeCpuBusyness(activity, load_start, satisfied_end):
+  """Generates a breakdown of CPU activity between |load_start| and
+  |satisfied_end|."""
+  duration = float(satisfied_end - load_start)
+  result = {
+      'activity_frac': (
+          activity.MainRendererThreadBusyness(load_start, satisfied_end)
+          / duration),
+  }
+
+  activity_breakdown = activity.ComputeActivity(load_start, satisfied_end)
+  result['parsing_frac'] = (
+      sum(activity_breakdown['parsing'].values()) / duration)
+  result['script_frac'] = (
+      sum(activity_breakdown['script'].values()) / duration)
+  return result
+
+
+class PerUserLensReport(object):
+  """Generates a variety of metrics relative to a passed in user lens."""
+
+  def __init__(self, trace, user_lens, activity_lens, network_lens,
+               navigation_start_msec, preloaded_requests):
+    self._navigation_start_msec = navigation_start_msec
+
+    self._satisfied_msec = user_lens.SatisfiedMs()
+
+    graph = LoadingGraphView.FromTrace(trace)
+    self._inversions = graph.GetInversionsAtTime(self._satisfied_msec)
+
+    self._byte_frac = self._GenerateByteFrac(network_lens)
+
+    self._preloaded_requests = len(set(preloaded_requests) &
+                                   set(user_lens.CriticalRequests()))
+
+    self._cpu_busyness = _ComputeCpuBusyness(activity_lens,
+                                             navigation_start_msec,
+                                             self._satisfied_msec)
+
+  def GenerateReport(self):
+    report = {}
+
+    report['ms'] = self._satisfied_msec - self._navigation_start_msec
+    report['byte_frac'] = self._byte_frac
+    report['preloaded_requests'] = self._preloaded_requests
+
+    # Take the first (earliest) inversion.
+    report['inversion'] = self._inversions[0].url if self._inversions else None
+
+    report.update(self._cpu_busyness)
+    return report
+
+  def _GenerateByteFrac(self, network_lens):
+    if not network_lens.total_download_bytes:
+      return float('Nan')
+    byte_frac = (network_lens.DownloadedBytesAt(self._satisfied_msec)
+          / float(network_lens.total_download_bytes))
+    return byte_frac
+
+
 class LoadingReport(object):
   """Generates a loading report from a loading trace."""
   def __init__(self, trace, ad_rules=None, tracking_rules=None):
@@ -31,55 +91,38 @@ class LoadingReport(object):
     """
     self.trace = trace
 
-    first_text_paint_lens = FirstTextPaintLens(self.trace)
-    first_contentful_paint_lens = FirstContentfulPaintLens(self.trace)
-    first_significant_paint_lens = FirstSignificantPaintLens(self.trace)
-
-    self._text_msec = first_text_paint_lens.SatisfiedMs()
-    self._contentful_paint_msec = first_contentful_paint_lens.SatisfiedMs()
-    self._significant_paint_msec = first_significant_paint_lens.SatisfiedMs()
-
     navigation_start_events = trace.tracing_track.GetMatchingEvents(
         'blink.user_timing', 'navigationStart')
     self._navigation_start_msec = min(
         e.start_msec for e in navigation_start_events)
     self._load_end_msec = self._ComputePlt(trace)
 
-    network_lens = NetworkActivityLens(self.trace)
-    if network_lens.total_download_bytes > 0:
-      self._contentful_byte_frac = (
-          network_lens.DownloadedBytesAt(self._contentful_paint_msec)
-          / float(network_lens.total_download_bytes))
-      self._significant_byte_frac = (
-          network_lens.DownloadedBytesAt(self._significant_paint_msec)
-          / float(network_lens.total_download_bytes))
-    else:
-      self._contentful_byte_frac = float('Nan')
-      self._significant_byte_frac = float('Nan')
-
-    graph = LoadingGraphView.FromTrace(trace)
-    self._contentful_inversion = graph.GetInversionsAtTime(
-        self._contentful_paint_msec)
-    self._significant_inversion = graph.GetInversionsAtTime(
-        self._significant_paint_msec)
-    self._transfer_size = metrics.TotalTransferSize(trace)[1]
-
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
     requests = self.trace.request_track.GetEvents()
     preloaded_requests = \
        prefetch_view.PrefetchSimulationView.PreloadedRequests(
            requests[0], dependencies_lens, self.trace)
-    self._first_text_preloaded_requests = LoadingReport._ComputePreloadInfo(
-        preloaded_requests, first_text_paint_lens)
-    self._contentful_preloaded_requests = LoadingReport._ComputePreloadInfo(
-        preloaded_requests, first_contentful_paint_lens)
-    self._significant_preloaded_requests = LoadingReport._ComputePreloadInfo(
-        preloaded_requests, first_significant_paint_lens)
     self._preloaded_requests = len(preloaded_requests)
 
+    self._user_lens_reports = {}
+    first_text_paint_lens = FirstTextPaintLens(self.trace)
+    first_contentful_paint_lens = FirstContentfulPaintLens(self.trace)
+    first_significant_paint_lens = FirstSignificantPaintLens(self.trace)
     activity = ActivityLens(trace)
-    self._cpu_busyness = self._ComputeCpuBusyness(activity)
+    network_lens = NetworkActivityLens(self.trace)
+    for key, user_lens in [['first_text', first_text_paint_lens],
+                           ['contentful', first_contentful_paint_lens],
+                           ['significant', first_significant_paint_lens]]:
+      self._user_lens_reports[key] = PerUserLensReport(self.trace,
+          user_lens, activity, network_lens, self._navigation_start_msec,
+          preloaded_requests)
+
+    self._transfer_size = metrics.TotalTransferSize(trace)[1]
+
+    self._cpu_busyness = _ComputeCpuBusyness(activity,
+                                             self._navigation_start_msec,
+                                             self._load_end_msec)
 
     content_lens = ContentClassificationLens(
         trace, ad_rules or [], tracking_rules or [])
@@ -95,32 +138,16 @@ class LoadingReport(object):
     """Returns a report as a dict."""
     report = {
         'url': self.trace.url,
-        'first_text_ms': self._text_msec - self._navigation_start_msec,
-        'contentful_paint_ms': (self._contentful_paint_msec
-                                - self._navigation_start_msec),
-        'significant_paint_ms': (self._significant_paint_msec
-                                 - self._navigation_start_msec),
         'plt_ms': self._load_end_msec - self._navigation_start_msec,
-        'contentful_byte_frac': self._contentful_byte_frac,
-        'significant_byte_frac': self._significant_byte_frac,
         'preloaded_requests': self._preloaded_requests,
-        'first_text_preloaded_requests':
-            self._first_text_preloaded_requests,
-        'contentful_preloaded_requests':
-            self._contentful_preloaded_requests,
-        'significant_preloaded_requests':
-            self._significant_preloaded_requests,
-
-        # Take the first (earliest) inversions.
-        'contentful_inversion': (self._contentful_inversion[0].url
-                                 if self._contentful_inversion
-                                 else None),
-        'significant_inversion': (self._significant_inversion[0].url
-                                  if self._significant_inversion
-                                  else None),
         'transfer_size': self._transfer_size}
-    report.update(self._ad_report)
+
+    for user_lens_type, user_lens_report in self._user_lens_reports.iteritems():
+      for key, value in user_lens_report.GenerateReport().iteritems():
+        report[user_lens_type + '_' + key] = value
+
     report.update(self._cpu_busyness)
+    report.update(self._ad_report)
     report.update(self._ads_cost)
     return report
 
@@ -132,12 +159,6 @@ class LoadingReport(object):
     return LoadingReport(trace, ad_rules_filename, tracking_rules_filename)
 
   @classmethod
-  def _ComputePreloadInfo(cls, preloaded_requests, user_lens):
-    preloaded_critical_requests = [r for r in preloaded_requests
-          if r in user_lens.CriticalRequests()]
-    return len(preloaded_critical_requests)
-
-  @classmethod
   def _AdRequestsReport(
       cls, trace, content_lens, has_ad_rules, has_tracking_rules):
     requests = trace.request_track.GetEvents()
@@ -178,45 +199,6 @@ class LoadingReport(object):
     # request.
     return max(r.end_msec or -1 for r in trace.request_track.GetEvents())
 
-  def _ComputeCpuBusyness(self, activity):
-    load_start = self._navigation_start_msec
-    load_end = self._load_end_msec
-    contentful = self._contentful_paint_msec
-    significant = self._significant_paint_msec
-
-    load_time = float(load_end - load_start)
-    contentful_time = float(contentful - load_start)
-    significant_time = float(significant - load_start)
-
-    result = {
-        'activity_load_frac': (
-            activity.MainRendererThreadBusyness(load_start, load_end)
-            / load_time),
-        'activity_contentful_paint_frac': (
-            activity.MainRendererThreadBusyness(load_start, contentful)
-            / contentful_time),
-        'activity_significant_paint_frac': (
-            activity.MainRendererThreadBusyness(load_start, significant)
-            / significant_time)}
-
-    activity_load = activity.ComputeActivity(load_start, load_end)
-    activity_contentful = activity.ComputeActivity(load_start, contentful)
-    activity_significant = activity.ComputeActivity(load_start, significant)
-
-    result['parsing_load_frac'] = (
-        sum(activity_load['parsing'].values()) / load_time)
-    result['script_load_frac'] = (
-        sum(activity_load['script'].values()) / load_time)
-    result['parsing_contentful_frac'] = (
-        sum(activity_contentful['parsing'].values()) / contentful_time)
-    result['script_contentful_frac'] = (
-        sum(activity_contentful['script'].values()) / contentful_time)
-    result['parsing_significant_frac'] = (
-        sum(activity_significant['parsing'].values()) / significant_time)
-    result['script_significant_frac'] = (
-        sum(activity_significant['script'].values()) / significant_time)
-    return result
-
   @classmethod
   def _AdsAndTrackingCpuCost(
       cls, start_msec, end_msec, content_lens, activity, has_rules):
@@ -254,10 +236,15 @@ class LoadingReport(object):
 
 
 def _Main(args):
-  assert len(args) == 4, 'Usage: report.py trace.json ad_rules tracking_rules'
+  if len(args) not in (2, 4):
+    print 'Usage: report.py trace.json (ad_rules tracking_rules)'
+    sys.exit(1)
   trace_filename = args[1]
-  ad_rules = open(args[2]).readlines()
-  tracking_rules = open(args[3]).readlines()
+  ad_rules = None
+  tracking_rules = None
+  if len(args) == 4:
+    ad_rules = open(args[2]).readlines()
+    tracking_rules = open(args[3]).readlines()
   report = LoadingReport.FromTraceFilename(
       trace_filename, ad_rules, tracking_rules)
   print json.dumps(report.GenerateReport(), indent=2, sort_keys=True)
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 5774660..4299ed4 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -97,9 +97,9 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertEqual(self._TEXT_PAINT - self._NAVIGATION_START_TIME,
                      loading_report['first_text_ms'])
     self.assertEqual(self._SIGNIFICANT_PAINT - self._NAVIGATION_START_TIME,
-                     loading_report['significant_paint_ms'])
+                     loading_report['significant_ms'])
     self.assertEqual(self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME,
-                     loading_report['contentful_paint_ms'])
+                     loading_report['contentful_ms'])
     self.assertAlmostEqual(self._LOAD_END_TIME - self._NAVIGATION_START_TIME,
                            loading_report['plt_ms'])
     self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'], 2)
@@ -162,15 +162,15 @@ class LoadingReportTestCase(unittest.TestCase):
   def testThreadBusyness(self):
     loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
     self.assertAlmostEqual(
-        1., loading_report['activity_significant_paint_frac'])
+        1., loading_report['significant_activity_frac'])
     self.assertAlmostEqual(
         float(self._TOPLEVEL_EVENT_DURATION - self._TOPLEVEL_EVENT_OFFSET)
         / (self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME),
-        loading_report['activity_contentful_paint_frac'])
+        loading_report['contentful_activity_frac'])
     self.assertAlmostEqual(
         float(self._TOPLEVEL_EVENT_DURATION - self._TOPLEVEL_EVENT_OFFSET)
         / (self._LOAD_END_TIME - self._NAVIGATION_START_TIME),
-        loading_report['activity_load_frac'])
+        loading_report['activity_frac'])
 
   def testActivityBreakdown(self):
     loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
@@ -179,20 +179,20 @@ class LoadingReportTestCase(unittest.TestCase):
         self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME)
 
     self.assertAlmostEqual(self._SCRIPT_EVENT_DURATION / load_time,
-                           loading_report['script_load_frac'])
+                           loading_report['script_frac'])
     self.assertAlmostEqual(
         (self._PARSING_EVENT_DURATION - self._SCRIPT_EVENT_DURATION)
         / load_time,
-        loading_report['parsing_load_frac'])
+        loading_report['parsing_frac'])
 
-    self.assertAlmostEqual(1., loading_report['script_significant_frac'])
-    self.assertAlmostEqual(0., loading_report['parsing_significant_frac'])
+    self.assertAlmostEqual(1., loading_report['significant_script_frac'])
+    self.assertAlmostEqual(0., loading_report['significant_parsing_frac'])
 
     self.assertAlmostEqual(self._SCRIPT_EVENT_DURATION / contentful_time,
-                           loading_report['script_contentful_frac'])
+                           loading_report['contentful_script_frac'])
     self.assertAlmostEqual(
         (self._PARSING_EVENT_DURATION - self._SCRIPT_EVENT_DURATION)
-        / contentful_time, loading_report['parsing_contentful_frac'])
+        / contentful_time, loading_report['contentful_parsing_frac'])
 
   def testAdsAndTrackingCost(self):
     load_time = float(self._LOAD_END_TIME - self._NAVIGATION_START_TIME)

commit 2b1aa322ed1b83e75c381b97849fe4f5da5f09f3
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 27 06:11:11 2016 -0700

    sandwich: Implement collect subcommand
    
    Sandwich output a metrics of every benchmarks in separate CSV files.
    This CL implement the collect subcommand that browse a Sandwich
    output directory and merge found CSV into one.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2019663003
    Cr-Original-Commit-Position: refs/heads/master@{#396451}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ce475fefc0e52c40b9d7add6357af9f27ce75492

diff --git a/loading/csv_util.py b/loading/csv_util.py
new file mode 100644
index 0000000..e27d74a
--- /dev/null
+++ b/loading/csv_util.py
@@ -0,0 +1,52 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import csv
+import logging
+import os
+
+
+def CollectCSVsFromDirectory(directory_path, file_output):
+  """Collects recursively all .csv files from directory into one.
+
+  Note: The list of CSV columns must be identical across all files.
+
+  Args:
+    directory_path: Path of the directory to collect from.
+    file_output: File-like object to dump the CSV to.
+  """
+  # List CSVs.
+  csv_list = []
+  for root, _, files in os.walk(directory_path):
+    for file_name in files:
+      file_path = os.path.join(root, file_name)
+      if os.path.abspath(file_path) == os.path.abspath(file_output.name):
+        continue
+      if file_name.endswith('.csv'):
+        csv_list.append(os.path.join(root, file_name))
+  if not csv_list:
+    logging.error('No CSV files found in %s' % directory_path)
+    return False
+
+  # List rows.
+  csv_list.sort()
+  csv_field_names = None
+  csv_rows = []
+  for csv_file in csv_list:
+    logging.info('collecting %s' % csv_file)
+    with open(csv_file) as csvfile:
+      reader = csv.DictReader(csvfile)
+      if csv_field_names is None:
+        csv_field_names = reader.fieldnames
+      else:
+        assert reader.fieldnames == csv_field_names
+      for row in reader:
+        csv_rows.append(row)
+
+  # Export rows.
+  writer = csv.DictWriter(file_output, fieldnames=csv_field_names)
+  writer.writeheader()
+  for row in csv_rows:
+    writer.writerow(row)
+  return True
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 67332c1..31fb4a8 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -10,7 +10,6 @@ WepPageReplay serving all connections.
 """
 
 import argparse
-import csv
 import json
 import logging
 import os
@@ -30,6 +29,7 @@ import devil_chromium
 
 import chrome_cache
 import common_util
+import csv_util
 import device_setup
 import emulation
 import options
@@ -125,6 +125,15 @@ def _ArgumentParser():
   run_parser.add_argument('--swr-benchmark', action='store_true',
                           help='Run the Stale-While-Revalidate benchmarks.')
 
+  # Collect subcommand.
+  collect_csv_parser = subparsers.add_parser('collect-csv',
+      help='Collects all CSVs from Sandwich output directory into a single '
+           'CSV.')
+  collect_csv_parser.add_argument('output_dir', type=str,
+                                  help='Path to the run output directory.')
+  collect_csv_parser.add_argument('output_csv', type=argparse.FileType('w'),
+                                  help='Path to the output CSV.')
+
   return parser
 
 
@@ -232,6 +241,11 @@ def main(command_line_args):
     return _RecordWebServerTestTrace(args)
   if args.subcommand == 'run':
     return _RunAllMain(args)
+  if args.subcommand == 'collect-csv':
+    with args.output_csv as output_file:
+      if not csv_util.CollectCSVsFromDirectory(args.output_dir, output_file):
+        return 1
+    return 0
   assert False
 
 

commit 83c26df038d2ba5edb40825d075141f13fb49df4
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 27 05:42:34 2016 -0700

    sandwich: Make _ExtractDefaultMetrics() best effort
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2016703002
    Cr-Original-Commit-Position: refs/heads/master@{#396448}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1efcb31f07d8f2e18387b7ab78a8ee3048ad892c

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 610d594..0a6b167 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -162,21 +162,24 @@ def _ExtractDefaultMetrics(loading_trace):
   Returns:
     Dictionary with all trace extracted fields set.
   """
+  END_REQUEST_EVENTS = [
+      ('first_layout', 'requestStart', 'firstLayout'),
+      ('first_contentful_paint', 'requestStart', 'firstContentfulPaint'),
+      ('total_load', 'requestStart', 'loadEventEnd'),
+      ('js_onload_event', 'loadEventStart', 'loadEventEnd')]
   web_page_tracked_events = _GetWebPageTrackedEvents(
       loading_trace.tracing_track)
-  assert len(web_page_tracked_events) == len(_TRACKED_EVENT_NAMES)
-  request_start_time = web_page_tracked_events['requestStart'].start_msec
-  return {
-    'first_layout': (web_page_tracked_events['firstLayout'].start_msec -
-                     request_start_time),
-    'first_contentful_paint': (
-        web_page_tracked_events['firstContentfulPaint'].start_msec -
-        request_start_time),
-    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
-                   request_start_time),
-    'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
-                        web_page_tracked_events['loadEventStart'].start_msec)
-  }
+  metrics = {}
+  for metric_name, start_event_name, end_event_name in END_REQUEST_EVENTS:
+    try:
+      metrics[metric_name] = (
+          web_page_tracked_events[end_event_name].start_msec -
+          web_page_tracked_events[start_event_name].start_msec)
+    except KeyError as error:
+      logging.error('could not extract metric %s: missing trace event: %s' % (
+          metric_name, str(error)))
+      metrics[metric_name] = _FAILED_CSV_VALUE
+  return metrics
 
 
 def _ExtractMemoryMetrics(loading_trace):
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 641c7dc..12ed037 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -200,6 +200,19 @@ class PageTrackTest(unittest.TestCase):
     self.assertEquals(4, metrics['first_layout'])
     self.assertEquals(11, metrics['first_contentful_paint'])
 
+  def testExtractDefaultMetricsBestEffort(self):
+    metrics = puller._ExtractDefaultMetrics(LoadingTrace([
+        {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _UNLOAD},
+        {'ph': 'R', 'ts': 11000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _START}]))
+    self.assertEquals(4, len(metrics))
+    self.assertEquals(puller._FAILED_CSV_VALUE, metrics['total_load'])
+    self.assertEquals(puller._FAILED_CSV_VALUE, metrics['js_onload_event'])
+    self.assertEquals(puller._FAILED_CSV_VALUE, metrics['first_layout'])
+    self.assertEquals(puller._FAILED_CSV_VALUE,
+                      metrics['first_contentful_paint'])
+
   def testExtractMemoryMetrics(self):
     metrics = puller._ExtractMemoryMetrics(LoadingTrace(
         _MINIMALIST_TRACE_EVENTS))

commit e82f537052caec9e70d66541f83f25f7e8c72eb7
Author: gabadie <gabadie@chromium.org>
Date:   Thu May 26 10:19:26 2016 -0700

    tools/android/loading: Make CachingPolicy parse integers like chrome.
    
    Some servers sometimes send absolute crap in response header such
    as "Cache-Control:max-age=80000 private". Currently, the max-age's
    integer parsing fails in this case.
    
    This CL implements request_track._ParseStringToInt() that is the
    exact same implementation as in chrome and uses it to parse integers
    in response headers in request_track.CachingPolicy.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2011003002
    Cr-Original-Commit-Position: refs/heads/master@{#396213}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 656256577ce40be5376a1559b0f1e23367982a6d

diff --git a/loading/request_track.py b/loading/request_track.py
index 2743eed..6c3277d 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -16,6 +16,7 @@ import hashlib
 import json
 import logging
 import re
+import sys
 import urlparse
 
 import devtools_monitor
@@ -381,6 +382,26 @@ class Request(object):
     return json.dumps(self.ToJsonDict(), sort_keys=True, indent=2)
 
 
+def _ParseStringToInt(string):
+  """Parses a string to an integer like base::StringToInt64().
+
+  Returns:
+    Parsed integer.
+  """
+  string = string.strip()
+  while string:
+    try:
+      parsed_integer = int(string)
+      if parsed_integer > sys.maxint:
+        return sys.maxint
+      if parsed_integer < -sys.maxint - 1:
+        return -sys.maxint - 1
+      return parsed_integer
+    except ValueError:
+      string = string[:-1]
+  return 0
+
+
 class CachingPolicy(object):
   """Represents the caching policy at an arbitrary time for a cached response.
   """
@@ -452,11 +473,11 @@ class CachingPolicy(object):
         'Cache-Control', 'must-revalidate')
     swr_header = r.GetCacheControlDirective('stale-while-revalidate')
     if not must_revalidate and swr_header:
-      result[1] = int(swr_header)
+      result[1] = _ParseStringToInt(swr_header)
 
     max_age_header = r.GetCacheControlDirective('max-age')
     if max_age_header:
-      result[0] = int(max_age_header)
+      result[0] = _ParseStringToInt(max_age_header)
       return result
 
     date = self._GetDateValue('Date') or self._response_time
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index bac6218..336e46c 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -4,10 +4,11 @@
 
 import copy
 import json
+import sys
 import unittest
 
 from request_track import (TimeBetween, Request, CachingPolicy, RequestTrack,
-                           Timing)
+                           Timing, _ParseStringToInt)
 
 
 class TimeBetweenTestCase(unittest.TestCase):
@@ -84,6 +85,47 @@ class RequestTestCase(unittest.TestCase):
                       r.GetRawResponseHeaders())
 
 
+class ParseStringToIntTestCase(unittest.TestCase):
+  def runTest(self):
+    MININT = -sys.maxint - 1
+    # Same test cases as in string_number_conversions_unittest.cc
+    CASES = [
+        ("0", 0),
+        ("42", 42),
+        ("-2147483648", -2147483648),
+        ("2147483647", 2147483647),
+        ("-2147483649", -2147483649),
+        ("-99999999999", -99999999999),
+        ("2147483648", 2147483648),
+        ("99999999999", 99999999999),
+        ("9223372036854775807", sys.maxint),
+        ("-9223372036854775808", MININT),
+        ("09", 9),
+        ("-09", -9),
+        ("", 0),
+        (" 42", 42),
+        ("42 ", 42),
+        ("0x42", 0),
+        ("\t\n\v\f\r 42", 42),
+        ("blah42", 0),
+        ("42blah", 42),
+        ("blah42blah", 0),
+        ("-273.15", -273),
+        ("+98.6", 98),
+        ("--123", 0),
+        ("++123", 0),
+        ("-+123", 0),
+        ("+-123", 0),
+        ("-", 0),
+        ("-9223372036854775809", MININT),
+        ("-99999999999999999999", MININT),
+        ("9223372036854775808", sys.maxint),
+        ("99999999999999999999", sys.maxint)]
+    for string, expected_int in CASES:
+      parsed_int = _ParseStringToInt(string)
+      self.assertEquals(expected_int, parsed_int)
+
+
 class CachingPolicyTestCase(unittest.TestCase):
   _REQUEST = {
       'encoded_data_length': 14726,
@@ -132,7 +174,7 @@ class CachingPolicyTestCase(unittest.TestCase):
 
   def testPolicyMaxAge(self):
     r = self._MakeRequest()
-    r.response_headers['Cache-Control'] = 'whatever,max-age=1000,whatever'
+    r.response_headers['Cache-Control'] = 'whatever,max-age=  1000,whatever'
     self.assertEqual(
         CachingPolicy.VALIDATION_NONE,
         CachingPolicy(r).PolicyAtDate(r.wall_time))
@@ -149,7 +191,7 @@ class CachingPolicyTestCase(unittest.TestCase):
         CachingPolicy.VALIDATION_NONE,
         CachingPolicy(r).PolicyAtDate(r.wall_time))
     # Max-Age < age
-    r.response_headers['Cache-Control'] = 'whatever,max-age=100,whatever'
+    r.response_headers['Cache-Control'] = 'whatever,max-age=100crap,whatever'
     self.assertEqual(
         CachingPolicy.VALIDATION_SYNC,
         CachingPolicy(r).PolicyAtDate(r.wall_time + 2))

commit 48257dd167ea2b53bb01eef1bb23d575ec4093c0
Author: gabadie <gabadie@chromium.org>
Date:   Thu May 26 10:06:45 2016 -0700

    tools/android/loading: Dump command line flags to resume tasks in file
    
    Now that sandwich support multiple URLs, when on tasks fails, the
    amount of listed tasks to freeze by appending in the command line
    to resume become too big for a simple copy past.
    
    This CL fixes this issue by dumping the additionals command line
    flags in resume.txt in the output directory and message the user
    on what to append to the command line to resume.
    
    This CL also take this oportunity for avoiding freezing tasks that
    are not reachable by the selected final tasks leading to an error
    when doing a sandwich command such as "-e wikipedia -f common".
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2009973003
    Cr-Original-Commit-Position: refs/heads/master@{#396207}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9d0cfde261748450da8748733765b3e7206d3579

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 86e8c74..67332c1 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -87,7 +87,8 @@ def _ArgumentParser():
   plumbing_parser = OPTIONS.GetParentParser('plumbing options')
 
   # Main parser
-  parser = argparse.ArgumentParser(parents=[plumbing_parser])
+  parser = argparse.ArgumentParser(parents=[plumbing_parser],
+      fromfile_prefix_chars=task_manager.FROMFILE_PREFIX_CHARS)
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
   # Record test trace subcommand.
diff --git a/loading/task_manager.py b/loading/task_manager.py
index 0e5e72c..c6f5661 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -62,6 +62,7 @@ Example:
 import argparse
 import logging
 import os
+import re
 import subprocess
 import sys
 
@@ -70,6 +71,9 @@ import common_util
 
 _TASK_GRAPH_DOTFILE_NAME = 'tasks_graph.dot'
 _TASK_GRAPH_PNG_NAME = 'tasks_graph.png'
+_TASK_RESUME_ARGUMENTS_FILE = 'resume.txt'
+
+FROMFILE_PREFIX_CHARS = '@'
 
 
 class TaskError(Exception):
@@ -309,6 +313,9 @@ def CommandLineParser():
   """Creates command line arguments parser meant to be used as a parent parser
   for any entry point that use the ExecuteWithCommandLine() function.
 
+  The root parser must be created with:
+    fromfile_prefix_chars=FROMFILE_PREFIX_CHARS.
+
   Returns:
     The command line arguments parser.
   """
@@ -316,10 +323,10 @@ def CommandLineParser():
   parser.add_argument('-d', '--dry-run', action='store_true',
                       help='Only prints the deps of tasks to build.')
   parser.add_argument('-e', '--to-execute', metavar='REGEX', type=str,
-                      nargs='+', dest='run_regexes', default=[],
+                      action='append', dest='run_regexes', default=[],
                       help='Regex selecting tasks to execute.')
   parser.add_argument('-f', '--to-freeze', metavar='REGEX', type=str,
-                      nargs='+', dest='frozen_regexes', default=[],
+                      action='append', dest='frozen_regexes', default=[],
                       help='Regex selecting tasks to not execute.')
   parser.add_argument('-o', '--output', type=str, required=True,
                       help='Path of the output directory.')
@@ -329,17 +336,6 @@ def CommandLineParser():
   return parser
 
 
-def _GetCommandLineArgumentsStr(final_task_regexes, frozen_tasks):
-  arguments = []
-  if frozen_tasks:
-    arguments.append('-f')
-    arguments.extend([task.name for task in frozen_tasks])
-  if final_task_regexes:
-    arguments.append('-e')
-    arguments.extend(final_task_regexes)
-  return subprocess.list2cmdline(arguments)
-
-
 def ExecuteWithCommandLine(args, default_final_tasks):
   """Helper to execute tasks using command line arguments.
 
@@ -356,31 +352,28 @@ def ExecuteWithCommandLine(args, default_final_tasks):
   run_regexes = [common_util.VerboseCompileRegexOrAbort(e)
                    for e in args.run_regexes]
 
-  # Traverse the graph in the normal execution order starting from
-  # |default_final_tasks| in case of command line regex selection.
-  tasks = []
-  if frozen_regexes or run_regexes:
-    tasks = GenerateScenario(default_final_tasks, frozen_tasks=set())
-
-  # Lists frozen tasks
-  frozen_tasks = set()
-  if frozen_regexes:
-    for task in tasks:
-      for regex in frozen_regexes:
-        if regex.search(task.name):
-          frozen_tasks.add(task)
-          break
-
   # Lists final tasks.
   final_tasks = default_final_tasks
   if run_regexes:
     final_tasks = []
+    # Traverse the graph in the normal execution order starting from
+    # |default_final_tasks| in case of command line regex selection.
+    tasks = GenerateScenario(default_final_tasks, frozen_tasks=set())
     # Order of run regexes prevails on the traversing order of tasks.
     for regex in run_regexes:
       for task in tasks:
         if regex.search(task.name):
           final_tasks.append(task)
 
+  # Lists parents of |final_tasks| to freeze.
+  frozen_tasks = set()
+  if frozen_regexes:
+    for task in GenerateScenario(final_tasks, frozen_tasks=set()):
+      for regex in frozen_regexes:
+        if regex.search(task.name):
+          frozen_tasks.add(task)
+          break
+
   # Create the scenario.
   scenario = GenerateScenario(final_tasks, frozen_tasks)
 
@@ -410,14 +403,16 @@ def ExecuteWithCommandLine(args, default_final_tasks):
       try:
         task.Execute()
       except:
+        resume_path = os.path.join(args.output, _TASK_RESUME_ARGUMENTS_FILE)
+        resume_additonal_arguments = []
+        for task in ListResumingTasksToFreeze(
+            scenario, final_tasks, task):
+          resume_additonal_arguments.extend(['-f', re.escape(task.name)])
+        with open(resume_path, 'w') as file_output:
+          file_output.write('\n'.join(resume_additonal_arguments))
         print '# Looks like something went wrong in \'{}\''.format(task.name)
         print '#'
-        print '# To re-execute only this task, add the following parameters:'
-        print '#   ' + _GetCommandLineArgumentsStr(
-            [task.name], task._dependencies)
-        print '#'
-        print '# To resume from this task, add the following parameters:'
-        print '#   ' + _GetCommandLineArgumentsStr(args.run_regexes,
-            ListResumingTasksToFreeze(scenario, final_tasks, task))
+        print '# To resume from this task, append the following parameter:'
+        print '#   ' + FROMFILE_PREFIX_CHARS + resume_path
         raise
   return 0
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index cd18619..b674b2d 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -379,7 +379,7 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     self.assertEqual(0, self.Execute('-d'))
 
   def testRegex(self):
-    self.assertEqual(0, self.Execute('-e', 'b', 'd'))
+    self.assertEqual(0, self.Execute('-e', 'b', '-e', 'd'))
     self.assertEqual(1, self.Execute('-e', r'\d'))
 
   def testFreezing(self):
@@ -387,6 +387,10 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     self.TouchOutputFile('c')
     self.assertEqual(0, self.Execute('-f', 'c'))
 
+  def testDontFreezeUnreachableTasks(self):
+    self.TouchOutputFile('c')
+    self.assertEqual(0, self.Execute('-e', 'e', '-f', 'c', '-f', 'd'))
+
   def testTaskFailure(self):
     self.with_raise_exception_task = True
     with self.assertRaisesRegexp(TestException, r'^Expected error\.$'):

commit b7f182f42987ecee0f1837043ee2937840a5f260
Author: blundell <blundell@chromium.org>
Date:   Thu May 26 09:41:27 2016 -0700

    tools/android/loading: Track preload info in report.py
    
    This CL adds information on the fraction of critical requests that could
    be found by the preload scanner for the
    {PLT, first text, first contentful paint, first significant paint} events.
    
    Review-Url: https://codereview.chromium.org/2013683005
    Cr-Original-Commit-Position: refs/heads/master@{#396198}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 85d6e97413141782ed3fc4bc61857a9697b4f668

diff --git a/loading/report.py b/loading/report.py
index 5ace49e..6830170 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -13,6 +13,8 @@ from loading_graph_view import LoadingGraphView
 import loading_trace
 import metrics
 from network_activity_lens import NetworkActivityLens
+import prefetch_view
+import request_dependencies_lens
 from user_satisfied_lens import (
     FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
 
@@ -28,11 +30,14 @@ class LoadingReport(object):
       tracking_rules: ([str]) List of tracking filtering rules.
     """
     self.trace = trace
-    self._text_msec = FirstTextPaintLens(self.trace).SatisfiedMs()
-    self._contentful_paint_msec = (
-        FirstContentfulPaintLens(self.trace).SatisfiedMs())
-    self._significant_paint_msec = (
-        FirstSignificantPaintLens(self.trace).SatisfiedMs())
+
+    first_text_paint_lens = FirstTextPaintLens(self.trace)
+    first_contentful_paint_lens = FirstContentfulPaintLens(self.trace)
+    first_significant_paint_lens = FirstSignificantPaintLens(self.trace)
+
+    self._text_msec = first_text_paint_lens.SatisfiedMs()
+    self._contentful_paint_msec = first_contentful_paint_lens.SatisfiedMs()
+    self._significant_paint_msec = first_significant_paint_lens.SatisfiedMs()
 
     navigation_start_events = trace.tracing_track.GetMatchingEvents(
         'blink.user_timing', 'navigationStart')
@@ -59,6 +64,20 @@ class LoadingReport(object):
         self._significant_paint_msec)
     self._transfer_size = metrics.TotalTransferSize(trace)[1]
 
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        self.trace)
+    requests = self.trace.request_track.GetEvents()
+    preloaded_requests = \
+       prefetch_view.PrefetchSimulationView.PreloadedRequests(
+           requests[0], dependencies_lens, self.trace)
+    self._first_text_preloaded_requests = LoadingReport._ComputePreloadInfo(
+        preloaded_requests, first_text_paint_lens)
+    self._contentful_preloaded_requests = LoadingReport._ComputePreloadInfo(
+        preloaded_requests, first_contentful_paint_lens)
+    self._significant_preloaded_requests = LoadingReport._ComputePreloadInfo(
+        preloaded_requests, first_significant_paint_lens)
+    self._preloaded_requests = len(preloaded_requests)
+
     activity = ActivityLens(trace)
     self._cpu_busyness = self._ComputeCpuBusyness(activity)
 
@@ -84,6 +103,13 @@ class LoadingReport(object):
         'plt_ms': self._load_end_msec - self._navigation_start_msec,
         'contentful_byte_frac': self._contentful_byte_frac,
         'significant_byte_frac': self._significant_byte_frac,
+        'preloaded_requests': self._preloaded_requests,
+        'first_text_preloaded_requests':
+            self._first_text_preloaded_requests,
+        'contentful_preloaded_requests':
+            self._contentful_preloaded_requests,
+        'significant_preloaded_requests':
+            self._significant_preloaded_requests,
 
         # Take the first (earliest) inversions.
         'contentful_inversion': (self._contentful_inversion[0].url
@@ -106,6 +132,12 @@ class LoadingReport(object):
     return LoadingReport(trace, ad_rules_filename, tracking_rules_filename)
 
   @classmethod
+  def _ComputePreloadInfo(cls, preloaded_requests, user_lens):
+    preloaded_critical_requests = [r for r in preloaded_requests
+          if r in user_lens.CriticalRequests()]
+    return len(preloaded_critical_requests)
+
+  @classmethod
   def _AdRequestsReport(
       cls, trace, content_lens, has_ad_rules, has_tracking_rules):
     requests = trace.request_track.GetEvents()
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 5338e07..5774660 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -104,6 +104,10 @@ class LoadingReportTestCase(unittest.TestCase):
                            loading_report['plt_ms'])
     self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'], 2)
     self.assertAlmostEqual(0.1844, loading_report['significant_byte_frac'], 2)
+    self.assertEqual(1, loading_report['preloaded_requests'])
+    self.assertEqual(1, loading_report['first_text_preloaded_requests'])
+    self.assertEqual(1, loading_report['contentful_preloaded_requests'])
+    self.assertEqual(1, loading_report['significant_preloaded_requests'])
     self.assertIsNone(loading_report['contentful_inversion'])
     self.assertIsNone(loading_report['significant_inversion'])
     self.assertIsNone(loading_report['ad_requests'])

commit c0f75b6cddb66c26188fb7e55f847b7241d498a3
Author: gabadie <gabadie@chromium.org>
Date:   Thu May 26 08:30:58 2016 -0700

    sandwich: Implement cache patching task for SWR benchmarks
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2009223002
    Cr-Original-Commit-Position: refs/heads/master@{#396179}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 85e51e23998cc31bc8a2aa26a4c1203ab4c0fc06

diff --git a/loading/controller.py b/loading/controller.py
index 3311db2..00e5c8b 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -156,9 +156,9 @@ class ChromeControllerBase(object):
     self._network_name = None
     self._slow_death = False
 
-  def AddChromeArgument(self, arg):
-    """Add command-line argument to the chrome execution."""
-    self._chrome_args.append(arg)
+  def AddChromeArguments(self, args):
+    """Add command-line arguments to the chrome execution."""
+    self._chrome_args.extend(args)
 
   @contextlib.contextmanager
   def Open(self):
@@ -407,7 +407,7 @@ class LocalChromeController(ChromeControllerBase):
     """
     super(LocalChromeController, self).__init__()
     if OPTIONS.no_sandbox:
-      self.AddChromeArgument('--no-sandbox')
+      self.AddChromeArguments(['--no-sandbox'])
     self._profile_dir = OPTIONS.local_profile_dir
     self._using_temp_profile_dir = self._profile_dir is None
     if self._using_temp_profile_dir:
diff --git a/loading/request_track.py b/loading/request_track.py
index 8caac11..2743eed 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -263,8 +263,16 @@ class Request(object):
         break
     return result
 
+  def SetHTTPResponseHeader(self, header, header_value):
+    """Sets the value of a HTTP response header."""
+    assert header.islower()
+    for name in self.response_headers.keys():
+      if name.lower() == header:
+        del self.response_headers[name]
+    self.response_headers[header] = header_value
+
   def GetResponseHeaderValue(self, header, value):
-    """Returns True iff the response headers |header| contains |value|."""
+    """Returns a copy of |value| iff response |header| contains it."""
     header_values = self.GetHTTPResponseHeader(header)
     if not header_values:
       return None
@@ -420,7 +428,7 @@ class CachingPolicy(object):
     # net/http/http_response_headers.cc, itself following RFC 2616.
     if not self.IsCacheable():
       return self.FETCH
-    freshness = self._GetFreshnessLifetimes()
+    freshness = self.GetFreshnessLifetimes()
     if freshness[0] == 0 and freshness[1] == 0:
       return self.VALIDATION_SYNC
     age = self._GetCurrentAge(timestamp)
@@ -430,7 +438,7 @@ class CachingPolicy(object):
       return self.VALIDATION_ASYNC
     return self.VALIDATION_SYNC
 
-  def _GetFreshnessLifetimes(self):
+  def GetFreshnessLifetimes(self):
     """Returns [freshness, stale-while-revalidate freshness] in seconds."""
     # This is adapted from GetFreshnessLifetimes() in
     # //net/http/http_response_headers.cc (which follows the RFC).
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index f8ca187..fb48b7c 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -75,6 +75,9 @@ class SandwichRunner(object):
     # The cache archive's path to save to or push from. Is str or None.
     self.cache_archive_path = None
 
+    # List of additional chrome command line flags.
+    self.chrome_args = []
+
     # Controls whether the WPR server should do script injection.
     self.disable_wpr_script_injection = False
 
@@ -220,7 +223,8 @@ class SandwichRunner(object):
       self._chrome_ctl = controller.RemoteChromeController(self.android_device)
     else:
       self._chrome_ctl = controller.LocalChromeController()
-    self._chrome_ctl.AddChromeArgument('--disable-infobars')
+    self._chrome_ctl.AddChromeArguments(['--disable-infobars'])
+    self._chrome_ctl.AddChromeArguments(self.chrome_args)
     if self.cache_operation == CacheOperation.SAVE:
       self._chrome_ctl.SetSlowDeath()
 
diff --git a/loading/sandwich_swr.py b/loading/sandwich_swr.py
index 8d92c63..ee4831a 100644
--- a/loading/sandwich_swr.py
+++ b/loading/sandwich_swr.py
@@ -3,15 +3,48 @@
 # found in the LICENSE file.
 
 import csv
+import logging
 import os
 import shutil
 
+import chrome_cache
 import common_util
+import loading_trace
+import request_track
 import sandwich_metrics
 import sandwich_runner
 import task_manager
 
 
+def _BuildPatchedCache(original_cache_run_path, original_cache_archive_path,
+      cache_archive_dest_path):
+  CACHE_CONTROL_VALUE = 'max-age=0,stale-while-revalidate=315360000'
+  trace_path = os.path.join(
+      original_cache_run_path, '0', sandwich_runner.TRACE_FILENAME)
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_path)
+  patch_count = 0
+  with common_util.TemporaryDirectory(prefix='sandwich_tmp') as tmp_path:
+    cache_path = os.path.join(tmp_path, 'cache')
+    chrome_cache.UnzipDirectoryContent(original_cache_archive_path, cache_path)
+    cache_backend = chrome_cache.CacheBackend(cache_path, 'simple')
+    cache_keys = set(cache_backend.ListKeys())
+    for request in trace.request_track.GetEvents():
+      if request.url not in cache_keys:
+        continue
+      caching_policy = request_track.CachingPolicy(request)
+      assert caching_policy.IsCacheable()
+      freshness = caching_policy.GetFreshnessLifetimes()
+      if freshness[0] == 0:
+        continue
+      request.SetHTTPResponseHeader('cache-control', CACHE_CONTROL_VALUE)
+      raw_headers = request.GetRawResponseHeaders()
+      cache_backend.UpdateRawResponseHeaders(request.url, raw_headers)
+      patch_count += 1
+    chrome_cache.ZipDirectoryContent(cache_path, cache_archive_dest_path)
+  logging.info('Patched %d cached resources out of %d' % (
+      patch_count, len(cache_keys)))
+
+
 class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
   """A builder for a graph of tasks for Stale-While-Revalidate study benchmarks.
   """
@@ -21,16 +54,15 @@ class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
                                   common_builder.output_directory,
                                   common_builder.output_subdirectory)
     self._common_builder = common_builder
-    self._worstcase_cache_task = None
-    self._swr_cache_task = None
+    self._patched_cache_task = None
     self._PopulateCommonPipelines()
 
   def _PopulateCommonPipelines(self):
     """Creates necessary tasks to produce initial cache archives.
 
     Here is the full dependency tree for the returned task:
-    common/swr-patched-cache.zip
-      depends on: common/worstcase-patched-cache.zip
+    common/patched-cache.zip
+      depends on: common/original-cache.zip
         depends on: common/webpages.wpr
     """
     @self.RegisterTask('common/original-cache.zip',
@@ -40,23 +72,17 @@ class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
       runner.wpr_archive_path = self._common_builder.original_wpr_task.path
       runner.cache_archive_path = BuildOriginalCache.path
       runner.cache_operation = sandwich_runner.CacheOperation.SAVE
-      runner.output_dir = BuildOriginalCache.path[:-4] + '-run'
+      runner.output_dir = BuildOriginalCache.run_path
       runner.Run()
+    BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
 
-    @self.RegisterTask('common/worstcase-patched-cache.zip',
+    @self.RegisterTask('common/patched-cache.zip',
                        dependencies=[BuildOriginalCache])
-    def BuildWorstCaseCache():
-      # TODO(gabadie): Patch cache-control's max-age=0 if any.
-      shutil.copyfile(BuildOriginalCache.path, BuildWorstCaseCache.path)
-
-    @self.RegisterTask('common/swr-patched-cache.zip', [BuildWorstCaseCache])
-    def BuildSWRCache():
-      # TODO(gabadie): Patch cache-control's stale-while-revalidate=1 year if
-      # any max-age.
-      shutil.copyfile(BuildWorstCaseCache.path, BuildSWRCache.path)
+    def BuildPatchedCache():
+      _BuildPatchedCache(BuildOriginalCache.run_path, BuildOriginalCache.path,
+                         BuildPatchedCache.path)
 
-    self._worstcase_cache_task = BuildWorstCaseCache
-    self._swr_cache_task = BuildSWRCache
+    self._patched_cache_task = BuildPatchedCache
 
   def PopulateBenchmark(self, enable_swr, transformer_list_name,
                         transformer_list):
@@ -77,13 +103,11 @@ class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
     """
     task_prefix = os.path.join(transformer_list_name, '')
     if enable_swr:
-      cache_task = self._swr_cache_task
       task_prefix += 'swr'
     else:
-      cache_task = self._worstcase_cache_task
       task_prefix += 'worstcase'
 
-    @self.RegisterTask(task_prefix + '-run/', [cache_task])
+    @self.RegisterTask(task_prefix + '-run/', [self._patched_cache_task])
     def RunBenchmark():
       runner = self._common_builder.CreateSandwichRunner()
       for transformer in transformer_list:
@@ -91,9 +115,11 @@ class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
       runner.wpr_archive_path = self._common_builder.original_wpr_task.path
       runner.wpr_out_log_path = os.path.join(
           RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
-      runner.cache_archive_path = cache_task.path
+      runner.cache_archive_path = self._patched_cache_task.path
       runner.cache_operation = sandwich_runner.CacheOperation.PUSH
       runner.output_dir = RunBenchmark.path
+      if enable_swr:
+        runner.chrome_args.append('--enable-features=StaleWhileRevalidate2')
       runner.Run()
 
     @self.RegisterTask(task_prefix + '-metrics.csv', [RunBenchmark])

commit 98cf6b8b26529ac3fa7ea892040859f0e8c385b8
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 25 09:24:46 2016 -0700

    sandwich: Remove common/subresources-for-urls.json task
    
    Before, we were running chrome to specifically lists the available
    url's subresources. Because we are using this subresources lists
    to validate cache and validate benchmark runs, list of expected
    cached resources could sometimes list JavaScript emitted requests
    that the cache doesn't have because has close chrome earlier.
    
    This CL removes common/subresources-for-urls.json and generates
    the subresources lists using the common/original-cache.zip's trace.
    
    This CL also take the oportunity to copy root's logger into
    common/patched-cache-validation.log when running.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1974913002
    Cr-Original-Commit-Position: refs/heads/master@{#395905}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 361da8288fbff5cfa54e214f2b057c5d3c3216ba

diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 26dba8c..61a4828 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import csv
+import logging
 import json
 import os
 import shutil
@@ -95,7 +96,7 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
 
     self._patched_wpr_task = None
     self._reference_cache_task = None
-    self._subresources_for_urls_run_task = None
+    self._trace_from_grabbing_reference_cache = None
     self._subresources_for_urls_task = None
     self._PopulateCommonPipelines()
 
@@ -112,8 +113,7 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
           depends on: common/webpages-patched.wpr
             depends on: common/webpages.wpr
       depends on: common/urls-resources.json
-        depends on: common/urls-resources-run/
-          depends on: common/webpages.wpr
+        depends on: common/original-cache.zip
     """
     @self.RegisterTask('common/webpages-patched.wpr',
                        dependencies=[self._common_builder.original_wpr_task])
@@ -140,31 +140,28 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
       sandwich_misc.PatchCacheArchive(BuildOriginalCache.path,
           original_cache_trace_path, BuildPatchedCache.path)
 
-    @self.RegisterTask('common/subresources-for-urls-run/',
-                       dependencies=[self._common_builder.original_wpr_task])
-    def UrlsResourcesRun():
-      runner = self._common_builder.CreateSandwichRunner()
-      runner.wpr_archive_path = self._common_builder.original_wpr_task.path
-      runner.cache_operation = sandwich_runner.CacheOperation.CLEAR
-      runner.output_dir = UrlsResourcesRun.path
-      runner.Run()
-
-    @self.RegisterTask('common/subresources-for-urls.json', [UrlsResourcesRun])
+    @self.RegisterTask('common/subresources-for-urls.json',
+                       [BuildOriginalCache])
     def ListUrlsResources():
       url_resources = sandwich_misc.ReadSubresourceFromRunnerOutputDir(
-          UrlsResourcesRun.path)
+          BuildOriginalCache.run_path)
       with open(ListUrlsResources.path, 'w') as output:
         json.dump(url_resources, output)
 
     @self.RegisterTask('common/patched-cache-validation.log',
                        [BuildPatchedCache])
     def ValidatePatchedCache():
-      sandwich_misc.ValidateCacheArchiveContent(
-          original_cache_trace_path, BuildPatchedCache.path)
+      handler = logging.FileHandler(ValidatePatchedCache.path)
+      logging.getLogger().addHandler(handler)
+      try:
+        sandwich_misc.ValidateCacheArchiveContent(
+            original_cache_trace_path, BuildPatchedCache.path)
+      finally:
+        logging.getLogger().removeHandler(handler)
 
     self._patched_wpr_task = BuildPatchedWpr
+    self._trace_from_grabbing_reference_cache = original_cache_trace_path
     self._reference_cache_task = BuildPatchedCache
-    self._subresources_for_urls_run_task = UrlsResourcesRun
     self._subresources_for_urls_task = ListUrlsResources
 
     self._common_builder.default_final_tasks.append(ValidatePatchedCache)
@@ -197,10 +194,8 @@ class PrefetchBenchmarkBuilder(task_manager.Builder):
     @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
                        dependencies=[self._subresources_for_urls_task])
     def SetupBenchmark():
-      trace_path = os.path.join(self._subresources_for_urls_run_task.path, '0',
-                                sandwich_runner.TRACE_FILENAME)
       whitelisted_urls = sandwich_misc.ExtractDiscoverableUrls(
-          trace_path, subresource_discoverer)
+          self._trace_from_grabbing_reference_cache, subresource_discoverer)
 
       url_resources = json.load(open(self._subresources_for_urls_task.path))
       common_util.EnsureParentDirectoryExists(SetupBenchmark.path)

commit ee3de351c3eba0e570206fa43fb42e010a70da3e
Author: lizeb <lizeb@chromium.org>
Date:   Wed May 25 07:16:00 2016 -0700

    clovis: Report the amount of CPU taken by Ads.
    
    Review-Url: https://codereview.chromium.org/2011923002
    Cr-Original-Commit-Position: refs/heads/master@{#395888}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2263aa432eac0edf2b8aee8ed79af4ae76b12255

diff --git a/loading/report.py b/loading/report.py
index 86dc2f7..5ace49e 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -68,45 +68,9 @@ class LoadingReport(object):
     has_tracking_rules = bool(tracking_rules)
     self._ad_report = self._AdRequestsReport(
         trace, content_lens, has_ad_rules, has_tracking_rules)
-
-  def _ComputeCpuBusyness(self, activity):
-    load_start = self._navigation_start_msec
-    load_end = self._load_end_msec
-    contentful = self._contentful_paint_msec
-    significant = self._significant_paint_msec
-
-    load_time = float(load_end - load_start)
-    contentful_time = float(contentful - load_start)
-    significant_time = float(significant - load_start)
-
-    result = {
-        'activity_load_frac': (
-            activity.MainRendererThreadBusyness(load_start, load_end)
-            / load_time),
-        'activity_contentful_paint_frac': (
-            activity.MainRendererThreadBusyness(load_start, contentful)
-            / contentful_time),
-        'activity_significant_paint_frac': (
-            activity.MainRendererThreadBusyness(load_start, significant)
-            / significant_time)}
-
-    activity_load = activity.ComputeActivity(load_start, load_end)
-    activity_contentful = activity.ComputeActivity(load_start, contentful)
-    activity_significant = activity.ComputeActivity(load_start, significant)
-
-    result['parsing_load_frac'] = (
-        sum(activity_load['parsing'].values()) / load_time)
-    result['script_load_frac'] = (
-        sum(activity_load['script'].values()) / load_time)
-    result['parsing_contentful_frac'] = (
-        sum(activity_contentful['parsing'].values()) / contentful_time)
-    result['script_contentful_frac'] = (
-        sum(activity_contentful['script'].values()) / contentful_time)
-    result['parsing_significant_frac'] = (
-        sum(activity_significant['parsing'].values()) / significant_time)
-    result['script_significant_frac'] = (
-        sum(activity_significant['script'].values()) / significant_time)
-    return result
+    self._ads_cost = self._AdsAndTrackingCpuCost(
+        self._navigation_start_msec, self._load_end_msec, content_lens,
+        activity, has_tracking_rules or has_ad_rules)
 
   def GenerateReport(self):
     """Returns a report as a dict."""
@@ -131,6 +95,7 @@ class LoadingReport(object):
         'transfer_size': self._transfer_size}
     report.update(self._ad_report)
     report.update(self._cpu_busyness)
+    report.update(self._ads_cost)
     return report
 
   @classmethod
@@ -181,6 +146,80 @@ class LoadingReport(object):
     # request.
     return max(r.end_msec or -1 for r in trace.request_track.GetEvents())
 
+  def _ComputeCpuBusyness(self, activity):
+    load_start = self._navigation_start_msec
+    load_end = self._load_end_msec
+    contentful = self._contentful_paint_msec
+    significant = self._significant_paint_msec
+
+    load_time = float(load_end - load_start)
+    contentful_time = float(contentful - load_start)
+    significant_time = float(significant - load_start)
+
+    result = {
+        'activity_load_frac': (
+            activity.MainRendererThreadBusyness(load_start, load_end)
+            / load_time),
+        'activity_contentful_paint_frac': (
+            activity.MainRendererThreadBusyness(load_start, contentful)
+            / contentful_time),
+        'activity_significant_paint_frac': (
+            activity.MainRendererThreadBusyness(load_start, significant)
+            / significant_time)}
+
+    activity_load = activity.ComputeActivity(load_start, load_end)
+    activity_contentful = activity.ComputeActivity(load_start, contentful)
+    activity_significant = activity.ComputeActivity(load_start, significant)
+
+    result['parsing_load_frac'] = (
+        sum(activity_load['parsing'].values()) / load_time)
+    result['script_load_frac'] = (
+        sum(activity_load['script'].values()) / load_time)
+    result['parsing_contentful_frac'] = (
+        sum(activity_contentful['parsing'].values()) / contentful_time)
+    result['script_contentful_frac'] = (
+        sum(activity_contentful['script'].values()) / contentful_time)
+    result['parsing_significant_frac'] = (
+        sum(activity_significant['parsing'].values()) / significant_time)
+    result['script_significant_frac'] = (
+        sum(activity_significant['script'].values()) / significant_time)
+    return result
+
+  @classmethod
+  def _AdsAndTrackingCpuCost(
+      cls, start_msec, end_msec, content_lens, activity, has_rules):
+    """Returns the CPU cost associated with Ads and tracking between timestamps.
+
+    Can return an overestimate, as execution slices are tagged by URL, and not
+    by requests.
+
+    Args:
+      start_msec: (float)
+      end_msec: (float)
+      content_lens: (ContentClassificationLens)
+      activity: (ActivityLens)
+
+    Returns:
+      {'ad_and_tracking_script_frac': float,
+       'ad_and_tracking_parsing_frac': float}
+    """
+    result = {'ad_or_tracking_script_frac': None,
+              'ad_or_tracking_parsing_frac': None}
+    if not has_rules:
+      return result
+
+    duration = float(end_msec - start_msec)
+    requests = content_lens.AdAndTrackingRequests()
+    urls = {r.url for r in requests}
+    cpu_breakdown = activity.ComputeActivity(start_msec, end_msec)
+    result['ad_or_tracking_script_frac'] = sum(
+            value for (url, value) in cpu_breakdown['script'].items()
+            if url in urls) / duration
+    result['ad_or_tracking_parsing_frac'] = sum(
+            value for (url, value) in cpu_breakdown['parsing'].items()
+            if url in urls) / duration
+    return result
+
 
 def _Main(args):
   assert len(args) == 4, 'Usage: report.py trace.json ad_rules tracking_rules'
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index e2e1250..5338e07 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -39,6 +39,10 @@ class LoadingReportTestCase(unittest.TestCase):
     self.requests[0].encoded_data_length = self._FIRST_REQUEST_DATA_LENGTH
     self.requests[1].encoded_data_length = self._SECOND_REQUEST_DATA_LENGTH
 
+    self.ad_domain = 'i-ve-got-the-best-ads.com'
+    self.ad_url = 'http://www.' + self.ad_domain + '/i-m-really-rich.js'
+    self.requests[0].url = self.ad_url
+
     self.trace_events = [
         {'args': {'name': 'CrRendererMain'}, 'cat': '__metadata',
          'name': 'thread_name', 'ph': 'M', 'pid': 1, 'tid': 1, 'ts': 0},
@@ -106,6 +110,8 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertIsNone(loading_report['ad_or_tracking_requests'])
     self.assertIsNone(loading_report['ad_or_tracking_initiated_requests'])
     self.assertIsNone(loading_report['ad_or_tracking_initiated_transfer_size'])
+    self.assertIsNone(loading_report['ad_or_tracking_script_frac'])
+    self.assertIsNone(loading_report['ad_or_tracking_parsing_frac'])
     self.assertEqual(
         self._FIRST_REQUEST_DATA_LENGTH + self._SECOND_REQUEST_DATA_LENGTH
         + metrics.HTTP_OK_LENGTH * 2,
@@ -138,11 +144,9 @@ class LoadingReportTestCase(unittest.TestCase):
                            loading_report['plt_ms'])
 
   def testAdTrackingRules(self):
-    ad_domain = 'i-ve-got-the-best-ads.com'
-    self.requests[0].url = 'http://www.' + ad_domain
     trace = self._MakeTrace()
     loading_report = report.LoadingReport(
-        trace, [ad_domain], []).GenerateReport()
+        trace, [self.ad_domain], []).GenerateReport()
     self.assertEqual(1, loading_report['ad_requests'])
     self.assertEqual(1, loading_report['ad_or_tracking_requests'])
     self.assertEqual(1, loading_report['ad_or_tracking_initiated_requests'])
@@ -186,6 +190,19 @@ class LoadingReportTestCase(unittest.TestCase):
         (self._PARSING_EVENT_DURATION - self._SCRIPT_EVENT_DURATION)
         / contentful_time, loading_report['parsing_contentful_frac'])
 
+  def testAdsAndTrackingCost(self):
+    load_time = float(self._LOAD_END_TIME - self._NAVIGATION_START_TIME)
+    self.trace_events.append(
+       {'ts':  load_time / 3. * self.MILLI_TO_MICRO,
+        'pid': 1, 'tid': 1, 'ph': 'X',
+        'dur': load_time / 2. * self.MILLI_TO_MICRO,
+        'cat': 'devtools.timeline', 'name': 'EvaluateScript',
+        'args': {'data': {'scriptName': self.ad_url}}})
+    loading_report = report.LoadingReport(
+        self._MakeTrace(), [self.ad_domain]).GenerateReport()
+    self.assertAlmostEqual(.5, loading_report['ad_or_tracking_script_frac'])
+    self.assertAlmostEqual(0., loading_report['ad_or_tracking_parsing_frac'])
+
 
 if __name__ == '__main__':
   unittest.main()

commit eb545903aa5b6231a3d1abcb38bf4d4ab21194f6
Author: wnwen <wnwen@chromium.org>
Date:   Wed May 25 06:44:15 2016 -0700

    Add presubmit to prevent SharedPreferences misuse.
    
    BUG=599284
    
    Review-Url: https://codereview.chromium.org/1993243002
    Cr-Original-Commit-Position: refs/heads/master@{#395881}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bdc444ee2eebb0970f239cc6f651c4e794915c37

diff --git a/checkstyle/chromium-style-5.0.xml b/checkstyle/chromium-style-5.0.xml
index 6557f61..700f449 100644
--- a/checkstyle/chromium-style-5.0.xml
+++ b/checkstyle/chromium-style-5.0.xml
@@ -7,6 +7,9 @@
 <module name="Checker">
   <property name="severity" value="warning"/>
   <property name="charset" value="UTF-8"/>
+  <module name="SuppressionFilter">
+    <property name="file" value="tools/android/checkstyle/suppressions.xml"/>
+  </module>
   <module name="TreeWalker">
     <module name="AvoidStarImport">
       <property name="severity" value="error"/>
@@ -208,6 +211,13 @@
       <property name="ignoreComments" value="true"/>
       <property name="message" value="Avoid android.app.AlertDialog; if possible, use android.support.v7.app.AlertDialog instead, which has a Material look on all devices. (Some parts of the codebase cant depend on the support library, in which case android.app.AlertDialog is the only option)"/>
     </module>
+    <module name="RegexpSinglelineJava">
+      <property name="id" value="SharedPreferencesCheck"/>
+      <property name="severity" value="error"/>
+      <property name="format" value="getDefaultSharedPreferences"/>
+      <property name="ignoreComments" value="true"/>
+      <property name="message" value="Use ContextUtils.getAppSharedPreferences() instead to access app-wide SharedPreferences."/>
+    </module>
   </module>
 
   <!-- Non-TreeWalker modules -->
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
new file mode 100644
index 0000000..850d7d4
--- /dev/null
+++ b/checkstyle/suppressions.xml
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!DOCTYPE suppressions PUBLIC "-//Puppy Crawl//DTD Suppressions 1.1//EN" "http://www.puppycrawl.com/dtds/suppressions_1_1.dtd">
+
+<suppressions>
+  <suppress id="SharedPreferencesCheck" files="ContextUtils.java"/>
+</suppressions>

commit 7d36130f750802aeb073b83c0a30d0a64b7f7cb9
Author: lizeb <lizeb@chromium.org>
Date:   Wed May 25 06:42:07 2016 -0700

    clovis: Report CPU activity breakdown.
    
    Review-Url: https://codereview.chromium.org/2013453004
    Cr-Original-Commit-Position: refs/heads/master@{#395880}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 20540c1e2bf22c6f016bde06f09b013ce8ec703b

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index 7fdd9af..3e3a98e 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -182,8 +182,24 @@ class ActivityLens(object):
     assert end_msec - start_msec >= 0.
     events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
     result = {'edge_cost': end_msec - start_msec,
-              'busy': self._ThreadBusyness(events, start_msec, end_msec),
-              'parsing': self._Parsing(events, start_msec, end_msec),
+              'busy': self._ThreadBusyness(events, start_msec, end_msec)}
+    result.update(self.ComputeActivity(start_msec, end_msec))
+    return result
+
+  def ComputeActivity(self, start_msec, end_msec):
+    """Returns a breakdown of the main renderer thread activity between two
+    timestamps.
+
+    Args:
+      start_msec: (float)
+      end_msec: (float)
+
+    Returns:
+       {'parsing': {'url' -> time_ms}, 'script': {'url' -> time_ms}}.
+    """
+    assert end_msec - start_msec >= 0.
+    events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
+    result = {'parsing': self._Parsing(events, start_msec, end_msec),
               'script': self._ScriptsExecuting(events, start_msec, end_msec)}
     return result
 
diff --git a/loading/report.py b/loading/report.py
index 0004d06..86dc2f7 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -51,8 +51,6 @@ class LoadingReport(object):
     else:
       self._contentful_byte_frac = float('Nan')
       self._significant_byte_frac = float('Nan')
-    self._ad_report = self._AdRequestsReport(
-        trace, ad_rules or [], tracking_rules or [])
 
     graph = LoadingGraphView.FromTrace(trace)
     self._contentful_inversion = graph.GetInversionsAtTime(
@@ -60,25 +58,55 @@ class LoadingReport(object):
     self._significant_inversion = graph.GetInversionsAtTime(
         self._significant_paint_msec)
     self._transfer_size = metrics.TotalTransferSize(trace)[1]
-    self._cpu_busyness = self._ComputeCpuBusyness(trace)
 
-  def _ComputeCpuBusyness(self, trace):
     activity = ActivityLens(trace)
+    self._cpu_busyness = self._ComputeCpuBusyness(activity)
+
+    content_lens = ContentClassificationLens(
+        trace, ad_rules or [], tracking_rules or [])
+    has_ad_rules = bool(ad_rules)
+    has_tracking_rules = bool(tracking_rules)
+    self._ad_report = self._AdRequestsReport(
+        trace, content_lens, has_ad_rules, has_tracking_rules)
+
+  def _ComputeCpuBusyness(self, activity):
     load_start = self._navigation_start_msec
     load_end = self._load_end_msec
     contentful = self._contentful_paint_msec
     significant = self._significant_paint_msec
 
-    return {
+    load_time = float(load_end - load_start)
+    contentful_time = float(contentful - load_start)
+    significant_time = float(significant - load_start)
+
+    result = {
         'activity_load_frac': (
             activity.MainRendererThreadBusyness(load_start, load_end)
-            / float(load_end - load_start)),
+            / load_time),
         'activity_contentful_paint_frac': (
             activity.MainRendererThreadBusyness(load_start, contentful)
-            / float(contentful - load_start)),
+            / contentful_time),
         'activity_significant_paint_frac': (
             activity.MainRendererThreadBusyness(load_start, significant)
-            / float(significant - load_start))}
+            / significant_time)}
+
+    activity_load = activity.ComputeActivity(load_start, load_end)
+    activity_contentful = activity.ComputeActivity(load_start, contentful)
+    activity_significant = activity.ComputeActivity(load_start, significant)
+
+    result['parsing_load_frac'] = (
+        sum(activity_load['parsing'].values()) / load_time)
+    result['script_load_frac'] = (
+        sum(activity_load['script'].values()) / load_time)
+    result['parsing_contentful_frac'] = (
+        sum(activity_contentful['parsing'].values()) / contentful_time)
+    result['script_contentful_frac'] = (
+        sum(activity_contentful['script'].values()) / contentful_time)
+    result['parsing_significant_frac'] = (
+        sum(activity_significant['parsing'].values()) / significant_time)
+    result['script_significant_frac'] = (
+        sum(activity_significant['script'].values()) / significant_time)
+    return result
 
   def GenerateReport(self):
     """Returns a report as a dict."""
@@ -113,29 +141,28 @@ class LoadingReport(object):
     return LoadingReport(trace, ad_rules_filename, tracking_rules_filename)
 
   @classmethod
-  def _AdRequestsReport(cls, trace, ad_rules, tracking_rules):
-    has_rules = bool(ad_rules) or bool(tracking_rules)
+  def _AdRequestsReport(
+      cls, trace, content_lens, has_ad_rules, has_tracking_rules):
     requests = trace.request_track.GetEvents()
+    has_rules = has_ad_rules or has_tracking_rules
     result = {
         'request_count': len(requests),
-        'ad_requests': 0 if ad_rules else None,
-        'tracking_requests': 0 if tracking_rules else None,
+        'ad_requests': 0 if has_ad_rules else None,
+        'tracking_requests': 0 if has_tracking_rules else None,
         'ad_or_tracking_requests': 0 if has_rules else None,
         'ad_or_tracking_initiated_requests': 0 if has_rules else None,
         'ad_or_tracking_initiated_transfer_size': 0 if has_rules else None}
-    content_classification_lens = ContentClassificationLens(
-        trace, ad_rules, tracking_rules)
     if not has_rules:
       return result
-    for request in trace.request_track.GetEvents():
-      is_ad = content_classification_lens.IsAdRequest(request)
-      is_tracking = content_classification_lens.IsTrackingRequest(request)
-      if ad_rules:
+    for request in requests:
+      is_ad = content_lens.IsAdRequest(request)
+      is_tracking = content_lens.IsTrackingRequest(request)
+      if has_ad_rules:
         result['ad_requests'] += int(is_ad)
-      if tracking_rules:
+      if has_tracking_rules:
         result['tracking_requests'] += int(is_tracking)
       result['ad_or_tracking_requests'] += int(is_ad or is_tracking)
-    ad_tracking_requests = content_classification_lens.AdAndTrackingRequests()
+    ad_tracking_requests = content_lens.AdAndTrackingRequests()
     result['ad_or_tracking_initiated_requests'] = len(ad_tracking_requests)
     result['ad_or_tracking_initiated_transfer_size'] = metrics.TransferSize(
         ad_tracking_requests)[1]
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 03cdc9a..e2e1250 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -25,6 +25,8 @@ class LoadingReportTestCase(unittest.TestCase):
   _SECOND_REQUEST_DATA_LENGTH = 1024
   _TOPLEVEL_EVENT_OFFSET = 10
   _TOPLEVEL_EVENT_DURATION = 100
+  _SCRIPT_EVENT_DURATION = 50
+  _PARSING_EVENT_DURATION = 60
 
   def setUp(self):
     self.trace_creator = test_utils.TraceCreator()
@@ -67,7 +69,17 @@ class LoadingReportTestCase(unittest.TestCase):
          * self.MILLI_TO_MICRO,
          'pid': 1, 'tid': 1, 'ph': 'X',
          'dur': self._TOPLEVEL_EVENT_DURATION * self.MILLI_TO_MICRO,
-         'cat': 'toplevel', 'name': 'MessageLoop::RunTask'}]
+         'cat': 'toplevel', 'name': 'MessageLoop::RunTask'},
+        {'ts': self._NAVIGATION_START_TIME * self.MILLI_TO_MICRO,
+         'pid': 1, 'tid': 1, 'ph': 'X',
+         'dur': self._PARSING_EVENT_DURATION * self.MILLI_TO_MICRO,
+         'cat': 'devtools.timeline', 'name': 'ParseHTML',
+         'args': {'beginData': {'url': ''}}},
+        {'ts': self._NAVIGATION_START_TIME * self.MILLI_TO_MICRO,
+         'pid': 1, 'tid': 1, 'ph': 'X',
+         'dur': self._SCRIPT_EVENT_DURATION * self.MILLI_TO_MICRO,
+         'cat': 'devtools.timeline', 'name': 'EvaluateScript',
+         'args': {'data': {'scriptName': ''}}}]
 
   def _MakeTrace(self):
     trace = self.trace_creator.CreateTrace(
@@ -152,6 +164,28 @@ class LoadingReportTestCase(unittest.TestCase):
         / (self._LOAD_END_TIME - self._NAVIGATION_START_TIME),
         loading_report['activity_load_frac'])
 
+  def testActivityBreakdown(self):
+    loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
+    load_time = float(self._LOAD_END_TIME - self._NAVIGATION_START_TIME)
+    contentful_time = float(
+        self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME)
+
+    self.assertAlmostEqual(self._SCRIPT_EVENT_DURATION / load_time,
+                           loading_report['script_load_frac'])
+    self.assertAlmostEqual(
+        (self._PARSING_EVENT_DURATION - self._SCRIPT_EVENT_DURATION)
+        / load_time,
+        loading_report['parsing_load_frac'])
+
+    self.assertAlmostEqual(1., loading_report['script_significant_frac'])
+    self.assertAlmostEqual(0., loading_report['parsing_significant_frac'])
+
+    self.assertAlmostEqual(self._SCRIPT_EVENT_DURATION / contentful_time,
+                           loading_report['script_contentful_frac'])
+    self.assertAlmostEqual(
+        (self._PARSING_EVENT_DURATION - self._SCRIPT_EVENT_DURATION)
+        / contentful_time, loading_report['parsing_contentful_frac'])
+
 
 if __name__ == '__main__':
   unittest.main()

commit c43988e391f838675bd2b65089a30b883ee15d45
Author: droger <droger@chromium.org>
Date:   Wed May 25 06:37:29 2016 -0700

    tools/android/loading Remove None values from BigQuery reports
    
    NaN values were already removed, this CL also removes None values.
    When a value is None, the key is omitted instead, which results in a
    "null" entry in the table.
    
    Review-Url: https://codereview.chromium.org/1996473004
    Cr-Original-Commit-Position: refs/heads/master@{#395878}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 774f0092c58de1458d086aa600da34e3f986c399

diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index 0081139..f0f4cf7 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -70,6 +70,15 @@ class ReportTaskHandler(object):
     self._ad_rules_filename = ad_rules_filename
     self._tracking_rules_filename = tracking_rules_filename
 
+  def _IsBigQueryValueValid(self, value):
+    """Returns whether a value is valid and can be uploaded to BigQuery."""
+    if value is None:
+      return False
+    # BigQuery rejects NaN.
+    if type(value) is float and (math.isnan(value) or math.isinf(value)):
+      return False
+    return True
+
   def _StreamRowsToBigQuery(self, rows, table_id):
     """Uploads a list of rows to the BigQuery table associated with the given
     table_id.
@@ -144,9 +153,10 @@ class ReportTaskHandler(object):
         continue
       # Filter out bad values.
       for key, value in report.items():
-        if type(value) is float and (math.isnan(value) or math.isinf(value)):
-          self._logger.error('Invalid %s for URL:%s' % (key, report.get('url')))
-          self._failure_database.AddFailure('invalid_bigquery_value', key)
+        if not self._IsBigQueryValueValid(value):
+          url = report.get('url')
+          self._logger.error('Invalid %s for URL:%s' % (key, url))
+          self._failure_database.AddFailure('invalid_bigquery_value', url)
           del report[key]
       rows.append(report)
 

commit 85c24f8e858dc2e713a1c83fcdb2b1ad0ccfd0c0
Author: droger <droger@chromium.org>
Date:   Wed May 25 04:47:08 2016 -0700

    tools/android/loading Fix OOM hangs and process leak.
    
    Review-Url: https://codereview.chromium.org/2010453003
    Cr-Original-Commit-Position: refs/heads/master@{#395865}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 79b7bdfe016c37a41741a0d1485c690852a839b6

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index aa66514..c127fd9 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -5,9 +5,12 @@
 import multiprocessing
 import os
 import re
+import resource
 import sys
 import traceback
 
+import psutil
+
 import common.clovis_paths
 from common.clovis_task import ClovisTask
 from common.loading_trace_database import LoadingTraceDatabase
@@ -18,6 +21,18 @@ import options
 import xvfb_helper
 
 
+def LimitMemory(memory_share):
+  """Limits the memory available to this process, to avoid OOM issues.
+
+  Args:
+    memory_share: (float) Share coefficient of the total physical memory that
+                          the process can use.
+  """
+  total_memory = psutil.virtual_memory().total
+  memory_limit = memory_share * total_memory
+  resource.setrlimit(resource.RLIMIT_AS, (memory_limit, -1L))
+
+
 def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
   """ Generates a trace.
 
@@ -150,25 +165,35 @@ class TraceTaskHandler(object):
     See the GenerateTrace() documentation for a description of the parameters
     and return values.
     """
-    self._logger.info('Starting external process for trace generation')
+    self._logger.info('Starting external process for trace generation.')
     failed_metadata = {'succeeded':False, 'url':url}
-    pool = multiprocessing.Pool(1)
+    failed = False
+    pool = multiprocessing.Pool(1, initializer=LimitMemory, initargs=(0.9,))
 
     apply_result = pool.apply_async(
         GenerateTrace,
         (url, emulate_device, emulate_network, filename, log_filename))
-    apply_result.wait(timeout=300)
+    pool.close()
+    apply_result.wait(timeout=180)
 
     if not apply_result.ready():
       self._logger.error('Process timeout for trace generation of URL: ' + url)
       self._failure_database.AddFailure('trace_process_timeout', url)
-      return failed_metadata
+      # Explicitly kill Chrome now, or pool.terminate() will hang.
+      controller.LocalChromeController.KillChromeProcesses()
+      pool.terminate()
+      failed = True
 
     if not apply_result.successful():
       self._logger.error('Process failure for trace generation of URL: ' + url)
       self._failure_database.AddFailure('trace_process_error', url)
-      return failed_metadata
+      failed = True
 
+    self._logger.info('Cleaning up external process.')
+    pool.join()
+
+    if failed:
+      return failed_metadata
     return apply_result.get()
 
   def _HandleTraceGenerationResults(self, local_filename, log_filename,
diff --git a/loading/controller.py b/loading/controller.py
index 7ea59a0..3311db2 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -24,6 +24,8 @@ import tempfile
 import time
 import traceback
 
+import psutil
+
 import chrome_cache
 import common_util
 import device_setup
@@ -420,6 +422,29 @@ class LocalChromeController(ChromeControllerBase):
     if self._using_temp_profile_dir:
       shutil.rmtree(self._profile_dir)
 
+  @staticmethod
+  def KillChromeProcesses():
+    """Kills all the running instances of Chrome.
+
+    Returns: (int) The number of processes that were killed.
+    """
+    killed_count = 0
+    chrome_path = OPTIONS.LocalBinary('chrome')
+    for process in psutil.process_iter():
+      try:
+        if process.exe() == chrome_path:
+          process.terminate()
+          killed_count += 1
+          try:
+            process.wait(timeout=10)
+          except psutil.TimeoutExpired:
+            process.kill()
+      except psutil.AccessDenied:
+        pass
+      except psutil.NoSuchProcess:
+        pass
+    return killed_count
+
   def SetChromeEnvOverride(self, env):
     """Set the environment for Chrome.
 
@@ -431,6 +456,11 @@ class LocalChromeController(ChromeControllerBase):
   @contextlib.contextmanager
   def Open(self):
     """Overridden connection creation."""
+    # Kill all existing Chrome instances.
+    killed_count = LocalChromeController.KillChromeProcesses()
+    if killed_count > 0:
+      logging.warning('Killed existing Chrome instance.')
+
     chrome_cmd = [OPTIONS.LocalBinary('chrome')]
     chrome_cmd.extend(self._GetChromeArguments())
     # Force use of simple cache.
@@ -445,7 +475,7 @@ class LocalChromeController(ChromeControllerBase):
         tempfile.NamedTemporaryFile(prefix="chrome_controller_", suffix='.log')
     chrome_process = None
     try:
-      chrome_env_override = self._chrome_env_override or {}
+      chrome_env_override = self._chrome_env_override.copy() or {}
       if self._wpr_attributes:
         chrome_env_override.update(self._wpr_attributes.chrome_env_override)
 
@@ -490,7 +520,10 @@ class LocalChromeController(ChromeControllerBase):
         sys.stderr.write(open(tmp_log.name).read())
       del tmp_log
       if chrome_process:
-        chrome_process.kill()
+        try:
+          chrome_process.kill()
+        except OSError:
+          pass  # Chrome is already dead.
 
   def ResetBrowserState(self):
     """Override for chrome state reseting."""

commit 642d01a0f2f88dbf386d5d30d3ddde6e84d8439b
Author: droger <droger@chromium.org>
Date:   Wed May 25 03:15:47 2016 -0700

    tools/android/loading Delete trace files after upload.
    
    Trace files are often 100 mb large and can even reach 1Gb.
    Delete them after uploading to avoid filling up the hard disk.
    
    Review-Url: https://codereview.chromium.org/2010493003
    Cr-Original-Commit-Position: refs/heads/master@{#395853}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9829b4139700e2b3e52c8853a75def303de2400f

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index 1a9cf9a..aa66514 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -204,6 +204,7 @@ class TraceTaskHandler(object):
       self._logger.debug('Uploading: %s' % remote_trace_location)
       self._google_storage_accessor.UploadFile(local_filename,
                                                remote_trace_location)
+      os.remove(local_filename)  # The trace may be very large.
     else:
       self._logger.warning('No trace found at: ' + local_filename)
 

commit 8183033f3e436b938d37bcb3eb2f7a53cc461be5
Author: gabadie <gabadie@chromium.org>
Date:   Tue May 24 10:47:29 2016 -0700

    sandwich: Implement a separate task builder for State-While-Revalidate
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2001383003
    Cr-Original-Commit-Position: refs/heads/master@{#395632}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 09eee579aec7a8014ff8bec5b385ff8fb3a8e163

diff --git a/loading/sandwich.py b/loading/sandwich.py
index d7485d2..86e8c74 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -36,6 +36,7 @@ import options
 import sandwich_metrics
 import sandwich_misc
 import sandwich_runner
+import sandwich_swr
 import sandwich_task_builder
 import task_manager
 from trace_test.webserver_test import WebServer
@@ -118,8 +119,10 @@ def _ArgumentParser():
                           dest='wpr_archive_path',
                           help='WebPageReplay archive to use, instead of '
                                'generating one.')
-  run_parser.add_argument('--url-repeat', default=1, type=int,
+  run_parser.add_argument('-r', '--url-repeat', default=1, type=int,
                           help='How many times to repeat the urls.')
+  run_parser.add_argument('--swr-benchmark', action='store_true',
+                          help='Run the Stale-While-Revalidate benchmarks.')
 
   return parser
 
@@ -149,43 +152,57 @@ def _RecordWebServerTestTrace(args):
   return 0
 
 
-def _GenerateNoStatePrefetchBenchmarkTasks(args, url, output_subdirectory):
-  builder = sandwich_task_builder.SandwichTaskBuilder(
+def _GenerateBenchmarkTasks(args, url, output_subdirectory):
+  MAIN_TRANSFORMER_LIST_NAME = 'no-network-emulation'
+  common_builder = sandwich_task_builder.SandwichCommonBuilder(
       android_device=_GetAndroidDeviceFromArgs(args),
       url=url,
       output_directory=args.output,
       output_subdirectory=output_subdirectory)
   if args.wpr_archive_path:
-    builder.OverridePathToWprArchive(args.wpr_archive_path)
+    common_builder.OverridePathToWprArchive(args.wpr_archive_path)
   else:
-    builder.PopulateWprRecordingTask()
-  builder.PopulateCommonPipelines()
+    common_builder.PopulateWprRecordingTask()
 
   def MainTransformer(runner):
     runner.record_video = _SPEED_INDEX_MEASUREMENT in args.optional_measures
     runner.record_memory_dumps = _MEMORY_MEASUREMENT in args.optional_measures
     runner.repeat = args.url_repeat
 
-  transformer_list_name = 'no-network-emulation'
-  builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
-                                transformer_list_name,
-                                transformer_list=[MainTransformer])
-  builder.PopulateLoadBenchmark(sandwich_misc.FULL_CACHE_DISCOVERER,
-                                transformer_list_name,
+  if not args.swr_benchmark:
+    builder = sandwich_task_builder.PrefetchBenchmarkBuilder(common_builder)
+    builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
+                                  MAIN_TRANSFORMER_LIST_NAME,
+                                  transformer_list=[MainTransformer])
+    builder.PopulateLoadBenchmark(sandwich_misc.FULL_CACHE_DISCOVERER,
+                                  MAIN_TRANSFORMER_LIST_NAME,
+                                  transformer_list=[MainTransformer])
+    if args.gen_full:
+      for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
+        transformer_list_name = network_condition.lower()
+        network_transformer = \
+            sandwich_task_builder.NetworkSimulationTransformer(
+                network_condition)
+        transformer_list = [MainTransformer, network_transformer]
+        for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
+          if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
+            continue
+          builder.PopulateLoadBenchmark(subresource_discoverer,
+              transformer_list_name, transformer_list)
+  else:
+    builder = sandwich_swr.StaleWhileRevalidateBenchmarkBuilder(common_builder)
+    for enable_swr in [False, True]:
+      builder.PopulateBenchmark(enable_swr, MAIN_TRANSFORMER_LIST_NAME,
                                 transformer_list=[MainTransformer])
-
-  if args.gen_full:
-    for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
-      transformer_list_name = network_condition.lower()
-      network_transformer = \
-          sandwich_task_builder.NetworkSimulationTransformer(network_condition)
-      transformer_list = [MainTransformer, network_transformer]
-      for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
-        if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
-          continue
-        builder.PopulateLoadBenchmark(subresource_discoverer,
-            transformer_list_name, transformer_list)
-  return builder.default_final_tasks
+      for network_condition in ['Regular3G', 'Regular2G']:
+        transformer_list_name = network_condition.lower()
+        network_transformer = \
+            sandwich_task_builder.NetworkSimulationTransformer(
+                network_condition)
+        transformer_list = [MainTransformer, network_transformer]
+        builder.PopulateBenchmark(enable_swr, transformer_list_name,
+                                  transformer_list)
+  return common_builder.default_final_tasks
 
 
 def _RunAllMain(args):
@@ -199,7 +216,7 @@ def _RunAllMain(args):
     output_subdirectory = '{}.{}'.format(domain, domain_times_encountered)
     domain_times_encountered_per_domain[domain] = domain_times_encountered + 1
     default_final_tasks.extend(
-        _GenerateNoStatePrefetchBenchmarkTasks(args, url, output_subdirectory))
+        _GenerateBenchmarkTasks(args, url, output_subdirectory))
   return task_manager.ExecuteWithCommandLine(args, default_final_tasks)
 
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index fff96bf..610d594 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -215,7 +215,7 @@ def _ExtractBenchmarkStatistics(benchmark_setup, loading_trace):
   Args:
     benchmark_setup: benchmark_setup: dict representing the benchmark setup
         JSON. The JSON format is according to:
-            SandwichTaskBuilder.PopulateLoadBenchmark.SetupBenchmark.
+            PrefetchBenchmarkBuilder.PopulateLoadBenchmark.SetupBenchmark.
     loading_trace: loading_trace_module.LoadingTrace.
 
   Returns:
@@ -298,7 +298,7 @@ def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
   Args:
     benchmark_setup: benchmark_setup: dict representing the benchmark setup
         JSON. The JSON format is according to:
-            SandwichTaskBuilder.PopulateLoadBenchmark.SetupBenchmark.
+            PrefetchBenchmarkBuilder.PopulateLoadBenchmark.SetupBenchmark.
     run_directory_path: Path of the run directory.
 
   Returns:
@@ -315,8 +315,9 @@ def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
   }
   run_metrics.update(_ExtractDefaultMetrics(loading_trace))
   run_metrics.update(_ExtractMemoryMetrics(loading_trace))
-  run_metrics.update(
-      _ExtractBenchmarkStatistics(benchmark_setup, loading_trace))
+  if benchmark_setup:
+    run_metrics.update(
+        _ExtractBenchmarkStatistics(benchmark_setup, loading_trace))
   video_path = os.path.join(run_directory_path, 'video.mp4')
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
@@ -347,7 +348,9 @@ def ExtractMetricsFromRunnerOutputDirectory(benchmark_setup_path,
   Returns:
     List of dictionaries.
   """
-  benchmark_setup = json.load(open(benchmark_setup_path))
+  benchmark_setup = None
+  if benchmark_setup_path:
+    benchmark_setup = json.load(open(benchmark_setup_path))
   assert os.path.isdir(output_directory_path)
   metrics = []
   for node_name in os.listdir(output_directory_path):
@@ -361,7 +364,9 @@ def ExtractMetricsFromRunnerOutputDirectory(benchmark_setup_path,
     run_metrics = _ExtractMetricsFromRunDirectory(
         benchmark_setup, run_directory_path)
     run_metrics['repeat_id'] = repeat_id
-    assert set(run_metrics.keys()) == set(CSV_FIELD_NAMES)
+    # TODO(gabadie): Make common metrics extraction with benchmark type
+    # specific CSV column.
+    # assert set(run_metrics.keys()) == set(CSV_FIELD_NAMES)
     metrics.append(run_metrics)
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich runner ' +
                             'output directory.').format(output_directory_path)
diff --git a/loading/sandwich_swr.py b/loading/sandwich_swr.py
new file mode 100644
index 0000000..8d92c63
--- /dev/null
+++ b/loading/sandwich_swr.py
@@ -0,0 +1,112 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import csv
+import os
+import shutil
+
+import common_util
+import sandwich_metrics
+import sandwich_runner
+import task_manager
+
+
+class StaleWhileRevalidateBenchmarkBuilder(task_manager.Builder):
+  """A builder for a graph of tasks for Stale-While-Revalidate study benchmarks.
+  """
+
+  def __init__(self, common_builder):
+    task_manager.Builder.__init__(self,
+                                  common_builder.output_directory,
+                                  common_builder.output_subdirectory)
+    self._common_builder = common_builder
+    self._worstcase_cache_task = None
+    self._swr_cache_task = None
+    self._PopulateCommonPipelines()
+
+  def _PopulateCommonPipelines(self):
+    """Creates necessary tasks to produce initial cache archives.
+
+    Here is the full dependency tree for the returned task:
+    common/swr-patched-cache.zip
+      depends on: common/worstcase-patched-cache.zip
+        depends on: common/webpages.wpr
+    """
+    @self.RegisterTask('common/original-cache.zip',
+                       dependencies=[self._common_builder.original_wpr_task])
+    def BuildOriginalCache():
+      runner = self._common_builder.CreateSandwichRunner()
+      runner.wpr_archive_path = self._common_builder.original_wpr_task.path
+      runner.cache_archive_path = BuildOriginalCache.path
+      runner.cache_operation = sandwich_runner.CacheOperation.SAVE
+      runner.output_dir = BuildOriginalCache.path[:-4] + '-run'
+      runner.Run()
+
+    @self.RegisterTask('common/worstcase-patched-cache.zip',
+                       dependencies=[BuildOriginalCache])
+    def BuildWorstCaseCache():
+      # TODO(gabadie): Patch cache-control's max-age=0 if any.
+      shutil.copyfile(BuildOriginalCache.path, BuildWorstCaseCache.path)
+
+    @self.RegisterTask('common/swr-patched-cache.zip', [BuildWorstCaseCache])
+    def BuildSWRCache():
+      # TODO(gabadie): Patch cache-control's stale-while-revalidate=1 year if
+      # any max-age.
+      shutil.copyfile(BuildWorstCaseCache.path, BuildSWRCache.path)
+
+    self._worstcase_cache_task = BuildWorstCaseCache
+    self._swr_cache_task = BuildSWRCache
+
+  def PopulateBenchmark(self, enable_swr, transformer_list_name,
+                        transformer_list):
+    """Populate benchmarking tasks.
+
+    Args:
+      enable_swr: Enable SWR patching or not.
+      transformer_list_name: A string describing the transformers, will be used
+          in Task names (prefer names without spaces and special characters).
+      transformer_list: An ordered list of function that takes an instance of
+          SandwichRunner as parameter, would be applied immediately before
+          SandwichRunner.Run() in the given order.
+
+    Here is the full dependency of the added tree for the returned task:
+    <transformer_list_name>/{swr,worstcase}-metrics.csv
+      depends on: <transformer_list_name>/{swr,worstcase}-run/
+        depends on: some tasks saved by PopulateCommonPipelines()
+    """
+    task_prefix = os.path.join(transformer_list_name, '')
+    if enable_swr:
+      cache_task = self._swr_cache_task
+      task_prefix += 'swr'
+    else:
+      cache_task = self._worstcase_cache_task
+      task_prefix += 'worstcase'
+
+    @self.RegisterTask(task_prefix + '-run/', [cache_task])
+    def RunBenchmark():
+      runner = self._common_builder.CreateSandwichRunner()
+      for transformer in transformer_list:
+        transformer(runner)
+      runner.wpr_archive_path = self._common_builder.original_wpr_task.path
+      runner.wpr_out_log_path = os.path.join(
+          RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
+      runner.cache_archive_path = cache_task.path
+      runner.cache_operation = sandwich_runner.CacheOperation.PUSH
+      runner.output_dir = RunBenchmark.path
+      runner.Run()
+
+    @self.RegisterTask(task_prefix + '-metrics.csv', [RunBenchmark])
+    def ExtractMetrics():
+      trace_metrics_list = \
+          sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
+              None, RunBenchmark.path)
+      trace_metrics_list.sort(key=lambda e: e['repeat_id'])
+      with open(ExtractMetrics.path, 'w') as csv_file:
+        writer = csv.DictWriter(csv_file,
+                                fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
+        writer.writeheader()
+        for trace_metrics in trace_metrics_list:
+          writer.writerow(trace_metrics)
+
+    self._common_builder.default_final_tasks.append(ExtractMetrics)
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index d02141c..26dba8c 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -33,7 +33,7 @@ def NetworkSimulationTransformer(network_condition):
   return Transformer
 
 
-class SandwichTaskBuilder(task_manager.Builder):
+class SandwichCommonBuilder(task_manager.Builder):
   """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
   """
 
@@ -51,19 +51,11 @@ class SandwichTaskBuilder(task_manager.Builder):
     task_manager.Builder.__init__(self, output_directory, output_subdirectory)
     self._android_device = android_device
     self._url = url
-    self._default_final_tasks = []
+    self.default_final_tasks = []
 
-    self._original_wpr_task = None
-    self._patched_wpr_task = None
-    self._reference_cache_task = None
-    self._subresources_for_urls_run_task = None
-    self._subresources_for_urls_task = None
+    self.original_wpr_task = None
 
-  @property
-  def default_final_tasks(self):
-      return self._default_final_tasks
-
-  def _CreateSandwichRunner(self):
+  def CreateSandwichRunner(self):
     """Create a runner for non benchmark purposes."""
     runner = sandwich_runner.SandwichRunner()
     runner.url = self._url
@@ -76,7 +68,7 @@ class SandwichTaskBuilder(task_manager.Builder):
     Args:
       original_wpr_path: Path of the original WPR archive to be used.
     """
-    self._original_wpr_task = \
+    self.original_wpr_task = \
         self.CreateStaticTask('common/webpages.wpr', original_wpr_path)
 
   def PopulateWprRecordingTask(self):
@@ -84,14 +76,30 @@ class SandwichTaskBuilder(task_manager.Builder):
     @self.RegisterTask('common/webpages.wpr')
     def BuildOriginalWpr():
       common_util.EnsureParentDirectoryExists(BuildOriginalWpr.path)
-      runner = self._CreateSandwichRunner()
+      runner = self.CreateSandwichRunner()
       runner.wpr_archive_path = BuildOriginalWpr.path
       runner.wpr_record = True
       runner.Run()
 
-    self._original_wpr_task = BuildOriginalWpr
+    self.original_wpr_task = BuildOriginalWpr
+
+
+class PrefetchBenchmarkBuilder(task_manager.Builder):
+  """A builder for a graph of tasks for NoState-Prefetch emulated benchmarks."""
+
+  def __init__(self, common_builder):
+    task_manager.Builder.__init__(self,
+                                  common_builder.output_directory,
+                                  common_builder.output_subdirectory)
+    self._common_builder = common_builder
 
-  def PopulateCommonPipelines(self):
+    self._patched_wpr_task = None
+    self._reference_cache_task = None
+    self._subresources_for_urls_run_task = None
+    self._subresources_for_urls_task = None
+    self._PopulateCommonPipelines()
+
+  def _PopulateCommonPipelines(self):
     """Creates necessary tasks to produce initial cache archive.
 
     Also creates a task for producing a json file with a mapping of URLs to
@@ -106,19 +114,18 @@ class SandwichTaskBuilder(task_manager.Builder):
       depends on: common/urls-resources.json
         depends on: common/urls-resources-run/
           depends on: common/webpages.wpr
-
-    Returns:
-      The last task of the pipeline.
     """
-    @self.RegisterTask('common/webpages-patched.wpr', [self._original_wpr_task])
+    @self.RegisterTask('common/webpages-patched.wpr',
+                       dependencies=[self._common_builder.original_wpr_task])
     def BuildPatchedWpr():
       common_util.EnsureParentDirectoryExists(BuildPatchedWpr.path)
-      shutil.copyfile(self._original_wpr_task.path, BuildPatchedWpr.path)
+      shutil.copyfile(
+          self._common_builder.original_wpr_task.path, BuildPatchedWpr.path)
       sandwich_misc.PatchWpr(BuildPatchedWpr.path)
 
     @self.RegisterTask('common/original-cache.zip', [BuildPatchedWpr])
     def BuildOriginalCache():
-      runner = self._CreateSandwichRunner()
+      runner = self._common_builder.CreateSandwichRunner()
       runner.wpr_archive_path = BuildPatchedWpr.path
       runner.cache_archive_path = BuildOriginalCache.path
       runner.cache_operation = sandwich_runner.CacheOperation.SAVE
@@ -134,10 +141,10 @@ class SandwichTaskBuilder(task_manager.Builder):
           original_cache_trace_path, BuildPatchedCache.path)
 
     @self.RegisterTask('common/subresources-for-urls-run/',
-                       dependencies=[self._original_wpr_task])
+                       dependencies=[self._common_builder.original_wpr_task])
     def UrlsResourcesRun():
-      runner = self._CreateSandwichRunner()
-      runner.wpr_archive_path = self._original_wpr_task.path
+      runner = self._common_builder.CreateSandwichRunner()
+      runner.wpr_archive_path = self._common_builder.original_wpr_task.path
       runner.cache_operation = sandwich_runner.CacheOperation.CLEAR
       runner.output_dir = UrlsResourcesRun.path
       runner.Run()
@@ -160,8 +167,7 @@ class SandwichTaskBuilder(task_manager.Builder):
     self._subresources_for_urls_run_task = UrlsResourcesRun
     self._subresources_for_urls_task = ListUrlsResources
 
-    self._default_final_tasks.append(ValidatePatchedCache)
-    return ValidatePatchedCache
+    self._common_builder.default_final_tasks.append(ValidatePatchedCache)
 
   def PopulateLoadBenchmark(self, subresource_discoverer,
                             transformer_list_name, transformer_list):
@@ -182,10 +188,6 @@ class SandwichTaskBuilder(task_manager.Builder):
           depends on: some tasks saved by PopulateCommonPipelines()
           depends on: common/<subresource_discoverer>-setup.json
             depends on: some tasks saved by PopulateCommonPipelines()
-
-    Returns:
-      task_manager.Task for
-          <transformer_list_name>/<subresource_discoverer>-metrics.csv
     """
     assert subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS
     assert 'common' not in sandwich_misc.SUBRESOURCE_DISCOVERERS
@@ -222,7 +224,7 @@ class SandwichTaskBuilder(task_manager.Builder):
     @self.RegisterTask(task_prefix + '-run/',
                        dependencies=[BuildBenchmarkCacheArchive])
     def RunBenchmark():
-      runner = self._CreateSandwichRunner()
+      runner = self._common_builder.CreateSandwichRunner()
       for transformer in transformer_list:
         transformer(runner)
       runner.wpr_archive_path = self._patched_wpr_task.path
@@ -249,5 +251,4 @@ class SandwichTaskBuilder(task_manager.Builder):
         for trace_metrics in trace_metrics_list:
           writer.writerow(trace_metrics)
 
-    self._default_final_tasks.append(ExtractMetrics)
-    return ExtractMetrics
+    self._common_builder.default_final_tasks.append(ExtractMetrics)
diff --git a/loading/task_manager.py b/loading/task_manager.py
index afae86b..0e5e72c 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -118,7 +118,7 @@ class Builder(object):
       output_subdirectory: Subdirectory to put all created tasks in or None.
     """
     self.output_directory = output_directory
-    self._output_subdirectory = output_subdirectory
+    self.output_subdirectory = output_subdirectory
     self._tasks = {}
 
   def CreateStaticTask(self, task_name, path):
@@ -182,8 +182,8 @@ class Builder(object):
     return InnerAddTaskWithNewPath
 
   def _RebaseTaskName(self,  task_name):
-    if self._output_subdirectory:
-      return os.path.join(self._output_subdirectory, task_name)
+    if self.output_subdirectory:
+      return os.path.join(self.output_subdirectory, task_name)
     return task_name
 
 

commit 2cd47a4f4daf97385048275996c1b506355112fa
Author: lizeb <lizeb@chromium.org>
Date:   Tue May 24 08:38:15 2016 -0700

    clovis: Add CPU busyness to the report.
    
    Review-Url: https://codereview.chromium.org/2007063002
    Cr-Original-Commit-Position: refs/heads/master@{#395604}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 90f768fcf9e15cb833094fe0a456200066426107

diff --git a/loading/report.py b/loading/report.py
index a32924c..0004d06 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -7,6 +7,7 @@
 When executed as a script, takes a trace filename and print the report.
 """
 
+from activity_lens import ActivityLens
 from content_classification_lens import ContentClassificationLens
 from loading_graph_view import LoadingGraphView
 import loading_trace
@@ -32,11 +33,13 @@ class LoadingReport(object):
         FirstContentfulPaintLens(self.trace).SatisfiedMs())
     self._significant_paint_msec = (
         FirstSignificantPaintLens(self.trace).SatisfiedMs())
+
     navigation_start_events = trace.tracing_track.GetMatchingEvents(
         'blink.user_timing', 'navigationStart')
     self._navigation_start_msec = min(
         e.start_msec for e in navigation_start_events)
     self._load_end_msec = self._ComputePlt(trace)
+
     network_lens = NetworkActivityLens(self.trace)
     if network_lens.total_download_bytes > 0:
       self._contentful_byte_frac = (
@@ -57,6 +60,25 @@ class LoadingReport(object):
     self._significant_inversion = graph.GetInversionsAtTime(
         self._significant_paint_msec)
     self._transfer_size = metrics.TotalTransferSize(trace)[1]
+    self._cpu_busyness = self._ComputeCpuBusyness(trace)
+
+  def _ComputeCpuBusyness(self, trace):
+    activity = ActivityLens(trace)
+    load_start = self._navigation_start_msec
+    load_end = self._load_end_msec
+    contentful = self._contentful_paint_msec
+    significant = self._significant_paint_msec
+
+    return {
+        'activity_load_frac': (
+            activity.MainRendererThreadBusyness(load_start, load_end)
+            / float(load_end - load_start)),
+        'activity_contentful_paint_frac': (
+            activity.MainRendererThreadBusyness(load_start, contentful)
+            / float(contentful - load_start)),
+        'activity_significant_paint_frac': (
+            activity.MainRendererThreadBusyness(load_start, significant)
+            / float(significant - load_start))}
 
   def GenerateReport(self):
     """Returns a report as a dict."""
@@ -80,6 +102,7 @@ class LoadingReport(object):
                                   else None),
         'transfer_size': self._transfer_size}
     report.update(self._ad_report)
+    report.update(self._cpu_busyness)
     return report
 
   @classmethod
@@ -139,7 +162,7 @@ def _Main(args):
   tracking_rules = open(args[3]).readlines()
   report = LoadingReport.FromTraceFilename(
       trace_filename, ad_rules, tracking_rules)
-  print json.dumps(report.GenerateReport(), indent=2)
+  print json.dumps(report.GenerateReport(), indent=2, sort_keys=True)
 
 
 if __name__ == '__main__':
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 3c6b216..03cdc9a 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -23,6 +23,8 @@ class LoadingReportTestCase(unittest.TestCase):
   _MAIN_FRAME_ID = 1
   _FIRST_REQUEST_DATA_LENGTH = 128
   _SECOND_REQUEST_DATA_LENGTH = 1024
+  _TOPLEVEL_EVENT_OFFSET = 10
+  _TOPLEVEL_EVENT_DURATION = 100
 
   def setUp(self):
     self.trace_creator = test_utils.TraceCreator()
@@ -36,30 +38,36 @@ class LoadingReportTestCase(unittest.TestCase):
     self.requests[1].encoded_data_length = self._SECOND_REQUEST_DATA_LENGTH
 
     self.trace_events = [
+        {'args': {'name': 'CrRendererMain'}, 'cat': '__metadata',
+         'name': 'thread_name', 'ph': 'M', 'pid': 1, 'tid': 1, 'ts': 0},
         {'ts': self._NAVIGATION_START_TIME * self.MILLI_TO_MICRO, 'ph': 'R',
-         'cat': 'blink.user_timing',
+         'cat': 'blink.user_timing', 'pid': 1, 'tid': 1,
          'name': 'navigationStart',
          'args': {'frame': 1}},
         {'ts': self._LOAD_END_TIME * self.MILLI_TO_MICRO, 'ph': 'I',
-         'cat': 'devtools.timeline',
+         'cat': 'devtools.timeline', 'pid': 1, 'tid': 1,
          'name': 'MarkLoad',
          'args': {'data': {'isMainFrame': True}}},
         {'ts': self._CONTENTFUL_PAINT * self.MILLI_TO_MICRO, 'ph': 'I',
-         'cat': 'blink.user_timing',
+         'cat': 'blink.user_timing', 'pid': 1, 'tid': 1,
          'name': 'firstContentfulPaint',
          'args': {'frame': self._MAIN_FRAME_ID}},
         {'ts': self._TEXT_PAINT * self.MILLI_TO_MICRO, 'ph': 'I',
-         'cat': 'blink.user_timing',
+         'cat': 'blink.user_timing', 'pid': 1, 'tid': 1,
          'name': 'firstPaint',
          'args': {'frame': self._MAIN_FRAME_ID}},
         {'ts': 90 * self.MILLI_TO_MICRO, 'ph': 'I',
-         'cat': 'blink',
+         'cat': 'blink', 'pid': 1, 'tid': 1,
          'name': 'FrameView::synchronizedPaint'},
         {'ts': self._SIGNIFICANT_PAINT * self.MILLI_TO_MICRO, 'ph': 'I',
-         'cat': 'foobar', 'name': 'biz',
+         'cat': 'foobar', 'name': 'biz', 'pid': 1, 'tid': 1,
          'args': {'counters': {
-             'LayoutObjectsThatHadNeverHadLayout': 10
-         }}}]
+             'LayoutObjectsThatHadNeverHadLayout': 10}}},
+        {'ts': (self._NAVIGATION_START_TIME - self._TOPLEVEL_EVENT_OFFSET)
+         * self.MILLI_TO_MICRO,
+         'pid': 1, 'tid': 1, 'ph': 'X',
+         'dur': self._TOPLEVEL_EVENT_DURATION * self.MILLI_TO_MICRO,
+         'cat': 'toplevel', 'name': 'MessageLoop::RunTask'}]
 
   def _MakeTrace(self):
     trace = self.trace_creator.CreateTrace(
@@ -131,6 +139,19 @@ class LoadingReportTestCase(unittest.TestCase):
         self._FIRST_REQUEST_DATA_LENGTH + metrics.HTTP_OK_LENGTH,
         loading_report['ad_or_tracking_initiated_transfer_size'])
 
+  def testThreadBusyness(self):
+    loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
+    self.assertAlmostEqual(
+        1., loading_report['activity_significant_paint_frac'])
+    self.assertAlmostEqual(
+        float(self._TOPLEVEL_EVENT_DURATION - self._TOPLEVEL_EVENT_OFFSET)
+        / (self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME),
+        loading_report['activity_contentful_paint_frac'])
+    self.assertAlmostEqual(
+        float(self._TOPLEVEL_EVENT_DURATION - self._TOPLEVEL_EVENT_OFFSET)
+        / (self._LOAD_END_TIME - self._NAVIGATION_START_TIME),
+        loading_report['activity_load_frac'])
+
 
 if __name__ == '__main__':
   unittest.main()

commit dd8643af3fcacd59a59fae1d5ed0760eae869dbd
Author: gabadie <gabadie@chromium.org>
Date:   Tue May 24 08:29:21 2016 -0700

    tools/android/loading: Make task_manager aware of task orders.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2007073002
    Cr-Original-Commit-Position: refs/heads/master@{#395602}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c3cafd121859df6f95fa551092c1c6a1d3490afa

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 2f681d6..d7485d2 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -185,13 +185,12 @@ def _GenerateNoStatePrefetchBenchmarkTasks(args, url, output_subdirectory):
           continue
         builder.PopulateLoadBenchmark(subresource_discoverer,
             transformer_list_name, transformer_list)
-  return builder.tasks.values(), builder.default_final_tasks
+  return builder.default_final_tasks
 
 
 def _RunAllMain(args):
   urls = ReadUrlsFromCorpus(args.corpus)
   domain_times_encountered_per_domain = {}
-  tasks = []
   default_final_tasks = []
   for url in urls:
     domain = '.'.join(urlparse(url).netloc.split('.')[-2:])
@@ -199,11 +198,9 @@ def _RunAllMain(args):
         domain, 0)
     output_subdirectory = '{}.{}'.format(domain, domain_times_encountered)
     domain_times_encountered_per_domain[domain] = domain_times_encountered + 1
-    gen_tasks, gen_default_final_tasks = \
-        _GenerateNoStatePrefetchBenchmarkTasks(args, url, output_subdirectory)
-    tasks.extend(gen_tasks)
-    default_final_tasks.extend(gen_default_final_tasks)
-  return task_manager.ExecuteWithCommandLine(args, tasks, default_final_tasks)
+    default_final_tasks.extend(
+        _GenerateNoStatePrefetchBenchmarkTasks(args, url, output_subdirectory))
+  return task_manager.ExecuteWithCommandLine(args, default_final_tasks)
 
 
 def main(command_line_args):
diff --git a/loading/task_manager.py b/loading/task_manager.py
index 93f14cf..afae86b 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -60,7 +60,6 @@ Example:
 
 
 import argparse
-import collections
 import logging
 import os
 import subprocess
@@ -120,7 +119,7 @@ class Builder(object):
     """
     self.output_directory = output_directory
     self._output_subdirectory = output_subdirectory
-    self.tasks = {}
+    self._tasks = {}
 
   def CreateStaticTask(self, task_name, path):
     """Creates and returns a new static task."""
@@ -128,10 +127,10 @@ class Builder(object):
     if not os.path.exists(path):
       raise TaskError('Error while creating task {}: File not found: {}'.format(
           task_name, path))
-    if task_name in self.tasks:
+    if task_name in self._tasks:
       raise TaskError('Task {} already exists.'.format(task_name))
     task = Task(task_name, path, [], None)
-    self.tasks[task_name] = task
+    self._tasks[task_name] = task
     return task
 
   # Caution:
@@ -168,17 +167,17 @@ class Builder(object):
     task_name = self._RebaseTaskName(task_name)
     dependencies = dependencies or []
     def InnerAddTaskWithNewPath(recipe):
-      if task_name in self.tasks:
+      if task_name in self._tasks:
         if not merge:
           raise TaskError('Task {} already exists.'.format(task_name))
-        task = self.tasks[task_name]
+        task = self._tasks[task_name]
         if task.IsStatic():
           raise TaskError('Should not merge dynamic task {} with the already '
                           'existing static one.'.format(task_name))
         return task
       task_path = os.path.join(self.output_directory, task_name)
       task = Task(task_name, task_path, dependencies, recipe)
-      self.tasks[task_name] = task
+      self._tasks[task_name] = task
       return task
     return InnerAddTaskWithNewPath
 
@@ -341,12 +340,11 @@ def _GetCommandLineArgumentsStr(final_task_regexes, frozen_tasks):
   return subprocess.list2cmdline(arguments)
 
 
-def ExecuteWithCommandLine(args, tasks, default_final_tasks):
+def ExecuteWithCommandLine(args, default_final_tasks):
   """Helper to execute tasks using command line arguments.
 
   Args:
     args: Command line argument parsed with CommandLineParser().
-    tasks: Unordered list of tasks to publish to command line regexes.
     default_final_tasks: Default final tasks if there is no -r command
       line arguments.
 
@@ -358,6 +356,12 @@ def ExecuteWithCommandLine(args, tasks, default_final_tasks):
   run_regexes = [common_util.VerboseCompileRegexOrAbort(e)
                    for e in args.run_regexes]
 
+  # Traverse the graph in the normal execution order starting from
+  # |default_final_tasks| in case of command line regex selection.
+  tasks = []
+  if frozen_regexes or run_regexes:
+    tasks = GenerateScenario(default_final_tasks, frozen_tasks=set())
+
   # Lists frozen tasks
   frozen_tasks = set()
   if frozen_regexes:
@@ -371,11 +375,11 @@ def ExecuteWithCommandLine(args, tasks, default_final_tasks):
   final_tasks = default_final_tasks
   if run_regexes:
     final_tasks = []
-    for task in tasks:
-      for regex in run_regexes:
+    # Order of run regexes prevails on the traversing order of tasks.
+    for regex in run_regexes:
+      for task in tasks:
         if regex.search(task.name):
           final_tasks.append(task)
-          break
 
   # Create the scenario.
   scenario = GenerateScenario(final_tasks, frozen_tasks)
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index d9990ac..cd18619 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -162,24 +162,24 @@ class BuilderTest(TaskManagerTestCase):
     builder = task_manager.Builder(self.output_directory, 'subdir')
 
     builder.CreateStaticTask('hello.py', __file__)
-    self.assertIn('subdir/hello.py', builder.tasks)
-    self.assertNotIn('hello.py', builder.tasks)
+    self.assertIn('subdir/hello.py', builder._tasks)
+    self.assertNotIn('hello.py', builder._tasks)
 
     builder.CreateStaticTask('subdir/hello.py', __file__)
-    self.assertIn('subdir/subdir/hello.py', builder.tasks)
+    self.assertIn('subdir/subdir/hello.py', builder._tasks)
 
     @builder.RegisterTask('world.txt')
     def TaskA():
       pass
     del TaskA # unused
-    self.assertIn('subdir/world.txt', builder.tasks)
-    self.assertNotIn('hello.py', builder.tasks)
+    self.assertIn('subdir/world.txt', builder._tasks)
+    self.assertNotIn('hello.py', builder._tasks)
 
     @builder.RegisterTask('subdir/world.txt')
     def TaskB():
       pass
     del TaskB # unused
-    self.assertIn('subdir/subdir/world.txt', builder.tasks)
+    self.assertIn('subdir/subdir/world.txt', builder._tasks)
 
 
 class GenerateScenarioTest(TaskManagerTestCase):
@@ -337,6 +337,10 @@ class GenerateScenarioTest(TaskManagerTestCase):
 
 
 class CommandLineControlledExecutionTest(TaskManagerTestCase):
+  def setUp(self):
+    TaskManagerTestCase.setUp(self)
+    self.with_raise_exception_task = False
+
   def Execute(self, *command_line_args):
     builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('a')
@@ -355,17 +359,18 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     def TaskE():
       pass
     @builder.RegisterTask('raise_exception', dependencies=[TaskB])
-    def TaskF():
+    def RaiseExceptionTask():
       raise TestException('Expected error.')
 
     default_final_tasks = [TaskD, TaskE]
+    if self.with_raise_exception_task:
+      default_final_tasks.append(RaiseExceptionTask)
     parser = task_manager.CommandLineParser()
     cmd = ['-o', self.output_directory]
     cmd.extend([i for i in command_line_args])
     args = parser.parse_args(cmd)
     with EatStdoutAndStderr():
-      return task_manager.ExecuteWithCommandLine(
-          args, [TaskA, TaskB, TaskC, TaskD, TaskE, TaskF], default_final_tasks)
+      return task_manager.ExecuteWithCommandLine(args, default_final_tasks)
 
   def testSimple(self):
     self.assertEqual(0, self.Execute())
@@ -383,6 +388,7 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     self.assertEqual(0, self.Execute('-f', 'c'))
 
   def testTaskFailure(self):
+    self.with_raise_exception_task = True
     with self.assertRaisesRegexp(TestException, r'^Expected error\.$'):
       self.Execute('-e', 'raise_exception')
 

commit ea7ffa015fe5f50c42b808b58e73bd3a14afd762
Author: gabadie <gabadie@chromium.org>
Date:   Tue May 24 02:14:25 2016 -0700

    sandwich: Handle POST requests in sandwich_misc.py
    
    Post requests are cached but not reusable (crbug.com/610725).
    This CL handle them in benchmark and cache validation codes.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1971263002
    Cr-Original-Commit-Position: refs/heads/master@{#395551}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8142c7197db39d71166b331a6ca5a6982310b94f

diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 49084ae..172f4e4 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -5,6 +5,7 @@
 import logging
 import json
 import os
+import re
 from urlparse import urlparse
 
 import chrome_cache
@@ -39,6 +40,8 @@ SUBRESOURCE_DISCOVERERS = set([
   HTML_PRELOAD_SCANNER_DISCOVERER
 ])
 
+_UPLOAD_DATA_STREAM_REQUESTS_REGEX = re.compile(r'^\d+/(?P<url>.*)$')
+
 
 def PatchWpr(wpr_archive_path):
   """Patches a WPR archive to get all resources into the HTTP cache and avoid
@@ -135,15 +138,8 @@ def PatchCacheArchive(cache_archive_path, loading_trace_path,
       #
       # The fact that these entries are kept in the cache after closing Chrome
       # properly by closing the Chrome tab as the ChromeControler.SetSlowDeath()
-      # do is a known Chrome bug (crbug.com/610725).
-      #
-      # TODO(gabadie): Add support in ValidateCacheArchiveContent() and in
-      #   VerifyBenchmarkOutputDirectory() for POST requests to be known as
-      #   impossible to use from cache.
+      # do is known chrome bug (crbug.com/610725).
       if request.url not in cache_entries:
-        if request.method != 'POST':
-          raise RuntimeError('Unexpected method that is not found in cache.'
-                             ''.format(request.method))
         continue
       # Chrome prunes Set-Cookie from response headers before storing them in
       # disk cache. Also, it adds implicit "Vary: cookie" header to all redirect
@@ -231,7 +227,7 @@ def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
 
 
 class RequestOutcome:
-  All, ServedFromCache, NotServedFromCache = range(3)
+  All, ServedFromCache, NotServedFromCache, Post = range(4)
 
 
 def ListUrlRequests(trace, request_kind):
@@ -250,6 +246,9 @@ def ListUrlRequests(trace, request_kind):
     if (request_kind == RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
+    elif (request_kind == RequestOutcome.Post and
+        request_event.method.upper().strip() == 'POST'):
+      urls.add(request_event.url)
     elif (request_kind == RequestOutcome.NotServedFromCache and
         not request_event.from_disk_cache):
       urls.add(request_event.url)
@@ -290,6 +289,7 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
     logging.info('verifying %s from %s' % (trace.url, trace_path))
 
     effective_requests = ListUrlRequests(trace, RequestOutcome.All)
+    effective_post_requests = ListUrlRequests(trace, RequestOutcome.Post)
     effective_cached_requests = \
         ListUrlRequests(trace, RequestOutcome.ServedFromCache)
     effective_uncached_requests = \
@@ -305,8 +305,14 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
         unexpected_requests).union(missing_cached_requests)
     all_sent_url_requests.update(effective_uncached_requests)
 
+    # POST requests are known to be unable to use the cache.
+    expected_cached_requests.difference_update(effective_post_requests)
+    expected_uncached_requests.update(effective_post_requests)
+
     _PrintUrlSetComparison(original_requests, effective_requests,
                            'All resources')
+    _PrintUrlSetComparison(set(), effective_post_requests,
+                           'POST resources')
     _PrintUrlSetComparison(expected_cached_requests, effective_cached_requests,
                            'Cached resources')
     _PrintUrlSetComparison(expected_uncached_requests,
@@ -359,17 +365,38 @@ def ReadSubresourceFromRunnerOutputDir(runner_output_dir):
   return [url for url in url_set]
 
 
-def ValidateCacheArchiveContent(ref_urls, cache_archive_path):
+def ValidateCacheArchiveContent(cache_build_trace_path, cache_archive_path):
   """Validates a cache archive content.
 
   Args:
-    ref_urls: Reference list of urls.
+    cache_build_trace_path: Path of the generated trace at the cache build time.
     cache_archive_path: Cache archive's path to validate.
   """
   # TODO(gabadie): What's the best way of propagating errors happening in here?
   logging.info('lists cached urls from %s' % cache_archive_path)
   with common_util.TemporaryDirectory() as cache_directory:
     chrome_cache.UnzipDirectoryContent(cache_archive_path, cache_directory)
-    cached_urls = \
-        chrome_cache.CacheBackend(cache_directory, 'simple').ListKeys()
-  _PrintUrlSetComparison(set(ref_urls), set(cached_urls), 'cached resources')
+    cache_keys = set(
+        chrome_cache.CacheBackend(cache_directory, 'simple').ListKeys())
+  trace = LoadingTrace.FromJsonFile(cache_build_trace_path)
+  effective_requests = ListUrlRequests(trace, RequestOutcome.All)
+  effective_post_requests = ListUrlRequests(trace, RequestOutcome.Post)
+
+  upload_data_stream_cache_entry_keys = set()
+  upload_data_stream_requests = set()
+  for cache_entry_key in cache_keys:
+    match = _UPLOAD_DATA_STREAM_REQUESTS_REGEX.match(cache_entry_key)
+    if not match:
+      continue
+    upload_data_stream_cache_entry_keys.add(cache_entry_key)
+    upload_data_stream_requests.add(match.group('url'))
+
+  expected_cached_requests = effective_requests.difference(
+      effective_post_requests)
+  effective_cache_keys = cache_keys.difference(
+      upload_data_stream_cache_entry_keys)
+
+  _PrintUrlSetComparison(effective_post_requests, upload_data_stream_requests,
+                         'POST resources')
+  _PrintUrlSetComparison(expected_cached_requests, effective_cache_keys,
+                         'Cached resources')
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 0ec40ae..f8ca187 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -190,8 +190,8 @@ class SandwichRunner(object):
       except controller.ChromeControllerError as error:
         if not error.IsIntermittent():
           raise
-        if self.trace_output_directory is not None:
-          dump_path = os.path.join(self.trace_output_directory, str(run_id),
+        if self.output_dir is not None:
+          dump_path = os.path.join(self.output_dir, str(run_id),
                                    'error{}'.format(attempt_id))
           with open(dump_path, 'w') as dump_output:
             error.Dump(dump_output)
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index ca3d481..d02141c 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -125,13 +125,13 @@ class SandwichTaskBuilder(task_manager.Builder):
       runner.output_dir = BuildOriginalCache.run_path
       runner.Run()
     BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
+    original_cache_trace_path = os.path.join(
+        BuildOriginalCache.run_path, '0', sandwich_runner.TRACE_FILENAME)
 
     @self.RegisterTask('common/patched-cache.zip', [BuildOriginalCache])
     def BuildPatchedCache():
       sandwich_misc.PatchCacheArchive(BuildOriginalCache.path,
-          os.path.join(BuildOriginalCache.run_path, '0',
-                       sandwich_runner.TRACE_FILENAME),
-          BuildPatchedCache.path)
+          original_cache_trace_path, BuildPatchedCache.path)
 
     @self.RegisterTask('common/subresources-for-urls-run/',
                        dependencies=[self._original_wpr_task])
@@ -150,14 +150,10 @@ class SandwichTaskBuilder(task_manager.Builder):
         json.dump(url_resources, output)
 
     @self.RegisterTask('common/patched-cache-validation.log',
-                       [BuildPatchedCache, ListUrlsResources])
+                       [BuildPatchedCache])
     def ValidatePatchedCache():
-      json_content = json.load(open(ListUrlsResources.path))
-      ref_urls = set()
-      for urls in json_content.values():
-        ref_urls.update(set(urls))
       sandwich_misc.ValidateCacheArchiveContent(
-          ref_urls, BuildPatchedCache.path)
+          original_cache_trace_path, BuildPatchedCache.path)
 
     self._patched_wpr_task = BuildPatchedWpr
     self._reference_cache_task = BuildPatchedCache

commit 5c6b848047dc7fa51489a3663ba3c92793029698
Author: droger <droger@chromium.org>
Date:   Tue May 24 01:32:04 2016 -0700

    tools/android/loading Do not crash when timing information is missing
    
    Some requests don't have the timing information.
    The existing code was sometimes catching this falling back to an
    approximation, but not always.
    
    This CL generalizes the fallback so that the timing information is
    always present.
    
    This CL also improves the error output of analyze.py so that
    this kind of error is easier to diagnose.
    
    Review-Url: https://codereview.chromium.org/2003493002
    Cr-Original-Commit-Position: refs/heads/master@{#395544}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b540a6fbc365215402e6ede92c36d7005711a2d1

diff --git a/loading/analyze.py b/loading/analyze.py
index 30305f3..9564acb 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -116,11 +116,15 @@ def _LogRequests(url, clear_cache_override=None):
     chrome_ctl.SetDeviceEmulation(OPTIONS.emulate_device)
   if OPTIONS.emulate_network:
     chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_network)
-  with chrome_ctl.Open() as connection:
-    if clear_cache:
-      connection.ClearCache()
-    trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-        url, connection, chrome_ctl.ChromeMetadata())
+  try:
+    with chrome_ctl.Open() as connection:
+      if clear_cache:
+        connection.ClearCache()
+      trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+          url, connection, chrome_ctl.ChromeMetadata())
+  except controller.ChromeControllerError as e:
+    e.Dump(sys.stderr)
+    raise
 
   if xvfb_process:
     xvfb_process.terminate()
diff --git a/loading/request_track.py b/loading/request_track.py
index 34e9ef8..8caac11 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -736,10 +736,10 @@ class RequestTrack(devtools_monitor.Track):
                       ('requestHeaders', 'request_headers'),
                       ('headers', 'response_headers')))
     timing_dict = {}
-    # data URLs don't have a timing dict, and timings for cached requests are
-    # stale.
+    # Some URLs don't have a timing dict (e.g. data URLs), and timings for
+    # cached requests are stale.
     # TODO(droger): the timestamp is inacurate, get the real timings instead.
-    if r.protocol in ('data', 'about') or r.served_from_cache:
+    if not response.get('timing') or r.served_from_cache:
       timing_dict = {'requestTime': r.timestamp}
     else:
       timing_dict = response['timing']

commit 903b5fbc64149d40c65b69a9eb6a7d9d442ec74c
Author: gabadie <gabadie@chromium.org>
Date:   Tue May 24 00:56:44 2016 -0700

    sandwich: Add first_contentful_paint and first_layout metrics
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2003063002
    Cr-Original-Commit-Position: refs/heads/master@{#395541}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c461665c531bb0185f820382156afe278fec4887

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index daa4c69..fff96bf 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -45,6 +45,8 @@ CSV_FIELD_NAMES = [
     # discoverer.
     'cached_subresource_count_theoretic',
     'cached_subresource_count',
+    'first_layout',
+    'first_contentful_paint',
     'total_load',
     'js_onload_event',
     'browser_malloc_avg',
@@ -59,7 +61,12 @@ _UNAVAILABLE_CSV_VALUE = 'unavailable'
 
 _FAILED_CSV_VALUE = 'failed'
 
-_TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
+_TRACKED_EVENT_NAMES = set([
+    'requestStart',
+    'loadEventStart',
+    'loadEventEnd',
+    'firstContentfulPaint',
+    'firstLayout'])
 
 # Points of a completeness record.
 #
@@ -143,7 +150,6 @@ def _GetWebPageTrackedEvents(tracing_track):
     if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
       logging.info('found url\'s event \'%s\'' % event_name)
       tracked_events[event_name] = event
-  assert len(tracked_events) == len(_TRACKED_EVENT_NAMES)
   return tracked_events
 
 
@@ -158,9 +164,16 @@ def _ExtractDefaultMetrics(loading_trace):
   """
   web_page_tracked_events = _GetWebPageTrackedEvents(
       loading_trace.tracing_track)
+  assert len(web_page_tracked_events) == len(_TRACKED_EVENT_NAMES)
+  request_start_time = web_page_tracked_events['requestStart'].start_msec
   return {
+    'first_layout': (web_page_tracked_events['firstLayout'].start_msec -
+                     request_start_time),
+    'first_contentful_paint': (
+        web_page_tracked_events['firstContentfulPaint'].start_msec -
+        request_start_time),
     'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
-                   web_page_tracked_events['requestStart'].start_msec),
+                   request_start_time),
     'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
                         web_page_tracked_events['loadEventStart'].start_msec)
   }
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index ea813e2..641c7dc 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -20,10 +20,12 @@ import tracing
 
 _BLINK_CAT = 'blink.user_timing'
 _MEM_CAT = sandwich_runner.MEMORY_DUMP_CATEGORY
-_START='requestStart'
-_LOADS='loadEventStart'
-_LOADE='loadEventEnd'
-_UNLOAD='unloadEventEnd'
+_START = 'requestStart'
+_LOADS = 'loadEventStart'
+_LOADE = 'loadEventEnd'
+_UNLOAD = 'unloadEventEnd'
+_PAINT = 'firstContentfulPaint'
+_LAYOUT = 'firstLayout'
 
 _MINIMALIST_TRACE_EVENTS = [
     {'ph': 'R', 'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10000,
@@ -33,6 +35,10 @@ _MINIMALIST_TRACE_EVENTS = [
     {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
         'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
             'units': 'bytes', 'value': '1af2', }}}}}}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LAYOUT,  'ts': 24000,
+        'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _PAINT,  'ts': 31000,
+        'args': {'frame': '0'}},
     {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35000,
         'args': {'frame': '0'}},
     {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40000,
@@ -188,9 +194,11 @@ class PageTrackTest(unittest.TestCase):
   def testExtractDefaultMetrics(self):
     metrics = puller._ExtractDefaultMetrics(LoadingTrace(
         _MINIMALIST_TRACE_EVENTS))
-    self.assertEquals(2, len(metrics))
+    self.assertEquals(4, len(metrics))
     self.assertEquals(20, metrics['total_load'])
     self.assertEquals(5, metrics['js_onload_event'])
+    self.assertEquals(4, metrics['first_layout'])
+    self.assertEquals(11, metrics['first_contentful_paint'])
 
   def testExtractMemoryMetrics(self):
     metrics = puller._ExtractMemoryMetrics(LoadingTrace(

commit 53a69ffcec8a7e946ad6c5e8c47986923a38ce68
Author: pkasting <pkasting@chromium.org>
Date:   Mon May 23 16:13:30 2016 -0700

    Fix "unused variable" warnings.
    
    These were exposed by changing DISALLOW_COPY_AND_ASSIGN to use "= delete", and
    must be fixed before that change can land.
    
    See https://codereview.chromium.org/1981053002/ for the full CL.
    
    BUG=447156
    TEST=none
    CQ_INCLUDE_TRYBOTS=tryserver.blink:linux_blink_rel
    
    Review-Url: https://codereview.chromium.org/1995983002
    Cr-Original-Commit-Position: refs/heads/master@{#395452}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 65f37925f106348f3b2e16c29b73347b990f127c

diff --git a/forwarder2/host_controller.cc b/forwarder2/host_controller.cc
index 25b253a..6fb3c81 100644
--- a/forwarder2/host_controller.cc
+++ b/forwarder2/host_controller.cc
@@ -48,9 +48,8 @@ std::unique_ptr<HostController> HostController::Create(
     return host_controller;
   }
   host_controller.reset(new HostController(
-      device_port_allocated, host_port, adb_port, exit_notifier_fd,
-      error_callback, std::move(adb_control_socket),
-      std::move(delete_controller_notifier)));
+      device_port_allocated, host_port, adb_port, error_callback,
+      std::move(adb_control_socket), std::move(delete_controller_notifier)));
   return host_controller;
 }
 
@@ -68,7 +67,6 @@ HostController::HostController(
     int device_port,
     int host_port,
     int adb_port,
-    int exit_notifier_fd,
     const ErrorCallback& error_callback,
     std::unique_ptr<Socket> adb_control_socket,
     std::unique_ptr<PipeNotifier> delete_controller_notifier)
@@ -76,7 +74,6 @@ HostController::HostController(
       device_port_(device_port),
       host_port_(host_port),
       adb_port_(adb_port),
-      global_exit_notifier_fd_(exit_notifier_fd),
       adb_control_socket_(std::move(adb_control_socket)),
       delete_controller_notifier_(std::move(delete_controller_notifier)),
       deletion_task_runner_(base::ThreadTaskRunnerHandle::Get()),
diff --git a/forwarder2/host_controller.h b/forwarder2/host_controller.h
index 7328245..20241d3 100644
--- a/forwarder2/host_controller.h
+++ b/forwarder2/host_controller.h
@@ -58,7 +58,6 @@ class HostController {
   HostController(int device_port,
                  int host_port,
                  int adb_port,
-                 int exit_notifier_fd,
                  const ErrorCallback& error_callback,
                  std::unique_ptr<Socket> adb_control_socket,
                  std::unique_ptr<PipeNotifier> delete_controller_notifier);
@@ -77,8 +76,6 @@ class HostController {
   const int device_port_;
   const int host_port_;
   const int adb_port_;
-  // Used to notify the controller when the process is killed.
-  const int global_exit_notifier_fd_;
   std::unique_ptr<Socket> adb_control_socket_;
   // Used to cancel the pending blocking IO operations when the host controller
   // instance is deleted.

commit aa5ce76896d706e6f00844cbd83c319ce7e3ddbd
Author: pasko <pasko@chromium.org>
Date:   Mon May 23 11:40:20 2016 -0700

    Sandwich: Skip SpeedIndex from CSV if cannot calculate
    
    Sometimes the bounding box is not available on a video, I guess this is because
    screenrecord misses a few frames. This happens rarely, so we'd mark these cases
    as "unavailable" in the CSV and will watch for these not to happen too often to
    skew results.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2007593002
    Cr-Original-Commit-Position: refs/heads/master@{#395372}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8dabb3564f823df7b0aae8faf220bee2a59649a3

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 6fbf1c8..daa4c69 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -57,6 +57,8 @@ CSV_FIELD_NAMES = [
 
 _UNAVAILABLE_CSV_VALUE = 'unavailable'
 
+_FAILED_CSV_VALUE = 'failed'
+
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
 # Points of a completeness record.
@@ -305,8 +307,13 @@ def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
   video_path = os.path.join(run_directory_path, 'video.mp4')
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
-    completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
-    run_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+    try:
+      completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
+      run_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+    except video.BoundingBoxNotFoundException:
+      # Sometimes the bounding box for the web content area is not present. Skip
+      # calculating Speed Index.
+      run_metrics['speed_index'] = _FAILED_CSV_VALUE
   else:
     run_metrics['speed_index'] = _UNAVAILABLE_CSV_VALUE
   for key, value in loading_trace.metadata['network_emulation'].iteritems():

commit 1cd5219b8b4f846a921719375d91ad5226f7709d
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 23 11:30:54 2016 -0700

    sandwich: Add support for multiple URLs
    
    Before, sandwich was using the old Job work. But could only have
    one URL per tasks graph. This CL adds the hability to creates
    tasks from task_manager in an output's subdirectory and use this
    feature to populate graphes for different urls in the same forest
    tasks.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1973373002
    Cr-Original-Commit-Position: refs/heads/master@{#395369}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a0aab25bb8d7871e6a75f61b7f7cc54783ef002f

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 0b72c73..2f681d6 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -7,16 +7,16 @@
 
 When running Chrome is sandwiched between preprocessed disk caches and
 WepPageReplay serving all connections.
-
-TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
 import csv
+import json
 import logging
 import os
 import shutil
 import sys
+from urlparse import urlparse
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -35,7 +35,7 @@ import emulation
 import options
 import sandwich_metrics
 import sandwich_misc
-from sandwich_runner import SandwichRunner
+import sandwich_runner
 import sandwich_task_builder
 import task_manager
 from trace_test.webserver_test import WebServer
@@ -46,6 +46,30 @@ OPTIONS = options.OPTIONS
 
 _SPEED_INDEX_MEASUREMENT = 'speed-index'
 _MEMORY_MEASUREMENT = 'memory'
+_CORPUS_DIR = 'sandwich_corpuses'
+
+
+def ReadUrlsFromCorpus(corpus_path):
+  """Retrieves the list of URLs associated with the corpus name."""
+  try:
+    # Attempt to read by regular file name.
+    json_file_name = corpus_path
+    with open(json_file_name) as f:
+      json_data = json.load(f)
+  except IOError:
+    # Extra sugar: attempt to load from _CORPUS_DIR.
+    json_file_name = os.path.join(
+        os.path.dirname(__file__), _CORPUS_DIR, corpus_path)
+    with open(json_file_name) as f:
+      json_data = json.load(f)
+
+  key = 'urls'
+  if json_data and key in json_data:
+    url_list = json_data[key]
+    if isinstance(url_list, list) and len(url_list) > 0:
+      return url_list
+  raise Exception(
+      'File {} does not define a list named "urls"'.format(json_file_name))
 
 
 def _ArgumentParser():
@@ -85,8 +109,8 @@ def _ArgumentParser():
   run_parser.add_argument('-g', '--gen-full', action='store_true',
                           help='Generate the full graph with all possible '
                                'benchmarks.')
-  run_parser.add_argument('--job', required=True,
-      help='JSON file with job description such as in sandwich_jobs/.')
+  run_parser.add_argument('-c', '--corpus', required=True,
+      help='Path to a JSON file with a corpus such as in %s/.' % _CORPUS_DIR)
   run_parser.add_argument('-m', '--measure', default=[], nargs='+',
       choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
       dest='optional_measures', help='Enable optional measurements.')
@@ -108,28 +132,29 @@ def _GetAndroidDeviceFromArgs(args):
 
 def _RecordWebServerTestTrace(args):
   with common_util.TemporaryDirectory() as out_path:
-    sandwich_runner = SandwichRunner()
-    sandwich_runner.android_device = _GetAndroidDeviceFromArgs(args)
+    runner = sandwich_runner.SandwichRunner()
+    runner.android_device = _GetAndroidDeviceFromArgs(args)
     # Reuse the WPR's forwarding to access the webpage from Android.
-    sandwich_runner.wpr_record = True
-    sandwich_runner.wpr_archive_path = os.path.join(out_path, 'wpr')
-    sandwich_runner.trace_output_directory = os.path.join(out_path, 'run')
+    runner.wpr_record = True
+    runner.wpr_archive_path = os.path.join(out_path, 'wpr')
+    runner.output_dir = os.path.join(out_path, 'run')
     with WebServer.Context(
         source_dir=args.source_dir, communication_dir=out_path) as server:
       address = server.Address()
-      sandwich_runner.urls = ['http://%s/%s' % (address, args.page)]
-      sandwich_runner.Run()
+      runner.url = 'http://%s/%s' % (address, args.page)
+      runner.Run()
     trace_path = os.path.join(
         out_path, 'run', '0', sandwich_runner.TRACE_FILENAME)
     shutil.copy(trace_path, args.output)
   return 0
 
 
-def _RunAllMain(args):
+def _GenerateNoStatePrefetchBenchmarkTasks(args, url, output_subdirectory):
   builder = sandwich_task_builder.SandwichTaskBuilder(
-      output_directory=args.output,
       android_device=_GetAndroidDeviceFromArgs(args),
-      job_path=args.job)
+      url=url,
+      output_directory=args.output,
+      output_subdirectory=output_subdirectory)
   if args.wpr_archive_path:
     builder.OverridePathToWprArchive(args.wpr_archive_path)
   else:
@@ -139,7 +164,7 @@ def _RunAllMain(args):
   def MainTransformer(runner):
     runner.record_video = _SPEED_INDEX_MEASUREMENT in args.optional_measures
     runner.record_memory_dumps = _MEMORY_MEASUREMENT in args.optional_measures
-    runner.job_repeat = args.url_repeat
+    runner.repeat = args.url_repeat
 
   transformer_list_name = 'no-network-emulation'
   builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
@@ -160,9 +185,25 @@ def _RunAllMain(args):
           continue
         builder.PopulateLoadBenchmark(subresource_discoverer,
             transformer_list_name, transformer_list)
+  return builder.tasks.values(), builder.default_final_tasks
+
 
-  return task_manager.ExecuteWithCommandLine(
-      args, builder.tasks.values(), builder.default_final_tasks)
+def _RunAllMain(args):
+  urls = ReadUrlsFromCorpus(args.corpus)
+  domain_times_encountered_per_domain = {}
+  tasks = []
+  default_final_tasks = []
+  for url in urls:
+    domain = '.'.join(urlparse(url).netloc.split('.')[-2:])
+    domain_times_encountered = domain_times_encountered_per_domain.get(
+        domain, 0)
+    output_subdirectory = '{}.{}'.format(domain, domain_times_encountered)
+    domain_times_encountered_per_domain[domain] = domain_times_encountered + 1
+    gen_tasks, gen_default_final_tasks = \
+        _GenerateNoStatePrefetchBenchmarkTasks(args, url, output_subdirectory)
+    tasks.extend(gen_tasks)
+    default_final_tasks.extend(gen_default_final_tasks)
+  return task_manager.ExecuteWithCommandLine(args, tasks, default_final_tasks)
 
 
 def main(command_line_args):
diff --git a/loading/sandwich_corpuses/mobile.json b/loading/sandwich_corpuses/mobile.json
new file mode 100644
index 0000000..d6ad19c
--- /dev/null
+++ b/loading/sandwich_corpuses/mobile.json
@@ -0,0 +1,9 @@
+{
+  "urls": [
+    "http://www.bbc.com/",
+    "http://cnn.com",
+    "https://en.m.wikipedia.org/wiki/Main_Page",
+    "https://en.m.wikipedia.org/wiki/Science",
+    "https://en.m.wikipedia.org/wiki/Wassily_Kandinsky"
+  ]
+}
diff --git a/loading/sandwich_jobs/wikipedia.json b/loading/sandwich_jobs/wikipedia.json
deleted file mode 100644
index 84e91d8..0000000
--- a/loading/sandwich_jobs/wikipedia.json
+++ /dev/null
@@ -1,5 +0,0 @@
-{
-  "urls": [
-    "https://en.m.wikipedia.org/wiki/Science"
-  ]
-}
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 8005350..49084ae 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -338,39 +338,25 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
                          'Distinct resource requests to WPR')
 
 
-def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
-  """Extracts a map URL-to-subresources for each navigation in benchmark
-  directory.
+def ReadSubresourceFromRunnerOutputDir(runner_output_dir):
+  """Extracts a list of subresources in runner output directory.
 
   Args:
-    benchmark_output_directory_path: Path of the benchmark output directory to
-        verify.
+    runner_output_dir: Path of the runner's output directory.
 
   Returns:
-    {url -> [URLs of sub-resources]}
+    [URLs of sub-resources]
   """
-  url_subresources = {}
-  run_id = -1
-  while True:
-    run_id += 1
-    run_path = os.path.join(benchmark_output_directory_path, str(run_id))
-    if not os.path.isdir(run_path):
-      break
-    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
-    if not os.path.isfile(trace_path):
-      continue
-    trace = LoadingTrace.FromJsonFile(trace_path)
-    if trace.url in url_subresources:
-      continue
-    logging.info('lists resources of %s from %s' % (trace.url, trace_path))
-    urls_set = set()
-    for request_event in _FilterOutDataAndIncompleteRequests(
-        trace.request_track.GetEvents()):
-      if request_event.url not in urls_set:
-        logging.info('  %s' % request_event.url)
-        urls_set.add(request_event.url)
-    url_subresources[trace.url] = [url for url in urls_set]
-  return url_subresources
+  trace_path = os.path.join(
+      runner_output_dir, '0', sandwich_runner.TRACE_FILENAME)
+  trace = LoadingTrace.FromJsonFile(trace_path)
+  url_set = set()
+  for request_event in _FilterOutDataAndIncompleteRequests(
+      trace.request_track.GetEvents()):
+    url_set.add(request_event.url)
+  logging.info('lists %s resources of %s from %s' % \
+               (len(url_set), trace.url, trace_path))
+  return [url for url in url_set]
 
 
 def ValidateCacheArchiveContent(ref_urls, cache_archive_path):
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index b532e47..0ec40ae 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -2,7 +2,6 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-import json
 import logging
 import os
 import shutil
@@ -30,36 +29,13 @@ WPR_LOG_FILENAME = 'wpr.log'
 # Memory dump category used to get memory metrics.
 MEMORY_DUMP_CATEGORY = 'disabled-by-default-memory-infra'
 
-_JOB_SEARCH_PATH = 'sandwich_jobs'
-
 # Devtools timeout of 1 minute to avoid websocket timeout on slow
 # network condition.
 _DEVTOOLS_TIMEOUT = 60
 
 
-def _ReadUrlsFromJobDescription(job_name):
-  """Retrieves the list of URLs associated with the job name."""
-  try:
-    # Extra sugar: attempt to load from a relative path.
-    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
-        job_name)
-    with open(json_file_name) as f:
-      json_data = json.load(f)
-  except IOError:
-    # Attempt to read by regular file name.
-    with open(job_name) as f:
-      json_data = json.load(f)
-
-  key = 'urls'
-  if json_data and key in json_data:
-    url_list = json_data[key]
-    if isinstance(url_list, list) and len(url_list) > 0:
-      return url_list
-  raise Exception('Job description does not define a list named "urls"')
-
-
-def _CleanPreviousTraces(output_directories_path):
-  """Cleans previous traces from the output directory.
+def _CleanArtefactsFromPastRuns(output_directories_path):
+  """Cleans artifacts generated from past run in the output directory.
 
   Args:
     output_directories_path: The output directory path where to clean the
@@ -76,27 +52,25 @@ def _CleanPreviousTraces(output_directories_path):
     shutil.rmtree(directory_path)
 
 
+class CacheOperation(object):
+  CLEAR, SAVE, PUSH = range(3)
+
+
 class SandwichRunner(object):
   """Sandwich runner.
 
   This object is meant to be configured first and then run using the Run()
-  method. The runner can configure itself conveniently with parsed arguement
-  using the PullConfigFromArgs() method. The only job is to make sure that the
-  command line flags have `dest` parameter set to existing runner members.
+  method.
   """
   _ATTEMPT_COUNT = 3
 
   def __init__(self):
     """Configures a sandwich runner out of the box.
 
-    Public members are meant to be configured as wished before calling Run().
-
-    Args:
-      job_name: The job name to get the associated urls.
+    Public members are meant to be configured before calling Run().
     """
     # Cache operation to do before doing the chrome navigation.
-    #   Can be: clear,save,push,reload
-    self.cache_operation = 'clear'
+    self.cache_operation = CacheOperation.CLEAR
 
     # The cache archive's path to save to or push from. Is str or None.
     self.cache_archive_path = None
@@ -104,11 +78,8 @@ class SandwichRunner(object):
     # Controls whether the WPR server should do script injection.
     self.disable_wpr_script_injection = False
 
-    # The job name. Is str.
-    self.job_name = '__unknown_job'
-
-    # Number of times to repeat the job.
-    self.job_repeat = 1
+    # Number of times to repeat the url.
+    self.repeat = 1
 
     # Network conditions to emulate. None if no emulation.
     self.network_condition = None
@@ -116,11 +87,11 @@ class SandwichRunner(object):
     # Network condition emulator. Can be: browser,wpr
     self.network_emulator = 'browser'
 
-    # Output directory where to save the traces. Is str or None.
-    self.trace_output_directory = None
+    # Output directory where to save the traces, videos, etc. Is str or None.
+    self.output_dir = None
 
-    # List of urls to run.
-    self.urls = []
+    # URL to navigate to.
+    self.url = None
 
     # Configures whether to record speed-index video.
     self.record_video = False
@@ -140,55 +111,34 @@ class SandwichRunner(object):
     self._chrome_ctl = None
     self._local_cache_directory_path = None
 
-  def LoadJob(self, job_name):
-    self.job_name = job_name
-    self.urls = _ReadUrlsFromJobDescription(job_name)
-
-  def PullConfigFromArgs(self, args):
-    """Configures the sandwich runner from parsed command line argument.
-
-    Args:
-      args: The command line parsed argument.
-    """
-    for config_name in self.__dict__.keys():
-      if config_name in args.__dict__:
-        self.__dict__[config_name] = args.__dict__[config_name]
-
-  def PrintConfig(self):
-    """Print the current sandwich runner configuration to stdout. """
-    for config_name in sorted(self.__dict__.keys()):
-      if config_name[0] != '_':
-        print '{} = {}'.format(config_name, self.__dict__[config_name])
-
   def _CleanTraceOutputDirectory(self):
-    assert self.trace_output_directory
-    if not os.path.isdir(self.trace_output_directory):
+    assert self.output_dir
+    if not os.path.isdir(self.output_dir):
       try:
-        os.makedirs(self.trace_output_directory)
+        os.makedirs(self.output_dir)
       except OSError:
         logging.error('Cannot create directory for results: %s',
-            self.trace_output_directory)
+            self.output_dir)
         raise
     else:
-      _CleanPreviousTraces(self.trace_output_directory)
+      _CleanArtefactsFromPastRuns(self.output_dir)
 
   def _GetEmulatorNetworkCondition(self, emulator):
     if self.network_emulator == emulator:
       return self.network_condition
     return None
 
-  def _RunNavigation(self, url, clear_cache, run_id=None):
+  def _RunNavigation(self, clear_cache, run_id=None):
     """Run a page navigation to the given URL.
 
     Args:
-      url: The URL to navigate to.
       clear_cache: Whether if the cache should be cleared before navigation.
       run_id: Id of the run in the output directory. If it is None, then no
         trace or video will be saved.
     """
     run_path = None
-    if self.trace_output_directory is not None and run_id is not None:
-      run_path = os.path.join(self.trace_output_directory, str(run_id))
+    if self.output_dir is not None and run_id is not None:
+      run_path = os.path.join(self.output_dir, str(run_id))
       if not os.path.isdir(run_path):
         os.makedirs(run_path)
     self._chrome_ctl.SetNetworkEmulation(
@@ -208,14 +158,14 @@ class SandwichRunner(object):
         with device_setup.RemoteSpeedIndexRecorder(device, connection,
                                                    video_recording_path):
           trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url=url,
+              url=self.url,
               connection=connection,
               chrome_metadata=self._chrome_ctl.ChromeMetadata(),
               additional_categories=additional_categories,
               timeout_seconds=_DEVTOOLS_TIMEOUT)
       else:
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-            url=url,
+            url=self.url,
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
             additional_categories=additional_categories,
@@ -224,20 +174,18 @@ class SandwichRunner(object):
       trace_path = os.path.join(run_path, TRACE_FILENAME)
       trace.ToJsonFile(trace_path)
 
-  def _RunUrl(self, url, run_id):
+  def _RunUrl(self, run_id):
     for attempt_id in xrange(self._ATTEMPT_COUNT):
       try:
         self._chrome_ctl.ResetBrowserState()
         clear_cache = False
-        if self.cache_operation == 'clear':
+        if self.cache_operation == CacheOperation.CLEAR:
           clear_cache = True
-        elif self.cache_operation == 'push':
+        elif self.cache_operation == CacheOperation.PUSH:
           self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
-        elif self.cache_operation == 'reload':
-          self._RunNavigation(url, clear_cache=True)
-        elif self.cache_operation == 'save':
+        elif self.cache_operation == CacheOperation.SAVE:
           clear_cache = run_id == 0
-        self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
+        self._RunNavigation(clear_cache=clear_cache, run_id=run_id)
         break
       except controller.ChromeControllerError as error:
         if not error.IsIntermittent():
@@ -249,11 +197,11 @@ class SandwichRunner(object):
             error.Dump(dump_output)
     else:
       logging.error('Failed to navigate to %s after %d attemps' % \
-                    (url, self._ATTEMPT_COUNT))
+                    (self.url, self._ATTEMPT_COUNT))
       raise
 
   def _PullCacheFromDevice(self):
-    assert self.cache_operation == 'save'
+    assert self.cache_operation == CacheOperation.SAVE
     assert self.cache_archive_path, 'Need to specify where to save the cache'
 
     cache_directory_path = self._chrome_ctl.PullBrowserCache()
@@ -265,7 +213,7 @@ class SandwichRunner(object):
     """SandwichRunner main entry point meant to be called once configured."""
     assert self._chrome_ctl == None
     assert self._local_cache_directory_path == None
-    if self.trace_output_directory:
+    if self.output_dir:
       self._CleanTraceOutputDirectory()
 
     if self.android_device:
@@ -273,34 +221,32 @@ class SandwichRunner(object):
     else:
       self._chrome_ctl = controller.LocalChromeController()
     self._chrome_ctl.AddChromeArgument('--disable-infobars')
-    if self.cache_operation == 'save':
+    if self.cache_operation == CacheOperation.SAVE:
       self._chrome_ctl.SetSlowDeath()
 
-    if self.cache_operation == 'push':
-      assert os.path.isfile(self.cache_archive_path)
-      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-      chrome_cache.UnzipDirectoryContent(
-          self.cache_archive_path, self._local_cache_directory_path)
-
-    out_log_path = None
-    if self.trace_output_directory:
-      out_log_path = os.path.join(self.trace_output_directory, WPR_LOG_FILENAME)
-
-    ran_urls = []
-    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
-        record=self.wpr_record,
-        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
-        disable_script_injection=self.disable_wpr_script_injection,
-        out_log_path=out_log_path):
-      for _ in xrange(self.job_repeat):
-        for url in self.urls:
-          self._RunUrl(url, run_id=len(ran_urls))
-          ran_urls.append(url)
-
-    if self._local_cache_directory_path:
-      shutil.rmtree(self._local_cache_directory_path)
-      self._local_cache_directory_path = None
-    if self.cache_operation == 'save':
+    wpr_log_path = None
+    if self.output_dir:
+      wpr_log_path = os.path.join(self.output_dir, WPR_LOG_FILENAME)
+
+    try:
+      if self.cache_operation == CacheOperation.PUSH:
+        assert os.path.isfile(self.cache_archive_path)
+        self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+        chrome_cache.UnzipDirectoryContent(
+            self.cache_archive_path, self._local_cache_directory_path)
+
+      with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
+          record=self.wpr_record,
+          network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
+          disable_script_injection=self.disable_wpr_script_injection,
+          out_log_path=wpr_log_path):
+        for repeat_id in xrange(self.repeat):
+          self._RunUrl(run_id=repeat_id)
+    finally:
+      if self._local_cache_directory_path:
+        shutil.rmtree(self._local_cache_directory_path)
+        self._local_cache_directory_path = None
+    if self.cache_operation == CacheOperation.SAVE:
       self._PullCacheFromDevice()
 
     self._chrome_ctl = None
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 8fc01af..ca3d481 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -37,18 +37,20 @@ class SandwichTaskBuilder(task_manager.Builder):
   """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
   """
 
-  def __init__(self, output_directory, android_device, job_path):
+  def __init__(self, android_device, url, output_directory,
+               output_subdirectory):
     """Constructor.
 
     Args:
-      output_directory: As in task_manager.Builder.__init__
       android_device: The android DeviceUtils to run sandwich on or None to run
         it locally.
-      job_path: Path of the sandwich's job.
+      url: URL to benchmark.
+      output_directory: As in task_manager.Builder.__init__
+      output_subdirectory: As in task_manager.Builder.__init__
     """
-    task_manager.Builder.__init__(self, output_directory)
+    task_manager.Builder.__init__(self, output_directory, output_subdirectory)
     self._android_device = android_device
-    self._job_path = job_path
+    self._url = url
     self._default_final_tasks = []
 
     self._original_wpr_task = None
@@ -64,7 +66,7 @@ class SandwichTaskBuilder(task_manager.Builder):
   def _CreateSandwichRunner(self):
     """Create a runner for non benchmark purposes."""
     runner = sandwich_runner.SandwichRunner()
-    runner.LoadJob(self._job_path)
+    runner.url = self._url
     runner.android_device = self._android_device
     return runner
 
@@ -119,15 +121,16 @@ class SandwichTaskBuilder(task_manager.Builder):
       runner = self._CreateSandwichRunner()
       runner.wpr_archive_path = BuildPatchedWpr.path
       runner.cache_archive_path = BuildOriginalCache.path
-      runner.cache_operation = 'save'
-      runner.trace_output_directory = BuildOriginalCache.run_path
+      runner.cache_operation = sandwich_runner.CacheOperation.SAVE
+      runner.output_dir = BuildOriginalCache.run_path
       runner.Run()
     BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
 
     @self.RegisterTask('common/patched-cache.zip', [BuildOriginalCache])
     def BuildPatchedCache():
       sandwich_misc.PatchCacheArchive(BuildOriginalCache.path,
-          os.path.join(BuildOriginalCache.run_path, '0', 'trace.json'),
+          os.path.join(BuildOriginalCache.run_path, '0',
+                       sandwich_runner.TRACE_FILENAME),
           BuildPatchedCache.path)
 
     @self.RegisterTask('common/subresources-for-urls-run/',
@@ -135,16 +138,16 @@ class SandwichTaskBuilder(task_manager.Builder):
     def UrlsResourcesRun():
       runner = self._CreateSandwichRunner()
       runner.wpr_archive_path = self._original_wpr_task.path
-      runner.cache_operation = 'clear'
-      runner.trace_output_directory = UrlsResourcesRun.path
+      runner.cache_operation = sandwich_runner.CacheOperation.CLEAR
+      runner.output_dir = UrlsResourcesRun.path
       runner.Run()
 
     @self.RegisterTask('common/subresources-for-urls.json', [UrlsResourcesRun])
     def ListUrlsResources():
-      json_content = sandwich_misc.ReadSubresourceMapFromBenchmarkOutput(
+      url_resources = sandwich_misc.ReadSubresourceFromRunnerOutputDir(
           UrlsResourcesRun.path)
       with open(ListUrlsResources.path, 'w') as output:
-        json.dump(json_content, output)
+        json.dump(url_resources, output)
 
     @self.RegisterTask('common/patched-cache-validation.log',
                        [BuildPatchedCache, ListUrlsResources])
@@ -201,11 +204,7 @@ class SandwichTaskBuilder(task_manager.Builder):
       whitelisted_urls = sandwich_misc.ExtractDiscoverableUrls(
           trace_path, subresource_discoverer)
 
-      urls_resources = json.load(open(self._subresources_for_urls_task.path))
-      # TODO(gabadie): Implement support for multiple URLs in this Task.
-      assert len(urls_resources) == 1
-      url = urls_resources.keys()[0]
-      url_resources = urls_resources[url]
+      url_resources = json.load(open(self._subresources_for_urls_task.path))
       common_util.EnsureParentDirectoryExists(SetupBenchmark.path)
       with open(SetupBenchmark.path, 'w') as output:
         json.dump({
@@ -231,10 +230,11 @@ class SandwichTaskBuilder(task_manager.Builder):
       for transformer in transformer_list:
         transformer(runner)
       runner.wpr_archive_path = self._patched_wpr_task.path
-      runner.wpr_out_log_path = os.path.join(RunBenchmark.path, 'wpr.log')
+      runner.wpr_out_log_path = os.path.join(
+          RunBenchmark.path, sandwich_runner.WPR_LOG_FILENAME)
       runner.cache_archive_path = BuildBenchmarkCacheArchive.path
-      runner.cache_operation = 'push'
-      runner.trace_output_directory = RunBenchmark.path
+      runner.cache_operation = sandwich_runner.CacheOperation.PUSH
+      runner.output_dir = RunBenchmark.path
       runner.Run()
 
     @self.RegisterTask(task_prefix + '-metrics.csv',
diff --git a/loading/task_manager.py b/loading/task_manager.py
index 88a473d..93f14cf 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -60,6 +60,7 @@ Example:
 
 
 import argparse
+import collections
 import logging
 import os
 import subprocess
@@ -110,16 +111,20 @@ class Task(object):
 class Builder(object):
   """Utilities for creating sub-graphs of tasks with dependencies."""
 
-  def __init__(self, output_directory):
+  def __init__(self, output_directory, output_subdirectory):
     """Constructor.
 
     Args:
       output_directory: Output directory where the dynamic tasks work.
+      output_subdirectory: Subdirectory to put all created tasks in or None.
     """
     self.output_directory = output_directory
+    self._output_subdirectory = output_subdirectory
     self.tasks = {}
 
   def CreateStaticTask(self, task_name, path):
+    """Creates and returns a new static task."""
+    task_name = self._RebaseTaskName(task_name)
     if not os.path.exists(path):
       raise TaskError('Error while creating task {}: File not found: {}'.format(
           task_name, path))
@@ -160,6 +165,7 @@ class Builder(object):
       A Task that was created by wrapping the function or an existing registered
       wrapper (that have wrapped a different function).
     """
+    task_name = self._RebaseTaskName(task_name)
     dependencies = dependencies or []
     def InnerAddTaskWithNewPath(recipe):
       if task_name in self.tasks:
@@ -176,6 +182,11 @@ class Builder(object):
       return task
     return InnerAddTaskWithNewPath
 
+  def _RebaseTaskName(self,  task_name):
+    if self._output_subdirectory:
+      return os.path.join(self._output_subdirectory, task_name)
+    return task_name
+
 
 def GenerateScenario(final_tasks, frozen_tasks):
   """Generates a list of tasks to execute in order of dependencies-first.
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index 68ae1ce..d9990ac 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -99,23 +99,23 @@ class TaskTest(TaskManagerTestCase):
 
 class BuilderTest(TaskManagerTestCase):
   def testCreateUnexistingStaticTask(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     with self.assertRaises(task_manager.TaskError):
       builder.CreateStaticTask('hello.txt', '/__unexisting/file/path')
 
   def testCreateStaticTask(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     task = builder.CreateStaticTask('hello.py', __file__)
     self.assertTrue(task.IsStatic())
 
   def testDuplicateStaticTask(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     builder.CreateStaticTask('hello.py', __file__)
     with self.assertRaises(task_manager.TaskError):
       builder.CreateStaticTask('hello.py', __file__)
 
   def testRegisterTask(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('hello.txt')
     def TaskA():
       TaskA.executed = True
@@ -128,7 +128,7 @@ class BuilderTest(TaskManagerTestCase):
     self.assertTrue(TaskA.executed)
 
   def testRegisterDuplicateTask(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('hello.txt')
     def TaskA():
       pass
@@ -140,7 +140,7 @@ class BuilderTest(TaskManagerTestCase):
       del TaskB # unused
 
   def testTaskMerging(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('hello.txt')
     def TaskA():
       pass
@@ -150,7 +150,7 @@ class BuilderTest(TaskManagerTestCase):
     self.assertEqual(TaskA, TaskB)
 
   def testStaticTaskMergingError(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     builder.CreateStaticTask('hello.py', __file__)
     with self.assertRaises(task_manager.TaskError):
       @builder.RegisterTask('hello.py', merge=True)
@@ -158,10 +158,33 @@ class BuilderTest(TaskManagerTestCase):
         pass
       del TaskA # unused
 
+  def testOutputSubdirectory(self):
+    builder = task_manager.Builder(self.output_directory, 'subdir')
+
+    builder.CreateStaticTask('hello.py', __file__)
+    self.assertIn('subdir/hello.py', builder.tasks)
+    self.assertNotIn('hello.py', builder.tasks)
+
+    builder.CreateStaticTask('subdir/hello.py', __file__)
+    self.assertIn('subdir/subdir/hello.py', builder.tasks)
+
+    @builder.RegisterTask('world.txt')
+    def TaskA():
+      pass
+    del TaskA # unused
+    self.assertIn('subdir/world.txt', builder.tasks)
+    self.assertNotIn('hello.py', builder.tasks)
+
+    @builder.RegisterTask('subdir/world.txt')
+    def TaskB():
+      pass
+    del TaskB # unused
+    self.assertIn('subdir/subdir/world.txt', builder.tasks)
+
 
 class GenerateScenarioTest(TaskManagerTestCase):
   def testParents(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('a')
     def TaskA():
       pass
@@ -184,7 +207,7 @@ class GenerateScenarioTest(TaskManagerTestCase):
     self.assertListEqual([TaskA, TaskB, TaskC], scenario)
 
   def testFreezing(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('a')
     def TaskA():
       pass
@@ -213,7 +236,7 @@ class GenerateScenarioTest(TaskManagerTestCase):
     self.assertListEqual([TaskC, TaskD], scenario)
 
   def testCycleError(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('a')
     def TaskA():
       pass
@@ -231,8 +254,8 @@ class GenerateScenarioTest(TaskManagerTestCase):
       task_manager.GenerateScenario([TaskD], set())
 
   def testCollisionError(self):
-    builder_a = task_manager.Builder(self.output_directory)
-    builder_b = task_manager.Builder(self.output_directory)
+    builder_a = task_manager.Builder(self.output_directory, None)
+    builder_b = task_manager.Builder(self.output_directory, None)
     @builder_a.RegisterTask('a')
     def TaskA():
       pass
@@ -243,7 +266,7 @@ class GenerateScenarioTest(TaskManagerTestCase):
       task_manager.GenerateScenario([TaskA, TaskB], set())
 
   def testGraphVizOutput(self):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     static_task = builder.CreateStaticTask('a', __file__)
     @builder.RegisterTask('b')
     def TaskB():
@@ -268,7 +291,7 @@ class GenerateScenarioTest(TaskManagerTestCase):
 
   def testListResumingTasksToFreeze(self):
     TaskManagerTestCase.setUp(self)
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     static_task = builder.CreateStaticTask('static', __file__)
     @builder.RegisterTask('a')
     def TaskA():
@@ -315,7 +338,7 @@ class GenerateScenarioTest(TaskManagerTestCase):
 
 class CommandLineControlledExecutionTest(TaskManagerTestCase):
   def Execute(self, *command_line_args):
-    builder = task_manager.Builder(self.output_directory)
+    builder = task_manager.Builder(self.output_directory, None)
     @builder.RegisterTask('a')
     def TaskA():
       pass

commit 52b650533dd209169b580e4061ecaf1c0468f3fa
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 23 07:48:05 2016 -0700

    tools/android/loading: Fixes WPR archive patching API
    
    Web page replay commit linked below changes how the response headers
    are stored internally, breaking sandwich's WPR patching task. This
    CL update the WPR archive patching API in wpr_backend.py to get it
    working again.
    
    https://github.com/chromium/web-page-replay/commit/d23e5a88d82d1e1f6ce759f701bb67d7821df8a1
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/2005723003
    Cr-Original-Commit-Position: refs/heads/master@{#395322}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fd23651b1d74fdc4e07136fe840a9356cb662f1b

diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
index 04410ad..25f753c 100644
--- a/loading/wpr_backend.py
+++ b/loading/wpr_backend.py
@@ -48,7 +48,7 @@ class WprUrlEntry(object):
       dict(name -> value)
     """
     headers = collections.defaultdict(list)
-    for (key, value) in self._wpr_response.headers:
+    for (key, value) in self._wpr_response.original_headers:
       headers[key.lower()].append(value)
     return {k: ','.join(v) for (k, v) in headers.items()}
 
@@ -66,16 +66,16 @@ class WprUrlEntry(object):
     assert name.islower()
     new_headers = []
     new_header_set = False
-    for header in self._wpr_response.headers:
+    for header in self._wpr_response.original_headers:
       if header[0].lower() != name:
         new_headers.append(header)
       elif not new_header_set:
         new_header_set = True
         new_headers.append((header[0], value))
     if new_header_set:
-      self._wpr_response.headers = new_headers
+      self._wpr_response.original_headers = new_headers
     else:
-      self._wpr_response.headers.append((name, value))
+      self._wpr_response.original_headers.append((name, value))
 
   def DeleteResponseHeader(self, name):
     """Delete a header.
@@ -88,8 +88,8 @@ class WprUrlEntry(object):
       name: The name of the response header field to delete.
     """
     assert name.islower()
-    self._wpr_response.headers = \
-        [x for x in self._wpr_response.headers if x[0].lower() != name]
+    self._wpr_response.original_headers = \
+        [x for x in self._wpr_response.original_headers if x[0].lower() != name]
 
   def RemoveResponseHeaderDirectives(self, name, directives_blacklist):
     """Removed a set of directives from response headers.
@@ -143,6 +143,9 @@ class WprArchiveBackend(object):
 
   def Persist(self):
     """Persists the archive to disk. """
+    for request in self._http_archive.get_requests():
+      response = self._http_archive[request]
+      response.headers = response._TrimHeaders(response.original_headers)
     self._http_archive.Persist(self._wpr_archive_path)
 
 
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
index f4c2fe6..fbcb517 100644
--- a/loading/wpr_backend_unittest.py
+++ b/loading/wpr_backend_unittest.py
@@ -20,7 +20,7 @@ LOADING_DIR = os.path.dirname(__file__)
 
 class MockWprResponse(object):
   def __init__(self, headers):
-    self.headers = headers
+    self.original_headers = headers
 
 class WprUrlEntryTest(unittest.TestCase):
 
@@ -64,7 +64,7 @@ class WprUrlEntryTest(unittest.TestCase):
     headers = entry.GetResponseHeadersDict()
     self.assertEquals(3, len(headers))
     self.assertEquals('new_value0', headers['new_header0'])
-    self.assertEquals('new_header0', entry._wpr_response.headers[2][0])
+    self.assertEquals('new_header0', entry._wpr_response.original_headers[2][0])
 
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
@@ -73,7 +73,7 @@ class WprUrlEntryTest(unittest.TestCase):
     headers = entry.GetResponseHeadersDict()
     self.assertEquals(3, len(headers))
     self.assertEquals('new_value1', headers['header1'])
-    self.assertEquals('header1', entry._wpr_response.headers[1][0])
+    self.assertEquals('header1', entry._wpr_response.original_headers[1][0])
 
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('hEADEr1', 'value1'),
@@ -82,7 +82,7 @@ class WprUrlEntryTest(unittest.TestCase):
     headers = entry.GetResponseHeadersDict()
     self.assertEquals(3, len(headers))
     self.assertEquals('new_value1', headers['header1'])
-    self.assertEquals('hEADEr1', entry._wpr_response.headers[1][0])
+    self.assertEquals('hEADEr1', entry._wpr_response.original_headers[1][0])
 
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
@@ -94,9 +94,9 @@ class WprUrlEntryTest(unittest.TestCase):
     headers = entry.GetResponseHeadersDict()
     self.assertEquals(4, len(headers))
     self.assertEquals('new_value2', headers['header1'])
-    self.assertEquals('header1', entry._wpr_response.headers[1][0])
-    self.assertEquals('header3', entry._wpr_response.headers[3][0])
-    self.assertEquals('value4', entry._wpr_response.headers[3][1])
+    self.assertEquals('header1', entry._wpr_response.original_headers[1][0])
+    self.assertEquals('header3', entry._wpr_response.original_headers[3][0])
+    self.assertEquals('value4', entry._wpr_response.original_headers[3][1])
 
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('heADer1', 'value1'),
@@ -108,9 +108,9 @@ class WprUrlEntryTest(unittest.TestCase):
     headers = entry.GetResponseHeadersDict()
     self.assertEquals(4, len(headers))
     self.assertEquals('new_value2', headers['header1'])
-    self.assertEquals('heADer1', entry._wpr_response.headers[1][0])
-    self.assertEquals('header3', entry._wpr_response.headers[3][0])
-    self.assertEquals('value4', entry._wpr_response.headers[3][1])
+    self.assertEquals('heADer1', entry._wpr_response.original_headers[1][0])
+    self.assertEquals('header3', entry._wpr_response.original_headers[3][0])
+    self.assertEquals('value4', entry._wpr_response.original_headers[3][1])
 
   def testDeleteResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
@@ -146,8 +146,9 @@ class WprUrlEntryTest(unittest.TestCase):
     entry.RemoveResponseHeaderDirectives('header0', {'keyword1'})
     self.assertEquals(
         'keYWOrd0,keYwoRd2', entry.GetResponseHeadersDict()['header0'])
-    self.assertEquals(3, len(entry._wpr_response.headers))
-    self.assertEquals('keYWOrd0,keYwoRd2', entry._wpr_response.headers[0][1])
+    self.assertEquals(3, len(entry._wpr_response.original_headers))
+    self.assertEquals(
+        'keYWOrd0,keYwoRd2', entry._wpr_response.original_headers[0][1])
 
 
 class WprHostTest(unittest.TestCase):

commit b416bc13a7cd00a0f62584f13f57839268732626
Author: pasko <pasko@chromium.org>
Date:   Mon May 23 05:05:05 2016 -0700

    More detailed error message for empty devtools response
    
    Before running each benchmark we:
    1. open about:blank in Chrome
    2. connect to the page via devtools protocol
    3. verify that it is indeed the about:blank on the front
    
    But if we connected to a different Chrome instance from the one
    we opened about:blank with, this last check will likely fail.
    
    A devtools request /json/list would then return URLs without 'about:blank', it
    even can return an empty list.
    
    In this change we just provide this hint in an extra error message.
    
    BUG=none
    
    Review-Url: https://codereview.chromium.org/2003713002
    Cr-Original-Commit-Position: refs/heads/master@{#395303}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2d99cbba382be1fcbd496d8f8f5c68d033e8026e

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index c2d5971..f81ea4e 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -388,6 +388,9 @@ class DevToolsConnection(object):
       if target_descriptor['type'] == 'page':
         self._target_descriptor = target_descriptor
         break
+    if not self._target_descriptor:
+      raise DevToolsConnectionException(
+        'No pages are open, connected to a wrong instance?')
     if self._target_descriptor['url'] != 'about:blank':
       raise DevToolsConnectionException(
           'Looks like devtools connection was made to a different instance.')

commit 47e86cdb2d34af00c2d6cb6cc1223dd04fde38f4
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 23 05:04:16 2016 -0700

    sandwich: Re-try run when an intermittent failure happen
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1977573002
    Cr-Original-Commit-Position: refs/heads/master@{#395302}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 695a8a004fa9e2b983222715a48ddf3fe84b8066

diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 03c6b04..b532e47 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -84,6 +84,7 @@ class SandwichRunner(object):
   using the PullConfigFromArgs() method. The only job is to make sure that the
   command line flags have `dest` parameter set to existing runner members.
   """
+  _ATTEMPT_COUNT = 3
 
   def __init__(self):
     """Configures a sandwich runner out of the box.
@@ -224,17 +225,32 @@ class SandwichRunner(object):
       trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, run_id):
-    self._chrome_ctl.ResetBrowserState()
-    clear_cache = False
-    if self.cache_operation == 'clear':
-      clear_cache = True
-    elif self.cache_operation == 'push':
-      self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
-    elif self.cache_operation == 'reload':
-      self._RunNavigation(url, clear_cache=True)
-    elif self.cache_operation == 'save':
-      clear_cache = run_id == 0
-    self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
+    for attempt_id in xrange(self._ATTEMPT_COUNT):
+      try:
+        self._chrome_ctl.ResetBrowserState()
+        clear_cache = False
+        if self.cache_operation == 'clear':
+          clear_cache = True
+        elif self.cache_operation == 'push':
+          self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
+        elif self.cache_operation == 'reload':
+          self._RunNavigation(url, clear_cache=True)
+        elif self.cache_operation == 'save':
+          clear_cache = run_id == 0
+        self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
+        break
+      except controller.ChromeControllerError as error:
+        if not error.IsIntermittent():
+          raise
+        if self.trace_output_directory is not None:
+          dump_path = os.path.join(self.trace_output_directory, str(run_id),
+                                   'error{}'.format(attempt_id))
+          with open(dump_path, 'w') as dump_output:
+            error.Dump(dump_output)
+    else:
+      logging.error('Failed to navigate to %s after %d attemps' % \
+                    (url, self._ATTEMPT_COUNT))
+      raise
 
   def _PullCacheFromDevice(self):
     assert self.cache_operation == 'save'

commit da7114f444388485f39e29b8ba17cffbfded2c52
Author: ianwen <ianwen@chromium.org>
Date:   Fri May 20 16:55:17 2016 -0700

    [Android] Coordinate Infobars and Snackbars
    
    CoordinatorLayout in android design support library makes it possible to
    coordinate views' animation. This CL makes CompositorViewHolder a
    CoordinatorLayout, and adds several behaviors to control the interaction
    between snackbars and infobars.
    
    After this change, CompositorViewHolder will first check if a touch
    event is handled by behaviors in the children views, then give the
    touchevent to fullscreen manager and layout manager afterwards.
    
    BUG=581227
    
    Review-Url: https://codereview.chromium.org/1983353002
    Cr-Original-Commit-Position: refs/heads/master@{#395210}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 87ee877992e41f3037a23a164abf4689c1ba603c

diff --git a/eclipse/.classpath b/eclipse/.classpath
index a6d7e26..f9bb406 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -191,6 +191,7 @@ to the classpath for downstream development. See "additional_entries" below.
     </classpathentry>
     <classpathentry kind="lib" path="third_party/android_tools/sdk/platforms/android-23/data/layoutlib.jar" sourcepath="third_party/android_tools/sdk/sources/"/>
     <classpathentry kind="lib" path="third_party/android_tools/sdk/platforms/android-23/uiautomator.jar" sourcepath="third_party/android_tools/sdk/sources"/>
+    <classpathentry kind="lib" path="third_party/android_tools/sdk/extras/android/support/design/libs/android-support-design.jar" sourcepath="third_party/android_tools/sdk/sources"/>
     <classpathentry kind="lib" path="third_party/android_tools/sdk/extras/android/support/v7/mediarouter/libs/android-support-v7-mediarouter.jar" sourcepath="third_party/android_tools/sdk/sources"/>
     <classpathentry kind="lib" path="third_party/android_tools/sdk/extras/android/support/v7/recyclerview/libs/android-support-v7-recyclerview.jar" sourcepath="third_party/android_tools/sdk/sources"/>
     <classpathentry kind="lib" path="third_party/android_tools/sdk/extras/android/support/v13/android-support-v13.jar" sourcepath="third_party/android_tools/sdk/sources"/>

commit 085e4435bf72965718fb894efbb393a6439ea8a9
Author: droger <droger@chromium.org>
Date:   Fri May 20 08:43:16 2016 -0700

    tools/android/loading Write compact traces
    
    The method ToJsonFile is writing traces with indentation off
    intentionnally to save disk space.
    This CL uses this function instead of custom code, resulting in smaller
    traces.
    
    Review-Url: https://codereview.chromium.org/1999063002
    Cr-Original-Commit-Position: refs/heads/master@{#395077}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a03172520e7151b891c8e515fc700aeeffbb267c

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index 1488d02..1a9cf9a 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -2,7 +2,6 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-import json
 import multiprocessing
 import os
 import re
@@ -72,8 +71,7 @@ def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
       traceback.print_exc(file=sys.stderr)
 
     if trace:
-      with open(filename, 'w') as f:
-        json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+      trace.ToJsonFile(filename)
 
   sys.stdout = old_stdout
   sys.stderr = old_stderr

commit bbc88666da305c0e0fa759dcb6f564d6a2da94e3
Author: lizeb <lizeb@chromium.org>
Date:   Fri May 20 07:45:16 2016 -0700

    clovis: Plumbing for the ad filtering in the report generation.
    
    Review-Url: https://codereview.chromium.org/2001603002
    Cr-Original-Commit-Position: refs/heads/master@{#395072}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 84776397a2ab2e734220dd18b7cd564580a722ab

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index befe05a..3df4f60 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -104,6 +104,8 @@ dictionary with the keys:
 -   `src_path` (string): Path to the Chromium source directory.
 -   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
     `clovis-queue`.
+-   `ad_rules_filename` and `tracking_rules_filename` (string): Path to the ad
+     and tracking filtering rules.
 -   `instance_name` (string, optional): Name of the Compute Engine instance this
     script is running on.
 -   `worker_log_file` (string, optional): Path to the log file capturing the
diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index ddd07bb..92499e8 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -14,13 +14,20 @@ class ClovisTaskHandler(object):
   """Handles all the supported clovis tasks."""
 
   def __init__(self, project_name, base_path, failure_database,
-               google_storage_accessor, bigquery_service, binaries_path, logger,
+               google_storage_accessor, bigquery_service, binaries_path,
+               ad_rules_filename, tracking_rules_filename, logger,
                instance_name=None):
     """Creates a ClovisTaskHandler.
 
     Args:
+      project_name (str): Name of the project.
       base_path(str): Base path where results are written.
+      failure_database (FailureDatabase): Failure Database.
+      google_storage_accessor (GoogleStorageAccessor): Cloud storage accessor.
+      bigquery_service (googleapiclient.discovery.Resource): Bigquery service.
       binaries_path(str): Path to the directory where Chrome executables are.
+      ad_rules_filename (str): Path to the ad filtering rules.
+      tracking_rules_filename (str): Path to the tracking filtering rules.
       instance_name(str, optional): Name of the ComputeEngine instance.
     """
     self._failure_database = failure_database
@@ -31,7 +38,8 @@ class ClovisTaskHandler(object):
             binaries_path, logger, instance_name),
         'report': ReportTaskHandler(
             project_name, failure_database, google_storage_accessor,
-            bigquery_service, logger)}
+            bigquery_service, logger, ad_rules_filename,
+            tracking_rules_filename)}
 
   def Run(self, clovis_task):
     """Runs a clovis_task.
diff --git a/loading/cloud/backend/pip_requirements.txt b/loading/cloud/backend/pip_requirements.txt
index 390863a..f2eb36b 100644
--- a/loading/cloud/backend/pip_requirements.txt
+++ b/loading/cloud/backend/pip_requirements.txt
@@ -1,3 +1,4 @@
 gcloud==0.10.1
 google-api-python-client==1.5.0
 psutil==4.1.0
+adblockparser==0.5
diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index 24a891b..0081139 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -60,12 +60,15 @@ class ReportTaskHandler(object):
   """
 
   def __init__(self, project_name, failure_database, google_storage_accessor,
-               bigquery_service, logger):
+               bigquery_service, logger, ad_rules_filename,
+               tracking_rules_filename):
     self._project_name = project_name
     self._failure_database = failure_database
     self._google_storage_accessor = google_storage_accessor
     self._bigquery_service = bigquery_service
     self._logger = logger
+    self._ad_rules_filename = ad_rules_filename
+    self._tracking_rules_filename = tracking_rules_filename
 
   def _StreamRowsToBigQuery(self, rows, table_id):
     """Uploads a list of rows to the BigQuery table associated with the given
@@ -123,6 +126,9 @@ class ReportTaskHandler(object):
                                         'report_task_handler_run')
       return
 
+    ad_rules = open(self._ad_rules_filename).readlines()
+    tracking_rules = open(self._tracking_rules_filename).readlines()
+
     rows = []
     for path in clovis_task.ActionParams()['traces']:
       self._logger.info('Generating report for: ' + path)
@@ -131,7 +137,7 @@ class ReportTaskHandler(object):
         self._logger.error('Failed loading trace at: ' + path)
         self._failure_database.AddFailure('missing_trace_for_report', path)
         continue
-      report = LoadingReport(trace).GenerateReport()
+      report = LoadingReport(trace, ad_rules, tracking_rules).GenerateReport()
       if not report:
         self._logger.error('Failed generating report for: ' + path)
         self._failure_database.AddFailure('report_generation_failed', path)
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index 9660fc0..c74e56e 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -60,6 +60,13 @@ mkdir /opt/app/clovis/binaries
 gsutil cp gs://$DEPLOYMENT_PATH/binaries/* /opt/app/clovis/binaries/
 unzip /opt/app/clovis/binaries/linux.zip -d /opt/app/clovis/binaries/
 
+# Ad and tracking filtering rules.
+# Made by the EasyList authors (https://easylist.github.io/).
+DATA_DIR=/opt/app/clovis/data
+mkdir $DATA_DIR && cd $DATA_DIR
+curl https://easylist.github.io/easylist/easylist.txt > easylist.txt
+curl https://easylist.github.io/easylist/easyprivacy.txt > easyprivacy.txt
+
 # Install the Chrome sandbox
 cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
 chown root:root /usr/local/sbin/chrome-devel-sandbox
@@ -87,7 +94,9 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
   "worker_log_path" : "$WORKER_LOG_PATH",
-  "self_destruct" : "$SELF_DESTRUCT"
+  "self_destruct" : "$SELF_DESTRUCT",
+  "ad_rules_filename": "$DATA_DIR/easylist.txt",
+  "tracking_rules_filename": "$DATA_DIR/easyprivacy.txt"
 }
 EOF
 
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 3d44f1e..bc25bf9 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -65,7 +65,7 @@ class Worker(object):
                                                failure_database_filename)
 
     # Recover any existing failures in case the worker died.
-    self._DownloadFailureDatabase()
+    self._failure_database = self._GetFailureDatabase()
 
     if self._failure_database.ToJsonDict():
       # Script is restarting after a crash, or there are already files from a
@@ -74,11 +74,12 @@ class Worker(object):
                                         'failure_database')
 
     bigquery_service = discovery.build('bigquery', 'v2',
-                                      credentials=self._credentials)
+                                       credentials=self._credentials)
     self._clovis_task_handler = ClovisTaskHandler(
         self._project_name, self._base_path_in_bucket, self._failure_database,
         self._google_storage_accessor, bigquery_service,
-        config['binaries_path'], self._logger, self._instance_name)
+        config['binaries_path'], config['ad_rules_filename'],
+        config['tracking_rules_filename'], self._logger, self._instance_name)
 
     self._UploadFailureDatabase()
 
@@ -111,12 +112,12 @@ class Worker(object):
       self._logger.info('Finished task %s' % task_id)
     self._Finalize()
 
-  def _DownloadFailureDatabase(self):
+  def _GetFailureDatabase(self):
     """Downloads the failure database from CloudStorage."""
     self._logger.info('Downloading failure database')
     failure_database_string = self._google_storage_accessor.DownloadAsString(
         self._failure_database_path)
-    self._failure_database = FailureDatabase(failure_database_string)
+    return FailureDatabase(failure_database_string)
 
   def _UploadFailureDatabase(self):
     """Uploads the failure database to CloudStorage."""
@@ -211,4 +212,3 @@ if __name__ == '__main__':
   with open(args.config) as config_json:
     worker = Worker(json.load(config_json), worker_logger)
     worker.Start()
-

commit e2a897df328b229038054a244755b8b24eccfdf3
Author: lizeb <lizeb@chromium.org>
Date:   Fri May 20 03:28:58 2016 -0700

    clovis: Report ad and tracking transfer size, as well as total transfer size.
    
    Review-Url: https://codereview.chromium.org/1992223002
    Cr-Original-Commit-Position: refs/heads/master@{#395048}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5955d708aeb35ff9062adff6a1647680fca1cd43

diff --git a/loading/metrics.py b/loading/metrics.py
index 5f5e560..deb4619 100644
--- a/loading/metrics.py
+++ b/loading/metrics.py
@@ -12,6 +12,7 @@ page, with a given time interval.
 import content_classification_lens
 from request_track import CachingPolicy
 
+HTTP_OK_LENGTH = len("HTTP/1.1 200 OK\r\n")
 
 def _RequestTransferSize(request):
   def HeadersSize(headers):
@@ -25,7 +26,7 @@ def _RequestTransferSize(request):
           'body': request.encoded_data_length}
 
 
-def _TransferSize(requests):
+def TransferSize(requests):
   """Returns the total transfer size (uploaded, downloaded) of requests.
 
   This is an estimate as we assume:
@@ -43,7 +44,7 @@ def _TransferSize(requests):
   for request in requests:
     request_bytes = _RequestTransferSize(request)
     uploaded_bytes += request_bytes['get'] + request_bytes['request_headers']
-    downloaded_bytes += (len('HTTP/1.1 200 OK')
+    downloaded_bytes += (HTTP_OK_LENGTH
                          + request_bytes['response_headers']
                          + request_bytes['body'])
   return (uploaded_bytes, downloaded_bytes)
@@ -51,7 +52,7 @@ def _TransferSize(requests):
 
 def TotalTransferSize(trace):
   """Returns the total transfer size (uploaded, downloaded) from a trace."""
-  return _TransferSize(trace.request_track.GetEvents())
+  return TransferSize(trace.request_track.GetEvents())
 
 
 def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
@@ -80,7 +81,7 @@ def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
         and caching_policy.HasValidators() and assume_validation_ok):
       downloaded_bytes += len('HTTP/1.1 304 NOT MODIFIED\r\n')
       continue
-    downloaded_bytes += (len('HTTP/1.1 200 OK\r\n')
+    downloaded_bytes += (HTTP_OK_LENGTH
                          + request_bytes['response_headers']
                          + request_bytes['body'])
   return (uploaded_bytes, downloaded_bytes)
@@ -102,7 +103,7 @@ def AdsAndTrackingTransferSize(trace, ad_rules_filename,
       content_classification_lens.ContentClassificationLens.WithRulesFiles(
           trace, ad_rules_filename, tracking_rules_filename))
   requests = content_lens.AdAndTrackingRequests()
-  return _TransferSize(requests)
+  return TransferSize(requests)
 
 
 def PlotTransferSizeVsTimeBetweenVisits(trace):
diff --git a/loading/metrics_unittest.py b/loading/metrics_unittest.py
index cc439ce..56153bf 100644
--- a/loading/metrics_unittest.py
+++ b/loading/metrics_unittest.py
@@ -65,6 +65,13 @@ class MetricsTestCase(unittest.TestCase):
         self._BODY_SIZE + self._RESPONSE_HEADERS_SIZE + cache_control_length,
         downloaded)
 
+  def testTransferSize(self):
+    trace = self._MakeTrace()
+    r = trace.request_track.GetEvents()[0]
+    (_, downloaded) = metrics.TransferSize([r])
+    self.assertEqual(self._BODY_SIZE + self._RESPONSE_HEADERS_SIZE,
+                     downloaded)
+
   @classmethod
   def _MakeTrace(cls):
     request = request_track.Request.FromJsonDict(copy.deepcopy(cls._REQUEST))
diff --git a/loading/report.py b/loading/report.py
index 9599ab0..a32924c 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -10,6 +10,7 @@ When executed as a script, takes a trace filename and print the report.
 from content_classification_lens import ContentClassificationLens
 from loading_graph_view import LoadingGraphView
 import loading_trace
+import metrics
 from network_activity_lens import NetworkActivityLens
 from user_satisfied_lens import (
     FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
@@ -55,6 +56,7 @@ class LoadingReport(object):
         self._contentful_paint_msec)
     self._significant_inversion = graph.GetInversionsAtTime(
         self._significant_paint_msec)
+    self._transfer_size = metrics.TotalTransferSize(trace)[1]
 
   def GenerateReport(self):
     """Returns a report as a dict."""
@@ -75,7 +77,8 @@ class LoadingReport(object):
                                  else None),
         'significant_inversion': (self._significant_inversion[0].url
                                   if self._significant_inversion
-                                  else None)}
+                                  else None),
+        'transfer_size': self._transfer_size}
     report.update(self._ad_report)
     return report
 
@@ -95,7 +98,8 @@ class LoadingReport(object):
         'ad_requests': 0 if ad_rules else None,
         'tracking_requests': 0 if tracking_rules else None,
         'ad_or_tracking_requests': 0 if has_rules else None,
-        'ad_or_tracking_initiated_requests': 0 if has_rules else None}
+        'ad_or_tracking_initiated_requests': 0 if has_rules else None,
+        'ad_or_tracking_initiated_transfer_size': 0 if has_rules else None}
     content_classification_lens = ContentClassificationLens(
         trace, ad_rules, tracking_rules)
     if not has_rules:
@@ -108,8 +112,10 @@ class LoadingReport(object):
       if tracking_rules:
         result['tracking_requests'] += int(is_tracking)
       result['ad_or_tracking_requests'] += int(is_ad or is_tracking)
-    result['ad_or_tracking_initiated_requests'] = len(
-        content_classification_lens.AdAndTrackingRequests())
+    ad_tracking_requests = content_classification_lens.AdAndTrackingRequests()
+    result['ad_or_tracking_initiated_requests'] = len(ad_tracking_requests)
+    result['ad_or_tracking_initiated_transfer_size'] = metrics.TransferSize(
+        ad_tracking_requests)[1]
     return result
 
   @classmethod
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 718d423..3c6b216 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -4,6 +4,7 @@
 
 import unittest
 
+import metrics
 import report
 import test_utils
 import user_satisfied_lens_unittest
@@ -20,6 +21,8 @@ class LoadingReportTestCase(unittest.TestCase):
   _REQUEST_OFFSET = 5
   _LOAD_END_TIME = 1280
   _MAIN_FRAME_ID = 1
+  _FIRST_REQUEST_DATA_LENGTH = 128
+  _SECOND_REQUEST_DATA_LENGTH = 1024
 
   def setUp(self):
     self.trace_creator = test_utils.TraceCreator()
@@ -29,8 +32,8 @@ class LoadingReportTestCase(unittest.TestCase):
             self._NAVIGATION_START_TIME + self._REQUEST_OFFSET, self._DURATION)]
     self.requests[0].timing.receive_headers_end = 0
     self.requests[1].timing.receive_headers_end = 0
-    self.requests[0].encoded_data_length = 128
-    self.requests[1].encoded_data_length = 1024
+    self.requests[0].encoded_data_length = self._FIRST_REQUEST_DATA_LENGTH
+    self.requests[1].encoded_data_length = self._SECOND_REQUEST_DATA_LENGTH
 
     self.trace_events = [
         {'ts': self._NAVIGATION_START_TIME * self.MILLI_TO_MICRO, 'ph': 'R',
@@ -82,6 +85,11 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertIsNone(loading_report['ad_requests'])
     self.assertIsNone(loading_report['ad_or_tracking_requests'])
     self.assertIsNone(loading_report['ad_or_tracking_initiated_requests'])
+    self.assertIsNone(loading_report['ad_or_tracking_initiated_transfer_size'])
+    self.assertEqual(
+        self._FIRST_REQUEST_DATA_LENGTH + self._SECOND_REQUEST_DATA_LENGTH
+        + metrics.HTTP_OK_LENGTH * 2,
+        loading_report['transfer_size'])
 
   def testInversion(self):
     self.requests[0].timing.loading_finished = 4 * (
@@ -119,6 +127,9 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertEqual(1, loading_report['ad_or_tracking_requests'])
     self.assertEqual(1, loading_report['ad_or_tracking_initiated_requests'])
     self.assertIsNone(loading_report['tracking_requests'])
+    self.assertEqual(
+        self._FIRST_REQUEST_DATA_LENGTH + metrics.HTTP_OK_LENGTH,
+        loading_report['ad_or_tracking_initiated_transfer_size'])
 
 
 if __name__ == '__main__':

commit 1ba7b3100c02eb4689b267bfca47609122abfa2c
Author: droger <droger@chromium.org>
Date:   Fri May 20 02:53:44 2016 -0700

    tools/android/loading Improve README.md files.
    
    Review-Url: https://codereview.chromium.org/1984213002
    Cr-Original-Commit-Position: refs/heads/master@{#395043}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ff2b7d2d195dfaecb2b14128e8d11fefa6c7cb38

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 4e906cc..befe05a 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -17,6 +17,11 @@ The main files for the backend are:
 -   `startup-script.sh`: initializes an instance (installs the dependencies,
     downloads the code and the configuration).
 -   `worker.py`: the main worker script.
+-   Task handlers have a `Run()` method taking a `ClovisTask` parameter.
+    -   `clovis_task_handler.py`: Main entry point, dispatches the tasks to the
+        more specialized handlers below.
+    -   `trace_task_handler.py`: Handles `trace` tasks.
+    -   `report_task_handler.py`: Handles `report` tasks.
 
 [TOC]
 
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index 1384d10..0b564e1 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -47,10 +47,18 @@ generates a report in BigQuery.
 
 This action has no parameters.
 
+This requires an existing `clovis_dataset.report` BigQuery table that will be
+used as a template. The schema of this template is not updated automatically and
+must match the format of the report (as generated by `report.py`).
+To update the schema manually, delete and recreate the `clovis_dataset.report`
+from the BigQuery web interface as an empty table with the new schema.
+
 ## Development
 
 ### Design overview
 
+This is a [python AppEngine][5] application using [Flask][6].
+
 -   Appengine configuration:
     -   `app.yaml` defines the handlers. There is a static handler for all URLs
     in the `static/` directory, and all other URLs are handled by the
@@ -59,8 +67,8 @@ This action has no parameters.
         particular, the `clovis-queue` is a pull-queue where tasks are added by
         the AppEngine frontend and consummed by the ComputeEngine backend.
         See the [TaskQueue documentation][2] for more details.
--   `static/form.html` is a static HTML document allowing the user to upload a
-    JSON file. `clovis_frontend.py` is then invoked with the contents of the
+-   `templates/form.html` is a static HTML document allowing the user to upload
+    a JSON file. `clovis_frontend.py` is then invoked with the contents of the
     file (see the `/form_sent` handler).
 -   `clovis_task.py` defines a task to be run by the backend. It is sent through
     the `clovis-queue` task queue.
@@ -81,7 +89,7 @@ This action has no parameters.
 - name: clovis-queue
   mode: pull
   acl:
-    - user_email: me@address.com
+    - user_email: me@address.com  # For local development.
     - user_email: 123456789-compute@developer.gserviceaccount.com
 ```
 
@@ -116,3 +124,5 @@ gcloud preview app deploy app.yaml
 [2]: https://cloud.google.com/appengine/docs/python/taskqueue
 [3]: https://cloud.google.com/appengine/docs/python/config/queue
 [4]: ../backend/README.md#Deploy-the-code
+[5]: https://cloud.google.com/appengine/docs/python
+[6]: http://flask.pocoo.org

commit 5249a26b9907be045bc9f952eb3d6c7fd3fd6ed2
Author: lizeb <lizeb@chromium.org>
Date:   Thu May 19 09:37:52 2016 -0700

    Clovis: Report how many requests were due to / linked to ads/tracking.
    
    Review-Url: https://codereview.chromium.org/1990943002
    Cr-Original-Commit-Position: refs/heads/master@{#394792}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: af5651656d110a5253a2324a52d694f047896bb3

diff --git a/loading/report.py b/loading/report.py
index b74658f..9599ab0 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -7,6 +7,7 @@
 When executed as a script, takes a trace filename and print the report.
 """
 
+from content_classification_lens import ContentClassificationLens
 from loading_graph_view import LoadingGraphView
 import loading_trace
 from network_activity_lens import NetworkActivityLens
@@ -16,11 +17,13 @@ from user_satisfied_lens import (
 
 class LoadingReport(object):
   """Generates a loading report from a loading trace."""
-  def __init__(self, trace):
+  def __init__(self, trace, ad_rules=None, tracking_rules=None):
     """Constructor.
 
     Args:
       trace: (LoadingTrace) a loading trace.
+      ad_rules: ([str]) List of ad filtering rules.
+      tracking_rules: ([str]) List of tracking filtering rules.
     """
     self.trace = trace
     self._text_msec = FirstTextPaintLens(self.trace).SatisfiedMs()
@@ -44,6 +47,8 @@ class LoadingReport(object):
     else:
       self._contentful_byte_frac = float('Nan')
       self._significant_byte_frac = float('Nan')
+    self._ad_report = self._AdRequestsReport(
+        trace, ad_rules or [], tracking_rules or [])
 
     graph = LoadingGraphView.FromTrace(trace)
     self._contentful_inversion = graph.GetInversionsAtTime(
@@ -53,7 +58,7 @@ class LoadingReport(object):
 
   def GenerateReport(self):
     """Returns a report as a dict."""
-    return {
+    report = {
         'url': self.trace.url,
         'first_text_ms': self._text_msec - self._navigation_start_msec,
         'contentful_paint_ms': (self._contentful_paint_msec
@@ -70,14 +75,42 @@ class LoadingReport(object):
                                  else None),
         'significant_inversion': (self._significant_inversion[0].url
                                   if self._significant_inversion
-                                  else None)
-    }
+                                  else None)}
+    report.update(self._ad_report)
+    return report
 
   @classmethod
-  def FromTraceFilename(cls, filename):
+  def FromTraceFilename(cls, filename, ad_rules_filename,
+                        tracking_rules_filename):
     """Returns a LoadingReport from a trace filename."""
     trace = loading_trace.LoadingTrace.FromJsonFile(filename)
-    return LoadingReport(trace)
+    return LoadingReport(trace, ad_rules_filename, tracking_rules_filename)
+
+  @classmethod
+  def _AdRequestsReport(cls, trace, ad_rules, tracking_rules):
+    has_rules = bool(ad_rules) or bool(tracking_rules)
+    requests = trace.request_track.GetEvents()
+    result = {
+        'request_count': len(requests),
+        'ad_requests': 0 if ad_rules else None,
+        'tracking_requests': 0 if tracking_rules else None,
+        'ad_or_tracking_requests': 0 if has_rules else None,
+        'ad_or_tracking_initiated_requests': 0 if has_rules else None}
+    content_classification_lens = ContentClassificationLens(
+        trace, ad_rules, tracking_rules)
+    if not has_rules:
+      return result
+    for request in trace.request_track.GetEvents():
+      is_ad = content_classification_lens.IsAdRequest(request)
+      is_tracking = content_classification_lens.IsTrackingRequest(request)
+      if ad_rules:
+        result['ad_requests'] += int(is_ad)
+      if tracking_rules:
+        result['tracking_requests'] += int(is_tracking)
+      result['ad_or_tracking_requests'] += int(is_ad or is_tracking)
+    result['ad_or_tracking_initiated_requests'] = len(
+        content_classification_lens.AdAndTrackingRequests())
+    return result
 
   @classmethod
   def _ComputePlt(cls, trace):
@@ -93,11 +126,18 @@ class LoadingReport(object):
     return max(r.end_msec or -1 for r in trace.request_track.GetEvents())
 
 
+def _Main(args):
+  assert len(args) == 4, 'Usage: report.py trace.json ad_rules tracking_rules'
+  trace_filename = args[1]
+  ad_rules = open(args[2]).readlines()
+  tracking_rules = open(args[3]).readlines()
+  report = LoadingReport.FromTraceFilename(
+      trace_filename, ad_rules, tracking_rules)
+  print json.dumps(report.GenerateReport(), indent=2)
+
+
 if __name__ == '__main__':
   import sys
   import json
 
-  trace_filename = sys.argv[1]
-  print json.dumps(
-      LoadingReport.FromTraceFilename(trace_filename).GenerateReport(),
-      indent=2)
+  _Main(sys.argv)
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index cdb6a8d..718d423 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -23,10 +23,10 @@ class LoadingReportTestCase(unittest.TestCase):
 
   def setUp(self):
     self.trace_creator = test_utils.TraceCreator()
-    self.requests = [self.trace_creator.RequestAt(self._FIRST_REQUEST_TIME),
-                     self.trace_creator.RequestAt(
-                         self._NAVIGATION_START_TIME + self._REQUEST_OFFSET,
-                         self._DURATION)]
+    self.requests = [
+        self.trace_creator.RequestAt(self._FIRST_REQUEST_TIME, frame_id=1),
+        self.trace_creator.RequestAt(
+            self._NAVIGATION_START_TIME + self._REQUEST_OFFSET, self._DURATION)]
     self.requests[0].timing.receive_headers_end = 0
     self.requests[1].timing.receive_headers_end = 0
     self.requests[0].encoded_data_length = 128
@@ -77,8 +77,11 @@ class LoadingReportTestCase(unittest.TestCase):
                            loading_report['plt_ms'])
     self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'], 2)
     self.assertAlmostEqual(0.1844, loading_report['significant_byte_frac'], 2)
-    self.assertEqual(None, loading_report['contentful_inversion'])
-    self.assertEqual(None, loading_report['significant_inversion'])
+    self.assertIsNone(loading_report['contentful_inversion'])
+    self.assertIsNone(loading_report['significant_inversion'])
+    self.assertIsNone(loading_report['ad_requests'])
+    self.assertIsNone(loading_report['ad_or_tracking_requests'])
+    self.assertIsNone(loading_report['ad_or_tracking_initiated_requests'])
 
   def testInversion(self):
     self.requests[0].timing.loading_finished = 4 * (
@@ -106,6 +109,17 @@ class LoadingReportTestCase(unittest.TestCase):
     self.assertAlmostEqual(self._REQUEST_OFFSET + self._DURATION,
                            loading_report['plt_ms'])
 
+  def testAdTrackingRules(self):
+    ad_domain = 'i-ve-got-the-best-ads.com'
+    self.requests[0].url = 'http://www.' + ad_domain
+    trace = self._MakeTrace()
+    loading_report = report.LoadingReport(
+        trace, [ad_domain], []).GenerateReport()
+    self.assertEqual(1, loading_report['ad_requests'])
+    self.assertEqual(1, loading_report['ad_or_tracking_requests'])
+    self.assertEqual(1, loading_report['ad_or_tracking_initiated_requests'])
+    self.assertIsNone(loading_report['tracking_requests'])
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 5f0d743..8858da2 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -231,21 +231,25 @@ class TraceCreator(object):
   def __init__(self):
     self._request_index = 1
 
-  def RequestAt(self, timestamp_msec, duration=1):
+  def RequestAt(self, timestamp_msec, duration=1, frame_id=None):
     timestamp_sec = float(timestamp_msec) / 1000
     rq = request_track.Request.FromJsonDict({
         'url': 'http://bla-%s-.com' % timestamp_msec,
+        'document_url': 'http://bla.com',
         'request_id': '0.%s' % self._request_index,
-        'frame_id': '123.%s' % timestamp_msec,
+        'frame_id': frame_id or '123.%s' % timestamp_msec,
         'initiator': {'type': 'other'},
         'timestamp': timestamp_sec,
         'timing': {'request_time': timestamp_sec,
-                   'loading_finished': duration}
-        })
+                   'loading_finished': duration},
+        'status': 200})
     self._request_index += 1
     return rq
 
   def CreateTrace(self, requests, events, main_frame_id):
-    trace = LoadingTraceFromEvents(requests, trace_events=events)
+    page_event = {'method': 'Page.frameStartedLoading',
+                  'frame_id': main_frame_id}
+    trace = LoadingTraceFromEvents(
+        requests, trace_events=events, page_events=[page_event])
     trace.tracing_track.SetMainFrameID(main_frame_id)
     return trace

commit ce439b3533799067ff1c3b78241635f6d1bccabf
Author: droger <droger@chromium.org>
Date:   Thu May 19 09:23:06 2016 -0700

    tools/android/loading Generate traces out of process
    
    Generating the trace in the current process was causing
    the system to run out of memory, presumably because
    the python garbage collector was not releasing memory.
    
    Collecting the trace in a separate process allows the system
    to reclaim the memory.
    
    Review-Url: https://codereview.chromium.org/1992663004
    Cr-Original-Commit-Position: refs/heads/master@{#394785}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e7ce81949b779eecc5303ae14b0dd0b1025d7762

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index 82777ba..1488d02 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import json
+import multiprocessing
 import os
 import re
 import sys
@@ -18,6 +19,68 @@ import options
 import xvfb_helper
 
 
+def GenerateTrace(url, emulate_device, emulate_network, filename, log_filename):
+  """ Generates a trace.
+
+  Args:
+    url: URL as a string.
+    emulate_device: Name of the device to emulate. Empty for no emulation.
+    emulate_network: Type of network emulation. Empty for no emulation.
+    filename: Name of the file where the trace is saved.
+    log_filename: Name of the file where standard output and errors are
+                  logged.
+
+  Returns:
+    A dictionary of metadata about the trace, including a 'succeeded' field
+    indicating whether the trace was successfully generated.
+  """
+  try:
+    os.remove(filename)  # Remove any existing trace for this URL.
+  except OSError:
+    pass  # Nothing to remove.
+
+  old_stdout = sys.stdout
+  old_stderr = sys.stderr
+
+  trace_metadata = { 'succeeded' : False, 'url' : url }
+  trace = None
+  if not url.startswith('http') and not url.startswith('file'):
+    url = 'http://' + url
+  with open(log_filename, 'w') as sys.stdout:
+    try:
+      sys.stderr = sys.stdout
+
+      # Set up the controller.
+      chrome_ctl = controller.LocalChromeController()
+      chrome_ctl.SetChromeEnvOverride(xvfb_helper.GetChromeEnvironment())
+      if emulate_device:
+        chrome_ctl.SetDeviceEmulation(emulate_device)
+      if emulate_network:
+        chrome_ctl.SetNetworkEmulation(emulate_network)
+
+      # Record and write the trace.
+      with chrome_ctl.Open() as connection:
+        connection.ClearCache()
+        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+            url, connection, chrome_ctl.ChromeMetadata())
+        trace_metadata['succeeded'] = True
+        trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+    except controller.ChromeControllerError as e:
+      e.Dump(sys.stderr)
+    except Exception as e:
+      sys.stderr.write('Unknown exception:\n' + str(e))
+      traceback.print_exc(file=sys.stderr)
+
+    if trace:
+      with open(filename, 'w') as f:
+        json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+
+  sys.stdout = old_stdout
+  sys.stderr = old_stderr
+
+  return trace_metadata
+
+
 class TraceTaskHandler(object):
   """Handles 'trace' tasks."""
 
@@ -78,67 +141,37 @@ class TraceTaskHandler(object):
         self._trace_database.ToJsonString(),
         self._trace_database_path)
 
-  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
-                     log_filename):
-    """ Generates a trace.
+  def _GenerateTraceOutOfProcess(self, url, emulate_device, emulate_network,
+                                 filename, log_filename):
+    """ Generates a trace in a separate process by calling GenerateTrace().
 
-    Args:
-      url: URL as a string.
-      emulate_device: Name of the device to emulate. Empty for no emulation.
-      emulate_network: Type of network emulation. Empty for no emulation.
-      filename: Name of the file where the trace is saved.
-      log_filename: Name of the file where standard output and errors are
-                    logged.
-
-    Returns:
-      A dictionary of metadata about the trace, including a 'succeeded' field
-      indicating whether the trace was successfully generated.
+    The generation is done out of process to avoid issues where the system would
+    run out of memory when the trace is very large. This ensures that the system
+    can reclaim all the memory when the trace generation is done.
+
+    See the GenerateTrace() documentation for a description of the parameters
+    and return values.
     """
-    try:
-      os.remove(filename)  # Remove any existing trace for this URL.
-    except OSError:
-      pass  # Nothing to remove.
-
-    old_stdout = sys.stdout
-    old_stderr = sys.stderr
-
-    trace_metadata = { 'succeeded' : False, 'url' : url }
-    trace = None
-    if not url.startswith('http') and not url.startswith('file'):
-      url = 'http://' + url
-    with open(log_filename, 'w') as sys.stdout:
-      try:
-        sys.stderr = sys.stdout
-
-        # Set up the controller.
-        chrome_ctl = controller.LocalChromeController()
-        chrome_ctl.SetChromeEnvOverride(xvfb_helper.GetChromeEnvironment())
-        if emulate_device:
-          chrome_ctl.SetDeviceEmulation(emulate_device)
-        if emulate_network:
-          chrome_ctl.SetNetworkEmulation(emulate_network)
-
-        # Record and write the trace.
-        with chrome_ctl.Open() as connection:
-          connection.ClearCache()
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url, connection, chrome_ctl.ChromeMetadata())
-          trace_metadata['succeeded'] = True
-          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
-      except controller.ChromeControllerError as e:
-        e.Dump(sys.stderr)
-      except Exception as e:
-        sys.stderr.write('Unknown exception:\n' + str(e))
-        traceback.print_exc(file=sys.stderr)
-
-      if trace:
-        with open(filename, 'w') as f:
-          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
-
-    sys.stdout = old_stdout
-    sys.stderr = old_stderr
-
-    return trace_metadata
+    self._logger.info('Starting external process for trace generation')
+    failed_metadata = {'succeeded':False, 'url':url}
+    pool = multiprocessing.Pool(1)
+
+    apply_result = pool.apply_async(
+        GenerateTrace,
+        (url, emulate_device, emulate_network, filename, log_filename))
+    apply_result.wait(timeout=300)
+
+    if not apply_result.ready():
+      self._logger.error('Process timeout for trace generation of URL: ' + url)
+      self._failure_database.AddFailure('trace_process_timeout', url)
+      return failed_metadata
+
+    if not apply_result.successful():
+      self._logger.error('Process failure for trace generation of URL: ' + url)
+      self._failure_database.AddFailure('trace_process_error', url)
+      return failed_metadata
+
+    return apply_result.get()
 
   def _HandleTraceGenerationResults(self, local_filename, log_filename,
                                     remote_filename, trace_metadata):
@@ -220,7 +253,7 @@ class TraceTaskHandler(object):
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         self._logger.debug('Generating trace for URL: %s' % url)
-        trace_metadata = self._GenerateTrace(
+        trace_metadata = self._GenerateTraceOutOfProcess(
             url, emulate_device, emulate_network, local_filename, log_filename)
         if trace_metadata['succeeded']:
           success_happened = True

commit c8c6eac9f0830691d8aca4bf58c9668a6db6760a
Author: droger <droger@chromium.org>
Date:   Wed May 18 09:14:40 2016 -0700

    tools/android/loading Launch xvfb only once
    
    Xvfb was launched and killed every time a controller
    was used.
    This CL moves the creation of the Xvfb process to
    trace_task_handler.py and analyze.py, so that it is only
    created once.
    
    Review-Url: https://codereview.chromium.org/1984343002
    Cr-Original-Commit-Position: refs/heads/master@{#394435}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 08f5575ae5b7555e02df59f7b80564b75a3f9b7c

diff --git a/loading/analyze.py b/loading/analyze.py
index 05b0a9d..30305f3 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -35,6 +35,7 @@ import loading_trace
 import options
 import request_dependencies_lens
 import request_track
+import xvfb_helper
 
 # TODO(mattcary): logging.info isn't that useful, as the whole (tools) world
 # uses logging info; we need to introduce logging modules to get finer-grained
@@ -99,9 +100,12 @@ def _LogRequests(url, clear_cache_override=None):
   Returns:
     JSON dict of logged information (ie, a dict that describes JSON).
   """
+  xvfb_process = None
   if OPTIONS.local:
     chrome_ctl = controller.LocalChromeController()
-    chrome_ctl.SetHeadless(OPTIONS.headless)
+    if OPTIONS.headless:
+      xvfb_process =  xvfb_helper.LaunchXvfb()
+      chrome_ctl.SetChromeEnvOverride(xvfb_helper.GetChromeEnvironment())
   else:
     chrome_ctl = controller.RemoteChromeController(
         device_setup.GetFirstDevice())
@@ -117,6 +121,10 @@ def _LogRequests(url, clear_cache_override=None):
       connection.ClearCache()
     trace = loading_trace.LoadingTrace.RecordUrlNavigation(
         url, connection, chrome_ctl.ChromeMetadata())
+
+  if xvfb_process:
+    xvfb_process.terminate()
+
   return trace.ToJsonDict()
 
 
diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index 5a8be02..ddd07bb 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -46,3 +46,8 @@ class ClovisTaskHandler(object):
                                         clovis_task.Action())
       return
     handler.Run(clovis_task)
+
+  def Finalize(self):
+    """Called once before the handler is destroyed."""
+    for handler in self._handlers.values():
+      handler.Finalize()
diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index cd54ceb..24a891b 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -107,6 +107,10 @@ class ReportTaskHandler(object):
         self._failure_database.AddFailure('big_query_insert_error',
                                           str(insert_error.get('errors')))
 
+  def Finalize(self):
+    """Called once before the handler is destroyed."""
+    pass
+
   def Run(self, clovis_task):
     """Runs a 'report' clovis_task.
 
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index d718ab7..82777ba 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -6,6 +6,7 @@ import json
 import os
 import re
 import sys
+import traceback
 
 import common.clovis_paths
 from common.clovis_task import ClovisTask
@@ -14,6 +15,7 @@ import controller
 from failure_database import FailureDatabase
 import loading_trace
 import options
+import xvfb_helper
 
 
 class TraceTaskHandler(object):
@@ -33,6 +35,7 @@ class TraceTaskHandler(object):
     self._base_path = base_path
     self._is_initialized = False
     self._trace_database = None
+    self._xvfb_process = None
     trace_database_filename = common.clovis_paths.TRACE_DATABASE_PREFIX
     if instance_name:
       trace_database_filename += '_%s.json' % instance_name
@@ -49,6 +52,8 @@ class TraceTaskHandler(object):
       return
     self._is_initialized = True
 
+    self._xvfb_process = xvfb_helper.LaunchXvfb()
+
     # Recover any existing traces in case the worker died.
     self._DownloadTraceDatabase()
     if self._trace_database.ToJsonDict():
@@ -107,7 +112,7 @@ class TraceTaskHandler(object):
 
         # Set up the controller.
         chrome_ctl = controller.LocalChromeController()
-        chrome_ctl.SetHeadless(True)
+        chrome_ctl.SetChromeEnvOverride(xvfb_helper.GetChromeEnvironment())
         if emulate_device:
           chrome_ctl.SetDeviceEmulation(emulate_device)
         if emulate_network:
@@ -123,7 +128,8 @@ class TraceTaskHandler(object):
       except controller.ChromeControllerError as e:
         e.Dump(sys.stderr)
       except Exception as e:
-        sys.stderr.write(str(e))
+        sys.stderr.write('Unknown exception:\n' + str(e))
+        traceback.print_exc(file=sys.stderr)
 
       if trace:
         with open(filename, 'w') as f:
@@ -174,6 +180,14 @@ class TraceTaskHandler(object):
     remote_log_location = remote_trace_location + '.log'
     self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
 
+  def Finalize(self):
+    """Called once before the handler is destroyed."""
+    if self._xvfb_process:
+      try:
+        self._xvfb_process.terminate()
+      except OSError:
+        self._logger.error('Could not terminate Xvfb.')
+
   def Run(self, clovis_task):
     """Runs a 'trace' clovis_task.
 
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index e0ebebf..3d44f1e 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -174,6 +174,7 @@ class Worker(object):
   def _Finalize(self):
     """Called before exiting."""
     self._logger.info('Done')
+    self._clovis_task_handler.Finalize()
     # Upload the worker log.
     if self._worker_log_path:
       self._logger.info('Uploading worker log.')
diff --git a/loading/controller.py b/loading/controller.py
index 511685f..7ea59a0 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -86,8 +86,8 @@ class ChromeControllerError(Exception):
 
     Args:
       log: String containing the log of the running Chrome instance that was
-          running. It will be interleaved with xvfb with headless desktop or
-          interleaved with any other running Android package.
+          running. It will be interleaved with any other running Android
+          package.
     """
     self.error_type, self.error_value, self.error_traceback = sys.exc_info()
     super(ChromeControllerError, self).__init__(repr(self.error_value))
@@ -410,7 +410,7 @@ class LocalChromeController(ChromeControllerBase):
     self._using_temp_profile_dir = self._profile_dir is None
     if self._using_temp_profile_dir:
       self._profile_dir = tempfile.mkdtemp(suffix='.profile')
-    self._headless = False
+    self._chrome_env_override = None
     self._metadata['platform'] = {
         'os': platform.system()[0] + '-' + platform.release(),
         'product_model': 'unknown'
@@ -420,13 +420,13 @@ class LocalChromeController(ChromeControllerBase):
     if self._using_temp_profile_dir:
       shutil.rmtree(self._profile_dir)
 
-  def SetHeadless(self, headless=True):
-    """Set a headless run.
+  def SetChromeEnvOverride(self, env):
+    """Set the environment for Chrome.
 
     Args:
-      headless: true if the chrome instance should be headless.
+      env: (dict) Environment.
     """
-    self._headless = headless
+    self._chrome_env_override = env
 
   @contextlib.contextmanager
   def Open(self):
@@ -445,19 +445,10 @@ class LocalChromeController(ChromeControllerBase):
         tempfile.NamedTemporaryFile(prefix="chrome_controller_", suffix='.log')
     chrome_process = None
     try:
-      chrome_env_override = {}
+      chrome_env_override = self._chrome_env_override or {}
       if self._wpr_attributes:
         chrome_env_override.update(self._wpr_attributes.chrome_env_override)
 
-      if self._headless:
-        assert 'DISPLAY' not in chrome_env_override, \
-            'DISPLAY environment variable is reserved for headless.'
-        chrome_env_override['DISPLAY'] = 'localhost:99'
-        xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
-        logging.info(common_util.GetCommandLineForLogging(xvfb_cmd))
-        xvfb_process = \
-            subprocess.Popen(xvfb_cmd, stdout=tmp_log.file, stderr=tmp_log.file)
-
       chrome_env = os.environ.copy()
       chrome_env.update(chrome_env_override)
 
@@ -500,8 +491,6 @@ class LocalChromeController(ChromeControllerBase):
       del tmp_log
       if chrome_process:
         chrome_process.kill()
-      if self._headless:
-        xvfb_process.kill()
 
   def ResetBrowserState(self):
     """Override for chrome state reseting."""
diff --git a/loading/xvfb_helper.py b/loading/xvfb_helper.py
new file mode 100644
index 0000000..4939e76
--- /dev/null
+++ b/loading/xvfb_helper.py
@@ -0,0 +1,19 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import subprocess
+
+
+def LaunchXvfb():
+  """Launches Xvfb for running Chrome in headless mode, and returns the
+  subprocess."""
+  xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
+  return subprocess.Popen(xvfb_cmd, stdout=open(os.devnull, 'wb'),
+                          stderr=subprocess.STDOUT)
+
+
+def GetChromeEnvironment():
+  """Returns the environment for Chrome to run in headless mode with Xvfb."""
+  return {'DISPLAY': 'localhost:99'}

commit a55f134ebebf0e6cd8d672e11cd1e9bb3f7c2d99
Author: lizeb <lizeb@chromium.org>
Date:   Wed May 18 09:04:36 2016 -0700

    clovis: Properly report PLT.
    
    Review-Url: https://codereview.chromium.org/1988083002
    Cr-Original-Commit-Position: refs/heads/master@{#394433}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: afea550e4b8b8220408ef74c0073f6d849c79f37

diff --git a/loading/report.py b/loading/report.py
index 5fddfc4..b74658f 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -32,11 +32,7 @@ class LoadingReport(object):
         'blink.user_timing', 'navigationStart')
     self._navigation_start_msec = min(
         e.start_msec for e in navigation_start_events)
-    # TODO(lizeb): This is not PLT. Should correlate with
-    # RenderFrameImpl::didStopLoading.
-    self._max_msec = max(
-        r.end_msec or -1 for r in self.trace.request_track.GetEvents())
-
+    self._load_end_msec = self._ComputePlt(trace)
     network_lens = NetworkActivityLens(self.trace)
     if network_lens.total_download_bytes > 0:
       self._contentful_byte_frac = (
@@ -64,7 +60,7 @@ class LoadingReport(object):
                                 - self._navigation_start_msec),
         'significant_paint_ms': (self._significant_paint_msec
                                  - self._navigation_start_msec),
-        'plt_ms': self._max_msec - self._navigation_start_msec,
+        'plt_ms': self._load_end_msec - self._navigation_start_msec,
         'contentful_byte_frac': self._contentful_byte_frac,
         'significant_byte_frac': self._significant_byte_frac,
 
@@ -83,6 +79,19 @@ class LoadingReport(object):
     trace = loading_trace.LoadingTrace.FromJsonFile(filename)
     return LoadingReport(trace)
 
+  @classmethod
+  def _ComputePlt(cls, trace):
+    mark_load_events = trace.tracing_track.GetMatchingEvents(
+        'devtools.timeline', 'MarkLoad')
+    # Some traces contain several load events for the main frame.
+    main_frame_load_events = filter(
+        lambda e: e.args['data']['isMainFrame'], mark_load_events)
+    if main_frame_load_events:
+      return max(e.start_msec for e in main_frame_load_events)
+    # Main frame onLoad() didn't finish. Take the end of the last completed
+    # request.
+    return max(r.end_msec or -1 for r in trace.request_track.GetEvents())
+
 
 if __name__ == '__main__':
   import sys
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 6e5cf59..cdb6a8d 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -18,13 +18,14 @@ class LoadingReportTestCase(unittest.TestCase):
   _SIGNIFICANT_PAINT = 50
   _DURATION = 400
   _REQUEST_OFFSET = 5
+  _LOAD_END_TIME = 1280
   _MAIN_FRAME_ID = 1
 
   def setUp(self):
     self.trace_creator = test_utils.TraceCreator()
     self.requests = [self.trace_creator.RequestAt(self._FIRST_REQUEST_TIME),
                      self.trace_creator.RequestAt(
-                         self._FIRST_REQUEST_TIME + self._REQUEST_OFFSET,
+                         self._NAVIGATION_START_TIME + self._REQUEST_OFFSET,
                          self._DURATION)]
     self.requests[0].timing.receive_headers_end = 0
     self.requests[1].timing.receive_headers_end = 0
@@ -36,6 +37,10 @@ class LoadingReportTestCase(unittest.TestCase):
          'cat': 'blink.user_timing',
          'name': 'navigationStart',
          'args': {'frame': 1}},
+        {'ts': self._LOAD_END_TIME * self.MILLI_TO_MICRO, 'ph': 'I',
+         'cat': 'devtools.timeline',
+         'name': 'MarkLoad',
+         'args': {'data': {'isMainFrame': True}}},
         {'ts': self._CONTENTFUL_PAINT * self.MILLI_TO_MICRO, 'ph': 'I',
          'cat': 'blink.user_timing',
          'name': 'firstContentfulPaint',
@@ -68,11 +73,10 @@ class LoadingReportTestCase(unittest.TestCase):
                      loading_report['significant_paint_ms'])
     self.assertEqual(self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME,
                      loading_report['contentful_paint_ms'])
-    self.assertEqual(self._FIRST_REQUEST_TIME - self._NAVIGATION_START_TIME +
-                     self._REQUEST_OFFSET + self._DURATION,
-                     loading_report['plt_ms'])
-    self.assertAlmostEqual(0.333, loading_report['contentful_byte_frac'], 2)
-    self.assertAlmostEqual(0.178, loading_report['significant_byte_frac'], 2)
+    self.assertAlmostEqual(self._LOAD_END_TIME - self._NAVIGATION_START_TIME,
+                           loading_report['plt_ms'])
+    self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'], 2)
+    self.assertAlmostEqual(0.1844, loading_report['significant_byte_frac'], 2)
     self.assertEqual(None, loading_report['contentful_inversion'])
     self.assertEqual(None, loading_report['significant_inversion'])
 
@@ -92,6 +96,16 @@ class LoadingReportTestCase(unittest.TestCase):
                      loading_report['contentful_inversion'])
     self.assertEqual(None, loading_report['significant_inversion'])
 
+  def testPltNoLoadEvents(self):
+    trace = self._MakeTrace()
+    # Change the MarkLoad events.
+    for e in trace.tracing_track.GetEvents():
+      if e.name == 'MarkLoad':
+        e.tracing_event['name'] = 'dummy'
+    loading_report = report.LoadingReport(trace).GenerateReport()
+    self.assertAlmostEqual(self._REQUEST_OFFSET + self._DURATION,
+                           loading_report['plt_ms'])
+
 
 if __name__ == '__main__':
   unittest.main()

commit 205a7aeff49987990f227c098cb7c3de444376e8
Author: droger <droger@chromium.org>
Date:   Wed May 18 05:47:58 2016 -0700

    tools/android/loading Don't upload NaN values to BigQuery.
    
    Invalid values are rejected by BigQuery, filter them out.
    
    Review-Url: https://codereview.chromium.org/1982343003
    Cr-Original-Commit-Position: refs/heads/master@{#394393}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 87d553bb509309ac6662b4eabfb6c1cacf0c7f1a

diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index ef2335c..cd54ceb 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import json
+import math
 import uuid
 
 from googleapiclient import errors
@@ -131,6 +132,12 @@ class ReportTaskHandler(object):
         self._logger.error('Failed generating report for: ' + path)
         self._failure_database.AddFailure('report_generation_failed', path)
         continue
+      # Filter out bad values.
+      for key, value in report.items():
+        if type(value) is float and (math.isnan(value) or math.isinf(value)):
+          self._logger.error('Invalid %s for URL:%s' % (key, report.get('url')))
+          self._failure_database.AddFailure('invalid_bigquery_value', key)
+          del report[key]
       rows.append(report)
 
     if rows:

commit 8b52ee746eced7b8d62263102ae9a5c83d727120
Author: mattcary <mattcary@chromium.org>
Date:   Wed May 18 04:37:47 2016 -0700

    Clovis: loading event inversion to track incremental parsing and first paints.
    
    In the code I define an "inversion" as when a resource finishes loading while
    its parent is still loading, as in an incrementally parsed index.html. This CL
    computes inversions and adds those for contentful and significant paints to the
    report.
    
    Review-Url: https://codereview.chromium.org/1977863002
    Cr-Original-Commit-Position: refs/heads/master@{#394382}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cb379bd32eeecefc08838be03807ed3d20082cc9

diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index da55ad2..a2c1f17 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -129,6 +129,18 @@ class RequestDependencyGraph(object):
     else:
       return self._deps_graph.Cost(path_list=path_list, costs_out=costs_out)
 
+  def AncestorRequests(self, descendants):
+    """Return requests that are ancestors of a set of requests.
+
+    Args:
+      descendants: ([Request]) List of requests.
+
+    Returns:
+      List of Requests that are ancestors of descendants.
+    """
+    return [n.request for n in self.graph.AncestorNodes(
+        self._nodes_by_id[r.request_id] for r in descendants)]
+
   def _HandleTimingDependencies(self):
     try:
       for n in self._deps_graph.TopologicalSort():
diff --git a/loading/graph.py b/loading/graph.py
index 7c2d275..23c4ef8 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -161,18 +161,23 @@ class DirectedGraph(object):
       should_stop: (callable) Returns True when a node should stop the
                    exploration and be skipped.
     """
-    visited = set()
-    fifo = collections.deque([n for n in roots if not should_stop(n)])
-    while len(fifo) != 0:
-      node = fifo.pop()
-      if should_stop(node):
-        continue
-      visited.add(node)
-      for e in self.OutEdges(node):
-        if e.to_node not in visited and not should_stop(e.to_node):
-          visited.add(e.to_node)
-        fifo.appendleft(e.to_node)
-    return list(visited)
+    return self._ExploreFrom(
+        roots, lambda n: (e.to_node for e in self.OutEdges(n)),
+        should_stop=should_stop)
+
+  def AncestorNodes(self, descendants):
+    """Returns a set of nodes that are ancestors of a set of nodes.
+
+    This is not quite the opposite of ReachableNodes, because (in a tree) it
+    will not include |descendants|.
+
+    Args:
+      descendants: ([Node]) List of nodes to start from.
+
+    """
+    return set(self._ExploreFrom(
+        descendants,
+        lambda n: (e.from_node for e in self.InEdges(n)))) - set(descendants)
 
   def Cost(self, roots=None, path_list=None, costs_out=None):
     """Compute the cost of the graph.
@@ -246,3 +251,26 @@ class DirectedGraph(object):
       edges.append(edge)
     result = DirectedGraph(index_to_node.values(), edges)
     return result
+
+  def _ExploreFrom(self, initial, expand, should_stop=lambda n: False):
+    """Explore from a set of nodes.
+
+    Args:
+      initial: ([Node]) List of nodes to start from.
+      expand: (callable) Given a node, return an iterator of nodes to explore
+        from that node.
+      should_stop: (callable) Returns True when a node should stop the
+                   exploration and be skipped.
+    """
+    visited = set()
+    fifo = collections.deque([n for n in initial if not should_stop(n)])
+    while fifo:
+      node = fifo.pop()
+      if should_stop(node):
+        continue
+      visited.add(node)
+      for n in expand(node):
+        if n not in visited and not should_stop(n):
+          visited.add(n)
+          fifo.appendleft(n)
+    return list(visited)
diff --git a/loading/graph_unittest.py b/loading/graph_unittest.py
index e0e5f5b..28e9e21 100644
--- a/loading/graph_unittest.py
+++ b/loading/graph_unittest.py
@@ -143,6 +143,36 @@ class GraphTestCase(unittest.TestCase):
         set([6]),
         set(n.index for n in g.ReachableNodes([nodes[6]])))
 
+  def testAncestorNodes(self, serialize=False):
+    (nodes, _, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)], serialize)
+    self.assertSetEqual(
+        set([0, 1, 3]),
+        set(n.index for n in g.AncestorNodes([nodes[4]])))
+    self.assertSetEqual(
+        set([0, 1]),
+        set(n.index for n in g.AncestorNodes([nodes[3]])))
+    self.assertSetEqual(
+        set([0]),
+        set(n.index for n in g.AncestorNodes([nodes[1]])))
+    self.assertSetEqual(
+        set(),
+        set(n.index for n in g.AncestorNodes([nodes[0]])))
+    self.assertSetEqual(
+        set([0]),
+        set(n.index for n in g.AncestorNodes([nodes[2]])))
+    self.assertSetEqual(
+        set([5]),
+        set(n.index for n in g.AncestorNodes([nodes[6]])))
+    self.assertSetEqual(
+        set(),
+        set(n.index for n in g.AncestorNodes([nodes[5]])))
+
   def testCost(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
diff --git a/loading/loading_graph_view.py b/loading/loading_graph_view.py
index c312af0..67c993c 100644
--- a/loading/loading_graph_view.py
+++ b/loading/loading_graph_view.py
@@ -5,6 +5,7 @@
 """Views a trace as an annotated request dependency graph."""
 
 import dependency_graph
+import request_dependencies_lens
 
 
 class RequestNode(dependency_graph.RequestNode):
@@ -52,6 +53,11 @@ class LoadingGraphView(object):
     self._graph = None
     self._BuildGraph()
 
+  @classmethod
+  def FromTrace(cls, trace):
+    """Create a graph from a trace with no additional annotation."""
+    return cls(trace, request_dependencies_lens.RequestDependencyLens(trace))
+
   def RemoveAds(self):
     """Updates the graph to remove the Ads.
 
@@ -62,6 +68,34 @@ class LoadingGraphView(object):
         roots, should_stop=lambda n: n.is_ad or n.is_tracking)]
     self._BuildGraph()
 
+  def GetInversionsAtTime(self, msec):
+    """Return the inversions, if any for an event.
+
+    An inversion is when a node is finished before an event, but an ancestor is
+    not finished. For example, an image is loaded before a first paint, but the
+    HTML which requested the image has not finished loading at the time of the
+    paint due to incremental parsing.
+
+    Args:
+      msec: the time of the event, from the same base as requests.
+
+    Returns:
+      The inverted Requests, ordered by start time, or None if there is no
+      inversion.
+    """
+    completed_requests = []
+    for rq in self._requests:
+      if rq.end_msec <= msec:
+        completed_requests.append(rq)
+    inversions = []
+    for rq in self._graph.AncestorRequests(completed_requests):
+      if rq.end_msec > msec:
+        inversions.append(rq)
+    if inversions:
+      inversions.sort(key=lambda rq: rq.start_msec)
+      return inversions
+    return None
+
   @property
   def deps_graph(self):
     return self._graph
diff --git a/loading/loading_graph_view_unittest.py b/loading/loading_graph_view_unittest.py
index b1a93c0..d134d8e 100644
--- a/loading/loading_graph_view_unittest.py
+++ b/loading/loading_graph_view_unittest.py
@@ -79,6 +79,29 @@ class LoadingGraphViewTestCase(unittest.TestCase):
         [TestRequests.FIRST_REDIRECT_REQUEST.request_id])
     self.assertSetEqual(expected_request_ids, request_ids)
 
+  def testEventInversion(self):
+    self._UpdateRequestTiming({
+        '1234.redirect.1': (0, 0),
+        '1234.redirect.2': (0, 0),
+        '1234.1': (10, 100),
+        '1234.12': (20, 50),
+        '1234.42': (40, 70),
+        '1234.56': (40, 150)})
+    graph_view = loading_graph_view.LoadingGraphView(
+        self.trace, self.deps_lens)
+    self.assertEqual(None, graph_view.GetInversionsAtTime(40))
+    self.assertEqual('1234.1', graph_view.GetInversionsAtTime(60)[0].request_id)
+    self.assertEqual('1234.1', graph_view.GetInversionsAtTime(80)[0].request_id)
+    self.assertEqual(None, graph_view.GetInversionsAtTime(110))
+    self.assertEqual(None, graph_view.GetInversionsAtTime(160))
+
+  def _UpdateRequestTiming(self, changes):
+    for rq in self.trace.request_track.GetEvents():
+      if rq.request_id in changes:
+        start_msec, end_msec = changes[rq.request_id]
+        rq.timing.request_time = float(start_msec) / 1000
+        rq.timing.loading_finished = end_msec - start_msec
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/report.py b/loading/report.py
index 50fedd4..5fddfc4 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -7,6 +7,7 @@
 When executed as a script, takes a trace filename and print the report.
 """
 
+from loading_graph_view import LoadingGraphView
 import loading_trace
 from network_activity_lens import NetworkActivityLens
 from user_satisfied_lens import (
@@ -48,6 +49,12 @@ class LoadingReport(object):
       self._contentful_byte_frac = float('Nan')
       self._significant_byte_frac = float('Nan')
 
+    graph = LoadingGraphView.FromTrace(trace)
+    self._contentful_inversion = graph.GetInversionsAtTime(
+        self._contentful_paint_msec)
+    self._significant_inversion = graph.GetInversionsAtTime(
+        self._significant_paint_msec)
+
   def GenerateReport(self):
     """Returns a report as a dict."""
     return {
@@ -59,7 +66,16 @@ class LoadingReport(object):
                                  - self._navigation_start_msec),
         'plt_ms': self._max_msec - self._navigation_start_msec,
         'contentful_byte_frac': self._contentful_byte_frac,
-        'significant_byte_frac': self._significant_byte_frac,}
+        'significant_byte_frac': self._significant_byte_frac,
+
+        # Take the first (earliest) inversions.
+        'contentful_inversion': (self._contentful_inversion[0].url
+                                 if self._contentful_inversion
+                                 else None),
+        'significant_inversion': (self._significant_inversion[0].url
+                                  if self._significant_inversion
+                                  else None)
+    }
 
   @classmethod
   def FromTraceFilename(cls, filename):
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index d5b2304..6e5cf59 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -18,40 +18,44 @@ class LoadingReportTestCase(unittest.TestCase):
   _SIGNIFICANT_PAINT = 50
   _DURATION = 400
   _REQUEST_OFFSET = 5
+  _MAIN_FRAME_ID = 1
 
-  @classmethod
-  def _MakeTrace(cls):
-    trace_creator = test_utils.TraceCreator()
-    requests = [trace_creator.RequestAt(cls._FIRST_REQUEST_TIME),
-                trace_creator.RequestAt(
-                    cls._NAVIGATION_START_TIME + cls._REQUEST_OFFSET,
-                    cls._DURATION)]
-    requests[0].timing.receive_headers_end = 0
-    requests[1].timing.receive_headers_end = 0
-    requests[0].encoded_data_length = 128
-    requests[1].encoded_data_length = 1024
-    trace = trace_creator.CreateTrace(
-        requests,
-        [{'ts': cls._NAVIGATION_START_TIME * cls.MILLI_TO_MICRO, 'ph': 'R',
-          'cat': 'blink.user_timing',
-          'name': 'navigationStart',
-          'args': {'frame': 1}},
-         {'ts': cls._CONTENTFUL_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
-          'cat': 'blink.user_timing',
-          'name': 'firstContentfulPaint',
-          'args': {'frame': 1}},
-         {'ts': cls._TEXT_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
-          'cat': 'blink.user_timing',
-          'name': 'firstPaint',
-          'args': {'frame': 1}},
-         {'ts': 90 * cls.MILLI_TO_MICRO, 'ph': 'I',
-          'cat': 'blink',
-          'name': 'FrameView::synchronizedPaint'},
-         {'ts': cls._SIGNIFICANT_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
-          'cat': 'foobar', 'name': 'biz',
-          'args': {'counters': {
-              'LayoutObjectsThatHadNeverHadLayout': 10
-          }}}], 1)
+  def setUp(self):
+    self.trace_creator = test_utils.TraceCreator()
+    self.requests = [self.trace_creator.RequestAt(self._FIRST_REQUEST_TIME),
+                     self.trace_creator.RequestAt(
+                         self._FIRST_REQUEST_TIME + self._REQUEST_OFFSET,
+                         self._DURATION)]
+    self.requests[0].timing.receive_headers_end = 0
+    self.requests[1].timing.receive_headers_end = 0
+    self.requests[0].encoded_data_length = 128
+    self.requests[1].encoded_data_length = 1024
+
+    self.trace_events = [
+        {'ts': self._NAVIGATION_START_TIME * self.MILLI_TO_MICRO, 'ph': 'R',
+         'cat': 'blink.user_timing',
+         'name': 'navigationStart',
+         'args': {'frame': 1}},
+        {'ts': self._CONTENTFUL_PAINT * self.MILLI_TO_MICRO, 'ph': 'I',
+         'cat': 'blink.user_timing',
+         'name': 'firstContentfulPaint',
+         'args': {'frame': self._MAIN_FRAME_ID}},
+        {'ts': self._TEXT_PAINT * self.MILLI_TO_MICRO, 'ph': 'I',
+         'cat': 'blink.user_timing',
+         'name': 'firstPaint',
+         'args': {'frame': self._MAIN_FRAME_ID}},
+        {'ts': 90 * self.MILLI_TO_MICRO, 'ph': 'I',
+         'cat': 'blink',
+         'name': 'FrameView::synchronizedPaint'},
+        {'ts': self._SIGNIFICANT_PAINT * self.MILLI_TO_MICRO, 'ph': 'I',
+         'cat': 'foobar', 'name': 'biz',
+         'args': {'counters': {
+             'LayoutObjectsThatHadNeverHadLayout': 10
+         }}}]
+
+  def _MakeTrace(self):
+    trace = self.trace_creator.CreateTrace(
+        self.requests, self.trace_events, self._MAIN_FRAME_ID)
     return trace
 
   def testGenerateReport(self):
@@ -64,10 +68,29 @@ class LoadingReportTestCase(unittest.TestCase):
                      loading_report['significant_paint_ms'])
     self.assertEqual(self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME,
                      loading_report['contentful_paint_ms'])
-    self.assertEqual(self._REQUEST_OFFSET + self._DURATION,
+    self.assertEqual(self._FIRST_REQUEST_TIME - self._NAVIGATION_START_TIME +
+                     self._REQUEST_OFFSET + self._DURATION,
                      loading_report['plt_ms'])
-    self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'])
-    self.assertAlmostEqual(0.184, loading_report['significant_byte_frac'], 2)
+    self.assertAlmostEqual(0.333, loading_report['contentful_byte_frac'], 2)
+    self.assertAlmostEqual(0.178, loading_report['significant_byte_frac'], 2)
+    self.assertEqual(None, loading_report['contentful_inversion'])
+    self.assertEqual(None, loading_report['significant_inversion'])
+
+  def testInversion(self):
+    self.requests[0].timing.loading_finished = 4 * (
+        self._REQUEST_OFFSET + self._DURATION)
+    self.requests[1].initiator['type'] = 'parser'
+    self.requests[1].initiator['url'] = self.requests[0].url
+    for e in self.trace_events:
+      if e['name'] == 'firstContentfulPaint':
+        e['ts'] = self.MILLI_TO_MICRO * (
+            self._FIRST_REQUEST_TIME +  self._REQUEST_OFFSET +
+            self._DURATION + 1)
+        break
+    loading_report = report.LoadingReport(self._MakeTrace()).GenerateReport()
+    self.assertEqual(self.requests[0].url,
+                     loading_report['contentful_inversion'])
+    self.assertEqual(None, loading_report['significant_inversion'])
 
 
 if __name__ == '__main__':
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 67ab3f5..bf2d858 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -72,6 +72,15 @@ class TestRequests(object):
 
   @classmethod
   def CreateLoadingTrace(cls, trace_events=None):
+    # This creates a set of requests with the following dependency structure.
+    #
+    # 1234.redirect.1 -> 1234.redirect.2
+    # 1234.redirect.2 -> 1234.1
+    # 1234.1 -> 1234.12
+    # 1234.1 -> 1234.42
+    # 1234.1 -> 1234.56
+    # 1234.12 -> 1234.13
+
     trace = test_utils.LoadingTraceFromEvents(
         [cls.FIRST_REDIRECT_REQUEST, cls.SECOND_REDIRECT_REQUEST,
          cls.REDIRECTED_REQUEST, cls.REQUEST, cls.JS_REQUEST, cls.JS_REQUEST_2,

commit f84bc798a6dbce9e5d1fa1130a428a51a22c7148
Author: lizeb <lizeb@chromium.org>
Date:   Tue May 17 08:49:34 2016 -0700

    clovis: Make the base offset of timings be navigationStart in reports.
    
    Also fixes the bytes fractions computations.
    
    Review-Url: https://codereview.chromium.org/1988773002
    Cr-Original-Commit-Position: refs/heads/master@{#394137}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 843f37977ef64cd23b14c5bcba4af58b0ba2a2da

diff --git a/loading/report.py b/loading/report.py
index eb19779..50fedd4 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -27,8 +27,10 @@ class LoadingReport(object):
         FirstContentfulPaintLens(self.trace).SatisfiedMs())
     self._significant_paint_msec = (
         FirstSignificantPaintLens(self.trace).SatisfiedMs())
-    self._base_msec = min(
-        r.start_msec for r in self.trace.request_track.GetEvents())
+    navigation_start_events = trace.tracing_track.GetMatchingEvents(
+        'blink.user_timing', 'navigationStart')
+    self._navigation_start_msec = min(
+        e.start_msec for e in navigation_start_events)
     # TODO(lizeb): This is not PLT. Should correlate with
     # RenderFrameImpl::didStopLoading.
     self._max_msec = max(
@@ -36,12 +38,12 @@ class LoadingReport(object):
 
     network_lens = NetworkActivityLens(self.trace)
     if network_lens.total_download_bytes > 0:
-      self._contentful_byte_frac = network_lens.DownloadedBytesAt(
-          self._contentful_paint_msec -
-          self._base_msec) / float(network_lens.total_download_bytes)
-      self._significant_byte_frac = network_lens.DownloadedBytesAt(
-          self._significant_paint_msec -
-          self._base_msec) / float(network_lens.total_download_bytes)
+      self._contentful_byte_frac = (
+          network_lens.DownloadedBytesAt(self._contentful_paint_msec)
+          / float(network_lens.total_download_bytes))
+      self._significant_byte_frac = (
+          network_lens.DownloadedBytesAt(self._significant_paint_msec)
+          / float(network_lens.total_download_bytes))
     else:
       self._contentful_byte_frac = float('Nan')
       self._significant_byte_frac = float('Nan')
@@ -50,10 +52,12 @@ class LoadingReport(object):
     """Returns a report as a dict."""
     return {
         'url': self.trace.url,
-        'first_text_ms': self._text_msec - self._base_msec,
-        'contentful_paint_ms': self._contentful_paint_msec - self._base_msec,
-        'significant_paint_ms': self._significant_paint_msec - self._base_msec,
-        'plt_ms': self._max_msec - self._base_msec,
+        'first_text_ms': self._text_msec - self._navigation_start_msec,
+        'contentful_paint_ms': (self._contentful_paint_msec
+                                - self._navigation_start_msec),
+        'significant_paint_ms': (self._significant_paint_msec
+                                 - self._navigation_start_msec),
+        'plt_ms': self._max_msec - self._navigation_start_msec,
         'contentful_byte_frac': self._contentful_byte_frac,
         'significant_byte_frac': self._significant_byte_frac,}
 
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 58c08c8..d5b2304 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -11,6 +11,7 @@ import user_satisfied_lens_unittest
 
 class LoadingReportTestCase(unittest.TestCase):
   MILLI_TO_MICRO = 1000
+  _NAVIGATION_START_TIME = 12
   _FIRST_REQUEST_TIME = 15
   _CONTENTFUL_PAINT = 120
   _TEXT_PAINT = 30
@@ -23,7 +24,7 @@ class LoadingReportTestCase(unittest.TestCase):
     trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(cls._FIRST_REQUEST_TIME),
                 trace_creator.RequestAt(
-                    cls._FIRST_REQUEST_TIME + cls._REQUEST_OFFSET,
+                    cls._NAVIGATION_START_TIME + cls._REQUEST_OFFSET,
                     cls._DURATION)]
     requests[0].timing.receive_headers_end = 0
     requests[1].timing.receive_headers_end = 0
@@ -31,7 +32,11 @@ class LoadingReportTestCase(unittest.TestCase):
     requests[1].encoded_data_length = 1024
     trace = trace_creator.CreateTrace(
         requests,
-        [{'ts': cls._CONTENTFUL_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
+        [{'ts': cls._NAVIGATION_START_TIME * cls.MILLI_TO_MICRO, 'ph': 'R',
+          'cat': 'blink.user_timing',
+          'name': 'navigationStart',
+          'args': {'frame': 1}},
+         {'ts': cls._CONTENTFUL_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
           'cat': 'blink.user_timing',
           'name': 'firstContentfulPaint',
           'args': {'frame': 1}},
@@ -53,16 +58,16 @@ class LoadingReportTestCase(unittest.TestCase):
     trace = self._MakeTrace()
     loading_report = report.LoadingReport(trace).GenerateReport()
     self.assertEqual(trace.url, loading_report['url'])
-    self.assertEqual(self._TEXT_PAINT - self._FIRST_REQUEST_TIME,
+    self.assertEqual(self._TEXT_PAINT - self._NAVIGATION_START_TIME,
                      loading_report['first_text_ms'])
-    self.assertEqual(self._SIGNIFICANT_PAINT - self._FIRST_REQUEST_TIME,
+    self.assertEqual(self._SIGNIFICANT_PAINT - self._NAVIGATION_START_TIME,
                      loading_report['significant_paint_ms'])
-    self.assertEqual(self._CONTENTFUL_PAINT - self._FIRST_REQUEST_TIME,
+    self.assertEqual(self._CONTENTFUL_PAINT - self._NAVIGATION_START_TIME,
                      loading_report['contentful_paint_ms'])
     self.assertEqual(self._REQUEST_OFFSET + self._DURATION,
                      loading_report['plt_ms'])
-    self.assertAlmostEqual(0.3, loading_report['contentful_byte_frac'])
-    self.assertAlmostEqual(0.14, loading_report['significant_byte_frac'], 2)
+    self.assertAlmostEqual(0.34, loading_report['contentful_byte_frac'])
+    self.assertAlmostEqual(0.184, loading_report['significant_byte_frac'], 2)
 
 
 if __name__ == '__main__':
diff --git a/loading/tracing.py b/loading/tracing.py
index 55009a6..b04787c 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -102,8 +102,8 @@ class TracingTrack(devtools_monitor.Track):
   def GetMainFrameID(self):
     """Returns the main frame ID."""
     if not self._main_frame_id:
-      navigation_start_events = [e for e in self.GetEvents()
-          if e.Matches('blink.user_timing', 'navigationStart')]
+      navigation_start_events = self.GetMatchingEvents(
+          'blink.user_timing', 'navigationStart')
       first_event = min(navigation_start_events, key=lambda e: e.start_msec)
       self._main_frame_id = first_event.args['frame']
 

commit a51902c5c83db1f09583e6ac19b67a5444969313
Author: droger <droger@chromium.org>
Date:   Tue May 17 02:47:09 2016 -0700

    tools/android/loading Share path constants between frontend and backend
    
    The frontend and the backend need to share knowledge about
    file and URL paths (such as the path where the loading
    trace databases are generated, and the BigQuery table URL).
    
    This CL removes the duplication and moves the code to a
    common location.
    
    Review-Url: https://codereview.chromium.org/1979623002
    Cr-Original-Commit-Position: refs/heads/master@{#394092}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7305d984f9092c165c4b3ec9debb13336d51572c

diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index 1836bbf..ef2335c 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -7,6 +7,7 @@ import uuid
 
 from googleapiclient import errors
 
+import common.clovis_paths
 from common.loading_trace_database import LoadingTraceDatabase
 import common.google_error_helper as google_error_helper
 from failure_database import FailureDatabase
@@ -134,7 +135,5 @@ class ReportTaskHandler(object):
 
     if rows:
       tag = clovis_task.BackendParams()['tag']
-      # BigQuery table names can contain only alpha numeric characters and
-      # underscores.
-      table_id = ''.join(c for c in tag if c.isalnum() or c == '_')
+      table_id = common.clovis_paths.GetBigQueryTableID(tag)
       self._StreamRowsToBigQuery(rows, table_id)
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index a549c29..d718ab7 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -7,6 +7,7 @@ import os
 import re
 import sys
 
+import common.clovis_paths
 from common.clovis_task import ClovisTask
 from common.loading_trace_database import LoadingTraceDatabase
 import controller
@@ -32,10 +33,11 @@ class TraceTaskHandler(object):
     self._base_path = base_path
     self._is_initialized = False
     self._trace_database = None
+    trace_database_filename = common.clovis_paths.TRACE_DATABASE_PREFIX
     if instance_name:
-      trace_database_filename = 'trace_database_%s.json' % instance_name
+      trace_database_filename += '_%s.json' % instance_name
     else:
-      trace_database_filename = 'trace_database.json'
+      trace_database_filename += '.json'
     self._trace_database_path = os.path.join(base_path, trace_database_filename)
 
     # Initialize the global options that will be used during trace generation.
diff --git a/loading/cloud/common/clovis_paths.py b/loading/cloud/common/clovis_paths.py
new file mode 100644
index 0000000..03c1b33
--- /dev/null
+++ b/loading/cloud/common/clovis_paths.py
@@ -0,0 +1,36 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+
+# BigQuery path constants.
+
+# Name of the dataset.
+BIGQUERY_DATASET = 'clovis_dataset'
+# Name of the table used as a template for new tables.
+BIGQUERY_TABLE_TEMPLATE = 'report'
+
+
+# Trace path constants.
+
+# Prefix for the loading trace database files.
+TRACE_DATABASE_PREFIX = 'trace_database'
+# Name of the directory where traces are located.
+TRACE_DIR = 'trace'
+
+
+def GetBigQueryTableID(tag):
+  """Returns the ID of the BigQuery table associated with tag. This ID is
+  appended at the end of the table name.
+  """
+  # BigQuery table names can contain only alpha numeric characters and
+  # underscores.
+  return ''.join(c for c in tag if c.isalnum() or c == '_')
+
+
+def GetBigQueryTableURL(project_name, tag):
+  """Returns the full URL for the BigQuery table associated with tag."""
+  table_id = GetBigQueryTableID(tag)
+  table_name = BIGQUERY_DATASET + '.' + BIGQUERY_TABLE_TEMPLATE + '_' + table_id
+  return 'https://bigquery.cloud.google.com/table/%s:%s' % (project_name,
+                                                            table_name)
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 2fb4049..2a23aa1 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -13,6 +13,7 @@ from google.appengine.api import (app_identity, taskqueue)
 from google.appengine.ext import deferred
 from oauth2client.client import GoogleCredentials
 
+import common.clovis_paths
 from common.clovis_task import ClovisTask
 import common.google_instance_helper
 from common.loading_trace_database import LoadingTraceDatabase
@@ -201,15 +202,16 @@ def GetTracePaths(bucket):
 
   This function assumes a specific structure for the files in the bucket. These
   assumptions must match the behavior of the backend:
-  - The trace databases are located under a 'trace' directory under |bucket|.
-  - The trace databases files are the only objects with the 'trace_database'
-    prefix in their name.
+  - The trace databases are located under the TRACE_DIR directory in the bucket.
+  - The trace databases files are the only objects with the
+    TRACE_DATABASE_PREFIX prefix in their name.
 
   Returns:
     list: The list of paths to traces, as strings.
   """
   traces = []
-  prefix = os.path.join('/', bucket, 'trace', 'trace_database')
+  prefix = os.path.join('/', bucket, common.clovis_paths.TRACE_DIR,
+                        common.clovis_paths.TRACE_DATABASE_PREFIX)
   file_stats = cloudstorage.listbucket(prefix)
 
   for file_stat in file_stats:
@@ -260,11 +262,7 @@ def StartFromJsonString(http_body_str):
     if bucket:
       task_url = 'https://console.cloud.google.com/storage/' + bucket
   elif task.Action() == 'report':
-    # This must match the table path defined in ReportTaskHandler.
-    table_id = ''.join(c for c in task_tag if c.isalnum() or c == '_')
-    table_name = 'clovis_dataset.report_' + table_id
-    task_url = 'https://bigquery.cloud.google.com/table/%s:%s' %(project_name,
-                                                                 table_name)
+    task_url = common.clovis_paths.GetBigQueryTableURL(project_name, task_tag)
   else:
     error_string = 'Unsupported action: %s.' % task.Action()
     clovis_logger.error(error_string)

commit b72136b5f01c387ff42db97180b7f1eb5be1333d
Author: droger <droger@chromium.org>
Date:   Tue May 17 01:48:15 2016 -0700

    tools/android/loading Support "report" tasks in the Clovis frontend
    
    Review-Url: https://codereview.chromium.org/1962283002
    Cr-Original-Commit-Position: refs/heads/master@{#394085}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 48ddb18aa346a2206e339991de689c1465f600e2

diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index 7e06898..1384d10 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -7,7 +7,8 @@
 Visit the application URL in your browser, and upload a JSON dictionary with the
 following keys:
 
--   `action` (string): the action to perform. Only `trace` is supported.
+-   `action` (string): the action to perform. Only `trace` and `report` are
+    supported.
 -   `action_params` (dictionary): the parameters associated to the action.
     See below for more details.
 -   `backend_params` (dictionary): the parameters configuring the backend for
@@ -30,12 +31,22 @@ following keys:
 
 ### Parameters for the `trace` action
 
+The trace action takes a list of URLs as input and generates a list of traces by
+running Chrome.
+
 -   `urls` (list of strings): the list of URLs to process.
 -   `repeat_count` (integer, optional): the number of traces to be generated
     for each URL. Defaults to 1.
 -   `emulate_device` (string, optional): the device to emulate (e.g. `Nexus 4`).
 -   `emulate_network` (string, optional): the network to emulate.
 
+### Parameters for the `report` action
+
+Finds all the traces in the bucket (specified in the backend parameters) and
+generates a report in BigQuery.
+
+This action has no parameters.
+
 ## Development
 
 ### Design overview
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 33f14d9..2fb4049 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -7,6 +7,7 @@ import os
 import sys
 import time
 
+import cloudstorage
 import flask
 from google.appengine.api import (app_identity, taskqueue)
 from google.appengine.ext import deferred
@@ -14,6 +15,7 @@ from oauth2client.client import GoogleCredentials
 
 from common.clovis_task import ClovisTask
 import common.google_instance_helper
+from common.loading_trace_database import LoadingTraceDatabase
 import email_helper
 from memory_logs import MemoryLogs
 
@@ -149,6 +151,88 @@ def DeleteInstanceTemplate(tag, try_count=0):
   clovis_logger.info('Cleanup complete for tag: ' + tag)
 
 
+def SplitClovisTask(task):
+  """Splits a ClovisTask in smaller ClovisTasks.
+
+  Args:
+    task: (ClovisTask) The task to split.
+
+  Returns:
+    list: The list of ClovisTasks.
+  """
+  # For report task, need to find the traces first.
+  if task.Action() == 'report':
+    bucket = task.BackendParams().get('storage_bucket')
+    if not bucket:
+      clovis_logger.error('Missing storage bucket for report task.')
+      return None
+    traces = GetTracePaths(bucket)
+    if not traces:
+      clovis_logger.error('No traces found in bucket: ' + bucket)
+      return None
+    task.ActionParams()['traces'] = traces
+
+  # Compute the split key.
+  split_params_for_action = {'trace': ('urls', 1), 'report': ('traces', 5)}
+  (split_key, slice_size) = split_params_for_action.get(task.Action(),
+                                                        (None, 0))
+  if not split_key:
+    clovis_logger.error('Cannot split task with action: ' + task.Action())
+    return None
+
+  # Split the task using the split key.
+  clovis_logger.debug('Splitting task by: ' + split_key)
+  action_params = task.ActionParams()
+  values = action_params[split_key]
+  sub_tasks = []
+  for i in range(0, len(values), slice_size):
+    sub_task_params = action_params.copy()
+    sub_task_params[split_key] = [v for v in values[i:i+slice_size]]
+    sub_tasks.append(ClovisTask(task.Action(), sub_task_params,
+                                task.BackendParams()))
+  return sub_tasks
+
+
+def GetTracePaths(bucket):
+  """Returns a list of trace files in a bucket.
+
+  Finds and loads the trace databases, and returns their content as a list of
+  paths.
+
+  This function assumes a specific structure for the files in the bucket. These
+  assumptions must match the behavior of the backend:
+  - The trace databases are located under a 'trace' directory under |bucket|.
+  - The trace databases files are the only objects with the 'trace_database'
+    prefix in their name.
+
+  Returns:
+    list: The list of paths to traces, as strings.
+  """
+  traces = []
+  prefix = os.path.join('/', bucket, 'trace', 'trace_database')
+  file_stats = cloudstorage.listbucket(prefix)
+
+  for file_stat in file_stats:
+    database_file = file_stat.filename
+    clovis_logger.info('Loading trace database: ' + database_file)
+
+    with cloudstorage.open(database_file) as remote_file:
+      json_string = remote_file.read()
+    if not json_string:
+      clovis_logger.warning('Failed to download: ' + database_file)
+      continue
+
+    database = LoadingTraceDatabase.FromJsonString(json_string)
+    if not database:
+      clovis_logger.warning('Failed to parse: ' + database_file)
+      continue
+
+    for path in database.ToJsonDict():
+      traces.append(path)
+
+  return traces
+
+
 def StartFromJsonString(http_body_str):
   """Main function handling a JSON task posted by the user."""
   # Set up logging.
@@ -162,23 +246,35 @@ def StartFromJsonString(http_body_str):
     return Render('Invalid JSON task:\n' + http_body_str, memory_logs)
 
   task_tag = task.BackendParams()['tag']
+  clovis_logger.info('Start processing %s task with tag %s.' % (task.Action(),
+                                                                task_tag))
 
   # Create the instance template if required.
   if not CreateInstanceTemplate(task):
     return Render('Template creation failed.', memory_logs)
 
-  # Split the task in smaller tasks.
-  sub_tasks = []
+  # Build the URL where the result will live.
   task_url = None
   if task.Action() == 'trace':
     bucket = task.BackendParams().get('storage_bucket')
     if bucket:
       task_url = 'https://console.cloud.google.com/storage/' + bucket
-    sub_tasks = SplitTraceTask(task)
+  elif task.Action() == 'report':
+    # This must match the table path defined in ReportTaskHandler.
+    table_id = ''.join(c for c in task_tag if c.isalnum() or c == '_')
+    table_name = 'clovis_dataset.report_' + table_id
+    task_url = 'https://bigquery.cloud.google.com/table/%s:%s' %(project_name,
+                                                                 table_name)
   else:
     error_string = 'Unsupported action: %s.' % task.Action()
     clovis_logger.error(error_string)
     return Render(error_string, memory_logs)
+  clovis_logger.info('Task result URL: ' + task_url)
+
+  # Split the task in smaller tasks.
+  sub_tasks = SplitClovisTask(task)
+  if not sub_tasks:
+    return Render('Task split failed.', memory_logs)
 
   if not EnqueueTasks(sub_tasks, task_tag):
     return Render('Task creation failed.', memory_logs)
@@ -201,25 +297,6 @@ def StartFromJsonString(http_body_str):
       memory_logs)
 
 
-def SplitTraceTask(task):
-  """Splits a tracing task with potentially many URLs into several tracing tasks
-  with few URLs.
-  """
-  clovis_logger.debug('Splitting trace task.')
-  action_params = task.ActionParams()
-  urls = action_params['urls']
-
-  # Split the task in smaller tasks with fewer URLs each.
-  urls_per_task = 1
-  sub_tasks = []
-  for i in range(0, len(urls), urls_per_task):
-    sub_task_params = action_params.copy()
-    sub_task_params['urls'] = [url for url in urls[i:i+urls_per_task]]
-    sub_tasks.append(ClovisTask(task.Action(), sub_task_params,
-                                task.BackendParams()))
-  return sub_tasks
-
-
 def EnqueueTasks(tasks, task_tag):
   """Enqueues a list of tasks in the Google Cloud task queue, for consumption by
   Google Compute Engine.
diff --git a/loading/cloud/frontend/requirements.txt b/loading/cloud/frontend/requirements.txt
index 483195b..4d9f882 100644
--- a/loading/cloud/frontend/requirements.txt
+++ b/loading/cloud/frontend/requirements.txt
@@ -1,2 +1,3 @@
 Flask==0.10
 google-api-python-client
+GoogleAppEngineCloudStorageClient

commit eab227add8d7aad2d72c67862fdcabdf9636f38b
Author: lizeb <lizeb@chromium.org>
Date:   Fri May 13 09:57:22 2016 -0700

    clovis: Silence noisy NetworkActivityLens.
    
    Review-Url: https://codereview.chromium.org/1972413002
    Cr-Original-Commit-Position: refs/heads/master@{#393556}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f4b254570593e5e1adeeb95facd1b83285710707

diff --git a/loading/network_activity_lens.py b/loading/network_activity_lens.py
index b30cac8..7e087e4 100644
--- a/loading/network_activity_lens.py
+++ b/loading/network_activity_lens.py
@@ -237,6 +237,4 @@ class NetworkEvent(object):
     """Returns the download rate of this event in Bytes / s."""
     downloaded_bytes = self.DownloadedBytes()
     value = 1000 * downloaded_bytes / float(self.end_msec - self.start_msec)
-    if value > 1e6:
-      print self._kind, downloaded_bytes, self.end_msec - self.start_msec
     return value

commit f465864c69b754f9cfd1b738bab5c0eeb09c85d2
Author: droger <droger@chromium.org>
Date:   Fri May 13 08:53:29 2016 -0700

    tools/android/loading Move LoadingTraceDatabase under cloud/
    
    The file is moved so that it can be used from the AppEngine frontend.
    
    Review-Url: https://codereview.chromium.org/1969373002
    Cr-Original-Commit-Position: refs/heads/master@{#393532}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9312f6d6c8c18b177ca14fb82246ca8e6175b492

diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index 47365b5..1836bbf 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -7,10 +7,10 @@ import uuid
 
 from googleapiclient import errors
 
+from common.loading_trace_database import LoadingTraceDatabase
 import common.google_error_helper as google_error_helper
 from failure_database import FailureDatabase
 from loading_trace import LoadingTrace
-from loading_trace_database import LoadingTraceDatabase
 from report import LoadingReport
 
 
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index 33591d3..a549c29 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -8,10 +8,10 @@ import re
 import sys
 
 from common.clovis_task import ClovisTask
+from common.loading_trace_database import LoadingTraceDatabase
 import controller
 from failure_database import FailureDatabase
 import loading_trace
-from loading_trace_database import LoadingTraceDatabase
 import options
 
 
diff --git a/loading/loading_trace_database.py b/loading/cloud/common/loading_trace_database.py
similarity index 100%
rename from loading/loading_trace_database.py
rename to loading/cloud/common/loading_trace_database.py
diff --git a/loading/loading_trace_database_unittest.py b/loading/cloud/common/loading_trace_database_unittest.py
similarity index 95%
rename from loading/loading_trace_database_unittest.py
rename to loading/cloud/common/loading_trace_database_unittest.py
index e64fb8c..72ffcba 100644
--- a/loading/loading_trace_database_unittest.py
+++ b/loading/cloud/common/loading_trace_database_unittest.py
@@ -4,7 +4,7 @@
 
 import unittest
 
-from loading_trace_database import LoadingTraceDatabase
+from cloud.common.loading_trace_database import LoadingTraceDatabase
 
 
 class LoadingTraceDatabaseUnittest(unittest.TestCase):

commit d5d55763bff71980ad52337f86b1480d5358fbf4
Author: droger <droger@chromium.org>
Date:   Fri May 13 07:09:34 2016 -0700

    tools/android/loading Remove dead code in LoadingTraceDatabase
    
    Review-Url: https://codereview.chromium.org/1969393002
    Cr-Original-Commit-Position: refs/heads/master@{#393518}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d0cedd7896dc80c8619c4e2828cef41ee0244080

diff --git a/loading/google_storage_util.py b/loading/google_storage_util.py
deleted file mode 100644
index 82a59a5..0000000
--- a/loading/google_storage_util.py
+++ /dev/null
@@ -1,19 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Contains utility functions for interacting with Google Storage."""
-
-import subprocess
-
-def ReadFromGoogleStorage(path):
-  """Given a Google Storage path, returns the contents of the file at that path
-     as a string. Will fail if the user does not have authorization to access
-     the path or if the path does not exist. To gain authorization, follow the
-     instructions for installing gsutil and setting up credentials to access
-     protected data that are on this page:
-     https://cloud.google.com/storage/docs/gsutil_install"""
-  # TODO(blundell): Change this to use the gcloud Python module once
-  # https://github.com/GoogleCloudPlatform/gcloud-python/issues/14360 is fixed.
-  contents = subprocess.check_output(["gsutil", "cat", path])
-  return contents
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index 91705aa..f6e946a 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -5,7 +5,6 @@
 """Represents a database of on-disk traces."""
 
 import json
-from google_storage_util import ReadFromGoogleStorage
 
 
 class LoadingTraceDatabase(object):
@@ -56,10 +55,3 @@ class LoadingTraceDatabase(object):
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:
       return cls.FromJsonDict(json.load(input_file))
-
-  @classmethod
-  def FromJsonFileInGoogleStorage(cls, json_google_storage_path):
-    """Returns an instance from a json file in Google Storage whose contents
-       were generated by ToJsonFile()."""
-    json_string = ReadFromGoogleStorage(json_google_storage_path)
-    return cls.FromJsonDict(json.loads(json_string))

commit c20f81f0f10fdfc3473b8cbb8468e1031c619fb9
Author: droger <droger@chromium.org>
Date:   Fri May 13 06:24:53 2016 -0700

    tools/android/loading Lazy initialization of TraceTaskHandler
    
    Now that the backend has another task handler, it is possible that
    TraceTaskHandler is not used.
    This CL makes the initialization of TraceTaskHandler lazy, to avoid
    unnecessary calls to cloud storage.
    
    Review-Url: https://codereview.chromium.org/1966953003
    Cr-Original-Commit-Position: refs/heads/master@{#393513}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: df043a065371182d04f2d600ba258264c751c2f3

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index a6e83a7..33591d3 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -30,23 +30,31 @@ class TraceTaskHandler(object):
     self._logger = logger
     self._google_storage_accessor = google_storage_accessor
     self._base_path = base_path
+    self._is_initialized = False
+    self._trace_database = None
     if instance_name:
       trace_database_filename = 'trace_database_%s.json' % instance_name
     else:
       trace_database_filename = 'trace_database.json'
     self._trace_database_path = os.path.join(base_path, trace_database_filename)
 
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs(['--local_build_dir', binaries_path])
+
+  def _Initialize(self):
+    """Initializes the trace task handler. Can be called multiple times."""
+    if self._is_initialized:
+      return
+    self._is_initialized = True
+
     # Recover any existing traces in case the worker died.
     self._DownloadTraceDatabase()
     if self._trace_database.ToJsonDict():
-      # Script is restarting after a crash, or there are already files from a
-      # previous run in the directory.
+      # There are already files from a previous run in the directory, likely
+      # because the script is restarting after a crash.
       self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
                                         'trace_database')
 
-    # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs(['--local_build_dir', binaries_path])
-
   def _DownloadTraceDatabase(self):
     """Downloads the trace database from CloudStorage."""
     self._logger.info('Downloading trace database')
@@ -58,6 +66,7 @@ class TraceTaskHandler(object):
   def _UploadTraceDatabase(self):
     """Uploads the trace database to CloudStorage."""
     self._logger.info('Uploading trace database')
+    assert self._is_initialized
     self._google_storage_accessor.UploadString(
         self._trace_database.ToJsonString(),
         self._trace_database_path)
@@ -137,6 +146,7 @@ class TraceTaskHandler(object):
                              the log (with a .log extension added) are uploaded.
       trace_metadata (dict): Metadata associated with the trace generation.
     """
+    assert self._is_initialized
     if trace_metadata['succeeded']:
       traces_dir = os.path.join(self._base_path, 'traces')
       remote_trace_location = os.path.join(traces_dir, remote_filename)
@@ -174,6 +184,8 @@ class TraceTaskHandler(object):
                                         'trace_task_handler_run')
       return
 
+    self._Initialize()
+
     # Extract the task parameters.
     params = clovis_task.ActionParams()
     urls = params['urls']

commit 5abd28ff3adda80e4324dbc94e98a2d46eb22641
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 13 06:23:56 2016 -0700

    sandwich: Reset the chrome state between repeated runs
    
    Before Sandwich was not reseting the chrome state between repeated
    runs that could lead to JavaScript entropy because of the previous
    cookies.
    
    This CL fixes this issue by replacing the options --clear_device_data
    with the programmatic ChromeControllerBase.ResetBrowserState().
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1976113002
    Cr-Original-Commit-Position: refs/heads/master@{#393512}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a5d46d30ea9597918275a45f03bfe5f68068f95f

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index ead016e..24d3f75 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -121,7 +121,7 @@ def PushBrowserCache(device, local_cache_path):
 
   # Clear previous cache.
   _AdbShell(device.adb, ['rm', '-rf', remote_cache_directory])
-  _AdbShell(device.adb, ['mkdir', remote_cache_directory])
+  _AdbShell(device.adb, ['mkdir', '-p', remote_cache_directory])
 
   # Push cache content.
   device.adb.Push(local_cache_path, remote_cache_directory)
diff --git a/loading/controller.py b/loading/controller.py
index 5ccf688..511685f 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -199,6 +199,10 @@ class ChromeControllerBase(object):
     assert network_name in emulation.NETWORK_CONDITIONS or network_name is None
     self._network_name = network_name
 
+  def ResetBrowserState(self):
+    """Resets the chrome's browser state."""
+    raise NotImplementedError
+
   def PushBrowserCache(self, cache_path):
     """Pushes the HTTP chrome cache to the profile directory.
 
@@ -287,8 +291,10 @@ class RemoteChromeController(ChromeControllerBase):
   def __init__(self, device):
     """Initialize the controller.
 
+    Caution: The browser state might need to be manually reseted.
+
     Args:
-      device: an andriod device.
+      device: an android device.
     """
     assert device is not None, 'Should you be using LocalController instead?'
     super(RemoteChromeController, self).__init__()
@@ -312,9 +318,6 @@ class RemoteChromeController(ChromeControllerBase):
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
     self._device.ForceStop(package_info.package)
-    if OPTIONS.clear_device_data:
-      logging.info('Clear Chrome data')
-      self._device.adb.Shell('pm clear ' + package_info.package)
     chrome_args = self._GetChromeArguments()
     logging.info('Launching %s with flags: %s' % (package_info.package,
         subprocess.list2cmdline(chrome_args)))
@@ -355,13 +358,24 @@ class RemoteChromeController(ChromeControllerBase):
       finally:
         self._device.ForceStop(package_info.package)
 
+  def ResetBrowserState(self):
+    """Override for chrome state reseting."""
+    logging.info('Reset chrome\'s profile')
+    package_info = OPTIONS.ChromePackage()
+    # We assume all the browser is in the Default user profile directory.
+    cmd = ['rm', '-rf', '/data/data/{}/app_chrome/Default'.format(
+               package_info.package)]
+    self._device.adb.Shell(subprocess.list2cmdline(cmd))
+
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
+    logging.info('Push cache from %s' % cache_path)
     chrome_cache.PushBrowserCache(self._device, cache_path)
 
   def PullBrowserCache(self):
     """Override for chrome cache pulling."""
     assert self._slow_death, 'Must do SetSlowDeath() before opening chrome.'
+    logging.info('Pull cache from device')
     return chrome_cache.PullBrowserCache(self._device)
 
   @contextlib.contextmanager
@@ -385,6 +399,10 @@ class LocalChromeController(ChromeControllerBase):
   """Controller for a local (desktop) chrome instance."""
 
   def __init__(self):
+    """Initialize the controller.
+
+    Caution: The browser state might need to be manually reseted.
+    """
     super(LocalChromeController, self).__init__()
     if OPTIONS.no_sandbox:
       self.AddChromeArgument('--no-sandbox')
@@ -485,6 +503,18 @@ class LocalChromeController(ChromeControllerBase):
       if self._headless:
         xvfb_process.kill()
 
+  def ResetBrowserState(self):
+    """Override for chrome state reseting."""
+    assert os.path.isdir(self._profile_dir)
+    logging.info('Reset chrome\'s profile')
+    # Don't do a rmtree(self._profile_dir) because it might be a temp directory.
+    for filename in os.listdir(self._profile_dir):
+      path = os.path.join(self._profile_dir, filename)
+      if os.path.isdir(path):
+        shutil.rmtree(path)
+      else:
+        os.remove(path)
+
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
     self._EnsureProfileDirectory()
@@ -524,7 +554,6 @@ class LocalChromeController(ChromeControllerBase):
       # Launch chrome so that it populates the profile directory.
       with self.Open():
         pass
-      print os.listdir(self._profile_dir + '/Default')
     assert os.path.isdir(self._profile_dir)
     assert os.path.isdir(os.path.dirname(self._GetCacheDirectoryPath()))
 
diff --git a/loading/options.py b/loading/options.py
index b7f03a2..af21ae3 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -18,9 +18,7 @@ class Options(object):
   be available as instance attributes (eg, OPTIONS.clear_cache).
   """
   # Tuples of (argument name, default value, help string).
-  _ARGS = [ ('clear_device_data', False,
-             'Clear Chrome data from device before loading'),
-            ('chrome_package_name', 'chrome',
+  _ARGS = [ ('chrome_package_name', 'chrome',
              'build/android/pylib/constants package description'),
             ('devtools_hostname', 'localhost',
              'hostname for devtools websocket connection'),
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 665824b..03c6b04 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -224,6 +224,7 @@ class SandwichRunner(object):
       trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, run_id):
+    self._chrome_ctl.ResetBrowserState()
     clear_cache = False
     if self.cache_operation == 'clear':
       clear_cache = True
diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
index 3dc6c81..ec85dca 100755
--- a/loading/unmaintained/gce_validation_collect.sh
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -37,7 +37,6 @@ EOF
  for ((run=0;run<$repeat_count;++run)); do
    echo '****'  $run
    tools/android/loading/analyze.py log_requests \
-      --clear_device_data \
       --devtools_port 9222 \
       --url $site \
       --output $outdir/${output_subdir}/${run}

commit ec2e204d8d44d271fc2d338a63c3f091b7e2f24a
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 13 05:35:05 2016 -0700

    tools/android/loading: Implement DevToolsConnectionTargetCrashed
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1972133002
    Cr-Original-Commit-Position: refs/heads/master@{#393507}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6d741930ddf7556885fa3abdfe8da4dd903b02e5

diff --git a/loading/controller.py b/loading/controller.py
index e9f5995..5ccf688 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -78,7 +78,8 @@ class ChromeControllerError(Exception):
     Some of these errors might be known intermittent errors that can usually be
     retried by the caller after re-doing any specific setup again.
   """
-  _INTERMITTENT_WHITE_LIST = {websocket.WebSocketTimeoutException}
+  _INTERMITTENT_WHITE_LIST = {websocket.WebSocketTimeoutException,
+                              devtools_monitor.DevToolsConnectionTargetCrashed}
 
   def __init__(self, log):
     """Constructor
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 4952d10..c2d5971 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -33,6 +33,9 @@ class DevToolsConnectionException(Exception):
     super(DevToolsConnectionException, self).__init__(message)
     logging.warning("DevToolsConnectionException: " + message)
 
+class DevToolsConnectionTargetCrashed(DevToolsConnectionException):
+  pass
+
 
 # Taken from telemetry.internal.backends.chrome_inspector.tracing_backend.
 # TODO(mattcary): combine this with the above and export?
@@ -111,6 +114,7 @@ class DevToolsConnection(object):
     self._target_descriptor = None
 
     self._Connect()
+    self.RegisterListener('Inspector.targetCrashed', self)
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -297,6 +301,11 @@ class DevToolsConnection(object):
     if not self._please_stop:
       logging.warning('%s stopped on a timeout.' % kind)
 
+  def Handle(self, method, event):
+    del event # unused
+    if method == 'Inspector.targetCrashed':
+      raise DevToolsConnectionTargetCrashed('Renderer crashed.')
+
   def _TearDownMonitoring(self):
     if self.TRACING_DOMAIN in self._domains_to_enable:
       logging.info('Fetching tracing')

commit 141c8659df4e0c159f18940e52c86c9c7bb79f00
Author: droger <droger@chromium.org>
Date:   Fri May 13 04:43:50 2016 -0700

    tools/android/loading Add a task handler for generating trace reports
    
    The report task:
    - searches files with prefix "trace_database" and loads them.
    - for each URL, lookup in the database and find the traces
    - load the trace, generate a report from it
    - upload to BigQuery using the streaming API. This creates
      the table lazily, but requires a table template (not
      created automatically).
    
    The change of the URL in TraceReportHandler is to ensure
    that the URL in the loading trace database is the same as
    the one given by the user.
    Otherwise, the user will not be able to find some URLs in
    the database when generating reports.
    
    Review-Url: https://codereview.chromium.org/1947123002
    Cr-Original-Commit-Position: refs/heads/master@{#393500}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d47c255f3700621eeb9b8b5ba3897cb106b8364c

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index 44191e0..5a8be02 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -6,14 +6,16 @@ import os
 
 from common.clovis_task import ClovisTask
 from failure_database import FailureDatabase
+from report_task_handler import ReportTaskHandler
 from trace_task_handler import TraceTaskHandler
 
 
 class ClovisTaskHandler(object):
   """Handles all the supported clovis tasks."""
 
-  def __init__(self, base_path, failure_database, google_storage_accessor,
-               binaries_path, logger, instance_name=None):
+  def __init__(self, project_name, base_path, failure_database,
+               google_storage_accessor, bigquery_service, binaries_path, logger,
+               instance_name=None):
     """Creates a ClovisTaskHandler.
 
     Args:
@@ -22,10 +24,14 @@ class ClovisTaskHandler(object):
       instance_name(str, optional): Name of the ComputeEngine instance.
     """
     self._failure_database = failure_database
+    trace_path = os.path.join(base_path, 'trace')
     self._handlers = {
         'trace': TraceTaskHandler(
-            os.path.join(base_path, 'trace'), failure_database,
-            google_storage_accessor, binaries_path, logger, instance_name)}
+            trace_path, failure_database, google_storage_accessor,
+            binaries_path, logger, instance_name),
+        'report': ReportTaskHandler(
+            project_name, failure_database, google_storage_accessor,
+            bigquery_service, logger)}
 
   def Run(self, clovis_task):
     """Runs a clovis_task.
diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
new file mode 100644
index 0000000..47365b5
--- /dev/null
+++ b/loading/cloud/backend/report_task_handler.py
@@ -0,0 +1,140 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import uuid
+
+from googleapiclient import errors
+
+import common.google_error_helper as google_error_helper
+from failure_database import FailureDatabase
+from loading_trace import LoadingTrace
+from loading_trace_database import LoadingTraceDatabase
+from report import LoadingReport
+
+
+def LoadRemoteTrace(storage_accessor, remote_trace_path, logger):
+  """Loads and returns the LoadingTrace located at the remote trace path.
+
+  Args:
+    storage_accessor: (GoogleStorageAccessor) Used to download the trace from
+                                              CloudStorage.
+    remote_trace_path: (str) Path to the trace file.
+  """
+
+  # Cut the gs://<bucket_name> prefix from trace paths if needed.
+  prefix = 'gs://%s/' % storage_accessor.BucketName()
+  prefix_length = len(prefix)
+  if remote_trace_path.startswith(prefix):
+    remote_trace_path = remote_trace_path[prefix_length:]
+
+  trace_string = storage_accessor.DownloadAsString(
+      remote_trace_path)
+  if not trace_string:
+    logger.error('Failed to download: ' + remote_trace_path)
+    return None
+
+  trace_dict = json.loads(trace_string)
+  if not trace_dict:
+    logger.error('Failed to parse: ' + remote_trace_path)
+    return None
+
+  trace = LoadingTrace.FromJsonDict(trace_dict)
+  if not trace:
+    logger.error('Invalid format for: ' + remote_trace_path)
+    return None
+
+  return trace
+
+
+class ReportTaskHandler(object):
+  """Handles 'report' tasks.
+
+  This handler loads the traces given in the task parameters, generates a report
+  from them, and add them to a BigQuery table.
+  The BigQuery table is implicitly created from a template (using the stream
+  mode), and identified by the task tag.
+  """
+
+  def __init__(self, project_name, failure_database, google_storage_accessor,
+               bigquery_service, logger):
+    self._project_name = project_name
+    self._failure_database = failure_database
+    self._google_storage_accessor = google_storage_accessor
+    self._bigquery_service = bigquery_service
+    self._logger = logger
+
+  def _StreamRowsToBigQuery(self, rows, table_id):
+    """Uploads a list of rows to the BigQuery table associated with the given
+    table_id.
+
+    Args:
+      rows: (list of dict) Each dictionary is a row to add to the table.
+      table_id: (str) Identifier of the BigQuery table to update.
+    """
+    # Assumes that the dataset and the table template already exist.
+    dataset = 'clovis_dataset'
+    template = 'report'
+    rows_data = [{'json': row, 'insertId': str(uuid.uuid4())} for row in rows]
+    body = {'rows': rows_data, 'templateSuffix':'_'+table_id}
+    self._logger.info('BigQuery API request:\n' + str(body))
+
+    try:
+      response = self._bigquery_service.tabledata().insertAll(
+          projectId=self._project_name, datasetId=dataset, tableId=template,
+          body=body).execute()
+      self._logger.info('BigQuery API response:\n' + str(response))
+    except errors.HttpError as http_error:
+      # Handles HTTP error response codes (such as 404), typically indicating a
+      # problem in parameters other than 'body'.
+      error_content = google_error_helper.GetErrorContent(http_error)
+      error_reason = google_error_helper.GetErrorReason(error_content)
+      self._logger.error('BigQuery API error (reason: "%s"):\n%s' % (
+            error_reason, http_error))
+      self._failure_database.AddFailure('big_query_error', error_reason)
+      if error_content:
+        self._logger.error('Error details:\n%s' % error_content)
+      return
+
+    # Handles other errors, typically when the body is ill-formatted.
+    insert_errors = response.get('insertErrors')
+    if insert_errors:
+      self._logger.error('BigQuery API error:\n' + str(insert_errors))
+      for insert_error in insert_errors:
+        self._failure_database.AddFailure('big_query_insert_error',
+                                          str(insert_error.get('errors')))
+
+  def Run(self, clovis_task):
+    """Runs a 'report' clovis_task.
+
+    Args:
+      clovis_task: (ClovisTask) The task to run.
+    """
+    if clovis_task.Action() != 'report':
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      self._failure_database.AddFailure(FailureDatabase.CRITICAL_ERROR,
+                                        'report_task_handler_run')
+      return
+
+    rows = []
+    for path in clovis_task.ActionParams()['traces']:
+      self._logger.info('Generating report for: ' + path)
+      trace = LoadRemoteTrace(self._google_storage_accessor, path, self._logger)
+      if not trace:
+        self._logger.error('Failed loading trace at: ' + path)
+        self._failure_database.AddFailure('missing_trace_for_report', path)
+        continue
+      report = LoadingReport(trace).GenerateReport()
+      if not report:
+        self._logger.error('Failed generating report for: ' + path)
+        self._failure_database.AddFailure('report_generation_failed', path)
+        continue
+      rows.append(report)
+
+    if rows:
+      tag = clovis_task.BackendParams()['tag']
+      # BigQuery table names can contain only alpha numeric characters and
+      # underscores.
+      table_id = ''.join(c for c in tag if c.isalnum() or c == '_')
+      self._StreamRowsToBigQuery(rows, table_id)
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index bcb185b..a6e83a7 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -83,14 +83,13 @@ class TraceTaskHandler(object):
     except OSError:
       pass  # Nothing to remove.
 
-    if not url.startswith('http') and not url.startswith('file'):
-      url = 'http://' + url
-
     old_stdout = sys.stdout
     old_stderr = sys.stderr
 
     trace_metadata = { 'succeeded' : False, 'url' : url }
     trace = None
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
     with open(log_filename, 'w') as sys.stdout:
       try:
         sys.stderr = sys.stdout
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index eb6cbdb..e0ebebf 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -73,10 +73,12 @@ class Worker(object):
       self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
                                         'failure_database')
 
+    bigquery_service = discovery.build('bigquery', 'v2',
+                                      credentials=self._credentials)
     self._clovis_task_handler = ClovisTaskHandler(
-        self._base_path_in_bucket, self._failure_database,
-        self._google_storage_accessor, config['binaries_path'], self._logger,
-        self._instance_name)
+        self._project_name, self._base_path_in_bucket, self._failure_database,
+        self._google_storage_accessor, bigquery_service,
+        config['binaries_path'], self._logger, self._instance_name)
 
     self._UploadFailureDatabase()
 
diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
index 20a5182..cf470e7 100644
--- a/loading/cloud/common/clovis_task.py
+++ b/loading/cloud/common/clovis_task.py
@@ -23,7 +23,7 @@ class ClovisTask(object):
           'tag' key, a unique tag will be generated.
     """
     self._action = action
-    self._action_params = action_params
+    self._action_params = action_params or {}
     self._backend_params = backend_params or {}
     # If no tag is specified, generate a unique tag.
     if not self._backend_params.get('tag'):
@@ -39,12 +39,14 @@ class ClovisTask(object):
     try:
       data = json.loads(json_dict)
       action = data['action']
-      action_params = data['action_params']
       # Vaidate the format.
       if action == 'trace':
+        action_params = data['action_params']
         urls = action_params['urls']
         if (type(urls) is not list) or (len(urls) == 0):
           return None
+      elif action == 'report':
+        action_params = data.get('action_params')
       else:
         # When more actions are supported, check that they are valid here.
         return None

commit 76833cf5b6ade8e13437185218da28cab5db7903
Author: mattcary <mattcary@chromium.org>
Date:   Fri May 13 04:17:33 2016 -0700

    Clovis: Enable tracking of % of downloaded bytes for first X paint events.
    
    Adds these stats to report.py, and includes some minor test refactoring to make
    everything easier.
    
    BUG=
    
    Review-Url: https://codereview.chromium.org/1971123002
    Cr-Original-Commit-Position: refs/heads/master@{#393498}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1b1572b4493d59e4af008b1351137297f6fb42c5

diff --git a/loading/network_activity_lens.py b/loading/network_activity_lens.py
index 2040093..b30cac8 100644
--- a/loading/network_activity_lens.py
+++ b/loading/network_activity_lens.py
@@ -36,6 +36,7 @@ class NetworkActivityLens(object):
     self._downloaded_bytes_timeline = []
     self._upload_rate_timeline = []
     self._download_rate_timeline = []
+    self._total_downloaded_bytes = 0
     requests = trace.request_track.GetEvents()
     self._network_events = list(itertools.chain.from_iterable(
         NetworkEvent.EventsFromRequest(request) for request in requests))
@@ -58,6 +59,36 @@ class NetworkActivityLens(object):
   def download_rate_timeline(self):
     return (self._start_end_times, self._download_rate_timeline)
 
+  @property
+  def total_download_bytes(self):
+    return self._total_downloaded_bytes
+
+  def DownloadedBytesAt(self, time_msec):
+    """Return the the downloaded bytes at a given timestamp.
+
+    Args:
+      time_msec: a timestamp, in the same scale as the timelines.
+
+    Returns:
+      The total bytes downloaded up until the time period ending at time_msec.
+    """
+    # We just do a linear cumulative sum. Currently this is only called a couple
+    # of times, so making an indexed cumulative sum does not seem to be worth
+    # the bother.
+    total_bytes = 0
+    previous_msec = self.downloaded_bytes_timeline[0][0]
+    for msec, nbytes in zip(*self.downloaded_bytes_timeline):
+      if msec < time_msec:
+        total_bytes += nbytes
+        previous_msec = msec
+      else:
+        if time_msec > previous_msec:
+          fraction_of_chunk = ((time_msec - previous_msec)
+                               / (msec - previous_msec))
+          total_bytes += float(nbytes) * fraction_of_chunk
+        break
+    return total_bytes
+
   def _IndexEvents(self):
     start_end_times_set = set()
     for event in self._network_events:
@@ -87,6 +118,7 @@ class NetworkActivityLens(object):
       downloaded_bytes = sum(
           e.DownloadedBytes() for e in self._active_events_list[index]
           if timestamp == e.end_msec)
+      self._total_downloaded_bytes += downloaded_bytes
       self._uploaded_bytes_timeline.append(uploaded_bytes)
       self._downloaded_bytes_timeline.append(downloaded_bytes)
       self._upload_rate_timeline.append(upload_rate)
diff --git a/loading/network_activity_lens_unittest.py b/loading/network_activity_lens_unittest.py
index e857679..7b2031e 100644
--- a/loading/network_activity_lens_unittest.py
+++ b/loading/network_activity_lens_unittest.py
@@ -67,6 +67,7 @@ class NetworkActivityLensTestCase(unittest.TestCase):
     download_rate = lens.download_rate_timeline
     self.assertEquals(4 / 10e-3, download_rate[1][5])
     self.assertEquals(0, download_rate[1][6])
+    self.assertAlmostEquals(4, lens.total_download_bytes)
 
   def testLongRequest(self):
     timing_dict = {
@@ -102,6 +103,32 @@ class NetworkActivityLensTestCase(unittest.TestCase):
         self.assertEquals(0, downloaded_bytes[index])
     self.assertEquals(1000, downloaded_bytes[-1])
 
+  def testDownloadedBytesAt(self):
+    timing_dict = {
+        'requestTime': 1.2,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    lens = self._NetworkActivityLens([request])
+    # See testTransferredBytes for key events times. We test around events at
+    # the start, middle and end of the data transfer as well as for the
+    # interpolation.
+    self.assertEquals(0, lens.DownloadedBytesAt(1219))
+    self.assertEquals(0, lens.DownloadedBytesAt(1220))
+    self.assertEquals(0, lens.DownloadedBytesAt(1225))
+    self.assertEquals(0, lens.DownloadedBytesAt(1280))
+    self.assertEquals(1.6, lens.DownloadedBytesAt(1281))
+    self.assertEquals(8, lens.DownloadedBytesAt(1285))
+    self.assertEquals(14.4, lens.DownloadedBytesAt(1289))
+    self.assertEquals(16, lens.DownloadedBytesAt(1290))
+    self.assertEquals(16, lens.DownloadedBytesAt(1291))
+    self.assertEquals(16, lens.DownloadedBytesAt(1295))
+    self.assertEquals(16, lens.DownloadedBytesAt(1300))
+    self.assertEquals(16, lens.DownloadedBytesAt(1400))
+
   def _NetworkActivityLens(self, requests):
     trace = test_utils.LoadingTraceFromEvents(requests)
     return NetworkActivityLens(trace)
diff --git a/loading/report.py b/loading/report.py
index a38ed5d..eb19779 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -8,6 +8,7 @@ When executed as a script, takes a trace filename and print the report.
 """
 
 import loading_trace
+from network_activity_lens import NetworkActivityLens
 from user_satisfied_lens import (
     FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
 
@@ -33,6 +34,18 @@ class LoadingReport(object):
     self._max_msec = max(
         r.end_msec or -1 for r in self.trace.request_track.GetEvents())
 
+    network_lens = NetworkActivityLens(self.trace)
+    if network_lens.total_download_bytes > 0:
+      self._contentful_byte_frac = network_lens.DownloadedBytesAt(
+          self._contentful_paint_msec -
+          self._base_msec) / float(network_lens.total_download_bytes)
+      self._significant_byte_frac = network_lens.DownloadedBytesAt(
+          self._significant_paint_msec -
+          self._base_msec) / float(network_lens.total_download_bytes)
+    else:
+      self._contentful_byte_frac = float('Nan')
+      self._significant_byte_frac = float('Nan')
+
   def GenerateReport(self):
     """Returns a report as a dict."""
     return {
@@ -40,7 +53,9 @@ class LoadingReport(object):
         'first_text_ms': self._text_msec - self._base_msec,
         'contentful_paint_ms': self._contentful_paint_msec - self._base_msec,
         'significant_paint_ms': self._significant_paint_msec - self._base_msec,
-        'plt_ms': self._max_msec - self._base_msec}
+        'plt_ms': self._max_msec - self._base_msec,
+        'contentful_byte_frac': self._contentful_byte_frac,
+        'significant_byte_frac': self._significant_byte_frac,}
 
   @classmethod
   def FromTraceFilename(cls, filename):
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 4d4fb3f..58c08c8 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -20,11 +20,15 @@ class LoadingReportTestCase(unittest.TestCase):
 
   @classmethod
   def _MakeTrace(cls):
-    trace_creator = user_satisfied_lens_unittest.TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(cls._FIRST_REQUEST_TIME),
                 trace_creator.RequestAt(
                     cls._FIRST_REQUEST_TIME + cls._REQUEST_OFFSET,
                     cls._DURATION)]
+    requests[0].timing.receive_headers_end = 0
+    requests[1].timing.receive_headers_end = 0
+    requests[0].encoded_data_length = 128
+    requests[1].encoded_data_length = 1024
     trace = trace_creator.CreateTrace(
         requests,
         [{'ts': cls._CONTENTFUL_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
@@ -57,6 +61,8 @@ class LoadingReportTestCase(unittest.TestCase):
                      loading_report['contentful_paint_ms'])
     self.assertEqual(self._REQUEST_OFFSET + self._DURATION,
                      loading_report['plt_ms'])
+    self.assertAlmostEqual(0.3, loading_report['contentful_byte_frac'])
+    self.assertAlmostEqual(0.14, loading_report['significant_byte_frac'], 2)
 
 
 if __name__ == '__main__':
diff --git a/loading/request_track.py b/loading/request_track.py
index 8f7829b..34e9ef8 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -51,6 +51,9 @@ class Timing(object):
     return all(getattr(self, attr) == getattr(o, attr)
                for attr in self.__slots__)
 
+  def __str__(self):
+    return str(self.ToJsonDict())
+
   def LargestOffset(self):
     """Returns the largest offset in the available timings."""
     return max(0, max(
@@ -60,6 +63,7 @@ class Timing(object):
   def ToJsonDict(self):
     return {attr: getattr(self, attr)
             for attr in self.__slots__ if getattr(self, attr) != -1}
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     return cls(**json_dict)
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 10385b5..5f0d743 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -225,3 +225,27 @@ class MockUserSatisfiedLens(user_satisfied_lens._FirstEventLens):
   def _CalculateTimes(self, _):
     self._satisfied_msec = float('inf')
     self._event_msec = float('inf')
+
+
+class TraceCreator(object):
+  def __init__(self):
+    self._request_index = 1
+
+  def RequestAt(self, timestamp_msec, duration=1):
+    timestamp_sec = float(timestamp_msec) / 1000
+    rq = request_track.Request.FromJsonDict({
+        'url': 'http://bla-%s-.com' % timestamp_msec,
+        'request_id': '0.%s' % self._request_index,
+        'frame_id': '123.%s' % timestamp_msec,
+        'initiator': {'type': 'other'},
+        'timestamp': timestamp_sec,
+        'timing': {'request_time': timestamp_sec,
+                   'loading_finished': duration}
+        })
+    self._request_index += 1
+    return rq
+
+  def CreateTrace(self, requests, events, main_frame_id):
+    trace = LoadingTraceFromEvents(requests, trace_events=events)
+    trace.tracing_track.SetMainFrameID(main_frame_id)
+    return trace
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 4d0bda7..225548d 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -46,7 +46,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testFirstContentfulPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -71,7 +71,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
 
   def testCantGetNoSatisfaction(self):
     MAINFRAME = 1
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -88,7 +88,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testFirstTextPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -115,7 +115,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
 
   def testFirstSignificantPaintLens(self):
     MAINFRAME = 1
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(15), trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -154,7 +154,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testRequestFingerprintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(

commit 7572784ca61ddae6c8b288994ab389d94e14fea1
Author: droger <droger@chromium.org>
Date:   Fri May 13 02:22:02 2016 -0700

    tools/android/loading Split error helpers to their own file
    
    This will allow this code to be shared more easily in a follow up CL.
    
    Review-Url: https://codereview.chromium.org/1961353002
    Cr-Original-Commit-Position: refs/heads/master@{#393489}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 992640148a4d737e9b7cfd8338b1054adba1205e

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index 00ba434..44191e0 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -4,7 +4,7 @@
 
 import os
 
-from cloud.common.clovis_task import ClovisTask
+from common.clovis_task import ClovisTask
 from failure_database import FailureDatabase
 from trace_task_handler import TraceTaskHandler
 
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index 25aff4f..bcb185b 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -7,7 +7,7 @@ import os
 import re
 import sys
 
-from cloud.common.clovis_task import ClovisTask
+from common.clovis_task import ClovisTask
 import controller
 from failure_database import FailureDatabase
 import loading_trace
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index e5777c8..eb6cbdb 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -14,11 +14,15 @@ from oauth2client.client import GoogleCredentials
 # NOTE: The parent directory needs to be first in sys.path to avoid conflicts
 # with catapult modules that have colliding names, as catapult inserts itself
 # into the path as the second element. This is an ugly and fragile hack.
-sys.path.insert(0,
-    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir,
-                 os.pardir))
-from cloud.common.clovis_task import ClovisTask
-from cloud.common.google_instance_helper import GoogleInstanceHelper
+_CLOUD_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)),
+                          os.pardir)
+sys.path.insert(0, os.path.join(_CLOUD_DIR, os.pardir))
+# Add _CLOUD_DIR to the path to access common code through the same path as the
+# frontend.
+sys.path.append(_CLOUD_DIR)
+
+from common.clovis_task import ClovisTask
+from common.google_instance_helper import GoogleInstanceHelper
 from clovis_task_handler import ClovisTaskHandler
 from failure_database import FailureDatabase
 from google_storage_accessor import GoogleStorageAccessor
diff --git a/loading/cloud/common/google_error_helper.py b/loading/cloud/common/google_error_helper.py
new file mode 100644
index 0000000..eac968a
--- /dev/null
+++ b/loading/cloud/common/google_error_helper.py
@@ -0,0 +1,33 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Helper functions to manage errors returned by Google Compute APIs."""
+
+import json
+
+
+# Error reason returned by GetErrorReason() when a resource is not found.
+REASON_NOT_FOUND = 'notFound'
+
+
+def GetErrorContent(error):
+  """Returns the contents of an error returned by Google Compute APIs as a
+  dictionary or None.
+  """
+  if not error.resp.get('content-type', '').startswith('application/json'):
+    return None
+  return json.loads(error.content)
+
+
+def GetErrorReason(error_content):
+  """Returns the error reason as a string."""
+  if not error_content:
+    return None
+  if (not error_content.get('error') or
+      not error_content['error'].get('errors')):
+    return None
+  error_list = error_content['error']['errors']
+  if not error_list:
+    return None
+  return error_list[0].get('reason')
diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 746512d..02c4121 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -7,6 +7,8 @@ import time
 
 from googleapiclient import (discovery, errors)
 
+import common.google_error_helper as google_error_helper
+
 
 class GoogleInstanceHelper(object):
   """Helper class for the Google Compute API, allowing to manage groups of
@@ -32,8 +34,8 @@ class GoogleInstanceHelper(object):
       self._logger.info('Compute API response:\n' + str(response))
       return (True, response)
     except errors.HttpError as err:
-      error_content = self._GetErrorContent(err)
-      error_reason = self._GetErrorReason(error_content)
+      error_content = google_error_helper.GetErrorContent(err)
+      error_reason = google_error_helper.GetErrorReason(error_content)
       if error_reason == 'resourceNotReady' and retry_count > 0:
         # Retry after a delay
         delay_seconds = 1
@@ -56,26 +58,6 @@ class GoogleInstanceHelper(object):
     """Returns the name of the instance group associated with tag."""
     return 'group-' + tag
 
-  def _GetErrorContent(self, error):
-    """Returns the contents of an error returned by the Compute API as a
-    dictionary or None.
-    """
-    if not error.resp.get('content-type', '').startswith('application/json'):
-      return None
-    return json.loads(error.content)
-
-  def _GetErrorReason(self, error_content):
-    """Returns the error reason as a string."""
-    if not error_content:
-      return None
-    if (not error_content.get('error') or
-        not error_content['error'].get('errors')):
-      return None
-    error_list = error_content['error']['errors']
-    if not error_list:
-      return None
-    return error_list[0].get('reason')
-
   def CreateTemplate(self, tag, bucket):
     """Creates an instance template for instances identified by tag and using
     bucket for deployment. Returns True if successful.
@@ -128,7 +110,8 @@ class GoogleInstanceHelper(object):
     (success, result) = self._ExecuteApiRequest(request)
     if success:
       return True
-    if self._GetErrorReason(result) == 'notFound':
+    if google_error_helper.GetErrorReason(result) == \
+        google_error_helper.REASON_NOT_FOUND:
       # The template does not exist, nothing to do.
       self._logger.warning('Template not found: ' + template_name)
       return True
@@ -176,7 +159,8 @@ class GoogleInstanceHelper(object):
     (success, result) = self._ExecuteApiRequest(request)
     if success:
       return True
-    if self._GetErrorReason(result) == 'notFound':
+    if google_error_helper.GetErrorReason(result) == \
+        google_error_helper.REASON_NOT_FOUND:
       # The group does not exist, nothing to do.
       self._logger.warning('Instance group not found: ' + group_name)
       return True

commit ba1cc0c57318dd1d5487ff3dbf8e5584cdb3f6a6
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 13 01:51:40 2016 -0700

    sandwich: Add the cache archive patching task.
    
    Set-cookie response header is by design pruned from the request's
    reponse headers that are cached to keep the cookie modification
    coherent from JavaScript and Server sided modifications.
    
    But redirected requests are getting an implicit Vary: cookie. As
    a result, some redirected requests were getting invalidated.
    
    Moreover, this change is believe to reduce noise in the requests
    interminism brought by JavaScript.
    
    This CL take this oportunity to also remove stream index 2 from
    cached entries since NoState-Prefetch will only fetch resources,
    but not parse them.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1964643003
    Cr-Original-Commit-Position: refs/heads/master@{#393481}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 75174ba408c51748e01d6462c1f74b02c00573b0

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index df8f955..ead016e 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -29,7 +29,7 @@ OPTIONS = options.OPTIONS
 
 
 # Cache back-end types supported by cachetool.
-BACKEND_TYPES = ['simple']
+BACKEND_TYPES = {'simple', 'blockfile'}
 
 # Regex used to parse HTTP headers line by line.
 HEADER_PARSING_REGEX = re.compile(r'^(?P<header>\S+):(?P<value>.*)$')
@@ -251,6 +251,11 @@ class CacheBackend(object):
     # Make sure cache_directory_path is a valid cache.
     self._CachetoolCmd('validate')
 
+  def GetSize(self):
+    """Gets total size of cache entries in bytes."""
+    size = self._CachetoolCmd('get_size')
+    return int(size.strip())
+
   def ListKeys(self):
     """Lists cache's keys.
 
@@ -272,7 +277,16 @@ class CacheBackend(object):
     Returns:
       String holding stream binary content.
     """
-    return self._CachetoolCmd('get_stream', key, str(index))
+    return self._CachetoolCmd('get_stream', [key, str(index)])
+
+  def DeleteStreamForKey(self, key, index):
+    """Delete a key's stream.
+
+    Args:
+      key: The key to access the stream.
+      index: The stream index
+    """
+    self._CachetoolCmd('delete_stream', [key, str(index)])
 
   def DeleteKey(self, key):
     """Deletes a key from the cache.
@@ -280,14 +294,15 @@ class CacheBackend(object):
     Args:
       key: The key delete.
     """
-    self._CachetoolCmd('delete_key', key)
+    self._CachetoolCmd('delete_key', [key])
 
-  def _CachetoolCmd(self, operation, *args):
+  def _CachetoolCmd(self, operation, args=None, stdin=''):
     """Runs the cache editor tool and return the stdout.
 
     Args:
       operation: Cachetool operation.
-      *args: Additional operation argument to append to the command line.
+      args: Additional operation argument to append to the command line.
+      stdin: String to pipe to the Cachetool's stdin.
 
     Returns:
       Cachetool's stdout string.
@@ -297,12 +312,22 @@ class CacheBackend(object):
         self._cache_directory_path,
         self._cache_backend_type,
         operation]
-    editor_tool_cmd.extend(args)
-    process = subprocess.Popen(editor_tool_cmd, stdout=subprocess.PIPE)
-    stdout_data, _ = process.communicate()
+    editor_tool_cmd.extend(args or [])
+    process = subprocess.Popen(
+        editor_tool_cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE)
+    stdout_data, _ = process.communicate(input=stdin)
     assert process.returncode == 0
     return stdout_data
 
+  def UpdateRawResponseHeaders(self, key, raw_headers):
+    """Updates a key's raw response headers.
+
+    Args:
+      key: The key to modify.
+      raw_headers: Raw response headers to set.
+    """
+    self._CachetoolCmd('update_raw_headers', [key], stdin=raw_headers)
+
   def GetDecodedContentForKey(self, key):
     """Gets a key's decoded content.
 
diff --git a/loading/request_track.py b/loading/request_track.py
index 5da2abe..8f7829b 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -163,6 +163,7 @@ class Request(object):
                          Worker.
     timing: (Timing) Request timing, extended with loading_finished.
     status: (int) Response status code.
+    status_text: (str) Response status text received in the status line.
     encoded_data_length: (int) Total encoded data length.
     data_chunks: (list) [(offset, encoded_data_length), ...] List of data
                  chunks received, with their offset in ms relative to
@@ -200,6 +201,7 @@ class Request(object):
     self.from_service_worker = False
     self.timing = None
     self.status = None
+    self.status_text = None
     self.encoded_data_length = 0
     self.data_chunks = []
     self.failed = False
@@ -346,6 +348,17 @@ class Request(object):
     # All fields in timing are millis relative to request_time.
     return self.timing.LargestOffset()
 
+  def GetRawResponseHeaders(self):
+    """Gets the request's raw response headers compatible with
+    net::HttpResponseHeaders's constructor.
+    """
+    assert not self.IsDataRequest()
+    headers = '{} {} {}\x00'.format(
+        self.protocol.upper(), self.status, self.status_text)
+    for key in sorted(self.response_headers.keys()):
+      headers += '{}: {}\x00'.format(key, self.response_headers[key])
+    return headers
+
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
 
@@ -668,7 +681,8 @@ class RequestTrack(devtools_monitor.Track):
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache'),
-                           ('protocol', 'protocol'), ('status', 'status')))
+                           ('protocol', 'protocol'), ('status', 'status'),
+                           ('statusText', 'status_text')))
     r.timing = Timing.FromDevToolsDict(redirect_response['timing'])
 
     redirect_index = self._redirects_count_by_id[request_id]
@@ -712,7 +726,7 @@ class RequestTrack(devtools_monitor.Track):
         response, r, (('status', 'status'), ('mimeType', 'mime_type'),
                       ('fromDiskCache', 'from_disk_cache'),
                       ('fromServiceWorker', 'from_service_worker'),
-                      ('protocol', 'protocol'),
+                      ('protocol', 'protocol'), ('statusText', 'status_text'),
                       # Actual request headers are not known before reaching the
                       # network stack.
                       ('requestHeaders', 'request_headers'),
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 19f7b4a..bac6218 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -74,6 +74,15 @@ class RequestTestCase(unittest.TestCase):
     r.response_headers = {'foo': 'Bar', 'Baz': 'Foo'}
     self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
 
+  def testGetRawResponseHeaders(self):
+    r = Request()
+    r.protocol = 'http/1.1'
+    r.status = 200
+    r.status_text = 'Hello world'
+    r.response_headers = {'Foo': 'Bar', 'Baz': 'Foo'}
+    self.assertEquals('HTTP/1.1 200 Hello world\x00Baz: Foo\x00Foo: Bar\x00',
+                      r.GetRawResponseHeaders())
+
 
 class CachingPolicyTestCase(unittest.TestCase):
   _REQUEST = {
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index bbd2e72..8005350 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -101,6 +101,63 @@ def _FilterOutDataAndIncompleteRequests(requests):
     yield request
 
 
+def PatchCacheArchive(cache_archive_path, loading_trace_path,
+                      cache_archive_dest_path):
+  """Patch the cache archive.
+
+  Note: This method update the raw response headers of cache entries' to store
+    the ones such as Set-Cookie that were pruned by the
+    net::HttpCacheTransaction, and remove the stream index 2 holding resource's
+    compile meta data.
+
+  Args:
+    cache_archive_path: Input archive's path to patch.
+    loading_trace_path: Path of the loading trace that have recorded the cache
+        archive <cache_archive_path>.
+    cache_archive_dest_path: Archive destination's path.
+  """
+  trace = LoadingTrace.FromJsonFile(loading_trace_path)
+  with common_util.TemporaryDirectory(prefix='sandwich_tmp') as tmp_path:
+    cache_path = os.path.join(tmp_path, 'cache')
+    chrome_cache.UnzipDirectoryContent(cache_archive_path, cache_path)
+    cache_backend = chrome_cache.CacheBackend(cache_path, 'simple')
+    cache_entries = set(cache_backend.ListKeys())
+    logging.info('Original cache size: %d bytes' % cache_backend.GetSize())
+    for request in _FilterOutDataAndIncompleteRequests(
+        trace.request_track.GetEvents()):
+      # On requests having an upload data stream such as POST requests,
+      # net::HttpCache::GenerateCacheKey() prefixes the cache entry's key with
+      # the upload data stream's session unique identifier.
+      #
+      # It is fine to not patch these requests since when reopening Chrome,
+      # there is no way the entry can be reused since the upload data stream's
+      # identifier will be different.
+      #
+      # The fact that these entries are kept in the cache after closing Chrome
+      # properly by closing the Chrome tab as the ChromeControler.SetSlowDeath()
+      # do is a known Chrome bug (crbug.com/610725).
+      #
+      # TODO(gabadie): Add support in ValidateCacheArchiveContent() and in
+      #   VerifyBenchmarkOutputDirectory() for POST requests to be known as
+      #   impossible to use from cache.
+      if request.url not in cache_entries:
+        if request.method != 'POST':
+          raise RuntimeError('Unexpected method that is not found in cache.'
+                             ''.format(request.method))
+        continue
+      # Chrome prunes Set-Cookie from response headers before storing them in
+      # disk cache. Also, it adds implicit "Vary: cookie" header to all redirect
+      # response headers. Sandwich manages the cache, but between recording the
+      # cache and benchmarking the cookie jar is invalidated. This leads to
+      # invalidation of all cacheable redirects.
+      raw_headers = request.GetRawResponseHeaders()
+      cache_backend.UpdateRawResponseHeaders(request.url, raw_headers)
+      # NoState-Prefetch would only fetch the resources, but not parse them.
+      cache_backend.DeleteStreamForKey(request.url, 2)
+    chrome_cache.ZipDirectoryContent(cache_path, cache_archive_dest_path)
+    logging.info('Patched cache size: %d bytes' % cache_backend.GetSize())
+
+
 def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   """Extracts discoverable resource urls from a loading trace according to a
   sub-resource discoverer.
@@ -161,12 +218,15 @@ def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
   if ref_url_set == url_set:
     logging.info('  %d %s are matching.' % (len(ref_url_set), url_set_name))
     return
-  logging.error('  %s are not matching.' % url_set_name)
-  logging.error('    List of missing resources:')
-  for url in ref_url_set.difference(url_set):
+  missing_urls = ref_url_set.difference(url_set)
+  unexpected_urls = url_set.difference(ref_url_set)
+  logging.error('  %s are not matching (expected %d, had %d)' % \
+      (url_set_name, len(ref_url_set), len(url_set)))
+  logging.error('    List of %d missing resources:' % len(missing_urls))
+  for url in sorted(missing_urls):
     logging.error('-     ' + url)
-  logging.error('    List of unexpected resources:')
-  for url in url_set.difference(ref_url_set):
+  logging.error('    List of %d unexpected resources:' % len(unexpected_urls))
+  for url in sorted(unexpected_urls):
     logging.error('+     ' + url)
 
 
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 1d36c78..8fc01af 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -96,10 +96,11 @@ class SandwichTaskBuilder(task_manager.Builder):
     subresources (urls-resources.json).
 
     Here is the full dependency tree for the returned task:
-    common/cache-ref-validation.log
-      depends on: common/cache-ref.zip
-        depends on: common/webpages-patched.wpr
-          depends on: common/webpages.wpr
+    common/patched-cache-validation.log
+      depends on: common/patched-cache.zip
+        depends on: common/original-cache.zip
+          depends on: common/webpages-patched.wpr
+            depends on: common/webpages.wpr
       depends on: common/urls-resources.json
         depends on: common/urls-resources-run/
           depends on: common/webpages.wpr
@@ -113,14 +114,21 @@ class SandwichTaskBuilder(task_manager.Builder):
       shutil.copyfile(self._original_wpr_task.path, BuildPatchedWpr.path)
       sandwich_misc.PatchWpr(BuildPatchedWpr.path)
 
-    @self.RegisterTask('common/cache-ref.zip', [BuildPatchedWpr])
-    def BuildReferenceCache():
+    @self.RegisterTask('common/original-cache.zip', [BuildPatchedWpr])
+    def BuildOriginalCache():
       runner = self._CreateSandwichRunner()
       runner.wpr_archive_path = BuildPatchedWpr.path
-      runner.cache_archive_path = BuildReferenceCache.path
+      runner.cache_archive_path = BuildOriginalCache.path
       runner.cache_operation = 'save'
-      runner.trace_output_directory = BuildReferenceCache.path[:-4] + '-run'
+      runner.trace_output_directory = BuildOriginalCache.run_path
       runner.Run()
+    BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
+
+    @self.RegisterTask('common/patched-cache.zip', [BuildOriginalCache])
+    def BuildPatchedCache():
+      sandwich_misc.PatchCacheArchive(BuildOriginalCache.path,
+          os.path.join(BuildOriginalCache.run_path, '0', 'trace.json'),
+          BuildPatchedCache.path)
 
     @self.RegisterTask('common/subresources-for-urls-run/',
                        dependencies=[self._original_wpr_task])
@@ -138,23 +146,23 @@ class SandwichTaskBuilder(task_manager.Builder):
       with open(ListUrlsResources.path, 'w') as output:
         json.dump(json_content, output)
 
-    @self.RegisterTask('common/cache-ref-validation.log',
-                       [BuildReferenceCache, ListUrlsResources])
-    def ValidateReferenceCache():
+    @self.RegisterTask('common/patched-cache-validation.log',
+                       [BuildPatchedCache, ListUrlsResources])
+    def ValidatePatchedCache():
       json_content = json.load(open(ListUrlsResources.path))
       ref_urls = set()
       for urls in json_content.values():
         ref_urls.update(set(urls))
       sandwich_misc.ValidateCacheArchiveContent(
-          ref_urls, BuildReferenceCache.path)
+          ref_urls, BuildPatchedCache.path)
 
     self._patched_wpr_task = BuildPatchedWpr
-    self._reference_cache_task = BuildReferenceCache
+    self._reference_cache_task = BuildPatchedCache
     self._subresources_for_urls_run_task = UrlsResourcesRun
     self._subresources_for_urls_task = ListUrlsResources
 
-    self._default_final_tasks.append(ValidateReferenceCache)
-    return ValidateReferenceCache
+    self._default_final_tasks.append(ValidatePatchedCache)
+    return ValidatePatchedCache
 
   def PopulateLoadBenchmark(self, subresource_discoverer,
                             transformer_list_name, transformer_list):

commit 1527f66f5082d56574e6202a131fbb6d8d64f400
Author: blundell <blundell@chromium.org>
Date:   Fri May 13 01:37:59 2016 -0700

    tools/android/loading: Add URL to LoadingReport
    
    This CL adds the URL of a trace to the LoadingReport generated for that
    trace.
    
    Review-Url: https://codereview.chromium.org/1972993002
    Cr-Original-Commit-Position: refs/heads/master@{#393477}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a6ea2f67fa437099ba7f7eaf51fb1165b0227eec

diff --git a/loading/report.py b/loading/report.py
index 3918723..a38ed5d 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -36,6 +36,7 @@ class LoadingReport(object):
   def GenerateReport(self):
     """Returns a report as a dict."""
     return {
+        'url': self.trace.url,
         'first_text_ms': self._text_msec - self._base_msec,
         'contentful_paint_ms': self._contentful_paint_msec - self._base_msec,
         'significant_paint_ms': self._significant_paint_msec - self._base_msec,
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 3475681..4d4fb3f 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -48,6 +48,7 @@ class LoadingReportTestCase(unittest.TestCase):
   def testGenerateReport(self):
     trace = self._MakeTrace()
     loading_report = report.LoadingReport(trace).GenerateReport()
+    self.assertEqual(trace.url, loading_report['url'])
     self.assertEqual(self._TEXT_PAINT - self._FIRST_REQUEST_TIME,
                      loading_report['first_text_ms'])
     self.assertEqual(self._SIGNIFICANT_PAINT - self._FIRST_REQUEST_TIME,
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 5bc4715..4d0bda7 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -31,6 +31,7 @@ class TraceCreator(object):
     loading_trace = test_utils.LoadingTraceFromEvents(
         requests, trace_events=events)
     loading_trace.tracing_track.SetMainFrameID(main_frame_id)
+    loading_trace.url = 'http://www.dummy.com'
     return loading_trace
 
 

commit 9e54e0ffbda81d28acfc51c09f67db6f93926b86
Author: gabadie <gabadie@chromium.org>
Date:   Thu May 12 10:33:15 2016 -0700

    tools/android/loading: Get ChromeControllerMetadataGatherer working outside of git
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1972953003
    Cr-Original-Commit-Position: refs/heads/master@{#393286}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7448dac70f1ffdc633d6cf56112c14f4414ebb38

diff --git a/loading/controller.py b/loading/controller.py
index 56d8fd8..e9f5995 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -54,9 +54,12 @@ class ChromeControllerMetadataGatherer(object):
     if self._chromium_commit is None:
       def _GitCommand(subcmd):
         return subprocess.check_output(['git', '-C', _SRC_DIR] + subcmd).strip()
-      self._chromium_commit = _GitCommand(['merge-base', 'master', 'HEAD'])
-      if self._chromium_commit != _GitCommand(['rev-parse', 'HEAD']):
-        self._chromium_commit = 'unknown'
+      try:
+        self._chromium_commit = _GitCommand(['merge-base', 'master', 'HEAD'])
+        if self._chromium_commit != _GitCommand(['rev-parse', 'HEAD']):
+          self._chromium_commit = 'unknown'
+      except subprocess.CalledProcessError:
+        self._chromium_commit = 'git_error'
     return {
       'chromium_commit': self._chromium_commit,
       'date': datetime.datetime.utcnow().isoformat(),

commit 443f9735ee85b225cbf0a1fa1faf94fdc21de55d
Author: gabadie <gabadie@chromium.org>
Date:   Thu May 12 05:44:15 2016 -0700

    sandwich: Add platform infos into the CSVs
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1951033002
    Cr-Original-Commit-Position: refs/heads/master@{#393233}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e860e1d12970b7867e086acaa429b97778bf9171

diff --git a/loading/controller.py b/loading/controller.py
index 8b36d2b..56d8fd8 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -15,6 +15,7 @@ import datetime
 import errno
 import logging
 import os
+import platform
 import shutil
 import socket
 import subprocess
@@ -42,6 +43,27 @@ sys.path.append(
 import websocket
 
 
+class ChromeControllerMetadataGatherer(object):
+  """Gather metadata for the ChromeControllerBase."""
+
+  def __init__(self):
+    self._chromium_commit = None
+
+  def GetMetadata(self):
+    """Gets metadata to update in the ChromeControllerBase"""
+    if self._chromium_commit is None:
+      def _GitCommand(subcmd):
+        return subprocess.check_output(['git', '-C', _SRC_DIR] + subcmd).strip()
+      self._chromium_commit = _GitCommand(['merge-base', 'master', 'HEAD'])
+      if self._chromium_commit != _GitCommand(['rev-parse', 'HEAD']):
+        self._chromium_commit = 'unknown'
+    return {
+      'chromium_commit': self._chromium_commit,
+      'date': datetime.datetime.utcnow().isoformat(),
+      'seconds_since_epoch': time.time()
+    }
+
+
 class ChromeControllerInternalError(Exception):
   pass
 
@@ -92,6 +114,7 @@ class ChromeControllerBase(object):
 
   Defines common operations but should not be created directly.
   """
+  METADATA_GATHERER = ChromeControllerMetadataGatherer()
   DEVTOOLS_CONNECTION_ATTEMPTS = 10
   DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
 
@@ -240,8 +263,7 @@ class ChromeControllerBase(object):
     else:
       self._metadata['network_emulation'] = \
           {k: 'disabled' for k in ['name', 'download', 'upload', 'latency']}
-    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
-                          seconds_since_epoch=time.time())
+    self._metadata.update(self.METADATA_GATHERER.GetMetadata())
     logging.info('Devtools connection success')
 
   def _GetChromeArguments(self):
@@ -268,6 +290,10 @@ class RemoteChromeController(ChromeControllerBase):
     super(RemoteChromeController, self).__init__()
     self._device = device
     self._device.EnableRoot()
+    self._metadata['platform'] = {
+        'os': 'A-' + device.build_id,
+        'product_model': device.product_model
+    }
 
   def GetDevice(self):
     """Overridden android device."""
@@ -363,6 +389,10 @@ class LocalChromeController(ChromeControllerBase):
     if self._using_temp_profile_dir:
       self._profile_dir = tempfile.mkdtemp(suffix='.profile')
     self._headless = False
+    self._metadata['platform'] = {
+        'os': platform.system()[0] + '-' + platform.release(),
+        'product_model': 'unknown'
+    }
 
   def __del__(self):
     if self._using_temp_profile_dir:
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index b472ce6..6fbf1c8 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -35,6 +35,8 @@ import tracing
 CSV_FIELD_NAMES = [
     'repeat_id',
     'url',
+    'chromium_commit',
+    'platform',
     'subresource_discoverer',
     'subresource_count',
     # The amount of subresources detected at SetupBenchmark step.
@@ -290,7 +292,12 @@ def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
   trace_path = os.path.join(run_directory_path, 'trace.json')
   logging.info('processing trace \'%s\'' % trace_path)
   loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
-  run_metrics = {'url': loading_trace.url}
+  run_metrics = {
+      'url': loading_trace.url,
+      'chromium_commit': loading_trace.metadata['chromium_commit'],
+      'platform': (loading_trace.metadata['platform']['os'] + '-' +
+          loading_trace.metadata['platform']['product_model'])
+  }
   run_metrics.update(_ExtractDefaultMetrics(loading_trace))
   run_metrics.update(_ExtractMemoryMetrics(loading_trace))
   run_metrics.update(

commit d7f2a4009b18b4c2553bf169e51f43bd7797e31b
Author: gab <gab@chromium.org>
Date:   Wed May 11 12:40:11 2016 -0700

    Fix include path for moved thread_task_runner_handle.h header in tools/
    
    Changes made by tools/git/move_source_file.py
    
    BUG=610438
    TBR=scottmg
    
    Review-Url: https://codereview.chromium.org/1969863002
    Cr-Original-Commit-Position: refs/heads/master@{#393030}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d3d8663862fb3e5d7f2b75e135c5dd39a96722de

diff --git a/forwarder2/device_controller.cc b/forwarder2/device_controller.cc
index a3c5505..7236baf 100644
--- a/forwarder2/device_controller.cc
+++ b/forwarder2/device_controller.cc
@@ -11,7 +11,7 @@
 #include "base/callback_helpers.h"
 #include "base/logging.h"
 #include "base/single_thread_task_runner.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/device_listener.h"
 #include "tools/android/forwarder2/socket.h"
diff --git a/forwarder2/device_listener.cc b/forwarder2/device_listener.cc
index 98459f0..31f9687 100644
--- a/forwarder2/device_listener.cc
+++ b/forwarder2/device_listener.cc
@@ -12,7 +12,7 @@
 #include "base/callback.h"
 #include "base/logging.h"
 #include "base/single_thread_task_runner.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/forwarder.h"
 #include "tools/android/forwarder2/socket.h"
diff --git a/forwarder2/host_controller.cc b/forwarder2/host_controller.cc
index 0510890..25b253a 100644
--- a/forwarder2/host_controller.cc
+++ b/forwarder2/host_controller.cc
@@ -11,7 +11,7 @@
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/logging.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/forwarder.h"
 #include "tools/android/forwarder2/socket.h"
diff --git a/forwarder2/self_deleter_helper.h b/forwarder2/self_deleter_helper.h
index 089df03..582f87b 100644
--- a/forwarder2/self_deleter_helper.h
+++ b/forwarder2/self_deleter_helper.h
@@ -15,7 +15,7 @@
 #include "base/memory/ptr_util.h"
 #include "base/memory/ref_counted.h"
 #include "base/memory/weak_ptr.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 
 namespace base {
 

commit b5e0c67f40b3ed9bf4e02927c40748ebefcca592
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 11 09:17:52 2016 -0700

    sandwich: Ignore XmlHttpRequest that have received a response after trace recording
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1972493003
    Cr-Original-Commit-Position: refs/heads/master@{#392944}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 73b8dd6d253cc7ccee8eaa5b4a65324dd81ebc6f

diff --git a/loading/request_track.py b/loading/request_track.py
index 8248af1..5da2abe 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -296,6 +296,9 @@ class Request(object):
   def IsDataRequest(self):
     return self.protocol == 'data'
 
+  def HasReceivedResponse(self):
+    return self.status is not None
+
   def GetCacheControlDirective(self, directive_name):
     """Returns the value of a Cache-Control directive, or None."""
     cache_control_str = self.GetHTTPResponseHeader('Cache-Control')
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 3b44e05..bbd2e72 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -87,8 +87,15 @@ def PatchWpr(wpr_archive_path):
   wpr_archive.Persist()
 
 
-def _FilterOutDataRequests(requests):
+def _FilterOutDataAndIncompleteRequests(requests):
   for request in filter(lambda r: not r.IsDataRequest(), requests):
+    # The protocol is only known once the response has been received. But the
+    # trace recording might have been stopped with still some JavaScript
+    # originated requests that have not received any responses yet.
+    if request.protocol is None:
+      assert not request.HasReceivedResponse()
+      assert request.initiator['type'] == 'script'
+      continue
     if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
       raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
     yield request
@@ -133,10 +140,9 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   else:
     assert False
 
-  # Prune out data:// requests.
   whitelisted_urls = set()
   logging.info('white-listing %s' % first_resource_request.url)
-  for request in _FilterOutDataRequests(discovered_requests):
+  for request in _FilterOutDataAndIncompleteRequests(discovered_requests):
     logging.info('white-listing %s' % request.url)
     whitelisted_urls.add(request.url)
   return whitelisted_urls
@@ -179,7 +185,8 @@ def ListUrlRequests(trace, request_kind):
     set([str])
   """
   urls = set()
-  for request_event in _FilterOutDataRequests(trace.request_track.GetEvents()):
+  for request_event in _FilterOutDataAndIncompleteRequests(
+      trace.request_track.GetEvents()):
     if (request_kind == RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
@@ -297,7 +304,7 @@ def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
       continue
     logging.info('lists resources of %s from %s' % (trace.url, trace_path))
     urls_set = set()
-    for request_event in _FilterOutDataRequests(
+    for request_event in _FilterOutDataAndIncompleteRequests(
         trace.request_track.GetEvents()):
       if request_event.url not in urls_set:
         logging.info('  %s' % request_event.url)

commit e392b111be64f92f664f035eb7a2c75498c887a9
Author: lizeb <lizeb@chromium.org>
Date:   Tue May 10 11:23:37 2016 -0700

    clovis: Skeleton loading report.
    
    Review-Url: https://codereview.chromium.org/1946223002
    Cr-Original-Commit-Position: refs/heads/master@{#392655}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 44fe62959cffedd9fc96b1573d84f61a64bdb5aa

diff --git a/loading/report.py b/loading/report.py
new file mode 100644
index 0000000..3918723
--- /dev/null
+++ b/loading/report.py
@@ -0,0 +1,58 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Generates a loading report.
+
+When executed as a script, takes a trace filename and print the report.
+"""
+
+import loading_trace
+from user_satisfied_lens import (
+    FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
+
+
+class LoadingReport(object):
+  """Generates a loading report from a loading trace."""
+  def __init__(self, trace):
+    """Constructor.
+
+    Args:
+      trace: (LoadingTrace) a loading trace.
+    """
+    self.trace = trace
+    self._text_msec = FirstTextPaintLens(self.trace).SatisfiedMs()
+    self._contentful_paint_msec = (
+        FirstContentfulPaintLens(self.trace).SatisfiedMs())
+    self._significant_paint_msec = (
+        FirstSignificantPaintLens(self.trace).SatisfiedMs())
+    self._base_msec = min(
+        r.start_msec for r in self.trace.request_track.GetEvents())
+    # TODO(lizeb): This is not PLT. Should correlate with
+    # RenderFrameImpl::didStopLoading.
+    self._max_msec = max(
+        r.end_msec or -1 for r in self.trace.request_track.GetEvents())
+
+  def GenerateReport(self):
+    """Returns a report as a dict."""
+    return {
+        'first_text_ms': self._text_msec - self._base_msec,
+        'contentful_paint_ms': self._contentful_paint_msec - self._base_msec,
+        'significant_paint_ms': self._significant_paint_msec - self._base_msec,
+        'plt_ms': self._max_msec - self._base_msec}
+
+  @classmethod
+  def FromTraceFilename(cls, filename):
+    """Returns a LoadingReport from a trace filename."""
+    trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+    return LoadingReport(trace)
+
+
+if __name__ == '__main__':
+  import sys
+  import json
+
+  trace_filename = sys.argv[1]
+  print json.dumps(
+      LoadingReport.FromTraceFilename(trace_filename).GenerateReport(),
+      indent=2)
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
new file mode 100644
index 0000000..3475681
--- /dev/null
+++ b/loading/report_unittest.py
@@ -0,0 +1,62 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import report
+import test_utils
+import user_satisfied_lens_unittest
+
+
+class LoadingReportTestCase(unittest.TestCase):
+  MILLI_TO_MICRO = 1000
+  _FIRST_REQUEST_TIME = 15
+  _CONTENTFUL_PAINT = 120
+  _TEXT_PAINT = 30
+  _SIGNIFICANT_PAINT = 50
+  _DURATION = 400
+  _REQUEST_OFFSET = 5
+
+  @classmethod
+  def _MakeTrace(cls):
+    trace_creator = user_satisfied_lens_unittest.TraceCreator()
+    requests = [trace_creator.RequestAt(cls._FIRST_REQUEST_TIME),
+                trace_creator.RequestAt(
+                    cls._FIRST_REQUEST_TIME + cls._REQUEST_OFFSET,
+                    cls._DURATION)]
+    trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': cls._CONTENTFUL_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': 1}},
+         {'ts': cls._TEXT_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstPaint',
+          'args': {'frame': 1}},
+         {'ts': 90 * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': cls._SIGNIFICANT_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 10
+          }}}], 1)
+    return trace
+
+  def testGenerateReport(self):
+    trace = self._MakeTrace()
+    loading_report = report.LoadingReport(trace).GenerateReport()
+    self.assertEqual(self._TEXT_PAINT - self._FIRST_REQUEST_TIME,
+                     loading_report['first_text_ms'])
+    self.assertEqual(self._SIGNIFICANT_PAINT - self._FIRST_REQUEST_TIME,
+                     loading_report['significant_paint_ms'])
+    self.assertEqual(self._CONTENTFUL_PAINT - self._FIRST_REQUEST_TIME,
+                     loading_report['contentful_paint_ms'])
+    self.assertEqual(self._REQUEST_OFFSET + self._DURATION,
+                     loading_report['plt_ms'])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 523a240..14fdfc3 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -56,6 +56,14 @@ class _UserSatisfiedLens(object):
     """
     return 0
 
+  def SatisfiedMs(self):
+    """Returns user satisfied timestamp, in ms.
+
+    This is *not* a unix timestamp. It is relative to the same point in time
+    as the request_time field in request_track.Timing.
+    """
+    return self._satisfied_msec
+
 
 class RequestFingerprintLens(_UserSatisfiedLens):
   """A lens built using requests in a trace that match a set of fingerprints."""
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index c603bbe..5bc4715 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -8,16 +8,12 @@ import request_track
 import test_utils
 import user_satisfied_lens
 
-class UserSatisfiedLensTestCase(unittest.TestCase):
-  # We track all times in milliseconds, but raw trace events are in
-  # microseconds.
-  MILLI_TO_MICRO = 1000
 
-  def setUp(self):
-    super(UserSatisfiedLensTestCase, self).setUp()
+class TraceCreator(object):
+  def __init__(self):
     self._request_index = 1
 
-  def _RequestAt(self, timestamp_msec, duration=1):
+  def RequestAt(self, timestamp_msec, duration=1):
     timestamp_sec = float(timestamp_msec) / 1000
     rq = request_track.Request.FromJsonDict({
         'url': 'http://bla-%s-.com' % timestamp_msec,
@@ -31,38 +27,58 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     self._request_index += 1
     return rq
 
+  def CreateTrace(self, requests, events, main_frame_id):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        requests, trace_events=events)
+    loading_trace.tracing_track.SetMainFrameID(main_frame_id)
+    return loading_trace
+
+
+class UserSatisfiedLensTestCase(unittest.TestCase):
+  # We track all times in milliseconds, but raw trace events are in
+  # microseconds.
+  MILLI_TO_MICRO = 1000
+
+  def setUp(self):
+    super(UserSatisfiedLensTestCase, self).setUp()
+
   def testFirstContentfulPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink.some_other_user_timing',
-                       'name': 'firstContentfulPaint'},
-                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstDiscontentPaint'},
-                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': SUBFRAME} },
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': MAINFRAME}}])
-    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink.some_other_user_timing',
+          'name': 'firstContentfulPaint'},
+         {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstDiscontentPaint'},
+         {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': SUBFRAME} },
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testCantGetNoSatisfaction(self):
     MAINFRAME = 1
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'not_my_cat',
-                       'name': 'someEvent',
-                       'args': {'frame': MAINFRAME}}])
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'not_my_cat',
+          'name': 'someEvent',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequestIds())
@@ -71,60 +87,65 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testFirstTextPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink.some_other_user_timing',
-                       'name': 'firstPaint'},
-                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstishPaint',
-                       'args': {'frame': MAINFRAME}},
-                      {'ts': 3 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstPaint',
-                       'args': {'frame': SUBFRAME}},
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstPaint',
-                       'args': {'frame': MAINFRAME}}])
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink.some_other_user_timing',
+          'name': 'firstPaint'},
+         {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstishPaint',
+          'args': {'frame': MAINFRAME}},
+         {'ts': 3 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstPaint',
+          'args': {'frame': SUBFRAME}},
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstPaint',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstTextPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testFirstSignificantPaintLens(self):
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10),
-         self._RequestAt(15), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink',
-                       'name': 'firstPaint'},
-                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'FrameView::synchronizedPaint'},
-                      {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink',
-                       'name': 'FrameView::synchronizedPaint'},
-                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink',
-                       'name': 'FrameView::synchronizedPaint'},
-
-                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'foobar', 'name': 'biz',
-                       'args': {'counters': {
-                           'LayoutObjectsThatHadNeverHadLayout': 10
-                       } } },
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'foobar', 'name': 'biz',
-                       'args': {'counters': {
-                           'LayoutObjectsThatHadNeverHadLayout': 12
-                       } } },
-                      {'ts': 15 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'foobar', 'name': 'biz',
-                       'args': {'counters': {
-                           'LayoutObjectsThatHadNeverHadLayout': 10
-                       } } } ])
+    MAINFRAME = 1
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(15), trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'firstPaint'},
+         {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 10
+          } } },
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 12
+          } } },
+         {'ts': 15 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 10
+          } } } ], MAINFRAME)
     lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(7, lens.PostloadTimeMsec())
@@ -132,23 +153,25 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testRequestFingerprintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink.some_other_user_timing',
-                       'name': 'firstContentfulPaint'},
-                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstDiscontentPaint'},
-                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': SUBFRAME} },
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': MAINFRAME}}])
-    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink.some_other_user_timing',
+          'name': 'firstContentfulPaint'},
+         {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstDiscontentPaint'},
+         {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': SUBFRAME} },
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())

commit 3adba710bd4834761e19619f0bd0f80c5932f2d8
Author: droger <droger@chromium.org>
Date:   Tue May 10 08:36:33 2016 -0700

    tools/android/loading Add a dirty bit to the failure database
    
    This is a code simplification, which will be useful when
    adding more actions to the backend.
    
    This is only refactoring, no significant change in behavior.
    
    Review-Url: https://codereview.chromium.org/1964873002
    Cr-Original-Commit-Position: refs/heads/master@{#392616}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 38c3770b5a466fbb027e5b48d1672fcc5189a660

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index da5de14..00ba434 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -32,15 +32,11 @@ class ClovisTaskHandler(object):
 
     Args:
       clovis_task(ClovisTask): The task to run.
-
-    Returns:
-      boolean: True in case of success, False if a failure has been added to the
-               failure database.
     """
     handler = self._handlers.get(clovis_task.Action())
     if not handler:
       self._logger.error('Unsupported task action: %s' % clovis_task.Action())
       self._failure_database.AddFailure('unsupported_action',
                                         clovis_task.Action())
-      return False
-    return handler.Run(clovis_task)
+      return
+    handler.Run(clovis_task)
diff --git a/loading/cloud/backend/failure_database.py b/loading/cloud/backend/failure_database.py
index ac134af..a044f04 100644
--- a/loading/cloud/backend/failure_database.py
+++ b/loading/cloud/backend/failure_database.py
@@ -11,6 +11,7 @@ class FailureDatabase(object):
 
   def __init__(self, json_string=None):
     """Loads a FailureDatabase from a string returned by ToJsonString()."""
+    self.is_dirty = False
     if json_string:
       self._failures_dict = json.loads(json_string)
     else:
@@ -27,12 +28,14 @@ class FailureDatabase(object):
   def AddFailure(self, failure_name, failure_content=None):
     """Adds a failure with the given name and content. If the failure already
     exists, it will increment the associated count.
+    Sets the 'is_dirty' bit to True.
 
     Args:
       failure_name (str): name of the failure.
       failure_content (str): content of the failure (e.g. the URL or task that
                              is failing).
     """
+    self.is_dirty = True
     content = failure_content if failure_content else 'error_count'
     if failure_name not in self._failures_dict:
       self._failures_dict[failure_name] = {}
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index a3acd33..25aff4f 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -146,10 +146,11 @@ class TraceTaskHandler(object):
           remote_trace_location)
       self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
     else:
+      url = trace_metadata['url']
+      self._logger.warning('Trace generation failed for URL: %s' % url)
       failures_dir = os.path.join(self._base_path, 'failures')
       remote_trace_location = os.path.join(failures_dir, remote_filename)
-      self._failure_database.AddFailure('trace_collection',
-                                        trace_metadata['url'])
+      self._failure_database.AddFailure('trace_collection', url)
 
     if os.path.isfile(local_filename):
       self._logger.debug('Uploading: %s' % remote_trace_location)
@@ -167,16 +168,12 @@ class TraceTaskHandler(object):
 
     Args:
       clovis_task(ClovisTask): The task to run.
-
-    Returns:
-      boolean: True in case of success, False if a failure has been added to the
-               failure database.
     """
     if clovis_task.Action() != 'trace':
       self._logger.error('Unsupported task action: %s' % clovis_task.Action())
       self._failure_database.AddFailure(FailureDatabase.CRITICAL_ERROR,
                                         'trace_task_handler_run')
-      return False
+      return
 
     # Extract the task parameters.
     params = clovis_task.ActionParams()
@@ -189,7 +186,6 @@ class TraceTaskHandler(object):
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
 
-    failure_happened = False
     success_happened = False
 
     while len(urls) > 0:
@@ -201,9 +197,6 @@ class TraceTaskHandler(object):
             url, emulate_device, emulate_network, local_filename, log_filename)
         if trace_metadata['succeeded']:
           success_happened = True
-        else:
-          self._logger.warning('Trace generation failed for URL: %s' % url)
-          failure_happened = True
         remote_filename = os.path.join(local_filename, str(repeat))
         self._HandleTraceGenerationResults(
             local_filename, log_filename, remote_filename, trace_metadata)
@@ -211,5 +204,3 @@ class TraceTaskHandler(object):
     if success_happened:
       self._UploadTraceDatabase()
 
-    return not failure_happened
-
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index b1f6d7c..e5777c8 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -74,8 +74,7 @@ class Worker(object):
         self._google_storage_accessor, config['binaries_path'], self._logger,
         self._instance_name)
 
-    if self._failure_database.ToJsonDict():
-      self._UploadFailureDatabase()
+    self._UploadFailureDatabase()
 
   def Start(self):
     """Main worker loop.
@@ -98,8 +97,8 @@ class Worker(object):
         break
 
       self._logger.info('Processing task %s' % task_id)
-      if not self._clovis_task_handler.Run(clovis_task):
-        self._UploadFailureDatabase()
+      self._clovis_task_handler.Run(clovis_task)
+      self._UploadFailureDatabase()
       self._logger.debug('Deleting task %s' % task_id)
       task_api.tasks().delete(project=project, taskqueue=queue_name,
                               task=task_id).execute()
@@ -115,10 +114,13 @@ class Worker(object):
 
   def _UploadFailureDatabase(self):
     """Uploads the failure database to CloudStorage."""
+    if not self._failure_database.is_dirty:
+      return
     self._logger.info('Uploading failure database')
     self._google_storage_accessor.UploadString(
         self._failure_database.ToJsonString(),
         self._failure_database_path)
+    self._failure_database.is_dirty = False
 
   def _FetchClovisTask(self, project_name, task_api, queue_name):
     """Fetches a ClovisTask from the task queue.

commit 2592374a9346a71558f3a4c7cd93b03230c54bdb
Author: droger <droger@chromium.org>
Date:   Tue May 10 07:42:08 2016 -0700

    tools/android/loading Abstract the processing of a ClovisTask
    
    To prepare for the support of multiple type of tasks, this CL abstracts
    the handling of a task.
    
    Two classes are introduced:
    - TraceTaskHandler: Handles a 'trace' task
    - ClovisTaskHandler: Owns the handlers for the individual task types.
                         For now only owns a TraceTaskHandler, but others
                         can be added to handle more actions.
    
    A lot of code is moved from worker.py to trace_task_handler.py.
    
    Review-Url: https://codereview.chromium.org/1945303002
    Cr-Original-Commit-Position: refs/heads/master@{#392601}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f0bdd43b3825dcb275bce24b52216fb4821f1f74

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
new file mode 100644
index 0000000..da5de14
--- /dev/null
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -0,0 +1,46 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+
+from cloud.common.clovis_task import ClovisTask
+from failure_database import FailureDatabase
+from trace_task_handler import TraceTaskHandler
+
+
+class ClovisTaskHandler(object):
+  """Handles all the supported clovis tasks."""
+
+  def __init__(self, base_path, failure_database, google_storage_accessor,
+               binaries_path, logger, instance_name=None):
+    """Creates a ClovisTaskHandler.
+
+    Args:
+      base_path(str): Base path where results are written.
+      binaries_path(str): Path to the directory where Chrome executables are.
+      instance_name(str, optional): Name of the ComputeEngine instance.
+    """
+    self._failure_database = failure_database
+    self._handlers = {
+        'trace': TraceTaskHandler(
+            os.path.join(base_path, 'trace'), failure_database,
+            google_storage_accessor, binaries_path, logger, instance_name)}
+
+  def Run(self, clovis_task):
+    """Runs a clovis_task.
+
+    Args:
+      clovis_task(ClovisTask): The task to run.
+
+    Returns:
+      boolean: True in case of success, False if a failure has been added to the
+               failure database.
+    """
+    handler = self._handlers.get(clovis_task.Action())
+    if not handler:
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      self._failure_database.AddFailure('unsupported_action',
+                                        clovis_task.Action())
+      return False
+    return handler.Run(clovis_task)
diff --git a/loading/cloud/backend/failure_database.py b/loading/cloud/backend/failure_database.py
index 4fdfde7..ac134af 100644
--- a/loading/cloud/backend/failure_database.py
+++ b/loading/cloud/backend/failure_database.py
@@ -6,6 +6,8 @@ import json
 
 class FailureDatabase(object):
   """Logs the failures happening in the Clovis backend."""
+  DIRTY_STATE_ERROR = 'startup_with_dirty_state'
+  CRITICAL_ERROR = 'critical_error'
 
   def __init__(self, json_string=None):
     """Loads a FailureDatabase from a string returned by ToJsonString()."""
diff --git a/loading/cloud/backend/google_storage_accessor.py b/loading/cloud/backend/google_storage_accessor.py
index ded3fe8..c95d742 100644
--- a/loading/cloud/backend/google_storage_accessor.py
+++ b/loading/cloud/backend/google_storage_accessor.py
@@ -18,13 +18,17 @@ class GoogleStorageAccessor(object):
     self._bucket_name = bucket_name
 
   def _GetStorageClient(self):
-    """Returns the storage client associated with the project"""
+    """Returns the storage client associated with the project."""
     return gcloud.storage.Client(project = self._project_name,
                                  credentials = self._credentials)
 
   def _GetStorageBucket(self, storage_client):
     return storage_client.get_bucket(self._bucket_name)
 
+  def BucketName(self):
+    """Returns the name of the bucket associated with this instance."""
+    return self._bucket_name
+
   def DownloadAsString(self, remote_filename):
     """Returns the content of a remote file as a string, or None if the file
     does not exist.
@@ -40,11 +44,11 @@ class GoogleStorageAccessor(object):
       return None
 
   def UploadFile(self, filename_src, filename_dest):
-    """Uploads a file to Google Cloud Storage
+    """Uploads a file to Google Cloud Storage.
 
     Args:
-      filename_src: name of the local file
-      filename_dest: name of the file in Google Cloud Storage
+      filename_src: name of the local file.
+      filename_dest: name of the file in Google Cloud Storage.
 
     Returns:
       The URL of the file in Google Cloud Storage.
@@ -57,11 +61,11 @@ class GoogleStorageAccessor(object):
     return blob.public_url
 
   def UploadString(self, data_string, filename_dest):
-    """Uploads a string to Google Cloud Storage
+    """Uploads a string to Google Cloud Storage.
 
     Args:
-      data_string: the contents of the file to be uploaded
-      filename_dest: name of the file in Google Cloud Storage
+      data_string: the contents of the file to be uploaded.
+      filename_dest: name of the file in Google Cloud Storage.
 
     Returns:
       The URL of the file in Google Cloud Storage.
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
new file mode 100644
index 0000000..a3acd33
--- /dev/null
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -0,0 +1,215 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import os
+import re
+import sys
+
+from cloud.common.clovis_task import ClovisTask
+import controller
+from failure_database import FailureDatabase
+import loading_trace
+from loading_trace_database import LoadingTraceDatabase
+import options
+
+
+class TraceTaskHandler(object):
+  """Handles 'trace' tasks."""
+
+  def __init__(self, base_path, failure_database,
+               google_storage_accessor, binaries_path, logger,
+               instance_name=None):
+    """Args:
+      base_path(str): Base path where results are written.
+      binaries_path(str): Path to the directory where Chrome executables are.
+      instance_name(str, optional): Name of the ComputeEngine instance.
+    """
+    self._failure_database = failure_database
+    self._logger = logger
+    self._google_storage_accessor = google_storage_accessor
+    self._base_path = base_path
+    if instance_name:
+      trace_database_filename = 'trace_database_%s.json' % instance_name
+    else:
+      trace_database_filename = 'trace_database.json'
+    self._trace_database_path = os.path.join(base_path, trace_database_filename)
+
+    # Recover any existing traces in case the worker died.
+    self._DownloadTraceDatabase()
+    if self._trace_database.ToJsonDict():
+      # Script is restarting after a crash, or there are already files from a
+      # previous run in the directory.
+      self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
+                                        'trace_database')
+
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs(['--local_build_dir', binaries_path])
+
+  def _DownloadTraceDatabase(self):
+    """Downloads the trace database from CloudStorage."""
+    self._logger.info('Downloading trace database')
+    trace_database_string = self._google_storage_accessor.DownloadAsString(
+        self._trace_database_path) or '{}'
+    self._trace_database = LoadingTraceDatabase.FromJsonString(
+        trace_database_string)
+
+  def _UploadTraceDatabase(self):
+    """Uploads the trace database to CloudStorage."""
+    self._logger.info('Uploading trace database')
+    self._google_storage_accessor.UploadString(
+        self._trace_database.ToJsonString(),
+        self._trace_database_path)
+
+  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
+                     log_filename):
+    """ Generates a trace.
+
+    Args:
+      url: URL as a string.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
+      filename: Name of the file where the trace is saved.
+      log_filename: Name of the file where standard output and errors are
+                    logged.
+
+    Returns:
+      A dictionary of metadata about the trace, including a 'succeeded' field
+      indicating whether the trace was successfully generated.
+    """
+    try:
+      os.remove(filename)  # Remove any existing trace for this URL.
+    except OSError:
+      pass  # Nothing to remove.
+
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
+
+    old_stdout = sys.stdout
+    old_stderr = sys.stderr
+
+    trace_metadata = { 'succeeded' : False, 'url' : url }
+    trace = None
+    with open(log_filename, 'w') as sys.stdout:
+      try:
+        sys.stderr = sys.stdout
+
+        # Set up the controller.
+        chrome_ctl = controller.LocalChromeController()
+        chrome_ctl.SetHeadless(True)
+        if emulate_device:
+          chrome_ctl.SetDeviceEmulation(emulate_device)
+        if emulate_network:
+          chrome_ctl.SetNetworkEmulation(emulate_network)
+
+        # Record and write the trace.
+        with chrome_ctl.Open() as connection:
+          connection.ClearCache()
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url, connection, chrome_ctl.ChromeMetadata())
+          trace_metadata['succeeded'] = True
+          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+      except controller.ChromeControllerError as e:
+        e.Dump(sys.stderr)
+      except Exception as e:
+        sys.stderr.write(str(e))
+
+      if trace:
+        with open(filename, 'w') as f:
+          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+
+    sys.stdout = old_stdout
+    sys.stderr = old_stderr
+
+    return trace_metadata
+
+  def _HandleTraceGenerationResults(self, local_filename, log_filename,
+                                    remote_filename, trace_metadata):
+    """Updates the trace database and the failure database after a trace
+    generation. Uploads the trace and the log.
+    Results related to successful traces are uploaded in the 'traces' directory,
+    and failures are uploaded in the 'failures' directory.
+
+    Args:
+      local_filename (str): Path to the local file containing the trace.
+      log_filename (str): Path to the local file containing the log.
+      remote_filename (str): Name of the target remote file where the trace and
+                             the log (with a .log extension added) are uploaded.
+      trace_metadata (dict): Metadata associated with the trace generation.
+    """
+    if trace_metadata['succeeded']:
+      traces_dir = os.path.join(self._base_path, 'traces')
+      remote_trace_location = os.path.join(traces_dir, remote_filename)
+      full_cloud_storage_path = os.path.join(
+          'gs://' + self._google_storage_accessor.BucketName(),
+          remote_trace_location)
+      self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
+    else:
+      failures_dir = os.path.join(self._base_path, 'failures')
+      remote_trace_location = os.path.join(failures_dir, remote_filename)
+      self._failure_database.AddFailure('trace_collection',
+                                        trace_metadata['url'])
+
+    if os.path.isfile(local_filename):
+      self._logger.debug('Uploading: %s' % remote_trace_location)
+      self._google_storage_accessor.UploadFile(local_filename,
+                                               remote_trace_location)
+    else:
+      self._logger.warning('No trace found at: ' + local_filename)
+
+    self._logger.debug('Uploading analyze log')
+    remote_log_location = remote_trace_location + '.log'
+    self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
+
+  def Run(self, clovis_task):
+    """Runs a 'trace' clovis_task.
+
+    Args:
+      clovis_task(ClovisTask): The task to run.
+
+    Returns:
+      boolean: True in case of success, False if a failure has been added to the
+               failure database.
+    """
+    if clovis_task.Action() != 'trace':
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      self._failure_database.AddFailure(FailureDatabase.CRITICAL_ERROR,
+                                        'trace_task_handler_run')
+      return False
+
+    # Extract the task parameters.
+    params = clovis_task.ActionParams()
+    urls = params['urls']
+    repeat_count = params.get('repeat_count', 1)
+    emulate_device = params.get('emulate_device')
+    emulate_network = params.get('emulate_network')
+
+    log_filename = 'analyze.log'
+    # Avoid special characters in storage object names
+    pattern = re.compile(r"[#\?\[\]\*/]")
+
+    failure_happened = False
+    success_happened = False
+
+    while len(urls) > 0:
+      url = urls.pop()
+      local_filename = pattern.sub('_', url)
+      for repeat in range(repeat_count):
+        self._logger.debug('Generating trace for URL: %s' % url)
+        trace_metadata = self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename)
+        if trace_metadata['succeeded']:
+          success_happened = True
+        else:
+          self._logger.warning('Trace generation failed for URL: %s' % url)
+          failure_happened = True
+        remote_filename = os.path.join(local_filename, str(repeat))
+        self._HandleTraceGenerationResults(
+            local_filename, log_filename, remote_filename, trace_metadata)
+
+    if success_happened:
+      self._UploadTraceDatabase()
+
+    return not failure_happened
+
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 05e8498..b1f6d7c 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -6,7 +6,6 @@ import argparse
 import json
 import logging
 import os
-import re
 import sys
 
 from googleapiclient import discovery
@@ -18,14 +17,11 @@ from oauth2client.client import GoogleCredentials
 sys.path.insert(0,
     os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir,
                  os.pardir))
-import controller
 from cloud.common.clovis_task import ClovisTask
 from cloud.common.google_instance_helper import GoogleInstanceHelper
+from clovis_task_handler import ClovisTaskHandler
 from failure_database import FailureDatabase
 from google_storage_accessor import GoogleStorageAccessor
-import loading_trace
-from loading_trace_database import LoadingTraceDatabase
-import options
 
 
 class Worker(object):
@@ -57,31 +53,29 @@ class Worker(object):
         bucket_name=self._bucket_name)
 
     if self._instance_name:
-      trace_database_filename = 'trace_database_%s.json' % self._instance_name
       failure_database_filename = \
           'failure_database_%s.json' % self._instance_name
     else:
-      trace_database_filename = 'trace_database.json'
       failure_database_filename = 'failure_dabatase.json'
-    self._traces_dir = os.path.join(self._base_path_in_bucket, 'traces')
-    self._trace_database_path = os.path.join(self._traces_dir,
-                                             trace_database_filename)
-    self._failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
-    self._failure_database_path = os.path.join(self._failures_dir,
+    self._failure_database_path = os.path.join(self._base_path_in_bucket,
                                                failure_database_filename)
 
-    # Recover any existing trace database and failures in case the worker died.
-    self._DownloadTraceDatabase()
+    # Recover any existing failures in case the worker died.
     self._DownloadFailureDatabase()
 
-    if self._trace_database.ToJsonDict() or self._failure_database.ToJsonDict():
+    if self._failure_database.ToJsonDict():
       # Script is restarting after a crash, or there are already files from a
       # previous run in the directory.
-      self._failure_database.AddFailure('startup_with_dirty_state')
-      self._UploadFailureDatabase()
+      self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
+                                        'failure_database')
+
+    self._clovis_task_handler = ClovisTaskHandler(
+        self._base_path_in_bucket, self._failure_database,
+        self._google_storage_accessor, config['binaries_path'], self._logger,
+        self._instance_name)
 
-    # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs(['--local_build_dir', config['binaries_path']])
+    if self._failure_database.ToJsonDict():
+      self._UploadFailureDatabase()
 
   def Start(self):
     """Main worker loop.
@@ -104,28 +98,14 @@ class Worker(object):
         break
 
       self._logger.info('Processing task %s' % task_id)
-      self._ProcessClovisTask(clovis_task)
+      if not self._clovis_task_handler.Run(clovis_task):
+        self._UploadFailureDatabase()
       self._logger.debug('Deleting task %s' % task_id)
       task_api.tasks().delete(project=project, taskqueue=queue_name,
                               task=task_id).execute()
       self._logger.info('Finished task %s' % task_id)
     self._Finalize()
 
-  def _DownloadTraceDatabase(self):
-    """Downloads the trace database from CloudStorage."""
-    self._logger.info('Downloading trace database')
-    trace_database_string = self._google_storage_accessor.DownloadAsString(
-        self._trace_database_path) or '{}'
-    self._trace_database = LoadingTraceDatabase.FromJsonString(
-        trace_database_string)
-
-  def _UploadTraceDatabase(self):
-    """Uploads the trace database to CloudStorage."""
-    self._logger.info('Uploading trace database')
-    self._google_storage_accessor.UploadString(
-        self._trace_database.ToJsonString(),
-        self._trace_database_path)
-
   def _DownloadFailureDatabase(self):
     """Downloads the failure database from CloudStorage."""
     self._logger.info('Downloading failure database')
@@ -206,144 +186,6 @@ class Worker(object):
     # Do not add anything after this line, as the instance might be killed at
     # any time.
 
-  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
-                     log_filename):
-    """ Generates a trace.
-
-    Args:
-      url: URL as a string.
-      emulate_device: Name of the device to emulate. Empty for no emulation.
-      emulate_network: Type of network emulation. Empty for no emulation.
-      filename: Name of the file where the trace is saved.
-      log_filename: Name of the file where standard output and errors are
-                    logged.
-
-    Returns:
-      A dictionary of metadata about the trace, including a 'succeeded' field
-      indicating whether the trace was successfully generated.
-    """
-    try:
-      os.remove(filename)  # Remove any existing trace for this URL.
-    except OSError:
-      pass  # Nothing to remove.
-
-    if not url.startswith('http') and not url.startswith('file'):
-      url = 'http://' + url
-
-    old_stdout = sys.stdout
-    old_stderr = sys.stderr
-
-    trace_metadata = { 'succeeded' : False, 'url' : url }
-    trace = None
-    with open(log_filename, 'w') as sys.stdout:
-      try:
-        sys.stderr = sys.stdout
-
-        # Set up the controller.
-        chrome_ctl = controller.LocalChromeController()
-        chrome_ctl.SetHeadless(True)
-        if emulate_device:
-          chrome_ctl.SetDeviceEmulation(emulate_device)
-        if emulate_network:
-          chrome_ctl.SetNetworkEmulation(emulate_network)
-
-        # Record and write the trace.
-        with chrome_ctl.Open() as connection:
-          connection.ClearCache()
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url, connection, chrome_ctl.ChromeMetadata())
-          trace_metadata['succeeded'] = True
-          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
-      except controller.ChromeControllerError as e:
-        e.Dump(sys.stderr)
-      except Exception as e:
-        sys.stderr.write(str(e))
-
-      if trace:
-        with open(filename, 'w') as f:
-          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
-
-    sys.stdout = old_stdout
-    sys.stderr = old_stderr
-
-    return trace_metadata
-
-  def _HandleTraceGenerationResults(self, local_filename, log_filename,
-                                    remote_filename, trace_metadata):
-    """Updates the trace database and the failure database after a trace
-    generation. Uploads the trace and the log.
-    Results related to successful traces are uploaded in the _traces_dir
-    directory, and failures are uploaded in the _failures_dir directory.
-
-    Args:
-      local_filename (str): Path to the local file containing the trace.
-      log_filename (str): Path to the local file containing the log.
-      remote_filename (str): Name of the target remote file where the trace and
-                             the log (with a .log extension added) are uploaded.
-      trace_metadata (dict): Metadata associated with the trace generation.
-    """
-    if trace_metadata['succeeded']:
-      remote_trace_location = os.path.join(self._traces_dir, remote_filename)
-      full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
-                                             remote_trace_location)
-      self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
-    else:
-      remote_trace_location = os.path.join(self._failures_dir, remote_filename)
-      self._failure_database.AddFailure('trace_collection',
-                                        trace_metadata['url'])
-
-    if os.path.isfile(local_filename):
-      self._logger.debug('Uploading: %s' % remote_trace_location)
-      self._google_storage_accessor.UploadFile(local_filename,
-                                               remote_trace_location)
-    else:
-      self._logger.warning('No trace found at: ' + local_filename)
-
-    self._logger.debug('Uploading analyze log')
-    remote_log_location = remote_trace_location + '.log'
-    self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
-
-  def _ProcessClovisTask(self, clovis_task):
-    """Processes one clovis_task."""
-    if clovis_task.Action() != 'trace':
-      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
-      return
-
-    # Extract the task parameters.
-    params = clovis_task.ActionParams()
-    urls = params['urls']
-    repeat_count = params.get('repeat_count', 1)
-    emulate_device = params.get('emulate_device')
-    emulate_network = params.get('emulate_network')
-
-    log_filename = 'analyze.log'
-    # Avoid special characters in storage object names
-    pattern = re.compile(r"[#\?\[\]\*/]")
-
-    failure_happened = False
-    success_happened = False
-
-    while len(urls) > 0:
-      url = urls.pop()
-      local_filename = pattern.sub('_', url)
-      for repeat in range(repeat_count):
-        self._logger.debug('Generating trace for URL: %s' % url)
-        trace_metadata = self._GenerateTrace(
-            url, emulate_device, emulate_network, local_filename, log_filename)
-        if trace_metadata['succeeded']:
-          success_happened = True
-        else:
-          self._logger.warning('Trace generation failed for URL: %s' % url)
-          failure_happened = True
-        remote_filename = os.path.join(local_filename, str(repeat))
-        self._HandleTraceGenerationResults(
-            local_filename, log_filename, remote_filename, trace_metadata)
-
-    if success_happened:
-      self._UploadTraceDatabase()
-    if failure_happened:
-      self._UploadFailureDatabase()
-
 if __name__ == '__main__':
   parser = argparse.ArgumentParser(
       description='ComputeEngine Worker for Clovis')

commit c8ae7ecb67de547346cc37f75a0a579da120176a
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 9 03:07:27 2016 -0700

    sandwich: Blacklist some keywords from Vary and Pragma response header
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1937213002
    Cr-Original-Commit-Position: refs/heads/master@{#392295}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 814d734820f33ec5e5aaa05e834671394621df1c

diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 3c7c740..3b44e05 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -60,7 +60,6 @@ def PatchWpr(wpr_archive_path):
     logging.info('patching %s' % url_entry.url)
     # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
     # TODO(gabadie): may need to delete ETag.
-    # TODO(gabadie): may need to patch Vary.
     # TODO(gabadie): may need to take care of x-cache.
     #
     # Override the cache-control header to set the resources max age to MAX_AGE.
@@ -72,6 +71,19 @@ def PatchWpr(wpr_archive_path):
     # choices but save absolutely all cached resources on disk so they survive
     # after killing chrome for cache save, modification and push.
     url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
+
+    # TODO(gabadie): May need to extend Vary blacklist (referer?)
+    #
+    # All of these Vary and Pragma possibilities need to be removed from
+    # response headers in order for Chrome to store a resource in HTTP cache and
+    # not to invalidate it.
+    #
+    # Note: HttpVaryData::Init() in Chrome adds an implicit 'Vary: cookie'
+    # header to any redirect.
+    # TODO(gabadie): Find a way to work around this issue.
+    url_entry.RemoveResponseHeaderDirectives('vary', {'*', 'cookie'})
+    url_entry.RemoveResponseHeaderDirectives('pragma', {'no-cache'})
+
   wpr_archive.Persist()
 
 
diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
index a24d282..04410ad 100644
--- a/loading/wpr_backend.py
+++ b/loading/wpr_backend.py
@@ -91,6 +91,28 @@ class WprUrlEntry(object):
     self._wpr_response.headers = \
         [x for x in self._wpr_response.headers if x[0].lower() != name]
 
+  def RemoveResponseHeaderDirectives(self, name, directives_blacklist):
+    """Removed a set of directives from response headers.
+
+    Also removes the cache header in case no more directives are left.
+    It is useful, for example, to remove 'no-cache' from 'pragma: no-cache'.
+
+    Args:
+      name: The name of the response header field to modify.
+      directives_blacklist: Set of lowered directives to remove from list.
+    """
+    response_headers = self.GetResponseHeadersDict()
+    if name not in response_headers:
+      return
+    new_value = []
+    for header_name in response_headers[name].split(','):
+      if header_name.strip().lower() not in directives_blacklist:
+        new_value.append(header_name)
+    if new_value:
+      self.SetResponseHeader(name, ','.join(new_value))
+    else:
+      self.DeleteResponseHeader(name)
+
   @classmethod
   def _ExtractUrl(cls, request_string):
     match = _PARSE_WPR_REQUEST_REGEX.match(request_string)
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
index f744c10..f4c2fe6 100644
--- a/loading/wpr_backend_unittest.py
+++ b/loading/wpr_backend_unittest.py
@@ -29,7 +29,7 @@ class WprUrlEntryTest(unittest.TestCase):
     wpr_response = MockWprResponse(headers)
     return WprUrlEntry('GET http://a.com/', wpr_response)
 
-  def test_ExtractUrl(self):
+  def testExtractUrl(self):
     self.assertEquals('http://aa.bb/c',
                       WprUrlEntry._ExtractUrl('GET http://aa.bb/c'))
     self.assertEquals('http://aa.b/c',
@@ -43,7 +43,7 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('http://aa.bb',
                       WprUrlEntry._ExtractUrl('GET http://aa.bb FOO BAR'))
 
-  def test_GetResponseHeadersDict(self):
+  def testGetResponseHeadersDict(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
                                      ('header0', 'value2'),
@@ -57,7 +57,7 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('value3', headers['header2'])
     self.assertEquals('VaLue4', headers['header3'])
 
-  def test_SetResponseHeader(self):
+  def testSetResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1')])
     entry.SetResponseHeader('new_header0', 'new_value0')
@@ -112,7 +112,7 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('header3', entry._wpr_response.headers[3][0])
     self.assertEquals('value4', entry._wpr_response.headers[3][1])
 
-  def test_DeleteResponseHeader(self):
+  def testDeleteResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
                                      ('header0', 'value2'),
@@ -132,6 +132,23 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertNotIn('header1', entry.GetResponseHeadersDict())
     self.assertEquals(2, len(entry.GetResponseHeadersDict()))
 
+  def testRemoveResponseHeaderDirectives(self):
+    entry = self._CreateWprUrlEntry([('hEAder0', 'keyWOrd0,KEYword1'),
+                                     ('heaDER1', 'value1'),
+                                     ('headeR2', 'value3')])
+    entry.RemoveResponseHeaderDirectives('header0', {'keyword1', 'keyword0'})
+    self.assertNotIn('header0', entry.GetResponseHeadersDict())
+
+    entry = self._CreateWprUrlEntry([('heADEr0', 'keYWOrd0'),
+                                     ('hEADERr1', 'value1'),
+                                     ('HEAder0', 'keywoRD1,keYwoRd2'),
+                                     ('hEADer2', 'value3')])
+    entry.RemoveResponseHeaderDirectives('header0', {'keyword1'})
+    self.assertEquals(
+        'keYWOrd0,keYwoRd2', entry.GetResponseHeadersDict()['header0'])
+    self.assertEquals(3, len(entry._wpr_response.headers))
+    self.assertEquals('keYWOrd0,keYwoRd2', entry._wpr_response.headers[0][1])
+
 
 class WprHostTest(unittest.TestCase):
   def setUp(self):

commit 03702ec1f703956a053b7423092e23200bec290f
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 4 11:46:45 2016 -0700

    sandwich: Log more effectively error in VerifyBenchmarkOutputDirectory()
    
    Before, sandwich_misc.VerifyBenchmarkOutputDirectory() was just
    comparing theoretical requests sets with in practice one. However
    there was some issues. As an example, the missing requests (may be
    because the URL as changed because of some JavaScript) were getting
    logged several times: list of all requests, list of {,un}cached
    requests. When URLs of requests are getting long (because of the
    JS that have generated them for instance), it is becoming difficult
    for the human eye to see if a such URL was missing from the expected
    {,un}cache resources because was missing from the all the requests.
    
    This CL fixes this issue by modifying the expected sets of url that
    were {,not} served from cache according to the expected and missing
    requests, so that a given miss-behaving URL gets logged at most once.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1949023004
    Cr-Original-Commit-Position: refs/heads/master@{#391583}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5f05514899b1a64200bb26fd23f04ef4b25f6f8a

diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index a99f973..3c7c740 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -191,7 +191,9 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
   # TODO(gabadie): What's the best way of propagating errors happening in here?
   benchmark_setup = json.load(open(benchmark_setup_path))
   cache_whitelist = set(benchmark_setup['cache_whitelist'])
-  url_resources = set(benchmark_setup['url_resources'])
+  original_requests = set(benchmark_setup['url_resources'])
+  original_cached_requests = original_requests.intersection(cache_whitelist)
+  original_uncached_requests = original_requests.difference(cache_whitelist)
   all_sent_url_requests = set()
 
   # Verify requests from traces.
@@ -207,16 +209,29 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
       continue
     trace = LoadingTrace.FromJsonFile(trace_path)
     logging.info('verifying %s from %s' % (trace.url, trace_path))
-    _PrintUrlSetComparison(url_resources,
-        ListUrlRequests(trace, RequestOutcome.All), 'All resources')
-    _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
-        ListUrlRequests(trace, RequestOutcome.ServedFromCache),
-        'Cached resources')
-    sent_url_requests = \
+
+    effective_requests = ListUrlRequests(trace, RequestOutcome.All)
+    effective_cached_requests = \
+        ListUrlRequests(trace, RequestOutcome.ServedFromCache)
+    effective_uncached_requests = \
         ListUrlRequests(trace, RequestOutcome.NotServedFromCache)
-    _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
-        sent_url_requests, 'Non cached resources')
-    all_sent_url_requests.update(sent_url_requests)
+
+    missing_requests = original_requests.difference(effective_requests)
+    unexpected_requests = effective_requests.difference(original_requests)
+    expected_cached_requests = \
+        original_cached_requests.difference(missing_requests)
+    missing_cached_requests = \
+        expected_cached_requests.difference(effective_cached_requests)
+    expected_uncached_requests = original_uncached_requests.union(
+        unexpected_requests).union(missing_cached_requests)
+    all_sent_url_requests.update(effective_uncached_requests)
+
+    _PrintUrlSetComparison(original_requests, effective_requests,
+                           'All resources')
+    _PrintUrlSetComparison(expected_cached_requests, effective_cached_requests,
+                           'Cached resources')
+    _PrintUrlSetComparison(expected_uncached_requests,
+                           effective_uncached_requests, 'Non cached resources')
 
   # Verify requests from WPR.
   wpr_log_path = os.path.join(
@@ -241,7 +256,7 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
   _PrintUrlSetComparison(set(), wpr_command_colliding_urls,
                          'Distinct resources colliding to WPR commands')
   _PrintUrlSetComparison(all_wpr_urls, all_sent_url_requests,
-                         'Distinct resources requests to WPR')
+                         'Distinct resource requests to WPR')
 
 
 def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 318f869..1d36c78 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -119,6 +119,7 @@ class SandwichTaskBuilder(task_manager.Builder):
       runner.wpr_archive_path = BuildPatchedWpr.path
       runner.cache_archive_path = BuildReferenceCache.path
       runner.cache_operation = 'save'
+      runner.trace_output_directory = BuildReferenceCache.path[:-4] + '-run'
       runner.Run()
 
     @self.RegisterTask('common/subresources-for-urls-run/',

commit 8d70cfe624fcf89969e22de7c2a5cd5a8a9cd4b5
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 4 10:55:24 2016 -0700

    tools/android/loading: Implements ChromeControllerError class
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1941743002
    Cr-Original-Commit-Position: refs/heads/master@{#391555}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8c50c742fe8f82a73b8488034b5cbf24833c6480

diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 07faf79..05e8498 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -248,13 +248,14 @@ class Worker(object):
           chrome_ctl.SetNetworkEmulation(emulate_network)
 
         # Record and write the trace.
-        with chrome_ctl.OpenWithRedirection(sys.stdout,
-                                            sys.stderr) as connection:
+        with chrome_ctl.Open() as connection:
           connection.ClearCache()
           trace = loading_trace.LoadingTrace.RecordUrlNavigation(
               url, connection, chrome_ctl.ChromeMetadata())
           trace_metadata['succeeded'] = True
           trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+      except controller.ChromeControllerError as e:
+        e.Dump(sys.stderr)
       except Exception as e:
         sys.stderr.write(str(e))
 
diff --git a/loading/controller.py b/loading/controller.py
index 6ff1389..8b36d2b 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -21,21 +21,70 @@ import subprocess
 import sys
 import tempfile
 import time
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
+import traceback
 
 import chrome_cache
 import common_util
 import device_setup
 import devtools_monitor
 import emulation
-import options
+from options import OPTIONS
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+_CATAPULT_DIR = os.path.join(_SRC_DIR, 'third_party', 'catapult')
 
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+sys.path.append(os.path.join(_CATAPULT_DIR, 'devil'))
 from devil.android.sdk import intent
 
-OPTIONS = options.OPTIONS
+sys.path.append(
+    os.path.join(_CATAPULT_DIR, 'telemetry', 'third_party', 'websocket-client'))
+import websocket
+
+
+class ChromeControllerInternalError(Exception):
+  pass
+
+
+class ChromeControllerError(Exception):
+  """Chrome error with detailed log.
+
+  Note:
+    Some of these errors might be known intermittent errors that can usually be
+    retried by the caller after re-doing any specific setup again.
+  """
+  _INTERMITTENT_WHITE_LIST = {websocket.WebSocketTimeoutException}
+
+  def __init__(self, log):
+    """Constructor
+
+    Args:
+      log: String containing the log of the running Chrome instance that was
+          running. It will be interleaved with xvfb with headless desktop or
+          interleaved with any other running Android package.
+    """
+    self.error_type, self.error_value, self.error_traceback = sys.exc_info()
+    super(ChromeControllerError, self).__init__(repr(self.error_value))
+    self.parent_stack = traceback.extract_stack()
+    self.log = log
+
+  def Dump(self, output):
+    """Dumps the entire error's infos into file-like object."""
+    output.write('-' * 60 + ' {}:\n'.format(self.__class__.__name__))
+    output.write(repr(self) + '\n')
+    output.write('{} is {}known as intermittent.\n'.format(
+        self.error_type.__name__, '' if self.IsIntermittent() else 'NOT '))
+    output.write(
+        '-' * 60 + ' {}\'s full traceback:\n'.format(self.error_type.__name__))
+    output.write(''.join(traceback.format_list(self.parent_stack)))
+    traceback.print_tb(self.error_traceback, file=output)
+    output.write('-' * 60 + ' Begin log\n')
+    output.write(self.log)
+    output.write('-' * 60 + ' End log\n')
+
+  def IsIntermittent(self):
+    """Returns whether the error is an known intermittent error."""
+    return self.error_type in self._INTERMITTENT_WHITE_LIST
 
 
 class ChromeControllerBase(object):
@@ -67,6 +116,10 @@ class ChromeControllerBase(object):
         # Tests & dev-tools related stuff.
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
+
+        # Detailed log.
+        '--enable-logging=stderr',
+        '--v=1',
     ]
     self._wpr_attributes = None
     self._metadata = {}
@@ -240,6 +293,7 @@ class RemoteChromeController(ChromeControllerBase):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
+      self._device.adb.Logcat(clear=True, dump=True)
       self._device.StartActivity(start_intent, blocking=True)
       try:
         for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
@@ -262,8 +316,12 @@ class RemoteChromeController(ChromeControllerBase):
               time.sleep(self.TIME_TO_IDLE_SECONDS)
             break
         else:
-          raise RuntimeError('Failed to connect to chrome devtools after {} '
-                             'attempts.'.format(attempt_id))
+          raise ChromeControllerInternalError(
+              'Failed to connect to Chrome devtools after {} '
+              'attempts.'.format(self.DEVTOOLS_CONNECTION_ATTEMPTS))
+      except:
+        logcat = ''.join([l + '\n' for l in self._device.adb.Logcat(dump=True)])
+        raise ChromeControllerError(log=logcat)
       finally:
         self._device.ForceStop(package_info.package)
 
@@ -319,47 +377,50 @@ class LocalChromeController(ChromeControllerBase):
     self._headless = headless
 
   @contextlib.contextmanager
-  def OpenWithRedirection(self, stdout, stderr):
-    """Override for connection context. stdout and stderr are passed to the
-       child processes used to run Chrome and XVFB."""
+  def Open(self):
+    """Overridden connection creation."""
     chrome_cmd = [OPTIONS.LocalBinary('chrome')]
     chrome_cmd.extend(self._GetChromeArguments())
     # Force use of simple cache.
     chrome_cmd.append('--use-simple-cache-backend=on')
     chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
-    chrome_cmd.extend(['--enable-logging=stderr', '--v=1'])
     # Navigates to about:blank for couples of reasons:
     #   - To find the correct target descriptor at devtool connection;
     #   - To avoid cache and WPR pollution by the NTP.
     chrome_cmd.append('about:blank')
 
-    chrome_env_override = {}
-    if self._wpr_attributes:
-      chrome_env_override.update(self._wpr_attributes.chrome_env_override)
-
-    if self._headless:
-      assert 'DISPLAY' not in chrome_env_override, \
-          'DISPLAY environment variable is reserved for headless.'
-      chrome_env_override['DISPLAY'] = 'localhost:99'
-      xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
-      logging.info(common_util.GetCommandLineForLogging(xvfb_cmd))
-      xvfb_process = subprocess.Popen(xvfb_cmd, stdout=stdout, stderr=stderr)
-
-    # Launch chrome.
-    logging.info(
-        common_util.GetCommandLineForLogging(chrome_cmd, chrome_env_override))
-    chrome_env = os.environ.copy()
-    chrome_env.update(chrome_env_override)
-    chrome_process = subprocess.Popen(chrome_cmd, stdout=stdout, stderr=stderr,
-                                      env=chrome_env)
-    connection = None
+    tmp_log = \
+        tempfile.NamedTemporaryFile(prefix="chrome_controller_", suffix='.log')
+    chrome_process = None
     try:
+      chrome_env_override = {}
+      if self._wpr_attributes:
+        chrome_env_override.update(self._wpr_attributes.chrome_env_override)
+
+      if self._headless:
+        assert 'DISPLAY' not in chrome_env_override, \
+            'DISPLAY environment variable is reserved for headless.'
+        chrome_env_override['DISPLAY'] = 'localhost:99'
+        xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
+        logging.info(common_util.GetCommandLineForLogging(xvfb_cmd))
+        xvfb_process = \
+            subprocess.Popen(xvfb_cmd, stdout=tmp_log.file, stderr=tmp_log.file)
+
+      chrome_env = os.environ.copy()
+      chrome_env.update(chrome_env_override)
+
+      # Launch Chrome.
+      logging.info(common_util.GetCommandLineForLogging(chrome_cmd,
+                                                        chrome_env_override))
+      chrome_process = subprocess.Popen(chrome_cmd, stdout=tmp_log.file,
+                                        stderr=tmp_log.file, env=chrome_env)
       # Attempt to connect to Chrome's devtools
       for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
         logging.info('Devtools connection attempt %d' % attempt_id)
         process_result = chrome_process.poll()
         if process_result is not None:
-          raise RuntimeError('Unexpected Chrome exit: %s', process_result)
+          raise ChromeControllerInternalError(
+              'Unexpected Chrome exit: {}'.format(process_result))
         try:
           connection = devtools_monitor.DevToolsConnection(
               OPTIONS.devtools_hostname, OPTIONS.devtools_port)
@@ -369,8 +430,9 @@ class LocalChromeController(ChromeControllerBase):
             raise
           time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
       else:
-        raise RuntimeError('Failed to connect to Chrome devtools after {} '
-                           'attempts.'.format(attempt_id))
+        raise ChromeControllerInternalError(
+            'Failed to connect to Chrome devtools after {} '
+            'attempts.'.format(self.DEVTOOLS_CONNECTION_ATTEMPTS))
       # Start and yield the devtool connection.
       self._StartConnection(connection)
       yield connection
@@ -378,19 +440,17 @@ class LocalChromeController(ChromeControllerBase):
         connection.Close()
         chrome_process.wait()
         chrome_process = None
+    except:
+      raise ChromeControllerError(log=open(tmp_log.name).read())
     finally:
+      if OPTIONS.local_noisy:
+        sys.stderr.write(open(tmp_log.name).read())
+      del tmp_log
       if chrome_process:
         chrome_process.kill()
       if self._headless:
         xvfb_process.kill()
 
-  def Open(self):
-    """Wrapper around the more-specialized version of Open() above that sets
-    the value of stdout/stderr based on the value of OPTIONS.local_noisy."""
-    stdout = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-    stderr = stdout
-    return self.OpenWithRedirection(stdout, stderr)
-
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
     self._EnsureProfileDirectory()
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index ead9bce..4952d10 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -379,7 +379,9 @@ class DevToolsConnection(object):
       if target_descriptor['type'] == 'page':
         self._target_descriptor = target_descriptor
         break
-    assert self._target_descriptor['url'] == 'about:blank'
+    if self._target_descriptor['url'] != 'about:blank':
+      raise DevToolsConnectionException(
+          'Looks like devtools connection was made to a different instance.')
     self._ws = inspector_websocket.InspectorWebsocket()
     self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'],
                      timeout=_WEBSOCKET_TIMEOUT_SECONDS)

commit 110689d593f74bf296535e57b98506903ecb6f8a
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 4 08:06:43 2016 -0700

    tools/android/loading: Replace --local_binary by --local_build_dir in OPTIONS
    
    Before, we could only configures optionally the location of the
    local chrome binary with the --local_binary. But the cachetool or
    content_decoder_tool binaries path were set using an environment
    variable.
    
    This CL refactor these issue behind a replacing flag
    --local_build_dir to fix the inconsistency. One important
    difference is that this new flag doesn't have a default value to
    remove the out of the box magic that can sometimes cause issue if
    the program use a different binary from the one the command line
    expect.
    
    This CL also remove some analyze.py specific command line
    arguments from OPTIONS.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1941833002
    Cr-Original-Commit-Position: refs/heads/master@{#391510}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8ae350aa3a53e1eb62b7dddb63d66140caf46aa6

diff --git a/loading/analyze.py b/loading/analyze.py
index 82cc071..05b0a9d 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -288,6 +288,14 @@ COMMAND_MAP = {
 def main():
   logging.basicConfig(level=logging.WARNING)
   OPTIONS.AddGlobalArgument(
+      'clear_cache', True, 'clear browser cache before loading')
+  OPTIONS.AddGlobalArgument(
+      'emulate_device', '',
+      'Name of the device to emulate. Must be present '
+      'in --devices_file, or empty for no emulation.')
+  OPTIONS.AddGlobalArgument('emulate_network', '',
+      'Type of network emulation. Empty for no emulation.')
+  OPTIONS.AddGlobalArgument(
       'local', False,
       'run against local desktop chrome rather than device '
       '(see also --local_binary and local_profile_dir)')
diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 4c1f226..df8f955 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -31,17 +31,6 @@ OPTIONS = options.OPTIONS
 # Cache back-end types supported by cachetool.
 BACKEND_TYPES = ['simple']
 
-# Default build output directory.
-OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
-    os.path.dirname(__file__), '../../../out/Release'))
-
-# Default cachetool binary location.
-CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
-
-# Default content_decoder_tool binary location.
-CONTENT_DECODER_TOOL_BIN_PATH = os.path.join(OUT_DIRECTORY,
-                                             'content_decoder_tool')
-
 # Regex used to parse HTTP headers line by line.
 HEADER_PARSING_REGEX = re.compile(r'^(?P<header>\S+):(?P<value>.*)$')
 
@@ -247,22 +236,18 @@ class CacheBackend(object):
   """Takes care of reading and deleting cached keys.
   """
 
-  def __init__(self, cache_directory_path, cache_backend_type,
-               cachetool_bin_path=CACHETOOL_BIN_PATH):
+  def __init__(self, cache_directory_path, cache_backend_type):
     """Chrome cache back-end constructor.
 
     Args:
       cache_directory_path: The directory path where the cache is locally
         stored.
       cache_backend_type: A cache back-end type in BACKEND_TYPES.
-      cachetool_bin_path: Path of the cachetool binary.
     """
     assert os.path.isdir(cache_directory_path)
     assert cache_backend_type in BACKEND_TYPES
-    assert os.path.isfile(cachetool_bin_path), 'invalid ' + cachetool_bin_path
     self._cache_directory_path = cache_directory_path
     self._cache_backend_type = cache_backend_type
-    self._cachetool_bin_path = cachetool_bin_path
     # Make sure cache_directory_path is a valid cache.
     self._CachetoolCmd('validate')
 
@@ -308,7 +293,7 @@ class CacheBackend(object):
       Cachetool's stdout string.
     """
     editor_tool_cmd = [
-        self._cachetool_bin_path,
+        OPTIONS.LocalBinary('cachetool'),
         self._cache_directory_path,
         self._cache_backend_type,
         operation]
@@ -345,7 +330,7 @@ class CacheBackend(object):
     if content_encoding == None:
       return encoded_content
 
-    cmd = [CONTENT_DECODER_TOOL_BIN_PATH]
+    cmd = [OPTIONS.LocalBinary('content_decoder_tool')]
     cmd.extend([s.strip() for s in content_encoding.split(',')])
     process = subprocess.Popen(cmd,
                                stdin=subprocess.PIPE,
diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 49fc0ca..4e906cc 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -95,7 +95,7 @@ dictionary with the keys:
 -   `project_name` (string): Name of the Google Cloud project
 -   `cloud_storage_path` (string): Path in Google Storage where generated traces
     will be stored.
--   `chrome_path` (string): Path to the Chrome executable.
+-   `binaries_path` (string): Path to the executables (Containing chrome).
 -   `src_path` (string): Path to the Chromium source directory.
 -   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
     `clovis-queue`.
@@ -161,7 +161,7 @@ cat >$CONFIG_FILE << EOF
 {
   "project_name" : "$PROJECT_NAME",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "$CHROME_PATH",
+  "binaries_path" : "$BUILD_DIR",
   "src_path" : "$CHROMIUM_SRC",
   "taskqueue_tag" : "some-tag"
 }
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index 23cedb0..9660fc0 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -83,7 +83,7 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
   "instance_name" : "$INSTANCE_NAME",
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "/opt/app/clovis/binaries/chrome",
+  "binaries_path" : "/opt/app/clovis/binaries",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
   "worker_log_path" : "$WORKER_LOG_PATH",
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 038f251..07faf79 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -81,8 +81,7 @@ class Worker(object):
       self._UploadFailureDatabase()
 
     # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs([])
-    options.OPTIONS.local_binary = config['chrome_path']
+    options.OPTIONS.ParseArgs(['--local_build_dir', config['binaries_path']])
 
   def Start(self):
     """Main worker loop.
diff --git a/loading/controller.py b/loading/controller.py
index 8a2d4a6..6ff1389 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -322,7 +322,7 @@ class LocalChromeController(ChromeControllerBase):
   def OpenWithRedirection(self, stdout, stderr):
     """Override for connection context. stdout and stderr are passed to the
        child processes used to run Chrome and XVFB."""
-    chrome_cmd = [OPTIONS.local_binary]
+    chrome_cmd = [OPTIONS.LocalBinary('chrome')]
     chrome_cmd.extend(self._GetChromeArguments())
     # Force use of simple cache.
     chrome_cmd.append('--use-simple-cache-backend=on')
diff --git a/loading/options.py b/loading/options.py
index f7e8ee2..b7f03a2 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -18,9 +18,7 @@ class Options(object):
   be available as instance attributes (eg, OPTIONS.clear_cache).
   """
   # Tuples of (argument name, default value, help string).
-  _ARGS = [ ('clear_cache', True,
-             'clear browser cache before loading'),
-            ('clear_device_data', False,
+  _ARGS = [ ('clear_device_data', False,
              'Clear Chrome data from device before loading'),
             ('chrome_package_name', 'chrome',
              'build/android/pylib/constants package description'),
@@ -28,8 +26,8 @@ class Options(object):
              'hostname for devtools websocket connection'),
             ('devtools_port', 9222,
              'port for devtools websocket connection'),
-            ('local_binary', os.path.join(_SRC_DIR, 'out/Release/chrome'),
-             'chrome binary for local runs'),
+            ('local_build_dir', None,
+             'Build directory for local binary files such as chrome'),
             ('local_noisy', False,
              'Enable local chrome console output'),
             ('local_profile_dir', None,
@@ -40,11 +38,7 @@ class Options(object):
              'docs/linux_suid_sandbox_development.md)'),
             ('devices_file', _SRC_DIR + '/third_party/WebKit/Source/devtools'
              '/front_end/emulated_devices/module.json', 'File containing a'
-             ' list of emulated devices characteristics.'),
-            ('emulate_device', '', 'Name of the device to emulate. Must be '
-             'present in --devices_file, or empty for no emulation.'),
-            ('emulate_network', '', 'Type of network emulation. Empty for no'
-             ' emulation.')
+             ' list of emulated devices characteristics.')
           ]
 
 
@@ -170,4 +164,13 @@ class Options(object):
   def ChromePackage(self):
     return constants.PACKAGE_INFO[self.chrome_package_name]
 
+  def LocalBinary(self, binary_name):
+    """Get local binary path from its name."""
+    assert self.local_build_dir, '--local_build_dir needs to be set.'
+    path = os.path.join(self.local_build_dir, binary_name)
+    assert os.path.isfile(path), \
+        'Missing binary file {} (wrong --local_build_dir?).'.format(path)
+    return path
+
+
 OPTIONS = Options()

commit 55a41569c137c7476da8f7da79c23eb4414d4eb9
Author: droger <droger@chromium.org>
Date:   Wed May 4 06:39:01 2016 -0700

    tools/android/loading UI improvements for Clovis frontend
    
    This CL adds a menu at the top of the pages, a favicon, and some
    basic UI features such as a monospace font for logs.
    
    All pages can now inherit from base.html and share a basic
    template.
    
    Review-Url: https://codereview.chromium.org/1949543002
    Cr-Original-Commit-Position: refs/heads/master@{#391494}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fb948f8eedd6a59e656618fb622a7d2ebaafe7d6

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index b5379c6..33f14d9 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -29,9 +29,17 @@ instance_helper = common.google_instance_helper.GoogleInstanceHelper(
 app = flask.Flask(__name__)
 
 
-def Render(message, memory_logs):
-  return flask.render_template(
-      'log.html', body=message, log=memory_logs.Flush().split('\n'))
+def Render(message, memory_logs=None):
+  """Renders the log.html template.
+
+  Args:
+    message (str): Main content of the page.
+    memory_logs (MemoryLogs): Optional logs.
+  """
+  log = None
+  if memory_logs:
+    log = memory_logs.Flush().split('\n')
+  return flask.render_template('log.html', body=message, log=log)
 
 
 def PollWorkers(tag, start_time, timeout_hours, email_address, task_url):
@@ -243,8 +251,8 @@ def EnqueueTasks(tasks, task_tag):
 
 @app.route('/')
 def Root():
-  """Home page: redirect to the static form."""
-  return flask.redirect('/static/form.html')
+  """Home page: show the new task form."""
+  return flask.render_template('form.html')
 
 
 @app.route('/form_sent', methods=['POST'])
@@ -252,7 +260,7 @@ def StartFromForm():
   """HTML form endpoint."""
   data_stream = flask.request.files.get('json_task')
   if not data_stream:
-    return 'failed'
+    return Render('Failed, no content.')
   http_body_str = data_stream.read()
   return StartFromJsonString(http_body_str)
 
diff --git a/loading/cloud/frontend/static/base.css b/loading/cloud/frontend/static/base.css
new file mode 100644
index 0000000..3a5dfbf
--- /dev/null
+++ b/loading/cloud/frontend/static/base.css
@@ -0,0 +1,46 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file. */
+
+body {
+  font-family:'Arial', sans-serif;
+}
+
+.menu {
+  margin-bottom: 1em;
+  --padding: 14px 16px;
+  border: 1px solid #e7e7e7;
+}
+
+.menu div {
+  padding: var(--padding);
+  font-weight:bold;
+  color: #ffca00;
+  background-color: #cc0000;
+  cursor: default;
+}
+
+.menu ul {
+  list-style-type: none;
+  margin: 0;
+  padding: 0;
+  overflow: hidden;
+}
+
+.menu li {
+  float: left;
+  text-align: center;
+}
+
+.menu li a {
+  color: #666;
+  background-color: #f3f3f3;
+  text-decoration: none;
+  display: block;
+  padding: var(--padding);
+}
+
+.menu li a:hover {
+  background-color: #ddd;
+}
+
diff --git a/loading/cloud/frontend/static/crown_icon.png b/loading/cloud/frontend/static/crown_icon.png
new file mode 100644
index 0000000..2d5ac45
Binary files /dev/null and b/loading/cloud/frontend/static/crown_icon.png differ
diff --git a/loading/cloud/frontend/static/form.html b/loading/cloud/frontend/static/form.html
deleted file mode 100644
index 927cf2b..0000000
--- a/loading/cloud/frontend/static/form.html
+++ /dev/null
@@ -1,17 +0,0 @@
-<!DOCTYPE html>
-<html>
-
-<head>
-<meta charset="utf-8">
-<title>Submmit</title>
-</head>
-
-<body>
-<p> Select JSON file </p>
-<form action="/form_sent" method="POST" enctype="multipart/form-data">
-<input type="file" name="json_task"/>
-<input type="submit" name="submit" value="Upload"/>
-</form>
-</body>
-
-</html>
diff --git a/loading/cloud/frontend/templates/base.html b/loading/cloud/frontend/templates/base.html
new file mode 100644
index 0000000..611f2e7
--- /dev/null
+++ b/loading/cloud/frontend/templates/base.html
@@ -0,0 +1,34 @@
+{# Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file.
+#}
+
+<!DOCTYPE html>
+<html>
+
+<head>
+  <meta charset="utf-8">
+  <title>Clovis</title>
+  <link rel="stylesheet" type="text/css" href="/static/base.css"/>
+  <link rel="icon" href="/static/crown_icon.png"/>
+</head>
+
+<body>
+<header>
+  <div class="menu">
+    <ul style="border: 1px solid #e7e7e7; background-color: #f3f3f3;">
+      <li> <div> Clovis </div>
+      <li> <a href="/">New Task</a>
+      <li> <a href="https://chromium.googlesource.com/chromium/src/+/master/tools/android/loading/cloud/frontend/README.md">
+             Documentation
+           </a>
+    </ul>
+  </div>
+</header>
+
+{# The main content of the page goes here #}
+{% block content %}
+{% endblock %}
+
+</body>
+</html>
diff --git a/loading/cloud/frontend/templates/form.html b/loading/cloud/frontend/templates/form.html
new file mode 100644
index 0000000..9003259
--- /dev/null
+++ b/loading/cloud/frontend/templates/form.html
@@ -0,0 +1,15 @@
+{# Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file.
+#}
+
+{% extends "base.html" %}
+
+{% block content %}
+<h2>Submit New Task</h2>
+<p> Select JSON file </p>
+<form action="/form_sent" method="POST" enctype="multipart/form-data">
+  <input type="file" name="json_task"/>
+  <input type="submit" name="submit" value="Upload"/>
+</form>
+{% endblock %}
diff --git a/loading/cloud/frontend/templates/log.html b/loading/cloud/frontend/templates/log.html
index 4717896..d6b6dcf 100644
--- a/loading/cloud/frontend/templates/log.html
+++ b/loading/cloud/frontend/templates/log.html
@@ -1,14 +1,17 @@
+{# Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file.
+#}
+
 {# Template for a page displaying a body and logs (optional) under a collapsible
    section.
 #}
-<!DOCTYPE html>
-<html>
-<head>
-<meta charset="utf-8">
-<title>Clovis</title>
-</head>
-
-<body>
+{% extends "base.html" %}
+
+{% block content %}
+
+<h2> Task Creation Status </h2>
+
 {{ body }}
 
 {% if log %}
@@ -16,7 +19,9 @@
 <p><a onclick="javascript:ShowHide('HiddenDiv'); return false;" href="#">
   Show/hide details
 </a></p>
-<div id="HiddenDiv" style="display:none";>
+
+<div id="HiddenDiv"
+  style="display:none; font: 0.8em 'Droid Sans Mono', monospace;">
 
 {# Loop over the lines of the log to add linebreaks. #}
 {%- for line in log -%}
@@ -34,6 +39,4 @@ function ShowHide(divId) {
 
 {% endif %}
 
-</body>
-
-</html>
+{% endblock %}

commit d5b6e2272dafec4f84677537588f21e477735fb4
Author: gabadie <gabadie@chromium.org>
Date:   Tue May 3 13:18:47 2016 -0700

    tools/android/loading: Get protocol and status of redirected requests
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1923693004
    Cr-Original-Commit-Position: refs/heads/master@{#391340}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0268a16d1779e910d9dfef4869dc5a071efa1a5d

diff --git a/loading/request_track.py b/loading/request_track.py
index 8bd62da..8248af1 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -664,7 +664,8 @@ class RequestTrack(devtools_monitor.Track):
     _CopyFromDictToObject(redirect_response, r,
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
-                           ('fromDiskCache', 'from_disk_cache')))
+                           ('fromDiskCache', 'from_disk_cache'),
+                           ('protocol', 'protocol'), ('status', 'status')))
     r.timing = Timing.FromDevToolsDict(redirect_response['timing'])
 
     redirect_index = self._redirects_count_by_id[request_id]
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 9adb2e5..a99f973 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -75,6 +75,13 @@ def PatchWpr(wpr_archive_path):
   wpr_archive.Persist()
 
 
+def _FilterOutDataRequests(requests):
+  for request in filter(lambda r: not r.IsDataRequest(), requests):
+    if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
+      raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
+    yield request
+
+
 def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   """Extracts discoverable resource urls from a loading trace according to a
   sub-resource discoverer.
@@ -117,16 +124,7 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   # Prune out data:// requests.
   whitelisted_urls = set()
   logging.info('white-listing %s' % first_resource_request.url)
-  for request in discovered_requests:
-    # Work-around where the protocol may be none for an unclear reason yet.
-    # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
-    #   this work-around.
-    if not request.protocol:
-      logging.warning('ignoring %s (no protocol)' % request.url)
-      continue
-    # Ignore data protocols.
-    if not request.protocol.startswith('http'):
-      continue
+  for request in _FilterOutDataRequests(discovered_requests):
     logging.info('white-listing %s' % request.url)
     whitelisted_urls.add(request.url)
   return whitelisted_urls
@@ -169,13 +167,7 @@ def ListUrlRequests(trace, request_kind):
     set([str])
   """
   urls = set()
-  for request_event in trace.request_track.GetEvents():
-    if request_event.protocol == None:
-      continue
-    if request_event.protocol.startswith('data'):
-      continue
-    if not request_event.protocol.startswith('http'):
-      raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
+  for request_event in _FilterOutDataRequests(trace.request_track.GetEvents()):
     if (request_kind == RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
@@ -278,9 +270,8 @@ def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
       continue
     logging.info('lists resources of %s from %s' % (trace.url, trace_path))
     urls_set = set()
-    for request_event in trace.request_track.GetEvents():
-      if not request_event.protocol.startswith('http'):
-        continue
+    for request_event in _FilterOutDataRequests(
+        trace.request_track.GetEvents()):
       if request_event.url not in urls_set:
         logging.info('  %s' % request_event.url)
         urls_set.add(request_event.url)

commit 9a0dba4dfefeb08ab049d19cf00ab06f88f2badb
Author: droger <droger@chromium.org>
Date:   Tue May 3 05:47:22 2016 -0700

    tools/android/loading Upload worker log
    
    This is the log from worker.py (including any crash stack
    traces).
    
    Add the -u option to python so that the output is not buffered,
    to avoid missing some parts of the log.
    
    Review-Url: https://codereview.chromium.org/1939033002
    Cr-Original-Commit-Position: refs/heads/master@{#391211}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fe8ef82212c043a0d73e5666e9002f591c32b27a

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 4e9248b..49fc0ca 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -101,6 +101,8 @@ dictionary with the keys:
     `clovis-queue`.
 -   `instance_name` (string, optional): Name of the Compute Engine instance this
     script is running on.
+-   `worker_log_file` (string, optional): Path to the log file capturing the
+    output of `worker.py`, to be uploaded to Cloud Storage.
 -   `self_destruct` (boolean, optional): Whether the worker will destroy the
     Compute Engine instance when there are no remaining tasks to process. This
     is only relevant when running in the cloud, and requires `instance_name` to
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index a1df3f3..23cedb0 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -65,7 +65,7 @@ cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
 chown root:root /usr/local/sbin/chrome-devel-sandbox
 chmod 4755 /usr/local/sbin/chrome-devel-sandbox
 
-# Make sure the pythonapp user owns the application code
+# Make sure the pythonapp user owns the application code.
 chown -R pythonapp:pythonapp /opt/app
 
 # Create the configuration file for this deployment.
@@ -76,6 +76,7 @@ if [ "$(get_instance_metadata self-destruct)" == "false" ]; then
 else
   SELF_DESTRUCT="True"
 fi
+WORKER_LOG_PATH=/opt/app/clovis/worker.log
 
 cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
@@ -85,6 +86,7 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
+  "worker_log_path" : "$WORKER_LOG_PATH",
   "self_destruct" : "$SELF_DESTRUCT"
 }
 EOF
@@ -101,7 +103,7 @@ fi
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
 directory=/opt/app/clovis/src/tools/android/loading/cloud/backend
-command=python worker.py --config $DEPLOYMENT_CONFIG_PATH
+command=python -u worker.py --config $DEPLOYMENT_CONFIG_PATH
 autostart=true
 autorestart=unexpected
 user=pythonapp
@@ -111,8 +113,8 @@ environment=VIRTUAL_ENV="/opt/app/clovis/env", \
     PATH="/opt/app/clovis/env/bin:/usr/bin", \
     HOME="/home/pythonapp",USER="pythonapp", \
     CHROME_DEVEL_SANDBOX="/usr/local/sbin/chrome-devel-sandbox"
-stdout_logfile=syslog
-stderr_logfile=syslog
+stdout_logfile=$WORKER_LOG_PATH
+stderr_logfile=$WORKER_LOG_PATH
 EOF
 
 supervisorctl reread
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 5b2ad60..038f251 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -35,6 +35,7 @@ class Worker(object):
     self._taskqueue_tag = config['taskqueue_tag']
     self._src_path = config['src_path']
     self._instance_name = config.get('instance_name')
+    self._worker_log_path = config.get('worker_log_path')
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
     self._self_destruct = config.get('self_destruct')
@@ -186,6 +187,14 @@ class Worker(object):
   def _Finalize(self):
     """Called before exiting."""
     self._logger.info('Done')
+    # Upload the worker log.
+    if self._worker_log_path:
+      self._logger.info('Uploading worker log.')
+      remote_log_path = os.path.join(self._base_path_in_bucket, 'worker_log')
+      if self._instance_name:
+        remote_log_path += '_' + self._instance_name
+      self._google_storage_accessor.UploadFile(self._worker_log_path,
+                                               remote_log_path)
     # Self destruct.
     if self._self_destruct:
       self._logger.info('Starting instance destruction: ' + self._instance_name)

commit a19fa11c045899c181e6d3f0ac04e0267947e441
Author: droger <droger@chromium.org>
Date:   Tue May 3 04:57:12 2016 -0700

    tools/android/loading Improve logging of failures
    
    Adds a failure database that keeps track of various failures:
    - trace generation failure,
    - TaskQueue failure,
    - worker.py starting up with an dirty state, most likely
      because it crashed and was restarted.
    
    The trace database is uploaded to Cloud Storage.
    
    As part of this CL, some code from _ProcessClovisTask() is
    split to a separate function _HandleTraceGenerationResults(),
    to make the code cleaner.
    
    The "autorestart" parameter for supervisor is changed
    to "unexpected" so that the worker script is restarted
    only if it crashed, and not if it exits cleanly.
    This avoids the worker to be restarted if there are no
    tasks to process, which has the side effect of triggering
    a "startup_with_dirty_state" error.
    
    The "destruct_instance_name" config field is split in
    "instance_name" (string) and "self_destruct" (bool).
    "trace_database_filename" is removed and is now inferred
    from the instance name.
    
    Review-Url: https://codereview.chromium.org/1930223003
    Cr-Original-Commit-Position: refs/heads/master@{#391203}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8cc5d09a2b6230b88b6e0a747f59e277a22ba145

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index e9e1a9a..4e9248b 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -99,12 +99,12 @@ dictionary with the keys:
 -   `src_path` (string): Path to the Chromium source directory.
 -   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
     `clovis-queue`.
--   `trace_database_filename` (string, optional): Filename for the trace
-    database in Cloud Storage. Must be unique per worker to avoid concurrent
-    access. Defaults to `trace_database.json`.
--   `destruct_instance_name` (string, optional): Name of the instance the worker
-    will destroy when there are no remaining tasks to process. This is only
-    relevant when running in the cloud.
+-   `instance_name` (string, optional): Name of the Compute Engine instance this
+    script is running on.
+-   `self_destruct` (boolean, optional): Whether the worker will destroy the
+    Compute Engine instance when there are no remaining tasks to process. This
+    is only relevant when running in the cloud, and requires `instance_name` to
+    be defined.
 
 ## Use the app
 
diff --git a/loading/cloud/backend/failure_database.py b/loading/cloud/backend/failure_database.py
new file mode 100644
index 0000000..4fdfde7
--- /dev/null
+++ b/loading/cloud/backend/failure_database.py
@@ -0,0 +1,39 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+
+class FailureDatabase(object):
+  """Logs the failures happening in the Clovis backend."""
+
+  def __init__(self, json_string=None):
+    """Loads a FailureDatabase from a string returned by ToJsonString()."""
+    if json_string:
+      self._failures_dict = json.loads(json_string)
+    else:
+      self._failures_dict = {}
+
+  def ToJsonDict(self):
+    """Returns a dict representing this instance."""
+    return self._failures_dict
+
+  def ToJsonString(self):
+    """Returns a string representing this instance."""
+    return json.dumps(self.ToJsonDict(), indent=2)
+
+  def AddFailure(self, failure_name, failure_content=None):
+    """Adds a failure with the given name and content. If the failure already
+    exists, it will increment the associated count.
+
+    Args:
+      failure_name (str): name of the failure.
+      failure_content (str): content of the failure (e.g. the URL or task that
+                             is failing).
+    """
+    content = failure_content if failure_content else 'error_count'
+    if failure_name not in self._failures_dict:
+      self._failures_dict[failure_name] = {}
+    error_count = self._failures_dict[failure_name].get(content, 0)
+    self._failures_dict[failure_name][content] = error_count + 1
+
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index 69cb7d9..a1df3f3 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -72,20 +72,20 @@ chown -R pythonapp:pythonapp /opt/app
 DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
 TASKQUEUE_TAG=`get_instance_metadata taskqueue-tag`
 if [ "$(get_instance_metadata self-destruct)" == "false" ]; then
-  SELF_DESTRUCT_CONFIG_LINE=""
+  SELF_DESTRUCT="False"
 else
-  SELF_DESTRUCT_CONFIG_LINE="\"destruct_instance_name\" : \"$INSTANCE_NAME\","
+  SELF_DESTRUCT="True"
 fi
 
 cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
-  $SELF_DESTRUCT_CONFIG_LINE
+  "instance_name" : "$INSTANCE_NAME",
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
-  "trace_database_filename" : "trace_database_${INSTANCE_NAME}.json"
+  "self_destruct" : "$SELF_DESTRUCT"
 }
 EOF
 
@@ -103,7 +103,7 @@ cat >/etc/supervisor/conf.d/python-app.conf << EOF
 directory=/opt/app/clovis/src/tools/android/loading/cloud/backend
 command=python worker.py --config $DEPLOYMENT_CONFIG_PATH
 autostart=true
-autorestart=true
+autorestart=unexpected
 user=pythonapp
 # Environment variables ensure that the application runs inside of the
 # configured virtualenv.
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index cef7044..5b2ad60 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -21,6 +21,7 @@ sys.path.insert(0,
 import controller
 from cloud.common.clovis_task import ClovisTask
 from cloud.common.google_instance_helper import GoogleInstanceHelper
+from failure_database import FailureDatabase
 from google_storage_accessor import GoogleStorageAccessor
 import loading_trace
 from loading_trace_database import LoadingTraceDatabase
@@ -33,9 +34,12 @@ class Worker(object):
     self._project_name = config['project_name']
     self._taskqueue_tag = config['taskqueue_tag']
     self._src_path = config['src_path']
-    self._destruct_instance_name = config.get('destruct_instance_name')
+    self._instance_name = config.get('instance_name')
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
+    self._self_destruct = config.get('self_destruct')
+    if self._self_destruct and not self._instance_name:
+      self._logger.error('Self destruction requires an instance name.')
 
     # Separate the cloud storage path into the bucket and the base path under
     # the bucket.
@@ -51,13 +55,29 @@ class Worker(object):
         credentials=self._credentials, project_name=self._project_name,
         bucket_name=self._bucket_name)
 
+    if self._instance_name:
+      trace_database_filename = 'trace_database_%s.json' % self._instance_name
+      failure_database_filename = \
+          'failure_database_%s.json' % self._instance_name
+    else:
+      trace_database_filename = 'trace_database.json'
+      failure_database_filename = 'failure_dabatase.json'
     self._traces_dir = os.path.join(self._base_path_in_bucket, 'traces')
-    self._trace_database_path = os.path.join(
-        self._traces_dir,
-        config.get('trace_database_filename', 'trace_database.json'))
+    self._trace_database_path = os.path.join(self._traces_dir,
+                                             trace_database_filename)
+    self._failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
+    self._failure_database_path = os.path.join(self._failures_dir,
+                                               failure_database_filename)
 
-    # Recover any existing trace database in case the worker died.
+    # Recover any existing trace database and failures in case the worker died.
     self._DownloadTraceDatabase()
+    self._DownloadFailureDatabase()
+
+    if self._trace_database.ToJsonDict() or self._failure_database.ToJsonDict():
+      # Script is restarting after a crash, or there are already files from a
+      # previous run in the directory.
+      self._failure_database.AddFailure('startup_with_dirty_state')
+      self._UploadFailureDatabase()
 
     # Initialize the global options that will be used during trace generation.
     options.OPTIONS.ParseArgs([])
@@ -96,16 +116,30 @@ class Worker(object):
     self._logger.info('Downloading trace database')
     trace_database_string = self._google_storage_accessor.DownloadAsString(
         self._trace_database_path) or '{}'
-    trace_database_dict = json.loads(trace_database_string)
-    self._trace_database = LoadingTraceDatabase(trace_database_dict)
+    self._trace_database = LoadingTraceDatabase.FromJsonString(
+        trace_database_string)
 
   def _UploadTraceDatabase(self):
     """Uploads the trace database to CloudStorage."""
     self._logger.info('Uploading trace database')
     self._google_storage_accessor.UploadString(
-        json.dumps(self._trace_database.ToJsonDict(), indent=2),
+        self._trace_database.ToJsonString(),
         self._trace_database_path)
 
+  def _DownloadFailureDatabase(self):
+    """Downloads the failure database from CloudStorage."""
+    self._logger.info('Downloading failure database')
+    failure_database_string = self._google_storage_accessor.DownloadAsString(
+        self._failure_database_path)
+    self._failure_database = FailureDatabase(failure_database_string)
+
+  def _UploadFailureDatabase(self):
+    """Uploads the failure database to CloudStorage."""
+    self._logger.info('Uploading failure database')
+    self._google_storage_accessor.UploadString(
+        self._failure_database.ToJsonString(),
+        self._failure_database_path)
+
   def _FetchClovisTask(self, project_name, task_api, queue_name):
     """Fetches a ClovisTask from the task queue.
 
@@ -132,27 +166,35 @@ class Worker(object):
     # once it is fixed.
     retry_count = google_task['retry_count']
     max_retry_count = 3
-    if retry_count >= max_retry_count:
+    skip_task = retry_count >= max_retry_count
+    if skip_task:
       task_api.tasks().delete(project=project_name, taskqueue=queue_name,
                               task=task_id).execute()
-      return self._FetchClovisTask(project_name, task_api, queue_name)
 
     clovis_task = ClovisTask.FromBase64(google_task['payloadBase64'])
+
+    if retry_count > 0:
+      self._failure_database.AddFailure('task_queue_retry',
+                                        clovis_task.ToJsonString())
+      self._UploadFailureDatabase()
+
+    if skip_task:
+      return self._FetchClovisTask(project_name, task_api, queue_name)
+
     return (clovis_task, task_id)
 
   def _Finalize(self):
     """Called before exiting."""
     self._logger.info('Done')
     # Self destruct.
-    if self._destruct_instance_name:
-      self._logger.info('Starting instance destruction: ' +
-                        self._destruct_instance_name)
+    if self._self_destruct:
+      self._logger.info('Starting instance destruction: ' + self._instance_name)
       google_instance_helper = GoogleInstanceHelper(
           self._credentials, self._project_name, self._logger)
-      success = google_instance_helper.DeleteInstance(
-          self._taskqueue_tag, self._destruct_instance_name)
+      success = google_instance_helper.DeleteInstance(self._taskqueue_tag,
+                                                      self._instance_name)
       if not success:
-        self._logger.error('Self destruction failed')
+        self._logger.error('Self destruction failed.')
     # Do not add anything after this line, as the instance might be killed at
     # any time.
 
@@ -217,6 +259,41 @@ class Worker(object):
 
     return trace_metadata
 
+  def _HandleTraceGenerationResults(self, local_filename, log_filename,
+                                    remote_filename, trace_metadata):
+    """Updates the trace database and the failure database after a trace
+    generation. Uploads the trace and the log.
+    Results related to successful traces are uploaded in the _traces_dir
+    directory, and failures are uploaded in the _failures_dir directory.
+
+    Args:
+      local_filename (str): Path to the local file containing the trace.
+      log_filename (str): Path to the local file containing the log.
+      remote_filename (str): Name of the target remote file where the trace and
+                             the log (with a .log extension added) are uploaded.
+      trace_metadata (dict): Metadata associated with the trace generation.
+    """
+    if trace_metadata['succeeded']:
+      remote_trace_location = os.path.join(self._traces_dir, remote_filename)
+      full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
+                                             remote_trace_location)
+      self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
+    else:
+      remote_trace_location = os.path.join(self._failures_dir, remote_filename)
+      self._failure_database.AddFailure('trace_collection',
+                                        trace_metadata['url'])
+
+    if os.path.isfile(local_filename):
+      self._logger.debug('Uploading: %s' % remote_trace_location)
+      self._google_storage_accessor.UploadFile(local_filename,
+                                               remote_trace_location)
+    else:
+      self._logger.warning('No trace found at: ' + local_filename)
+
+    self._logger.debug('Uploading analyze log')
+    remote_log_location = remote_trace_location + '.log'
+    self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
+
   def _ProcessClovisTask(self, clovis_task):
     """Processes one clovis_task."""
     if clovis_task.Action() != 'trace':
@@ -230,39 +307,33 @@ class Worker(object):
     emulate_device = params.get('emulate_device')
     emulate_network = params.get('emulate_network')
 
-    failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
-    # TODO(blundell): Fix this up.
-    logs_dir = os.path.join(self._base_path_in_bucket, 'analyze_logs')
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
 
+    failure_happened = False
+    success_happened = False
+
     while len(urls) > 0:
       url = urls.pop()
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         self._logger.debug('Generating trace for URL: %s' % url)
-        remote_filename = os.path.join(local_filename, str(repeat))
         trace_metadata = self._GenerateTrace(
             url, emulate_device, emulate_network, local_filename, log_filename)
         if trace_metadata['succeeded']:
-          self._logger.debug('Uploading: %s' % remote_filename)
-          remote_trace_location = os.path.join(self._traces_dir,
-                                               remote_filename)
-          self._google_storage_accessor.UploadFile(local_filename,
-                                                   remote_trace_location)
-          full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
-                                                 remote_trace_location)
-          self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
+          success_happened = True
         else:
           self._logger.warning('Trace generation failed for URL: %s' % url)
-          if os.path.isfile(local_filename):
-            self._google_storage_accessor.UploadFile(
-                local_filename, os.path.join(failures_dir, remote_filename))
-        self._logger.debug('Uploading analyze log')
-        self._google_storage_accessor.UploadFile(
-            log_filename, os.path.join(logs_dir, remote_filename))
-    self._UploadTraceDatabase()
+          failure_happened = True
+        remote_filename = os.path.join(local_filename, str(repeat))
+        self._HandleTraceGenerationResults(
+            local_filename, log_filename, remote_filename, trace_metadata)
+
+    if success_happened:
+      self._UploadTraceDatabase()
+    if failure_happened:
+      self._UploadFailureDatabase()
 
 if __name__ == '__main__':
   parser = argparse.ArgumentParser(
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index dad6153..91705aa 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -31,6 +31,10 @@ class LoadingTraceDatabase(object):
     """Returns a dict representing this instance."""
     return self._traces_dict
 
+  def ToJsonString(self):
+    """Returns a string representing this instance."""
+    return json.dumps(self._traces_dict, indent=2)
+
   def ToJsonFile(self, json_path):
     """Saves a json file representing this instance."""
     json_dict = self.ToJsonDict()
@@ -43,6 +47,11 @@ class LoadingTraceDatabase(object):
     return LoadingTraceDatabase(json_dict)
 
   @classmethod
+  def FromJsonString(cls, json_string):
+    """Returns an instance from a string returned by ToJsonString()."""
+    return LoadingTraceDatabase(json.loads(json_string))
+
+  @classmethod
   def FromJsonFile(cls, json_path):
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:

commit 2b7f2680cbf25573526a620df590c58ced904fe3
Author: droger <droger@chromium.org>
Date:   Tue May 3 03:14:39 2016 -0700

    tools/android/loading Add missing argument to PollWorkers() call
    
    This CL fixes a crash happening because a PollWorkers() call was missing
    the timeout argument.
    
    Review-Url: https://codereview.chromium.org/1936073002
    Cr-Original-Commit-Position: refs/heads/master@{#391190}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1e591d0097ad9903666d75e182cbc29ea032274b

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index cb2f2c2..b5379c6 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -63,8 +63,8 @@ def PollWorkers(tag, start_time, timeout_hours, email_address, task_url):
   if live_instance_count > 0 or live_instance_count == -1:
     clovis_logger.info('Retry later, instances still alive for tag: ' + tag)
     poll_interval_minutes = 10
-    deferred.defer(PollWorkers, tag, start_time, email_address, task_url,
-                   _countdown=(60 * poll_interval_minutes))
+    deferred.defer(PollWorkers, tag, start_time, timeout_hours, email_address,
+                   task_url, _countdown=(60 * poll_interval_minutes))
     return
 
   Finalize(tag, email_address, 'SUCCESS', task_url)

commit 9b37e4422dc6c9eda2ffb31d3aeae9d38f3ec7de
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 2 12:28:23 2016 -0700

    sandwich: Add benchmark statistics into CSVs
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1939013002
    Cr-Original-Commit-Position: refs/heads/master@{#391024}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 269460a9ad0db2b669ee312cf220fb4e1849a2b0

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 339cf72..b472ce6 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -8,6 +8,7 @@ python pull_sandwich_metrics.py -h
 """
 
 import collections
+import json
 import logging
 import os
 import shutil
@@ -27,12 +28,21 @@ from telemetry.util import rgba_color
 
 import loading_trace as loading_trace_module
 import sandwich_runner
+import sandwich_misc
 import tracing
 
 
 CSV_FIELD_NAMES = [
     'repeat_id',
     'url',
+    'subresource_discoverer',
+    'subresource_count',
+    # The amount of subresources detected at SetupBenchmark step.
+    'subresource_count_theoretic',
+    # Amount of subresources for caching as suggested by the subresource
+    # discoverer.
+    'cached_subresource_count_theoretic',
+    'cached_subresource_count',
     'total_load',
     'js_onload_event',
     'browser_malloc_avg',
@@ -182,6 +192,30 @@ def _ExtractMemoryMetrics(loading_trace):
   }
 
 
+def _ExtractBenchmarkStatistics(benchmark_setup, loading_trace):
+  """Extracts some useful statistics from a benchmark run.
+
+  Args:
+    benchmark_setup: benchmark_setup: dict representing the benchmark setup
+        JSON. The JSON format is according to:
+            SandwichTaskBuilder.PopulateLoadBenchmark.SetupBenchmark.
+    loading_trace: loading_trace_module.LoadingTrace.
+
+  Returns:
+    Dictionary with all extracted fields set.
+  """
+  return {
+    'subresource_discoverer': benchmark_setup['subresource_discoverer'],
+    'subresource_count': len(sandwich_misc.ListUrlRequests(
+        loading_trace, sandwich_misc.RequestOutcome.All)),
+    'subresource_count_theoretic': len(benchmark_setup['url_resources']),
+    'cached_subresource_count': len(sandwich_misc.ListUrlRequests(
+        loading_trace, sandwich_misc.RequestOutcome.ServedFromCache)),
+    'cached_subresource_count_theoretic':
+        len(benchmark_setup['cache_whitelist']),
+  }
+
+
 def _ExtractCompletenessRecordFromVideo(video_path):
   """Extracts the completeness record from a video.
 
@@ -241,10 +275,13 @@ def ComputeSpeedIndex(completeness_record):
   return speed_index
 
 
-def _ExtractMetricsFromRunDirectory(run_directory_path):
+def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
   """Extracts all the metrics from traces and video of a sandwich run.
 
   Args:
+    benchmark_setup: benchmark_setup: dict representing the benchmark setup
+        JSON. The JSON format is according to:
+            SandwichTaskBuilder.PopulateLoadBenchmark.SetupBenchmark.
     run_directory_path: Path of the run directory.
 
   Returns:
@@ -256,6 +293,8 @@ def _ExtractMetricsFromRunDirectory(run_directory_path):
   run_metrics = {'url': loading_trace.url}
   run_metrics.update(_ExtractDefaultMetrics(loading_trace))
   run_metrics.update(_ExtractMemoryMetrics(loading_trace))
+  run_metrics.update(
+      _ExtractBenchmarkStatistics(benchmark_setup, loading_trace))
   video_path = os.path.join(run_directory_path, 'video.mp4')
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
@@ -268,17 +307,20 @@ def _ExtractMetricsFromRunDirectory(run_directory_path):
   return run_metrics
 
 
-def ExtractMetricsFromRunnerOutputDirectory(output_directory_path):
+def ExtractMetricsFromRunnerOutputDirectory(benchmark_setup_path,
+                                            output_directory_path):
   """Extracts all the metrics from all the traces of a sandwich runner output
   directory.
 
   Args:
+    benchmark_setup_path: Path of the JSON of the benchmark setup.
     output_directory_path: The sandwich runner's output directory to extract the
         metrics from.
 
   Returns:
     List of dictionaries.
   """
+  benchmark_setup = json.load(open(benchmark_setup_path))
   assert os.path.isdir(output_directory_path)
   metrics = []
   for node_name in os.listdir(output_directory_path):
@@ -289,7 +331,8 @@ def ExtractMetricsFromRunnerOutputDirectory(output_directory_path):
     except ValueError:
       continue
     run_directory_path = os.path.join(output_directory_path, node_name)
-    run_metrics = _ExtractMetricsFromRunDirectory(run_directory_path)
+    run_metrics = _ExtractMetricsFromRunDirectory(
+        benchmark_setup, run_directory_path)
     run_metrics['repeat_id'] = repeat_id
     assert set(run_metrics.keys()) == set(CSV_FIELD_NAMES)
     metrics.append(run_metrics)
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index e7553bc..ea813e2 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -232,19 +232,6 @@ class PageTrackTest(unittest.TestCase):
     with self.assertRaises(ValueError):
       puller.ComputeSpeedIndex(completness_record)
 
-  def testCommandLine(self):
-    tmp_dir = tempfile.mkdtemp()
-    for dirname in ['1', '2', 'whatever']:
-      os.mkdir(os.path.join(tmp_dir, dirname))
-      LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
-          os.path.join(tmp_dir, dirname, sandwich_runner.TRACE_FILENAME))
-
-    process = subprocess.Popen(['python', puller.__file__, tmp_dir])
-    process.wait()
-    shutil.rmtree(tmp_dir)
-
-    self.assertEquals(0, process.returncode)
-
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 5ecc064..9adb2e5 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -154,16 +154,16 @@ def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
     logging.error('+     ' + url)
 
 
-class _RequestOutcome:
+class RequestOutcome:
   All, ServedFromCache, NotServedFromCache = range(3)
 
 
-def _ListUrlRequests(trace, request_kind):
+def ListUrlRequests(trace, request_kind):
   """Lists requested URLs from a trace.
 
   Args:
     trace: (LoadingTrace) loading trace.
-    request_kind: _RequestOutcome indicating the subset of requests to output.
+    request_kind: RequestOutcome indicating the subset of requests to output.
 
   Returns:
     set([str])
@@ -176,13 +176,13 @@ def _ListUrlRequests(trace, request_kind):
       continue
     if not request_event.protocol.startswith('http'):
       raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
-    if (request_kind == _RequestOutcome.ServedFromCache and
+    if (request_kind == RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
-    elif (request_kind == _RequestOutcome.NotServedFromCache and
+    elif (request_kind == RequestOutcome.NotServedFromCache and
         not request_event.from_disk_cache):
       urls.add(request_event.url)
-    elif request_kind == _RequestOutcome.All:
+    elif request_kind == RequestOutcome.All:
       urls.add(request_event.url)
   return urls
 
@@ -216,12 +216,12 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
     trace = LoadingTrace.FromJsonFile(trace_path)
     logging.info('verifying %s from %s' % (trace.url, trace_path))
     _PrintUrlSetComparison(url_resources,
-        _ListUrlRequests(trace, _RequestOutcome.All), 'All resources')
+        ListUrlRequests(trace, RequestOutcome.All), 'All resources')
     _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
-        _ListUrlRequests(trace, _RequestOutcome.ServedFromCache),
+        ListUrlRequests(trace, RequestOutcome.ServedFromCache),
         'Cached resources')
     sent_url_requests = \
-        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache)
+        ListUrlRequests(trace, RequestOutcome.NotServedFromCache)
     _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
         sent_url_requests, 'Non cached resources')
     all_sent_url_requests.update(sent_url_requests)
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index ea763ec..318f869 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -201,6 +201,7 @@ class SandwichTaskBuilder(task_manager.Builder):
       with open(SetupBenchmark.path, 'w') as output:
         json.dump({
             'cache_whitelist': [url for url in whitelisted_urls],
+            'subresource_discoverer': subresource_discoverer,
             'url_resources': url_resources,
           }, output)
 
@@ -234,7 +235,7 @@ class SandwichTaskBuilder(task_manager.Builder):
           SetupBenchmark.path, RunBenchmark.path)
       trace_metrics_list = \
           sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
-              RunBenchmark.path)
+              SetupBenchmark.path, RunBenchmark.path)
       trace_metrics_list.sort(key=lambda e: e['repeat_id'])
       with open(ExtractMetrics.path, 'w') as csv_file:
         writer = csv.DictWriter(csv_file,

commit c58a3ccb2cf3c10a78283e9f968489850b00d69b
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 2 08:36:59 2016 -0700

    tools/android/loading: make DevToolsConnection._HttpRequest() more reliable
    
    Sometimes, HTTPConnection.getresponse() can raise a BadStatusLine exception
    failing the connection to Chrome devtools. This CL add a retry mechanism in
    DevtoolsConnection._HttpRequest() to address this issue, increasing success
    rate of loading trace recording.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1941753002
    Cr-Original-Commit-Position: refs/heads/master@{#390946}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 243b15b6cad3b601e34bb6a6cb457f25a4ed602d

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index e8930e9..ead9bce 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -10,6 +10,7 @@ import json
 import logging
 import os
 import sys
+import time
 
 file_dir = os.path.dirname(__file__)
 sys.path.append(os.path.join(file_dir, '..', '..', 'perf'))
@@ -88,6 +89,8 @@ class DevToolsConnection(object):
   TRACING_DONE_EVENT = 'Tracing.tracingComplete'
   TRACING_STREAM_EVENT = 'Tracing.tracingComplete'  # Same as TRACING_DONE.
   TRACING_TIMEOUT = 300
+  HTTP_ATTEMPTS = 10
+  HTTP_ATTEMPT_INTERVAL_SECONDS = 0.1
 
   def __init__(self, hostname, port):
     """Initializes the connection with a DevTools server.
@@ -352,17 +355,22 @@ class DevToolsConnection(object):
 
   def _HttpRequest(self, path):
     assert path[0] == '/'
-    r = httplib.HTTPConnection(self._http_hostname, self._http_port)
-    try:
-      r.request('GET', '/json' + path)
-      response = r.getresponse()
-      if response.status != 200:
-        raise DevToolsConnectionException(
-            'Cannot connect to DevTools, reponse code %d' % response.status)
-      raw_response = response.read()
-    finally:
-      r.close()
-    return raw_response
+    for _ in xrange(self.HTTP_ATTEMPTS):
+      r = httplib.HTTPConnection(self._http_hostname, self._http_port)
+      try:
+        r.request('GET', '/json' + path)
+        response = r.getresponse()
+        if response.status != 200:
+          raise DevToolsConnectionException(
+              'Cannot connect to DevTools, reponse code %d' % response.status)
+        return response.read()
+      except httplib.BadStatusLine as exception:
+        logging.warning('Devtools HTTP connection failed: %s' % repr(exception))
+        time.sleep(self.HTTP_ATTEMPT_INTERVAL_SECONDS)
+      finally:
+        r.close()
+    # Raise the exception that has failed the last attempt.
+    raise
 
   def _Connect(self):
     assert not self._ws

commit 8877aa29ec94ac725a88653e304b9fa8b4b23980
Author: droger <droger@chromium.org>
Date:   Fri Apr 29 09:17:54 2016 -0700

    tools/android/loading Delete tasks that failed multiple times.
    
    The strategy for handling task failures was implemented using the task
    retry options.
    However, it seems that these don't work at all and the task is never
    actually removed from the queue.
    
    Google internal bug b/28442122 is tracking this.
    
    This CL implements the handling of the failed task in the backend
    worker instead, as a workaround.
    
    Review-Url: https://codereview.chromium.org/1931873002
    Cr-Original-Commit-Position: refs/heads/master@{#390664}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9db2c2e89cd4fa7d444f75e41b3e8fd2ab05d03e

diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index ff0d473..cef7044 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -8,7 +8,6 @@ import logging
 import os
 import re
 import sys
-import time
 
 from googleapiclient import discovery
 from oauth2client.client import GoogleCredentials
@@ -123,10 +122,21 @@ class Worker(object):
         project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=600,
         groupByTag=True, tag=self._taskqueue_tag).execute()
     if (not response.get('items')) or (len(response['items']) < 1):
-      return (None, None)
+      return (None, None)  # The task queue is empty.
 
     google_task = response['items'][0]
     task_id = google_task['id']
+
+    # Delete the task without processing if it already failed multiple times.
+    # TODO(droger): This is a workaround for internal bug b/28442122, revisit
+    # once it is fixed.
+    retry_count = google_task['retry_count']
+    max_retry_count = 3
+    if retry_count >= max_retry_count:
+      task_api.tasks().delete(project=project_name, taskqueue=queue_name,
+                              task=task_id).execute()
+      return self._FetchClovisTask(project_name, task_api, queue_name)
+
     clovis_task = ClovisTask.FromBase64(google_task['payloadBase64'])
     return (clovis_task, task_id)
 
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 42b7b09..cb2f2c2 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -217,7 +217,6 @@ def EnqueueTasks(tasks, task_tag):
   Google Compute Engine.
   """
   q = taskqueue.Queue('clovis-queue')
-  retry_options = taskqueue.TaskRetryOptions(task_retry_limit=3)
   # Add tasks to the queue by groups.
   # TODO(droger): This supports thousands of tasks, but maybe not millions.
   # Defer the enqueuing if it times out.
@@ -228,7 +227,7 @@ def EnqueueTasks(tasks, task_tag):
       group = tasks[i:i+group_size]
       taskqueue_tasks = [
           taskqueue.Task(payload=task.ToJsonString(), method='PULL',
-                         tag=task_tag, retry_options=retry_options)
+                         tag=task_tag)
           for task in group]
       rpc = taskqueue.create_rpc()
       q.add_async(task=taskqueue_tasks, rpc=rpc)

commit a48350800ef9604981b4feb68809b7848499affe
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 29 06:04:34 2016 -0700

    sandwich: List requests that couldn't be served by web page replay.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1922923004
    Cr-Original-Commit-Position: refs/heads/master@{#390634}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a181d8d4fb6ca04b9ed2707a67046f08618a1387

diff --git a/loading/options.py b/loading/options.py
index 3ea91b7..f7e8ee2 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -113,6 +113,7 @@ class Options(object):
     self._parsed_args = parsed_args
 
   def _MakeParser(self, description=None, extra=None, group=None):
+    self._arg_set = set()
     add_help = True if group is None else False
     parser = argparse.ArgumentParser(
         description=description, add_help=add_help)
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 5df6351..5ecc064 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -5,6 +5,7 @@
 import logging
 import json
 import os
+from urlparse import urlparse
 
 import chrome_cache
 import common_util
@@ -199,6 +200,7 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
   benchmark_setup = json.load(open(benchmark_setup_path))
   cache_whitelist = set(benchmark_setup['cache_whitelist'])
   url_resources = set(benchmark_setup['url_resources'])
+  all_sent_url_requests = set()
 
   # Verify requests from traces.
   run_id = -1
@@ -218,9 +220,36 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
     _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
         _ListUrlRequests(trace, _RequestOutcome.ServedFromCache),
         'Cached resources')
+    sent_url_requests = \
+        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache)
     _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
-        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache),
-        'Non cached resources')
+        sent_url_requests, 'Non cached resources')
+    all_sent_url_requests.update(sent_url_requests)
+
+  # Verify requests from WPR.
+  wpr_log_path = os.path.join(
+      benchmark_output_directory_path, sandwich_runner.WPR_LOG_FILENAME)
+  logging.info('verifying requests from %s' % wpr_log_path)
+  all_wpr_requests = wpr_backend.ExtractRequestsFromLog(wpr_log_path)
+  all_wpr_urls = set()
+  unserved_wpr_urls = set()
+  wpr_command_colliding_urls = set()
+
+  for request in all_wpr_requests:
+    if request.is_wpr_host:
+      continue
+    if urlparse(request.url).path.startswith('/web-page-replay'):
+      wpr_command_colliding_urls.add(request.url)
+    elif request.is_served is False:
+      unserved_wpr_urls.add(request.url)
+    all_wpr_urls.add(request.url)
+
+  _PrintUrlSetComparison(set(), unserved_wpr_urls,
+                         'Distinct unserved resources from WPR')
+  _PrintUrlSetComparison(set(), wpr_command_colliding_urls,
+                         'Distinct resources colliding to WPR commands')
+  _PrintUrlSetComparison(all_wpr_urls, all_sent_url_requests,
+                         'Distinct resources requests to WPR')
 
 
 def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 3dc0415..665824b 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -25,6 +25,7 @@ import loading_trace
 # Standard filenames in the sandwich runner's output directory.
 TRACE_FILENAME = 'trace.json'
 VIDEO_FILENAME = 'video.mp4'
+WPR_LOG_FILENAME = 'wpr.log'
 
 # Memory dump category used to get memory metrics.
 MEMORY_DUMP_CATEGORY = 'disabled-by-default-memory-infra'
@@ -264,12 +265,16 @@ class SandwichRunner(object):
       chrome_cache.UnzipDirectoryContent(
           self.cache_archive_path, self._local_cache_directory_path)
 
+    out_log_path = None
+    if self.trace_output_directory:
+      out_log_path = os.path.join(self.trace_output_directory, WPR_LOG_FILENAME)
+
     ran_urls = []
     with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
         record=self.wpr_record,
         network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
-        disable_script_injection=self.disable_wpr_script_injection
-        ):
+        disable_script_injection=self.disable_wpr_script_injection,
+        out_log_path=out_log_path):
       for _ in xrange(self.job_repeat):
         for url in self.urls:
           self._RunUrl(url, run_id=len(ran_urls))
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index d687b13..f4b232c 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -24,8 +24,7 @@ OPTIONS = options.OPTIONS
 
 class WebServerTestCase(unittest.TestCase):
   def setUp(self):
-    if not OPTIONS._parsed_args:
-      OPTIONS.ParseArgs('', extra=[('--noisy', False)])
+    OPTIONS.ParseArgs('', extra=[('--noisy', False)])
     self._temp_dir = tempfile.mkdtemp()
     self._server = webserver_test.WebServer(self._temp_dir, self._temp_dir)
 
diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
index e983572..a24d282 100644
--- a/loading/wpr_backend.py
+++ b/loading/wpr_backend.py
@@ -9,6 +9,7 @@ import collections
 import os
 import re
 import sys
+from urlparse import urlparse
 
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -23,6 +24,15 @@ import httparchive
 # Regex used to parse httparchive.py stdout's when listing all urls.
 _PARSE_WPR_REQUEST_REGEX = re.compile(r'^\S+\s+(?P<url>\S+)')
 
+# Regex used to extract WPR domain from WPR log.
+_PARSE_WPR_DOMAIN_REGEX = re.compile(r'^\(WARNING\)\s.*\sHTTP server started on'
+                                     r' (?P<netloc>\S+)\s*$')
+
+# Regex used to extract URLs requests from WPR log.
+_PARSE_WPR_URL_REGEX = re.compile(
+    r'^\((?P<level>\S+)\)\s.*\shttpproxy\..*\s(?P<method>[A-Z]+)\s+'
+    r'(?P<url>https?://[a-zA-Z0-9\-_:.]+/?\S*)\s.*$')
+
 
 class WprUrlEntry(object):
   """Wpr url entry holding request and response infos. """
@@ -114,6 +124,51 @@ class WprArchiveBackend(object):
     self._http_archive.Persist(self._wpr_archive_path)
 
 
+# WPR request seen by the WPR's HTTP proxy.
+#   is_served: Boolean whether WPR has found a matching resource in the archive.
+#   method: HTTP method of the request ['GET', 'POST' and so on...].
+#   url: The requested URL.
+#   is_wpr_host: Whether the requested url have WPR has an host such as:
+#     http://127.0.0.1:<WPR's HTTP listening port>/web-page-replay-command-exit
+WprRequest = collections.namedtuple('WprRequest',
+    ['is_served', 'method', 'url', 'is_wpr_host'])
+
+
+def ExtractRequestsFromLog(log_path):
+  """Extract list of requested handled by the WPR's HTTP proxy from a WPR log.
+
+  Args:
+    log_path: The path of the WPR log to parse.
+
+  Returns:
+    List of WprRequest.
+  """
+  requests = []
+  wpr_http_netloc = None
+  with open(log_path) as log_file:
+    for line in log_file.readlines():
+      # Extract WPR's HTTP proxy's listening network location.
+      match = _PARSE_WPR_DOMAIN_REGEX.match(line)
+      if match:
+        wpr_http_netloc = match.group('netloc')
+        assert wpr_http_netloc.startswith('127.0.0.1:')
+        continue
+      # Extract the WPR requested URLs.
+      match = _PARSE_WPR_URL_REGEX.match(line)
+      if match:
+        parsed_url = urlparse(match.group('url'))
+        # Ignore strange URL requests such as http://ousvtzkizg/
+        # TODO(gabadie): Find and terminate the location where they are queried.
+        if '.' not in parsed_url.netloc and ':' not in parsed_url.netloc:
+          continue
+        assert wpr_http_netloc
+        request = WprRequest(is_served=(match.group('level') == 'DEBUG'),
+            method=match.group('method'), url=match.group('url'),
+            is_wpr_host=parsed_url.netloc == wpr_http_netloc)
+        requests.append(request)
+  return requests
+
+
 if __name__ == '__main__':
   import argparse
   parser = argparse.ArgumentParser(description='Tests cache back-end.')
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
index 3cdf581..f744c10 100644
--- a/loading/wpr_backend_unittest.py
+++ b/loading/wpr_backend_unittest.py
@@ -2,9 +2,20 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import contextlib
+import httplib
+import os
+import shutil
+import tempfile
 import unittest
 
-from wpr_backend import WprUrlEntry
+from device_setup import _WprHost
+from options import OPTIONS
+from trace_test.webserver_test import WebServer
+from wpr_backend import WprUrlEntry, WprRequest, ExtractRequestsFromLog
+
+
+LOADING_DIR = os.path.dirname(__file__)
 
 
 class MockWprResponse(object):
@@ -122,5 +133,123 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals(2, len(entry.GetResponseHeadersDict()))
 
 
+class WprHostTest(unittest.TestCase):
+  def setUp(self):
+    OPTIONS.ParseArgs([])
+    self._server_address = None
+    self._wpr_http_port = None
+    self._tmp_directory = tempfile.mkdtemp(prefix='tmp_test_')
+
+  def tearDown(self):
+    shutil.rmtree(self._tmp_directory)
+
+  def _TmpPath(self, name):
+    return os.path.join(self._tmp_directory, name)
+
+  def _LogPath(self):
+    return self._TmpPath('wpr.log')
+
+  def _ArchivePath(self):
+    return self._TmpPath('wpr')
+
+  @contextlib.contextmanager
+  def RunWebServer(self):
+    assert self._server_address is None
+    with WebServer.Context(
+        source_dir=os.path.join(LOADING_DIR, 'trace_test', 'tests'),
+        communication_dir=self._tmp_directory) as server:
+      self._server_address = server.Address()
+      yield
+
+  @contextlib.contextmanager
+  def RunWpr(self, record):
+    assert self._server_address is not None
+    assert self._wpr_http_port is None
+    with _WprHost(self._ArchivePath(), record=record,
+                  out_log_path=self._LogPath()) as (http_port, https_port):
+      del https_port # unused
+      self._wpr_http_port = http_port
+      yield http_port
+
+  def DoHttpRequest(self, path, expected_status=200, destination='wpr'):
+    assert self._server_address is not None
+    if destination == 'wpr':
+      assert self._wpr_http_port is not None
+      connection = httplib.HTTPConnection('127.0.0.1', self._wpr_http_port)
+    elif destination == 'server':
+      connection = httplib.HTTPConnection(self._server_address)
+    else:
+      assert False
+    try:
+      connection.request(
+          "GET", '/' + path, headers={'Host': self._server_address})
+      response = connection.getresponse()
+    finally:
+      connection.close()
+    self.assertEquals(expected_status, response.status)
+
+  def _GenRawWprRequest(self, path):
+    assert self._wpr_http_port is not None
+    url = 'http://127.0.0.1:{}/web-page-replay-{}'.format(
+        self._wpr_http_port, path)
+    return WprRequest(is_served=True, method='GET', is_wpr_host=True, url=url)
+
+  def GenRawRequest(self, path, is_served):
+    assert self._server_address is not None
+    return WprRequest(is_served=is_served, method='GET', is_wpr_host=False,
+        url='http://{}/{}'.format(self._server_address, path))
+
+  def AssertWprParsedRequests(self, ref_requests):
+    all_ref_requests = []
+    all_ref_requests.append(self._GenRawWprRequest('generate-200'))
+    all_ref_requests.extend(ref_requests)
+    all_ref_requests.append(self._GenRawWprRequest('generate-200'))
+    all_ref_requests.append(self._GenRawWprRequest('command-exit'))
+    requests = ExtractRequestsFromLog(self._LogPath())
+    self.assertEquals(all_ref_requests, requests)
+    self._wpr_http_port = None
+
+  def testExtractRequestsFromLog(self):
+    with self.RunWebServer():
+      with self.RunWpr(record=True):
+        self.DoHttpRequest('1.html')
+        self.DoHttpRequest('2.html')
+        ref_requests = [
+            self.GenRawRequest('1.html', is_served=True),
+            self.GenRawRequest('2.html', is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+    with self.RunWpr(record=False):
+      self.DoHttpRequest('2.html')
+      self.DoHttpRequest('1.html')
+      ref_requests = [
+          self.GenRawRequest('2.html', is_served=True),
+          self.GenRawRequest('1.html', is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+  def testExtractRequestsFromLogHaveCorrectIsServed(self):
+    with self.RunWebServer():
+      with self.RunWpr(record=True):
+        self.DoHttpRequest('4.html', expected_status=404)
+        ref_requests = [self.GenRawRequest('4.html', is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+    with self.RunWpr(record=False):
+      self.DoHttpRequest('4.html', expected_status=404)
+      self.DoHttpRequest('5.html', expected_status=404)
+      ref_requests = [self.GenRawRequest('4.html', is_served=True),
+                      self.GenRawRequest('5.html', is_served=False)]
+    self.AssertWprParsedRequests(ref_requests)
+
+  def testExtractRequestsFromLogHaveCorrectIsWprHost(self):
+    PATH = 'web-page-replay-generate-200'
+    with self.RunWebServer():
+      self.DoHttpRequest(PATH, expected_status=404, destination='server')
+      with self.RunWpr(record=True):
+        self.DoHttpRequest(PATH)
+      ref_requests = [self.GenRawRequest(PATH, is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+
 if __name__ == '__main__':
   unittest.main()

commit 3ab408b5262299d9a8cf4d3f9a1c515885d2262f
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 29 04:11:29 2016 -0700

    sandwich: Remove deprecated sub-command and rename `run-all` to `run`
    
    Sandwich has been migrated to the task manager that is going to let
    us do more complex operations than the deprecated sub-commands that
    required to be executed in a sequence of different processes.
    
    This CL simply remove the old sub-commands to reduce test coverage
    and release us from non necessary constrains on further sandwich
    improvments.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1930023002
    Cr-Original-Commit-Position: refs/heads/master@{#390616}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bc8010bd66683defcad902d7b51868ed1c16538c

diff --git a/loading/sandwich.py b/loading/sandwich.py
index d7ec1a1..0b72c73 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -50,16 +50,14 @@ _MEMORY_MEASUREMENT = 'memory'
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
-  # Command parser when dealing with jobs.
-  common_job_parser = argparse.ArgumentParser(add_help=False)
-  common_job_parser.add_argument('--job', required=True,
-                                 help='JSON file with job description.')
-  common_job_parser.add_argument('--android', default=None, type=str,
-                                 dest='android_device_serial',
-                                 help='Android device\'s serial to use.')
-
   task_parser = task_manager.CommandLineParser()
 
+  # Command parser when dealing with SandwichRunner.
+  sandwich_runner_parser = argparse.ArgumentParser(add_help=False)
+  sandwich_runner_parser.add_argument('--android', default=None, type=str,
+                                      dest='android_device_serial',
+                                      help='Android device\'s serial to use.')
+
   # Plumbing parser to configure OPTIONS.
   plumbing_parser = OPTIONS.GetParentParser('plumbing options')
 
@@ -67,109 +65,12 @@ def _ArgumentParser():
   parser = argparse.ArgumentParser(parents=[plumbing_parser])
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
-  # Record WPR subcommand.
-  record_wpr = subparsers.add_parser('record-wpr', parents=[common_job_parser],
-                                     help='Record WPR from sandwich job.')
-  record_wpr.add_argument('--wpr-archive', required=True, type=str,
-                          dest='wpr_archive_path',
-                          help='Web page replay archive to generate.')
-
-  # Patch WPR subcommand.
-  patch_wpr = subparsers.add_parser('patch-wpr',
-                                     help='Patch WPR response headers.')
-  patch_wpr.add_argument('--wpr-archive', required=True, type=str,
-                         dest='wpr_archive_path',
-                         help='Web page replay archive to patch.')
-
-  # Create cache subcommand.
-  create_cache_parser = subparsers.add_parser('create-cache',
-      parents=[common_job_parser],
-      help='Create cache from sandwich job.')
-  create_cache_parser.add_argument('--cache-archive', required=True, type=str,
-                                   dest='cache_archive_path',
-                                   help='Cache archive destination path.')
-  create_cache_parser.add_argument('--wpr-archive', default=None, type=str,
-                                   dest='wpr_archive_path',
-                                   help='Web page replay archive to create ' +
-                                       'the cache from.')
-
-  # Run subcommand.
-  run_parser = subparsers.add_parser('run', parents=[common_job_parser],
-                                     help='Run sandwich benchmark.')
-  run_parser.add_argument('--output', required=True, type=str,
-                          dest='trace_output_directory',
-                          help='Path of output directory to create.')
-  run_parser.add_argument('--cache-archive', type=str,
-                          dest='cache_archive_path',
-                          help='Cache archive destination path.')
-  run_parser.add_argument('--cache-op',
-                          choices=['clear', 'push', 'reload'],
-                          dest='cache_operation',
-                          default='clear',
-                          help='Configures cache operation to do before '
-                              +'launching Chrome. (Default is clear). The push'
-                              +' cache operation requires --cache-archive to '
-                              +'set.')
-  run_parser.add_argument('--disable-wpr-script-injection',
-                          action='store_true',
-                          help='Disable WPR default script injection such as ' +
-                              'overriding javascript\'s Math.random() and ' +
-                              'Date() with deterministic implementations.')
-  run_parser.add_argument('--network-condition', default=None,
-      choices=sorted(emulation.NETWORK_CONDITIONS.keys()),
-      help='Set a network profile.')
-  run_parser.add_argument('--network-emulator', default='browser',
-      choices=['browser', 'wpr'],
-      help='Set which component is emulating the network condition.' +
-          ' (Default to browser). Wpr network emulator requires --wpr-archive' +
-          ' to be set.')
-  run_parser.add_argument('--job-repeat', default=1, type=int,
-                          help='How many times to run the job.')
-  run_parser.add_argument('--record-video', action='store_true',
-                          help='Configures either to record or not a video of '
-                              +'chrome loading the web pages.')
-  run_parser.add_argument('--wpr-archive', default=None, type=str,
-                          dest='wpr_archive_path',
-                          help='Web page replay archive to load job\'s urls ' +
-                              'from.')
-
-  # Pull metrics subcommand.
-  create_cache_parser = subparsers.add_parser('extract-metrics',
-      help='Extracts metrics from a loading trace and saves as CSV.')
-  create_cache_parser.add_argument('--trace-directory', required=True,
-                                   dest='trace_output_directory', type=str,
-                                   help='Path of loading traces directory.')
-  create_cache_parser.add_argument('--out-metrics', default=None, type=str,
-                                   dest='metrics_csv_path',
-                                   help='Path where to save the metrics\'s '+
-                                      'CSV.')
-
-  # Filter cache subcommand.
-  filter_cache_parser = subparsers.add_parser('filter-cache',
-      help='Cache filtering that keeps only resources discoverable by the HTML'+
-          ' document parser.')
-  filter_cache_parser.add_argument('--cache-archive', type=str, required=True,
-                                   dest='cache_archive_path',
-                                   help='Path of the cache archive to filter.')
-  filter_cache_parser.add_argument('--subresource-discoverer', required=True,
-      help='Strategy for populating the cache with a subset of resources, '
-           'according to the way they can be discovered',
-      choices=sandwich_misc.SUBRESOURCE_DISCOVERERS)
-  filter_cache_parser.add_argument('--output', type=str, required=True,
-                                   dest='output_cache_archive_path',
-                                   help='Path of filtered cache archive.')
-  filter_cache_parser.add_argument('loading_trace_paths', type=str, nargs='+',
-      metavar='LOADING_TRACE',
-      help='A list of loading traces generated by a sandwich run for a given' +
-          ' url. This is used to have a resource dependency graph to white-' +
-          'list the ones discoverable by the HTML pre-scanner for that given ' +
-          'url.')
-
   # Record test trace subcommand.
   record_trace_parser = subparsers.add_parser('record-test-trace',
+      parents=[sandwich_runner_parser],
       help='Record a test trace using the trace_test.webserver_test.')
   record_trace_parser.add_argument('--source-dir', type=str, required=True,
-                                   help='Base path where the files are opened'
+                                   help='Base path where the files are opened '
                                         'by the web server.')
   record_trace_parser.add_argument('--page', type=str, required=True,
                                    help='Source page in source-dir to navigate '
@@ -177,93 +78,38 @@ def _ArgumentParser():
   record_trace_parser.add_argument('-o', '--output', type=str, required=True,
                                    help='Output path of the generated trace.')
 
-  # Run all subcommand.
-  run_all = subparsers.add_parser('run-all',
-                       parents=[common_job_parser, task_parser],
-                       help='Run all steps using the task manager '
-                            'infrastructure.')
-  run_all.add_argument('-m', '--measure', default=[], dest='optional_measures',
-                       choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
-                       nargs='+', help='Enable optional measurements.')
-  run_all.add_argument('-g', '--gen-full', action='store_true',
-                       help='Generate the full graph with all possible'
-                            'benchmarks.')
-  run_all.add_argument('--wpr-archive', default=None, type=str,
-                       dest='wpr_archive_path',
-                       help='WebPageReplay archive to use, instead of '
-                            'generating one.')
-  run_all.add_argument('--url-repeat', default=1, type=int,
-                       help='How many times to repeat the urls.')
+  # Run subcommand.
+  run_parser = subparsers.add_parser('run',
+      parents=[sandwich_runner_parser, task_parser],
+      help='Run all steps using the task manager infrastructure.')
+  run_parser.add_argument('-g', '--gen-full', action='store_true',
+                          help='Generate the full graph with all possible '
+                               'benchmarks.')
+  run_parser.add_argument('--job', required=True,
+      help='JSON file with job description such as in sandwich_jobs/.')
+  run_parser.add_argument('-m', '--measure', default=[], nargs='+',
+      choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
+      dest='optional_measures', help='Enable optional measurements.')
+  run_parser.add_argument('--wpr-archive', default=None, type=str,
+                          dest='wpr_archive_path',
+                          help='WebPageReplay archive to use, instead of '
+                               'generating one.')
+  run_parser.add_argument('--url-repeat', default=1, type=int,
+                          help='How many times to repeat the urls.')
 
   return parser
 
 
-def _CreateSandwichRunner(args):
-  sandwich_runner = SandwichRunner()
-  sandwich_runner.LoadJob(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
-  if args.android_device_serial is not None:
-    sandwich_runner.android_device = \
-        device_setup.GetDeviceFromSerial(args.android_device_serial)
-  return sandwich_runner
-
-
-def _RecordWprMain(args):
-  sandwich_runner = _CreateSandwichRunner(args)
-  sandwich_runner.wpr_record = True
-  sandwich_runner.PrintConfig()
-  if not os.path.isdir(os.path.dirname(args.wpr_archive_path)):
-    os.makedirs(os.path.dirname(args.wpr_archive_path))
-  sandwich_runner.Run()
-  return 0
-
-
-def _CreateCacheMain(args):
-  sandwich_runner = _CreateSandwichRunner(args)
-  sandwich_runner.cache_operation = 'save'
-  sandwich_runner.PrintConfig()
-  if not os.path.isdir(os.path.dirname(args.cache_archive_path)):
-    os.makedirs(os.path.dirname(args.cache_archive_path))
-  sandwich_runner.Run()
-  return 0
-
-
-def _RunJobMain(args):
-  sandwich_runner = _CreateSandwichRunner(args)
-  sandwich_runner.PrintConfig()
-  sandwich_runner.Run()
-  return 0
-
-
-def _ExtractMetricsMain(args):
-  run_metrics_list = sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
-      args.trace_output_directory)
-  run_metrics_list.sort(key=lambda e: e['repeat_id'])
-  with open(args.metrics_csv_path, 'w') as csv_file:
-    writer = csv.DictWriter(csv_file,
-                            fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
-    writer.writeheader()
-    for run_metrics in run_metrics_list:
-      writer.writerow(run_metrics)
-  return 0
-
-
-def _FilterCacheMain(args):
-  whitelisted_urls = set()
-  for loading_trace_path in args.loading_trace_paths:
-    whitelisted_urls.update(sandwich_misc.ExtractDiscoverableUrls(
-        loading_trace_path, args.subresource_discoverer))
-  if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
-    os.makedirs(os.path.dirname(args.output_cache_archive_path))
-  chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
-                                               whitelisted_urls,
-                                               args.output_cache_archive_path)
-  return 0
+def _GetAndroidDeviceFromArgs(args):
+  if args.android_device_serial:
+    return device_setup.GetDeviceFromSerial(args.android_device_serial)
+  return None
 
 
 def _RecordWebServerTestTrace(args):
   with common_util.TemporaryDirectory() as out_path:
     sandwich_runner = SandwichRunner()
+    sandwich_runner.android_device = _GetAndroidDeviceFromArgs(args)
     # Reuse the WPR's forwarding to access the webpage from Android.
     sandwich_runner.wpr_record = True
     sandwich_runner.wpr_archive_path = os.path.join(out_path, 'wpr')
@@ -280,13 +126,9 @@ def _RecordWebServerTestTrace(args):
 
 
 def _RunAllMain(args):
-  android_device = None
-  if args.android_device_serial:
-    android_device = \
-        device_setup.GetDeviceFromSerial(args.android_device_serial)
   builder = sandwich_task_builder.SandwichTaskBuilder(
       output_directory=args.output,
-      android_device=android_device,
+      android_device=_GetAndroidDeviceFromArgs(args),
       job_path=args.job)
   if args.wpr_archive_path:
     builder.OverridePathToWprArchive(args.wpr_archive_path)
@@ -330,22 +172,9 @@ def main(command_line_args):
   args = _ArgumentParser().parse_args(command_line_args)
   OPTIONS.SetParsedArgs(args)
 
-  if args.subcommand == 'record-wpr':
-    return _RecordWprMain(args)
-  if args.subcommand == 'patch-wpr':
-    sandwich_misc.PatchWpr(args.wpr_archive_path)
-    return 0
-  if args.subcommand == 'create-cache':
-    return _CreateCacheMain(args)
-  if args.subcommand == 'run':
-    return _RunJobMain(args)
-  if args.subcommand == 'extract-metrics':
-    return _ExtractMetricsMain(args)
-  if args.subcommand == 'filter-cache':
-    return _FilterCacheMain(args)
   if args.subcommand == 'record-test-trace':
     return _RecordWebServerTestTrace(args)
-  if args.subcommand == 'run-all':
+  if args.subcommand == 'run':
     return _RunAllMain(args)
   assert False
 

commit ba3731d5f642416a1191cb0e2860606e4aa1a853
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 28 10:02:26 2016 -0700

    sandwich: Make speed-index and memory measurement optional from run-all
    
    Before, the run-all sub-command was not recording the speed-index
    video, and the memory measurements were always done using the memory
    dump trace events.
    
    Memory dump trace event are large and may impact loading performance
    on low-end devices. This CL makes memory measurement optional and
    also add the option to measure speed-index in the new command line
    flag --measure.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1925803003
    Cr-Original-Commit-Position: refs/heads/master@{#390407}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 413a55d3b89527353ae88aa8f9881b791379f652

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 6dda5d0..d7ec1a1 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -44,6 +44,9 @@ from trace_test.webserver_test import WebServer
 # Use options layer to access constants.
 OPTIONS = options.OPTIONS
 
+_SPEED_INDEX_MEASUREMENT = 'speed-index'
+_MEMORY_MEASUREMENT = 'memory'
+
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
@@ -179,6 +182,9 @@ def _ArgumentParser():
                        parents=[common_job_parser, task_parser],
                        help='Run all steps using the task manager '
                             'infrastructure.')
+  run_all.add_argument('-m', '--measure', default=[], dest='optional_measures',
+                       choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
+                       nargs='+', help='Enable optional measurements.')
   run_all.add_argument('-g', '--gen-full', action='store_true',
                        help='Generate the full graph with all possible'
                             'benchmarks.')
@@ -281,31 +287,37 @@ def _RunAllMain(args):
   builder = sandwich_task_builder.SandwichTaskBuilder(
       output_directory=args.output,
       android_device=android_device,
-      job_path=args.job,
-      url_repeat=args.url_repeat)
+      job_path=args.job)
   if args.wpr_archive_path:
     builder.OverridePathToWprArchive(args.wpr_archive_path)
   else:
     builder.PopulateWprRecordingTask()
   builder.PopulateCommonPipelines()
 
-  runner_transformer_name = 'no-network-emulation'
-  runner_transformer = lambda arg: None
+  def MainTransformer(runner):
+    runner.record_video = _SPEED_INDEX_MEASUREMENT in args.optional_measures
+    runner.record_memory_dumps = _MEMORY_MEASUREMENT in args.optional_measures
+    runner.job_repeat = args.url_repeat
+
+  transformer_list_name = 'no-network-emulation'
   builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
-                                runner_transformer_name, runner_transformer)
+                                transformer_list_name,
+                                transformer_list=[MainTransformer])
   builder.PopulateLoadBenchmark(sandwich_misc.FULL_CACHE_DISCOVERER,
-                                runner_transformer_name, runner_transformer)
+                                transformer_list_name,
+                                transformer_list=[MainTransformer])
 
   if args.gen_full:
-    for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
-      if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
-        continue
-      for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
-        runner_transformer_name = network_condition.lower()
-        runner_transformer = sandwich_task_builder.NetworkSimulationTransformer(
-            network_condition)
-        builder.PopulateLoadBenchmark(
-            subresource_discoverer, runner_transformer_name, runner_transformer)
+    for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
+      transformer_list_name = network_condition.lower()
+      network_transformer = \
+          sandwich_task_builder.NetworkSimulationTransformer(network_condition)
+      transformer_list = [MainTransformer, network_transformer]
+      for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
+        if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
+          continue
+        builder.PopulateLoadBenchmark(subresource_discoverer,
+            transformer_list_name, transformer_list)
 
   return task_manager.ExecuteWithCommandLine(
       args, builder.tasks.values(), builder.default_final_tasks)
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 1324b75..339cf72 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -43,6 +43,8 @@ CSV_FIELD_NAMES = [
     'net_emul.upload',
     'net_emul.latency']
 
+_UNAVAILABLE_CSV_VALUE = 'unavailable'
+
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
 # Points of a completeness record.
@@ -81,6 +83,7 @@ def _GetBrowserDumpEvents(tracing_track):
   Returns:
     List of memory dump events.
   """
+  assert sandwich_runner.MEMORY_DUMP_CATEGORY in tracing_track.Categories()
   browser_pid = _GetBrowserPID(tracing_track)
   browser_dumps_events = []
   for event in tracing_track.GetEvents():
@@ -130,8 +133,8 @@ def _GetWebPageTrackedEvents(tracing_track):
   return tracked_events
 
 
-def _ExtractMetricsFromLoadingTrace(loading_trace):
-  """Pulls all the metrics from a given trace.
+def _ExtractDefaultMetrics(loading_trace):
+  """Extracts all the default metrics from a given trace.
 
   Args:
     loading_trace: loading_trace_module.LoadingTrace.
@@ -139,15 +142,32 @@ def _ExtractMetricsFromLoadingTrace(loading_trace):
   Returns:
     Dictionary with all trace extracted fields set.
   """
-  assert all(
-      cat in loading_trace.tracing_track.Categories()
-      for cat in sandwich_runner.ADDITIONAL_CATEGORIES), (
-          'This trace was not generated with the required set of categories '
-          'to be processed by this script.')
-  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   web_page_tracked_events = _GetWebPageTrackedEvents(
       loading_trace.tracing_track)
+  return {
+    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
+                   web_page_tracked_events['requestStart'].start_msec),
+    'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
+                        web_page_tracked_events['loadEventStart'].start_msec)
+  }
+
+
+def _ExtractMemoryMetrics(loading_trace):
+  """Extracts all the memory metrics from a given trace.
+
+  Args:
+    loading_trace: loading_trace_module.LoadingTrace.
 
+  Returns:
+    Dictionary with all trace extracted fields set.
+  """
+  if (sandwich_runner.MEMORY_DUMP_CATEGORY not in
+          loading_trace.tracing_track.Categories()):
+    return {
+      'browser_malloc_avg': _UNAVAILABLE_CSV_VALUE,
+      'browser_malloc_max': _UNAVAILABLE_CSV_VALUE
+    }
+  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   browser_malloc_sum = 0
   browser_malloc_max = 0
   for dump_event in browser_dump_events:
@@ -156,12 +176,7 @@ def _ExtractMetricsFromLoadingTrace(loading_trace):
     size = int(attr['value'], 16)
     browser_malloc_sum += size
     browser_malloc_max = max(browser_malloc_max, size)
-
   return {
-    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
-                   web_page_tracked_events['requestStart'].start_msec),
-    'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
-                        web_page_tracked_events['loadEventStart'].start_msec),
     'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
     'browser_malloc_max': browser_malloc_max
   }
@@ -239,14 +254,15 @@ def _ExtractMetricsFromRunDirectory(run_directory_path):
   logging.info('processing trace \'%s\'' % trace_path)
   loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
   run_metrics = {'url': loading_trace.url}
-  run_metrics.update(_ExtractMetricsFromLoadingTrace(loading_trace))
+  run_metrics.update(_ExtractDefaultMetrics(loading_trace))
+  run_metrics.update(_ExtractMemoryMetrics(loading_trace))
   video_path = os.path.join(run_directory_path, 'video.mp4')
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
     completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
     run_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
   else:
-    run_metrics['speed_index'] = 'disabled'
+    run_metrics['speed_index'] = _UNAVAILABLE_CSV_VALUE
   for key, value in loading_trace.metadata['network_emulation'].iteritems():
     run_metrics['net_emul.' + key] = value
   return run_metrics
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 73c8f3e..e7553bc 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -19,7 +19,7 @@ import tracing
 
 
 _BLINK_CAT = 'blink.user_timing'
-_MEM_CAT = 'disabled-by-default-memory-infra'
+_MEM_CAT = sandwich_runner.MEMORY_DUMP_CATEGORY
 _START='requestStart'
 _LOADS='loadEventStart'
 _LOADE='loadEventEnd'
@@ -48,7 +48,7 @@ def TracingTrack(events):
   return tracing.TracingTrack.FromJsonDict({
       'events': events,
       'categories': (tracing.INITIAL_CATEGORIES +
-          sandwich_runner.ADDITIONAL_CATEGORIES)})
+          (sandwich_runner.MEMORY_DUMP_CATEGORY,))})
 
 
 def LoadingTrace(events):
@@ -107,8 +107,6 @@ class PageTrackTest(unittest.TestCase):
         {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
         {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
-    self.assertTrue(_MEM_CAT in sandwich_runner.ADDITIONAL_CATEGORIES)
-
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
     self.assertEquals(5, bump_events[0].start_msec)
@@ -187,12 +185,17 @@ class PageTrackTest(unittest.TestCase):
     self.assertEquals(17, trace_events['loadEventStart'].start_msec)
     self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
 
-  def testPullMetricsFromLoadingTrace(self):
-    metrics = puller._ExtractMetricsFromLoadingTrace(LoadingTrace(
+  def testExtractDefaultMetrics(self):
+    metrics = puller._ExtractDefaultMetrics(LoadingTrace(
         _MINIMALIST_TRACE_EVENTS))
-    self.assertEquals(4, len(metrics))
+    self.assertEquals(2, len(metrics))
     self.assertEquals(20, metrics['total_load'])
     self.assertEquals(5, metrics['js_onload_event'])
+
+  def testExtractMemoryMetrics(self):
+    metrics = puller._ExtractMemoryMetrics(LoadingTrace(
+        _MINIMALIST_TRACE_EVENTS))
+    self.assertEquals(2, len(metrics))
     self.assertEquals(30971, metrics['browser_malloc_avg'])
     self.assertEquals(55044, metrics['browser_malloc_max'])
 
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 3a56b59..3dc0415 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -26,9 +26,8 @@ import loading_trace
 TRACE_FILENAME = 'trace.json'
 VIDEO_FILENAME = 'video.mp4'
 
-# List of selected trace event categories when running chrome.
-ADDITIONAL_CATEGORIES = (
-    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
+# Memory dump category used to get memory metrics.
+MEMORY_DUMP_CATEGORY = 'disabled-by-default-memory-infra'
 
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
@@ -124,6 +123,9 @@ class SandwichRunner(object):
     # Configures whether to record speed-index video.
     self.record_video = False
 
+    # Configures whether to record memory dumps.
+    self.record_memory_dumps = False
+
     # Path to the WPR archive to load or save. Is str or None.
     self.wpr_archive_path = None
 
@@ -189,13 +191,17 @@ class SandwichRunner(object):
         os.makedirs(run_path)
     self._chrome_ctl.SetNetworkEmulation(
         self._GetEmulatorNetworkCondition('browser'))
+    additional_categories = []
+    if self.record_memory_dumps:
+      additional_categories = [MEMORY_DUMP_CATEGORY]
     # TODO(gabadie): add a way to avoid recording a trace.
     with self._chrome_ctl.Open() as connection:
       if clear_cache:
         connection.ClearCache()
       if run_path is not None and self.record_video:
         device = self._chrome_ctl.GetDevice()
-        assert device, 'Can only record video on a remote device.'
+        if device is None:
+          raise RuntimeError('Can only record video on a remote device.')
         video_recording_path = os.path.join(run_path, VIDEO_FILENAME)
         with device_setup.RemoteSpeedIndexRecorder(device, connection,
                                                    video_recording_path):
@@ -203,14 +209,14 @@ class SandwichRunner(object):
               url=url,
               connection=connection,
               chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              additional_categories=ADDITIONAL_CATEGORIES,
+              additional_categories=additional_categories,
               timeout_seconds=_DEVTOOLS_TIMEOUT)
       else:
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
             url=url,
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            additional_categories=ADDITIONAL_CATEGORIES,
+            additional_categories=additional_categories,
             timeout_seconds=_DEVTOOLS_TIMEOUT)
     if run_path is not None:
       trace_path = os.path.join(run_path, TRACE_FILENAME)
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 954c67e..ea763ec 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -37,7 +37,7 @@ class SandwichTaskBuilder(task_manager.Builder):
   """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
   """
 
-  def __init__(self, output_directory, android_device, job_path, url_repeat):
+  def __init__(self, output_directory, android_device, job_path):
     """Constructor.
 
     Args:
@@ -45,13 +45,10 @@ class SandwichTaskBuilder(task_manager.Builder):
       android_device: The android DeviceUtils to run sandwich on or None to run
         it locally.
       job_path: Path of the sandwich's job.
-      url_repeat: Non null integer controlling how many times the URLs should be
-        repeated in the benchmarks.
     """
     task_manager.Builder.__init__(self, output_directory)
     self._android_device = android_device
     self._job_path = job_path
-    self._url_repeat = url_repeat
     self._default_final_tasks = []
 
     self._original_wpr_task = None
@@ -159,20 +156,20 @@ class SandwichTaskBuilder(task_manager.Builder):
     return ValidateReferenceCache
 
   def PopulateLoadBenchmark(self, subresource_discoverer,
-                            runner_transformer_name, runner_transformer):
+                            transformer_list_name, transformer_list):
     """Populate benchmarking tasks from its setup tasks.
 
     Args:
       subresource_discoverer: Name of a subresources discoverer.
-      runner_transformer: A function that takes an instance of SandwichRunner as
-          parameter, would be applied immediately before SandwichRunner.Run().
-      runner_transformer_name: Name of the runner transformer used to generate
-          task names.
-      benchmark_name: The benchmark's name for that runner modifier.
+      transformer_list_name: A string describing the transformers, will be used
+          in Task names (prefer names without spaces and special characters).
+      transformer_list: An ordered list of function that takes an instance of
+          SandwichRunner as parameter, would be applied immediately before
+          SandwichRunner.Run() in the given order.
 
     Here is the full dependency of the added tree for the returned task:
-    <runner_transformer_name>/<subresource_discoverer>-metrics.csv
-      depends on: <runner_transformer_name>/<subresource_discoverer>-run/
+    <transformer_list_name>/<subresource_discoverer>-metrics.csv
+      depends on: <transformer_list_name>/<subresource_discoverer>-run/
         depends on: common/<subresource_discoverer>-cache.zip
           depends on: some tasks saved by PopulateCommonPipelines()
           depends on: common/<subresource_discoverer>-setup.json
@@ -180,12 +177,12 @@ class SandwichTaskBuilder(task_manager.Builder):
 
     Returns:
       task_manager.Task for
-          <runner_transformer_name>/<subresource_discoverer>-metrics.csv
+          <transformer_list_name>/<subresource_discoverer>-metrics.csv
     """
     assert subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS
     assert 'common' not in sandwich_misc.SUBRESOURCE_DISCOVERERS
     shared_task_prefix = os.path.join('common', subresource_discoverer)
-    task_prefix = os.path.join(runner_transformer_name, subresource_discoverer)
+    task_prefix = os.path.join(transformer_list_name, subresource_discoverer)
 
     @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
                        dependencies=[self._subresources_for_urls_task])
@@ -221,9 +218,8 @@ class SandwichTaskBuilder(task_manager.Builder):
                        dependencies=[BuildBenchmarkCacheArchive])
     def RunBenchmark():
       runner = self._CreateSandwichRunner()
-      # runner.record_video = True
-      runner.job_repeat = self._url_repeat
-      runner_transformer(runner)
+      for transformer in transformer_list:
+        transformer(runner)
       runner.wpr_archive_path = self._patched_wpr_task.path
       runner.wpr_out_log_path = os.path.join(RunBenchmark.path, 'wpr.log')
       runner.cache_archive_path = BuildBenchmarkCacheArchive.path

commit e5560ea60ce190f3e9809216390a96e8a6060da1
Author: droger <droger@chromium.org>
Date:   Thu Apr 28 08:06:14 2016 -0700

    tools/android/loading Send email notification when the task is finished
    
    Review-Url: https://codereview.chromium.org/1926693004
    Cr-Original-Commit-Position: refs/heads/master@{#390381}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b8ecf4fd6e15fe92dcf0608b8c997f919b5d6a67

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index ffef8be..42b7b09 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -14,6 +14,7 @@ from oauth2client.client import GoogleCredentials
 
 from common.clovis_task import ClovisTask
 import common.google_instance_helper
+import email_helper
 from memory_logs import MemoryLogs
 
 
@@ -33,7 +34,7 @@ def Render(message, memory_logs):
       'log.html', body=message, log=memory_logs.Flush().split('\n'))
 
 
-def PollWorkers(tag, start_time, timeout_hours):
+def PollWorkers(tag, start_time, timeout_hours, email_address, task_url):
   """Checks if there are workers associated with tag, by polling the instance
   group. When all workers are finished, the instance group and the instance
   template are destroyed.
@@ -46,10 +47,12 @@ def PollWorkers(tag, start_time, timeout_hours):
     start_time (float): Time when the polling started, as returned by
                         time.time().
     timeout_hours (int): Timeout after which workers are terminated.
+    email_address (str): Email address to notify when the task is complete.
+    task_url (str): URL where the results of the task can be found.
   """
   if (time.time() - start_time) > (3600 * timeout_hours):
     clovis_logger.error('Worker timeout for tag %s, shuting down.' % tag)
-    deferred.defer(DeleteInstanceGroup, tag)
+    Finalize(tag, email_address, 'TIMEOUT', task_url)
     return
 
   clovis_logger.info('Polling workers for tag: ' + tag)
@@ -60,10 +63,26 @@ def PollWorkers(tag, start_time, timeout_hours):
   if live_instance_count > 0 or live_instance_count == -1:
     clovis_logger.info('Retry later, instances still alive for tag: ' + tag)
     poll_interval_minutes = 10
-    deferred.defer(PollWorkers, tag, start_time,
+    deferred.defer(PollWorkers, tag, start_time, email_address, task_url,
                    _countdown=(60 * poll_interval_minutes))
     return
 
+  Finalize(tag, email_address, 'SUCCESS', task_url)
+
+
+def Finalize(tag, email_address, status, task_url):
+  """Cleans up the remaining ComputeEngine resources and notifies the user.
+
+  Args:
+    tag (str): Tag of the task to finalize.
+    email_address (str): Email address of the user to be notified.
+    status (str): Status of the task, indicating the success or the cause of
+                  failure.
+    task_url (str): URL where the results of the task can be found.
+  """
+  email_helper.SendEmailTaskComplete(
+      to_address=email_address, tag=tag, status=status, task_url=task_url,
+      logger=clovis_logger)
   clovis_logger.info('Scheduling instance group destruction for tag: ' + tag)
   deferred.defer(DeleteInstanceGroup, tag)
 
@@ -142,7 +161,11 @@ def StartFromJsonString(http_body_str):
 
   # Split the task in smaller tasks.
   sub_tasks = []
+  task_url = None
   if task.Action() == 'trace':
+    bucket = task.BackendParams().get('storage_bucket')
+    if bucket:
+      task_url = 'https://console.cloud.google.com/storage/' + bucket
     sub_tasks = SplitTraceTask(task)
   else:
     error_string = 'Unsupported action: %s.' % task.Action()
@@ -160,10 +183,14 @@ def StartFromJsonString(http_body_str):
   clovis_logger.info('Creating worker polling task.')
   first_poll_delay_minutes = 10
   timeout_hours = task.BackendParams().get('timeout_hours', 5)
-  deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours,
-                 _countdown=(60 * first_poll_delay_minutes))
-
-  return Render('Success', memory_logs)
+  user_email = email_helper.GetUserEmail()
+  deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours, user_email,
+                 task_url, _countdown=(60 * first_poll_delay_minutes))
+
+  return Render(flask.Markup(
+      'Success!<br>Your task %s has started.<br>'
+      'You will be notified at %s when completed.') % (task_tag, user_email),
+      memory_logs)
 
 
 def SplitTraceTask(task):
diff --git a/loading/cloud/frontend/email_helper.py b/loading/cloud/frontend/email_helper.py
new file mode 100644
index 0000000..dd82ee7
--- /dev/null
+++ b/loading/cloud/frontend/email_helper.py
@@ -0,0 +1,42 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from google.appengine.api import (mail, users)
+
+
+def GetUserEmail():
+  """Returns the email address of the user currently making the request or None.
+  """
+  user = users.get_current_user()
+  if user:
+    return user.email()
+  return None
+
+
+def SendEmailTaskComplete(to_address, tag, status, task_url, logger):
+  """Sends an email to to_address notifying that the task identified by tag is
+  complete.
+
+  Args:
+    to_address (str): The email address to notify.
+    tag (str): The tag of the task.
+    status (str): Status of the task.
+    task_url (str): URL where the results of the task can be found.
+    logger (logging.logger): Used for logging.
+  """
+  if not to_address:
+    logger.error('No email address to notify for task ' + tag)
+    return
+
+  logger.info('Notify task %s complete to %s.' % (tag, to_address))
+  # The sender address must be in the "Email API authorized senders", configured
+  # in the Application Settings of AppEngine.
+  sender_address = 'clovis-noreply@google.com'
+  subject = 'Task %s complete' % tag
+  body = 'Your Clovis task %s is now complete with status: %s.' % (tag, status)
+  if task_url:
+    body += '\nCheck the results at ' + task_url
+  mail.send_mail(sender=sender_address, to=to_address, subject=subject,
+                 body=body)
+

commit bf86f9bc6f1d4d7f8fdb6d746bc842a20b2599eb
Author: mattcary <mattcary@chromium.org>
Date:   Thu Apr 28 05:47:26 2016 -0700

    Clovis: contentful paint upgrades.
    
    Use last contentful paint event for timing, and emit request fingerprints so
    that critical resources can be collated across runs. This adds the implied lens
    that defines critical resources from a fingerprint list.
    
    Review-Url: https://codereview.chromium.org/1888343003
    Cr-Original-Commit-Position: refs/heads/master@{#390359}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 89dbec666c4357c58b3b7ba748cfb842059f90b8

diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 0df9025..cbd882a 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -52,7 +52,7 @@ class PrefetchSimulationView(object):
     if trace is None:
       return
     requests = trace.request_track.GetEvents()
-    critical_requests_ids = user_lens.CriticalRequests()
+    critical_requests_ids = user_lens.CriticalRequestIds()
     self.postload_msec = user_lens.PostloadTimeMsec()
     self.graph = dependency_graph.RequestDependencyGraph(
         requests, dependencies_lens, node_class=RequestNode)
diff --git a/loading/request_track.py b/loading/request_track.py
index b4e3a15..8bd62da 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -12,6 +12,7 @@ import collections
 import copy
 import datetime
 import email.utils
+import hashlib
 import json
 import logging
 import re
@@ -213,6 +214,12 @@ class Request(object):
       return None
     return self.start_msec + self.timing.LargestOffset()
 
+  @property
+  def fingerprint(self):
+    h = hashlib.sha256()
+    h.update(self.url)
+    return h.hexdigest()[:10]
+
   def _TimestampOffsetFromStartMs(self, timestamp):
     assert self.timing.request_time != -1
     request_time = self.timing.request_time
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 8f61969..10385b5 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -221,7 +221,7 @@ class MockConnection(object):
     return response
 
 
-class MockUserSatisfiedLens(user_satisfied_lens._UserSatisfiedLens):
+class MockUserSatisfiedLens(user_satisfied_lens._FirstEventLens):
   def _CalculateTimes(self, _):
     self._satisfied_msec = float('inf')
     self._event_msec = float('inf')
diff --git a/loading/tracing.py b/loading/tracing.py
index 068c5b6..55009a6 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -92,10 +92,26 @@ class TracingTrack(devtools_monitor.Track):
 
   def GetMatchingMainFrameEvents(self, category, name):
     """Gets events matching |category| and |name| that occur in the main frame.
-    Assumes that the events in question have a 'frame' key in their |args|."""
+
+    Events without a 'frame' key in their |args| are discarded.
+    """
     matching_events = self.GetMatchingEvents(category, name)
     return [e for e in matching_events
-        if e.args['frame'] == self._GetMainFrameID()]
+        if 'frame' in e.args and e.args['frame'] == self.GetMainFrameID()]
+
+  def GetMainFrameID(self):
+    """Returns the main frame ID."""
+    if not self._main_frame_id:
+      navigation_start_events = [e for e in self.GetEvents()
+          if e.Matches('blink.user_timing', 'navigationStart')]
+      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
+      self._main_frame_id = first_event.args['frame']
+
+    return self._main_frame_id
+
+  def SetMainFrameID(self, frame_id):
+    """Set the main frame ID. Normally this is used only for testing."""
+    self._main_frame_id = frame_id
 
   def EventsAt(self, msec):
     """Gets events active at a timestamp.
@@ -197,16 +213,6 @@ class TracingTrack(devtools_monitor.Track):
         return event
     return None
 
-  def _GetMainFrameID(self):
-    """Returns the main frame ID."""
-    if not self._main_frame_id:
-      navigation_start_events = [e for e in self.GetEvents()
-          if e.Matches('blink.user_timing', 'navigationStart')]
-      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
-      self._main_frame_id = first_event.args['frame']
-
-    return self._main_frame_id
-
   def _IndexEvents(self, strict=False):
     if self._interval_tree:
       return
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index cce5809..95acef8 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -309,7 +309,7 @@ class TracingTrackTestCase(unittest.TestCase):
          'args': {'frame': _MAIN_FRAME_ID}},
         ]
     self._HandleEvents(events)
-    self.assertEquals(_MAIN_FRAME_ID, self.track._GetMainFrameID())
+    self.assertEquals(_MAIN_FRAME_ID, self.track.GetMainFrameID())
 
   def testGetMatchingEvents(self):
     _MAIN_FRAME_ID = 0xffff
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index dd99d79..523a240 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -6,6 +6,9 @@
 
 Several lenses are defined, for example FirstTextPaintLens and
 FirstSignificantPaintLens.
+
+When run from the command line, takes a lens name and a trace, and prints the
+fingerprints of the critical resources to stdout.
 """
 import logging
 import operator
@@ -24,6 +27,53 @@ class _UserSatisfiedLens(object):
   _ATTRS = ['_satisfied_msec', '_event_msec', '_postload_msec',
             '_critical_request_ids']
 
+  def CriticalRequests(self):
+    """Critical requests.
+
+    Returns:
+      A sequence of request_track.Request objects representing an estimate of
+      all requests that are necessary for the user satisfaction defined by this
+      class.
+    """
+    raise NotImplementedError
+
+  def CriticalRequestIds(self):
+    """Ids of critical requests."""
+    return set(rq.request_id for rq in self.CriticalRequests())
+
+  def CriticalFingerprints(self):
+    """Fingerprints of critical requests."""
+    return set(rq.fingerprint for rq in self.CriticalRequests())
+
+  def PostloadTimeMsec(self):
+    """Return postload time.
+
+    The postload time is an estimate of the amount of time needed by chrome to
+    transform the critical results into the satisfying event.
+
+    Returns:
+      Postload time in milliseconds.
+    """
+    return 0
+
+
+class RequestFingerprintLens(_UserSatisfiedLens):
+  """A lens built using requests in a trace that match a set of fingerprints."""
+  def __init__(self, trace, fingerprints):
+    fingerprints = set(fingerprints)
+    self._critical_requests = [rq for rq in trace.request_track.GetEvents()
+                               if rq.fingerprint in fingerprints]
+
+  def CriticalRequests(self):
+    """Ids of critical requests."""
+    return set(self._critical_requests)
+
+
+class _FirstEventLens(_UserSatisfiedLens):
+  """Helper abstract subclass that defines users first event manipulations."""
+  # pylint can't handle abstract subclasses.
+  # pylint: disable=abstract-method
+
   def __init__(self, trace):
     """Initialize the lens.
 
@@ -36,34 +86,23 @@ class _UserSatisfiedLens(object):
     self._critical_request_ids = None
     if trace is None:
       return
-    self._CalculateTimes(trace.tracing_track)
-    critical_requests = self._RequestsBefore(
+    self._CalculateTimes(trace)
+    self._critical_requests = self._RequestsBefore(
         trace.request_track, self._satisfied_msec)
-    self._critical_request_ids = set(rq.request_id for rq in critical_requests)
-    if critical_requests:
-      last_load = max(rq.end_msec for rq in critical_requests)
+    self._critical_request_ids = set(rq.request_id
+                                     for rq in self._critical_requests)
+    if self._critical_requests:
+      last_load = max(rq.end_msec for rq in self._critical_requests)
     else:
       last_load = float('inf')
     self._postload_msec = self._event_msec - last_load
 
   def CriticalRequests(self):
-    """Request ids of critical requests.
-
-    Returns:
-      A set of request ids (as strings) of an estimate of all requests that are
-      necessary for the user satisfaction defined by this class.
-    """
-    return self._critical_request_ids
+    """Override."""
+    return self._critical_requests
 
   def PostloadTimeMsec(self):
-    """Return postload time.
-
-    The postload time is an estimate of the amount of time needed by chrome to
-    transform the critical results into the satisfying event.
-
-    Returns:
-      Postload time in milliseconds.
-    """
+    """Override."""
     return self._postload_msec
 
   def ToJsonDict(self):
@@ -75,7 +114,7 @@ class _UserSatisfiedLens(object):
     return common_util.DeserializeAttributesFromJsonDict(
         json_dict, result, cls._ATTRS)
 
-  def _CalculateTimes(self, tracing_track):
+  def _CalculateTimes(self, trace):
     """Subclasses should implement to set _satisfied_msec and _event_msec."""
     raise NotImplementedError
 
@@ -84,26 +123,19 @@ class _UserSatisfiedLens(object):
     return [rq for rq in request_track.GetEvents()
             if rq.end_msec <= time_ms]
 
-
-class _FirstEventLens(_UserSatisfiedLens):
-  """Helper abstract subclass that defines users first event manipulations."""
-  # pylint can't handle abstract subclasses.
-  # pylint: disable=abstract-method
-
   @classmethod
   def _CheckCategory(cls, tracing_track, category):
     assert category in tracing_track.Categories(), (
         'The "%s" category must be enabled.' % category)
 
   @classmethod
-  def _ExtractFirstTiming(cls, times):
+  def _ExtractBestTiming(cls, times):
     if not times:
       return float('inf')
-    if len(times) != 1:
-      # TODO(mattcary): in some cases a trace has two first paint events. Why?
-      logging.error('%d %s with spread of %s', len(times),
-                    str(cls), max(times) - min(times))
-    return float(min(times))
+    assert len(times) == 1, \
+        'Unexpected duplicate {}: {} with spread of {}'.format(
+            str(cls), len(times), max(times) - min(times))
+    return float(max(times))
 
 
 class FirstTextPaintLens(_FirstEventLens):
@@ -112,12 +144,13 @@ class FirstTextPaintLens(_FirstEventLens):
   This event is taken directly from a trace.
   """
   _EVENT_CATEGORY = 'blink.user_timing'
-  def _CalculateTimes(self, tracing_track):
-    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
-    first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches(self._EVENT_CATEGORY, 'firstPaint')]
+  def _CalculateTimes(self, trace):
+    self._CheckCategory(trace.tracing_track, self._EVENT_CATEGORY)
+    first_paints = [
+        e.start_msec for e in trace.tracing_track.GetMatchingMainFrameEvents(
+            'blink.user_timing', 'firstPaint')]
     self._satisfied_msec = self._event_msec = \
-        self._ExtractFirstTiming(first_paints)
+        self._ExtractBestTiming(first_paints)
 
 
 class FirstContentfulPaintLens(_FirstEventLens):
@@ -127,12 +160,13 @@ class FirstContentfulPaintLens(_FirstEventLens):
   by filtering out things like background paint from firstPaint.
   """
   _EVENT_CATEGORY = 'blink.user_timing'
-  def _CalculateTimes(self, tracing_track):
-    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
-    first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches(self._EVENT_CATEGORY, 'firstContentfulPaint')]
+  def _CalculateTimes(self, trace):
+    self._CheckCategory(trace.tracing_track, self._EVENT_CATEGORY)
+    first_paints = [
+        e.start_msec for e in trace.tracing_track.GetMatchingMainFrameEvents(
+            'blink.user_timing', 'firstContentfulPaint')]
     self._satisfied_msec = self._event_msec = \
-       self._ExtractFirstTiming(first_paints)
+       self._ExtractBestTiming(first_paints)
 
 
 class FirstSignificantPaintLens(_FirstEventLens):
@@ -143,12 +177,18 @@ class FirstSignificantPaintLens(_FirstEventLens):
   that is the observable event.
   """
   _FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
-  _EVENT_CATEGORY = 'disabled-by-default-blink.debug.layout'
-  def _CalculateTimes(self, tracing_track):
-    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
+  _EVENT_CATEGORIES = ['blink', 'disabled-by-default-blink.debug.layout']
+  def _CalculateTimes(self, trace):
+    for cat in self._EVENT_CATEGORIES:
+      self._CheckCategory(trace.tracing_track, cat)
     sync_paint_times = []
     layouts = []  # (layout item count, msec).
-    for e in tracing_track.GetEvents():
+    for e in trace.tracing_track.GetEvents():
+      if ('frame' in e.args and
+          e.args['frame'] != trace.tracing_track.GetMainFrameID()):
+        continue
+      # If we don't know have a frame id, we assume it applies to all events.
+
       # TODO(mattcary): is this the right paint event? Check if synchronized
       # paints appear at the same time as the first*Paint events, above.
       if e.Matches('blink', 'FrameView::synchronizedPaint'):
@@ -158,7 +198,25 @@ class FirstSignificantPaintLens(_FirstEventLens):
         layouts.append((e.args['counters'][self._FIRST_LAYOUT_COUNTER],
                         e.start_msec))
     assert layouts, 'No layout events'
+    assert sync_paint_times,'No sync paint times'
     layouts.sort(key=operator.itemgetter(0), reverse=True)
     self._satisfied_msec = layouts[0][1]
-    self._event_msec = self._ExtractFirstTiming([
-        min(t for t in sync_paint_times if t > self._satisfied_msec)])
+    self._event_msec = min(t for t in sync_paint_times
+                           if t > self._satisfied_msec)
+
+
+def main(lens_name, trace_file):
+  assert (lens_name in globals() and
+          not lens_name.startswith('_') and
+          lens_name.endswith('Lens')), 'Bad lens %s' % lens_name
+  lens_cls = globals()[lens_name]
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_file)
+  lens = lens_cls(trace)
+  for fp in sorted(lens.CriticalFingerprints()):
+    print fp
+
+
+if __name__ == '__main__':
+  import sys
+  import loading_trace
+  main(sys.argv[1], sys.argv[2])
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index c7cdb10..c603bbe 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -32,51 +32,65 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     return rq
 
   def testFirstContentfulPaintLens(self):
+    MAINFRAME = 1
+    SUBFRAME = 2
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'blink.some_other_user_timing',
                        'name': 'firstContentfulPaint'},
-                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstDiscontentPaint'},
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint'},
-                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': SUBFRAME} },
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint'}])
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testCantGetNoSatisfaction(self):
+    MAINFRAME = 1
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'not_my_cat',
-                       'name': 'someEvent'}])
+                       'name': 'someEvent',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequestIds())
     self.assertEqual(float('inf'), lens.PostloadTimeMsec())
 
   def testFirstTextPaintLens(self):
+    MAINFRAME = 1
+    SUBFRAME = 2
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'blink.some_other_user_timing',
                        'name': 'firstPaint'},
-                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstishPaint'},
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'name': 'firstishPaint',
+                       'args': {'frame': MAINFRAME}},
+                      {'ts': 3 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstPaint'},
-                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'name': 'firstPaint',
+                       'args': {'frame': SUBFRAME}},
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstPaint'}])
+                       'name': 'firstPaint',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstTextPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testFirstSignificantPaintLens(self):
@@ -112,9 +126,37 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
                            'LayoutObjectsThatHadNeverHadLayout': 10
                        } } } ])
     lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(7, lens.PostloadTimeMsec())
 
+  def testRequestFingerprintLens(self):
+    MAINFRAME = 1
+    SUBFRAME = 2
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink.some_other_user_timing',
+                       'name': 'firstContentfulPaint'},
+                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstDiscontentPaint'},
+                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': SUBFRAME} },
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
+    lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
+    self.assertEqual(1, lens.PostloadTimeMsec())
+    request_lens = user_satisfied_lens.RequestFingerprintLens(
+      loading_trace, lens.CriticalFingerprints())
+    self.assertEqual(set(['0.1', '0.2']), request_lens.CriticalRequestIds())
+    self.assertEqual(0, request_lens.PostloadTimeMsec())
+
 
 if __name__ == '__main__':
   unittest.main()

commit b890ad44b219a347bcf8010055138ab89167ab74
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 28 05:15:08 2016 -0700

    tools/android/loading: Force to use of simple cache on desktop.
    
    Before on desktop, the cache backend type of the HTTP cache was
    choosen by the SimpleCacheTrial on field trial. This can cause
    issues with sandwich were the cache backend type might change
    between runs.
    
    This CL override the cache backend type to use in Chrome on
    Desktop to not have to handle the two backend types in the cache
    archive processing.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1931523002
    Cr-Original-Commit-Position: refs/heads/master@{#390352}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0e6c8e0d72463672af06ecd6a634e6d9e5078927

diff --git a/loading/controller.py b/loading/controller.py
index d8c20e0..8a2d4a6 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -324,6 +324,8 @@ class LocalChromeController(ChromeControllerBase):
        child processes used to run Chrome and XVFB."""
     chrome_cmd = [OPTIONS.local_binary]
     chrome_cmd.extend(self._GetChromeArguments())
+    # Force use of simple cache.
+    chrome_cmd.append('--use-simple-cache-backend=on')
     chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
     chrome_cmd.extend(['--enable-logging=stderr', '--v=1'])
     # Navigates to about:blank for couples of reasons:

commit 83f74d8859ac674cece0588735f672d4bd10f70c
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 28 02:11:20 2016 -0700

    clovis: Don't pretty-print JSON traces.
    
    Size:
    - before: 167MB
    - after:  45MB
    
    Review-Url: https://codereview.chromium.org/1923823003
    Cr-Original-Commit-Position: refs/heads/master@{#390335}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2382627811eeea2398c38feaa36077cbc1bd973c

diff --git a/loading/analyze.py b/loading/analyze.py
index db0106f..82cc071 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -59,16 +59,6 @@ def _LoadPage(device, url):
   device.StartActivity(load_intent, blocking=True)
 
 
-def _WriteJson(output, json_data):
-  """Write JSON data in a nice way.
-
-  Args:
-    output: a file object
-    json_data: JSON data as a dict.
-  """
-  json.dump(json_data, output, sort_keys=True, indent=2)
-
-
 def _GetPrefetchHtml(graph_view, name=None):
   """Generate prefetch page for the resources in resource graph.
 
@@ -154,14 +144,14 @@ def _FullFetch(url, json_output, prefetch):
     logging.warning('Warm fetch')
     warm_data = _LogRequests(url, clear_cache_override=False)
     with open(json_output, 'w') as f:
-      _WriteJson(f, warm_data)
+      json.dump(warm_data, f)
     logging.warning('Wrote ' + json_output)
     with open(json_output + '.cold', 'w') as f:
-      _WriteJson(f, cold_data)
+      json.dump(cold_data, f)
     logging.warning('Wrote ' + json_output + '.cold')
   else:
     with open(json_output, 'w') as f:
-      _WriteJson(f, cold_data)
+      json.dump(cold_data, f)
     logging.warning('Wrote ' + json_output)
 
 
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 2a291e5..0804e27 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -52,7 +52,7 @@ class LoadingTrace(object):
     """Save a json file representing this instance."""
     json_dict = self.ToJsonDict()
     with open(json_path, 'w') as output_file:
-       json.dump(json_dict, output_file, indent=2)
+       json.dump(json_dict, output_file)
 
   @classmethod
   def FromJsonDict(cls, json_dict):

commit 41fc05c86971ee57e815e9d1bff10d461f3c5a29
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 28 02:10:33 2016 -0700

    clovis: about:tracing traces don't need to be gzipped.
    
    Makes trace_to_chrome_trace.py go from >1m to 20s.
    
    Review-Url: https://codereview.chromium.org/1926513003
    Cr-Original-Commit-Position: refs/heads/master@{#390334}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a01c59e9c11f164302c7fbfbb21912876e98157f

diff --git a/loading/trace_to_chrome_trace.py b/loading/trace_to_chrome_trace.py
index 382d87d..23c3632 100755
--- a/loading/trace_to_chrome_trace.py
+++ b/loading/trace_to_chrome_trace.py
@@ -5,12 +5,11 @@
 
 """Convert trace output for Chrome.
 
-Takes a loading trace from 'analyze.py log_requests' and outputs a zip'd json
+Takes a loading trace from 'analyze.py log_requests' and outputs a json file
 that can be loaded by chrome's about:tracing..
 """
 
 import argparse
-import gzip
 import json
 
 if __name__ == '__main__':
@@ -18,6 +17,6 @@ if __name__ == '__main__':
   parser.add_argument('input')
   parser.add_argument('output')
   args = parser.parse_args()
-  with gzip.GzipFile(args.output, 'w') as output_f, file(args.input) as input_f:
+  with file(args.output, 'w') as output_f, file(args.input) as input_f:
     events = json.load(input_f)['tracing_track']['events']
     json.dump({'traceEvents': events, 'metadata': {}}, output_f)

commit 370155bd6c0191b15aa019c4a2d4848ba2ad0359
Author: droger <droger@chromium.org>
Date:   Wed Apr 27 12:40:52 2016 -0700

    tools/android/loading Cleanup cloud resources when workers are done.
    
    The frontend now polls the backend to check if the work is finished, and
    then destroys the instance group and the instance template.
    
    Review-Url: https://codereview.chromium.org/1920093004
    Cr-Original-Commit-Position: refs/heads/master@{#390151}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 35480ea8a3d068c38e4917b48d231e5a2d5ff192

diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 3f766dd..746512d 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -21,12 +21,16 @@ class GoogleInstanceHelper(object):
     self._logger = logger
 
   def _ExecuteApiRequest(self, request, retry_count=3):
-    """ Executes a Compute API request and returns True on success."""
+    """ Executes a Compute API request and returns True on success.
+
+    Returns:
+      (True, Response) in case of success, or (False, error_content) otherwise.
+    """
     self._logger.info('Compute API request:\n' + request.to_json())
     try:
       response = request.execute()
-      self._logger.info('Compute API response:\n' + response)
-      return True
+      self._logger.info('Compute API response:\n' + str(response))
+      return (True, response)
     except errors.HttpError as err:
       error_content = self._GetErrorContent(err)
       error_reason = self._GetErrorReason(error_content)
@@ -42,7 +46,7 @@ class GoogleInstanceHelper(object):
             error_reason, err))
         if error_content:
           self._logger.error('Error details:\n%s' % error_content)
-        return False
+        return (False, error_content)
 
   def _GetTemplateName(self, tag):
     """Returns the name of the instance template associated with tag."""
@@ -112,16 +116,23 @@ class GoogleInstanceHelper(object):
                 {'key': 'taskqueue-tag', 'value': tag}]}}}
     request = self._compute_api.instanceTemplates().insert(
         project=self._project, body=request_body)
-    return self._ExecuteApiRequest(request)
+    return self._ExecuteApiRequest(request)[0]
 
   def DeleteTemplate(self, tag):
     """Deletes the instance template associated with tag. Returns True if
     successful.
     """
+    template_name = self._GetTemplateName(tag)
     request = self._compute_api.instanceTemplates().delete(
-        project=self._project,
-        instanceTemplate=self._GetTemplateName(tag))
-    return self._ExecuteApiRequest(request)
+        project=self._project, instanceTemplate=template_name)
+    (success, result) = self._ExecuteApiRequest(request)
+    if success:
+      return True
+    if self._GetErrorReason(result) == 'notFound':
+      # The template does not exist, nothing to do.
+      self._logger.warning('Template not found: ' + template_name)
+      return True
+    return False
 
   def CreateInstances(self, tag, instance_count):
     """Creates an instance group associated with tag. The instance template must
@@ -137,10 +148,10 @@ class GoogleInstanceHelper(object):
     request = self._compute_api.instanceGroupManagers().insert(
         project=self._project, zone=self._zone,
         body=request_body)
-    return self._ExecuteApiRequest(request)
+    return self._ExecuteApiRequest(request)[0]
 
   def DeleteInstance(self, tag, instance_hostname):
-    """Deletes one instance from the instance group identified with tag. Returns
+    """Deletes one instance from the instance group identified by tag. Returns
     True if successful.
     """
     # The instance hostname may be of the form <name>.c.<project>.internal but
@@ -152,4 +163,34 @@ class GoogleInstanceHelper(object):
         project=self._project, zone=self._zone,
         instanceGroupManager=self._GetInstanceGroupName(tag),
         body={'instances': [instance_url]})
-    return self._ExecuteApiRequest(request)
+    return self._ExecuteApiRequest(request)[0]
+
+  def DeleteInstanceGroup(self, tag):
+    """Deletes the instance group identified by tag. If instances are still
+    running in this group, they are deleted as well.
+    """
+    group_name = self._GetInstanceGroupName(tag)
+    request = self._compute_api.instanceGroupManagers().delete(
+        project=self._project, zone=self._zone,
+        instanceGroupManager=group_name)
+    (success, result) = self._ExecuteApiRequest(request)
+    if success:
+      return True
+    if self._GetErrorReason(result) == 'notFound':
+      # The group does not exist, nothing to do.
+      self._logger.warning('Instance group not found: ' + group_name)
+      return True
+    return False
+
+  def GetInstanceCount(self, tag):
+    """Returns the number of instances in the instance group identified by
+    tag, or -1 in case of failure.
+    """
+    request = self._compute_api.instanceGroupManagers().listManagedInstances(
+        project=self._project, zone=self._zone,
+        instanceGroupManager=self._GetInstanceGroupName(tag))
+    (success, response) = self._ExecuteApiRequest(request)
+    if not success:
+      return -1
+    return len(response.get('managedInstances', []))
+
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index 7720203..7e06898 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -24,6 +24,9 @@ following keys:
     ComputeEngine instances. This parameter should not be set in general, as it
     is mostly exposed for development purposes. If this parameter is not
     specified, a unique tag will be generated.
+-   `timeout_hours` (int, optional): if workers are still alive after this
+    delay, they will be forcibly killed, to avoid wasting Compute Engine
+    resources. Defaults to `5`.
 
 ### Parameters for the `trace` action
 
diff --git a/loading/cloud/frontend/app.yaml b/loading/cloud/frontend/app.yaml
index 6544088..2801e9d 100644
--- a/loading/cloud/frontend/app.yaml
+++ b/loading/cloud/frontend/app.yaml
@@ -2,8 +2,18 @@ runtime: python27
 api_version: 1
 threadsafe: yes
 
+builtins:
+- deferred: on
+
 handlers:
+
+- url: /_ah/queue/deferred
+  # For the deferred API (https://cloud.google.com/appengine/articles/deferred).
+  script: google.appengine.ext.deferred.deferred.application
+  login: admin
+
 - url: /static
+  # Static content.
   static_dir: static
 
 - url: .*
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 656fa9d..ffef8be 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -5,9 +5,11 @@
 import logging
 import os
 import sys
+import time
 
 import flask
 from google.appengine.api import (app_identity, taskqueue)
+from google.appengine.ext import deferred
 from oauth2client.client import GoogleCredentials
 
 from common.clovis_task import ClovisTask
@@ -31,6 +33,41 @@ def Render(message, memory_logs):
       'log.html', body=message, log=memory_logs.Flush().split('\n'))
 
 
+def PollWorkers(tag, start_time, timeout_hours):
+  """Checks if there are workers associated with tag, by polling the instance
+  group. When all workers are finished, the instance group and the instance
+  template are destroyed.
+  After some timeout delay, the instance group is destroyed even if there are
+  still workers associated to it, which has the effect of killing all these
+  workers.
+
+  Args:
+    tag (string): Tag of the task that is polled.
+    start_time (float): Time when the polling started, as returned by
+                        time.time().
+    timeout_hours (int): Timeout after which workers are terminated.
+  """
+  if (time.time() - start_time) > (3600 * timeout_hours):
+    clovis_logger.error('Worker timeout for tag %s, shuting down.' % tag)
+    deferred.defer(DeleteInstanceGroup, tag)
+    return
+
+  clovis_logger.info('Polling workers for tag: ' + tag)
+  live_instance_count = instance_helper.GetInstanceCount(tag)
+  clovis_logger.info('%i live instances for tag %s.' % (
+      live_instance_count, tag))
+
+  if live_instance_count > 0 or live_instance_count == -1:
+    clovis_logger.info('Retry later, instances still alive for tag: ' + tag)
+    poll_interval_minutes = 10
+    deferred.defer(PollWorkers, tag, start_time,
+                   _countdown=(60 * poll_interval_minutes))
+    return
+
+  clovis_logger.info('Scheduling instance group destruction for tag: ' + tag)
+  deferred.defer(DeleteInstanceGroup, tag)
+
+
 def CreateInstanceTemplate(task):
   """Create the Compute Engine instance template that will be used to create the
   instances.
@@ -48,7 +85,7 @@ def CreateInstanceTemplate(task):
 
 
 def CreateInstances(task):
-  """Creates the Compute engine requested by the task"""
+  """Creates the Compute engine requested by the task."""
   backend_params = task.BackendParams()
   instance_count = backend_params.get('instance_count', 0)
   if instance_count <= 0:
@@ -57,8 +94,36 @@ def CreateInstances(task):
   return instance_helper.CreateInstances(backend_params['tag'], instance_count)
 
 
+def DeleteInstanceGroup(tag, try_count=0):
+  """Deletes the instance group associated with tag, and schedules the deletion
+  of the instance template."""
+  clovis_logger.info('Instance group destruction for tag: ' + tag)
+  if not instance_helper.DeleteInstanceGroup(tag):
+    clovis_logger.info('Instance group destruction failed for: ' + tag)
+    if try_count <= 5:
+      deferred.defer(DeleteInstanceGroup, tag, try_count + 1, _countdown=60)
+      return
+    clovis_logger.error('Giving up group destruction for: ' + tag)
+  clovis_logger.info('Scheduling instance template destruction for tag: ' + tag)
+  # Wait a little before deleting the instance template, because it may still be
+  # considered in use, causing failures.
+  deferred.defer(DeleteInstanceTemplate, tag, _countdown=30)
+
+
+def DeleteInstanceTemplate(tag, try_count=0):
+  """Deletes the instance template associated with tag."""
+  clovis_logger.info('Instance template destruction for tag: ' + tag)
+  if not instance_helper.DeleteTemplate(tag):
+    clovis_logger.info('Instance template destruction failed for: ' + tag)
+    if try_count <= 5:
+      deferred.defer(DeleteInstanceTemplate, tag, try_count + 1, _countdown=60)
+      return
+    clovis_logger.error('Giving up template destruction for: ' + tag)
+  clovis_logger.info('Cleanup complete for tag: ' + tag)
+
+
 def StartFromJsonString(http_body_str):
-  """Main function handling a JSON task posted by the user"""
+  """Main function handling a JSON task posted by the user."""
   # Set up logging.
   memory_logs = MemoryLogs(clovis_logger)
   memory_logs.Start()
@@ -85,11 +150,18 @@ def StartFromJsonString(http_body_str):
     return Render(error_string, memory_logs)
 
   if not EnqueueTasks(sub_tasks, task_tag):
-    return Render('Task creation failed', memory_logs)
+    return Render('Task creation failed.', memory_logs)
 
   # Start the instances if required.
   if not CreateInstances(task):
-    return Render('Instance creation failed', memory_logs)
+    return Render('Instance creation failed.', memory_logs)
+
+  # Start polling the progress.
+  clovis_logger.info('Creating worker polling task.')
+  first_poll_delay_minutes = 10
+  timeout_hours = task.BackendParams().get('timeout_hours', 5)
+  deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours,
+                 _countdown=(60 * first_poll_delay_minutes))
 
   return Render('Success', memory_logs)
 
@@ -139,7 +211,7 @@ def EnqueueTasks(tasks, task_tag):
   except Exception as e:
     clovis_logger.error('Exception:' + type(e).__name__ + ' ' + str(e.args))
     return False
-  clovis_logger.info('Pushed %i tasks with tag: %s' % (len(tasks), task_tag))
+  clovis_logger.info('Pushed %i tasks with tag: %s.' % (len(tasks), task_tag))
   return True
 
 
@@ -151,7 +223,7 @@ def Root():
 
 @app.route('/form_sent', methods=['POST'])
 def StartFromForm():
-  """HTML form endpoint"""
+  """HTML form endpoint."""
   data_stream = flask.request.files.get('json_task')
   if not data_stream:
     return 'failed'

commit e693c0735729a44fbaccbb7bb5c88af478b973f0
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 08:05:39 2016 -0700

    sandwich: Output further information in metrics' CSV
    
    To make sure there is not misinterpretation of the metrics
    from a sandwich CSV, this CL adds the network emulation column
    in the CSVs.
    
    This CL is also the opportunity to get ride of the sandwich's
    run_info.json that is completely replaceable with LoadingTrace's
    metadata dictionary.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1891183003
    
    Cr-Original-Commit-Position: refs/heads/master@{#390078}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 929aa5f0811b7e9a3c66d8b59fb4ff2b95f0a243

diff --git a/loading/controller.py b/loading/controller.py
index 1eb2f03..d8c20e0 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,6 +10,7 @@ desktop-specific versions.
 """
 
 import contextlib
+import copy
 import datetime
 import errno
 import logging
@@ -70,7 +71,7 @@ class ChromeControllerBase(object):
     self._wpr_attributes = None
     self._metadata = {}
     self._emulated_device = None
-    self._emulated_network = None
+    self._network_name = None
     self._slow_death = False
 
   def AddChromeArgument(self, arg):
@@ -115,10 +116,8 @@ class ChromeControllerBase(object):
       network_name: (str) Key from emulation.NETWORK_CONDITIONS or None to
         disable network emulation.
     """
-    if network_name:
-      self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
-    else:
-      self._emulated_network = None
+    assert network_name in emulation.NETWORK_CONDITIONS or network_name is None
+    self._network_name = network_name
 
   def PushBrowserCache(self, cache_path):
     """Pushes the HTTP chrome cache to the profile directory.
@@ -177,9 +176,17 @@ class ChromeControllerBase(object):
     if self._emulated_device:
       self._metadata.update(emulation.SetUpDeviceEmulationAndReturnMetadata(
           connection, self._emulated_device))
-    if self._emulated_network:
-      emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
-      self._metadata.update(self._emulated_network)
+    if self._network_name:
+      network_condition = emulation.NETWORK_CONDITIONS[self._network_name]
+      logging.info('Set up network emulation %s (latency=%dms, down=%d, up=%d)'
+          % (self._network_name, network_condition['latency'],
+              network_condition['download'], network_condition['upload']))
+      emulation.SetUpNetworkEmulation(connection, **network_condition)
+      self._metadata['network_emulation'] = copy.copy(network_condition)
+      self._metadata['network_emulation']['name'] = self._network_name
+    else:
+      self._metadata['network_emulation'] = \
+          {k: 'disabled' for k in ['name', 'download', 'upload', 'latency']}
     self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
                           seconds_since_epoch=time.time())
     logging.info('Devtools connection success')
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index bfbf00f..e8930e9 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -236,6 +236,7 @@ class DevToolsConnection(object):
                                  self._scoped_states[scoped_state][0])
     self._tearing_down_tracing = False
 
+    logging.info('Navigate to %s' % url)
     self.SendAndIgnoreResponse('Page.navigate', {'url': url})
 
     self._Dispatch(timeout=timeout_seconds)
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 55d61b4..6dda5d0 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -230,15 +230,15 @@ def _RunJobMain(args):
 
 
 def _ExtractMetricsMain(args):
-  trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
+  run_metrics_list = sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
       args.trace_output_directory)
-  trace_metrics_list.sort(key=lambda e: e['id'])
+  run_metrics_list.sort(key=lambda e: e['repeat_id'])
   with open(args.metrics_csv_path, 'w') as csv_file:
     writer = csv.DictWriter(csv_file,
                             fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
     writer.writeheader()
-    for trace_metrics in trace_metrics_list:
-      writer.writerow(trace_metrics)
+    for run_metrics in run_metrics_list:
+      writer.writerow(run_metrics)
   return 0
 
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index a30b327..1324b75 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -31,13 +31,17 @@ import tracing
 
 
 CSV_FIELD_NAMES = [
-    'id',
+    'repeat_id',
     'url',
     'total_load',
-    'onload',
+    'js_onload_event',
     'browser_malloc_avg',
     'browser_malloc_max',
-    'speed_index']
+    'speed_index',
+    'net_emul.name', # Should be in emulation.NETWORK_CONDITIONS.keys()
+    'net_emul.download',
+    'net_emul.upload',
+    'net_emul.latency']
 
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
@@ -126,14 +130,14 @@ def _GetWebPageTrackedEvents(tracing_track):
   return tracked_events
 
 
-def _PullMetricsFromLoadingTrace(loading_trace):
+def _ExtractMetricsFromLoadingTrace(loading_trace):
   """Pulls all the metrics from a given trace.
 
   Args:
     loading_trace: loading_trace_module.LoadingTrace.
 
   Returns:
-    Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
+    Dictionary with all trace extracted fields set.
   """
   assert all(
       cat in loading_trace.tracing_track.Categories()
@@ -156,8 +160,8 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   return {
     'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
                    web_page_tracked_events['requestStart'].start_msec),
-    'onload': (web_page_tracked_events['loadEventEnd'].start_msec -
-               web_page_tracked_events['loadEventStart'].start_msec),
+    'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
+                        web_page_tracked_events['loadEventStart'].start_msec),
     'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
     'browser_malloc_max': browser_malloc_max
   }
@@ -222,15 +226,42 @@ def ComputeSpeedIndex(completeness_record):
   return speed_index
 
 
-def PullMetricsFromOutputDirectory(output_directory_path):
-  """Pulls all the metrics from all the traces of a sandwich run directory.
+def _ExtractMetricsFromRunDirectory(run_directory_path):
+  """Extracts all the metrics from traces and video of a sandwich run.
 
   Args:
-    output_directory_path: The sandwich run's output directory to pull the
+    run_directory_path: Path of the run directory.
+
+  Returns:
+    Dictionary of extracted metrics.
+  """
+  trace_path = os.path.join(run_directory_path, 'trace.json')
+  logging.info('processing trace \'%s\'' % trace_path)
+  loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
+  run_metrics = {'url': loading_trace.url}
+  run_metrics.update(_ExtractMetricsFromLoadingTrace(loading_trace))
+  video_path = os.path.join(run_directory_path, 'video.mp4')
+  if os.path.isfile(video_path):
+    logging.info('processing speed-index video \'%s\'' % video_path)
+    completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
+    run_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+  else:
+    run_metrics['speed_index'] = 'disabled'
+  for key, value in loading_trace.metadata['network_emulation'].iteritems():
+    run_metrics['net_emul.' + key] = value
+  return run_metrics
+
+
+def ExtractMetricsFromRunnerOutputDirectory(output_directory_path):
+  """Extracts all the metrics from all the traces of a sandwich runner output
+  directory.
+
+  Args:
+    output_directory_path: The sandwich runner's output directory to extract the
         metrics from.
 
   Returns:
-    List of dictionaries with all CSV_FIELD_NAMES's field set.
+    List of dictionaries.
   """
   assert os.path.isdir(output_directory_path)
   metrics = []
@@ -238,25 +269,14 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     if not os.path.isdir(os.path.join(output_directory_path, node_name)):
       continue
     try:
-      page_id = int(node_name)
+      repeat_id = int(node_name)
     except ValueError:
       continue
-    run_path = os.path.join(output_directory_path, node_name)
-    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
-    if not os.path.isfile(trace_path):
-      continue
-    logging.info('processing \'%s\'' % trace_path)
-    loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
-    row_metrics = {key: 'unavailable' for key in CSV_FIELD_NAMES}
-    row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
-    row_metrics['id'] = page_id
-    row_metrics['url'] = loading_trace.url
-    video_path = os.path.join(run_path, sandwich_runner.VIDEO_FILENAME)
-    if os.path.isfile(video_path):
-      logging.info('processing \'%s\'' % video_path)
-      completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
-      row_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
-    metrics.append(row_metrics)
-  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
-                            'run directory.').format(output_directory_path)
+    run_directory_path = os.path.join(output_directory_path, node_name)
+    run_metrics = _ExtractMetricsFromRunDirectory(run_directory_path)
+    run_metrics['repeat_id'] = repeat_id
+    assert set(run_metrics.keys()) == set(CSV_FIELD_NAMES)
+    metrics.append(run_metrics)
+  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich runner ' +
+                            'output directory.').format(output_directory_path)
   return metrics
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index d9c1700..73c8f3e 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -188,11 +188,11 @@ class PageTrackTest(unittest.TestCase):
     self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
 
   def testPullMetricsFromLoadingTrace(self):
-    metrics = puller._PullMetricsFromLoadingTrace(LoadingTrace(
+    metrics = puller._ExtractMetricsFromLoadingTrace(LoadingTrace(
         _MINIMALIST_TRACE_EVENTS))
     self.assertEquals(4, len(metrics))
     self.assertEquals(20, metrics['total_load'])
-    self.assertEquals(5, metrics['onload'])
+    self.assertEquals(5, metrics['js_onload_event'])
     self.assertEquals(30971, metrics['browser_malloc_avg'])
     self.assertEquals(55044, metrics['browser_malloc_max'])
 
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 571a76d..3a56b59 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -168,17 +168,6 @@ class SandwichRunner(object):
     else:
       _CleanPreviousTraces(self.trace_output_directory)
 
-  def _SaveRunInfos(self, urls):
-    assert self.trace_output_directory
-    run_infos = {
-      'cache-op': self.cache_operation,
-      'job_name': self.job_name,
-      'urls': urls
-    }
-    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
-              'w') as file_output:
-      json.dump(run_infos, file_output, indent=2)
-
   def _GetEmulatorNetworkCondition(self, emulator):
     if self.network_emulator == emulator:
       return self.network_condition
@@ -285,7 +274,5 @@ class SandwichRunner(object):
       self._local_cache_directory_path = None
     if self.cache_operation == 'save':
       self._PullCacheFromDevice()
-    if self.trace_output_directory:
-      self._SaveRunInfos(ran_urls)
 
     self._chrome_ctl = None
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 22572d2..954c67e 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -236,9 +236,10 @@ class SandwichTaskBuilder(task_manager.Builder):
     def ExtractMetrics():
       sandwich_misc.VerifyBenchmarkOutputDirectory(
           SetupBenchmark.path, RunBenchmark.path)
-      trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
-          RunBenchmark.path)
-      trace_metrics_list.sort(key=lambda e: e['id'])
+      trace_metrics_list = \
+          sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
+              RunBenchmark.path)
+      trace_metrics_list.sort(key=lambda e: e['repeat_id'])
       with open(ExtractMetrics.path, 'w') as csv_file:
         writer = csv.DictWriter(csv_file,
                                 fieldnames=sandwich_metrics.CSV_FIELD_NAMES)

commit 707c46d99b18b2e4a4e8776a82e60f9c40cef5e3
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 07:07:35 2016 -0700

    sandwich: Add Desktop support
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1908173004
    
    Cr-Original-Commit-Position: refs/heads/master@{#390063}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3cab289c7572a4f94a078c9cc02d12fa48ad46b0

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 1322a66..c1b17d9 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -68,6 +68,16 @@ def GetFirstDevice():
   return devices[0]
 
 
+def GetDeviceFromSerial(android_device_serial):
+  """Returns the DeviceUtils instance."""
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  for device in devices:
+    if device.adb._device_serial == android_device_serial:
+      return device
+  raise DeviceSetupException(
+      'Device {} not found'.format(android_device_serial))
+
+
 def DeviceSubmitShellCommandQueue(device, command_queue):
   """Executes on the device a command queue.
 
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 30c3d65..55d61b4 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -30,6 +30,7 @@ import devil_chromium
 
 import chrome_cache
 import common_util
+import device_setup
 import emulation
 import options
 import sandwich_metrics
@@ -50,6 +51,9 @@ def _ArgumentParser():
   common_job_parser = argparse.ArgumentParser(add_help=False)
   common_job_parser.add_argument('--job', required=True,
                                  help='JSON file with job description.')
+  common_job_parser.add_argument('--android', default=None, type=str,
+                                 dest='android_device_serial',
+                                 help='Android device\'s serial to use.')
 
   task_parser = task_manager.CommandLineParser()
 
@@ -188,10 +192,18 @@ def _ArgumentParser():
   return parser
 
 
-def _RecordWprMain(args):
+def _CreateSandwichRunner(args):
   sandwich_runner = SandwichRunner()
   sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
+  if args.android_device_serial is not None:
+    sandwich_runner.android_device = \
+        device_setup.GetDeviceFromSerial(args.android_device_serial)
+  return sandwich_runner
+
+
+def _RecordWprMain(args):
+  sandwich_runner = _CreateSandwichRunner(args)
   sandwich_runner.wpr_record = True
   sandwich_runner.PrintConfig()
   if not os.path.isdir(os.path.dirname(args.wpr_archive_path)):
@@ -201,9 +213,7 @@ def _RecordWprMain(args):
 
 
 def _CreateCacheMain(args):
-  sandwich_runner = SandwichRunner()
-  sandwich_runner.LoadJob(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner = _CreateSandwichRunner(args)
   sandwich_runner.cache_operation = 'save'
   sandwich_runner.PrintConfig()
   if not os.path.isdir(os.path.dirname(args.cache_archive_path)):
@@ -213,9 +223,7 @@ def _CreateCacheMain(args):
 
 
 def _RunJobMain(args):
-  sandwich_runner = SandwichRunner()
-  sandwich_runner.LoadJob(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner = _CreateSandwichRunner(args)
   sandwich_runner.PrintConfig()
   sandwich_runner.Run()
   return 0
@@ -266,8 +274,13 @@ def _RecordWebServerTestTrace(args):
 
 
 def _RunAllMain(args):
+  android_device = None
+  if args.android_device_serial:
+    android_device = \
+        device_setup.GetDeviceFromSerial(args.android_device_serial)
   builder = sandwich_task_builder.SandwichTaskBuilder(
       output_directory=args.output,
+      android_device=android_device,
       job_path=args.job,
       url_repeat=args.url_repeat)
   if args.wpr_archive_path:
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 04520d5..5df6351 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -173,7 +173,7 @@ def _ListUrlRequests(trace, request_kind):
       continue
     if request_event.protocol.startswith('data'):
       continue
-    if request_event.protocol.startswith('http'):
+    if not request_event.protocol.startswith('http'):
       raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
     if (request_kind == _RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 0d6d105..571a76d 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -130,6 +130,9 @@ class SandwichRunner(object):
     # Configures whether the WPR archive should be read or generated.
     self.wpr_record = False
 
+    # The android DeviceUtils to run sandwich on or None to run it locally.
+    self.android_device = None
+
     self._chrome_ctl = None
     self._local_cache_directory_path = None
 
@@ -252,9 +255,10 @@ class SandwichRunner(object):
     if self.trace_output_directory:
       self._CleanTraceOutputDirectory()
 
-    # TODO(gabadie): Make sandwich working on desktop.
-    device = device_utils.DeviceUtils.HealthyDevices()[0]
-    self._chrome_ctl = controller.RemoteChromeController(device)
+    if self.android_device:
+      self._chrome_ctl = controller.RemoteChromeController(self.android_device)
+    else:
+      self._chrome_ctl = controller.LocalChromeController()
     self._chrome_ctl.AddChromeArgument('--disable-infobars')
     if self.cache_operation == 'save':
       self._chrome_ctl.SetSlowDeath()
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 1120d77..22572d2 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -37,16 +37,19 @@ class SandwichTaskBuilder(task_manager.Builder):
   """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
   """
 
-  def __init__(self, output_directory, job_path, url_repeat):
+  def __init__(self, output_directory, android_device, job_path, url_repeat):
     """Constructor.
 
     Args:
       output_directory: As in task_manager.Builder.__init__
+      android_device: The android DeviceUtils to run sandwich on or None to run
+        it locally.
       job_path: Path of the sandwich's job.
       url_repeat: Non null integer controlling how many times the URLs should be
         repeated in the benchmarks.
     """
     task_manager.Builder.__init__(self, output_directory)
+    self._android_device = android_device
     self._job_path = job_path
     self._url_repeat = url_repeat
     self._default_final_tasks = []
@@ -65,6 +68,7 @@ class SandwichTaskBuilder(task_manager.Builder):
     """Create a runner for non benchmark purposes."""
     runner = sandwich_runner.SandwichRunner()
     runner.LoadJob(self._job_path)
+    runner.android_device = self._android_device
     return runner
 
   def OverridePathToWprArchive(self, original_wpr_path):

commit 719ae362f823716ee81db19c1e3b09b094d3122f
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 05:41:57 2016 -0700

    tools/android/loading: Add HTTPS certificate authority support on Linux
    
    Before on Linux, Chrome was opened with the --ignore-certificate-errors
    command line argument to ignore HTTPS certificate failure and still load
    the web page. But the HTTPS resources were not getting cached by design,
    unabling sandwich to run on Linux.
    
    This CL launch Chrome with a different $HOME directory and use the
    certutil command line tool to fill up the standard $HOME/.pki/nssdb
    to let Chrome to trust the root certification authority's certificate
    of Web Page Replay for SSL connections.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1903773002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390053}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e4a8d81705b6a3cd3cfca976505e0481b9ae25ab

diff --git a/loading/common_util.py b/loading/common_util.py
index 350a418..c4eac2c 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -8,6 +8,7 @@ import logging
 import os
 import re
 import shutil
+import subprocess
 import sys
 import tempfile
 import time
@@ -80,11 +81,11 @@ def DeserializeAttributesFromJsonDict(json_dict, instance, attributes):
 
 
 @contextlib.contextmanager
-def TemporaryDirectory():
+def TemporaryDirectory(suffix='', prefix='tmp'):
   """Returns a freshly-created directory that gets automatically deleted after
   usage.
   """
-  name = tempfile.mkdtemp()
+  name = tempfile.mkdtemp(suffix=suffix, prefix=prefix)
   try:
     yield name
   finally:
@@ -96,3 +97,20 @@ def EnsureParentDirectoryExists(path):
   parent_directory_path = os.path.abspath(os.path.dirname(path))
   if not os.path.isdir(parent_directory_path):
     os.makedirs(parent_directory_path)
+
+
+def GetCommandLineForLogging(cmd, env_diff=None):
+  """Get command line string.
+
+  Args:
+    cmd: Command line argument
+    env_diff: Environment modification for the command line.
+
+  Returns:
+    Command line string.
+  """
+  cmd_str = ''
+  if env_diff:
+    for key, value in env_diff.iteritems():
+      cmd_str += '{}={} '.format(key, value)
+  return cmd_str + subprocess.list2cmdline(cmd)
diff --git a/loading/controller.py b/loading/controller.py
index 50595e4..1eb2f03 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -25,6 +25,7 @@ _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
 
 import chrome_cache
+import common_util
 import device_setup
 import devtools_monitor
 import emulation
@@ -66,7 +67,7 @@ class ChromeControllerBase(object):
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
     ]
-    self._chrome_wpr_specific_args = []
+    self._wpr_attributes = None
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
@@ -185,7 +186,10 @@ class ChromeControllerBase(object):
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
-    return self._chrome_args + self._chrome_wpr_specific_args
+    chrome_args = self._chrome_args[:]
+    if self._wpr_attributes:
+      chrome_args.extend(self._wpr_attributes.chrome_args)
+    return chrome_args
 
 
 class RemoteChromeController(ChromeControllerBase):
@@ -212,6 +216,9 @@ class RemoteChromeController(ChromeControllerBase):
   @contextlib.contextmanager
   def Open(self):
     """Overridden connection creation."""
+    if self._wpr_attributes:
+      assert self._wpr_attributes.chrome_env_override == {}, \
+          'Remote controller doesn\'t support chrome environment variables.'
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
     self._device.ForceStop(package_info.package)
@@ -268,15 +275,15 @@ class RemoteChromeController(ChromeControllerBase):
                   disable_script_injection=False,
                   out_log_path=None):
     """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
-    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    assert not self._wpr_attributes, 'WPR is already running.'
     with device_setup.RemoteWprHost(self._device, wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection,
-        out_log_path=out_log_path) as additional_flags:
-      self._chrome_wpr_specific_args = additional_flags
+        out_log_path=out_log_path) as wpr_attributes:
+      self._wpr_attributes = wpr_attributes
       yield
-    self._chrome_wpr_specific_args = []
+    self._wpr_attributes = None
 
 
 class LocalChromeController(ChromeControllerBase):
@@ -316,16 +323,26 @@ class LocalChromeController(ChromeControllerBase):
     #   - To find the correct target descriptor at devtool connection;
     #   - To avoid cache and WPR pollution by the NTP.
     chrome_cmd.append('about:blank')
-    environment = os.environ.copy()
+
+    chrome_env_override = {}
+    if self._wpr_attributes:
+      chrome_env_override.update(self._wpr_attributes.chrome_env_override)
+
     if self._headless:
-      environment['DISPLAY'] = 'localhost:99'
-      xvfb_process = subprocess.Popen(
-          ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
-          stdout=stdout, stderr=stderr)
-    logging.debug(subprocess.list2cmdline(chrome_cmd))
-    chrome_process = subprocess.Popen(chrome_cmd, shell=False,
-                                      stdout=stdout, stderr=stderr,
-                                      env=environment)
+      assert 'DISPLAY' not in chrome_env_override, \
+          'DISPLAY environment variable is reserved for headless.'
+      chrome_env_override['DISPLAY'] = 'localhost:99'
+      xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
+      logging.info(common_util.GetCommandLineForLogging(xvfb_cmd))
+      xvfb_process = subprocess.Popen(xvfb_cmd, stdout=stdout, stderr=stderr)
+
+    # Launch chrome.
+    logging.info(
+        common_util.GetCommandLineForLogging(chrome_cmd, chrome_env_override))
+    chrome_env = os.environ.copy()
+    chrome_env.update(chrome_env_override)
+    chrome_process = subprocess.Popen(chrome_cmd, stdout=stdout, stderr=stderr,
+                                      env=chrome_env)
     connection = None
     try:
       # Attempt to connect to Chrome's devtools
@@ -388,15 +405,15 @@ class LocalChromeController(ChromeControllerBase):
                   disable_script_injection=False,
                   out_log_path=None):
     """Override for WPR context."""
-    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    assert not self._wpr_attributes, 'WPR is already running.'
     with device_setup.LocalWprHost(wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection,
-        out_log_path=out_log_path) as additional_flags:
-      self._chrome_wpr_specific_args = additional_flags
+        out_log_path=out_log_path) as wpr_attributes:
+      self._wpr_attributes = wpr_attributes
       yield
-    self._chrome_wpr_specific_args = []
+    self._wpr_attributes = None
 
   def _EnsureProfileDirectory(self):
     if (not os.path.isdir(self._profile_dir) or
diff --git a/loading/device_setup.py b/loading/device_setup.py
index d1abaff..1322a66 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import collections
 import contextlib
 import logging
 import os
@@ -14,7 +15,8 @@ import time
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
 
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+_CATAPULT_DIR = os.path.join(_SRC_DIR, 'third_party', 'catapult')
+sys.path.append(os.path.join(_CATAPULT_DIR, 'devil'))
 from devil.android import device_utils
 from devil.android import flag_changer
 from devil.android import forwarder
@@ -31,10 +33,12 @@ sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.image_processing import video
 from telemetry.internal.util import webpagereplay
 
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
+sys.path.append(os.path.join(
+    _CATAPULT_DIR, 'telemetry', 'third_party', 'webpagereplay'))
 import adb_install_cert
 import certutils
 
+import common_util
 import devtools_monitor
 import emulation
 import options
@@ -127,6 +131,17 @@ def ForwardPort(device, local, remote):
     device.adb.ForwardRemove(local)
 
 
+# WPR specific attributes to set up chrome.
+#
+# Members:
+#   chrome_args: Additional flags list that may be used for chromium to load web
+#     page through the running web page replay host.
+#   chrome_env_override: Dictionary of environment variables to override at
+#     Chrome's launch time.
+WprAttribute = collections.namedtuple('WprAttribute',
+                                      ['chrome_args', 'chrome_env_override'])
+
+
 @contextlib.contextmanager
 def _WprHost(wpr_archive_path, record=False,
              network_condition_name=None,
@@ -222,24 +237,33 @@ def LocalWprHost(wpr_archive_path, record=False,
     out_log_path: Path of the WPR host's log.
 
   Returns:
-    Additional flags list that may be used for chromium to load web page through
-    the running web page replay host.
+    WprAttribute
   """
   if wpr_archive_path == None:
     _VerifySilentWprHost(record, network_condition_name)
     yield []
     return
-  with _WprHost(
-      wpr_archive_path,
-      record=record,
-      network_condition_name=network_condition_name,
-      disable_script_injection=disable_script_injection,
-      out_log_path=out_log_path) as (http_port, https_port):
-    chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
-                                                     escape=False)
-    # Certification authority is handled only available on Android.
-    chrome_args.append('--ignore-certificate-errors')
-    yield chrome_args
+
+  with common_util.TemporaryDirectory() as temp_home_dir:
+    # Generate a root certification authority certificate for WPR.
+    private_ca_cert_path = os.path.join(temp_home_dir, 'wpr.pem')
+    ca_cert_path = os.path.join(temp_home_dir, 'wpr-cert.pem')
+    certutils.write_dummy_ca_cert(*certutils.generate_dummy_ca_cert(),
+                                  cert_path=private_ca_cert_path)
+    assert os.path.isfile(ca_cert_path)
+    certutils.install_cert_in_nssdb(temp_home_dir, ca_cert_path)
+
+    with _WprHost(
+        wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection,
+        wpr_ca_cert_path=private_ca_cert_path,
+        out_log_path=out_log_path) as (http_port, https_port):
+      chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
+                                                       escape=False)
+      yield WprAttribute(chrome_args=chrome_args,
+                         chrome_env_override={'HOME': temp_home_dir})
 
 
 @contextlib.contextmanager
@@ -260,8 +284,7 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
     out_log_path: Path of the WPR host's log.
 
   Returns:
-    Additional flags list that may be used for chromium to load web page through
-    the running web page replay host.
+    WprAttribute
   """
   assert device
   if wpr_archive_path == None:
@@ -290,9 +313,10 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
       device_http_port = forwarder.Forwarder.DevicePortForHostPort(http_port)
       device_https_port = forwarder.Forwarder.DevicePortForHostPort(https_port)
       try:
-        yield _FormatWPRRelatedChromeArgumentFor(device_http_port,
-                                                 device_https_port,
-                                                 escape=True)
+        chrome_args = _FormatWPRRelatedChromeArgumentFor(device_http_port,
+                                                         device_https_port,
+                                                         escape=True)
+        yield WprAttribute(chrome_args=chrome_args, chrome_env_override={})
       finally:
         # Tear down the forwarder.
         forwarder.Forwarder.UnmapDevicePort(device_http_port, device)

commit ca71a96319c6ee52f1ed473713ee9a0049380431
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 04:49:40 2016 -0700

    sandwich: Implement SandwichTaskBuilder
    
    Sandwich automation is moving to the task_manager API. This CL
    implements the SandwichTaskBuilder that builds the sandwich's
    task dependency graph.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1872313002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390048}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e88c3b13065fac6f02ec54b95c3fe66ee89f0c37

diff --git a/loading/common_util.py b/loading/common_util.py
index 855284f..350a418 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -89,3 +89,10 @@ def TemporaryDirectory():
     yield name
   finally:
     shutil.rmtree(name)
+
+
+def EnsureParentDirectoryExists(path):
+  """Verifies that the parent directory exists or creates it if missing."""
+  parent_directory_path = os.path.abspath(os.path.dirname(path))
+  if not os.path.isdir(parent_directory_path):
+    os.makedirs(parent_directory_path)
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 5f61aac..30c3d65 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -35,6 +35,8 @@ import options
 import sandwich_metrics
 import sandwich_misc
 from sandwich_runner import SandwichRunner
+import sandwich_task_builder
+import task_manager
 from trace_test.webserver_test import WebServer
 
 
@@ -49,6 +51,8 @@ def _ArgumentParser():
   common_job_parser.add_argument('--job', required=True,
                                  help='JSON file with job description.')
 
+  task_parser = task_manager.CommandLineParser()
+
   # Plumbing parser to configure OPTIONS.
   plumbing_parser = OPTIONS.GetParentParser('plumbing options')
 
@@ -166,6 +170,21 @@ def _ArgumentParser():
   record_trace_parser.add_argument('-o', '--output', type=str, required=True,
                                    help='Output path of the generated trace.')
 
+  # Run all subcommand.
+  run_all = subparsers.add_parser('run-all',
+                       parents=[common_job_parser, task_parser],
+                       help='Run all steps using the task manager '
+                            'infrastructure.')
+  run_all.add_argument('-g', '--gen-full', action='store_true',
+                       help='Generate the full graph with all possible'
+                            'benchmarks.')
+  run_all.add_argument('--wpr-archive', default=None, type=str,
+                       dest='wpr_archive_path',
+                       help='WebPageReplay archive to use, instead of '
+                            'generating one.')
+  run_all.add_argument('--url-repeat', default=1, type=int,
+                       help='How many times to repeat the urls.')
+
   return parser
 
 
@@ -240,10 +259,45 @@ def _RecordWebServerTestTrace(args):
       address = server.Address()
       sandwich_runner.urls = ['http://%s/%s' % (address, args.page)]
       sandwich_runner.Run()
-    shutil.copy(os.path.join(out_path, 'run', '0', 'trace.json'), args.output)
+    trace_path = os.path.join(
+        out_path, 'run', '0', sandwich_runner.TRACE_FILENAME)
+    shutil.copy(trace_path, args.output)
   return 0
 
 
+def _RunAllMain(args):
+  builder = sandwich_task_builder.SandwichTaskBuilder(
+      output_directory=args.output,
+      job_path=args.job,
+      url_repeat=args.url_repeat)
+  if args.wpr_archive_path:
+    builder.OverridePathToWprArchive(args.wpr_archive_path)
+  else:
+    builder.PopulateWprRecordingTask()
+  builder.PopulateCommonPipelines()
+
+  runner_transformer_name = 'no-network-emulation'
+  runner_transformer = lambda arg: None
+  builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
+                                runner_transformer_name, runner_transformer)
+  builder.PopulateLoadBenchmark(sandwich_misc.FULL_CACHE_DISCOVERER,
+                                runner_transformer_name, runner_transformer)
+
+  if args.gen_full:
+    for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
+      if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
+        continue
+      for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
+        runner_transformer_name = network_condition.lower()
+        runner_transformer = sandwich_task_builder.NetworkSimulationTransformer(
+            network_condition)
+        builder.PopulateLoadBenchmark(
+            subresource_discoverer, runner_transformer_name, runner_transformer)
+
+  return task_manager.ExecuteWithCommandLine(
+      args, builder.tasks.values(), builder.default_final_tasks)
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -266,6 +320,8 @@ def main(command_line_args):
     return _FilterCacheMain(args)
   if args.subcommand == 'record-test-trace':
     return _RecordWebServerTestTrace(args)
+  if args.subcommand == 'run-all':
+    return _RunAllMain(args)
   assert False
 
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 98d6317..a30b327 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -26,13 +26,10 @@ from telemetry.util import image_util
 from telemetry.util import rgba_color
 
 import loading_trace as loading_trace_module
+import sandwich_runner
 import tracing
 
 
-# List of selected trace event categories when running chrome.
-ADDITIONAL_CATEGORIES = (
-    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
-
 CSV_FIELD_NAMES = [
     'id',
     'url',
@@ -140,7 +137,7 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   """
   assert all(
       cat in loading_trace.tracing_track.Categories()
-      for cat in ADDITIONAL_CATEGORIES), (
+      for cat in sandwich_runner.ADDITIONAL_CATEGORIES), (
           'This trace was not generated with the required set of categories '
           'to be processed by this script.')
   browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
@@ -245,7 +242,7 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     except ValueError:
       continue
     run_path = os.path.join(output_directory_path, node_name)
-    trace_path = os.path.join(run_path, 'trace.json')
+    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
     if not os.path.isfile(trace_path):
       continue
     logging.info('processing \'%s\'' % trace_path)
@@ -254,7 +251,7 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
     row_metrics['id'] = page_id
     row_metrics['url'] = loading_trace.url
-    video_path = os.path.join(run_path, 'video.mp4')
+    video_path = os.path.join(run_path, sandwich_runner.VIDEO_FILENAME)
     if os.path.isfile(video_path):
       logging.info('processing \'%s\'' % video_path)
       completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 1976462..d9c1700 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -13,6 +13,7 @@ import unittest
 import loading_trace
 import page_track
 import sandwich_metrics as puller
+import sandwich_runner
 import request_track
 import tracing
 
@@ -46,7 +47,8 @@ _MINIMALIST_TRACE_EVENTS = [
 def TracingTrack(events):
   return tracing.TracingTrack.FromJsonDict({
       'events': events,
-      'categories': tracing.INITIAL_CATEGORIES + puller.ADDITIONAL_CATEGORIES})
+      'categories': (tracing.INITIAL_CATEGORIES +
+          sandwich_runner.ADDITIONAL_CATEGORIES)})
 
 
 def LoadingTrace(events):
@@ -105,7 +107,7 @@ class PageTrackTest(unittest.TestCase):
         {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
         {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
-    self.assertTrue(_MEM_CAT in puller.ADDITIONAL_CATEGORIES)
+    self.assertTrue(_MEM_CAT in sandwich_runner.ADDITIONAL_CATEGORIES)
 
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
@@ -232,7 +234,7 @@ class PageTrackTest(unittest.TestCase):
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
       LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
-          os.path.join(tmp_dir, dirname, 'trace.json'))
+          os.path.join(tmp_dir, dirname, sandwich_runner.TRACE_FILENAME))
 
     process = subprocess.Popen(['python', puller.__file__, tmp_dir])
     process.wait()
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index e7b00c6..04520d5 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -3,14 +3,24 @@
 # found in the LICENSE file.
 
 import logging
+import json
+import os
 
+import chrome_cache
+import common_util
 from loading_trace import LoadingTrace
 from prefetch_view import PrefetchSimulationView
 from request_dependencies_lens import RequestDependencyLens
-from user_satisfied_lens import FirstContentfulPaintLens
+import sandwich_runner
 import wpr_backend
 
 
+# Do not prefetch anything.
+EMPTY_CACHE_DISCOVERER = 'empty-cache'
+
+# Prefetches everything to load fully from cache (impossible in practice).
+FULL_CACHE_DISCOVERER = 'full-cache'
+
 # Prefetches the first resource following the redirection chain.
 REDIRECTED_MAIN_DISCOVERER = 'redirected-main'
 
@@ -21,6 +31,8 @@ PARSER_DISCOVERER = 'parser'
 HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner'
 
 SUBRESOURCE_DISCOVERERS = set([
+  EMPTY_CACHE_DISCOVERER,
+  FULL_CACHE_DISCOVERER,
   REDIRECTED_MAIN_DISCOVERER,
   PARSER_DISCOVERER,
   HTML_PRELOAD_SCANNER_DISCOVERER
@@ -85,7 +97,11 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
 
   # Build the list of discovered requests according to the desired simulation.
   discovered_requests = []
-  if subresource_discoverer == REDIRECTED_MAIN_DISCOVERER:
+  if subresource_discoverer == EMPTY_CACHE_DISCOVERER:
+    pass
+  elif subresource_discoverer == FULL_CACHE_DISCOVERER:
+    discovered_requests = trace.request_track.GetEvents()
+  elif subresource_discoverer == REDIRECTED_MAIN_DISCOVERER:
     discovered_requests = \
         [dependencies_lens.GetRedirectChain(first_resource_request)[-1]]
   elif subresource_discoverer == PARSER_DISCOVERER:
@@ -100,7 +116,6 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   # Prune out data:// requests.
   whitelisted_urls = set()
   logging.info('white-listing %s' % first_resource_request.url)
-  whitelisted_urls.add(first_resource_request.url)
   for request in discovered_requests:
     # Work-around where the protocol may be none for an unclear reason yet.
     # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
@@ -114,3 +129,147 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
     logging.info('white-listing %s' % request.url)
     whitelisted_urls.add(request.url)
   return whitelisted_urls
+
+
+def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
+  """Compare URL sets and log the diffs.
+
+  Args:
+    ref_url_set: Set of reference urls.
+    url_set: Set of urls to compare to the reference.
+    url_set_name: The set name for logging purposes.
+  """
+  assert type(ref_url_set) == set
+  assert type(url_set) == set
+  if ref_url_set == url_set:
+    logging.info('  %d %s are matching.' % (len(ref_url_set), url_set_name))
+    return
+  logging.error('  %s are not matching.' % url_set_name)
+  logging.error('    List of missing resources:')
+  for url in ref_url_set.difference(url_set):
+    logging.error('-     ' + url)
+  logging.error('    List of unexpected resources:')
+  for url in url_set.difference(ref_url_set):
+    logging.error('+     ' + url)
+
+
+class _RequestOutcome:
+  All, ServedFromCache, NotServedFromCache = range(3)
+
+
+def _ListUrlRequests(trace, request_kind):
+  """Lists requested URLs from a trace.
+
+  Args:
+    trace: (LoadingTrace) loading trace.
+    request_kind: _RequestOutcome indicating the subset of requests to output.
+
+  Returns:
+    set([str])
+  """
+  urls = set()
+  for request_event in trace.request_track.GetEvents():
+    if request_event.protocol == None:
+      continue
+    if request_event.protocol.startswith('data'):
+      continue
+    if request_event.protocol.startswith('http'):
+      raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
+    if (request_kind == _RequestOutcome.ServedFromCache and
+        request_event.from_disk_cache):
+      urls.add(request_event.url)
+    elif (request_kind == _RequestOutcome.NotServedFromCache and
+        not request_event.from_disk_cache):
+      urls.add(request_event.url)
+    elif request_kind == _RequestOutcome.All:
+      urls.add(request_event.url)
+  return urls
+
+
+def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
+                                   benchmark_output_directory_path):
+  """Verifies that all run inside the run_output_directory worked as expected.
+
+  Args:
+    benchmark_setup_path: Path of the JSON of the benchmark setup.
+    benchmark_output_directory_path: Path of the benchmark output directory to
+        verify.
+  """
+  # TODO(gabadie): What's the best way of propagating errors happening in here?
+  benchmark_setup = json.load(open(benchmark_setup_path))
+  cache_whitelist = set(benchmark_setup['cache_whitelist'])
+  url_resources = set(benchmark_setup['url_resources'])
+
+  # Verify requests from traces.
+  run_id = -1
+  while True:
+    run_id += 1
+    run_path = os.path.join(benchmark_output_directory_path, str(run_id))
+    if not os.path.isdir(run_path):
+      break
+    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
+    if not os.path.isfile(trace_path):
+      logging.error('missing trace %s' % trace_path)
+      continue
+    trace = LoadingTrace.FromJsonFile(trace_path)
+    logging.info('verifying %s from %s' % (trace.url, trace_path))
+    _PrintUrlSetComparison(url_resources,
+        _ListUrlRequests(trace, _RequestOutcome.All), 'All resources')
+    _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
+        _ListUrlRequests(trace, _RequestOutcome.ServedFromCache),
+        'Cached resources')
+    _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
+        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache),
+        'Non cached resources')
+
+
+def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
+  """Extracts a map URL-to-subresources for each navigation in benchmark
+  directory.
+
+  Args:
+    benchmark_output_directory_path: Path of the benchmark output directory to
+        verify.
+
+  Returns:
+    {url -> [URLs of sub-resources]}
+  """
+  url_subresources = {}
+  run_id = -1
+  while True:
+    run_id += 1
+    run_path = os.path.join(benchmark_output_directory_path, str(run_id))
+    if not os.path.isdir(run_path):
+      break
+    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
+    if not os.path.isfile(trace_path):
+      continue
+    trace = LoadingTrace.FromJsonFile(trace_path)
+    if trace.url in url_subresources:
+      continue
+    logging.info('lists resources of %s from %s' % (trace.url, trace_path))
+    urls_set = set()
+    for request_event in trace.request_track.GetEvents():
+      if not request_event.protocol.startswith('http'):
+        continue
+      if request_event.url not in urls_set:
+        logging.info('  %s' % request_event.url)
+        urls_set.add(request_event.url)
+    url_subresources[trace.url] = [url for url in urls_set]
+  return url_subresources
+
+
+def ValidateCacheArchiveContent(ref_urls, cache_archive_path):
+  """Validates a cache archive content.
+
+  Args:
+    ref_urls: Reference list of urls.
+    cache_archive_path: Cache archive's path to validate.
+  """
+  # TODO(gabadie): What's the best way of propagating errors happening in here?
+  logging.info('lists cached urls from %s' % cache_archive_path)
+  with common_util.TemporaryDirectory() as cache_directory:
+    chrome_cache.UnzipDirectoryContent(cache_archive_path, cache_directory)
+    cached_urls = \
+        chrome_cache.CacheBackend(cache_directory, 'simple').ListKeys()
+  _PrintUrlSetComparison(set(ref_urls), set(cached_urls), 'cached resources')
diff --git a/loading/sandwich_misc_unittest.py b/loading/sandwich_misc_unittest.py
index f75288c..9e5e455 100644
--- a/loading/sandwich_misc_unittest.py
+++ b/loading/sandwich_misc_unittest.py
@@ -19,26 +19,40 @@ class SandwichMiscTest(unittest.TestCase):
   def GetResourceUrl(self, path):
     return urlparse.urljoin('http://l/', path)
 
+  def testNoDiscovererWhitelisting(self):
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.EMPTY_CACHE_DISCOVERER)
+    self.assertEquals(set(), url_set)
+
+  def testFullCacheWhitelisting(self):
+    reference_url_set = set([self.GetResourceUrl('./'),
+                             self.GetResourceUrl('0.png'),
+                             self.GetResourceUrl('1.png'),
+                             self.GetResourceUrl('favicon.ico')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.FULL_CACHE_DISCOVERER)
+    self.assertEquals(reference_url_set, url_set)
+
   def testRedirectedMainWhitelisting(self):
-    urls_set_ref = set([self.GetResourceUrl('./')])
-    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+    reference_url_set = set([self.GetResourceUrl('./')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
         self._TRACE_PATH, sandwich_misc.REDIRECTED_MAIN_DISCOVERER)
-    self.assertEquals(urls_set_ref, urls_set)
+    self.assertEquals(reference_url_set, url_set)
 
   def testParserDiscoverableWhitelisting(self):
-    urls_set_ref = set([self.GetResourceUrl('./'),
-                        self.GetResourceUrl('0.png'),
-                        self.GetResourceUrl('1.png')])
-    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+    reference_url_set = set([self.GetResourceUrl('./'),
+                             self.GetResourceUrl('0.png'),
+                             self.GetResourceUrl('1.png')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
         self._TRACE_PATH, sandwich_misc.PARSER_DISCOVERER)
-    self.assertEquals(urls_set_ref, urls_set)
+    self.assertEquals(reference_url_set, url_set)
 
   def testHTMLPreloadScannerWhitelisting(self):
-    urls_set_ref = set([self.GetResourceUrl('./'),
-                        self.GetResourceUrl('0.png')])
-    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+    reference_url_set = set([self.GetResourceUrl('./'),
+                             self.GetResourceUrl('0.png')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
         self._TRACE_PATH, sandwich_misc.HTML_PRELOAD_SCANNER_DISCOVERER)
-    self.assertEquals(urls_set_ref, urls_set)
+    self.assertEquals(reference_url_set, url_set)
 
 
 if __name__ == '__main__':
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 39bf4d6..0d6d105 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -20,9 +20,16 @@ import controller
 import devtools_monitor
 import device_setup
 import loading_trace
-import sandwich_metrics
 
 
+# Standard filenames in the sandwich runner's output directory.
+TRACE_FILENAME = 'trace.json'
+VIDEO_FILENAME = 'video.mp4'
+
+# List of selected trace event categories when running chrome.
+ADDITIONAL_CATEGORIES = (
+    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
+
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
 # Devtools timeout of 1 minute to avoid websocket timeout on slow
@@ -197,24 +204,24 @@ class SandwichRunner(object):
       if run_path is not None and self.record_video:
         device = self._chrome_ctl.GetDevice()
         assert device, 'Can only record video on a remote device.'
-        video_recording_path = os.path.join(run_path, 'video.mp4')
+        video_recording_path = os.path.join(run_path, VIDEO_FILENAME)
         with device_setup.RemoteSpeedIndexRecorder(device, connection,
                                                    video_recording_path):
           trace = loading_trace.LoadingTrace.RecordUrlNavigation(
               url=url,
               connection=connection,
               chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
+              additional_categories=ADDITIONAL_CATEGORIES,
               timeout_seconds=_DEVTOOLS_TIMEOUT)
       else:
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
             url=url,
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
+            additional_categories=ADDITIONAL_CATEGORIES,
             timeout_seconds=_DEVTOOLS_TIMEOUT)
     if run_path is not None:
-      trace_path = os.path.join(run_path, 'trace.json')
+      trace_path = os.path.join(run_path, TRACE_FILENAME)
       trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, run_id):
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
new file mode 100644
index 0000000..1120d77
--- /dev/null
+++ b/loading/sandwich_task_builder.py
@@ -0,0 +1,246 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import csv
+import json
+import os
+import shutil
+
+import chrome_cache
+import common_util
+import emulation
+import sandwich_metrics
+import sandwich_misc
+import sandwich_runner
+import task_manager
+
+
+def NetworkSimulationTransformer(network_condition):
+  """Creates a function that accepts a SandwichRunner as a parameter and sets
+  network emulation options on it.
+
+  Args:
+    network_condition: The network condition to apply to the sandwich runner.
+
+  Returns:
+    A callback transforming the SandwichRunner given in argument accordingly
+  """
+  assert network_condition in emulation.NETWORK_CONDITIONS
+  def Transformer(runner):
+    assert isinstance(runner, sandwich_runner.SandwichRunner)
+    runner.network_condition = network_condition
+  return Transformer
+
+
+class SandwichTaskBuilder(task_manager.Builder):
+  """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
+  """
+
+  def __init__(self, output_directory, job_path, url_repeat):
+    """Constructor.
+
+    Args:
+      output_directory: As in task_manager.Builder.__init__
+      job_path: Path of the sandwich's job.
+      url_repeat: Non null integer controlling how many times the URLs should be
+        repeated in the benchmarks.
+    """
+    task_manager.Builder.__init__(self, output_directory)
+    self._job_path = job_path
+    self._url_repeat = url_repeat
+    self._default_final_tasks = []
+
+    self._original_wpr_task = None
+    self._patched_wpr_task = None
+    self._reference_cache_task = None
+    self._subresources_for_urls_run_task = None
+    self._subresources_for_urls_task = None
+
+  @property
+  def default_final_tasks(self):
+      return self._default_final_tasks
+
+  def _CreateSandwichRunner(self):
+    """Create a runner for non benchmark purposes."""
+    runner = sandwich_runner.SandwichRunner()
+    runner.LoadJob(self._job_path)
+    return runner
+
+  def OverridePathToWprArchive(self, original_wpr_path):
+    """Sets the original WPR archive path's to be used.
+
+    Args:
+      original_wpr_path: Path of the original WPR archive to be used.
+    """
+    self._original_wpr_task = \
+        self.CreateStaticTask('common/webpages.wpr', original_wpr_path)
+
+  def PopulateWprRecordingTask(self):
+    """Records the original WPR archive."""
+    @self.RegisterTask('common/webpages.wpr')
+    def BuildOriginalWpr():
+      common_util.EnsureParentDirectoryExists(BuildOriginalWpr.path)
+      runner = self._CreateSandwichRunner()
+      runner.wpr_archive_path = BuildOriginalWpr.path
+      runner.wpr_record = True
+      runner.Run()
+
+    self._original_wpr_task = BuildOriginalWpr
+
+  def PopulateCommonPipelines(self):
+    """Creates necessary tasks to produce initial cache archive.
+
+    Also creates a task for producing a json file with a mapping of URLs to
+    subresources (urls-resources.json).
+
+    Here is the full dependency tree for the returned task:
+    common/cache-ref-validation.log
+      depends on: common/cache-ref.zip
+        depends on: common/webpages-patched.wpr
+          depends on: common/webpages.wpr
+      depends on: common/urls-resources.json
+        depends on: common/urls-resources-run/
+          depends on: common/webpages.wpr
+
+    Returns:
+      The last task of the pipeline.
+    """
+    @self.RegisterTask('common/webpages-patched.wpr', [self._original_wpr_task])
+    def BuildPatchedWpr():
+      common_util.EnsureParentDirectoryExists(BuildPatchedWpr.path)
+      shutil.copyfile(self._original_wpr_task.path, BuildPatchedWpr.path)
+      sandwich_misc.PatchWpr(BuildPatchedWpr.path)
+
+    @self.RegisterTask('common/cache-ref.zip', [BuildPatchedWpr])
+    def BuildReferenceCache():
+      runner = self._CreateSandwichRunner()
+      runner.wpr_archive_path = BuildPatchedWpr.path
+      runner.cache_archive_path = BuildReferenceCache.path
+      runner.cache_operation = 'save'
+      runner.Run()
+
+    @self.RegisterTask('common/subresources-for-urls-run/',
+                       dependencies=[self._original_wpr_task])
+    def UrlsResourcesRun():
+      runner = self._CreateSandwichRunner()
+      runner.wpr_archive_path = self._original_wpr_task.path
+      runner.cache_operation = 'clear'
+      runner.trace_output_directory = UrlsResourcesRun.path
+      runner.Run()
+
+    @self.RegisterTask('common/subresources-for-urls.json', [UrlsResourcesRun])
+    def ListUrlsResources():
+      json_content = sandwich_misc.ReadSubresourceMapFromBenchmarkOutput(
+          UrlsResourcesRun.path)
+      with open(ListUrlsResources.path, 'w') as output:
+        json.dump(json_content, output)
+
+    @self.RegisterTask('common/cache-ref-validation.log',
+                       [BuildReferenceCache, ListUrlsResources])
+    def ValidateReferenceCache():
+      json_content = json.load(open(ListUrlsResources.path))
+      ref_urls = set()
+      for urls in json_content.values():
+        ref_urls.update(set(urls))
+      sandwich_misc.ValidateCacheArchiveContent(
+          ref_urls, BuildReferenceCache.path)
+
+    self._patched_wpr_task = BuildPatchedWpr
+    self._reference_cache_task = BuildReferenceCache
+    self._subresources_for_urls_run_task = UrlsResourcesRun
+    self._subresources_for_urls_task = ListUrlsResources
+
+    self._default_final_tasks.append(ValidateReferenceCache)
+    return ValidateReferenceCache
+
+  def PopulateLoadBenchmark(self, subresource_discoverer,
+                            runner_transformer_name, runner_transformer):
+    """Populate benchmarking tasks from its setup tasks.
+
+    Args:
+      subresource_discoverer: Name of a subresources discoverer.
+      runner_transformer: A function that takes an instance of SandwichRunner as
+          parameter, would be applied immediately before SandwichRunner.Run().
+      runner_transformer_name: Name of the runner transformer used to generate
+          task names.
+      benchmark_name: The benchmark's name for that runner modifier.
+
+    Here is the full dependency of the added tree for the returned task:
+    <runner_transformer_name>/<subresource_discoverer>-metrics.csv
+      depends on: <runner_transformer_name>/<subresource_discoverer>-run/
+        depends on: common/<subresource_discoverer>-cache.zip
+          depends on: some tasks saved by PopulateCommonPipelines()
+          depends on: common/<subresource_discoverer>-setup.json
+            depends on: some tasks saved by PopulateCommonPipelines()
+
+    Returns:
+      task_manager.Task for
+          <runner_transformer_name>/<subresource_discoverer>-metrics.csv
+    """
+    assert subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS
+    assert 'common' not in sandwich_misc.SUBRESOURCE_DISCOVERERS
+    shared_task_prefix = os.path.join('common', subresource_discoverer)
+    task_prefix = os.path.join(runner_transformer_name, subresource_discoverer)
+
+    @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
+                       dependencies=[self._subresources_for_urls_task])
+    def SetupBenchmark():
+      trace_path = os.path.join(self._subresources_for_urls_run_task.path, '0',
+                                sandwich_runner.TRACE_FILENAME)
+      whitelisted_urls = sandwich_misc.ExtractDiscoverableUrls(
+          trace_path, subresource_discoverer)
+
+      urls_resources = json.load(open(self._subresources_for_urls_task.path))
+      # TODO(gabadie): Implement support for multiple URLs in this Task.
+      assert len(urls_resources) == 1
+      url = urls_resources.keys()[0]
+      url_resources = urls_resources[url]
+      common_util.EnsureParentDirectoryExists(SetupBenchmark.path)
+      with open(SetupBenchmark.path, 'w') as output:
+        json.dump({
+            'cache_whitelist': [url for url in whitelisted_urls],
+            'url_resources': url_resources,
+          }, output)
+
+    @self.RegisterTask(shared_task_prefix + '-cache.zip', merge=True,
+                       dependencies=[
+                           SetupBenchmark, self._reference_cache_task])
+    def BuildBenchmarkCacheArchive():
+      setup = json.load(open(SetupBenchmark.path))
+      chrome_cache.ApplyUrlWhitelistToCacheArchive(
+          cache_archive_path=self._reference_cache_task.path,
+          whitelisted_urls=setup['cache_whitelist'],
+          output_cache_archive_path=BuildBenchmarkCacheArchive.path)
+
+    @self.RegisterTask(task_prefix + '-run/',
+                       dependencies=[BuildBenchmarkCacheArchive])
+    def RunBenchmark():
+      runner = self._CreateSandwichRunner()
+      # runner.record_video = True
+      runner.job_repeat = self._url_repeat
+      runner_transformer(runner)
+      runner.wpr_archive_path = self._patched_wpr_task.path
+      runner.wpr_out_log_path = os.path.join(RunBenchmark.path, 'wpr.log')
+      runner.cache_archive_path = BuildBenchmarkCacheArchive.path
+      runner.cache_operation = 'push'
+      runner.trace_output_directory = RunBenchmark.path
+      runner.Run()
+
+    @self.RegisterTask(task_prefix + '-metrics.csv',
+                       dependencies=[RunBenchmark])
+    def ExtractMetrics():
+      sandwich_misc.VerifyBenchmarkOutputDirectory(
+          SetupBenchmark.path, RunBenchmark.path)
+      trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
+          RunBenchmark.path)
+      trace_metrics_list.sort(key=lambda e: e['id'])
+      with open(ExtractMetrics.path, 'w') as csv_file:
+        writer = csv.DictWriter(csv_file,
+                                fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
+        writer.writeheader()
+        for trace_metrics in trace_metrics_list:
+          writer.writerow(trace_metrics)
+
+    self._default_final_tasks.append(ExtractMetrics)
+    return ExtractMetrics

commit 569f5b62652fd5049af0f1df00bf3e1015609039
Author: lizeb <lizeb@chromium.org>
Date:   Wed Apr 27 03:54:49 2016 -0700

    clovis: Enable the disabled-by-default-blink.debug.layout tracing category.
    
    This category is used by FirstSignificantPaintLens, as pointed out by mattcary@.
    
    Review URL: https://codereview.chromium.org/1924453002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390041}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 188dfe59bb04b695ee7186758a75b9ee8a1afce5

diff --git a/loading/tracing.py b/loading/tracing.py
index c1ecf71..068c5b6 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -14,9 +14,9 @@ import devtools_monitor
 
 _DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
 INITIAL_CATEGORIES = (
-    'toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
-    'blink.user_timing', 'blink.net') + tuple(
-        '-' + cat for cat in _DISABLED_CATEGORIES)
+    ('toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
+     'blink.user_timing', 'blink.net', 'disabled-by-default-blink.debug.layout')
+    + tuple('-' + cat for cat in _DISABLED_CATEGORIES))
 
 
 class TracingTrack(devtools_monitor.Track):
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 89ce7a1..dd99d79 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -91,6 +91,11 @@ class _FirstEventLens(_UserSatisfiedLens):
   # pylint: disable=abstract-method
 
   @classmethod
+  def _CheckCategory(cls, tracing_track, category):
+    assert category in tracing_track.Categories(), (
+        'The "%s" category must be enabled.' % category)
+
+  @classmethod
   def _ExtractFirstTiming(cls, times):
     if not times:
       return float('inf')
@@ -106,9 +111,11 @@ class FirstTextPaintLens(_FirstEventLens):
 
   This event is taken directly from a trace.
   """
+  _EVENT_CATEGORY = 'blink.user_timing'
   def _CalculateTimes(self, tracing_track):
+    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
     first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches('blink.user_timing', 'firstPaint')]
+                    if e.Matches(self._EVENT_CATEGORY, 'firstPaint')]
     self._satisfied_msec = self._event_msec = \
         self._ExtractFirstTiming(first_paints)
 
@@ -119,9 +126,11 @@ class FirstContentfulPaintLens(_FirstEventLens):
   This event is taken directly from a trace. Internally to chrome it's computed
   by filtering out things like background paint from firstPaint.
   """
+  _EVENT_CATEGORY = 'blink.user_timing'
   def _CalculateTimes(self, tracing_track):
+    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
     first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches('blink.user_timing', 'firstContentfulPaint')]
+                    if e.Matches(self._EVENT_CATEGORY, 'firstContentfulPaint')]
     self._satisfied_msec = self._event_msec = \
        self._ExtractFirstTiming(first_paints)
 
@@ -133,22 +142,22 @@ class FirstSignificantPaintLens(_FirstEventLens):
   been loaded to compute the layout. Our event time is that of the next paint as
   that is the observable event.
   """
-  FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
-
+  _FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
+  _EVENT_CATEGORY = 'disabled-by-default-blink.debug.layout'
   def _CalculateTimes(self, tracing_track):
+    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
     sync_paint_times = []
     layouts = []  # (layout item count, msec).
     for e in tracing_track.GetEvents():
       # TODO(mattcary): is this the right paint event? Check if synchronized
       # paints appear at the same time as the first*Paint events, above.
-      if e.Matches('blink', 'FrameView::SynchronizedPaint'):
+      if e.Matches('blink', 'FrameView::synchronizedPaint'):
         sync_paint_times.append(e.start_msec)
       if ('counters' in e.args and
-          self.FIRST_LAYOUT_COUNTER in e.args['counters']):
-        layouts.append((e.args['counters'][self.FIRST_LAYOUT_COUNTER],
+          self._FIRST_LAYOUT_COUNTER in e.args['counters']):
+        layouts.append((e.args['counters'][self._FIRST_LAYOUT_COUNTER],
                         e.start_msec))
-    assert layouts, ('No layout events, was the disabled-by-default-blink'
-                     '.debug.layout category enabled?')
+    assert layouts, 'No layout events'
     layouts.sort(key=operator.itemgetter(0), reverse=True)
     self._satisfied_msec = layouts[0][1]
     self._event_msec = self._ExtractFirstTiming([
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 2e35a50..c7cdb10 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -88,13 +88,13 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
                        'name': 'firstPaint'},
                       {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'FrameView::SynchronizedPaint'},
+                       'name': 'FrameView::synchronizedPaint'},
                       {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink',
-                       'name': 'FrameView::SynchronizedPaint'},
+                       'name': 'FrameView::synchronizedPaint'},
                       {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink',
-                       'name': 'FrameView::SynchronizedPaint'},
+                       'name': 'FrameView::synchronizedPaint'},
 
                       {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'foobar', 'name': 'biz',
@@ -114,3 +114,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
     self.assertEqual(7, lens.PostloadTimeMsec())
+
+
+if __name__ == '__main__':
+  unittest.main()

commit d4b3a72bcaebbd31b864c13d5fa8ddd0ea21c629
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 01:45:36 2016 -0700

    tools/android/loading: Improve LocalChromeController's chrome launch
    
    Before, the LocalChromeController was waiting 10 seconds before
    connecting to chrome's devtools connection. This CL replace this
    sleep by a devtool connection attempt every one seconds.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1912123002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390024}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 240be2d4535ca60df1b9e44cf6a4b70a3be7ce89

diff --git a/loading/controller.py b/loading/controller.py
index 96eec7f..50595e4 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,6 +10,8 @@ desktop-specific versions.
 """
 
 import contextlib
+import datetime
+import errno
 import logging
 import os
 import shutil
@@ -39,6 +41,9 @@ class ChromeControllerBase(object):
 
   Defines common operations but should not be created directly.
   """
+  DEVTOOLS_CONNECTION_ATTEMPTS = 10
+  DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
+
   def __init__(self):
     self._chrome_args = [
         # Disable backgound network requests that may pollute WPR archive,
@@ -174,6 +179,9 @@ class ChromeControllerBase(object):
     if self._emulated_network:
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
+    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
+                          seconds_since_epoch=time.time())
+    logging.info('Devtools connection success')
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
@@ -182,12 +190,6 @@ class ChromeControllerBase(object):
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
-  # Number of connection attempt to chrome's devtools.
-  DEVTOOLS_CONNECTION_ATTEMPTS = 10
-
-  # Time interval in seconds between chrome's devtools connection attempts.
-  DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
-
   # An estimate of time to wait for the device to become idle after expensive
   # operations, such as opening the launcher activity.
   TIME_TO_IDLE_SECONDS = 2
@@ -226,10 +228,7 @@ class RemoteChromeController(ChromeControllerBase):
           data='about:blank')
       self._device.StartActivity(start_intent, blocking=True)
       try:
-        for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS + 1):
-          if attempt_id == self.DEVTOOLS_CONNECTION_ATTEMPTS:
-            raise RuntimeError('Failed to connect to chrome devtools after {} '
-                               'attempts.'.format(attempt_id))
+        for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
           logging.info('Devtools connection attempt %d' % attempt_id)
           with device_setup.ForwardPort(
               self._device, 'tcp:%d' % OPTIONS.devtools_port,
@@ -239,15 +238,18 @@ class RemoteChromeController(ChromeControllerBase):
                   OPTIONS.devtools_hostname, OPTIONS.devtools_port)
               self._StartConnection(connection)
             except socket.error as e:
-              assert str(e).startswith('[Errno 104] Connection reset by peer')
+              if e.errno != errno.ECONNRESET:
+                raise
               time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
               continue
-            logging.info('Devtools connection success')
             yield connection
             if self._slow_death:
               self._device.adb.Shell('am start com.google.android.launcher')
               time.sleep(self.TIME_TO_IDLE_SECONDS)
             break
+        else:
+          raise RuntimeError('Failed to connect to chrome devtools after {} '
+                             'attempts.'.format(attempt_id))
       finally:
         self._device.ForceStop(package_info.package)
 
@@ -326,21 +328,32 @@ class LocalChromeController(ChromeControllerBase):
                                       env=environment)
     connection = None
     try:
-      time.sleep(10)
-      process_result = chrome_process.poll()
-      if process_result is not None:
-        logging.error('Unexpected process exit: %s', process_result)
+      # Attempt to connect to Chrome's devtools
+      for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
+        logging.info('Devtools connection attempt %d' % attempt_id)
+        process_result = chrome_process.poll()
+        if process_result is not None:
+          raise RuntimeError('Unexpected Chrome exit: %s', process_result)
+        try:
+          connection = devtools_monitor.DevToolsConnection(
+              OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+          break
+        except socket.error as e:
+          if e.errno != errno.ECONNREFUSED:
+            raise
+          time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
       else:
-        connection = devtools_monitor.DevToolsConnection(
-            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-        self._StartConnection(connection)
-        yield connection
-        if self._slow_death:
-          connection.Close()
-          connection = None
-          chrome_process.wait()
+        raise RuntimeError('Failed to connect to Chrome devtools after {} '
+                           'attempts.'.format(attempt_id))
+      # Start and yield the devtool connection.
+      self._StartConnection(connection)
+      yield connection
+      if self._slow_death:
+        connection.Close()
+        chrome_process.wait()
+        chrome_process = None
     finally:
-      if connection:
+      if chrome_process:
         chrome_process.kill()
       if self._headless:
         xvfb_process.kill()

commit 46ea801f27a7976d32ed47afdbe411cc63232733
Author: Newton Allen <newt@google.com>
Date:   Tue Apr 26 17:32:51 2016 -0700

    Remove newt@ from OWNERS and add replacements in a few cases.
    
    R=finnur@chromium.org, ianwen@chromium.org, sadrul@chromium.org, tedchoc@chromium.org, twellington@chromium.org
    
    Review URL: https://codereview.chromium.org/1915173002 .
    
    Cr-Original-Commit-Position: refs/heads/master@{#389956}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 963b18fb2197513651ad508974d9f78c7136867e

diff --git a/checkstyle/OWNERS b/checkstyle/OWNERS
deleted file mode 100644
index 577bf98..0000000
--- a/checkstyle/OWNERS
+++ /dev/null
@@ -1,2 +0,0 @@
-aurimas@chromium.org
-newt@chromium.org

commit 470798b904a09cc5aecab57867bbe95ecace6153
Author: lizeb <lizeb@chromium.org>
Date:   Tue Apr 26 10:33:36 2016 -0700

    clovis: Failed requests have no timings, don't crash in the serialization.
    
    Review URL: https://codereview.chromium.org/1915853007
    
    Cr-Original-Commit-Position: refs/heads/master@{#389815}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2f9e27bf548149fa3ba711f6151ef6b027ffc141

diff --git a/loading/request_track.py b/loading/request_track.py
index 31eca51..b4e3a15 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -220,7 +220,7 @@ class Request(object):
 
   def ToJsonDict(self):
     result = copy.deepcopy(self.__dict__)
-    result['timing'] = self.timing.ToJsonDict()
+    result['timing'] = self.timing.ToJsonDict() if self.timing else {}
     return result
 
   @classmethod

commit 1f80fd36042d5f238ccf4f4fbe81acd768186f9e
Author: jbudorick <jbudorick@chromium.org>
Date:   Tue Apr 26 10:11:20 2016 -0700

    [Android] Make tools/android/forwarder build with the current toolchain.
    
    BUG=604468
    
    Review URL: https://codereview.chromium.org/1915293005
    
    Cr-Original-Commit-Position: refs/heads/master@{#389812}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c2e43c5c93451f112a1aefc4d2677a98c1185f7e

diff --git a/forwarder/BUILD.gn b/forwarder/BUILD.gn
index 9cc99b4..c59cd90 100644
--- a/forwarder/BUILD.gn
+++ b/forwarder/BUILD.gn
@@ -4,21 +4,14 @@
 
 import("//build/symlink.gni")
 
-if (current_toolchain == host_toolchain) {
-  # GYP: //tools/android/forwarder/forwarder.gyp:forwarder
-  executable("forwarder") {
-    sources = [
-      "forwarder.cc",
-    ]
-    deps = [
-      "//base",
-      "//build/config/sanitizers:deps",
-      "//tools/android/common",
-    ]
-  }
-} else {
-  # Create a symlink from root_build_dir -> clang_x64/forwarder.
-  binary_symlink("forwarder") {
-    binary_label = ":$target_name($host_toolchain)"
-  }
+# GYP: //tools/android/forwarder/forwarder.gyp:forwarder
+executable("forwarder") {
+  sources = [
+    "forwarder.cc",
+  ]
+  deps = [
+    "//base",
+    "//build/config/sanitizers:deps",
+    "//tools/android/common",
+  ]
 }

commit 074b718c84b9b043f27e23c94a2da0d9eed8de5e
Author: droger <droger@chromium.org>
Date:   Tue Apr 26 06:10:19 2016 -0700

    tools/android/loading Cleanup clovis frontend
    
    Due to a bad rebase, CL https://codereview.chromium.org/1907233003/
    undid some of the changes from CL
    https://codereview.chromium.org/1920793003/.
    
    This new CL re-applies the changes.
    
    Review URL: https://codereview.chromium.org/1914163002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389764}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 488d24ccce7f9649be3abdbd6083068a0d4c0af6

diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index c00f43b..3f766dd 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -3,7 +3,6 @@
 # found in the LICENSE file.
 
 import json
-from googleapiclient import (discovery, errors)
 import time
 
 from googleapiclient import (discovery, errors)
@@ -12,22 +11,21 @@ from googleapiclient import (discovery, errors)
 class GoogleInstanceHelper(object):
   """Helper class for the Google Compute API, allowing to manage groups of
   instances more easily. Groups of instances are identified by a tag."""
+  _COMPUTE_API_ROOT = 'https://www.googleapis.com/compute/v1/projects/'
 
   def __init__(self, credentials, project, logger):
     self._compute_api = discovery.build('compute','v1', credentials=credentials)
     self._project = project
-    self._api_url = 'https://www.googleapis.com/compute/v1/projects/' + project
+    self._project_api_url = self._COMPUTE_API_ROOT + project
     self._zone = 'europe-west1-c'
     self._logger = logger
 
   def _ExecuteApiRequest(self, request, retry_count=3):
     """ Executes a Compute API request and returns True on success."""
-    self._logger.info('Compute API request:')
-    self._logger.info(request.to_json())
+    self._logger.info('Compute API request:\n' + request.to_json())
     try:
       response = request.execute()
-      self._logger.info('Compute API response:')
-      self._logger.info(response)
+      self._logger.info('Compute API response:\n' + response)
       return True
     except errors.HttpError as err:
       error_content = self._GetErrorContent(err)
@@ -56,7 +54,7 @@ class GoogleInstanceHelper(object):
 
   def _GetErrorContent(self, error):
     """Returns the contents of an error returned by the Compute API as a
-    dictionary.
+    dictionary or None.
     """
     if not error.resp.get('content-type', '').startswith('application/json'):
       return None
@@ -70,22 +68,22 @@ class GoogleInstanceHelper(object):
         not error_content['error'].get('errors')):
       return None
     error_list = error_content['error']['errors']
-    if len(error_list) == 0:
+    if not error_list:
       return None
-    return error_list[0].get('reason', '')
+    return error_list[0].get('reason')
 
   def CreateTemplate(self, tag, bucket):
     """Creates an instance template for instances identified by tag and using
     bucket for deployment. Returns True if successful.
     """
-    image_url = 'https://www.googleapis.com/compute/v1/projects/' \
+    image_url = self._COMPUTE_API_ROOT + \
                 'ubuntu-os-cloud/global/images/ubuntu-1404-trusty-v20160406'
     request_body = {
         'name': self._GetTemplateName(tag),
         'properties': {
             'machineType': 'n1-standard-1',
             'networkInterfaces': [{
-                'network': self._api_url + '/global/networks/default',
+                'network': self._project_api_url + '/global/networks/default',
                 'accessConfigs': [{
                     'name': 'external-IP',
                     'type': 'ONE_TO_ONE_NAT'
@@ -130,7 +128,7 @@ class GoogleInstanceHelper(object):
     exist for this to succeed. Returns True if successful.
     """
     template_url = '%s/global/instanceTemplates/%s' % (
-        self._api_url, self._GetTemplateName(tag))
+        self._project_api_url, self._GetTemplateName(tag))
     request_body = {
         'zone': self._zone, 'targetSize': instance_count,
         'baseInstanceName': 'instance-' + tag,
@@ -148,7 +146,7 @@ class GoogleInstanceHelper(object):
     # The instance hostname may be of the form <name>.c.<project>.internal but
     # only the <name> part should be passed to the compute API.
     name = instance_hostname.split('.')[0]
-    instance_url = self._api_url + (
+    instance_url = self._project_api_url + (
         "/zones/%s/instances/%s" % (self._zone, name))
     request = self._compute_api.instanceGroupManagers().deleteInstances(
         project=self._project, zone=self._zone,
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index b037a3d..656fa9d 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -2,13 +2,14 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-import flask
-from google.appengine.api import (app_identity, taskqueue)
-from oauth2client.client import GoogleCredentials
 import logging
 import os
 import sys
 
+import flask
+from google.appengine.api import (app_identity, taskqueue)
+from oauth2client.client import GoogleCredentials
+
 from common.clovis_task import ClovisTask
 import common.google_instance_helper
 from memory_logs import MemoryLogs

commit 3348b0077d92112bf998f1ac18b3f8718319d6e2
Author: droger <droger@chromium.org>
Date:   Tue Apr 26 03:19:39 2016 -0700

    tools/android/loading VM instances self destruct when finished
    
    Review URL: https://codereview.chromium.org/1919923002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389740}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6aa684686a2819515d31d39d873e06c72b195dca

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index b181cfa..e9e1a9a 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -1,16 +1,34 @@
 # Clovis in the Cloud: Developer Guide
 
-This document describes how to collect Chromium traces using Google Compute
-Engine.
+This document describes the backend-side of the trace collection, using Google
+Compute Engine.
+
+When the [frontend][3] spawns new tasks, it pushes them into a [TaskQueue][4]
+called `clovis-queue` with a unique tag.
+Then it creates backend instances (as an instance group) and passes them the
+TaskQueue tag.
+
+The backend instances then pull tasks from the TaskQueue and process them until
+it is empty. When there is no task left in the queue, the backend instances
+kill themselves.
+
+The main files for the backend are:
+
+-   `startup-script.sh`: initializes an instance (installs the dependencies,
+    downloads the code and the configuration).
+-   `worker.py`: the main worker script.
 
 [TOC]
 
-## Initial setup
+## Initial setup for development
 
 Install the [gcloud command line tool][1].
 
 ## Deploy the code
 
+This step deploys all the source code needed by the backend workers, as well as
+the Chromium binaries required for trace collection.
+
 ```shell
 # Build Chrome (do not use the component build).
 BUILD_DIR=out/Release
@@ -25,58 +43,97 @@ ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
 
 ## Start the app in the cloud
 
-Create an instance using latest ubuntu LTS:
+The application is automatically started by the frontend, and should not need to
+be started manually.
+
+If you really want to create an instance manually (when debugging for example),
+this can be done like this:
 
 ```shell
-gcloud compute instances create clovis-tracer-1 \
+gcloud compute instances create $INSTANCE_NAME \
  --machine-type n1-standard-1 \
  --image ubuntu-14-04 \
  --zone europe-west1-c \
  --scopes cloud-platform,https://www.googleapis.com/auth/cloud-taskqueue \
- --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,taskqueue_tag=some_tag \
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,taskqueue-tag=some_tag \
  --metadata-from-file \
      startup-script=$CHROMIUM_SRC/tools/android/loading/cloud/backend/startup-script.sh
 ```
 
-**Note:** To start an instance without automatically starting the app on it,
-add a `auto-start=false` metadata. This can be useful when doing iterative
-development on the instance, to be able to restart the app manually.
-
-This should output the IP address of the instance.
-Otherwise the IP address can be retrieved by doing:
+If you are debbugging, you probably want to set additional metadata:
+
+-   `auto-start=false`: to start an instance without automatically starting the
+    app on it. This can be useful when doing iterative development on the
+    instance using ssh, to be able to stop and restart the app manually.
+-   `self-destruct=false`: to prevent the instance from self-destructing when
+    the queue is empty.
+
+**Notes:**
+
+-   If you use `auto-start=false`, and then try to ssh on the instance and
+    launch `worker.py`, it will not work because of various issues, such as:
+    -   Environment variables defined by the startup script are not available
+        to your user and you will need to redefine them.
+    -   You will not have permissions to access the files, and need to run
+        `sudo chown` to give yourself permissions.
+    -   You need to activate `virtualenv`.
+    Get in touch with *droger@* if you need this or want to improve it.
+-   It can take a few minutes for the instance to start. You can follow the
+    progress of the startup script on the gcloud console web interface (menu
+    "Compute Engine" > "VM instances" then click on your instance and scroll
+    down to see the "Serial console output") or from the command line using:
 
 ```shell
-gcloud compute instances list
+gcloud compute instances get-serial-port-output $INSTANCE_NAME
 ```
 
-**Note:** It can take a few minutes for the instance to start. You can follow
-the progress of the startup script on the gcloud console web interface (menu
-"Compute Engine" > "VM instances" then click on your instance and scroll down to
-see the "Serial console output") or from the command line using:
+## `worker.py` configuration file
 
-```shell
-gcloud compute instances get-serial-port-output clovis-tracer-1
-```
+`worker.py` takes a configuration file as command line parameter. This is a JSON
+dictionary with the keys:
+
+-   `project_name` (string): Name of the Google Cloud project
+-   `cloud_storage_path` (string): Path in Google Storage where generated traces
+    will be stored.
+-   `chrome_path` (string): Path to the Chrome executable.
+-   `src_path` (string): Path to the Chromium source directory.
+-   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
+    `clovis-queue`.
+-   `trace_database_filename` (string, optional): Filename for the trace
+    database in Cloud Storage. Must be unique per worker to avoid concurrent
+    access. Defaults to `trace_database.json`.
+-   `destruct_instance_name` (string, optional): Name of the instance the worker
+    will destroy when there are no remaining tasks to process. This is only
+    relevant when running in the cloud.
 
 ## Use the app
 
 Create tasks from the associated AppEngine application, see [documentation][3].
-Make sure the `taskqueue_tag` of the AppEngine request matches the one of the
-ComputeEngine instances.
+
+If you want the frontend to send tasks to a particular instance that you created
+manually, make sure the `tag` and `storage_bucket` of the AppEngine request
+match the ones of your ComputeEngine instance, and set `instance_count` to `0`.
 
 ## Stop the app in the cloud
 
+To stop a single instance that you started manually, do:
+
 ```shell
-gcloud compute instances delete clovis-tracer-1
+gcloud compute instances delete $INSTANCE_NAME
 ```
 
+To stop instances that were created by the frontend, you must delete the
+instance group, not the individual instances. Otherwise the instance group will
+just recreate the deleted instances. You can do this from the Google Cloud
+console web interface, or using the `gcloud compute groups` commands.
+
 ## Connect to the instance with SSH
 
 ```shell
-gcloud compute ssh clovis-tracer-1
+gcloud compute ssh $INSTANCE_NAME
 ```
 
-## Use the app locally
+## Run the app locally
 
 From a new directory, set up a local environment:
 
@@ -95,20 +152,8 @@ gcloud beta auth application-default login --scopes \
     https://www.googleapis.com/auth/cloud-platform
 ```
 
-Create a JSON dictionary file describing the deployment configuration, with the
-keys:
-
--   `project_name` (string): Name of the Google Cloud project
--   `cloud_storage_path` (string): Path in Google Storage where generated traces
-    will be stored.
--   `chrome_path` (string): Path to the Chrome executable.
--   `src_path` (string): Path to the Chromium source directory.
--   `taskqueue_tag` (string):
--   `trace_database_filename` (string, optional): Filename for the trace
-    database in Cloud Storage. Must be unique per worker to avoid concurrent
-    access. Defaults to `trace_database.json`.
+Create a local configuration file for `worker.py`. Example:
 
-Example:
 ```shell
 cat >$CONFIG_FILE << EOF
 {
@@ -116,7 +161,7 @@ cat >$CONFIG_FILE << EOF
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "$CHROME_PATH",
   "src_path" : "$CHROMIUM_SRC",
-  "taskqueue_tag" : "some_tag"
+  "taskqueue_tag" : "some-tag"
 }
 EOF
 ```
@@ -139,3 +184,4 @@ deactivate
 [1]: https://cloud.google.com/sdk
 [2]: #Use-the-app
 [3]: ../frontend/README.md
+[4]: https://cloud.google.com/appengine/docs/python/taskqueue
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index 3d3084f..69cb7d9 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -17,7 +17,7 @@ PROJECTID=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
     -H "Metadata-Flavor: Google")
 
-INSTANCE_ID=$(curl -s \
+INSTANCE_NAME=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/instance/hostname" \
     -H "Metadata-Flavor: Google")
 
@@ -70,15 +70,22 @@ chown -R pythonapp:pythonapp /opt/app
 
 # Create the configuration file for this deployment.
 DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
-TASKQUEUE_TAG=`get_instance_metadata taskqueue_tag`
+TASKQUEUE_TAG=`get_instance_metadata taskqueue-tag`
+if [ "$(get_instance_metadata self-destruct)" == "false" ]; then
+  SELF_DESTRUCT_CONFIG_LINE=""
+else
+  SELF_DESTRUCT_CONFIG_LINE="\"destruct_instance_name\" : \"$INSTANCE_NAME\","
+fi
+
 cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
+  $SELF_DESTRUCT_CONFIG_LINE
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
-  "trace_database_filename" : "trace_database_${INSTANCE_ID}.json"
+  "trace_database_filename" : "trace_database_${INSTANCE_NAME}.json"
 }
 EOF
 
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index c724d2e..ff0d473 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -21,6 +21,7 @@ sys.path.insert(0,
                  os.pardir))
 import controller
 from cloud.common.clovis_task import ClovisTask
+from cloud.common.google_instance_helper import GoogleInstanceHelper
 from google_storage_accessor import GoogleStorageAccessor
 import loading_trace
 from loading_trace_database import LoadingTraceDatabase
@@ -33,6 +34,7 @@ class Worker(object):
     self._project_name = config['project_name']
     self._taskqueue_tag = config['taskqueue_tag']
     self._src_path = config['src_path']
+    self._destruct_instance_name = config.get('destruct_instance_name')
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
 
@@ -80,15 +82,7 @@ class Worker(object):
       (clovis_task, task_id) = self._FetchClovisTask(project, task_api,
                                                      queue_name)
       if not clovis_task:
-        if self._trace_database.ToJsonDict():
-          self._logger.info('No remaining tasks in the queue.')
-          break
-        else:
-          delay_seconds = 60
-          self._logger.info(
-              'Nothing in the queue, retrying in %i seconds.' % delay_seconds)
-          time.sleep(delay_seconds)
-          continue
+        break
 
       self._logger.info('Processing task %s' % task_id)
       self._ProcessClovisTask(clovis_task)
@@ -126,7 +120,7 @@ class Worker(object):
                          if no tasks are found.
     """
     response = task_api.tasks().lease(
-        project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=180,
+        project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=600,
         groupByTag=True, tag=self._taskqueue_tag).execute()
     if (not response.get('items')) or (len(response['items']) < 1):
       return (None, None)
@@ -138,8 +132,19 @@ class Worker(object):
 
   def _Finalize(self):
     """Called before exiting."""
-    # TODO(droger): Implement automatic instance destruction.
     self._logger.info('Done')
+    # Self destruct.
+    if self._destruct_instance_name:
+      self._logger.info('Starting instance destruction: ' +
+                        self._destruct_instance_name)
+      google_instance_helper = GoogleInstanceHelper(
+          self._credentials, self._project_name, self._logger)
+      success = google_instance_helper.DeleteInstance(
+          self._taskqueue_tag, self._destruct_instance_name)
+      if not success:
+        self._logger.error('Self destruction failed')
+    # Do not add anything after this line, as the instance might be killed at
+    # any time.
 
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index bd678b4..c00f43b 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -111,7 +111,7 @@ class GoogleInstanceHelper(object):
                  'value': bucket},
                 {'key': 'startup-script-url',
                  'value': 'gs://%s/deployment/startup-script.sh' % bucket},
-                {'key': 'taskqueue_tag', 'value': tag}]}}}
+                {'key': 'taskqueue-tag', 'value': tag}]}}}
     request = self._compute_api.instanceTemplates().insert(
         project=self._project, body=request_body)
     return self._ExecuteApiRequest(request)

commit 65b17a568116563bc3e74e717c42f00d9c8c2d1d
Author: lizeb <lizeb@chromium.org>
Date:   Tue Apr 26 01:05:56 2016 -0700

    clovis: Computes the amount of data downloaded due to ads.
    
    Review URL: https://codereview.chromium.org/1915093002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389723}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9e49615dccedf0d7e49515c7a754b81a94b2aaad

diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
index be05fcb..89f40e2 100644
--- a/loading/content_classification_lens.py
+++ b/loading/content_classification_lens.py
@@ -6,6 +6,7 @@
 
 import collections
 import logging
+import operator
 import os
 import urlparse
 
@@ -26,6 +27,7 @@ class ContentClassificationLens(object):
     """
     self._trace = trace
     self._requests = trace.request_track.GetEvents()
+    self._requests_by_id = {r.request_id: r for r in self._requests}
     self._main_frame_id = trace.page_track.GetEvents()[0]['frame_id']
     self._frame_to_requests = collections.defaultdict(list)
     self._ad_requests = set()
@@ -44,15 +46,32 @@ class ContentClassificationLens(object):
     """Returns True iff the request matches one of the tracking_rules."""
     return request.request_id in self._tracking_requests
 
-  def IsAdFrame(self, frame_id, ratio):
-    """A Frame is an Ad frame if more than |ratio| of its requests are
-    ad-related, and is not the main frame."""
-    if frame_id == self._main_frame_id:
+  def IsAdOrTrackingFrame(self, frame_id):
+    """A Frame is an Ad frame if it's not the main frame and its main resource
+    is ad or tracking-related.
+    """
+    if (frame_id not in self._frame_to_requests
+        or frame_id == self._main_frame_id):
       return False
-    ad_requests_count = sum(r in self._ad_requests
-                            for r in self._frame_to_requests[frame_id])
-    frame_requests_count = len(self._frame_to_requests[frame_id])
-    return (float(ad_requests_count) / frame_requests_count) > ratio
+    frame_requests = [self._requests_by_id[request_id]
+                      for request_id in self._frame_to_requests[frame_id]]
+    sorted_frame_resources = sorted(
+        frame_requests, key=operator.attrgetter('start_msec'))
+    frame_main_resource = sorted_frame_resources[0]
+    return (frame_main_resource.request_id in self._ad_requests
+            or frame_main_resource.request_id in self._tracking_requests)
+
+  def AdAndTrackingRequests(self):
+    """Returns a list of requests linked to ads and tracking.
+
+    Returns the union of:
+    - Requests tagged as ad or tracking.
+    - Requests originating from an ad frame.
+    """
+    frame_ids = {r.frame_id for r in self._requests}
+    ad_frame_ids = filter(self.IsAdOrTrackingFrame, frame_ids)
+    return filter(lambda r: self.IsAdRequest(r) or self.IsTrackingRequest(r)
+                  or r.frame_id in ad_frame_ids, self._requests)
 
   @classmethod
   def WithRulesFiles(cls, trace, ad_rules_filename, tracking_rules_filename):
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index 7d1631b..18025b0 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -14,14 +14,15 @@ import test_utils
 class ContentClassificationLensTestCase(unittest.TestCase):
   _DOCUMENT_URL = 'http://bla.com'
   _MAIN_FRAME_ID = '123.1'
-  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+  _REQUEST = Request.FromJsonDict({'url': _DOCUMENT_URL,
                                    'document_url': _DOCUMENT_URL,
                                    'request_id': '1234.1',
                                    'frame_id': _MAIN_FRAME_ID,
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
                                    'status': 200,
-                                   'timing': {}})
+                                   'timing': {},
+                                   'resource_type': 'Document'})
   _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
                    'frame_id': _MAIN_FRAME_ID},
                   {'method': 'Page.frameAttached',
@@ -74,19 +75,42 @@ class ContentClassificationLensTestCase(unittest.TestCase):
     self.assertFalse(lens.IsAdRequest(self._REQUEST))
     self.assertTrue(lens.IsTrackingRequest(self._REQUEST))
 
-  def testMainFrameIsNotAdFrame(self):
+  def testMainFrameIsNotAnAdFrame(self):
     trace = test_utils.LoadingTraceFromEvents(
-        [self._REQUEST] * 10, self._PAGE_EVENTS)
+        [self._REQUEST], self._PAGE_EVENTS)
     lens = ContentClassificationLens(trace, self._RULES, [])
-    self.assertFalse(lens.IsAdFrame(self._MAIN_FRAME_ID, .5))
+    self.assertFalse(lens.IsAdOrTrackingFrame(self._MAIN_FRAME_ID))
 
   def testAdFrame(self):
     request = copy.deepcopy(self._REQUEST)
+    request.request_id = '1234.2'
     request.frame_id = '123.123'
     trace = test_utils.LoadingTraceFromEvents(
-        [request] * 10 + [self._REQUEST] * 5, self._PAGE_EVENTS)
+        [self._REQUEST, request], self._PAGE_EVENTS)
     lens = ContentClassificationLens(trace, self._RULES, [])
-    self.assertTrue(lens.IsAdFrame(request.frame_id, .5))
+    self.assertTrue(lens.IsAdOrTrackingFrame(request.frame_id))
+
+  def testAdAndTrackingRequests(self):
+    ad_request = copy.deepcopy(self._REQUEST)
+    ad_request.request_id = '1234.2'
+    ad_request.frame_id = '123.123'
+    non_ad_request_non_ad_frame = copy.deepcopy(self._REQUEST)
+    non_ad_request_non_ad_frame.request_id = '1234.3'
+    non_ad_request_non_ad_frame.url = 'http://www.example.com'
+    non_ad_request_non_ad_frame.frame_id = '123.456'
+    non_ad_request_ad_frame = copy.deepcopy(self._REQUEST)
+    non_ad_request_ad_frame.request_id = '1234.4'
+    non_ad_request_ad_frame.url = 'http://www.example.com'
+    non_ad_request_ad_frame.frame_id = ad_request.frame_id
+
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST, ad_request, non_ad_request_non_ad_frame,
+         non_ad_request_ad_frame], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertSetEqual(
+        set([self._REQUEST, ad_request, non_ad_request_ad_frame]),
+        set(lens.AdAndTrackingRequests()))
+
 
 class _MatcherTestCase(unittest.TestCase):
   _RULES_WITH_WHITELIST = ['/thisisanad.', '@@myadvertisingdomain.com/*',
diff --git a/loading/metrics.py b/loading/metrics.py
index b56c54c..5f5e560 100644
--- a/loading/metrics.py
+++ b/loading/metrics.py
@@ -4,11 +4,12 @@
 
 """Descriptive metrics for Clovis.
 
-When executed as a script, shows a graph of the amount of data to download for
-a new visit to the same page, with a given time interval.
+When executed as a script, prints the amount of data attributed to Ads, and
+shows a graph of the amount of data to download for a new visit to the same
+page, with a given time interval.
 """
 
-
+import content_classification_lens
 from request_track import CachingPolicy
 
 
@@ -24,16 +25,22 @@ def _RequestTransferSize(request):
           'body': request.encoded_data_length}
 
 
-def TotalTransferSize(trace):
-  """Returns the total transfer size (uploaded, downloaded) from a trace.
+def _TransferSize(requests):
+  """Returns the total transfer size (uploaded, downloaded) of requests.
 
   This is an estimate as we assume:
   - 200s (for the size computation)
   - GET only.
+
+  Args:
+    requests: ([Request]) List of requests.
+
+  Returns:
+    (uploaded_bytes (int), downloaded_bytes (int))
   """
   uploaded_bytes = 0
   downloaded_bytes = 0
-  for request in trace.request_track.GetEvents():
+  for request in requests:
     request_bytes = _RequestTransferSize(request)
     uploaded_bytes += request_bytes['get'] + request_bytes['request_headers']
     downloaded_bytes += (len('HTTP/1.1 200 OK')
@@ -42,6 +49,11 @@ def TotalTransferSize(trace):
   return (uploaded_bytes, downloaded_bytes)
 
 
+def TotalTransferSize(trace):
+  """Returns the total transfer size (uploaded, downloaded) from a trace."""
+  return _TransferSize(trace.request_track.GetEvents())
+
+
 def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
   """Returns the amount of data transferred for a revisit.
 
@@ -74,6 +86,25 @@ def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
   return (uploaded_bytes, downloaded_bytes)
 
 
+def AdsAndTrackingTransferSize(trace, ad_rules_filename,
+                               tracking_rules_filename):
+  """Returns the transfer size attributed to ads and tracking.
+
+  Args:
+    trace: (LoadingTrace) a loading trace.
+    ad_rules_filename: (str) Path to an ad rules file.
+    tracking_rules_filename: (str) Path to a tracking rules file.
+
+  Returns:
+    (uploaded_bytes (int), downloaded_bytes (int))
+  """
+  content_lens = (
+      content_classification_lens.ContentClassificationLens.WithRulesFiles(
+          trace, ad_rules_filename, tracking_rules_filename))
+  requests = content_lens.AdAndTrackingRequests()
+  return _TransferSize(requests)
+
+
 def PlotTransferSizeVsTimeBetweenVisits(trace):
   times = [10, 60, 300, 600, 3600, 4 * 3600, 12 * 3600, 24 * 3600]
   labels = ['10s', '1m', '10m', '1h', '4h', '12h', '1d']
@@ -90,8 +121,14 @@ def PlotTransferSizeVsTimeBetweenVisits(trace):
   plt.show()
 
 
-def main(trace_filename):
+def main(trace_filename, ad_rules_filename, tracking_rules_filename):
   trace = loading_trace.LoadingTrace.FromJsonFile(trace_filename)
+  (_, ads_downloaded_bytes) = AdsAndTrackingTransferSize(
+      trace, ad_rules_filename, tracking_rules_filename)
+  (_, total_downloaded_bytes) = TotalTransferSize(trace)
+  print '%e bytes linked to Ads/Tracking (%.02f%%)' % (
+      ads_downloaded_bytes,
+      (100. * ads_downloaded_bytes) / total_downloaded_bytes)
   PlotTransferSizeVsTimeBetweenVisits(trace)
 
 
@@ -99,5 +136,9 @@ if __name__ == '__main__':
   import sys
   from matplotlib import pylab as plt
   import loading_trace
-
-  main(sys.argv[1])
+  if len(sys.argv) != 4:
+    print (
+        'Usage: %s trace_filename ad_rules_filename tracking_rules_filename'
+        % sys.argv[0])
+    sys.exit(0)
+  main(*sys.argv[1:])

commit 285ed29317561f77148e484a27a83543a216e9f7
Author: droger <droger@chromium.org>
Date:   Mon Apr 25 23:24:09 2016 -0700

    tools/android/loading Automatic creation of the backend by the frontend
    
    This CL adds a new 'backend_params' parameter in a task,
    which allows to control the settings for the backend (in
    particular the number of compute engine instances).
    
    The frontend uses these parameters to automatically create
    an instance template, an instance group, and starts the
    instances.
    
    A new HTML page template is added to display the results,
    including the log.
    
    Review URL: https://codereview.chromium.org/1907233003
    
    Cr-Original-Commit-Position: refs/heads/master@{#389707}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ff5ba7a68e23dfed0754faf2f29a542753bfd5ec

diff --git a/loading/cloud/backend/deploy.sh b/loading/cloud/backend/deploy.sh
index cbab438..00f19d7 100755
--- a/loading/cloud/backend/deploy.sh
+++ b/loading/cloud/backend/deploy.sh
@@ -55,6 +55,10 @@ chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
   $tmpdir/linux.zip
 gsutil cp $tmpdir/linux.zip gs://$deployment_gcs_path/binaries/linux.zip
 
+# Copy the startup script uncompressed so that it can be executed.
+gsutil cp tools/android/loading/cloud/backend/startup-script.sh \
+  gs://$deployment_gcs_path/
+
 # Generate and upload metadata about this deployment.
 CHROMIUM_REV=$(git merge-base HEAD origin/master)
 cat >$tmpdir/build_metadata.json << EOF
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 9518171..c724d2e 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -209,7 +209,7 @@ class Worker(object):
       return
 
     # Extract the task parameters.
-    params = clovis_task.Params()
+    params = clovis_task.ActionParams()
     urls = params['urls']
     repeat_count = params.get('repeat_count', 1)
     emulate_device = params.get('emulate_device')
diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
index 8bc3127..20a5182 100644
--- a/loading/cloud/common/clovis_task.py
+++ b/loading/cloud/common/clovis_task.py
@@ -4,24 +4,33 @@
 
 import base64
 import json
+import uuid
 
 class ClovisTask(object):
   """Generic task, generated by the AppEngine frontend and consumed by the
   ComputeEngine backend.
   """
 
-  def __init__(self, action, params, taskqueue_tag):
-    """Params:
+  def __init__(self, action, action_params, backend_params):
+    """ See tools/android/loading/cloud/frontend/README.md for a specification
+    of the parameters.
+
+    Args:
       action(str): Action accomplished by this task.
-      params(dict): Parameters of task.
-      taskqueue_tag(str): Tag of the task. Optional.
+      action_params(dict): Parameters of task.
+      backend_params(dict): Parameters of the instances running the task.
+          If this is None, no instances are created. If this dictionary has no
+          'tag' key, a unique tag will be generated.
     """
     self._action = action
-    self._params = params
-    self._taskqueue_tag = taskqueue_tag
+    self._action_params = action_params
+    self._backend_params = backend_params or {}
+    # If no tag is specified, generate a unique tag.
+    if not self._backend_params.get('tag'):
+      self._backend_params.update({'tag': str(uuid.uuid1())})
 
   @classmethod
-  def FromJsonDict(cls, json_dict):
+  def FromJsonString(cls, json_dict):
     """Loads a ClovisTask from a JSON string.
 
     Returns:
@@ -30,37 +39,36 @@ class ClovisTask(object):
     try:
       data = json.loads(json_dict)
       action = data['action']
-      params = data['params']
-      tag = data.get('taskqueue_tag')
+      action_params = data['action_params']
       # Vaidate the format.
       if action == 'trace':
-        urls = params['urls']
+        urls = action_params['urls']
         if (type(urls) is not list) or (len(urls) == 0):
           return None
       else:
         # When more actions are supported, check that they are valid here.
         return None
-      return cls(action, params, tag)
+      return cls(action, action_params, data.get('backend_params'))
     except Exception:
       return None
 
   @classmethod
   def FromBase64(cls, base64_string):
     """Loads a ClovisTask from a base 64 string."""
-    return ClovisTask.FromJsonDict(base64.b64decode(base64_string))
+    return ClovisTask.FromJsonString(base64.b64decode(base64_string))
 
-  def ToJsonDict(self):
+  def ToJsonString(self):
     """Returns the JSON representation of the task."""
-    task_dict = { 'action': self._action, 'params': self._params }
-    if self._taskqueue_tag:
-      task_dict['taskqueue_tag'] = self._taskqueue_tag
+    task_dict = {'action': self._action, 'action_params': self._action_params,
+                 'backend_params': self._backend_params}
     return json.dumps(task_dict)
 
   def Action(self):
     return self._action
 
-  def Params(self):
-    return self._params
+  def ActionParams(self):
+    return self._action_params
+
+  def BackendParams(self):
+    return self._backend_params
 
-  def TaskqueueTag(self):
-    return self._taskqueue_tag
diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 21cf120..bd678b4 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import json
+from googleapiclient import (discovery, errors)
 import time
 
 from googleapiclient import (discovery, errors)
@@ -11,21 +12,22 @@ from googleapiclient import (discovery, errors)
 class GoogleInstanceHelper(object):
   """Helper class for the Google Compute API, allowing to manage groups of
   instances more easily. Groups of instances are identified by a tag."""
-  _COMPUTE_API_ROOT = 'https://www.googleapis.com/compute/v1/projects/'
 
   def __init__(self, credentials, project, logger):
     self._compute_api = discovery.build('compute','v1', credentials=credentials)
     self._project = project
-    self._project_api_url = self._COMPUTE_API_ROOT + project
+    self._api_url = 'https://www.googleapis.com/compute/v1/projects/' + project
     self._zone = 'europe-west1-c'
     self._logger = logger
 
   def _ExecuteApiRequest(self, request, retry_count=3):
     """ Executes a Compute API request and returns True on success."""
-    self._logger.info('Compute API request:\n' + request.to_json())
+    self._logger.info('Compute API request:')
+    self._logger.info(request.to_json())
     try:
       response = request.execute()
-      self._logger.info('Compute API response:\n' + response)
+      self._logger.info('Compute API response:')
+      self._logger.info(response)
       return True
     except errors.HttpError as err:
       error_content = self._GetErrorContent(err)
@@ -54,7 +56,7 @@ class GoogleInstanceHelper(object):
 
   def _GetErrorContent(self, error):
     """Returns the contents of an error returned by the Compute API as a
-    dictionary or None.
+    dictionary.
     """
     if not error.resp.get('content-type', '').startswith('application/json'):
       return None
@@ -68,22 +70,22 @@ class GoogleInstanceHelper(object):
         not error_content['error'].get('errors')):
       return None
     error_list = error_content['error']['errors']
-    if not error_list:
+    if len(error_list) == 0:
       return None
-    return error_list[0].get('reason')
+    return error_list[0].get('reason', '')
 
   def CreateTemplate(self, tag, bucket):
     """Creates an instance template for instances identified by tag and using
     bucket for deployment. Returns True if successful.
     """
-    image_url = self._COMPUTE_API_ROOT + \
+    image_url = 'https://www.googleapis.com/compute/v1/projects/' \
                 'ubuntu-os-cloud/global/images/ubuntu-1404-trusty-v20160406'
     request_body = {
         'name': self._GetTemplateName(tag),
         'properties': {
             'machineType': 'n1-standard-1',
             'networkInterfaces': [{
-                'network': self._project_api_url + '/global/networks/default',
+                'network': self._api_url + '/global/networks/default',
                 'accessConfigs': [{
                     'name': 'external-IP',
                     'type': 'ONE_TO_ONE_NAT'
@@ -128,7 +130,7 @@ class GoogleInstanceHelper(object):
     exist for this to succeed. Returns True if successful.
     """
     template_url = '%s/global/instanceTemplates/%s' % (
-        self._project_api_url, self._GetTemplateName(tag))
+        self._api_url, self._GetTemplateName(tag))
     request_body = {
         'zone': self._zone, 'targetSize': instance_count,
         'baseInstanceName': 'instance-' + tag,
@@ -146,7 +148,7 @@ class GoogleInstanceHelper(object):
     # The instance hostname may be of the form <name>.c.<project>.internal but
     # only the <name> part should be passed to the compute API.
     name = instance_hostname.split('.')[0]
-    instance_url = self._project_api_url + (
+    instance_url = self._api_url + (
         "/zones/%s/instances/%s" % (self._zone, name))
     request = self._compute_api.instanceGroupManagers().deleteInstances(
         project=self._project, zone=self._zone,
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index f6f6f5f..7720203 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -8,13 +8,24 @@ Visit the application URL in your browser, and upload a JSON dictionary with the
 following keys:
 
 -   `action` (string): the action to perform. Only `trace` is supported.
--   `params` (dictionary): the parameters associated to the action. See below
-    for more details.
--   `taskqueue_tag` (string, optional): the [TaskQueue][2] tag internally used
-    to send the work from AppEngine to ComputeEngine.  If this parameter is not
-    specified, a unique tag will be created.
+-   `action_params` (dictionary): the parameters associated to the action.
+    See below for more details.
+-   `backend_params` (dictionary): the parameters configuring the backend for
+    this task. See below for more details.
 
-### Parameters for the `trace` action.
+### Parameters for `backend_params`
+
+-   `instance_count` (int): Number of Compute Engine instances that will be
+    started for this task.
+-   `storage_bucket` (string): Name of the storage bucket used by the backend
+    instances. Backend code and data must have been previously deployed to this
+    bucket using the `deploy.sh` [script][4].
+-   `tag` (string, optional): tag internally used to associate tasks to backend
+    ComputeEngine instances. This parameter should not be set in general, as it
+    is mostly exposed for development purposes. If this parameter is not
+    specified, a unique tag will be generated.
+
+### Parameters for the `trace` action
 
 -   `urls` (list of strings): the list of URLs to process.
 -   `repeat_count` (integer, optional): the number of traces to be generated
@@ -67,19 +78,20 @@ following keys:
 # Chromium checkout, see the cleanup intructions below.
 pip install -r requirements.txt -t lib
 # Start the local server.
-dev_appserver.py .
+dev_appserver.py -A $PROJECT_NAME .
 ```
 
 Visit the application [http://localhost:8080](http://localhost:8080).
 
 After you are done, cleanup your Chromium checkout:
+
 ```shell
 rm -rf $CHROMIUM_SRC/tools/android/loading/frontend/lib
 ```
 
 ### Deploy
 
-````shell
+```shell
 # Install dependencies in the lib/ directory.
 pip install -r requirements.txt -t lib
 # Deploy.
@@ -89,3 +101,4 @@ gcloud preview app deploy app.yaml
 [1]: https://cloud.google.com/sdk
 [2]: https://cloud.google.com/appengine/docs/python/taskqueue
 [3]: https://cloud.google.com/appengine/docs/python/config/queue
+[4]: ../backend/README.md#Deploy-the-code
diff --git a/loading/cloud/frontend/app.yaml b/loading/cloud/frontend/app.yaml
index 7dcc1d6..6544088 100644
--- a/loading/cloud/frontend/app.yaml
+++ b/loading/cloud/frontend/app.yaml
@@ -8,3 +8,7 @@ handlers:
 
 - url: .*
   script: clovis_frontend.app
+
+libraries:
+- name: ssl
+  version: latest
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 8eeee35..b037a3d 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -3,52 +3,112 @@
 # found in the LICENSE file.
 
 import flask
-from google.appengine.api import taskqueue
-import json
+from google.appengine.api import (app_identity, taskqueue)
+from oauth2client.client import GoogleCredentials
+import logging
 import os
 import sys
-import uuid
 
 from common.clovis_task import ClovisTask
+import common.google_instance_helper
+from memory_logs import MemoryLogs
+
+
+# Global variables.
+clovis_logger = logging.getLogger('clovis_frontend')
+clovis_logger.setLevel(logging.DEBUG)
+project_name = app_identity.get_application_id()
+instance_helper = common.google_instance_helper.GoogleInstanceHelper(
+    credentials=GoogleCredentials.get_application_default(),
+    project=project_name,
+    logger=clovis_logger)
+app = flask.Flask(__name__)
 
 
-app = flask.Flask(__name__)
+def Render(message, memory_logs):
+  return flask.render_template(
+      'log.html', body=message, log=memory_logs.Flush().split('\n'))
 
 
-def StartFromJson(http_body_str):
-  """Creates a new batch of tasks from its JSON representation."""
-  task = ClovisTask.FromJsonDict(http_body_str)
+def CreateInstanceTemplate(task):
+  """Create the Compute Engine instance template that will be used to create the
+  instances.
+  """
+  backend_params = task.BackendParams()
+  instance_count = backend_params.get('instance_count', 0)
+  if instance_count <= 0:
+    clovis_logger.info('No template required.')
+    return True
+  bucket = backend_params.get('storage_bucket')
+  if not bucket:
+    clovis_logger.error('Missing bucket in backend_params.')
+    return False
+  return instance_helper.CreateTemplate(task.BackendParams()['tag'], bucket)
+
+
+def CreateInstances(task):
+  """Creates the Compute engine requested by the task"""
+  backend_params = task.BackendParams()
+  instance_count = backend_params.get('instance_count', 0)
+  if instance_count <= 0:
+    clovis_logger.info('No instances to create.')
+    return True
+  return instance_helper.CreateInstances(backend_params['tag'], instance_count)
+
+
+def StartFromJsonString(http_body_str):
+  """Main function handling a JSON task posted by the user"""
+  # Set up logging.
+  memory_logs = MemoryLogs(clovis_logger)
+  memory_logs.Start()
+
+  # Load the task from JSON.
+  task = ClovisTask.FromJsonString(http_body_str)
   if not task:
-    return 'Invalid JSON task:\n%s\n' % http_body_str
+    clovis_logger.error('Invalid JSON task.')
+    return Render('Invalid JSON task:\n' + http_body_str, memory_logs)
 
-  task_tag = task.TaskqueueTag()
-  if not task_tag:
-    task_tag = uuid.uuid1()
+  task_tag = task.BackendParams()['tag']
 
+  # Create the instance template if required.
+  if not CreateInstanceTemplate(task):
+    return Render('Template creation failed.', memory_logs)
+
+  # Split the task in smaller tasks.
   sub_tasks = []
   if task.Action() == 'trace':
     sub_tasks = SplitTraceTask(task)
   else:
-    return 'Unsupported action: %s\n' % task.Action()
+    error_string = 'Unsupported action: %s.' % task.Action()
+    clovis_logger.error(error_string)
+    return Render(error_string, memory_logs)
+
+  if not EnqueueTasks(sub_tasks, task_tag):
+    return Render('Task creation failed', memory_logs)
+
+  # Start the instances if required.
+  if not CreateInstances(task):
+    return Render('Instance creation failed', memory_logs)
 
-  return EnqueueTasks(sub_tasks, task_tag)
+  return Render('Success', memory_logs)
 
 
 def SplitTraceTask(task):
-  """Split a tracing task with potentially many URLs into several tracing tasks
+  """Splits a tracing task with potentially many URLs into several tracing tasks
   with few URLs.
   """
-  params = task.Params()
-  urls = params['urls']
+  clovis_logger.debug('Splitting trace task.')
+  action_params = task.ActionParams()
+  urls = action_params['urls']
 
   # Split the task in smaller tasks with fewer URLs each.
   urls_per_task = 1
   sub_tasks = []
   for i in range(0, len(urls), urls_per_task):
-    sub_task_params = params.copy()
+    sub_task_params = action_params.copy()
     sub_task_params['urls'] = [url for url in urls[i:i+urls_per_task]]
     sub_tasks.append(ClovisTask(task.Action(), sub_task_params,
-                                task.TaskqueueTag()))
+                                task.BackendParams()))
   return sub_tasks
 
 
@@ -59,17 +119,16 @@ def EnqueueTasks(tasks, task_tag):
   q = taskqueue.Queue('clovis-queue')
   retry_options = taskqueue.TaskRetryOptions(task_retry_limit=3)
   # Add tasks to the queue by groups.
-  # TODO(droger): This support to thousands of tasks, but maybe not millions.
+  # TODO(droger): This supports thousands of tasks, but maybe not millions.
   # Defer the enqueuing if it times out.
-  # is too large.
   group_size = 100
   callbacks = []
   try:
     for i in range(0, len(tasks), group_size):
       group = tasks[i:i+group_size]
       taskqueue_tasks = [
-          taskqueue.Task(payload=task.ToJsonDict(), method='PULL', tag=task_tag,
-                         retry_options=retry_options)
+          taskqueue.Task(payload=task.ToJsonString(), method='PULL',
+                         tag=task_tag, retry_options=retry_options)
           for task in group]
       rpc = taskqueue.create_rpc()
       q.add_async(task=taskqueue_tasks, rpc=rpc)
@@ -77,8 +136,10 @@ def EnqueueTasks(tasks, task_tag):
     for callback in callbacks:
       callback.get_result()
   except Exception as e:
-    return 'Exception:' + type(e).__name__ + ' ' + str(e.args) + '\n'
-  return 'pushed %i tasks with tag: %s\n' % (len(tasks), task_tag)
+    clovis_logger.error('Exception:' + type(e).__name__ + ' ' + str(e.args))
+    return False
+  clovis_logger.info('Pushed %i tasks with tag: %s' % (len(tasks), task_tag))
+  return True
 
 
 @app.route('/')
@@ -94,7 +155,7 @@ def StartFromForm():
   if not data_stream:
     return 'failed'
   http_body_str = data_stream.read()
-  return StartFromJson(http_body_str)
+  return StartFromJsonString(http_body_str)
 
 
 @app.errorhandler(404)
diff --git a/loading/cloud/frontend/requirements.txt b/loading/cloud/frontend/requirements.txt
index 880a7bc..483195b 100644
--- a/loading/cloud/frontend/requirements.txt
+++ b/loading/cloud/frontend/requirements.txt
@@ -1 +1,2 @@
 Flask==0.10
+google-api-python-client
diff --git a/loading/cloud/frontend/templates/log.html b/loading/cloud/frontend/templates/log.html
new file mode 100644
index 0000000..4717896
--- /dev/null
+++ b/loading/cloud/frontend/templates/log.html
@@ -0,0 +1,39 @@
+{# Template for a page displaying a body and logs (optional) under a collapsible
+   section.
+#}
+<!DOCTYPE html>
+<html>
+<head>
+<meta charset="utf-8">
+<title>Clovis</title>
+</head>
+
+<body>
+{{ body }}
+
+{% if log %}
+
+<p><a onclick="javascript:ShowHide('HiddenDiv'); return false;" href="#">
+  Show/hide details
+</a></p>
+<div id="HiddenDiv" style="display:none";>
+
+{# Loop over the lines of the log to add linebreaks. #}
+{%- for line in log -%}
+  {{ line }}<br/>
+{%- endfor -%}
+
+</div>
+
+<script type="text/javascript">
+function ShowHide(divId) {
+  element = document.getElementById(divId)
+  element.style.display = (element.style.display == 'none') ? 'block' : 'none';
+}
+</script>
+
+{% endif %}
+
+</body>
+
+</html>

commit 01d8d51d6e00c9b73596c90d65d1dd2f39d03a4d
Author: droger <droger@chromium.org>
Date:   Mon Apr 25 16:24:53 2016 -0700

    tools/android/loading Helper to collect logs in a memory buffer
    
    Review URL: https://codereview.chromium.org/1921593002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389602}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3b3582e12c4c761637b8b8269895226f2c10512f

diff --git a/loading/cloud/frontend/memory_logs.py b/loading/cloud/frontend/memory_logs.py
new file mode 100644
index 0000000..afd1897
--- /dev/null
+++ b/loading/cloud/frontend/memory_logs.py
@@ -0,0 +1,34 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+from StringIO import StringIO
+
+
+class MemoryLogs(object):
+  """Collects logs in memory."""
+
+  def __init__(self, logger):
+    self._logger = logger
+    self._log_buffer = StringIO()
+    self._log_handler = logging.StreamHandler(self._log_buffer)
+    formatter = logging.Formatter("[%(asctime)s][%(levelname)s] %(message)s",
+                                  "%y-%m-%d %H:%M:%S")
+    self._log_handler.setFormatter(formatter)
+
+  def Start(self):
+    """Starts collecting the logs."""
+    self._logger.addHandler(self._log_handler)
+
+  def Flush(self):
+    """Stops collecting the logs and returns the logs collected since Start()
+    was called.
+    """
+    self._logger.removeHandler(self._log_handler)
+    self._log_handler.flush()
+    self._log_buffer.flush()
+    result = self._log_buffer.getvalue()
+    self._log_buffer.truncate(0)
+    return result
+

commit c79f039d0ec04d799bf044b42985ee9d6e30eb77
Author: droger <droger@chromium.org>
Date:   Mon Apr 25 15:03:38 2016 -0700

    tools/android/loading Helper to create Compute instances from Appengine
    
    Review URL: https://codereview.chromium.org/1920793003
    
    Cr-Original-Commit-Position: refs/heads/master@{#389577}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 93f1eeda75312f2ab5a027cd4a86353b4ab2ec5a

diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
new file mode 100644
index 0000000..21cf120
--- /dev/null
+++ b/loading/cloud/common/google_instance_helper.py
@@ -0,0 +1,155 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import time
+
+from googleapiclient import (discovery, errors)
+
+
+class GoogleInstanceHelper(object):
+  """Helper class for the Google Compute API, allowing to manage groups of
+  instances more easily. Groups of instances are identified by a tag."""
+  _COMPUTE_API_ROOT = 'https://www.googleapis.com/compute/v1/projects/'
+
+  def __init__(self, credentials, project, logger):
+    self._compute_api = discovery.build('compute','v1', credentials=credentials)
+    self._project = project
+    self._project_api_url = self._COMPUTE_API_ROOT + project
+    self._zone = 'europe-west1-c'
+    self._logger = logger
+
+  def _ExecuteApiRequest(self, request, retry_count=3):
+    """ Executes a Compute API request and returns True on success."""
+    self._logger.info('Compute API request:\n' + request.to_json())
+    try:
+      response = request.execute()
+      self._logger.info('Compute API response:\n' + response)
+      return True
+    except errors.HttpError as err:
+      error_content = self._GetErrorContent(err)
+      error_reason = self._GetErrorReason(error_content)
+      if error_reason == 'resourceNotReady' and retry_count > 0:
+        # Retry after a delay
+        delay_seconds = 1
+        self._logger.info(
+            'Resource not ready, retrying in %i seconds.' % delay_seconds)
+        time.sleep(delay_seconds)
+        return self._ExecuteApiRequest(request, retry_count - 1)
+      else:
+        self._logger.error('Compute API error (reason: "%s"):\n%s' % (
+            error_reason, err))
+        if error_content:
+          self._logger.error('Error details:\n%s' % error_content)
+        return False
+
+  def _GetTemplateName(self, tag):
+    """Returns the name of the instance template associated with tag."""
+    return 'template-' + tag
+
+  def _GetInstanceGroupName(self, tag):
+    """Returns the name of the instance group associated with tag."""
+    return 'group-' + tag
+
+  def _GetErrorContent(self, error):
+    """Returns the contents of an error returned by the Compute API as a
+    dictionary or None.
+    """
+    if not error.resp.get('content-type', '').startswith('application/json'):
+      return None
+    return json.loads(error.content)
+
+  def _GetErrorReason(self, error_content):
+    """Returns the error reason as a string."""
+    if not error_content:
+      return None
+    if (not error_content.get('error') or
+        not error_content['error'].get('errors')):
+      return None
+    error_list = error_content['error']['errors']
+    if not error_list:
+      return None
+    return error_list[0].get('reason')
+
+  def CreateTemplate(self, tag, bucket):
+    """Creates an instance template for instances identified by tag and using
+    bucket for deployment. Returns True if successful.
+    """
+    image_url = self._COMPUTE_API_ROOT + \
+                'ubuntu-os-cloud/global/images/ubuntu-1404-trusty-v20160406'
+    request_body = {
+        'name': self._GetTemplateName(tag),
+        'properties': {
+            'machineType': 'n1-standard-1',
+            'networkInterfaces': [{
+                'network': self._project_api_url + '/global/networks/default',
+                'accessConfigs': [{
+                    'name': 'external-IP',
+                    'type': 'ONE_TO_ONE_NAT'
+            }]}],
+            'disks': [{
+                'type': 'PERSISTENT',
+                'boot': True,
+                'autoDelete': True,
+                'mode': 'READ_WRITE',
+                'initializeParams': {'sourceImage': image_url}}],
+            'canIpForward': False,
+            'scheduling': {
+                'automaticRestart': True,
+                'onHostMaintenance': 'MIGRATE',
+                'preemptible': False},
+            'serviceAccounts': [{
+                'scopes': [
+                    'https://www.googleapis.com/auth/cloud-platform',
+                    'https://www.googleapis.com/auth/cloud-taskqueue'],
+                'email': 'default'}],
+            'metadata': { 'items': [
+                {'key': 'cloud-storage-path',
+                 'value': bucket},
+                {'key': 'startup-script-url',
+                 'value': 'gs://%s/deployment/startup-script.sh' % bucket},
+                {'key': 'taskqueue_tag', 'value': tag}]}}}
+    request = self._compute_api.instanceTemplates().insert(
+        project=self._project, body=request_body)
+    return self._ExecuteApiRequest(request)
+
+  def DeleteTemplate(self, tag):
+    """Deletes the instance template associated with tag. Returns True if
+    successful.
+    """
+    request = self._compute_api.instanceTemplates().delete(
+        project=self._project,
+        instanceTemplate=self._GetTemplateName(tag))
+    return self._ExecuteApiRequest(request)
+
+  def CreateInstances(self, tag, instance_count):
+    """Creates an instance group associated with tag. The instance template must
+    exist for this to succeed. Returns True if successful.
+    """
+    template_url = '%s/global/instanceTemplates/%s' % (
+        self._project_api_url, self._GetTemplateName(tag))
+    request_body = {
+        'zone': self._zone, 'targetSize': instance_count,
+        'baseInstanceName': 'instance-' + tag,
+        'instanceTemplate': template_url,
+        'name': self._GetInstanceGroupName(tag)}
+    request = self._compute_api.instanceGroupManagers().insert(
+        project=self._project, zone=self._zone,
+        body=request_body)
+    return self._ExecuteApiRequest(request)
+
+  def DeleteInstance(self, tag, instance_hostname):
+    """Deletes one instance from the instance group identified with tag. Returns
+    True if successful.
+    """
+    # The instance hostname may be of the form <name>.c.<project>.internal but
+    # only the <name> part should be passed to the compute API.
+    name = instance_hostname.split('.')[0]
+    instance_url = self._project_api_url + (
+        "/zones/%s/instances/%s" % (self._zone, name))
+    request = self._compute_api.instanceGroupManagers().deleteInstances(
+        project=self._project, zone=self._zone,
+        instanceGroupManager=self._GetInstanceGroupName(tag),
+        body={'instances': [instance_url]})
+    return self._ExecuteApiRequest(request)

commit c7668e8bfa06f5d01c9e6a7e0d4cb44c48339a04
Author: lizeb <lizeb@chromium.org>
Date:   Mon Apr 25 08:56:30 2016 -0700

    clovis: Computes the amount of data to re-download for repeated visits.
    
    This CL has two parts:
    - Replicate the caching policy mandated by the RFC (and Chrome's
      heuristics)
    - Use this to show the amount of data to re-download for repeated
      visits.
    
    Review URL: https://codereview.chromium.org/1916443002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389486}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 71705d9fc20cb8362d9962a25534c3cf59a579fb

diff --git a/loading/metrics.py b/loading/metrics.py
new file mode 100644
index 0000000..b56c54c
--- /dev/null
+++ b/loading/metrics.py
@@ -0,0 +1,103 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Descriptive metrics for Clovis.
+
+When executed as a script, shows a graph of the amount of data to download for
+a new visit to the same page, with a given time interval.
+"""
+
+
+from request_track import CachingPolicy
+
+
+def _RequestTransferSize(request):
+  def HeadersSize(headers):
+    # 4: ':', ' ', '\r', '\n'
+    return sum(len(k) + len(v) + 4 for (k, v) in headers.items())
+  if request.protocol == 'data':
+    return {'get': 0, 'request_headers': 0, 'response_headers': 0, 'body': 0}
+  return {'get': len('GET ') + len(request.url) + 2,
+          'request_headers': HeadersSize(request.request_headers or {}),
+          'response_headers': HeadersSize(request.response_headers or {}),
+          'body': request.encoded_data_length}
+
+
+def TotalTransferSize(trace):
+  """Returns the total transfer size (uploaded, downloaded) from a trace.
+
+  This is an estimate as we assume:
+  - 200s (for the size computation)
+  - GET only.
+  """
+  uploaded_bytes = 0
+  downloaded_bytes = 0
+  for request in trace.request_track.GetEvents():
+    request_bytes = _RequestTransferSize(request)
+    uploaded_bytes += request_bytes['get'] + request_bytes['request_headers']
+    downloaded_bytes += (len('HTTP/1.1 200 OK')
+                         + request_bytes['response_headers']
+                         + request_bytes['body'])
+  return (uploaded_bytes, downloaded_bytes)
+
+
+def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
+  """Returns the amount of data transferred for a revisit.
+
+  Args:
+    trace: (LoadingTrace) loading trace.
+    after_time_s: (float) Time in s after which the site is revisited.
+    assume_validation_ok: (bool) Assumes that the resources to validate return
+                          304s.
+
+  Returns:
+    (uploaded_bytes, downloaded_bytes)
+  """
+  uploaded_bytes = 0
+  downloaded_bytes = 0
+  for request in trace.request_track.GetEvents():
+    caching_policy = CachingPolicy(request)
+    policy = caching_policy.PolicyAtDate(request.wall_time + after_time_s)
+    request_bytes = _RequestTransferSize(request)
+    if policy == CachingPolicy.VALIDATION_NONE:
+      continue
+    uploaded_bytes += request_bytes['get'] + request_bytes['request_headers']
+    if (policy in (CachingPolicy.VALIDATION_SYNC,
+                   CachingPolicy.VALIDATION_ASYNC)
+        and caching_policy.HasValidators() and assume_validation_ok):
+      downloaded_bytes += len('HTTP/1.1 304 NOT MODIFIED\r\n')
+      continue
+    downloaded_bytes += (len('HTTP/1.1 200 OK\r\n')
+                         + request_bytes['response_headers']
+                         + request_bytes['body'])
+  return (uploaded_bytes, downloaded_bytes)
+
+
+def PlotTransferSizeVsTimeBetweenVisits(trace):
+  times = [10, 60, 300, 600, 3600, 4 * 3600, 12 * 3600, 24 * 3600]
+  labels = ['10s', '1m', '10m', '1h', '4h', '12h', '1d']
+  (_, total_downloaded) = TotalTransferSize(trace)
+  downloaded = [TransferredDataRevisit(trace, delta_t)[1] for delta_t in times]
+  plt.figure()
+  plt.title('Amount of data to download for a revisit - %s' % trace.url)
+  plt.xlabel('Time between visits (log)')
+  plt.ylabel('Amount of data (bytes)')
+  plt.plot(times, downloaded, 'k+--')
+  plt.axhline(total_downloaded, color='k', linewidth=2)
+  plt.xscale('log')
+  plt.xticks(times, labels)
+  plt.show()
+
+
+def main(trace_filename):
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_filename)
+  PlotTransferSizeVsTimeBetweenVisits(trace)
+
+
+if __name__ == '__main__':
+  import sys
+  from matplotlib import pylab as plt
+  import loading_trace
+
+  main(sys.argv[1])
diff --git a/loading/metrics_unittest.py b/loading/metrics_unittest.py
new file mode 100644
index 0000000..cc439ce
--- /dev/null
+++ b/loading/metrics_unittest.py
@@ -0,0 +1,75 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import unittest
+
+import metrics
+import request_track
+import test_utils
+
+
+class MetricsTestCase(unittest.TestCase):
+  _BODY_SIZE = 14187
+  _URL = 'http://www.example.com/'
+  _REQUEST_HEADERS_SIZE = (len(_URL) + len('GET ') + 2
+                           + len('Accept: Everything\r\n'))
+  _RESPONSE_HEADERS_SIZE = 124
+  _REQUEST = {
+      'encoded_data_length': _BODY_SIZE,
+      'request_id': '2291.1',
+      'request_headers': {
+          'Accept': 'Everything',
+      },
+      'response_headers': {
+          'Age': '866',
+          'Content-Length': str(_BODY_SIZE),
+          'Etag': 'ABCD',
+          'Date': 'Fri, 22 Apr 2016 08:56:19 -0200',
+          'Vary': 'Accept-Encoding',
+      },
+      'timestamp': 5535648.730768,
+      'timing': {
+          'receive_headers_end': 47.0650000497699,
+          'request_time': 5535648.73264,
+      },
+      'url': _URL,
+      'status': 200,
+      'wall_time': 1461322579.59422}
+
+  def testTransferredDataRevisitNoCache(self):
+    trace = self._MakeTrace()
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 10)
+    self.assertEqual(self._REQUEST_HEADERS_SIZE, uploaded)
+    self.assertEqual(self._BODY_SIZE + self._RESPONSE_HEADERS_SIZE, downloaded)
+
+  def testTransferredDataRevisitNoCacheAssumeValidates(self):
+    trace = self._MakeTrace()
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 10, True)
+    self.assertEqual(self._REQUEST_HEADERS_SIZE, uploaded)
+    not_modified_length = len('HTTP/1.1 304 NOT MODIFIED\r\n')
+    self.assertEqual(not_modified_length, downloaded)
+
+  def testTransferredDataRevisitCacheable(self):
+    trace = self._MakeTrace()
+    r = trace.request_track.GetEvents()[0]
+    r.response_headers['Cache-Control'] = 'max-age=1000'
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 10)
+    self.assertEqual(0, uploaded)
+    self.assertEqual(0, downloaded)
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 1000)
+    self.assertEqual(self._REQUEST_HEADERS_SIZE, uploaded)
+    cache_control_length = len('Cache-Control: max-age=1000\r\n')
+    self.assertEqual(
+        self._BODY_SIZE + self._RESPONSE_HEADERS_SIZE + cache_control_length,
+        downloaded)
+
+  @classmethod
+  def _MakeTrace(cls):
+    request = request_track.Request.FromJsonDict(copy.deepcopy(cls._REQUEST))
+    return test_utils.LoadingTraceFromEvents([request])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index 613f6e0..31eca51 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -10,6 +10,8 @@ When executed, parses a JSON dump of DevTools messages.
 import bisect
 import collections
 import copy
+import datetime
+import email.utils
 import json
 import logging
 import re
@@ -248,6 +250,21 @@ class Request(object):
         break
     return result
 
+  def GetResponseHeaderValue(self, header, value):
+    """Returns True iff the response headers |header| contains |value|."""
+    header_values = self.GetHTTPResponseHeader(header)
+    if not header_values:
+      return None
+    values = header_values.split(',')
+    for header_value in values:
+      if header_value.lower() == value.lower():
+        return header_value
+    return None
+
+  def HasResponseHeaderValue(self, header, value):
+    """Returns True iff the response headers |header| contains |value|."""
+    return self.GetResponseHeaderValue(header, value) is not None
+
   def GetContentType(self):
     """Returns the content type, or None."""
     # Check for redirects. Use the "Location" header, because the HTTP status is
@@ -272,6 +289,21 @@ class Request(object):
   def IsDataRequest(self):
     return self.protocol == 'data'
 
+  def GetCacheControlDirective(self, directive_name):
+    """Returns the value of a Cache-Control directive, or None."""
+    cache_control_str = self.GetHTTPResponseHeader('Cache-Control')
+    if cache_control_str is None:
+      return None
+    directives = [s.strip() for s in cache_control_str.split(',')]
+    for directive in directives:
+      parts = directive.split('=')
+      if len(parts) == 1:
+        continue
+      (name, value) = parts
+      if name == directive_name:
+        return value
+    return None
+
   def MaxAge(self):
     """Returns the max-age of a resource, or -1."""
     # TODO(lizeb): Handle the "Expires" header as well.
@@ -292,11 +324,9 @@ class Request(object):
         or u'no-cache' in cache_control
         or len(cache_control) == 0):
       return -1
-    if 'max-age' in cache_control:
-      age_match = re.match(r'\s*(\d+)+', cache_control['max-age'])
-      if not age_match:
-        return -1
-      return int(age_match.group(1))
+    max_age = self.GetCacheControlDirective('max-age')
+    if max_age:
+      return int(max_age)
     return -1
 
   def Cost(self):
@@ -316,6 +346,126 @@ class Request(object):
     return json.dumps(self.ToJsonDict(), sort_keys=True, indent=2)
 
 
+class CachingPolicy(object):
+  """Represents the caching policy at an arbitrary time for a cached response.
+  """
+  FETCH = 'FETCH'
+  VALIDATION_NONE = 'VALIDATION_NONE'
+  VALIDATION_SYNC = 'VALIDATION_SYNC'
+  VALIDATION_ASYNC = 'VALIDATION_ASYNC'
+  POLICIES = (FETCH, VALIDATION_NONE, VALIDATION_SYNC, VALIDATION_ASYNC)
+  def __init__(self, request):
+    """Constructor.
+
+    Args:
+      request: (Request)
+    """
+    assert request.response_headers is not None
+    self.request = request
+    # This is incorrect, as the timestamp corresponds to when devtools is made
+    # aware of the request, not when it was sent. However, this is good enough
+    # for computing cache expiration, which doesn't need sub-second precision.
+    self._request_time = self.request.wall_time
+    # Used when the date is not available.
+    self._response_time = (
+        self._request_time + self.request.timing.receive_headers_end)
+
+  def HasValidators(self):
+    """Returns wether the request has a validator."""
+    # Assuming HTTP 1.1+.
+    return (self.request.GetHTTPResponseHeader('Last-Modified')
+            or self.request.GetHTTPResponseHeader('Etag'))
+
+  def IsCacheable(self):
+    """Returns whether the request could be stored in the cache."""
+    return not self.request.HasResponseHeaderValue('Cache-Control', 'no-store')
+
+  def PolicyAtDate(self, timestamp):
+    """Returns the caching policy at an aribitrary timestamp.
+
+    Args:
+      timestamp: (float) Seconds since Epoch.
+
+    Returns:
+      A policy in POLICIES.
+    """
+    # Note: the implementation is largely transcribed from
+    # net/http/http_response_headers.cc, itself following RFC 2616.
+    if not self.IsCacheable():
+      return self.FETCH
+    freshness = self._GetFreshnessLifetimes()
+    if freshness[0] == 0 and freshness[1] == 0:
+      return self.VALIDATION_SYNC
+    age = self._GetCurrentAge(timestamp)
+    if freshness[0] > age:
+      return self.VALIDATION_NONE
+    if freshness[1] > age:
+      return self.VALIDATION_ASYNC
+    return self.VALIDATION_SYNC
+
+  def _GetFreshnessLifetimes(self):
+    """Returns [freshness, stale-while-revalidate freshness] in seconds."""
+    # This is adapted from GetFreshnessLifetimes() in
+    # //net/http/http_response_headers.cc (which follows the RFC).
+    r = self.request
+    result = [0, 0]
+    if (r.HasResponseHeaderValue('Cache-Control', 'no-cache')
+        or r.HasResponseHeaderValue('Cache-Control', 'no-store')
+        or r.HasResponseHeaderValue('Vary', '*')):  # RFC 2616, 13.6.
+      return result
+    must_revalidate = r.HasResponseHeaderValue(
+        'Cache-Control', 'must-revalidate')
+    swr_header = r.GetCacheControlDirective('stale-while-revalidate')
+    if not must_revalidate and swr_header:
+      result[1] = int(swr_header)
+
+    max_age_header = r.GetCacheControlDirective('max-age')
+    if max_age_header:
+      result[0] = int(max_age_header)
+      return result
+
+    date = self._GetDateValue('Date') or self._response_time
+    expires = self._GetDateValue('Expires')
+    if expires:
+      result[0] = expires - date
+      return result
+
+    if self.request.status in (200, 203, 206) and not must_revalidate:
+      last_modified = self._GetDateValue('Last-Modified')
+      if last_modified and last_modified < date:
+        result[0] = (date - last_modified) / 10
+        return result
+
+    if self.request.status in (300, 301, 308, 410):
+      return [2**48, 0] # ~forever.
+    # No header -> not fresh.
+    return result
+
+  def _GetDateValue(self, name):
+    date_str = self.request.GetHTTPResponseHeader(name)
+    if not date_str:
+      return None
+    parsed_date = email.utils.parsedate_tz(date_str)
+    if parsed_date is None:
+      return None
+    return email.utils.mktime_tz(parsed_date)
+
+  def _GetCurrentAge(self, current_time):
+    # See GetCurrentAge() in //net/http/http_response_headers.cc.
+    r = self.request
+    date_value = self._GetDateValue('Date') or self._response_time
+    age_value = int(r.GetHTTPResponseHeader('Age') or '0')
+
+    apparent_age = max(0, self._response_time - date_value)
+    corrected_received_age = max(apparent_age, age_value)
+    response_delay = self._response_time - self._request_time
+    corrected_initial_age = corrected_received_age + response_delay
+    resident_time = current_time - self._response_time
+    current_age = corrected_initial_age + resident_time
+
+    return current_age
+
+
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
   _REDIRECT_SUFFIX = '.redirect'
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index a613397..19f7b4a 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -6,7 +6,8 @@ import copy
 import json
 import unittest
 
-from request_track import (TimeBetween, Request, RequestTrack, Timing)
+from request_track import (TimeBetween, Request, CachingPolicy, RequestTrack,
+                           Timing)
 
 
 class TimeBetweenTestCase(unittest.TestCase):
@@ -74,6 +75,133 @@ class RequestTestCase(unittest.TestCase):
     self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
 
 
+class CachingPolicyTestCase(unittest.TestCase):
+  _REQUEST = {
+      'encoded_data_length': 14726,
+      'request_id': '2291.1',
+      'response_headers': {
+          'Age': '866',
+          'Content-Length': '14187',
+          'Date': 'Fri, 22 Apr 2016 08:56:19 -0200',
+          'Vary': 'Accept-Encoding',
+      },
+      'timestamp': 5535648.730768,
+      'timing': {
+          'connect_end': 34.0510001406074,
+          'connect_start': 21.6859998181462,
+          'dns_end': 21.6859998181462,
+          'dns_start': 0,
+          'loading_finished': 58.76399949193001,
+          'receive_headers_end': 47.0650000497699,
+          'request_time': 5535648.73264,
+          'send_end': 34.6099995076656,
+          'send_start': 34.2979999259114
+      },
+      'url': 'http://www.example.com/',
+      'status': 200,
+      'wall_time': 1461322579.59422}
+
+  def testHasValidators(self):
+    r = self._MakeRequest()
+    self.assertFalse(CachingPolicy(r).HasValidators())
+    r.response_headers['Last-Modified'] = 'Yesterday all my troubles'
+    self.assertTrue(CachingPolicy(r).HasValidators())
+    r = self._MakeRequest()
+    r.response_headers['ETAG'] = 'ABC'
+    self.assertTrue(CachingPolicy(r).HasValidators())
+
+  def testIsCacheable(self):
+    r = self._MakeRequest()
+    self.assertTrue(CachingPolicy(r).IsCacheable())
+    r.response_headers['Cache-Control'] = 'Whatever,no-store'
+    self.assertFalse(CachingPolicy(r).IsCacheable())
+
+  def testPolicyNoStore(self):
+    r = self._MakeRequest()
+    r.response_headers['Cache-Control'] = 'Whatever,no-store'
+    self.assertEqual(CachingPolicy.FETCH, CachingPolicy(r).PolicyAtDate(0))
+
+  def testPolicyMaxAge(self):
+    r = self._MakeRequest()
+    r.response_headers['Cache-Control'] = 'whatever,max-age=1000,whatever'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 10000))
+    # Take current age into account.
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 500))
+    # Max-Age before Expires.
+    r.response_headers['Expires'] = 'Thu, 21 Apr 2016 00:00:00 -0200'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    # Max-Age < age
+    r.response_headers['Cache-Control'] = 'whatever,max-age=100,whatever'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 2))
+
+  def testPolicyExpires(self):
+    r = self._MakeRequest()
+    # Already expired
+    r.response_headers['Expires'] = 'Thu, 21 Apr 2016 00:00:00 -0200'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    r.response_headers['Expires'] = 'Thu, 25 Apr 2016 00:00:00 -0200'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,\
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 86400))
+    self.assertEqual(CachingPolicy.VALIDATION_SYNC,
+                     CachingPolicy(r).PolicyAtDate(r.wall_time + 86400 * 5))
+
+  def testStaleWhileRevalidate(self):
+    r = self._MakeRequest()
+    r.response_headers['Cache-Control'] = (
+        'whatever,max-age=100,stale-while-revalidate=2000')
+    self.assertEqual(
+        CachingPolicy.VALIDATION_ASYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 200))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 2000))
+    # must-revalidate overrides stale-while-revalidate.
+    r.response_headers['Cache-Control'] += ',must-revalidate'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 200))
+
+  def test301NeverExpires(self):
+    r = self._MakeRequest()
+    r.status = 301
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 2000))
+
+  def testLastModifiedHeuristic(self):
+    r = self._MakeRequest()
+    # 8 hours ago.
+    r.response_headers['Last-Modified'] = 'Fri, 22 Apr 2016 00:56:19 -0200'
+    del r.response_headers['Age']
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 60))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 3600))
+
+  @classmethod
+  def _MakeRequest(cls):
+    return Request.FromJsonDict(copy.deepcopy(cls._REQUEST))
+
+
 class RequestTrackTestCase(unittest.TestCase):
   _REQUEST_WILL_BE_SENT = {
       'method': 'Network.requestWillBeSent',

commit 49ae5151989d0d7e72f98753ebb2e0130875f09f
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 22 02:09:03 2016 -0700

    clovis: Record the trace collection time when the navigation starts.
    
    Trace collection time is useful when looking at "Expires" headers. It is
    currently recorded when the DevTools connection is established, which is
    not when the page starts loading. This moves it later.
    
    Review URL: https://codereview.chromium.org/1912703002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389058}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: efe753e724d5b30a7359a345bc76c031ca30c8fe

diff --git a/loading/controller.py b/loading/controller.py
index 0511998..96eec7f 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,7 +10,6 @@ desktop-specific versions.
 """
 
 import contextlib
-import datetime
 import logging
 import os
 import shutil
@@ -175,8 +174,6 @@ class ChromeControllerBase(object):
     if self._emulated_network:
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
-    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
-                          seconds_since_epoch=time.time())
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index d4139f7..2a291e5 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -4,7 +4,9 @@
 
 """Represents the trace of a page load."""
 
+import datetime
 import json
+import time
 
 import devtools_monitor
 import page_track
@@ -94,8 +96,13 @@ class LoadingTrace(object):
     trace = tracing.TracingTrack(
         connection,
         additional_categories=additional_categories)
+    start_date_str = datetime.datetime.utcnow().isoformat()
+    seconds_since_epoch=time.time()
     connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
-    return cls(url, chrome_metadata, page, request, trace)
+    trace = cls(url, chrome_metadata, page, request, trace)
+    trace.metadata.update(date=start_date_str,
+                          seconds_since_epoch=seconds_since_epoch)
+    return trace
 
   @property
   def tracing_track(self):

commit da4f7a74a97f93eefc29c328cfebdc3627f0d15b
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 21 04:23:56 2016 -0700

    clovis: Convert Timing to a proper class.
    
    This overdue change remove some headaches, and more importantly,
    preserves compatibility with past traces when the timing events list is
    enriched.
    
    Review URL: https://codereview.chromium.org/1909893002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388742}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cf759fbe8f87d50598eb169df75522395300d2e6

diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index 6a64c86..7d1631b 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -7,7 +7,7 @@ import unittest
 
 from content_classification_lens import (ContentClassificationLens,
                                          _RulesMatcher)
-from request_track import (Request, TimingFromDict)
+from request_track import Request
 import test_utils
 
 
@@ -21,7 +21,7 @@ class ContentClassificationLensTestCase(unittest.TestCase):
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
                                    'status': 200,
-                                   'timing': TimingFromDict({})})
+                                   'timing': {}})
   _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
                    'frame_id': _MAIN_FRAME_ID},
                   {'method': 'Page.frameAttached',
@@ -100,7 +100,7 @@ class _MatcherTestCase(unittest.TestCase):
        'frame_id': '123.1',
        'initiator': {'type': 'other'},
        'timestamp': 2,
-       'timing': TimingFromDict({})})
+       'timing': {}})
 
   def testRemovesWhitelistRules(self):
     matcher = _RulesMatcher(self._RULES_WITH_WHITELIST, False)
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
index 68433f5..23441f4 100644
--- a/loading/dependency_graph_unittest.py
+++ b/loading/dependency_graph_unittest.py
@@ -18,7 +18,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
 
   def testUpdateRequestCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
-    requests[0].timing = request_track.TimingFromDict(
+    requests[0].timing = request_track.Timing.FromDevToolsDict(
         {'requestTime': 12, 'loadingFinished': 10})
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
@@ -35,7 +35,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
   def testCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
     for (index, request) in enumerate(requests):
-      request.timing = request_track.TimingFromDict(
+      request.timing = request_track.Timing.FromDevToolsDict(
           {'requestTime': index, 'receiveHeadersEnd': 10,
            'loadingFinished': 10})
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 08e9c17..67ab3f5 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -7,7 +7,7 @@ import unittest
 import devtools_monitor
 from loading_trace import LoadingTrace
 from request_dependencies_lens import RequestDependencyLens
-from request_track import (Request, TimingFromDict)
+from request_track import Request
 import test_utils
 
 
@@ -15,12 +15,12 @@ class TestRequests(object):
   FIRST_REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com', 'request_id': '1234.redirect.1',
        'initiator': {'type': 'other'},
-       'timestamp': 0.5, 'timing': TimingFromDict({})})
+       'timestamp': 0.5, 'timing': {}})
   SECOND_REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com/redirect1', 'request_id': '1234.redirect.2',
        'initiator': {'type': 'redirect',
                      'initiating_request': '1234.redirect.1'},
-       'timestamp': 1, 'timing': TimingFromDict({})})
+       'timestamp': 1, 'timing': {}})
   REDIRECTED_REQUEST = Request.FromJsonDict({
       'url': 'http://bla.com/index.html',
       'request_id': '1234.1',
@@ -28,13 +28,13 @@ class TestRequests(object):
       'initiator': {'type': 'redirect',
                     'initiating_request': '1234.redirect.2'},
       'timestamp': 2,
-      'timing': TimingFromDict({})})
+      'timing': {}})
   REQUEST = Request.FromJsonDict({'url': 'http://bla.com/index.html',
                                   'request_id': '1234.1',
                                   'frame_id': '123.1',
                                   'initiator': {'type': 'other'},
                                   'timestamp': 2,
-                                  'timing': TimingFromDict({})})
+                                  'timing': {}})
   JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
                                      'request_id': '1234.12',
                                      'frame_id': '123.123',
@@ -42,21 +42,21 @@ class TestRequests(object):
                                          'type': 'parser',
                                          'url': 'http://bla.com/index.html'},
                                      'timestamp': 3,
-                                     'timing': TimingFromDict({})})
+                                     'timing': {}})
   JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
        'request_id': '1234.42',
        'frame_id': '123.13',
        'initiator': {'type': 'parser',
                      'url': 'http://bla.com/index.html'},
-       'timestamp': 4, 'timing': TimingFromDict({})})
+       'timestamp': 4, 'timing': {}})
   JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
        'request_id': '1234.56',
        'frame_id': '123.99',
        'initiator': {'type': 'parser',
                      'url': 'http://bla.com/index.html'},
-       'timestamp': 5, 'timing': TimingFromDict({})})
+       'timestamp': 5, 'timing': {}})
   JS_REQUEST_2 = Request.FromJsonDict(
       {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
        'frame_id': '123.123',
@@ -64,7 +64,7 @@ class TestRequests(object):
                      'stack': {'callFrames': [
                          {'url': 'unknown'},
                          {'url': 'http://bla.com/nyancat.js'}]}},
-       'timestamp': 10, 'timing': TimingFromDict({})})
+       'timestamp': 10, 'timing': {}})
   PAGE_EVENTS = [{'method': 'Page.frameAttached',
                    'frame_id': '123.13', 'parent_frame_id': '123.1'},
                  {'method': 'Page.frameAttached',
@@ -139,7 +139,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
              'stack': {'callFrames': [],
                        'parent': {'callFrames': [
                                       {'url': 'http://bla.com/nyancat.js'}]}}},
-         'timestamp': 10, 'timing': TimingFromDict({})})
+         'timestamp': 10, 'timing': {}})
     loading_trace = test_utils.LoadingTraceFromEvents(
         [TestRequests.JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
diff --git a/loading/request_track.py b/loading/request_track.py
index 9f513d2..613f6e0 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -18,17 +18,55 @@ import urlparse
 import devtools_monitor
 
 
-_TIMING_NAMES_MAPPING = {
-    'connectEnd': 'connect_end', 'connectStart': 'connect_start',
-    'dnsEnd': 'dns_end', 'dnsStart': 'dns_start', 'proxyEnd': 'proxy_end',
-    'proxyStart': 'proxy_start', 'receiveHeadersEnd': 'receive_headers_end',
-    'requestTime': 'request_time', 'sendEnd': 'send_end',
-    'sendStart': 'send_start', 'sslEnd': 'ssl_end', 'sslStart': 'ssl_start',
-    'workerReady': 'worker_ready', 'workerStart': 'worker_start',
-    'loadingFinished': 'loading_finished', 'pushStart' : 'push_start',
-    'pushEnd' : 'push_end'}
+class Timing(object):
+  """Collects the timing data for a request."""
+  _TIMING_NAMES = (
+      ('connectEnd', 'connect_end'), ('connectStart', 'connect_start'),
+      ('dnsEnd', 'dns_end'), ('dnsStart', 'dns_start'),
+      ('proxyEnd', 'proxy_end'), ('proxyStart', 'proxy_start'),
+      ('receiveHeadersEnd', 'receive_headers_end'),
+      ('requestTime', 'request_time'), ('sendEnd', 'send_end'),
+      ('sendStart', 'send_start'), ('sslEnd', 'ssl_end'),
+      ('sslStart', 'ssl_start'), ('workerReady', 'worker_ready'),
+      ('workerStart', 'worker_start'),
+      ('loadingFinished', 'loading_finished'), ('pushStart', 'push_start'),
+      ('pushEnd', 'push_end'))
+  _TIMING_NAMES_MAPPING = dict(_TIMING_NAMES)
+  __slots__ = tuple(x[1] for x in _TIMING_NAMES)
+
+  def __init__(self, **kwargs):
+    """Constructor.
+
+    Initialize with keywords arguments from __slots__.
+    """
+    for slot in self.__slots__:
+      setattr(self, slot, -1)
+    for (attr, value) in kwargs.items():
+      setattr(self, attr, value)
 
-Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
+  def __eq__(self, o):
+    return all(getattr(self, attr) == getattr(o, attr)
+               for attr in self.__slots__)
+
+  def LargestOffset(self):
+    """Returns the largest offset in the available timings."""
+    return max(0, max(
+        getattr(self, attr) for attr in self.__slots__
+        if attr != 'request_time'))
+
+  def ToJsonDict(self):
+    return {attr: getattr(self, attr)
+            for attr in self.__slots__ if getattr(self, attr) != -1}
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    return cls(**json_dict)
+
+  @classmethod
+  def FromDevToolsDict(cls, json_dict):
+    """Returns an instance of Timing from a dict, as passed by DevTools."""
+    timing_dict = {
+        cls._TIMING_NAMES_MAPPING[k]: v for (k, v) in json_dict.items()}
+    return cls(**timing_dict)
 
 
 def ShortName(url):
@@ -70,9 +108,7 @@ def IntervalBetween(first, second, reason):
   if reason == 'parser':
     first_offset_ms = first.timing.receive_headers_end
   else:
-    first_offset_ms = max(
-        [0] + [t for f, t in first.timing._asdict().iteritems()
-               if f != 'request_time'])
+    first_offset_ms = first.timing.LargestOffset()
   return (first.timing.request_time * 1000 + first_offset_ms, second_ms)
 
 
@@ -173,10 +209,7 @@ class Request(object):
   def end_msec(self):
     if self.start_msec is None:
       return None
-    return self.start_msec + max(
-        [0] + [t for f, t in self.timing._asdict().iteritems()
-               if f != 'request_time'])
-
+    return self.start_msec + self.timing.LargestOffset()
 
   def _TimestampOffsetFromStartMs(self, timestamp):
     assert self.timing.request_time != -1
@@ -184,7 +217,9 @@ class Request(object):
     return (timestamp - request_time) * 1000
 
   def ToJsonDict(self):
-    return copy.deepcopy(self.__dict__)
+    result = copy.deepcopy(self.__dict__)
+    result['timing'] = self.timing.ToJsonDict()
+    return result
 
   @classmethod
   def FromJsonDict(cls, data_dict):
@@ -194,9 +229,9 @@ class Request(object):
     if not result.response_headers:
       result.response_headers = {}
     if result.timing:
-      result.timing = Timing(*result.timing)
+      result.timing = Timing.FromJsonDict(result.timing)
     else:
-      result.timing = TimingFromDict({'requestTime': result.timestamp})
+      result.timing = Timing(request_time=result.timestamp)
     return result
 
   def GetHTTPResponseHeader(self, header_name):
@@ -269,8 +304,7 @@ class Request(object):
     request_time and the latest timing event.
     """
     # All fields in timing are millis relative to request_time.
-    return max([0] + [t for f, t in self.timing._asdict().iteritems()
-                      if f != 'request_time'])
+    return self.timing.LargestOffset()
 
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
@@ -474,7 +508,7 @@ class RequestTrack(devtools_monitor.Track):
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache')))
-    r.timing = TimingFromDict(redirect_response['timing'])
+    r.timing = Timing.FromDevToolsDict(redirect_response['timing'])
 
     redirect_index = self._redirects_count_by_id[request_id]
     self._redirects_count_by_id[request_id] += 1
@@ -530,7 +564,7 @@ class RequestTrack(devtools_monitor.Track):
       timing_dict = {'requestTime': r.timestamp}
     else:
       timing_dict = response['timing']
-    r.timing = TimingFromDict(timing_dict)
+    r.timing = Timing.FromDevToolsDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
     self._request_id_to_response_received[request_id] = params
 
@@ -548,8 +582,8 @@ class RequestTrack(devtools_monitor.Track):
     assert (status == RequestTrack._STATUS_RESPONSE
             or status == RequestTrack._STATUS_DATA)
     r.encoded_data_length = params['encodedDataLength']
-    r.timing = r.timing._replace(
-        loading_finished=r._TimestampOffsetFromStartMs(params['timestamp']))
+    r.timing.loading_finished = r._TimestampOffsetFromStartMs(
+        params['timestamp'])
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
     self._FinalizeRequest(request_id)
 
@@ -581,15 +615,6 @@ RequestTrack._METHOD_TO_HANDLER = {
     'Network.loadingFailed': RequestTrack._LoadingFailed}
 
 
-def TimingFromDict(timing_dict):
-  """Returns an instance of Timing from an () dict."""
-  complete_timing_dict = {field: -1 for field in Timing._fields}
-  timing_dict_mapped = {
-      _TIMING_NAMES_MAPPING[k]: v for (k, v) in timing_dict.items()}
-  complete_timing_dict.update(timing_dict_mapped)
-  return Timing(**complete_timing_dict)
-
-
 def _CopyFromDictToObject(d, o, key_attrs):
   for (key, attr) in key_attrs:
     if key in d:
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 6e54208..a613397 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -6,7 +6,7 @@ import copy
 import json
 import unittest
 
-from request_track import (TimeBetween, Request, RequestTrack, TimingFromDict)
+from request_track import (TimeBetween, Request, RequestTrack, Timing)
 
 
 class TimeBetweenTestCase(unittest.TestCase):
@@ -15,17 +15,17 @@ class TimeBetweenTestCase(unittest.TestCase):
                                    'frame_id': '123.1',
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
-                                   'timing': TimingFromDict({})})
+                                   'timing': {}})
   def setUp(self):
     super(TimeBetweenTestCase, self).setUp()
     self.first = copy.deepcopy(self._REQUEST)
-    self.first.timing = TimingFromDict({'requestTime': 123456,
-                                        'receiveHeadersEnd': 100,
-                                        'loadingFinished': 500})
+    self.first.timing = Timing.FromDevToolsDict({'requestTime': 123456,
+                                                 'receiveHeadersEnd': 100,
+                                                 'loadingFinished': 500})
     self.second = copy.deepcopy(self._REQUEST)
-    self.second.timing = TimingFromDict({'requestTime': 123456 + 1,
-                                        'receiveHeadersEnd': 200,
-                                        'loadingFinished': 600})
+    self.second.timing = Timing.FromDevToolsDict({'requestTime': 123456 + 1,
+                                                  'receiveHeadersEnd': 200,
+                                                  'loadingFinished': 600})
 
   def testTimeBetweenParser(self):
     self.assertEquals(900, TimeBetween(self.first, self.second, 'parser'))
@@ -329,11 +329,11 @@ class RequestTrackTestCase(unittest.TestCase):
     self.assertEquals(False, r.served_from_cache)
     self.assertEquals(False, r.from_disk_cache)
     self.assertEquals(False, r.from_service_worker)
-    timing = TimingFromDict(response['timing'])
+    timing = Timing.FromDevToolsDict(response['timing'])
     loading_finished = RequestTrackTestCase._LOADING_FINISHED['params']
     loading_finished_offset = r._TimestampOffsetFromStartMs(
         loading_finished['timestamp'])
-    timing = timing._replace(loading_finished=loading_finished_offset)
+    timing.loading_finished = loading_finished_offset
     self.assertEquals(timing, r.timing)
     self.assertEquals(200, r.status)
     self.assertEquals(
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 8ae919f..8f61969 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -16,7 +16,9 @@ import user_satisfied_lens
 class FakeRequestTrack(devtools_monitor.Track):
   def __init__(self, events):
     super(FakeRequestTrack, self).__init__(None)
-    self._events = [self._RewriteEvent(e) for e in events]
+    self._events = events
+    for e in self._events:
+      e.timing.request_time = e.timestamp
 
   def Handle(self, _method, _msg):
     assert False  # Should never be called.
@@ -32,12 +34,6 @@ class FakeRequestTrack(devtools_monitor.Track):
                 cls._DUPLICATES_KEY: 0,
                 cls._INCONSISTENT_INITIATORS_KEY: 0}}
 
-  def _RewriteEvent(self, event):
-    # This modifies the instance used across tests, so this method
-    # must be idempotent.
-    event.timing = event.timing._replace(request_time=event.timestamp)
-    return event
-
 
 class FakePageTrack(devtools_monitor.Track):
   def __init__(self, events):
@@ -70,14 +66,14 @@ def MakeRequestWithTiming(
     source_url: a url or number which will be used as the source (initiating)
       url. If the source url is not present, then url will be a root. The
       convention in tests is to use a source_url of 'null' in this case.
-    timing_dict: (dict) Suitable to be passed to request_track.TimingFromDict().
+    timing_dict: (dict) Suitable to be passed to request_track.Timing().
     initiator_type: the initiator type to use.
 
   Returns:
     A request_track.Request.
   """
   assert initiator_type in ('other', 'parser')
-  timing = request_track.TimingFromDict(timing_dict)
+  timing = request_track.Timing.FromDevToolsDict(timing_dict)
   rq = request_track.Request.FromJsonDict({
       'timestamp': timing.request_time,
       'request_id': str(MakeRequestWithTiming._next_request_id),
@@ -86,7 +82,7 @@ def MakeRequestWithTiming(
       'response_headers': {'Content-Type':
                            'null' if not magic_content_type
                            else 'magic-debug-content' },
-      'timing': request_track.TimingAsList(timing)
+      'timing': timing.ToJsonDict()
   })
   MakeRequestWithTiming._next_request_id += 1
   return rq
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 29cf3d5..2e35a50 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -25,9 +25,8 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
         'frame_id': '123.%s' % timestamp_msec,
         'initiator': {'type': 'other'},
         'timestamp': timestamp_sec,
-        'timing': request_track.TimingFromDict({
-            'requestTime': timestamp_sec,
-            'loadingFinished': duration})
+        'timing': {'request_time': timestamp_sec,
+                   'loading_finished': duration}
         })
     self._request_index += 1
     return rq

commit 0c70b224115419437422751155bb9384d0aa0acb
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 21 01:54:14 2016 -0700

    tools/android/loading: rm testdata/rollingstone.trace.gz
    
    ./testdata/rollingstone.trace.gz has become out of date, causing
    ./loading_trace_analyzer_unittest.py to fail. This CL removes both
    files to get ./run_tests passing again.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1907603003
    
    Cr-Original-Commit-Position: refs/heads/master@{#388728}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c799dc85bde528d0bbf39e5e432a6ecb8bb22eca

diff --git a/loading/loading_trace_analyzer_unittest.py b/loading/loading_trace_analyzer_unittest.py
deleted file mode 100644
index b01aab2..0000000
--- a/loading/loading_trace_analyzer_unittest.py
+++ /dev/null
@@ -1,59 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import gzip
-import os
-import re
-import shutil
-import subprocess
-import tempfile
-import unittest
-
-import loading_trace_analyzer
-
-LOADING_DIR = os.path.dirname(__file__)
-TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
-
-
-class LoadingTraceAnalyzerTest(unittest.TestCase):
-  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
-
-  def setUp(self):
-    self._temp_dir = tempfile.mkdtemp()
-    self.trace_path = self._TmpPath('trace.json')
-    with gzip.GzipFile(self._ROLLING_STONE) as f:
-      with open(self.trace_path, 'w') as g:
-        g.write(f.read())
-
-  def tearDown(self):
-    shutil.rmtree(self._temp_dir)
-
-  def _TmpPath(self, name):
-    return os.path.join(self._temp_dir, name)
-
-  def testRequestsCmd(self):
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path)]
-    self.assertNotEqual(0, len(lines))
-
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
-        output_format='hello {protocol} world {url}')]
-    self.assertNotEqual(0, len(lines))
-    for line in lines:
-      self.assertTrue(re.match(r'^hello \S+ world \S+$', line))
-
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
-        where_format='{url}', where_statement=r'^http://.*$')]
-    self.assertNotEqual(0, len(lines))
-    for line in lines:
-      self.assertTrue(line.startswith('http://'))
-
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
-        where_format='{url}', where_statement=r'^https://.*$')]
-    self.assertNotEqual(0, len(lines))
-    for line in lines:
-      self.assertTrue(line.startswith('https://'))
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/testdata/rollingstone.trace.gz b/loading/testdata/rollingstone.trace.gz
deleted file mode 100644
index f4a239e..0000000
Binary files a/loading/testdata/rollingstone.trace.gz and /dev/null differ

commit ba5a071e1f45f139cab91a69103366d911970349
Author: droger <droger@chromium.org>
Date:   Wed Apr 20 10:52:07 2016 -0700

    tools/android/loading Loading trace database improvements
    
    The loading trace database is now reloaded from the cloud when the
    worker starts, which prevents losing data when the worker restarts after
    a failure.
    
    There is also now one trace database per worker, which solves the issues
    of concurrent access to the database.
    
    The CL also adds supports for the 'pushStart' and 'pushEnd'
    events that were added by CL:
    https://codereview.chromium.org/1828203005/
    
    Review URL: https://codereview.chromium.org/1908483002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388530}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4a50ba25fd9177163031cfc403e2d7e4bf036d99

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 5f534f2..b181cfa 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -95,15 +95,21 @@ gcloud beta auth application-default login --scopes \
     https://www.googleapis.com/auth/cloud-platform
 ```
 
-Create a JSON file describing the deployment configuration:
-
+Create a JSON dictionary file describing the deployment configuration, with the
+keys:
+
+-   `project_name` (string): Name of the Google Cloud project
+-   `cloud_storage_path` (string): Path in Google Storage where generated traces
+    will be stored.
+-   `chrome_path` (string): Path to the Chrome executable.
+-   `src_path` (string): Path to the Chromium source directory.
+-   `taskqueue_tag` (string):
+-   `trace_database_filename` (string, optional): Filename for the trace
+    database in Cloud Storage. Must be unique per worker to avoid concurrent
+    access. Defaults to `trace_database.json`.
+
+Example:
 ```shell
-# CONFIG_FILE is the output json file.
-# PROJECT_NAME is the Google Cloud project.
-# CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
-# be stored.
-# CHROME_PATH is the path to the Chrome executable on the host.
-# CHROMIUM_SRC is the Chromium src directory.
 cat >$CONFIG_FILE << EOF
 {
   "project_name" : "$PROJECT_NAME",
diff --git a/loading/cloud/backend/google_storage_accessor.py b/loading/cloud/backend/google_storage_accessor.py
index 86e238e..ded3fe8 100644
--- a/loading/cloud/backend/google_storage_accessor.py
+++ b/loading/cloud/backend/google_storage_accessor.py
@@ -2,7 +2,8 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-from gcloud import storage
+import gcloud.exceptions
+import gcloud.storage
 
 
 class GoogleStorageAccessor(object):
@@ -18,12 +19,26 @@ class GoogleStorageAccessor(object):
 
   def _GetStorageClient(self):
     """Returns the storage client associated with the project"""
-    return storage.Client(project = self._project_name,
-                          credentials = self._credentials)
+    return gcloud.storage.Client(project = self._project_name,
+                                 credentials = self._credentials)
 
   def _GetStorageBucket(self, storage_client):
     return storage_client.get_bucket(self._bucket_name)
 
+  def DownloadAsString(self, remote_filename):
+    """Returns the content of a remote file as a string, or None if the file
+    does not exist.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.get_blob(remote_filename)
+    if not blob:
+      return None
+    try:
+      return blob.download_as_string()
+    except gcloud.exceptions.NotFound:
+      return None
+
   def UploadFile(self, filename_src, filename_dest):
     """Uploads a file to Google Cloud Storage
 
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index e429c1e..3d3084f 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -12,11 +12,15 @@ get_instance_metadata() {
       -H "Metadata-Flavor: Google"
 }
 
-# Talk to the metadata server to get the project id
+# Talk to the metadata server to get the project id and the instance id
 PROJECTID=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
     -H "Metadata-Flavor: Google")
 
+INSTANCE_ID=$(curl -s \
+    "http://metadata.google.internal/computeMetadata/v1/instance/hostname" \
+    -H "Metadata-Flavor: Google")
+
 # Install dependencies from apt
 apt-get update
 # Basic dependencies
@@ -73,7 +77,8 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
-  "taskqueue_tag" : "$TASKQUEUE_TAG"
+  "taskqueue_tag" : "$TASKQUEUE_TAG",
+  "trace_database_filename" : "trace_database_${INSTANCE_ID}.json"
 }
 EOF
 
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index b9900ce..9518171 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -32,6 +32,7 @@ class Worker(object):
     """See README.md for the config format."""
     self._project_name = config['project_name']
     self._taskqueue_tag = config['taskqueue_tag']
+    self._src_path = config['src_path']
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
 
@@ -45,15 +46,18 @@ class Worker(object):
       if not self._base_path_in_bucket.endswith('/'):
         self._base_path_in_bucket += '/'
 
-    # TODO: improve the trace database to support concurrent access.
-    self._traces_dir = self._base_path_in_bucket + 'traces/'
-    self._trace_database = LoadingTraceDatabase({})
-
-    self._src_path = config['src_path']
     self._google_storage_accessor = GoogleStorageAccessor(
         credentials=self._credentials, project_name=self._project_name,
         bucket_name=self._bucket_name)
 
+    self._traces_dir = os.path.join(self._base_path_in_bucket, 'traces')
+    self._trace_database_path = os.path.join(
+        self._traces_dir,
+        config.get('trace_database_filename', 'trace_database.json'))
+
+    # Recover any existing trace database in case the worker died.
+    self._DownloadTraceDatabase()
+
     # Initialize the global options that will be used during trace generation.
     options.OPTIONS.ParseArgs([])
     options.OPTIONS.local_binary = config['chrome_path']
@@ -94,6 +98,21 @@ class Worker(object):
       self._logger.info('Finished task %s' % task_id)
     self._Finalize()
 
+  def _DownloadTraceDatabase(self):
+    """Downloads the trace database from CloudStorage."""
+    self._logger.info('Downloading trace database')
+    trace_database_string = self._google_storage_accessor.DownloadAsString(
+        self._trace_database_path) or '{}'
+    trace_database_dict = json.loads(trace_database_string)
+    self._trace_database = LoadingTraceDatabase(trace_database_dict)
+
+  def _UploadTraceDatabase(self):
+    """Uploads the trace database to CloudStorage."""
+    self._logger.info('Uploading trace database')
+    self._google_storage_accessor.UploadString(
+        json.dumps(self._trace_database.ToJsonDict(), indent=2),
+        self._trace_database_path)
+
   def _FetchClovisTask(self, project_name, task_api, queue_name):
     """Fetches a ClovisTask from the task queue.
 
@@ -119,14 +138,9 @@ class Worker(object):
 
   def _Finalize(self):
     """Called before exiting."""
-    self._logger.info('Uploading trace database')
-    self._google_storage_accessor.UploadString(
-        json.dumps(self._trace_database.ToJsonDict(), indent=2),
-        self._traces_dir + 'trace_database.json')
     # TODO(droger): Implement automatic instance destruction.
     self._logger.info('Done')
 
-
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
     """ Generates a trace.
@@ -201,9 +215,9 @@ class Worker(object):
     emulate_device = params.get('emulate_device')
     emulate_network = params.get('emulate_network')
 
-    failures_dir = self._base_path_in_bucket + 'failures/'
+    failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
     # TODO(blundell): Fix this up.
-    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
+    logs_dir = os.path.join(self._base_path_in_bucket, 'analyze_logs')
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
@@ -213,27 +227,27 @@ class Worker(object):
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         self._logger.debug('Generating trace for URL: %s' % url)
-        remote_filename = local_filename + '/' + str(repeat)
+        remote_filename = os.path.join(local_filename, str(repeat))
         trace_metadata = self._GenerateTrace(
             url, emulate_device, emulate_network, local_filename, log_filename)
         if trace_metadata['succeeded']:
           self._logger.debug('Uploading: %s' % remote_filename)
-          remote_trace_location = self._traces_dir + remote_filename
+          remote_trace_location = os.path.join(self._traces_dir,
+                                               remote_filename)
           self._google_storage_accessor.UploadFile(local_filename,
                                                    remote_trace_location)
-          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
-              remote_trace_location)
-          self._trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
+          full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
+                                                 remote_trace_location)
+          self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
         else:
           self._logger.warning('Trace generation failed for URL: %s' % url)
-          # TODO: upload the failure
           if os.path.isfile(local_filename):
-            self._google_storage_accessor.UploadFile(local_filename,
-                                            failures_dir + remote_filename)
-            self._logger.debug('Uploading log')
-        self._google_storage_accessor.UploadFile(log_filename,
-                                                 logs_dir + remote_filename)
-
+            self._google_storage_accessor.UploadFile(
+                local_filename, os.path.join(failures_dir, remote_filename))
+        self._logger.debug('Uploading analyze log')
+        self._google_storage_accessor.UploadFile(
+            log_filename, os.path.join(logs_dir, remote_filename))
+    self._UploadTraceDatabase()
 
 if __name__ == '__main__':
   parser = argparse.ArgumentParser(
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index 2ef33b0..dad6153 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -14,9 +14,10 @@ class LoadingTraceDatabase(object):
        about those traces."""
     self._traces_dict = traces_dict
 
-  def AddTrace(self, filename, trace_dict):
-    """Adds a mapping from |filename| to |trace_dict| into the database."""
-    assert filename not in self._traces_dict
+  def SetTrace(self, filename, trace_dict):
+    """Sets a mapping from |filename| to |trace_dict| into the database.
+    If there is an existing mapping for filename, it is replaced.
+    """
     self._traces_dict[filename] = trace_dict
 
   def GetTraceFilesForURL(self, url):
diff --git a/loading/loading_trace_database_unittest.py b/loading/loading_trace_database_unittest.py
index 31dc9b3..e64fb8c 100644
--- a/loading/loading_trace_database_unittest.py
+++ b/loading/loading_trace_database_unittest.py
@@ -32,11 +32,11 @@ class LoadingTraceDatabaseUnittest(unittest.TestCase):
     self.assertEqual(
         self._JSON_DATABASE, self.database.ToJsonDict())
 
-  def testAddTrace(self):
+  def testSetTrace(self):
     dummy_url = "http://dummy.com"
     new_trace_file = "traces/new_trace.json"
     self.assertEqual(self.database.GetTraceFilesForURL(dummy_url), [])
-    self.database.AddTrace(new_trace_file, {"url" : dummy_url})
+    self.database.SetTrace(new_trace_file, {"url" : dummy_url})
     self.assertEqual(self.database.GetTraceFilesForURL(dummy_url),
                      [new_trace_file])
 
diff --git a/loading/request_track.py b/loading/request_track.py
index bf19fbb..9f513d2 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -25,7 +25,8 @@ _TIMING_NAMES_MAPPING = {
     'requestTime': 'request_time', 'sendEnd': 'send_end',
     'sendStart': 'send_start', 'sslEnd': 'ssl_end', 'sslStart': 'ssl_start',
     'workerReady': 'worker_ready', 'workerStart': 'worker_start',
-    'loadingFinished': 'loading_finished'}
+    'loadingFinished': 'loading_finished', 'pushStart' : 'push_start',
+    'pushEnd' : 'push_end'}
 
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 

commit fb08d39e524f66c5f9ed3561b4dc5408711fb230
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 20 08:21:32 2016 -0700

    tools/android/loading: Fixes a bug in chrome_cache.UnzipDirectoryContent
    
    os.utime(file_path, ...) modifies modification time of file_path's parent
    directories. Therefore we now call os.utime on files and directories that
    have longer relative paths first.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1899843006
    
    Cr-Original-Commit-Position: refs/heads/master@{#388501}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 74668f8f0ab3a1c27997f5023aa463907d75fd2a

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 9d486e9..4c1f226 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -219,7 +219,11 @@ def UnzipDirectoryContent(archive_path, directory_dest_path):
           f.write(zip_input.read(file_archive_name))
 
     assert timestamps
-    for relative_path, stats in timestamps.iteritems():
+    # os.utime(file_path, ...) modifies modification time of file_path's parent
+    # directories. Therefore we call os.utime on files and directories that have
+    # longer relative paths first.
+    for relative_path in sorted(timestamps.keys(), key=len, reverse=True):
+      stats = timestamps[relative_path]
       output_path = os.path.join(directory_dest_path, relative_path)
       if not os.path.exists(output_path):
         os.makedirs(output_path)

commit 30f4a22ec0afb0ed8f3115f76a319775915e0d15
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 20 02:45:15 2016 -0700

    tools/android/loading: Fix cache pushing non deterministic bug on Android
    
    On Android, Sandwich create a cache of all resources by navigating to
    the URLs and then give a slow death to chrome: switch it to background
    so it can persist its cache on disk before killing it. However the next
    sandwich run was getting non deterministic cache use, because chrome
    was previously not killed enough.
    
    This CL fix this non deterministic bug by using am force-stop command
    line instead of kill -9 <pids>.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1902133002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388457}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: be415ce063ec0fb6799a1c4fe1b27845f7f2a30d

diff --git a/loading/controller.py b/loading/controller.py
index 5944711..0511998 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -215,7 +215,7 @@ class RemoteChromeController(ChromeControllerBase):
     """Overridden connection creation."""
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
-    self._device.KillAll(package_info.package, quiet=True)
+    self._device.ForceStop(package_info.package)
     if OPTIONS.clear_device_data:
       logging.info('Clear Chrome data')
       self._device.adb.Shell('pm clear ' + package_info.package)
@@ -252,7 +252,7 @@ class RemoteChromeController(ChromeControllerBase):
               time.sleep(self.TIME_TO_IDLE_SECONDS)
             break
       finally:
-        self._device.KillAll(package_info.package, quiet=True)
+        self._device.ForceStop(package_info.package)
 
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""

commit 0645d7f8da640b5fba042215ea8d3609334f47d4
Author: lizeb <lizeb@chromium.org>
Date:   Wed Apr 20 01:38:39 2016 -0700

    clovis: Improve the handling of tracing categories.
    
    The baseline categories exclude 'cc', drastically reducing the size of a
    loading trace (85 -> 39MB in one sample case), and are suitable for all
    tools but sandwich. This one adds an extra category which is too heavy
    to be in the default set.
    
    Review URL: https://codereview.chromium.org/1901653002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388446}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a3bccc967be8d4d4e27f2f7449b9c63ad5fc440a

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index ff8b8c5..d4139f7 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -74,7 +74,7 @@ class LoadingTrace(object):
 
   @classmethod
   def RecordUrlNavigation(
-      cls, url, connection, chrome_metadata, categories=None,
+      cls, url, connection, chrome_metadata, additional_categories=None,
       timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
     """Create a loading trace by using controller to fetch url.
 
@@ -82,7 +82,8 @@ class LoadingTrace(object):
       url: (str) url to fetch.
       connection: An opened devtools connection.
       chrome_metadata: Dictionary of chrome metadata.
-      categories: TracingTrack categories to capture.
+      additional_categories: ([str] or None) TracingTrack additional categories
+                             to capture.
       timeout_seconds: monitoring connection timeout in seconds.
 
     Returns:
@@ -92,8 +93,7 @@ class LoadingTrace(object):
     request = request_track.RequestTrack(connection)
     trace = tracing.TracingTrack(
         connection,
-        categories=(tracing.DEFAULT_CATEGORIES if categories is None
-                    else categories))
+        additional_categories=additional_categories)
     connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
     return cls(url, chrome_metadata, page, request, trace)
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 1a9e96d..98d6317 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -30,15 +30,8 @@ import tracing
 
 
 # List of selected trace event categories when running chrome.
-CATEGORIES = [
-    # Need blink network trace events for prefetch_view.PrefetchSimulationView
-    'blink.net',
-
-    # Need to get mark trace events for _GetWebPageTrackedEvents()
-    'blink.user_timing',
-
-    # Need to memory dump trace event for _GetBrowserDumpEvents()
-    'disabled-by-default-memory-infra']
+ADDITIONAL_CATEGORIES = (
+    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
 
 CSV_FIELD_NAMES = [
     'id',
@@ -145,6 +138,11 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   Returns:
     Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
   """
+  assert all(
+      cat in loading_trace.tracing_track.Categories()
+      for cat in ADDITIONAL_CATEGORIES), (
+          'This trace was not generated with the required set of categories '
+          'to be processed by this script.')
   browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   web_page_tracked_events = _GetWebPageTrackedEvents(
       loading_trace.tracing_track)
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 7cbe795..1976462 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -44,7 +44,9 @@ _MINIMALIST_TRACE_EVENTS = [
 
 
 def TracingTrack(events):
-  return tracing.TracingTrack.FromJsonDict({'events': events})
+  return tracing.TracingTrack.FromJsonDict({
+      'events': events,
+      'categories': tracing.INITIAL_CATEGORIES + puller.ADDITIONAL_CATEGORIES})
 
 
 def LoadingTrace(events):
@@ -103,7 +105,7 @@ class PageTrackTest(unittest.TestCase):
         {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
         {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
-    self.assertTrue(_MEM_CAT in puller.CATEGORIES)
+    self.assertTrue(_MEM_CAT in puller.ADDITIONAL_CATEGORIES)
 
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
@@ -124,8 +126,6 @@ class PageTrackTest(unittest.TestCase):
       RunHelper(TRACE_EVENTS, 895)
 
   def testGetWebPageTrackedEvents(self):
-    self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
-
     trace_events = puller._GetWebPageTrackedEvents(TracingTrack([
         {'ph': 'R', 'ts':  0000, 'args': {},             'cat': 'whatever',
             'name': _START},
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index ebef772..39bf4d6 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -204,14 +204,14 @@ class SandwichRunner(object):
               url=url,
               connection=connection,
               chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              categories=sandwich_metrics.CATEGORIES,
+              additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
               timeout_seconds=_DEVTOOLS_TIMEOUT)
       else:
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
             url=url,
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            categories=sandwich_metrics.CATEGORIES,
+            additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
             timeout_seconds=_DEVTOOLS_TIMEOUT)
     if run_path is not None:
       trace_path = os.path.join(run_path, 'trace.json')
diff --git a/loading/testdata/scanner_vs_parser.trace b/loading/testdata/scanner_vs_parser.trace
index 627d141..75d10a2 100644
--- a/loading/testdata/scanner_vs_parser.trace
+++ b/loading/testdata/scanner_vs_parser.trace
@@ -52,6 +52,7 @@
     }
   },
   "tracing_track": {
+   "categories": [],
     "events": [
       {
         "args": {
diff --git a/loading/tracing.py b/loading/tracing.py
index 4891d3e..c1ecf71 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -12,7 +12,11 @@ import operator
 import devtools_monitor
 
 
-DEFAULT_CATEGORIES = None
+_DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
+INITIAL_CATEGORIES = (
+    'toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
+    'blink.user_timing', 'blink.net') + tuple(
+        '-' + cat for cat in _DISABLED_CATEGORIES)
 
 
 class TracingTrack(devtools_monitor.Track):
@@ -20,16 +24,16 @@ class TracingTrack(devtools_monitor.Track):
 
   See https://goo.gl/Qabkqk for details on the protocol.
   """
-  def __init__(self, connection,
-               categories=DEFAULT_CATEGORIES,
+  def __init__(self, connection, additional_categories=None,
                fetch_stream=False):
     """Initialize this TracingTrack.
 
     Args:
       connection: a DevToolsConnection.
-      categories: None, or a string, or list of strings, of tracing categories
-        to filter.
-
+      additional_categories: ([str] or None) If set, a list of additional
+                             categories to add. This cannot be used to re-enable
+                             a category which is disabled by default (see
+                             INITIAL_CATEGORIES), nor to disable a category.
       fetch_stream: if true, use a websocket stream to fetch tracing data rather
         than dataCollected events. It appears based on very limited testing that
         a stream is slower than the default reporting as dataCollected events.
@@ -37,10 +41,15 @@ class TracingTrack(devtools_monitor.Track):
     super(TracingTrack, self).__init__(connection)
     if connection:
       connection.RegisterListener('Tracing.dataCollected', self)
+    extra_categories = additional_categories or []
+    assert not (set(extra_categories) & set(_DISABLED_CATEGORIES)), (
+        'Cannot enable a disabled category')
+    assert not any(cat.startswith('-') for cat in extra_categories), (
+        'Cannot disable a category')
+    self._categories = set(
+        itertools.chain(INITIAL_CATEGORIES, extra_categories))
     params = {}
-    if categories:
-      params['categories'] = (categories if type(categories) is str
-                              else ','.join(categories))
+    params['categories'] = ','.join(self._categories)
     if fetch_stream:
       params['transferMode'] = 'ReturnAsStream'
 
@@ -62,15 +71,9 @@ class TracingTrack(devtools_monitor.Track):
     # update.
     self._interval_tree = None
 
-  def _GetMainFrameID(self):
-    """Returns the main frame ID."""
-    if not self._main_frame_id:
-      navigation_start_events = [e for e in self.GetEvents()
-          if e.Matches('blink.user_timing', 'navigationStart')]
-      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
-      self._main_frame_id = first_event.args['frame']
-
-    return self._main_frame_id
+  def Categories(self):
+    """Returns the set of categories in this trace."""
+    return self._categories
 
   def GetFirstEventMillis(self):
     """Find the canonical start time for this track.
@@ -110,9 +113,6 @@ class TracingTrack(devtools_monitor.Track):
     self._IndexEvents()
     return self._interval_tree.EventsAt(msec)
 
-  def ToJsonDict(self):
-    return {'events': [e.ToJsonDict() for e in self._events]}
-
   def Filter(self, pid=None, tid=None, categories=None):
     """Returns a new TracingTrack with a subset of the events.
 
@@ -133,8 +133,15 @@ class TracingTrack(devtools_monitor.Track):
           events)
     tracing_track = TracingTrack(None)
     tracing_track._events = events
+    tracing_track._categories = self._categories
+    if categories is not None:
+      tracing_track._categories = self._categories.intersection(categories)
     return tracing_track
 
+  def ToJsonDict(self):
+    return {'categories': list(self._categories),
+            'events': [e.ToJsonDict() for e in self._events]}
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     if not json_dict:
@@ -142,6 +149,7 @@ class TracingTrack(devtools_monitor.Track):
     assert 'events' in json_dict
     events = [Event(e) for e in json_dict['events']]
     tracing_track = TracingTrack(None)
+    tracing_track._categories = set(json_dict.get('categories', []))
     tracing_track._events = events
     tracing_track._base_msec = events[0].start_msec if events else 0
     for e in events[1:]:
@@ -189,6 +197,16 @@ class TracingTrack(devtools_monitor.Track):
         return event
     return None
 
+  def _GetMainFrameID(self):
+    """Returns the main frame ID."""
+    if not self._main_frame_id:
+      navigation_start_events = [e for e in self.GetEvents()
+          if e.Matches('blink.user_timing', 'navigationStart')]
+      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
+      self._main_frame_id = first_event.args['frame']
+
+    return self._main_frame_id
+
   def _IndexEvents(self, strict=False):
     if self._interval_tree:
       return
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 0d7ba27..cce5809 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -42,7 +42,7 @@ class TracingTrackTestCase(unittest.TestCase):
   def setUp(self):
     self.tree_threshold = _IntervalTree._TRESHOLD
     _IntervalTree._TRESHOLD = 2  # Expose more edge cases in the tree.
-    self.track = TracingTrack(None)
+    self.track = TracingTrack(None, additional_categories=('A', 'B', 'C', 'D'))
 
   def tearDown(self):
     _IntervalTree._TRESHOLD = self.tree_threshold
@@ -357,6 +357,18 @@ class TracingTrackTestCase(unittest.TestCase):
     filtered_events = self.track.Filter(categories=set(['B', 'C'])).GetEvents()
     self.assertEquals(3, len(filtered_events))
     self.assertListEqual(tracing_events[1:], filtered_events)
+    self.assertSetEqual(
+        set('A'), self.track.Filter(categories=set('A')).Categories())
+
+  def testAdditionalCategories(self):
+    track = TracingTrack(None, additional_categories=('best-category-ever',))
+    self.assertIn('best-category-ever', track.Categories())
+    # Cannot re-enable a category.
+    with self.assertRaises(AssertionError):
+      TracingTrack(None, additional_categories=('cc',))
+    # Cannot disable categories.
+    with self.assertRaises(AssertionError):
+      TracingTrack(None, additional_categories=('-best-category-ever',))
 
   def _HandleEvents(self, events):
     self.track.Handle('Tracing.dataCollected', {'params': {'value': [

commit c7ec82ca8984405fd9d8ece58c7f2c0bd2bb23e1
Author: shaktisahu <shaktisahu@chromium.org>
Date:   Tue Apr 19 20:05:17 2016 -0700

    Allow Blimp user to choose assigner
    
    Currently the assigner URL is hard-coded. This patch will enable user to select an assigner from a list of assigner URLs. In the About Blimp page, the user can tap on the assigner URL and a list of assigners would show up. Once the new assigner is selected, it is saved to SharedPreference and a dialog is displayed to the user to restart the app. Upon restart, the new assigner takes into effect.
    
    Also fixed the java class path for blimp client in the eclipse .classpath file
    
    BUG=597141
    
    Review URL: https://codereview.chromium.org/1884293002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388416}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 430ff7cb9e5c3ea32dfb4142a28a255d3fdf338f

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 125f04e..a6d7e26 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -24,8 +24,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="base/android/java/src"/>
     <classpathentry kind="src" path="base/android/javatests/src"/>
     <classpathentry kind="src" path="base/test/android/javatests/src"/>
-    <classpathentry kind="src" path="blimp/client/android/java/src"/>
-    <classpathentry kind="src" path="blimp/client/android/javatests/src"/>
+    <classpathentry kind="src" path="blimp/client/app/android/java/src"/>
+    <classpathentry kind="src" path="blimp/client/app/android/javatests/src"/>
     <classpathentry kind="src" path="chrome/android/java/src"/>
     <classpathentry kind="src" path="chrome/android/javatests/src"/>
     <classpathentry kind="src" path="chrome/android/sync_shell/javatests/src"/>

commit a49482eda2e83616805c8f00946a8bc88546b8d3
Author: droger <droger@chromium.org>
Date:   Tue Apr 19 12:10:43 2016 -0700

    tools/android/loading Switch the GCE worker to pull queues
    
    The GCE worker now uses pull queues instead of HTTP requests.
    
    Review URL: https://codereview.chromium.org/1895033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388265}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 56df5e65bb133509403661c090e18617b1910d36

diff --git a/loading/gce/README.md b/loading/cloud/backend/README.md
similarity index 64%
rename from loading/gce/README.md
rename to loading/cloud/backend/README.md
index 2bbd06b..5f534f2 100644
--- a/loading/gce/README.md
+++ b/loading/cloud/backend/README.md
@@ -20,7 +20,7 @@ ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
 # CLOUD_STORAGE_PATH is the path in Google Cloud Storage under which the
 # Clovis deployment will be uploaded.
 
-./tools/android/loading/gce/deploy.sh $BUILD_DIR $CLOUD_STORAGE_PATH
+./tools/android/loading/cloud/backend/deploy.sh $BUILD_DIR $CLOUD_STORAGE_PATH
 ```
 
 ## Start the app in the cloud
@@ -32,15 +32,15 @@ gcloud compute instances create clovis-tracer-1 \
  --machine-type n1-standard-1 \
  --image ubuntu-14-04 \
  --zone europe-west1-c \
- --scopes cloud-platform \
- --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,auto-start=true \
+ --scopes cloud-platform,https://www.googleapis.com/auth/cloud-taskqueue \
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,taskqueue_tag=some_tag \
  --metadata-from-file \
-     startup-script=$CHROMIUM_SRC/tools/android/loading/gce/startup-script.sh
+     startup-script=$CHROMIUM_SRC/tools/android/loading/cloud/backend/startup-script.sh
 ```
 
 **Note:** To start an instance without automatically starting the app on it,
-remove the `--metadata auto-start=true` argument. This can be useful when doing
-iterative development on the instance, to be able to restart the app manually.
+add a `auto-start=false` metadata. This can be useful when doing iterative
+development on the instance, to be able to restart the app manually.
 
 This should output the IP address of the instance.
 Otherwise the IP address can be retrieved by doing:
@@ -60,24 +60,9 @@ gcloud compute instances get-serial-port-output clovis-tracer-1
 
 ## Use the app
 
-Check that `http://<instance-ip>:8080/test` prints `hello` when opened in a
-browser.
-
-To send a list of URLs to process:
-
-```shell
-curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
-```
-
-where `urls.json` is a JSON dictionary with the keys:
-
-*   `urls`: array of URLs
-*   `repeat_count`: Number of times each URL will be loaded. Each load of a URL
-    generates a separate trace file. Optional.
-*   `emulate_device`: Name of the device to emulate. Optional.
-*   `emulate_network`: Type of network emulation. Optional.
-
-You can follow the progress at `http://<instance-ip>:8080/status`.
+Create tasks from the associated AppEngine application, see [documentation][3].
+Make sure the `taskqueue_tag` of the AppEngine request matches the one of the
+ComputeEngine instances.
 
 ## Stop the app in the cloud
 
@@ -98,7 +83,16 @@ From a new directory, set up a local environment:
 ```shell
 virtualenv env
 source env/bin/activate
-pip install -r $CHROMIUM_SRC/tools/android/loading/gce/pip_requirements.txt
+pip install -r \
+    $CHROMIUM_SRC/tools/android/loading/cloud/backend/pip_requirements.txt
+```
+
+The first time, you may need to get more access tokens:
+
+```shell
+gcloud beta auth application-default login --scopes \
+    https://www.googleapis.com/auth/cloud-taskqueue \
+    https://www.googleapis.com/auth/cloud-platform
 ```
 
 Create a JSON file describing the deployment configuration:
@@ -115,7 +109,8 @@ cat >$CONFIG_FILE << EOF
   "project_name" : "$PROJECT_NAME",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "$CHROME_PATH",
-  "src_path" : "$CHROMIUM_SRC"
+  "src_path" : "$CHROMIUM_SRC",
+  "taskqueue_tag" : "some_tag"
 }
 EOF
 ```
@@ -123,12 +118,11 @@ EOF
 Launch the app, passing the path to the deployment configuration file:
 
 ```shell
-gunicorn --workers=1 --bind 127.0.0.1:8080 \
-    --pythonpath $CHROMIUM_SRC/tools/android/loading/gce \
-    'main:StartApp('\"$CONFIG_FILE\"')'
+python $CHROMIUM_SRC/tools/android/loading/cloud/backend/worker.py \
+    --config $CONFIG_FILE
 ```
 
-You can now [use the app][2], which is located at http://localhost:8080.
+You can now [use the app][2].
 
 Tear down the local environment:
 
@@ -138,3 +132,4 @@ deactivate
 
 [1]: https://cloud.google.com/sdk
 [2]: #Use-the-app
+[3]: ../frontend/README.md
diff --git a/loading/gce/deploy.sh b/loading/cloud/backend/deploy.sh
similarity index 87%
rename from loading/gce/deploy.sh
rename to loading/cloud/backend/deploy.sh
index 2362414..cbab438 100755
--- a/loading/gce/deploy.sh
+++ b/loading/cloud/backend/deploy.sh
@@ -21,9 +21,13 @@ src_suffix=src
 tmp_src_dir=$tmpdir/$src_suffix
 
 # Copy files from tools/android/loading.
-mkdir -p $tmp_src_dir/tools/android/loading
+mkdir -p $tmp_src_dir/tools/android/loading/cloud
+cp -r tools/android/loading/cloud/backend \
+  $tmp_src_dir/tools/android/loading/cloud/
+cp -r tools/android/loading/cloud/common \
+  $tmp_src_dir/tools/android/loading/cloud/
 cp tools/android/loading/*.py $tmp_src_dir/tools/android/loading
-cp -r tools/android/loading/gce $tmp_src_dir/tools/android/loading
+cp tools/android/loading/cloud/*.py $tmp_src_dir/tools/android/loading/cloud
 
 # Copy other dependencies.
 mkdir $tmp_src_dir/third_party
diff --git a/loading/gce/google_storage_accessor.py b/loading/cloud/backend/google_storage_accessor.py
similarity index 91%
rename from loading/gce/google_storage_accessor.py
rename to loading/cloud/backend/google_storage_accessor.py
index 59c47da..86e238e 100644
--- a/loading/gce/google_storage_accessor.py
+++ b/loading/cloud/backend/google_storage_accessor.py
@@ -3,17 +3,16 @@
 # found in the LICENSE file.
 
 from gcloud import storage
-from oauth2client.client import GoogleCredentials
 
 
 class GoogleStorageAccessor(object):
   """Utility class providing helpers for Google Cloud Storage.
   """
-  def __init__(self, project_name, bucket_name):
+  def __init__(self, credentials, project_name, bucket_name):
     """project_name is the name of the Google Cloud project.
     bucket_name is the name of the bucket that is used for Cloud Storage calls.
     """
-    self._credentials = GoogleCredentials.get_application_default()
+    self._credentials = credentials
     self._project_name = project_name
     self._bucket_name = bucket_name
 
diff --git a/loading/cloud/backend/pip_requirements.txt b/loading/cloud/backend/pip_requirements.txt
new file mode 100644
index 0000000..390863a
--- /dev/null
+++ b/loading/cloud/backend/pip_requirements.txt
@@ -0,0 +1,3 @@
+gcloud==0.10.1
+google-api-python-client==1.5.0
+psutil==4.1.0
diff --git a/loading/gce/startup-script.sh b/loading/cloud/backend/startup-script.sh
similarity index 85%
rename from loading/gce/startup-script.sh
rename to loading/cloud/backend/startup-script.sh
index 94edb22..e429c1e 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -38,7 +38,7 @@ pip install --upgrade pip virtualenv
 
 # Download the Clovis deployment from Google Cloud Storage and unzip it.
 # It is expected that the contents of the deployment have been generated using
-# the tools/android/loading/gce/deploy.sh script.
+# the tools/android/loading/cloud/backend/deploy.sh script.
 CLOUD_STORAGE_PATH=`get_instance_metadata cloud-storage-path`
 DEPLOYMENT_PATH=$CLOUD_STORAGE_PATH/deployment
 
@@ -49,8 +49,8 @@ rm /opt/app/clovis/source.tgz
 
 # Install app dependencies
 virtualenv /opt/app/clovis/env
-/opt/app/clovis/env/bin/pip install \
-    -r /opt/app/clovis/src/tools/android/loading/gce/pip_requirements.txt
+/opt/app/clovis/env/bin/pip install -r \
+   /opt/app/clovis/src/tools/android/loading/cloud/backend/pip_requirements.txt
 
 mkdir /opt/app/clovis/binaries
 gsutil cp gs://$DEPLOYMENT_PATH/binaries/* /opt/app/clovis/binaries/
@@ -66,12 +66,14 @@ chown -R pythonapp:pythonapp /opt/app
 
 # Create the configuration file for this deployment.
 DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
+TASKQUEUE_TAG=`get_instance_metadata taskqueue_tag`
 cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
-  "src_path" : "/opt/app/clovis/src"
+  "src_path" : "/opt/app/clovis/src",
+  "taskqueue_tag" : "$TASKQUEUE_TAG"
 }
 EOF
 
@@ -79,17 +81,15 @@ EOF
 AUTO_START=`get_instance_metadata auto-start`
 
 # Exit early if auto start is not enabled.
-if [ "$AUTO_START" != "true" ]; then
+if [ "$AUTO_START" == "false" ]; then
   exit 1
 fi
 
-# Configure supervisor to start gunicorn inside of our virtualenv and run the
-# applicaiton.
+# Configure supervisor to start the worker inside of our virtualenv.
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
-directory=/opt/app/clovis/src/tools/android/loading/gce
-command=/opt/app/clovis/env/bin/gunicorn --workers=1 --bind 0.0.0.0:8080 \
-    'main:StartApp('\"$DEPLOYMENT_CONFIG_PATH\"')'
+directory=/opt/app/clovis/src/tools/android/loading/cloud/backend
+command=python worker.py --config $DEPLOYMENT_CONFIG_PATH
 autostart=true
 autorestart=true
 user=pythonapp
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
new file mode 100644
index 0000000..b9900ce
--- /dev/null
+++ b/loading/cloud/backend/worker.py
@@ -0,0 +1,254 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import json
+import logging
+import os
+import re
+import sys
+import time
+
+from googleapiclient import discovery
+from oauth2client.client import GoogleCredentials
+
+# NOTE: The parent directory needs to be first in sys.path to avoid conflicts
+# with catapult modules that have colliding names, as catapult inserts itself
+# into the path as the second element. This is an ugly and fragile hack.
+sys.path.insert(0,
+    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir,
+                 os.pardir))
+import controller
+from cloud.common.clovis_task import ClovisTask
+from google_storage_accessor import GoogleStorageAccessor
+import loading_trace
+from loading_trace_database import LoadingTraceDatabase
+import options
+
+
+class Worker(object):
+  def __init__(self, config, logger):
+    """See README.md for the config format."""
+    self._project_name = config['project_name']
+    self._taskqueue_tag = config['taskqueue_tag']
+    self._credentials = GoogleCredentials.get_application_default()
+    self._logger = logger
+
+    # Separate the cloud storage path into the bucket and the base path under
+    # the bucket.
+    storage_path_components = config['cloud_storage_path'].split('/')
+    self._bucket_name = storage_path_components[0]
+    self._base_path_in_bucket = ''
+    if len(storage_path_components) > 1:
+      self._base_path_in_bucket = '/'.join(storage_path_components[1:])
+      if not self._base_path_in_bucket.endswith('/'):
+        self._base_path_in_bucket += '/'
+
+    # TODO: improve the trace database to support concurrent access.
+    self._traces_dir = self._base_path_in_bucket + 'traces/'
+    self._trace_database = LoadingTraceDatabase({})
+
+    self._src_path = config['src_path']
+    self._google_storage_accessor = GoogleStorageAccessor(
+        credentials=self._credentials, project_name=self._project_name,
+        bucket_name=self._bucket_name)
+
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs([])
+    options.OPTIONS.local_binary = config['chrome_path']
+
+  def Start(self):
+    """Main worker loop.
+
+    Repeatedly pulls tasks from the task queue and processes them. Returns when
+    the queue is empty.
+    """
+    task_api = discovery.build('taskqueue', 'v1beta2',
+                               credentials=self._credentials)
+    queue_name = 'clovis-queue'
+    # Workaround for
+    # https://code.google.com/p/googleappengine/issues/detail?id=10199
+    project = 's~' + self._project_name
+
+    while True:
+      self._logger.debug('Fetching new task.')
+      (clovis_task, task_id) = self._FetchClovisTask(project, task_api,
+                                                     queue_name)
+      if not clovis_task:
+        if self._trace_database.ToJsonDict():
+          self._logger.info('No remaining tasks in the queue.')
+          break
+        else:
+          delay_seconds = 60
+          self._logger.info(
+              'Nothing in the queue, retrying in %i seconds.' % delay_seconds)
+          time.sleep(delay_seconds)
+          continue
+
+      self._logger.info('Processing task %s' % task_id)
+      self._ProcessClovisTask(clovis_task)
+      self._logger.debug('Deleting task %s' % task_id)
+      task_api.tasks().delete(project=project, taskqueue=queue_name,
+                              task=task_id).execute()
+      self._logger.info('Finished task %s' % task_id)
+    self._Finalize()
+
+  def _FetchClovisTask(self, project_name, task_api, queue_name):
+    """Fetches a ClovisTask from the task queue.
+
+    Params:
+      project_name(str): The name of the Google Cloud project.
+      task_api: The TaskQueue service.
+      queue_name(str): The name of the task queue.
+
+    Returns:
+      (ClovisTask, str): The fetched ClovisTask and its task ID, or (None, None)
+                         if no tasks are found.
+    """
+    response = task_api.tasks().lease(
+        project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=180,
+        groupByTag=True, tag=self._taskqueue_tag).execute()
+    if (not response.get('items')) or (len(response['items']) < 1):
+      return (None, None)
+
+    google_task = response['items'][0]
+    task_id = google_task['id']
+    clovis_task = ClovisTask.FromBase64(google_task['payloadBase64'])
+    return (clovis_task, task_id)
+
+  def _Finalize(self):
+    """Called before exiting."""
+    self._logger.info('Uploading trace database')
+    self._google_storage_accessor.UploadString(
+        json.dumps(self._trace_database.ToJsonDict(), indent=2),
+        self._traces_dir + 'trace_database.json')
+    # TODO(droger): Implement automatic instance destruction.
+    self._logger.info('Done')
+
+
+  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
+                     log_filename):
+    """ Generates a trace.
+
+    Args:
+      url: URL as a string.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
+      filename: Name of the file where the trace is saved.
+      log_filename: Name of the file where standard output and errors are
+                    logged.
+
+    Returns:
+      A dictionary of metadata about the trace, including a 'succeeded' field
+      indicating whether the trace was successfully generated.
+    """
+    try:
+      os.remove(filename)  # Remove any existing trace for this URL.
+    except OSError:
+      pass  # Nothing to remove.
+
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
+
+    old_stdout = sys.stdout
+    old_stderr = sys.stderr
+
+    trace_metadata = { 'succeeded' : False, 'url' : url }
+    trace = None
+    with open(log_filename, 'w') as sys.stdout:
+      try:
+        sys.stderr = sys.stdout
+
+        # Set up the controller.
+        chrome_ctl = controller.LocalChromeController()
+        chrome_ctl.SetHeadless(True)
+        if emulate_device:
+          chrome_ctl.SetDeviceEmulation(emulate_device)
+        if emulate_network:
+          chrome_ctl.SetNetworkEmulation(emulate_network)
+
+        # Record and write the trace.
+        with chrome_ctl.OpenWithRedirection(sys.stdout,
+                                            sys.stderr) as connection:
+          connection.ClearCache()
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url, connection, chrome_ctl.ChromeMetadata())
+          trace_metadata['succeeded'] = True
+          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+      except Exception as e:
+        sys.stderr.write(str(e))
+
+      if trace:
+        with open(filename, 'w') as f:
+          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+
+    sys.stdout = old_stdout
+    sys.stderr = old_stderr
+
+    return trace_metadata
+
+  def _ProcessClovisTask(self, clovis_task):
+    """Processes one clovis_task."""
+    if clovis_task.Action() != 'trace':
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      return
+
+    # Extract the task parameters.
+    params = clovis_task.Params()
+    urls = params['urls']
+    repeat_count = params.get('repeat_count', 1)
+    emulate_device = params.get('emulate_device')
+    emulate_network = params.get('emulate_network')
+
+    failures_dir = self._base_path_in_bucket + 'failures/'
+    # TODO(blundell): Fix this up.
+    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
+    log_filename = 'analyze.log'
+    # Avoid special characters in storage object names
+    pattern = re.compile(r"[#\?\[\]\*/]")
+
+    while len(urls) > 0:
+      url = urls.pop()
+      local_filename = pattern.sub('_', url)
+      for repeat in range(repeat_count):
+        self._logger.debug('Generating trace for URL: %s' % url)
+        remote_filename = local_filename + '/' + str(repeat)
+        trace_metadata = self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename)
+        if trace_metadata['succeeded']:
+          self._logger.debug('Uploading: %s' % remote_filename)
+          remote_trace_location = self._traces_dir + remote_filename
+          self._google_storage_accessor.UploadFile(local_filename,
+                                                   remote_trace_location)
+          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
+              remote_trace_location)
+          self._trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
+        else:
+          self._logger.warning('Trace generation failed for URL: %s' % url)
+          # TODO: upload the failure
+          if os.path.isfile(local_filename):
+            self._google_storage_accessor.UploadFile(local_filename,
+                                            failures_dir + remote_filename)
+            self._logger.debug('Uploading log')
+        self._google_storage_accessor.UploadFile(log_filename,
+                                                 logs_dir + remote_filename)
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser(
+      description='ComputeEngine Worker for Clovis')
+  parser.add_argument('--config', required=True,
+                      help='Path to the configuration file.')
+  args = parser.parse_args()
+
+  # Configure logging.
+  logging.basicConfig(level=logging.WARNING)
+  worker_logger = logging.getLogger('worker')
+  worker_logger.setLevel(logging.INFO)
+
+  worker_logger.info('Reading configuration')
+  with open(args.config) as config_json:
+    worker = Worker(json.load(config_json), worker_logger)
+    worker.Start()
+
diff --git a/loading/cloud/frontend/lib/common b/loading/cloud/frontend/lib/common
new file mode 120000
index 0000000..dc879ab
--- /dev/null
+++ b/loading/cloud/frontend/lib/common
@@ -0,0 +1 @@
+../../common
\ No newline at end of file
diff --git a/loading/gce/main.py b/loading/gce/main.py
deleted file mode 100644
index 0ef2cae..0000000
--- a/loading/gce/main.py
+++ /dev/null
@@ -1,280 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import json
-import os
-import re
-import threading
-import time
-import subprocess
-import sys
-
-# NOTE: The parent directory needs to be first in sys.path to avoid conflicts
-# with catapult modules that have colliding names, as catapult inserts itself
-# into the path as the second element. This is an ugly and fragile hack.
-sys.path.insert(0,
-    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
-import controller
-from google_storage_accessor import GoogleStorageAccessor
-import loading_trace
-from loading_trace_database import LoadingTraceDatabase
-import options
-
-
-class ServerApp(object):
-  """Simple web server application, collecting traces and writing them in
-  Google Cloud Storage.
-  """
-
-  def __init__(self, configuration_file):
-    """|configuration_file| is a path to a file containing JSON as described in
-    README.md.
-    """
-    self._tasks = []  # List of remaining tasks, only modified by _thread.
-    self._failed_tasks = []  # Failed tasks, only modified by _thread.
-    self._thread = None
-    self._tasks_lock = threading.Lock()  # Protects _tasks and _failed_tasks.
-    self._initial_task_count = -1
-    self._start_time = None
-    print 'Reading configuration'
-    with open(configuration_file) as config_json:
-       config = json.load(config_json)
-
-       # Separate the cloud storage path into the bucket and the base path under
-       # the bucket.
-       storage_path_components = config['cloud_storage_path'].split('/')
-       self._bucket_name = storage_path_components[0]
-       self._base_path_in_bucket = ''
-       if len(storage_path_components) > 1:
-         self._base_path_in_bucket = '/'.join(storage_path_components[1:])
-         if not self._base_path_in_bucket.endswith('/'):
-           self._base_path_in_bucket += '/'
-
-       self._src_path = config['src_path']
-       self._google_storage_accessor = GoogleStorageAccessor(
-           project_name=config['project_name'], bucket_name=self._bucket_name)
-
-    # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs([])
-    options.OPTIONS.local_binary = config['chrome_path']
-
-  def _IsProcessingTasks(self):
-    """Returns True if the application is currently processing tasks."""
-    return self._thread is not None and self._thread.is_alive()
-
-  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
-                     log_filename):
-    """ Generates a trace on _thread.
-
-    Args:
-      url: URL as a string.
-      emulate_device: Name of the device to emulate. Empty for no emulation.
-      emulate_network: Type of network emulation. Empty for no emulation.
-      filename: Name of the file where the trace is saved.
-      log_filename: Name of the file where standard output and errors are logged
-
-    Returns:
-      A dictionary of metadata about the trace, including a 'succeeded' field
-      indicating whether the trace was successfully generated.
-    """
-    try:
-      os.remove(filename)  # Remove any existing trace for this URL.
-    except OSError:
-      pass  # Nothing to remove.
-
-    if not url.startswith('http') and not url.startswith('file'):
-      url = 'http://' + url
-
-    old_stdout = sys.stdout
-    old_stderr = sys.stderr
-
-    trace_metadata = { 'succeeded' : False, 'url' : url }
-    trace = None
-    with open(log_filename, 'w') as sys.stdout:
-      try:
-        sys.stderr = sys.stdout
-
-        # Set up the controller.
-        chrome_ctl = controller.LocalChromeController()
-        chrome_ctl.SetHeadless(True)
-        if emulate_device:
-          chrome_ctl.SetDeviceEmulation(emulate_device)
-        if emulate_network:
-          chrome_ctl.SetNetworkEmulation(emulate_network)
-
-        # Record and write the trace.
-        with chrome_ctl.OpenWithRedirection(sys.stdout,
-                                            sys.stderr) as connection:
-          connection.ClearCache()
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url, connection, chrome_ctl.ChromeMetadata())
-          trace_metadata['succeeded'] = True
-          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
-      except Exception as e:
-        sys.stderr.write(str(e))
-
-      if trace:
-        with open(filename, 'w') as f:
-          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
-
-    sys.stdout = old_stdout
-    sys.stderr = old_stderr
-
-    return trace_metadata
-
-  def _GetCurrentTaskCount(self):
-    """Returns the number of remaining tasks. Thread safe."""
-    self._tasks_lock.acquire()
-    task_count = len(self._tasks)
-    self._tasks_lock.release()
-    return task_count
-
-  def _ProcessTasks(self, tasks, repeat_count, emulate_device, emulate_network):
-    """Iterates over _task, generating a trace for each of them. Uploads the
-    resulting traces to Google Cloud Storage.  Runs on _thread.
-
-    Args:
-      tasks: The list of URLs to process.
-      repeat_count: The number of traces generated for each URL.
-      emulate_device: Name of the device to emulate. Empty for no emulation.
-      emulate_network: Type of network emulation. Empty for no emulation.
-    """
-    # The main thread might be reading the task lists, take the lock to modify.
-    self._tasks_lock.acquire()
-    self._tasks = tasks
-    self._failed_tasks = []
-    self._tasks_lock.release()
-    failures_dir = self._base_path_in_bucket + 'failures/'
-    traces_dir = self._base_path_in_bucket + 'traces/'
-
-    trace_database = LoadingTraceDatabase({})
-
-    # TODO(blundell): Fix this up.
-    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
-    log_filename = 'analyze.log'
-    # Avoid special characters in storage object names
-    pattern = re.compile(r"[#\?\[\]\*/]")
-    while len(self._tasks) > 0:
-      url = self._tasks[-1]
-      local_filename = pattern.sub('_', url)
-      for repeat in range(repeat_count):
-        print 'Generating trace for URL: %s' % url
-        remote_filename = local_filename + '/' + str(repeat)
-        trace_metadata = self._GenerateTrace(
-            url, emulate_device, emulate_network, local_filename, log_filename)
-        if trace_metadata['succeeded']:
-          print 'Uploading: %s' % remote_filename
-          remote_trace_location = traces_dir + remote_filename
-          self._google_storage_accessor.UploadFile(local_filename,
-                                           remote_trace_location)
-          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
-              remote_trace_location)
-          trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
-        else:
-          print 'Trace generation failed for URL: %s' % url
-          self._tasks_lock.acquire()
-          self._failed_tasks.append({ "url": url, "repeat": repeat})
-          self._tasks_lock.release()
-          if os.path.isfile(local_filename):
-            self._google_storage_accessor.UploadFile(local_filename,
-                                            failures_dir + remote_filename)
-        print 'Uploading log'
-        self._google_storage_accessor.UploadFile(log_filename,
-                                         logs_dir + remote_filename)
-      # Pop once task is finished, for accurate status tracking.
-      self._tasks_lock.acquire()
-      url = self._tasks.pop()
-      self._tasks_lock.release()
-
-    self._google_storage_accessor.UploadString(
-        json.dumps(trace_database.ToJsonDict(), indent=2),
-        traces_dir + 'trace_database.json')
-
-    if len(self._failed_tasks) > 0:
-      print 'Uploading failing URLs'
-      self._google_storage_accessor.UploadString(
-          json.dumps(self._failed_tasks, indent=2),
-          failures_dir + 'failures.json')
-
-  def _SetTaskList(self, http_body):
-    """Sets the list of tasks and starts processing them
-
-    Args:
-      http_body: JSON dictionary. See README.md for a description of the format.
-
-    Returns:
-      A string to be sent back to the client, describing the success status of
-      the request.
-    """
-    if self._IsProcessingTasks():
-      return 'Error: Already running\n'
-
-    load_parameters = json.loads(http_body)
-    try:
-      tasks = load_parameters['urls']
-    except KeyError:
-      return 'Error: invalid urls\n'
-    # Optional parameters.
-    try:
-      repeat_count = int(load_parameters.get('repeat_count', '1'))
-    except ValueError:
-      return 'Error: invalid repeat_count\n'
-    emulate_device = load_parameters.get('emulate_device', '')
-    emulate_network = load_parameters.get('emulate_network', '')
-
-    if len(tasks) == 0:
-      return 'Error: Empty task list\n'
-    else:
-      self._initial_task_count = len(tasks)
-      self._start_time = time.time()
-      self._thread = threading.Thread(
-          target = self._ProcessTasks,
-          args = (tasks, repeat_count, emulate_device, emulate_network))
-      self._thread.start()
-      return 'Starting generation of %s tasks\n' % str(self._initial_task_count)
-
-  def __call__(self, environ, start_response):
-    path = environ['PATH_INFO']
-
-    if path == '/set_tasks':
-      # Get the tasks from the HTTP body.
-      try:
-        body_size = int(environ.get('CONTENT_LENGTH', 0))
-      except (ValueError):
-        body_size = 0
-      body = environ['wsgi.input'].read(body_size)
-      data = self._SetTaskList(body)
-    elif path == '/test':
-      data = 'hello\n'
-    elif path == '/status':
-      if not self._IsProcessingTasks():
-        data = 'Idle\n'
-      else:
-        task_count = self._GetCurrentTaskCount()
-        if task_count == 0:
-          data = '%s tasks complete. Finalizing.\n' % self._initial_task_count
-        else:
-          data = 'Remaining tasks: %s / %s\n' % (
-              task_count, self._initial_task_count)
-        elapsed = time.time() - self._start_time
-        data += 'Elapsed time: %s seconds\n' % str(elapsed)
-        self._tasks_lock.acquire()
-        failed_tasks = self._failed_tasks
-        self._tasks_lock.release()
-        data += '%s failed tasks:\n' % len(failed_tasks)
-        data += json.dumps(failed_tasks, indent=2)
-    else:
-      start_response('404 NOT FOUND', [('Content-Length', '0')])
-      return iter([''])
-
-    response_headers = [
-        ('Content-type','text/plain'),
-        ('Content-Length', str(len(data)))
-    ]
-    start_response('200 OK', response_headers)
-    return iter([data])
-
-
-def StartApp(configuration_file):
-  return ServerApp(configuration_file)
diff --git a/loading/gce/pip_requirements.txt b/loading/gce/pip_requirements.txt
deleted file mode 100644
index 7d97e12..0000000
--- a/loading/gce/pip_requirements.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-gunicorn==19.4.5
-gcloud==0.10.1
-psutil==4.1.0

commit 3c3a0aacf519579124717d2d076d4027b8eef718
Author: agrieve <agrieve@chromium.org>
Date:   Tue Apr 19 10:33:02 2016 -0700

    Fix various findbugs warnings found when building "all" with GN
    
    TBR=mikecase
    BUG=604456
    
    Review URL: https://codereview.chromium.org/1903473003
    
    Cr-Original-Commit-Position: refs/heads/master@{#388234}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 794f776af830155360e6a24f575b6f9f1e763bec

diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
index 7030b8f..d8e3696 100644
--- a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
@@ -38,6 +38,8 @@ public class AudioFocusGrabberActivity extends Activity {
             case R.id.button_hide_notification:
                 intent.setAction(AudioFocusGrabberListenerService.ACTION_HIDE_NOTIFICATION);
                 break;
+            default:
+                break;
         }
         startService(intent);
     }
diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
index 9981fd8..44b9806 100644
--- a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
@@ -144,6 +144,8 @@ public class AudioFocusGrabberListenerService extends Service {
                             mMediaPlayer.setVolume(0.1f, 0.1f);
                             mIsDucking = true;
                             break;
+                        default:
+                            break;
                     }
                 }
             };

commit 700de317cc9b79ee4cfa9cf7e8b69c4768b50450
Author: droger <droger@chromium.org>
Date:   Tue Apr 19 10:23:18 2016 -0700

    tools/android/loading Add appengine frontend for Clovis
    
    Review URL: https://codereview.chromium.org/1878943013
    
    Cr-Original-Commit-Position: refs/heads/master@{#388229}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0ee8eea8d5648db41fecddac32edca241d00fa44

diff --git a/loading/PRESUBMIT.py b/loading/PRESUBMIT.py
index b9bcc19..cd740ef 100644
--- a/loading/PRESUBMIT.py
+++ b/loading/PRESUBMIT.py
@@ -11,7 +11,7 @@ for more details on the presubmit API built into depot_tools.
 
 def CommonChecks(input_api, output_api):
   output = []
-  blacklist = []
+  blacklist = [r'cloud/frontend/lib/*']
   output.extend(input_api.canned_checks.RunPylint(
       input_api, output_api, black_list=blacklist))
   output.extend(input_api.canned_checks.RunUnitTests(
diff --git a/loading/cloud/__init__.py b/loading/cloud/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/loading/cloud/common/__init__.py b/loading/cloud/common/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
new file mode 100644
index 0000000..8bc3127
--- /dev/null
+++ b/loading/cloud/common/clovis_task.py
@@ -0,0 +1,66 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import base64
+import json
+
+class ClovisTask(object):
+  """Generic task, generated by the AppEngine frontend and consumed by the
+  ComputeEngine backend.
+  """
+
+  def __init__(self, action, params, taskqueue_tag):
+    """Params:
+      action(str): Action accomplished by this task.
+      params(dict): Parameters of task.
+      taskqueue_tag(str): Tag of the task. Optional.
+    """
+    self._action = action
+    self._params = params
+    self._taskqueue_tag = taskqueue_tag
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Loads a ClovisTask from a JSON string.
+
+    Returns:
+      ClovisTask: The task, or None if the string is invalid.
+    """
+    try:
+      data = json.loads(json_dict)
+      action = data['action']
+      params = data['params']
+      tag = data.get('taskqueue_tag')
+      # Vaidate the format.
+      if action == 'trace':
+        urls = params['urls']
+        if (type(urls) is not list) or (len(urls) == 0):
+          return None
+      else:
+        # When more actions are supported, check that they are valid here.
+        return None
+      return cls(action, params, tag)
+    except Exception:
+      return None
+
+  @classmethod
+  def FromBase64(cls, base64_string):
+    """Loads a ClovisTask from a base 64 string."""
+    return ClovisTask.FromJsonDict(base64.b64decode(base64_string))
+
+  def ToJsonDict(self):
+    """Returns the JSON representation of the task."""
+    task_dict = { 'action': self._action, 'params': self._params }
+    if self._taskqueue_tag:
+      task_dict['taskqueue_tag'] = self._taskqueue_tag
+    return json.dumps(task_dict)
+
+  def Action(self):
+    return self._action
+
+  def Params(self):
+    return self._params
+
+  def TaskqueueTag(self):
+    return self._taskqueue_tag
diff --git a/loading/cloud/frontend/.gitignore b/loading/cloud/frontend/.gitignore
new file mode 100644
index 0000000..c3af857
--- /dev/null
+++ b/loading/cloud/frontend/.gitignore
@@ -0,0 +1 @@
+lib/
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
new file mode 100644
index 0000000..f6f6f5f
--- /dev/null
+++ b/loading/cloud/frontend/README.md
@@ -0,0 +1,91 @@
+# Appengine Frontend for Clovis
+
+[TOC]
+
+## Usage
+
+Visit the application URL in your browser, and upload a JSON dictionary with the
+following keys:
+
+-   `action` (string): the action to perform. Only `trace` is supported.
+-   `params` (dictionary): the parameters associated to the action. See below
+    for more details.
+-   `taskqueue_tag` (string, optional): the [TaskQueue][2] tag internally used
+    to send the work from AppEngine to ComputeEngine.  If this parameter is not
+    specified, a unique tag will be created.
+
+### Parameters for the `trace` action.
+
+-   `urls` (list of strings): the list of URLs to process.
+-   `repeat_count` (integer, optional): the number of traces to be generated
+    for each URL. Defaults to 1.
+-   `emulate_device` (string, optional): the device to emulate (e.g. `Nexus 4`).
+-   `emulate_network` (string, optional): the network to emulate.
+
+## Development
+
+### Design overview
+
+-   Appengine configuration:
+    -   `app.yaml` defines the handlers. There is a static handler for all URLs
+    in the `static/` directory, and all other URLs are handled by the
+    `clovis_frontend.py` script.
+    -   `queue.yaml` defines the task queues associated with the application. In
+        particular, the `clovis-queue` is a pull-queue where tasks are added by
+        the AppEngine frontend and consummed by the ComputeEngine backend.
+        See the [TaskQueue documentation][2] for more details.
+-   `static/form.html` is a static HTML document allowing the user to upload a
+    JSON file. `clovis_frontend.py` is then invoked with the contents of the
+    file (see the `/form_sent` handler).
+-   `clovis_task.py` defines a task to be run by the backend. It is sent through
+    the `clovis-queue` task queue.
+-   `clovis_frontend.py` is the script that processes the file uploaded by the
+    form, creates the tasks and enqueues them in `clovis-queue`.
+
+### Prerequisites
+
+-   Install the gcloud [tool][1]
+-   Add a `queue.yaml` file in the application directory (i.e. next to
+    `app.yaml`) defining a `clovis-queue` pull queue that can be accessed by the
+    ComputeEngine service worker associated to the project. Add your email too
+    if you want to run the application locally. See the [TaskQueue configuration
+    documentation][3] for more details. Example:
+
+```
+# queue.yaml
+- name: clovis-queue
+  mode: pull
+  acl:
+    - user_email: me@address.com
+    - user_email: 123456789-compute@developer.gserviceaccount.com
+```
+
+### Run Locally
+
+```shell
+# Install dependencies in the lib/ directory. Note that this will pollute your
+# Chromium checkout, see the cleanup intructions below.
+pip install -r requirements.txt -t lib
+# Start the local server.
+dev_appserver.py .
+```
+
+Visit the application [http://localhost:8080](http://localhost:8080).
+
+After you are done, cleanup your Chromium checkout:
+```shell
+rm -rf $CHROMIUM_SRC/tools/android/loading/frontend/lib
+```
+
+### Deploy
+
+````shell
+# Install dependencies in the lib/ directory.
+pip install -r requirements.txt -t lib
+# Deploy.
+gcloud preview app deploy app.yaml
+```
+
+[1]: https://cloud.google.com/sdk
+[2]: https://cloud.google.com/appengine/docs/python/taskqueue
+[3]: https://cloud.google.com/appengine/docs/python/config/queue
diff --git a/loading/cloud/frontend/app.yaml b/loading/cloud/frontend/app.yaml
new file mode 100644
index 0000000..7dcc1d6
--- /dev/null
+++ b/loading/cloud/frontend/app.yaml
@@ -0,0 +1,10 @@
+runtime: python27
+api_version: 1
+threadsafe: yes
+
+handlers:
+- url: /static
+  static_dir: static
+
+- url: .*
+  script: clovis_frontend.app
diff --git a/loading/cloud/frontend/appengine_config.py b/loading/cloud/frontend/appengine_config.py
new file mode 100644
index 0000000..608cd73
--- /dev/null
+++ b/loading/cloud/frontend/appengine_config.py
@@ -0,0 +1,6 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from google.appengine.ext import vendor
+vendor.add('lib')
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
new file mode 100644
index 0000000..8eeee35
--- /dev/null
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -0,0 +1,109 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import flask
+from google.appengine.api import taskqueue
+import json
+import os
+import sys
+import uuid
+
+from common.clovis_task import ClovisTask
+
+
+app = flask.Flask(__name__)
+
+
+def StartFromJson(http_body_str):
+  """Creates a new batch of tasks from its JSON representation."""
+  task = ClovisTask.FromJsonDict(http_body_str)
+  if not task:
+    return 'Invalid JSON task:\n%s\n' % http_body_str
+
+  task_tag = task.TaskqueueTag()
+  if not task_tag:
+    task_tag = uuid.uuid1()
+
+  sub_tasks = []
+  if task.Action() == 'trace':
+    sub_tasks = SplitTraceTask(task)
+  else:
+    return 'Unsupported action: %s\n' % task.Action()
+
+  return EnqueueTasks(sub_tasks, task_tag)
+
+
+def SplitTraceTask(task):
+  """Split a tracing task with potentially many URLs into several tracing tasks
+  with few URLs.
+  """
+  params = task.Params()
+  urls = params['urls']
+
+  # Split the task in smaller tasks with fewer URLs each.
+  urls_per_task = 1
+  sub_tasks = []
+  for i in range(0, len(urls), urls_per_task):
+    sub_task_params = params.copy()
+    sub_task_params['urls'] = [url for url in urls[i:i+urls_per_task]]
+    sub_tasks.append(ClovisTask(task.Action(), sub_task_params,
+                                task.TaskqueueTag()))
+  return sub_tasks
+
+
+def EnqueueTasks(tasks, task_tag):
+  """Enqueues a list of tasks in the Google Cloud task queue, for consumption by
+  Google Compute Engine.
+  """
+  q = taskqueue.Queue('clovis-queue')
+  retry_options = taskqueue.TaskRetryOptions(task_retry_limit=3)
+  # Add tasks to the queue by groups.
+  # TODO(droger): This support to thousands of tasks, but maybe not millions.
+  # Defer the enqueuing if it times out.
+  # is too large.
+  group_size = 100
+  callbacks = []
+  try:
+    for i in range(0, len(tasks), group_size):
+      group = tasks[i:i+group_size]
+      taskqueue_tasks = [
+          taskqueue.Task(payload=task.ToJsonDict(), method='PULL', tag=task_tag,
+                         retry_options=retry_options)
+          for task in group]
+      rpc = taskqueue.create_rpc()
+      q.add_async(task=taskqueue_tasks, rpc=rpc)
+      callbacks.append(rpc)
+    for callback in callbacks:
+      callback.get_result()
+  except Exception as e:
+    return 'Exception:' + type(e).__name__ + ' ' + str(e.args) + '\n'
+  return 'pushed %i tasks with tag: %s\n' % (len(tasks), task_tag)
+
+
+@app.route('/')
+def Root():
+  """Home page: redirect to the static form."""
+  return flask.redirect('/static/form.html')
+
+
+@app.route('/form_sent', methods=['POST'])
+def StartFromForm():
+  """HTML form endpoint"""
+  data_stream = flask.request.files.get('json_task')
+  if not data_stream:
+    return 'failed'
+  http_body_str = data_stream.read()
+  return StartFromJson(http_body_str)
+
+
+@app.errorhandler(404)
+def PageNotFound(e):  # pylint: disable=unused-argument
+  """Return a custom 404 error."""
+  return 'Sorry, Nothing at this URL.', 404
+
+
+@app.errorhandler(500)
+def ApplicationError(e):
+  """Return a custom 500 error."""
+  return 'Sorry, unexpected error: {}'.format(e), 499
diff --git a/loading/cloud/frontend/common b/loading/cloud/frontend/common
new file mode 120000
index 0000000..60d3b0a
--- /dev/null
+++ b/loading/cloud/frontend/common
@@ -0,0 +1 @@
+../common
\ No newline at end of file
diff --git a/loading/cloud/frontend/requirements.txt b/loading/cloud/frontend/requirements.txt
new file mode 100644
index 0000000..880a7bc
--- /dev/null
+++ b/loading/cloud/frontend/requirements.txt
@@ -0,0 +1 @@
+Flask==0.10
diff --git a/loading/cloud/frontend/static/form.html b/loading/cloud/frontend/static/form.html
new file mode 100644
index 0000000..927cf2b
--- /dev/null
+++ b/loading/cloud/frontend/static/form.html
@@ -0,0 +1,17 @@
+<!DOCTYPE html>
+<html>
+
+<head>
+<meta charset="utf-8">
+<title>Submmit</title>
+</head>
+
+<body>
+<p> Select JSON file </p>
+<form action="/form_sent" method="POST" enctype="multipart/form-data">
+<input type="file" name="json_task"/>
+<input type="submit" name="submit" value="Upload"/>
+</form>
+</body>
+
+</html>

commit a79cac683210973f39ab62dfb89e88799b803f7d
Author: blundell <blundell@chromium.org>
Date:   Tue Apr 19 07:54:52 2016 -0700

    tools/android/loading: Add ability to match only main frame events
    
    This CL adds the ability to get tracing events that occurred in the
    main frame (as opposed to in any frame). This functionality works for
    events whose 'args' have a 'frame' key (e.g.,
    'blink.user_timing.firstContentfulPaint'). The ID of the main frame is
    defined to be that of the earliest 'blink.user_timing.navigationStart'
    event in the trace.
    
    This functionality will be used to e.g. find the first contentful paint
    of the main frame.
    
    Review URL: https://codereview.chromium.org/1899843004
    
    Cr-Original-Commit-Position: refs/heads/master@{#388206}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8faf66ee0a7dd136ab4ec28b0821b1ec5c57a26c

diff --git a/loading/tracing.py b/loading/tracing.py
index c8db1df..4891d3e 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -50,6 +50,7 @@ class TracingTrack(devtools_monitor.Track):
     self._events = []
     self._base_msec = None
     self._interval_tree = None
+    self._main_frame_id = None
 
   def Handle(self, method, event):
     for e in event['params']['value']:
@@ -61,6 +62,16 @@ class TracingTrack(devtools_monitor.Track):
     # update.
     self._interval_tree = None
 
+  def _GetMainFrameID(self):
+    """Returns the main frame ID."""
+    if not self._main_frame_id:
+      navigation_start_events = [e for e in self.GetEvents()
+          if e.Matches('blink.user_timing', 'navigationStart')]
+      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
+      self._main_frame_id = first_event.args['frame']
+
+    return self._main_frame_id
+
   def GetFirstEventMillis(self):
     """Find the canonical start time for this track.
 
@@ -72,6 +83,17 @@ class TracingTrack(devtools_monitor.Track):
   def GetEvents(self):
     return self._events
 
+  def GetMatchingEvents(self, category, name):
+    """Gets events matching |category| and |name|."""
+    return [e for e in self.GetEvents() if e.Matches(category, name)]
+
+  def GetMatchingMainFrameEvents(self, category, name):
+    """Gets events matching |category| and |name| that occur in the main frame.
+    Assumes that the events in question have a 'frame' key in their |args|."""
+    matching_events = self.GetMatchingEvents(category, name)
+    return [e for e in matching_events
+        if e.args['frame'] == self._GetMainFrameID()]
+
   def EventsAt(self, msec):
     """Gets events active at a timestamp.
 
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 1bf0273..0d7ba27 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -295,6 +295,48 @@ class TracingTrackTestCase(unittest.TestCase):
     tracing_track = self.track.Filter(2, 42)
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
+  def testGetMainFrameID(self):
+    _MAIN_FRAME_ID = 0xffff
+    _SUBFRAME_ID = 0xaaaa
+    events = [
+        {'ts': 7, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x123',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _SUBFRAME_ID}},
+        {'ts': 8, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 1, 'id': '0x12343',
+        'name': 'A'},
+        {'ts': 3, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x125',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _MAIN_FRAME_ID}},
+        ]
+    self._HandleEvents(events)
+    self.assertEquals(_MAIN_FRAME_ID, self.track._GetMainFrameID())
+
+  def testGetMatchingEvents(self):
+    _MAIN_FRAME_ID = 0xffff
+    _SUBFRAME_ID = 0xaaaa
+    events = [
+        {'ts': 7, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x123',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _SUBFRAME_ID}},
+        {'ts': 8, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 1, 'id': '0x12343',
+        'name': 'A'},
+        {'ts': 3, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x125',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _MAIN_FRAME_ID}},
+        ]
+    self._HandleEvents(events)
+    matching_events = self.track.GetMatchingEvents('blink.user_timing',
+                                                   'navigationStart')
+    self.assertEquals(2, len(matching_events))
+    self.assertListEqual([self.track.GetEvents()[0],
+                         self.track.GetEvents()[2]], matching_events)
+
+    matching_main_frame_events = self.track.GetMatchingMainFrameEvents(
+        'blink.user_timing', 'navigationStart')
+    self.assertEquals(1, len(matching_main_frame_events))
+    self.assertListEqual([self.track.GetEvents()[2]],
+                         matching_main_frame_events)
+
   def testFilterCategories(self):
     events = [
         {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'A'},

commit a452684c7bda908ebf10e58769ea1e773982d291
Author: blundell <blundell@chromium.org>
Date:   Mon Apr 18 04:31:55 2016 -0700

    Add ability to execute loading graph view visualization
    
    Patch originally by Matt Cary (mattcary@chromium.org).
    
    Review URL: https://codereview.chromium.org/1900573002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387891}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 386a6486443ce0d5b6641fa6dc7f2b0624d69b7d

diff --git a/loading/loading_graph_view_visualization.py b/loading/loading_graph_view_visualization.py
index d083575..7a89f6b 100644
--- a/loading/loading_graph_view_visualization.py
+++ b/loading/loading_graph_view_visualization.py
@@ -2,8 +2,12 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Visualize a loading_graph_view.LoadingGraphView."""
+"""Visualize a loading_graph_view.LoadingGraphView.
 
+When executed as a script, takes a loading trace and generates a png of the
+loading graph."""
+
+import activity_lens
 import request_track
 
 
@@ -167,3 +171,28 @@ class LoadingGraphViewVisualization(object):
     from_request_id = edge.from_node.request.request_id
     to_request_id = edge.to_node.request.request_id
     return '"%s" -> "%s" %s;\n' % (from_request_id, to_request_id, arrow)
+
+def main(trace_file):
+  import subprocess
+
+  import loading_graph_view
+  import loading_trace
+  import request_dependencies_lens
+
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_file)
+  dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
+  activity = activity_lens.ActivityLens(trace)
+  graph_view = loading_graph_view.LoadingGraphView(trace, dependencies_lens,
+                                                   activity=activity)
+  visualization = LoadingGraphViewVisualization(graph_view)
+
+  dotfile = trace_file + '.dot'
+  pngfile = trace_file + '.png'
+  with file(dotfile, 'w') as output:
+    visualization.OutputDot(output)
+  subprocess.check_call(['dot', '-Tpng', dotfile, '-o', pngfile])
+
+
+if __name__ == '__main__':
+  import sys
+  main(sys.argv[1])

commit cef414409a92489b0a749b8eb348c620ff93da7e
Author: gabadie <gabadie@chromium.org>
Date:   Mon Apr 18 03:34:56 2016 -0700

    tools/android/loading: Fixes inspector_websocket's missing timeouts
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1895733002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387884}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6893a0e7dbb3f68ee5c8ce3f7554e9922466a13b

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index aade097..bfbf00f 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -22,7 +22,9 @@ from telemetry.internal.backends.chrome_inspector import websocket
 import common_util
 
 
-DEFAULT_TIMEOUT_SECONDS = 10 # seconds
+DEFAULT_TIMEOUT_SECONDS = 10
+
+_WEBSOCKET_TIMEOUT_SECONDS = 10
 
 
 class DevToolsConnectionException(Exception):
@@ -177,7 +179,7 @@ class DevToolsConnection(object):
     request = {'method': method}
     if params:
       request['params'] = params
-    return self._ws.SyncRequest(request)
+    return self._ws.SyncRequest(request, timeout=_WEBSOCKET_TIMEOUT_SECONDS)
 
   def SendAndIgnoreResponse(self, method, params=None):
     """Issues a request to the DevTools server, do not wait for the response.
@@ -370,7 +372,8 @@ class DevToolsConnection(object):
         break
     assert self._target_descriptor['url'] == 'about:blank'
     self._ws = inspector_websocket.InspectorWebsocket()
-    self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'])
+    self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'],
+                     timeout=_WEBSOCKET_TIMEOUT_SECONDS)
 
 
 class Listener(object):

commit 4715535a451ec7d4e23fbcbad113dc708a12b2fc
Author: droger <droger@chromium.org>
Date:   Mon Apr 18 01:41:02 2016 -0700

    tools/android/loading Split GoogleAPI wrappers to a separate library
    
    This is in preparation of the migration to a master/slave
    architecture.
    
    Review URL: https://codereview.chromium.org/1882793006
    
    Cr-Original-Commit-Position: refs/heads/master@{#387873}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d452ab4c620c15a11333b36d8e5984248e158e35

diff --git a/loading/gce/google_storage_accessor.py b/loading/gce/google_storage_accessor.py
new file mode 100644
index 0000000..59c47da
--- /dev/null
+++ b/loading/gce/google_storage_accessor.py
@@ -0,0 +1,60 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from gcloud import storage
+from oauth2client.client import GoogleCredentials
+
+
+class GoogleStorageAccessor(object):
+  """Utility class providing helpers for Google Cloud Storage.
+  """
+  def __init__(self, project_name, bucket_name):
+    """project_name is the name of the Google Cloud project.
+    bucket_name is the name of the bucket that is used for Cloud Storage calls.
+    """
+    self._credentials = GoogleCredentials.get_application_default()
+    self._project_name = project_name
+    self._bucket_name = bucket_name
+
+  def _GetStorageClient(self):
+    """Returns the storage client associated with the project"""
+    return storage.Client(project = self._project_name,
+                          credentials = self._credentials)
+
+  def _GetStorageBucket(self, storage_client):
+    return storage_client.get_bucket(self._bucket_name)
+
+  def UploadFile(self, filename_src, filename_dest):
+    """Uploads a file to Google Cloud Storage
+
+    Args:
+      filename_src: name of the local file
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    with open(filename_src) as file_src:
+      blob.upload_from_file(file_src)
+    return blob.public_url
+
+  def UploadString(self, data_string, filename_dest):
+    """Uploads a string to Google Cloud Storage
+
+    Args:
+      data_string: the contents of the file to be uploaded
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    blob.upload_from_string(data_string)
+    return blob.public_url
+
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 2988113..0ef2cae 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -10,19 +10,16 @@ import time
 import subprocess
 import sys
 
-from gcloud import storage
-from gcloud.exceptions import NotFound
-from oauth2client.client import GoogleCredentials
-
 # NOTE: The parent directory needs to be first in sys.path to avoid conflicts
 # with catapult modules that have colliding names, as catapult inserts itself
 # into the path as the second element. This is an ugly and fragile hack.
 sys.path.insert(0,
     os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
 import controller
+from google_storage_accessor import GoogleStorageAccessor
 import loading_trace
-import options
 from loading_trace_database import LoadingTraceDatabase
+import options
 
 
 class ServerApp(object):
@@ -40,12 +37,9 @@ class ServerApp(object):
     self._tasks_lock = threading.Lock()  # Protects _tasks and _failed_tasks.
     self._initial_task_count = -1
     self._start_time = None
-    print 'Initializing credentials'
-    self._credentials = GoogleCredentials.get_application_default()
     print 'Reading configuration'
     with open(configuration_file) as config_json:
        config = json.load(config_json)
-       self._project_name = config['project_name']
 
        # Separate the cloud storage path into the bucket and the base path under
        # the bucket.
@@ -58,6 +52,8 @@ class ServerApp(object):
            self._base_path_in_bucket += '/'
 
        self._src_path = config['src_path']
+       self._google_storage_accessor = GoogleStorageAccessor(
+           project_name=config['project_name'], bucket_name=self._bucket_name)
 
     # Initialize the global options that will be used during trace generation.
     options.OPTIONS.ParseArgs([])
@@ -67,46 +63,6 @@ class ServerApp(object):
     """Returns True if the application is currently processing tasks."""
     return self._thread is not None and self._thread.is_alive()
 
-  def _GetStorageClient(self):
-    return storage.Client(project = self._project_name,
-                          credentials = self._credentials)
-
-  def _GetStorageBucket(self, storage_client):
-    return storage_client.get_bucket(self._bucket_name)
-
-  def _UploadFile(self, filename_src, filename_dest):
-    """Uploads a file to Google Cloud Storage
-
-    Args:
-      filename_src: name of the local file
-      filename_dest: name of the file in Google Cloud Storage
-
-    Returns:
-      The URL of the file in Google Cloud Storage.
-    """
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename_dest)
-    with open(filename_src) as file_src:
-      blob.upload_from_file(file_src)
-    return blob.public_url
-
-  def _UploadString(self, data_string, filename_dest):
-    """Uploads a string to Google Cloud Storage
-
-    Args:
-      data_string: the contents of the file to be uploaded
-      filename_dest: name of the file in Google Cloud Storage
-
-    Returns:
-      The URL of the file in Google Cloud Storage.
-    """
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename_dest)
-    blob.upload_from_string(data_string)
-    return blob.public_url
-
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
     """ Generates a trace on _thread.
@@ -210,7 +166,8 @@ class ServerApp(object):
         if trace_metadata['succeeded']:
           print 'Uploading: %s' % remote_filename
           remote_trace_location = traces_dir + remote_filename
-          self._UploadFile(local_filename, remote_trace_location)
+          self._google_storage_accessor.UploadFile(local_filename,
+                                           remote_trace_location)
           full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
               remote_trace_location)
           trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
@@ -220,21 +177,25 @@ class ServerApp(object):
           self._failed_tasks.append({ "url": url, "repeat": repeat})
           self._tasks_lock.release()
           if os.path.isfile(local_filename):
-            self._UploadFile(local_filename, failures_dir + remote_filename)
+            self._google_storage_accessor.UploadFile(local_filename,
+                                            failures_dir + remote_filename)
         print 'Uploading log'
-        self._UploadFile(log_filename, logs_dir + remote_filename)
+        self._google_storage_accessor.UploadFile(log_filename,
+                                         logs_dir + remote_filename)
       # Pop once task is finished, for accurate status tracking.
       self._tasks_lock.acquire()
       url = self._tasks.pop()
       self._tasks_lock.release()
 
-    self._UploadString(json.dumps(trace_database.ToJsonDict(), indent=2),
-                       traces_dir + 'trace_database.json')
+    self._google_storage_accessor.UploadString(
+        json.dumps(trace_database.ToJsonDict(), indent=2),
+        traces_dir + 'trace_database.json')
 
     if len(self._failed_tasks) > 0:
       print 'Uploading failing URLs'
-      self._UploadString(json.dumps(self._failed_tasks, indent=2),
-                         failures_dir + 'failures.json')
+      self._google_storage_accessor.UploadString(
+          json.dumps(self._failed_tasks, indent=2),
+          failures_dir + 'failures.json')
 
   def _SetTaskList(self, http_body):
     """Sets the list of tasks and starts processing them

commit ac2ec40d93eb96e3f62610d657ed00c836135ec1
Author: johnme <johnme@chromium.org>
Date:   Fri Apr 15 11:17:44 2016 -0700

    Implement InstanceIDAndroid using InstanceIDWithSubtype.java
    
    Replaces the previous stub implementation with a fully functional
    implementation backed by InstanceIDWithSubtype.java.
    
    Part of a series of patches:
    1. https://codereview.chromium.org/1832833002 adds InstanceIDWithSubtype
    2. this patch
    3. https://codereview.chromium.org/1829023002 adds fake and test
    4. https://codereview.chromium.org/1854093002 enables InstanceID by default
    5. https://codereview.chromium.org/1851423003 switches Push to InstanceIDs
    
    BUG=589461
    
    Review URL: https://codereview.chromium.org/1830983002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387646}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2f8daf9d61b0bd6260dda858f852b258e03341f4

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 0495984..125f04e 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -216,6 +216,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="lib" path="out/Debug/lib.java/components/dom_distiller/android/dom_distiller_core_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/external_video_surface/java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/gcm_driver/android/gcm_driver_java.jar"/>
+    <classpathentry kind="lib" path="out/Debug/lib.java/components/gcm_driver/instance_id/android/instance_id_driver_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/invalidation/impl/java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/invalidation/impl/proto_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/navigation_interception/android/navigation_interception_java.jar"/>

commit b281445db2995a6e582f8e4bfb74c708c65ac18e
Author: mattcary <mattcary@chromium.org>
Date:   Fri Apr 15 08:37:01 2016 -0700

    Clovis: Update prefetch view to accept changed location of url field.
    
    Review URL: https://codereview.chromium.org/1890813004
    
    Cr-Original-Commit-Position: refs/heads/master@{#387605}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8cc4a1d8c3c03d52676ecb2407654e9b2c7c0fd3

diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 057ac0f..0df9025 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -154,7 +154,7 @@ class PrefetchSimulationView(object):
     for preload_step_event in preload_step_events:
       preload_event = resource_events.EventFromStep(preload_step_event)
       if preload_event:
-        preloaded_urls.add(preload_event.args['url'])
+        preloaded_urls.add(preload_event.args['data']['url'])
     parser_requests = cls.ParserDiscoverableRequests(
         request, dependencies_lens)
     preloaded_root_requests = filter(
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index be97809..1f85019 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -38,7 +38,7 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
         first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([first_request], preloaded_requests)
     self._SetUp(
-        [{'args': {'url': 'http://bla.com/nyancat.js'},
+        [{'args': {'data': {'url': 'http://bla.com/nyancat.js'}},
           'cat': 'blink.net', 'id': '0xaf9f14fa9dd6c314', 'name': 'Resource',
           'ph': 'X', 'ts': 1, 'dur': 120, 'pid': 12, 'tid': 12},
          {'args': {'step': 'Preload'}, 'cat': 'blink.net',
diff --git a/loading/testdata/scanner_vs_parser.trace b/loading/testdata/scanner_vs_parser.trace
index 427cdc8..627d141 100644
--- a/loading/testdata/scanner_vs_parser.trace
+++ b/loading/testdata/scanner_vs_parser.trace
@@ -55,8 +55,10 @@
     "events": [
       {
         "args": {
-          "priority": 4,
-          "url": "http://l/"
+          "data": {
+            "priority": 4,
+            "url": "http://l/"
+          }
         },
         "cat": "blink.net",
         "name": "Resource",
@@ -74,8 +76,10 @@
       },
       {
         "args": {
-          "priority": 1,
-          "url": "http://l/0.png"
+          "data": {
+            "priority": 1,
+            "url": "http://l/0.png"
+          }
         },
         "cat": "blink.net",
         "name": "Resource",
@@ -95,8 +99,10 @@
       },
       {
         "args": {
-          "priority": 1,
-          "url": "http://l/1.png"
+          "data": {
+            "priority": 1,
+            "url": "http://l/1.png"
+          }
         },
         "cat": "blink.net",
         "name": "Resource",

commit d0eb4e867e2ec64702b31af05b973ba72c4d66d0
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 15 08:32:25 2016 -0700

    tools/android/loading: Show cmd to resume sandwich in case of task failure.
    
    Sandwich is moving to the task_manager API. In case of a task failing,
    ExecuteWithCommandLine() was only helping by giving the command line
    to re-execute this task. This CL lets ExecuteWithCommandLine() also
    giving the command line argument to set in order to resume the initial
    command line, avoiding the re-executing previous tasks that went
    smoothly.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1878593002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387603}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 230c2bb243e9ce74981cc9cd7eafa092fabaa37d

diff --git a/loading/task_manager.py b/loading/task_manager.py
index 79a22fd..88a473d 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -216,6 +216,40 @@ def GenerateScenario(final_tasks, frozen_tasks):
   return scenario
 
 
+def ListResumingTasksToFreeze(scenario, final_tasks, failed_task):
+  """Lists the tasks that one needs to freeze to be able to resume the scenario
+  after failure.
+
+  Args:
+    scenario: The scenario (list of Task) to be resumed.
+    final_tasks: The list of final Task used to generate the scenario.
+    failed_task: A Task that have failed in the scenario.
+
+  Returns:
+    set(Task)
+  """
+  task_to_id = {t: i for i, t in enumerate(scenario)}
+  assert failed_task in task_to_id
+  frozen_tasks = set()
+  walked_tasks = set()
+
+  def InternalWalk(task):
+    if task.IsStatic() or task in walked_tasks:
+      return
+    walked_tasks.add(task)
+    if task not in task_to_id:
+      frozen_tasks.add(task)
+    elif task_to_id[task] < task_to_id[failed_task]:
+      frozen_tasks.add(task)
+    else:
+      for dependency in task._dependencies:
+        InternalWalk(dependency)
+
+  for final_task in final_tasks:
+    InternalWalk(final_task)
+  return frozen_tasks
+
+
 def OutputGraphViz(scenario, final_tasks, output):
   """Outputs the build dependency graph covered by this scenario.
 
@@ -285,6 +319,17 @@ def CommandLineParser():
   return parser
 
 
+def _GetCommandLineArgumentsStr(final_task_regexes, frozen_tasks):
+  arguments = []
+  if frozen_tasks:
+    arguments.append('-f')
+    arguments.extend([task.name for task in frozen_tasks])
+  if final_task_regexes:
+    arguments.append('-e')
+    arguments.extend(final_task_regexes)
+  return subprocess.list2cmdline(arguments)
+
+
 def ExecuteWithCommandLine(args, tasks, default_final_tasks):
   """Helper to execute tasks using command line arguments.
 
@@ -353,11 +398,11 @@ def ExecuteWithCommandLine(args, tasks, default_final_tasks):
         print '# Looks like something went wrong in \'{}\''.format(task.name)
         print '#'
         print '# To re-execute only this task, add the following parameters:'
-        suggested_flags = []
-        if task._dependencies:
-          suggested_flags.append('-f')
-          suggested_flags.extend([dep.name for dep in task._dependencies])
-        suggested_flags.extend(['-e', task.name])
-        print '#   ' + subprocess.list2cmdline(suggested_flags)
+        print '#   ' + _GetCommandLineArgumentsStr(
+            [task.name], task._dependencies)
+        print '#'
+        print '# To resume from this task, add the following parameters:'
+        print '#   ' + _GetCommandLineArgumentsStr(args.run_regexes,
+            ListResumingTasksToFreeze(scenario, final_tasks, task))
         raise
   return 0
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index 2217aef..68ae1ce 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -2,10 +2,12 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import contextlib
 import os
 import re
 import shutil
 import StringIO
+import sys
 import tempfile
 import unittest
 
@@ -27,6 +29,26 @@ _GOLDEN_GRAPHVIZ = """digraph graphname {
 }\n"""
 
 
+@contextlib.contextmanager
+def EatStdoutAndStderr():
+  """Overrides sys.std{out,err} to intercept write calls."""
+  sys.stdout.flush()
+  sys.stderr.flush()
+  original_stdout = sys.stdout
+  original_stderr = sys.stderr
+  try:
+    sys.stdout = StringIO.StringIO()
+    sys.stderr = StringIO.StringIO()
+    yield
+  finally:
+    sys.stdout = original_stdout
+    sys.stderr = original_stderr
+
+
+class TestException(Exception):
+  pass
+
+
 class TaskManagerTestCase(unittest.TestCase):
   def setUp(self):
     self.output_directory = tempfile.mkdtemp()
@@ -244,6 +266,52 @@ class GenerateScenarioTest(TaskManagerTestCase):
     task_manager.OutputGraphViz(scenario, [TaskF], output)
     self.assertEqual(_GOLDEN_GRAPHVIZ, output.getvalue())
 
+  def testListResumingTasksToFreeze(self):
+    TaskManagerTestCase.setUp(self)
+    builder = task_manager.Builder(self.output_directory)
+    static_task = builder.CreateStaticTask('static', __file__)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[static_task])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskA, TaskB])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskA])
+    def TaskD():
+      pass
+    @builder.RegisterTask('e', dependencies=[TaskC])
+    def TaskE():
+      pass
+    @builder.RegisterTask('f', dependencies=[TaskC])
+    def TaskF():
+      pass
+
+    for k in 'abcdef':
+      self.TouchOutputFile(k)
+
+    def RunSubTest(final_tasks, initial_frozen_tasks, failed_task, reference):
+      scenario = \
+          task_manager.GenerateScenario(final_tasks, initial_frozen_tasks)
+      resume_frozen_tasks = task_manager.ListResumingTasksToFreeze(
+          scenario, final_tasks, failed_task)
+      self.assertEqual(reference, resume_frozen_tasks)
+
+      failed_pos = scenario.index(failed_task)
+      new_scenario = \
+          task_manager.GenerateScenario(final_tasks, resume_frozen_tasks)
+      self.assertEqual(scenario[failed_pos:], new_scenario)
+
+    RunSubTest([TaskA], set([]), TaskA, set([]))
+    RunSubTest([TaskD], set([]), TaskA, set([]))
+    RunSubTest([TaskD], set([]), TaskD, set([TaskA]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskB, set([TaskA]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskC, set([TaskA, TaskB]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskE, set([TaskC]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskF, set([TaskC, TaskE]))
+
 
 class CommandLineControlledExecutionTest(TaskManagerTestCase):
   def Execute(self, *command_line_args):
@@ -265,15 +333,16 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
       pass
     @builder.RegisterTask('raise_exception', dependencies=[TaskB])
     def TaskF():
-      raise Exception('Expected error.')
+      raise TestException('Expected error.')
 
     default_final_tasks = [TaskD, TaskE]
     parser = task_manager.CommandLineParser()
     cmd = ['-o', self.output_directory]
     cmd.extend([i for i in command_line_args])
     args = parser.parse_args(cmd)
-    return task_manager.ExecuteWithCommandLine(
-        args, [TaskA, TaskB, TaskC, TaskD, TaskE, TaskF], default_final_tasks)
+    with EatStdoutAndStderr():
+      return task_manager.ExecuteWithCommandLine(
+          args, [TaskA, TaskB, TaskC, TaskD, TaskE, TaskF], default_final_tasks)
 
   def testSimple(self):
     self.assertEqual(0, self.Execute())
@@ -291,7 +360,7 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     self.assertEqual(0, self.Execute('-f', 'c'))
 
   def testTaskFailure(self):
-    with self.assertRaisesRegexp(Exception, r'^Expected error\.$'):
+    with self.assertRaisesRegexp(TestException, r'^Expected error\.$'):
       self.Execute('-e', 'raise_exception')
 
 

commit f83730cefa69b0731afcc6ba2813b9e700e33f36
Author: mattcary <mattcary@chromium.org>
Date:   Fri Apr 15 06:57:47 2016 -0700

    Clovis: unittesting fix and some general tweaks.
    
    The root problem was that dependency_graph_unittest called
    request_dependencies_lens.TestReqeusts.CreateLoadingTrace, and then changed some
    of the timing of those requests. As those requests were global, that meant that
    if a different unittest (eg, prefetch_view_unittest) happened to use the same
    TestRequests, it would see the changed timing, and so run differently than if it
    were run in isolation.
    
    A nice way to fix that is to serialize and deserialize the trace, which exposed
    some other holes in our organization, namely abstract classes that defined empty
    methods instead of abstract methods (in the python world, that means using
    "pass" instead of "raise NotImplementedError") and then some other gaps in our
    serialization methods.
    
    In order to diagnose & fix this, run_tests was extended to allow for multiple
    tests to be specified. This does not change existing behavior when a single
    argument is passed to run_tests, and does the right thing for multiple arguments
    instead of silently ignoring them.
    
    Review URL: https://codereview.chromium.org/1892073002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387590}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 046e2f3a95ac7bb7afd3b16bb18007c2bf71fd8e

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index ab78f8e..aade097 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -390,14 +390,14 @@ class Listener(object):
       event_name: (str) Event name, as registered.
       event: (dict) complete event.
     """
-    pass
+    raise NotImplementedError
 
 
 class Track(Listener):
   """Collects data from a DevTools server."""
   def GetEvents(self):
     """Returns a list of collected events, finalizing the state if necessary."""
-    pass
+    raise NotImplementedError
 
   def ToJsonDict(self):
     """Serializes to a dictionary, to be dumped as JSON.
@@ -406,10 +406,10 @@ class Track(Listener):
       A dict that can be dumped by the json module, and loaded by
       FromJsonDict().
     """
-    pass
+    raise NotImplementedError
 
   @classmethod
-  def FromJsonDict(cls, json_dict):
+  def FromJsonDict(cls, _json_dict):
     """Returns a Track instance constructed from data dumped by
        Track.ToJsonDict().
 
@@ -419,4 +419,8 @@ class Track(Listener):
     Returns:
       a Track instance.
     """
-    pass
+    # There is no sensible way to deserialize this abstract class, but
+    # subclasses are not required to define a deserialization method. For
+    # example, for testing we have a FakeRequestTrack which is never
+    # deserialized; instead fake instances are deserialized as RequestTracks.
+    assert False
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 7630bcc..ff8b8c5 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -42,7 +42,8 @@ class LoadingTrace(object):
     result = {self._URL_KEY: self.url, self._METADATA_KEY: self.metadata,
               self._PAGE_KEY: self.page_track.ToJsonDict(),
               self._REQUEST_KEY: self.request_track.ToJsonDict(),
-              self._TRACING_KEY: self.tracing_track.ToJsonDict()}
+              self._TRACING_KEY: (self.tracing_track.ToJsonDict()
+                                  if self.tracing_track else None)}
     return result
 
   def ToJsonFile(self, json_path):
@@ -111,7 +112,8 @@ class LoadingTrace(object):
     self._tracing_track = None
 
   def _RestoreTracingTrack(self):
-    assert self._tracing_json_str
+    if not self._tracing_json_str:
+      return None
     self._tracing_track = tracing.TracingTrack.FromJsonDict(
         json.loads(self._tracing_json_str))
     self._tracing_json_str = None
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 773af44..08e9c17 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -72,11 +72,14 @@ class TestRequests(object):
 
   @classmethod
   def CreateLoadingTrace(cls, trace_events=None):
-    return test_utils.LoadingTraceFromEvents(
+    trace = test_utils.LoadingTraceFromEvents(
         [cls.FIRST_REDIRECT_REQUEST, cls.SECOND_REDIRECT_REQUEST,
          cls.REDIRECTED_REQUEST, cls.REQUEST, cls.JS_REQUEST, cls.JS_REQUEST_2,
          cls.JS_REQUEST_OTHER_FRAME, cls.JS_REQUEST_UNRELATED_FRAME],
         cls.PAGE_EVENTS, trace_events)
+    # Serialize and deserialize so that clients can change events without
+    # affecting future tests.
+    return LoadingTrace.FromJsonDict(trace.ToJsonDict())
 
 
 class RequestDependencyLensTestCase(unittest.TestCase):
diff --git a/loading/run_tests b/loading/run_tests
index 7f182a7..1f04f05 100755
--- a/loading/run_tests
+++ b/loading/run_tests
@@ -16,9 +16,15 @@ if __name__ == '__main__':
 
   suite = unittest.TestSuite()
   loader = unittest.TestLoader()
-  pattern = '*%s*_unittest.py' % ('' if len(sys.argv) < 2 else sys.argv[1])
   root_dir = os.path.dirname(os.path.realpath(__file__))
-  suite.addTests(loader.discover(start_dir=root_dir, pattern=pattern))
+  if len(sys.argv) < 2:
+    cases = loader.discover(start_dir=root_dir, pattern='*_unittest.py')
+  else:
+    cases = []
+    for module in sys.argv[1:]:
+      pattern = '{}_unittest.py'.format(module)
+      cases.extend(loader.discover(start_dir=root_dir, pattern=pattern))
+  suite.addTests(cases)
   res = unittest.TextTestRunner(verbosity=2).run(suite)
   if res.wasSuccessful():
     sys.exit(0)
diff --git a/loading/test_utils.py b/loading/test_utils.py
index a0b5b16..8ae919f 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -18,9 +18,20 @@ class FakeRequestTrack(devtools_monitor.Track):
     super(FakeRequestTrack, self).__init__(None)
     self._events = [self._RewriteEvent(e) for e in events]
 
+  def Handle(self, _method, _msg):
+    assert False  # Should never be called.
+
   def GetEvents(self):
     return self._events
 
+  def ToJsonDict(self):
+    cls = request_track.RequestTrack
+    return {cls._EVENTS_KEY: [
+        rq.ToJsonDict() for rq in self.GetEvents()],
+            cls._METADATA_KEY: {
+                cls._DUPLICATES_KEY: 0,
+                cls._INCONSISTENT_INITIATORS_KEY: 0}}
+
   def _RewriteEvent(self, event):
     # This modifies the instance used across tests, so this method
     # must be idempotent.
@@ -33,6 +44,9 @@ class FakePageTrack(devtools_monitor.Track):
     super(FakePageTrack, self).__init__(None)
     self._events = events
 
+  def Handle(self, _method, _msg):
+    assert False  # Should never be called.
+
   def GetEvents(self):
     return self._events
 
@@ -42,6 +56,9 @@ class FakePageTrack(devtools_monitor.Track):
     assert event['method'] == page_track.PageTrack.FRAME_STARTED_LOADING
     return event['frame_id']
 
+  def ToJsonDict(self):
+    return {'events': [event for event in self._events]}
+
 
 def MakeRequestWithTiming(
     url, source_url, timing_dict, magic_content_type=False,
diff --git a/loading/tracing.py b/loading/tracing.py
index fb54d70..c8db1df 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -115,6 +115,8 @@ class TracingTrack(devtools_monitor.Track):
 
   @classmethod
   def FromJsonDict(cls, json_dict):
+    if not json_dict:
+      return None
     assert 'events' in json_dict
     events = [Event(e) for e in json_dict['events']]
     tracing_track = TracingTrack(None)

commit c75ed61aa96c817351cee960123030ac0c713a6c
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 14 08:58:57 2016 -0700

    sandwich: Remove some non necessary commas in sandwich_misc.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1890883003
    
    Cr-Original-Commit-Position: refs/heads/master@{#387329}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f3cb0fefb2586a52be3e562378a6d93b128d636b

diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 13988c3..e7b00c6 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -15,10 +15,10 @@ import wpr_backend
 REDIRECTED_MAIN_DISCOVERER = 'redirected-main'
 
 # All resources which are fetched from the main document and their redirections.
-PARSER_DISCOVERER = 'parser',
+PARSER_DISCOVERER = 'parser'
 
 # Simulation of HTMLPreloadScanner on the main document and their redirections.
-HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner',
+HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner'
 
 SUBRESOURCE_DISCOVERERS = set([
   REDIRECTED_MAIN_DISCOVERER,

commit 89bf0a5ede47303dcea43e76f44037e772f80be1
Author: agrieve <agrieve@chromium.org>
Date:   Tue Apr 12 17:05:39 2016 -0700

    Fix GN deps needed by third_party/WebKit/Tools/Scripts/run-webkit-tests
    
    BUG=587083
    TBR=mkwst
    
    Review URL: https://codereview.chromium.org/1882533004
    
    Cr-Original-Commit-Position: refs/heads/master@{#386870}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 993374cf1ac90ef6fbb188017ea2fb445cf0eb09

diff --git a/forwarder/BUILD.gn b/forwarder/BUILD.gn
new file mode 100644
index 0000000..9cc99b4
--- /dev/null
+++ b/forwarder/BUILD.gn
@@ -0,0 +1,24 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import("//build/symlink.gni")
+
+if (current_toolchain == host_toolchain) {
+  # GYP: //tools/android/forwarder/forwarder.gyp:forwarder
+  executable("forwarder") {
+    sources = [
+      "forwarder.cc",
+    ]
+    deps = [
+      "//base",
+      "//build/config/sanitizers:deps",
+      "//tools/android/common",
+    ]
+  }
+} else {
+  # Create a symlink from root_build_dir -> clang_x64/forwarder.
+  binary_symlink("forwarder") {
+    binary_label = ":$target_name($host_toolchain)"
+  }
+}
diff --git a/forwarder2/BUILD.gn b/forwarder2/BUILD.gn
index 6899a7e..d2af725 100644
--- a/forwarder2/BUILD.gn
+++ b/forwarder2/BUILD.gn
@@ -93,6 +93,6 @@ if (current_toolchain != default_toolchain) {
 } else {
   # Create a symlink from root_build_dir -> clang_x64/host_forwarder.
   binary_symlink("host_forwarder") {
-    binary_label = ":host_forwarder($host_toolchain)"
+    binary_label = ":$target_name($host_toolchain)"
   }
 }
diff --git a/md5sum/BUILD.gn b/md5sum/BUILD.gn
index 7cce4c9..35c386b 100644
--- a/md5sum/BUILD.gn
+++ b/md5sum/BUILD.gn
@@ -46,5 +46,6 @@ if (current_toolchain == default_toolchain) {
   # GYP: //tools/android/md5sum/md5sum.gyp:md5sum_bin_host
   binary_symlink("md5sum_bin_host") {
     binary_label = ":md5sum_bin($host_toolchain)"
+    output_name = "md5sum_bin_host"
   }
 }

commit fd1eca3c26d6f0e01d45f442468b61a5e6580161
Author: kjellander <kjellander@chromium.org>
Date:   Tue Apr 12 03:58:25 2016 -0700

    Revert of [Devil] Replace generated Devil config with jinja template. (patchset #8 id:140001 of https://codereview.chromium.org/1812383003/ )
    
    Reason for revert:
    I believe this change breaks content_browsertests on every Android tester in chromium.android.
    
    Examples:
    
    https://build.chromium.org/p/chromium.android/builders/Lollipop%20Phone%20Tester/builds/4031
    https://build.chromium.org/p/chromium.android/builders/Marshmallow%2064%20bit%20Tester/builds/1457
    https://build.chromium.org/p/chromium.android/builders/KitKat%20Tablet%20Tester/builds/3759
    
    Original issue's description:
    > [Devil] Replace generated Devil config with jinja template.
    >
    > This change will hopefully allow us to configure devil more based
    > on the build config. For example, it will let us use the same
    > android_sdk_tools to run tests that we use to build with.
    >
    > BUG=
    >
    > Committed: https://crrev.com/c2aa4243a9e00a2d0e254d9f6f73b41c00cb644d
    > Cr-Commit-Position: refs/heads/master@{#386461}
    
    TBR=droger@chromium.org,jbudorick@chromium.org,mikecase@chromium.org
    # Skipping CQ checks because original CL landed less than 1 days ago.
    NOPRESUBMIT=true
    NOTREECHECKS=true
    NOTRY=true
    BUG=
    
    Review URL: https://codereview.chromium.org/1885503002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386648}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d408713145a9b75e155e1bf106105803ad40b1bd

diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 80f0f29..2362414 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -34,9 +34,8 @@ cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
 mkdir -p $tmp_src_dir/build/android
 cp build/android/devil_chromium.py $tmp_src_dir/build/android/
 cp build/android/video_recorder.py $tmp_src_dir/build/android/
+cp build/android/devil_chromium.json $tmp_src_dir/build/android/
 cp -r build/android/pylib $tmp_src_dir/build/android/
-mkdir -p $tmp_src_dir/$builddir/gen/
-cp $builddir/gen/devil_chromium.json $tmp_src_dir/$builddir/gen/
 mkdir -p \
   $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
 cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \

commit 85d20d0922c72586e8ffc03495dbb9e9ff694d87
Author: mikecase <mikecase@chromium.org>
Date:   Mon Apr 11 13:26:41 2016 -0700

    [Devil] Replace generated Devil config with jinja template.
    
    This change will hopefully allow us to configure devil more based
    on the build config. For example, it will let us use the same
    android_sdk_tools to run tests that we use to build with.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1812383003
    
    Cr-Original-Commit-Position: refs/heads/master@{#386461}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c2aa4243a9e00a2d0e254d9f6f73b41c00cb644d

diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 2362414..80f0f29 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -34,8 +34,9 @@ cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
 mkdir -p $tmp_src_dir/build/android
 cp build/android/devil_chromium.py $tmp_src_dir/build/android/
 cp build/android/video_recorder.py $tmp_src_dir/build/android/
-cp build/android/devil_chromium.json $tmp_src_dir/build/android/
 cp -r build/android/pylib $tmp_src_dir/build/android/
+mkdir -p $tmp_src_dir/$builddir/gen/
+cp $builddir/gen/devil_chromium.json $tmp_src_dir/$builddir/gen/
 mkdir -p \
   $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
 cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \

commit d40f7000570b3ee63a5a19eb99058558046d134d
Author: gabadie <gabadie@chromium.org>
Date:   Mon Apr 11 02:08:28 2016 -0700

    tools/android/loading: Implements task_manager.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1869703002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386353}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c0fa3be7a5d809a3a0291cd97a3169fd028ff701

diff --git a/loading/common_util.py b/loading/common_util.py
index ca06da2..855284f 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -3,12 +3,25 @@
 # found in the LICENSE file.
 
 import contextlib
+import json
 import logging
+import os
+import re
 import shutil
+import sys
 import tempfile
 import time
 
 
+def VerboseCompileRegexOrAbort(regex):
+  """Compiles a user-provided regular expression, exits the program on error."""
+  try:
+    return re.compile(regex)
+  except re.error as e:
+    sys.stderr.write('invalid regex: {}\n{}\n'.format(regex, e))
+    sys.exit(2)
+
+
 def PollFor(condition, condition_name, interval=5):
   """Polls for a function to return true.
 
diff --git a/loading/task_manager.py b/loading/task_manager.py
new file mode 100644
index 0000000..79a22fd
--- /dev/null
+++ b/loading/task_manager.py
@@ -0,0 +1,363 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""API that build and execute recipes wrapped into a task dependency graph.
+
+A Task consists of a 'recipe' (a closure to be executed) and a list of refs to
+tasks that should be executed prior to executing this Task (i.e. dependencies).
+
+A Task can be either 'static' or 'dynamic'. A static tasks only represents an
+existing file on the filesystem, its recipe is a no-op. The responsibility of
+the recipe of a dynamic task is to produce the file with the name assigned at
+task creation.
+
+A scenario is a ordered list of dynamic tasks to execute such that the
+dependencies of a given task are execute before the said task. The scenario is
+built from a list of final tasks and a list of frozen tasks:
+  - A final task is a task to execute ultimately. Therefore the scenario is
+    composed of final tasks and their required intermediary tasks.
+  - A frozen task is dynamic task to not execute. This is a mechanism to morph a
+    dynamic task that may have dependencies to a static task with no dependency
+    at scenario generation time, injecting what the dynamic task have already
+    produced before as an input of the smaller tasks dependency graph covered
+    by the scenario.
+
+Example:
+  # -------------------------------------------------- Build my dependency graph
+  builder = Builder('my/output/dir')
+  input0 = builder.CreateStaticTask('input0', 'path/to/input/file0')
+  input1 = builder.CreateStaticTask('input1', 'path/to/input/file1')
+  input2 = builder.CreateStaticTask('input2', 'path/to/input/file2')
+  input3 = builder.CreateStaticTask('input3', 'path/to/input/file3')
+
+  @builder.RegisterTask('out0', dependencies=[input0, input2])
+  def BuildOut0():
+    DoStuff(input0.path, input2.path, out=BuildOut0.path)
+
+  @builder.RegisterTask('out1', dependencies=[input1, input3])
+  def BuildOut1():
+    DoStuff(input1.path, input3.path, out=BuildOut1.path)
+
+  @builder.RegisterTask('out2', dependencies=[BuildOut0, BuildOut1])
+  def BuildOut2():
+    DoStuff(BuildOut0.path, BuildOut1.path, out=BuildOut2.path)
+
+  @builder.RegisterTask('out3', dependencies=[BuildOut0])
+  def BuildOut3():
+    DoStuff(BuildOut0.path, out=BuildOut3.path)
+
+  # ---------------------------- Case 1: Execute BuildOut3 and its dependencies.
+  for task in GenerateScenario(final_tasks=[BuildOut3], frozen_tasks=[])
+    task.Execute()
+
+  # ---------- Case 2: Execute BuildOut2 and its dependencies but not BuildOut1.
+  # It is required that BuildOut1.path is already existing.
+  for task in GenerateScenario(final_tasks=[BuildOut2],
+                               frozen_tasks=[BuildOut1])
+    task.Execute()
+"""
+
+
+import argparse
+import logging
+import os
+import subprocess
+import sys
+
+import common_util
+
+
+_TASK_GRAPH_DOTFILE_NAME = 'tasks_graph.dot'
+_TASK_GRAPH_PNG_NAME = 'tasks_graph.png'
+
+
+class TaskError(Exception):
+  pass
+
+
+class Task(object):
+  """Task that can be either a static task or dynamic with a recipe."""
+
+  def __init__(self, name, path, dependencies, recipe):
+    """Constructor.
+
+    Args:
+      name: The name of the  task.
+      path: Path to the file or directory that this task produces.
+      dependencies: List of parent task to execute before.
+      recipe: Function to execute if a dynamic task or None if a static task.
+    """
+    self.name = name
+    self.path = path
+    self._dependencies = dependencies
+    self._recipe = recipe
+    self._is_done = recipe == None
+
+  def Execute(self):
+    """Executes this task."""
+    if self.IsStatic():
+      raise TaskError('Task {} is static.'.format(self.name))
+    if not self._is_done:
+      self._recipe()
+    self._is_done = True
+
+  def IsStatic(self):
+    """Returns whether this task is a static task."""
+    return self._recipe == None
+
+
+class Builder(object):
+  """Utilities for creating sub-graphs of tasks with dependencies."""
+
+  def __init__(self, output_directory):
+    """Constructor.
+
+    Args:
+      output_directory: Output directory where the dynamic tasks work.
+    """
+    self.output_directory = output_directory
+    self.tasks = {}
+
+  def CreateStaticTask(self, task_name, path):
+    if not os.path.exists(path):
+      raise TaskError('Error while creating task {}: File not found: {}'.format(
+          task_name, path))
+    if task_name in self.tasks:
+      raise TaskError('Task {} already exists.'.format(task_name))
+    task = Task(task_name, path, [], None)
+    self.tasks[task_name] = task
+    return task
+
+  # Caution:
+  #   This decorator may not create a dynamic task in the case where
+  #   merge=True and another dynamic target having the same name have already
+  #   been created. In this case, it will just reuse the former task. This is at
+  #   the user responsibility to ensure that merged tasks would do the exact
+  #   same thing.
+  #
+  #     @builder.RegisterTask('hello')
+  #     def TaskA():
+  #       my_object.a = 1
+  #
+  #     @builder.RegisterTask('hello', merge=True)
+  #     def TaskB():
+  #       # This function won't be executed ever.
+  #       my_object.a = 2 # <------- Wrong because different from what TaskA do.
+  #
+  #     assert TaskA == TaskB
+  #     TaskB.Execute() # Sets set my_object.a == 1
+  def RegisterTask(self, task_name, dependencies=None, merge=False):
+    """Decorator that wraps a function into a dynamic task.
+
+    Args:
+      task_name: The name of this new task to register.
+      dependencies: List of SandwichTarget to build before this task.
+      merge: If a task already have this name, don't create a new one and
+        reuse the existing one.
+
+    Returns:
+      A Task that was created by wrapping the function or an existing registered
+      wrapper (that have wrapped a different function).
+    """
+    dependencies = dependencies or []
+    def InnerAddTaskWithNewPath(recipe):
+      if task_name in self.tasks:
+        if not merge:
+          raise TaskError('Task {} already exists.'.format(task_name))
+        task = self.tasks[task_name]
+        if task.IsStatic():
+          raise TaskError('Should not merge dynamic task {} with the already '
+                          'existing static one.'.format(task_name))
+        return task
+      task_path = os.path.join(self.output_directory, task_name)
+      task = Task(task_name, task_path, dependencies, recipe)
+      self.tasks[task_name] = task
+      return task
+    return InnerAddTaskWithNewPath
+
+
+def GenerateScenario(final_tasks, frozen_tasks):
+  """Generates a list of tasks to execute in order of dependencies-first.
+
+  Args:
+    final_tasks: The final tasks to generate the scenario from.
+    frozen_tasks: Sets of task to freeze.
+
+  Returns:
+    [Task]
+  """
+  scenario = []
+  task_paths = {}
+  def InternalAppendTarget(task):
+    if task.IsStatic():
+      return
+    if task in frozen_tasks:
+      if not os.path.exists(task.path):
+        raise TaskError('Frozen target `{}`\'s path doesn\'t exist.'.format(
+            task.name))
+      return
+    if task.path in task_paths:
+      if task_paths[task.path] == None:
+        raise TaskError('Target `{}` depends on itself.'.format(task.name))
+      if task_paths[task.path] != task:
+        raise TaskError(
+            'Tasks `{}` and `{}` produce the same file: `{}`.'.format(
+                task.name, task_paths[task.path].name, task.path))
+      return
+    task_paths[task.path] = None
+    for dependency in task._dependencies:
+      InternalAppendTarget(dependency)
+    task_paths[task.path] = task
+    scenario.append(task)
+
+  for final_task in final_tasks:
+    InternalAppendTarget(final_task)
+  return scenario
+
+
+def OutputGraphViz(scenario, final_tasks, output):
+  """Outputs the build dependency graph covered by this scenario.
+
+  Args:
+    scenario: The generated scenario.
+    final_tasks: The final tasks used to generate the scenario.
+    output: A file-like output stream to receive the dot file.
+
+  Graph interpretations:
+    - Static tasks are shape less.
+    - Final tasks (the one that where directly appended) are box shaped.
+    - Non final dynamic tasks are ellipse shaped.
+    - Frozen dynamic tasks have a blue shape.
+  """
+  task_execution_ids = {t: i for i, t in enumerate(scenario)}
+  tasks_node_ids = dict()
+
+  def GetTaskNodeId(task):
+    if task in tasks_node_ids:
+      return tasks_node_ids[task]
+    node_id = len(tasks_node_ids)
+    node_label = task.name
+    node_color = 'blue'
+    node_shape = 'ellipse'
+    if task.IsStatic():
+      node_shape = 'plaintext'
+    elif task in task_execution_ids:
+      node_color = 'black'
+      node_label = str(task_execution_ids[task]) + ': ' + node_label
+    if task in final_tasks:
+      node_shape = 'box'
+    output.write('  n{} [label="{}", color={}, shape={}];\n'.format(
+        node_id, node_label, node_color, node_shape))
+    tasks_node_ids[task] = node_id
+    return node_id
+
+  output.write('digraph graphname {\n')
+  for task in scenario:
+    task_node_id = GetTaskNodeId(task)
+    for dep in task._dependencies:
+      dep_node_id = GetTaskNodeId(dep)
+      output.write('  n{} -> n{};\n'.format(dep_node_id, task_node_id))
+  output.write('}\n')
+
+
+def CommandLineParser():
+  """Creates command line arguments parser meant to be used as a parent parser
+  for any entry point that use the ExecuteWithCommandLine() function.
+
+  Returns:
+    The command line arguments parser.
+  """
+  parser = argparse.ArgumentParser(add_help=False)
+  parser.add_argument('-d', '--dry-run', action='store_true',
+                      help='Only prints the deps of tasks to build.')
+  parser.add_argument('-e', '--to-execute', metavar='REGEX', type=str,
+                      nargs='+', dest='run_regexes', default=[],
+                      help='Regex selecting tasks to execute.')
+  parser.add_argument('-f', '--to-freeze', metavar='REGEX', type=str,
+                      nargs='+', dest='frozen_regexes', default=[],
+                      help='Regex selecting tasks to not execute.')
+  parser.add_argument('-o', '--output', type=str, required=True,
+                      help='Path of the output directory.')
+  parser.add_argument('-v', '--output-graphviz', action='store_true',
+      help='Outputs the {} and {} file in the output directory.'
+           ''.format(_TASK_GRAPH_DOTFILE_NAME, _TASK_GRAPH_PNG_NAME))
+  return parser
+
+
+def ExecuteWithCommandLine(args, tasks, default_final_tasks):
+  """Helper to execute tasks using command line arguments.
+
+  Args:
+    args: Command line argument parsed with CommandLineParser().
+    tasks: Unordered list of tasks to publish to command line regexes.
+    default_final_tasks: Default final tasks if there is no -r command
+      line arguments.
+
+  Returns:
+    0 if success or 1 otherwise
+  """
+  frozen_regexes = [common_util.VerboseCompileRegexOrAbort(e)
+                      for e in args.frozen_regexes]
+  run_regexes = [common_util.VerboseCompileRegexOrAbort(e)
+                   for e in args.run_regexes]
+
+  # Lists frozen tasks
+  frozen_tasks = set()
+  if frozen_regexes:
+    for task in tasks:
+      for regex in frozen_regexes:
+        if regex.search(task.name):
+          frozen_tasks.add(task)
+          break
+
+  # Lists final tasks.
+  final_tasks = default_final_tasks
+  if run_regexes:
+    final_tasks = []
+    for task in tasks:
+      for regex in run_regexes:
+        if regex.search(task.name):
+          final_tasks.append(task)
+          break
+
+  # Create the scenario.
+  scenario = GenerateScenario(final_tasks, frozen_tasks)
+
+  if len(scenario) == 0:
+    logging.error('No tasks to build.')
+    return 1
+
+  if not os.path.isdir(args.output):
+    os.makedirs(args.output)
+
+  # Print the task dependency graph visualization.
+  if args.output_graphviz:
+    graphviz_path = os.path.join(args.output, _TASK_GRAPH_DOTFILE_NAME)
+    png_graph_path = os.path.join(args.output, _TASK_GRAPH_PNG_NAME)
+    with open(graphviz_path, 'w') as output:
+      OutputGraphViz(scenario, final_tasks, output)
+    subprocess.check_call(['dot', '-Tpng', graphviz_path, '-o', png_graph_path])
+
+  # Use the build scenario.
+  if args.dry_run:
+    for task in scenario:
+      print '{}:{}'.format(
+          task.name, ' '.join([' \\\n  ' + d.name for d in task._dependencies]))
+  else:
+    for task in scenario:
+      logging.info('%s %s' % ('-' * 60, task.name))
+      try:
+        task.Execute()
+      except:
+        print '# Looks like something went wrong in \'{}\''.format(task.name)
+        print '#'
+        print '# To re-execute only this task, add the following parameters:'
+        suggested_flags = []
+        if task._dependencies:
+          suggested_flags.append('-f')
+          suggested_flags.extend([dep.name for dep in task._dependencies])
+        suggested_flags.extend(['-e', task.name])
+        print '#   ' + subprocess.list2cmdline(suggested_flags)
+        raise
+  return 0
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
new file mode 100644
index 0000000..2217aef
--- /dev/null
+++ b/loading/task_manager_unittest.py
@@ -0,0 +1,299 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import re
+import shutil
+import StringIO
+import tempfile
+import unittest
+
+import task_manager
+
+
+_GOLDEN_GRAPHVIZ = """digraph graphname {
+  n0 [label="0: b", color=black, shape=ellipse];
+  n1 [label="1: c", color=black, shape=ellipse];
+  n0 -> n1;
+  n2 [label="a", color=blue, shape=plaintext];
+  n2 -> n1;
+  n3 [label="2: d", color=black, shape=ellipse];
+  n1 -> n3;
+  n4 [label="3: f", color=black, shape=box];
+  n3 -> n4;
+  n5 [label="e", color=blue, shape=ellipse];
+  n5 -> n4;
+}\n"""
+
+
+class TaskManagerTestCase(unittest.TestCase):
+  def setUp(self):
+    self.output_directory = tempfile.mkdtemp()
+
+  def tearDown(self):
+    shutil.rmtree(self.output_directory)
+
+  def TouchOutputFile(self, file_path):
+    with open(os.path.join(self.output_directory, file_path), 'w') as output:
+      output.write(file_path + '\n')
+
+
+class TaskTest(TaskManagerTestCase):
+  def testStaticTask(self):
+    task = task_manager.Task('hello.json', 'what/ever/hello.json', [], None)
+    self.assertTrue(task.IsStatic())
+    self.assertTrue(task._is_done)
+    with self.assertRaises(task_manager.TaskError):
+      task.Execute()
+
+  def testDynamicTask(self):
+    def Recipe():
+      Recipe.counter += 1
+    Recipe.counter = 0
+    task = task_manager.Task('hello.json', 'what/ever/hello.json', [], Recipe)
+    self.assertFalse(task.IsStatic())
+    self.assertFalse(task._is_done)
+    self.assertEqual(0, Recipe.counter)
+    task.Execute()
+    self.assertEqual(1, Recipe.counter)
+    task.Execute()
+    self.assertEqual(1, Recipe.counter)
+
+  def testDynamicTaskWithUnexecutedDeps(self):
+    def RecipeA():
+      self.fail()
+
+    def RecipeB():
+      RecipeB.counter += 1
+    RecipeB.counter = 0
+
+    a = task_manager.Task('hello.json', 'out/hello.json', [], RecipeA)
+    b = task_manager.Task('hello.json', 'out/hello.json', [a], RecipeB)
+    self.assertEqual(0, RecipeB.counter)
+    b.Execute()
+    self.assertEqual(1, RecipeB.counter)
+
+
+class BuilderTest(TaskManagerTestCase):
+  def testCreateUnexistingStaticTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    with self.assertRaises(task_manager.TaskError):
+      builder.CreateStaticTask('hello.txt', '/__unexisting/file/path')
+
+  def testCreateStaticTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    task = builder.CreateStaticTask('hello.py', __file__)
+    self.assertTrue(task.IsStatic())
+
+  def testDuplicateStaticTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    builder.CreateStaticTask('hello.py', __file__)
+    with self.assertRaises(task_manager.TaskError):
+      builder.CreateStaticTask('hello.py', __file__)
+
+  def testRegisterTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('hello.txt')
+    def TaskA():
+      TaskA.executed = True
+    TaskA.executed = False
+    self.assertFalse(TaskA.IsStatic())
+    self.assertEqual(os.path.join(self.output_directory, 'hello.txt'),
+                     TaskA.path)
+    self.assertFalse(TaskA.executed)
+    TaskA.Execute()
+    self.assertTrue(TaskA.executed)
+
+  def testRegisterDuplicateTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('hello.txt')
+    def TaskA():
+      pass
+    del TaskA # unused
+    with self.assertRaises(task_manager.TaskError):
+      @builder.RegisterTask('hello.txt')
+      def TaskB():
+        pass
+      del TaskB # unused
+
+  def testTaskMerging(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('hello.txt')
+    def TaskA():
+      pass
+    @builder.RegisterTask('hello.txt', merge=True)
+    def TaskB():
+      pass
+    self.assertEqual(TaskA, TaskB)
+
+  def testStaticTaskMergingError(self):
+    builder = task_manager.Builder(self.output_directory)
+    builder.CreateStaticTask('hello.py', __file__)
+    with self.assertRaises(task_manager.TaskError):
+      @builder.RegisterTask('hello.py', merge=True)
+      def TaskA():
+        pass
+      del TaskA # unused
+
+
+class GenerateScenarioTest(TaskManagerTestCase):
+  def testParents(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[TaskA])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskB])
+    def TaskC():
+      pass
+    scenario = task_manager.GenerateScenario([TaskA, TaskB, TaskC], set())
+    self.assertListEqual([TaskA, TaskB, TaskC], scenario)
+
+    scenario = task_manager.GenerateScenario([TaskB], set())
+    self.assertListEqual([TaskA, TaskB], scenario)
+
+    scenario = task_manager.GenerateScenario([TaskC], set())
+    self.assertListEqual([TaskA, TaskB, TaskC], scenario)
+
+    scenario = task_manager.GenerateScenario([TaskC, TaskB], set())
+    self.assertListEqual([TaskA, TaskB, TaskC], scenario)
+
+  def testFreezing(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[TaskA])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c')
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskB, TaskC])
+    def TaskD():
+      pass
+
+    # assert no exception raised.
+    task_manager.GenerateScenario([TaskB], set([TaskC]))
+
+    with self.assertRaises(task_manager.TaskError):
+      task_manager.GenerateScenario([TaskD], set([TaskA]))
+
+    self.TouchOutputFile('a')
+    scenario = task_manager.GenerateScenario([TaskD], set([TaskA]))
+    self.assertListEqual([TaskB, TaskC, TaskD], scenario)
+
+    self.TouchOutputFile('b')
+    scenario = task_manager.GenerateScenario([TaskD], set([TaskB]))
+    self.assertListEqual([TaskC, TaskD], scenario)
+
+  def testCycleError(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[TaskA])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskB])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskC])
+    def TaskD():
+      pass
+    TaskA._dependencies.append(TaskC)
+    with self.assertRaises(task_manager.TaskError):
+      task_manager.GenerateScenario([TaskD], set())
+
+  def testCollisionError(self):
+    builder_a = task_manager.Builder(self.output_directory)
+    builder_b = task_manager.Builder(self.output_directory)
+    @builder_a.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder_b.RegisterTask('a')
+    def TaskB():
+      pass
+    with self.assertRaises(task_manager.TaskError):
+      task_manager.GenerateScenario([TaskA, TaskB], set())
+
+  def testGraphVizOutput(self):
+    builder = task_manager.Builder(self.output_directory)
+    static_task = builder.CreateStaticTask('a', __file__)
+    @builder.RegisterTask('b')
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskB, static_task])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskC])
+    def TaskD():
+      pass
+    @builder.RegisterTask('e')
+    def TaskE():
+      pass
+    @builder.RegisterTask('f', dependencies=[TaskD, TaskE])
+    def TaskF():
+      pass
+    self.TouchOutputFile('e')
+    scenario = task_manager.GenerateScenario([TaskF], set([TaskE]))
+    output = StringIO.StringIO()
+    task_manager.OutputGraphViz(scenario, [TaskF], output)
+    self.assertEqual(_GOLDEN_GRAPHVIZ, output.getvalue())
+
+
+class CommandLineControlledExecutionTest(TaskManagerTestCase):
+  def Execute(self, *command_line_args):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b')
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskA, TaskB])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskA])
+    def TaskD():
+      pass
+    @builder.RegisterTask('e', dependencies=[TaskC])
+    def TaskE():
+      pass
+    @builder.RegisterTask('raise_exception', dependencies=[TaskB])
+    def TaskF():
+      raise Exception('Expected error.')
+
+    default_final_tasks = [TaskD, TaskE]
+    parser = task_manager.CommandLineParser()
+    cmd = ['-o', self.output_directory]
+    cmd.extend([i for i in command_line_args])
+    args = parser.parse_args(cmd)
+    return task_manager.ExecuteWithCommandLine(
+        args, [TaskA, TaskB, TaskC, TaskD, TaskE, TaskF], default_final_tasks)
+
+  def testSimple(self):
+    self.assertEqual(0, self.Execute())
+
+  def testDryRun(self):
+    self.assertEqual(0, self.Execute('-d'))
+
+  def testRegex(self):
+    self.assertEqual(0, self.Execute('-e', 'b', 'd'))
+    self.assertEqual(1, self.Execute('-e', r'\d'))
+
+  def testFreezing(self):
+    self.assertEqual(0, self.Execute('-f', r'\d'))
+    self.TouchOutputFile('c')
+    self.assertEqual(0, self.Execute('-f', 'c'))
+
+  def testTaskFailure(self):
+    with self.assertRaisesRegexp(Exception, r'^Expected error\.$'):
+      self.Execute('-e', 'raise_exception')
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 3588c91b93bd9b72853ecff2dc59a300da7f0ea2
Author: dcheng <dcheng@chromium.org>
Date:   Fri Apr 8 12:55:42 2016 -0700

    Convert //tools to use std::unique_ptr
    
    BUG=554298
    
    Review URL: https://codereview.chromium.org/1869503004
    
    Cr-Original-Commit-Position: refs/heads/master@{#386168}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a500b69aa06c2bc084a3b1867b9824f4218014de

diff --git a/forwarder2/daemon.cc b/forwarder2/daemon.cc
index e5ebe07..1ec390d 100644
--- a/forwarder2/daemon.cc
+++ b/forwarder2/daemon.cc
@@ -13,15 +13,16 @@
 #include <sys/types.h>
 #include <sys/wait.h>
 #include <unistd.h>
+
 #include <cstdlib>
 #include <cstring>
+#include <memory>
 #include <string>
 #include <utility>
 
 #include "base/files/file_path.h"
 #include "base/files/file_util.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/posix/eintr_wrapper.h"
 #include "base/strings/string_number_conversions.h"
 #include "base/strings/stringprintf.h"
@@ -53,7 +54,7 @@ bool RunServerAcceptLoop(const std::string& welcome_message,
                          Daemon::ServerDelegate* server_delegate) {
   bool failed = false;
   for (;;) {
-    scoped_ptr<Socket> client_socket(new Socket());
+    std::unique_ptr<Socket> client_socket(new Socket());
     if (!server_socket->Accept(client_socket.get())) {
       if (server_socket->DidReceiveEvent())
         break;
@@ -97,13 +98,13 @@ void SigChildHandler(int signal_number) {
   SIGNAL_SAFE_LOG(ERROR, string_builder.buffer());
 }
 
-scoped_ptr<Socket> ConnectToUnixDomainSocket(
+std::unique_ptr<Socket> ConnectToUnixDomainSocket(
     const std::string& socket_name,
     int tries_count,
     int idle_time_msec,
     const std::string& expected_welcome_message) {
   for (int i = 0; i < tries_count; ++i) {
-    scoped_ptr<Socket> socket(new Socket());
+    std::unique_ptr<Socket> socket(new Socket());
     if (!socket->ConnectUnix(socket_name)) {
       if (idle_time_msec)
         usleep(idle_time_msec * 1000);
@@ -122,7 +123,7 @@ scoped_ptr<Socket> ConnectToUnixDomainSocket(
     }
     return socket;
   }
-  return scoped_ptr<Socket>();
+  return nullptr;
 }
 
 }  // namespace
@@ -147,7 +148,7 @@ Daemon::~Daemon() {}
 bool Daemon::SpawnIfNeeded() {
   const int kSingleTry = 1;
   const int kNoIdleTime = 0;
-  scoped_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
+  std::unique_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
       identifier_, kSingleTry, kNoIdleTime, identifier_);
   if (!client_socket) {
     switch (fork()) {
@@ -170,7 +171,7 @@ bool Daemon::SpawnIfNeeded() {
         CHECK_EQ(dup(null_fd), STDERR_FILENO);
         Socket command_socket;
         if (!command_socket.BindUnix(identifier_)) {
-          scoped_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
+          std::unique_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
               identifier_, kSingleTry, kNoIdleTime, identifier_);
           if (client_socket.get()) {
             // The daemon was spawned by a concurrent process.
diff --git a/forwarder2/daemon.h b/forwarder2/daemon.h
index f983a2e..54285df 100644
--- a/forwarder2/daemon.h
+++ b/forwarder2/daemon.h
@@ -5,10 +5,10 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_DAEMON_H_
 #define TOOLS_ANDROID_FORWARDER2_DAEMON_H_
 
+#include <memory>
 #include <string>
 
 #include "base/macros.h"
-#include "base/memory/scoped_ptr.h"
 
 namespace forwarder2 {
 
@@ -37,7 +37,7 @@ class Daemon {
     // setup signal handlers or perform global initialization.
     virtual void Init() = 0;
 
-    virtual void OnClientConnected(scoped_ptr<Socket> client_socket) = 0;
+    virtual void OnClientConnected(std::unique_ptr<Socket> client_socket) = 0;
   };
 
   // |identifier| should be a unique string identifier. It is used to
diff --git a/forwarder2/device_controller.cc b/forwarder2/device_controller.cc
index 7ad38d9..a3c5505 100644
--- a/forwarder2/device_controller.cc
+++ b/forwarder2/device_controller.cc
@@ -4,12 +4,12 @@
 
 #include "tools/android/forwarder2/device_controller.h"
 
+#include <memory>
 #include <utility>
 
 #include "base/bind.h"
 #include "base/callback_helpers.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/single_thread_task_runner.h"
 #include "base/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
@@ -20,11 +20,11 @@
 namespace forwarder2 {
 
 // static
-scoped_ptr<DeviceController> DeviceController::Create(
+std::unique_ptr<DeviceController> DeviceController::Create(
     const std::string& adb_unix_socket,
     int exit_notifier_fd) {
-  scoped_ptr<DeviceController> device_controller;
-  scoped_ptr<Socket> host_socket(new Socket());
+  std::unique_ptr<DeviceController> device_controller;
+  std::unique_ptr<Socket> host_socket(new Socket());
   if (!host_socket->BindUnix(adb_unix_socket)) {
     PLOG(ERROR) << "Could not BindAndListen DeviceController socket on port "
                 << adb_unix_socket << ": ";
@@ -44,7 +44,7 @@ void DeviceController::Start() {
   AcceptHostCommandSoon();
 }
 
-DeviceController::DeviceController(scoped_ptr<Socket> host_socket,
+DeviceController::DeviceController(std::unique_ptr<Socket> host_socket,
                                    int exit_notifier_fd)
     : host_socket_(std::move(host_socket)),
       exit_notifier_fd_(exit_notifier_fd),
@@ -60,7 +60,7 @@ void DeviceController::AcceptHostCommandSoon() {
 }
 
 void DeviceController::AcceptHostCommandInternal() {
-  scoped_ptr<Socket> socket(new Socket);
+  std::unique_ptr<Socket> socket(new Socket);
   if (!host_socket_->Accept(socket.get())) {
     if (!host_socket_->DidReceiveEvent())
       PLOG(ERROR) << "Could not Accept DeviceController socket";
@@ -89,7 +89,7 @@ void DeviceController::AcceptHostCommandInternal() {
                      << ". Attempting to restart the listener.\n";
         DeleteRefCountedValueInMapFromIterator(listener_it, &listeners_);
       }
-      scoped_ptr<DeviceListener> new_listener(DeviceListener::Create(
+      std::unique_ptr<DeviceListener> new_listener(DeviceListener::Create(
           std::move(socket), port,
           base::Bind(&DeviceController::DeleteListenerOnError,
                      weak_ptr_factory_.GetWeakPtr())));
@@ -136,8 +136,8 @@ void DeviceController::AcceptHostCommandInternal() {
 
 // static
 void DeviceController::DeleteListenerOnError(
-      const base::WeakPtr<DeviceController>& device_controller_ptr,
-      scoped_ptr<DeviceListener> device_listener) {
+    const base::WeakPtr<DeviceController>& device_controller_ptr,
+    std::unique_ptr<DeviceListener> device_listener) {
   DeviceListener* const listener = device_listener.release();
   DeviceController* const controller = device_controller_ptr.get();
   if (!controller) {
diff --git a/forwarder2/device_controller.h b/forwarder2/device_controller.h
index 5a31e7b..28fbea4 100644
--- a/forwarder2/device_controller.h
+++ b/forwarder2/device_controller.h
@@ -5,13 +5,13 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_DEVICE_CONTROLLER_H_
 #define TOOLS_ANDROID_FORWARDER2_DEVICE_CONTROLLER_H_
 
+#include <memory>
 #include <string>
 
 #include "base/containers/hash_tables.h"
 #include "base/macros.h"
 #include "base/memory/linked_ptr.h"
 #include "base/memory/ref_counted.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/memory/weak_ptr.h"
 #include "tools/android/forwarder2/socket.h"
 
@@ -28,8 +28,9 @@ class DeviceListener;
 // DeviceListener each).
 class DeviceController {
  public:
-  static scoped_ptr<DeviceController> Create(const std::string& adb_unix_socket,
-                                             int exit_notifier_fd);
+  static std::unique_ptr<DeviceController> Create(
+      const std::string& adb_unix_socket,
+      int exit_notifier_fd);
   ~DeviceController();
 
   void Start();
@@ -38,7 +39,7 @@ class DeviceController {
   typedef base::hash_map<
       int /* port */, linked_ptr<DeviceListener> > ListenersMap;
 
-  DeviceController(scoped_ptr<Socket> host_socket, int exit_notifier_fd);
+  DeviceController(std::unique_ptr<Socket> host_socket, int exit_notifier_fd);
 
   void AcceptHostCommandSoon();
   void AcceptHostCommandInternal();
@@ -47,9 +48,9 @@ class DeviceController {
   // destroyed which is why a weak pointer is used.
   static void DeleteListenerOnError(
       const base::WeakPtr<DeviceController>& device_controller_ptr,
-      scoped_ptr<DeviceListener> device_listener);
+      std::unique_ptr<DeviceListener> device_listener);
 
-  const scoped_ptr<Socket> host_socket_;
+  const std::unique_ptr<Socket> host_socket_;
   // Used to notify the controller to exit.
   const int exit_notifier_fd_;
   // Lets ensure DeviceListener instances are deleted on the thread they were
diff --git a/forwarder2/device_forwarder_main.cc b/forwarder2/device_forwarder_main.cc
index 09790f9..af796b3 100644
--- a/forwarder2/device_forwarder_main.cc
+++ b/forwarder2/device_forwarder_main.cc
@@ -67,7 +67,7 @@ class ServerDelegate : public Daemon::ServerDelegate {
     controller_thread_->Start();
   }
 
-  void OnClientConnected(scoped_ptr<Socket> client_socket) override {
+  void OnClientConnected(std::unique_ptr<Socket> client_socket) override {
     if (initialized_) {
       client_socket->WriteString("OK");
       return;
@@ -80,9 +80,10 @@ class ServerDelegate : public Daemon::ServerDelegate {
   }
 
  private:
-  void StartController(int exit_notifier_fd, scoped_ptr<Socket> client_socket) {
+  void StartController(int exit_notifier_fd,
+                       std::unique_ptr<Socket> client_socket) {
     DCHECK(!controller_.get());
-    scoped_ptr<DeviceController> controller(
+    std::unique_ptr<DeviceController> controller(
         DeviceController::Create(kUnixDomainSocketPath, exit_notifier_fd));
     if (!controller.get()) {
       client_socket->WriteString(
@@ -97,8 +98,8 @@ class ServerDelegate : public Daemon::ServerDelegate {
     client_socket->Close();
   }
 
-  scoped_ptr<DeviceController> controller_;
-  scoped_ptr<base::Thread> controller_thread_;
+  std::unique_ptr<DeviceController> controller_;
+  std::unique_ptr<base::Thread> controller_thread_;
   bool initialized_;
 };
 
diff --git a/forwarder2/device_listener.cc b/forwarder2/device_listener.cc
index 9825c3c..98459f0 100644
--- a/forwarder2/device_listener.cc
+++ b/forwarder2/device_listener.cc
@@ -4,13 +4,13 @@
 
 #include "tools/android/forwarder2/device_listener.h"
 
+#include <memory>
 #include <utility>
 
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/callback.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/single_thread_task_runner.h"
 #include "base/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
@@ -20,12 +20,12 @@
 namespace forwarder2 {
 
 // static
-scoped_ptr<DeviceListener> DeviceListener::Create(
-    scoped_ptr<Socket> host_socket,
+std::unique_ptr<DeviceListener> DeviceListener::Create(
+    std::unique_ptr<Socket> host_socket,
     int listener_port,
     const ErrorCallback& error_callback) {
-  scoped_ptr<Socket> listener_socket(new Socket());
-  scoped_ptr<DeviceListener> device_listener;
+  std::unique_ptr<Socket> listener_socket(new Socket());
+  std::unique_ptr<DeviceListener> device_listener;
   if (!listener_socket->BindTcp("", listener_port)) {
     LOG(ERROR) << "Device could not bind and listen to local port "
                << listener_port;
@@ -52,15 +52,15 @@ void DeviceListener::Start() {
   AcceptNextClientSoon();
 }
 
-void DeviceListener::SetAdbDataSocket(scoped_ptr<Socket> adb_data_socket) {
+void DeviceListener::SetAdbDataSocket(std::unique_ptr<Socket> adb_data_socket) {
   thread_.task_runner()->PostTask(
       FROM_HERE,
       base::Bind(&DeviceListener::OnAdbDataSocketReceivedOnInternalThread,
                  base::Unretained(this), base::Passed(&adb_data_socket)));
 }
 
-DeviceListener::DeviceListener(scoped_ptr<Socket> listener_socket,
-                               scoped_ptr<Socket> host_socket,
+DeviceListener::DeviceListener(std::unique_ptr<Socket> listener_socket,
+                               std::unique_ptr<Socket> host_socket,
                                int port,
                                const ErrorCallback& error_callback)
     : self_deleter_helper_(this, error_callback),
@@ -116,7 +116,7 @@ void DeviceListener::AcceptClientOnInternalThread() {
 }
 
 void DeviceListener::OnAdbDataSocketReceivedOnInternalThread(
-    scoped_ptr<Socket> adb_data_socket) {
+    std::unique_ptr<Socket> adb_data_socket) {
   DCHECK(adb_data_socket);
   SendCommand(command::ADB_DATA_SOCKET_SUCCESS, listener_port_,
               host_socket_.get());
diff --git a/forwarder2/device_listener.h b/forwarder2/device_listener.h
index 2b1a5b2..c235518 100644
--- a/forwarder2/device_listener.h
+++ b/forwarder2/device_listener.h
@@ -5,12 +5,13 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_DEVICE_LISTENER_H_
 #define TOOLS_ANDROID_FORWARDER2_DEVICE_LISTENER_H_
 
+#include <memory>
+
 #include "base/callback.h"
 #include "base/compiler_specific.h"
 #include "base/logging.h"
 #include "base/macros.h"
 #include "base/memory/ref_counted.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/threading/thread.h"
 #include "tools/android/forwarder2/forwarders_manager.h"
 #include "tools/android/forwarder2/pipe_notifier.h"
@@ -45,23 +46,24 @@ class DeviceListener {
   // Callback that is used for self-deletion on error to let the device
   // controller perform some additional cleanup work (e.g. removing the device
   // listener instance from its internal map before deleting it).
-  typedef base::Callback<void (scoped_ptr<DeviceListener>)> ErrorCallback;
+  typedef base::Callback<void(std::unique_ptr<DeviceListener>)> ErrorCallback;
 
-  static scoped_ptr<DeviceListener> Create(scoped_ptr<Socket> host_socket,
-                                           int port,
-                                           const ErrorCallback& error_callback);
+  static std::unique_ptr<DeviceListener> Create(
+      std::unique_ptr<Socket> host_socket,
+      int port,
+      const ErrorCallback& error_callback);
 
   ~DeviceListener();
 
   void Start();
 
-  void SetAdbDataSocket(scoped_ptr<Socket> adb_data_socket);
+  void SetAdbDataSocket(std::unique_ptr<Socket> adb_data_socket);
 
   int listener_port() const { return listener_port_; }
 
  private:
-  DeviceListener(scoped_ptr<Socket> listener_socket,
-                 scoped_ptr<Socket> host_socket,
+  DeviceListener(std::unique_ptr<Socket> listener_socket,
+                 std::unique_ptr<Socket> host_socket,
                  int port,
                  const ErrorCallback& error_callback);
 
@@ -72,7 +74,7 @@ class DeviceListener {
   void AcceptClientOnInternalThread();
 
   void OnAdbDataSocketReceivedOnInternalThread(
-      scoped_ptr<Socket> adb_data_socket);
+      std::unique_ptr<Socket> adb_data_socket);
 
   void OnInternalThreadError();
 
@@ -87,10 +89,10 @@ class DeviceListener {
   PipeNotifier deletion_notifier_;
   // The local device listener socket for accepting connections from the local
   // port (listener_port_).
-  const scoped_ptr<Socket> listener_socket_;
+  const std::unique_ptr<Socket> listener_socket_;
   // The listener socket for sending control commands.
-  const scoped_ptr<Socket> host_socket_;
-  scoped_ptr<Socket> device_data_socket_;
+  const std::unique_ptr<Socket> host_socket_;
+  std::unique_ptr<Socket> device_data_socket_;
   const int listener_port_;
   // Task runner used for deletion set at construction time (i.e. the object is
   // deleted on the same thread it is created on).
diff --git a/forwarder2/forwarder.cc b/forwarder2/forwarder.cc
index 9945674..3c49eef 100644
--- a/forwarder2/forwarder.cc
+++ b/forwarder2/forwarder.cc
@@ -223,7 +223,8 @@ class Forwarder::BufferedCopier {
   DISALLOW_COPY_AND_ASSIGN(BufferedCopier);
 };
 
-Forwarder::Forwarder(scoped_ptr<Socket> socket1, scoped_ptr<Socket> socket2)
+Forwarder::Forwarder(std::unique_ptr<Socket> socket1,
+                     std::unique_ptr<Socket> socket2)
     : socket1_(std::move(socket1)),
       socket2_(std::move(socket2)),
       buffer1_(new BufferedCopier(socket1_.get(), socket2_.get())),
diff --git a/forwarder2/forwarder.h b/forwarder2/forwarder.h
index 0be86fc..857babd 100644
--- a/forwarder2/forwarder.h
+++ b/forwarder2/forwarder.h
@@ -7,7 +7,8 @@
 
 #include <sys/select.h>
 
-#include "base/memory/scoped_ptr.h"
+#include <memory>
+
 #include "base/threading/thread_checker.h"
 
 namespace forwarder2 {
@@ -18,7 +19,7 @@ class Socket;
 // that this class is not thread-safe.
 class Forwarder {
  public:
-  Forwarder(scoped_ptr<Socket> socket1, scoped_ptr<Socket> socket2);
+  Forwarder(std::unique_ptr<Socket> socket1, std::unique_ptr<Socket> socket2);
 
   ~Forwarder();
 
@@ -34,12 +35,12 @@ class Forwarder {
   class BufferedCopier;
 
   base::ThreadChecker thread_checker_;
-  const scoped_ptr<Socket> socket1_;
-  const scoped_ptr<Socket> socket2_;
+  const std::unique_ptr<Socket> socket1_;
+  const std::unique_ptr<Socket> socket2_;
   // Copies data from socket1 to socket2.
-  const scoped_ptr<BufferedCopier> buffer1_;
+  const std::unique_ptr<BufferedCopier> buffer1_;
   // Copies data from socket2 to socket1.
-  const scoped_ptr<BufferedCopier> buffer2_;
+  const std::unique_ptr<BufferedCopier> buffer2_;
 };
 
 }  // namespace forwarder2
diff --git a/forwarder2/forwarders_manager.cc b/forwarder2/forwarders_manager.cc
index fb6890f..c3dd026 100644
--- a/forwarder2/forwarders_manager.cc
+++ b/forwarder2/forwarders_manager.cc
@@ -31,8 +31,9 @@ ForwardersManager::~ForwardersManager() {
   deletion_notifier_.Notify();
 }
 
-void ForwardersManager::CreateAndStartNewForwarder(scoped_ptr<Socket> socket1,
-                                                   scoped_ptr<Socket> socket2) {
+void ForwardersManager::CreateAndStartNewForwarder(
+    std::unique_ptr<Socket> socket1,
+    std::unique_ptr<Socket> socket2) {
   // Note that the internal Forwarder vector is populated on the internal thread
   // which is the only thread from which it's accessed.
   thread_.task_runner()->PostTask(
@@ -47,8 +48,8 @@ void ForwardersManager::CreateAndStartNewForwarder(scoped_ptr<Socket> socket1,
 }
 
 void ForwardersManager::CreateNewForwarderOnInternalThread(
-    scoped_ptr<Socket> socket1,
-    scoped_ptr<Socket> socket2) {
+    std::unique_ptr<Socket> socket1,
+    std::unique_ptr<Socket> socket2) {
   DCHECK(thread_.task_runner()->RunsTasksOnCurrentThread());
   forwarders_.push_back(new Forwarder(std::move(socket1), std::move(socket2)));
 }
diff --git a/forwarder2/forwarders_manager.h b/forwarder2/forwarders_manager.h
index 4c6dea6..2cdafdb 100644
--- a/forwarder2/forwarders_manager.h
+++ b/forwarder2/forwarders_manager.h
@@ -5,7 +5,8 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_FORWARDERS_MANAGER_H_
 #define TOOLS_ANDROID_FORWARDER2_FORWARDERS_MANAGER_H_
 
-#include "base/memory/scoped_ptr.h"
+#include <memory>
+
 #include "base/memory/scoped_vector.h"
 #include "base/threading/thread.h"
 #include "tools/android/forwarder2/pipe_notifier.h"
@@ -24,12 +25,12 @@ class ForwardersManager {
   ~ForwardersManager();
 
   // Can be called on any thread.
-  void CreateAndStartNewForwarder(scoped_ptr<Socket> socket1,
-                                  scoped_ptr<Socket> socket2);
+  void CreateAndStartNewForwarder(std::unique_ptr<Socket> socket1,
+                                  std::unique_ptr<Socket> socket2);
 
  private:
-  void CreateNewForwarderOnInternalThread(scoped_ptr<Socket> socket1,
-                                          scoped_ptr<Socket> socket2);
+  void CreateNewForwarderOnInternalThread(std::unique_ptr<Socket> socket1,
+                                          std::unique_ptr<Socket> socket2);
 
   void WaitForEventsOnInternalThreadSoon();
   void WaitForEventsOnInternalThread();
diff --git a/forwarder2/host_controller.cc b/forwarder2/host_controller.cc
index 68394df..0510890 100644
--- a/forwarder2/host_controller.cc
+++ b/forwarder2/host_controller.cc
@@ -4,13 +4,13 @@
 
 #include "tools/android/forwarder2/host_controller.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/forwarder.h"
@@ -19,15 +19,15 @@
 namespace forwarder2 {
 
 // static
-scoped_ptr<HostController> HostController::Create(
+std::unique_ptr<HostController> HostController::Create(
     int device_port,
     int host_port,
     int adb_port,
     int exit_notifier_fd,
     const ErrorCallback& error_callback) {
-  scoped_ptr<HostController> host_controller;
-  scoped_ptr<PipeNotifier> delete_controller_notifier(new PipeNotifier());
-  scoped_ptr<Socket> adb_control_socket(new Socket());
+  std::unique_ptr<HostController> host_controller;
+  std::unique_ptr<PipeNotifier> delete_controller_notifier(new PipeNotifier());
+  std::unique_ptr<Socket> adb_control_socket(new Socket());
   adb_control_socket->AddEventFd(exit_notifier_fd);
   adb_control_socket->AddEventFd(delete_controller_notifier->receiver_fd());
   if (!adb_control_socket->ConnectTcp(std::string(), adb_port)) {
@@ -70,8 +70,8 @@ HostController::HostController(
     int adb_port,
     int exit_notifier_fd,
     const ErrorCallback& error_callback,
-    scoped_ptr<Socket> adb_control_socket,
-    scoped_ptr<PipeNotifier> delete_controller_notifier)
+    std::unique_ptr<Socket> adb_control_socket,
+    std::unique_ptr<PipeNotifier> delete_controller_notifier)
     : self_deleter_helper_(this, error_callback),
       device_port_(device_port),
       host_port_(host_port),
@@ -97,7 +97,7 @@ void HostController::ReadCommandOnInternalThread() {
     return;
   }
   // Try to connect to host server.
-  scoped_ptr<Socket> host_server_data_socket(new Socket());
+  std::unique_ptr<Socket> host_server_data_socket(new Socket());
   if (!host_server_data_socket->ConnectTcp(std::string(), host_port_)) {
     LOG(ERROR) << "Could not Connect HostServerData socket on port: "
                << host_port_;
@@ -121,8 +121,8 @@ void HostController::ReadCommandOnInternalThread() {
 }
 
 void HostController::StartForwarder(
-    scoped_ptr<Socket> host_server_data_socket) {
-  scoped_ptr<Socket> adb_data_socket(new Socket());
+    std::unique_ptr<Socket> host_server_data_socket) {
+  std::unique_ptr<Socket> adb_data_socket(new Socket());
   if (!adb_data_socket->ConnectTcp("", adb_port_)) {
     LOG(ERROR) << "Could not connect AdbDataSocket on port: " << adb_port_;
     OnInternalThreadError();
diff --git a/forwarder2/host_controller.h b/forwarder2/host_controller.h
index 484adac..7328245 100644
--- a/forwarder2/host_controller.h
+++ b/forwarder2/host_controller.h
@@ -5,12 +5,12 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_HOST_CONTROLLER_H_
 #define TOOLS_ANDROID_FORWARDER2_HOST_CONTROLLER_H_
 
+#include <memory>
 #include <string>
 
 #include "base/callback.h"
 #include "base/compiler_specific.h"
 #include "base/macros.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/memory/weak_ptr.h"
 #include "base/threading/thread.h"
 #include "tools/android/forwarder2/forwarders_manager.h"
@@ -34,15 +34,16 @@ class HostController {
  public:
   // Callback used for self-deletion when an error happens so that the client
   // can perform some cleanup work before deleting the HostController instance.
-  typedef base::Callback<void (scoped_ptr<HostController>)> ErrorCallback;
+  typedef base::Callback<void(std::unique_ptr<HostController>)> ErrorCallback;
 
   // If |device_port| is zero then a dynamic port is allocated (and retrievable
   // through device_port() below).
-  static scoped_ptr<HostController> Create(int device_port,
-                                           int host_port,
-                                           int adb_port,
-                                           int exit_notifier_fd,
-                                           const ErrorCallback& error_callback);
+  static std::unique_ptr<HostController> Create(
+      int device_port,
+      int host_port,
+      int adb_port,
+      int exit_notifier_fd,
+      const ErrorCallback& error_callback);
 
   ~HostController();
 
@@ -59,13 +60,13 @@ class HostController {
                  int adb_port,
                  int exit_notifier_fd,
                  const ErrorCallback& error_callback,
-                 scoped_ptr<Socket> adb_control_socket,
-                 scoped_ptr<PipeNotifier> delete_controller_notifier);
+                 std::unique_ptr<Socket> adb_control_socket,
+                 std::unique_ptr<PipeNotifier> delete_controller_notifier);
 
   void ReadNextCommandSoon();
   void ReadCommandOnInternalThread();
 
-  void StartForwarder(scoped_ptr<Socket> host_server_data_socket);
+  void StartForwarder(std::unique_ptr<Socket> host_server_data_socket);
 
   // Note that this gets also called when ~HostController() is invoked.
   void OnInternalThreadError();
@@ -78,10 +79,10 @@ class HostController {
   const int adb_port_;
   // Used to notify the controller when the process is killed.
   const int global_exit_notifier_fd_;
-  scoped_ptr<Socket> adb_control_socket_;
+  std::unique_ptr<Socket> adb_control_socket_;
   // Used to cancel the pending blocking IO operations when the host controller
   // instance is deleted.
-  scoped_ptr<PipeNotifier> delete_controller_notifier_;
+  std::unique_ptr<PipeNotifier> delete_controller_notifier_;
   // Task runner used for deletion set at deletion time (i.e. the object is
   // deleted on the same thread it is created on).
   const scoped_refptr<base::SingleThreadTaskRunner> deletion_task_runner_;
diff --git a/forwarder2/host_forwarder_main.cc b/forwarder2/host_forwarder_main.cc
index 04684f0..2fae616 100644
--- a/forwarder2/host_forwarder_main.cc
+++ b/forwarder2/host_forwarder_main.cc
@@ -102,7 +102,7 @@ class HostControllersManager {
                      const std::string& device_serial,
                      int device_port,
                      int host_port,
-                     scoped_ptr<Socket> client_socket) {
+                     std::unique_ptr<Socket> client_socket) {
     // Lazy initialize so that the CLI process doesn't get this thread created.
     InitOnce();
     thread_->task_runner()->PostTask(
@@ -135,7 +135,7 @@ class HostControllersManager {
   // controller manager was destroyed which is why a weak pointer is used.
   static void DeleteHostController(
       const base::WeakPtr<HostControllersManager>& manager_ptr,
-      scoped_ptr<HostController> host_controller) {
+      std::unique_ptr<HostController> host_controller) {
     HostController* const controller = host_controller.release();
     HostControllersManager* const manager = manager_ptr.get();
     if (!manager) {
@@ -156,7 +156,7 @@ class HostControllersManager {
                                      const std::string& device_serial,
                                      int device_port,
                                      int host_port,
-                                     scoped_ptr<Socket> client_socket) {
+                                     std::unique_ptr<Socket> client_socket) {
     const int adb_port = GetAdbPortForDevice(adb_path, device_serial);
     if (adb_port < 0) {
       SendMessage(
@@ -198,11 +198,10 @@ class HostControllersManager {
       }
     }
     // Create a new host controller.
-    scoped_ptr<HostController> host_controller(
-        HostController::Create(
-            device_port, host_port, adb_port, GetExitNotifierFD(),
-            base::Bind(&HostControllersManager::DeleteHostController,
-                       weak_ptr_factory_.GetWeakPtr())));
+    std::unique_ptr<HostController> host_controller(HostController::Create(
+        device_port, host_port, adb_port, GetExitNotifierFD(),
+        base::Bind(&HostControllersManager::DeleteHostController,
+                   weak_ptr_factory_.GetWeakPtr())));
     if (!host_controller.get()) {
       has_failed_ = true;
       SendMessage("ERROR: Connection to device failed.", client_socket.get());
@@ -222,7 +221,7 @@ class HostControllersManager {
                        linked_ptr<HostController>(host_controller.release())));
   }
 
-  void LogExistingControllers(const scoped_ptr<Socket>& client_socket) {
+  void LogExistingControllers(const std::unique_ptr<Socket>& client_socket) {
     SendMessage("ERROR: Existing controllers:", client_socket.get());
     for (const auto& controller : *controllers_) {
       SendMessage(base::StringPrintf("ERROR:   %s", controller.first.c_str()),
@@ -309,10 +308,11 @@ class HostControllersManager {
   }
 
   base::hash_map<std::string, int> device_serial_to_adb_port_map_;
-  scoped_ptr<HostControllerMap> controllers_;
+  std::unique_ptr<HostControllerMap> controllers_;
   bool has_failed_;
-  scoped_ptr<base::AtExitManager> at_exit_manager_;  // Needed by base::Thread.
-  scoped_ptr<base::Thread> thread_;
+  std::unique_ptr<base::AtExitManager>
+      at_exit_manager_;  // Needed by base::Thread.
+  std::unique_ptr<base::Thread> thread_;
   base::WeakPtrFactory<HostControllersManager> weak_ptr_factory_;
 };
 
@@ -334,7 +334,7 @@ class ServerDelegate : public Daemon::ServerDelegate {
     signal(SIGINT, KillHandler);
   }
 
-  void OnClientConnected(scoped_ptr<Socket> client_socket) override {
+  void OnClientConnected(std::unique_ptr<Socket> client_socket) override {
     char buf[kBufSize];
     const int bytes_read = client_socket->Read(buf, sizeof(buf));
     if (bytes_read <= 0) {
diff --git a/forwarder2/self_deleter_helper.h b/forwarder2/self_deleter_helper.h
index 9739bd0..089df03 100644
--- a/forwarder2/self_deleter_helper.h
+++ b/forwarder2/self_deleter_helper.h
@@ -5,13 +5,15 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_SELF_DELETER_HELPER_H_
 #define TOOLS_ANDROID_FORWARDER2_SELF_DELETER_HELPER_H_
 
+#include <memory>
+
 #include "base/bind.h"
 #include "base/callback.h"
 #include "base/location.h"
 #include "base/logging.h"
 #include "base/macros.h"
+#include "base/memory/ptr_util.h"
 #include "base/memory/ref_counted.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/memory/weak_ptr.h"
 #include "base/thread_task_runner_handle.h"
 
@@ -42,7 +44,7 @@ namespace forwarder2 {
 // Usage example:
 // class Object {
 //  public:
-//   typedef base::Callback<void (scoped_ptr<Object>)> ErrorCallback;
+//   typedef base::Callback<void (std::unique_ptr<Object>)> ErrorCallback;
 //
 //   Object(const ErrorCallback& error_callback)
 //       : self_deleter_helper_(this, error_callback) {
@@ -80,7 +82,7 @@ namespace forwarder2 {
 //   }
 //
 //  private:
-//   void DeleteObjectOnError(scoped_ptr<Object> object) {
+//   void DeleteObjectOnError(std::unique_ptr<Object> object) {
 //     DCHECK(thread_checker_.CalledOnValidThread());
 //     DCHECK_EQ(object_, object);
 //     // Do some extra work with |object| before it gets deleted...
@@ -89,13 +91,13 @@ namespace forwarder2 {
 //   }
 //
 //   base::ThreadChecker thread_checker_;
-//   scoped_ptr<Object> object_;
+//   std::unique_ptr<Object> object_;
 // };
 //
 template <typename T>
 class SelfDeleterHelper {
  public:
-  typedef base::Callback<void (scoped_ptr<T>)> DeletionCallback;
+  typedef base::Callback<void(std::unique_ptr<T>)> DeletionCallback;
 
   SelfDeleterHelper(T* self_deleting_object,
                     const DeletionCallback& deletion_callback)
@@ -119,7 +121,7 @@ class SelfDeleterHelper {
  private:
   void SelfDelete() {
     DCHECK(construction_runner_->RunsTasksOnCurrentThread());
-    deletion_callback_.Run(make_scoped_ptr(self_deleting_object_));
+    deletion_callback_.Run(base::WrapUnique(self_deleting_object_));
   }
 
   const scoped_refptr<base::SingleThreadTaskRunner> construction_runner_;
diff --git a/md5sum/md5sum.cc b/md5sum/md5sum.cc
index 94efa10..eaee434 100644
--- a/md5sum/md5sum.cc
+++ b/md5sum/md5sum.cc
@@ -9,6 +9,7 @@
 
 #include <fstream>
 #include <iostream>
+#include <memory>
 #include <set>
 #include <string>
 
@@ -17,7 +18,6 @@
 #include "base/files/file_util.h"
 #include "base/logging.h"
 #include "base/md5.h"
-#include "base/memory/scoped_ptr.h"
 
 namespace {
 
@@ -31,7 +31,7 @@ bool MD5Sum(const char* path, std::string* digest_string) {
   base::MD5Context ctx;
   base::MD5Init(&ctx);
   const size_t kBufferSize = 1 << 16;
-  scoped_ptr<char[]> buf(new char[kBufferSize]);
+  std::unique_ptr<char[]> buf(new char[kBufferSize]);
   size_t len;
   while ((len = fread(buf.get(), 1, kBufferSize, file.get())) > 0)
     base::MD5Update(&ctx, base::StringPiece(buf.get(), len));

commit 9d5d8d8c35e56cf10cc1b9a3a61df3a919985910
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 08:23:49 2016 -0700

    tools/android/loading GCE validation: Add delay after analyze failures
    
    Previously, when a adb connection error happenned it was typically
    causing several traces to fail in a row.
    This CL adds a 3 seconds delay when analyze fails, so that the adb
    connection has some time to recover.
    As a result, trace collection fails less often.
    
    Review URL: https://codereview.chromium.org/1873753002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386082}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5f90425cf39d2a58987cf409113d1aa32e9b8e24

diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
index b324a0b..3dc6c81 100755
--- a/loading/unmaintained/gce_validation_collect.sh
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -41,5 +41,9 @@ EOF
       --devtools_port 9222 \
       --url $site \
       --output $outdir/${output_subdir}/${run}
+   if [ $? -ne 0 ]; then
+    echo "Analyze failed. Wait a bit for device to recover."
+    sleep 3
+   fi
  done
 done

commit 7c85de8450c59c8f7e68a1de3edcc8451738bf8e
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 07:53:58 2016 -0700

    tools/android/loading Add argument to clear device data
    
    This option uses adb to clear all Chrome data from the device.
    The intent is to ensure that a fresh profile is used.
    
    Review URL: https://codereview.chromium.org/1869673003
    
    Cr-Original-Commit-Position: refs/heads/master@{#386074}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8770093f8cdc207286be2745eed7b42606fddeee

diff --git a/loading/controller.py b/loading/controller.py
index a0f6dff..5944711 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -216,6 +216,9 @@ class RemoteChromeController(ChromeControllerBase):
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
     self._device.KillAll(package_info.package, quiet=True)
+    if OPTIONS.clear_device_data:
+      logging.info('Clear Chrome data')
+      self._device.adb.Shell('pm clear ' + package_info.package)
     chrome_args = self._GetChromeArguments()
     logging.info('Launching %s with flags: %s' % (package_info.package,
         subprocess.list2cmdline(chrome_args)))
diff --git a/loading/options.py b/loading/options.py
index 021377c..3ea91b7 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -20,6 +20,8 @@ class Options(object):
   # Tuples of (argument name, default value, help string).
   _ARGS = [ ('clear_cache', True,
              'clear browser cache before loading'),
+            ('clear_device_data', False,
+             'Clear Chrome data from device before loading'),
             ('chrome_package_name', 'chrome',
              'build/android/pylib/constants package description'),
             ('devtools_hostname', 'localhost',
diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
index 41d8417..b324a0b 100755
--- a/loading/unmaintained/gce_validation_collect.sh
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -37,7 +37,7 @@ EOF
  for ((run=0;run<$repeat_count;++run)); do
    echo '****'  $run
    tools/android/loading/analyze.py log_requests \
-      --clear_cache \
+      --clear_device_data \
       --devtools_port 9222 \
       --url $site \
       --output $outdir/${output_subdir}/${run}

commit 85d14c9250c83ca5df2393cbb0b1ada6d28dfc76
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 8 07:35:21 2016 -0700

    clovis: Fix the main renderer thread detection.
    
    Also returns the figure instead of saving it in network_cpu_activity_view.py.
    
    Review URL: https://codereview.chromium.org/1869373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386071}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0fe5fae10dc8bfecc3847c43f114f8c692377e6c

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index bf6610d..7fdd9af 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -57,6 +57,9 @@ class ActivityLens(object):
           and tracing_event['name'] == 'thread_name'
           and event.args['name'] == 'CrRendererMain'):
         main_renderer_thread_ids.add((pid, tid))
+    events_count_per_pid_tid = {
+        pid_tid: count for (pid_tid, count) in events_count_per_pid_tid.items()
+        if pid_tid in main_renderer_thread_ids}
     pid_tid_events_counts = sorted(events_count_per_pid_tid.items(),
                                    key=operator.itemgetter(1), reverse=True)
     if (len(pid_tid_events_counts) > 1
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index f0c1275..839809a 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -211,6 +211,13 @@ class ActivityLensTestCase(unittest.TestCase):
          u'ph': u'X',
          u'pid': 1,
          u'tid': 1,
+         u'ts': 0},
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 1,
+         u'tid': 1,
          u'ts': 0}]
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'parser')
diff --git a/loading/network_cpu_activity_view.py b/loading/network_cpu_activity_view.py
index 09450e1..a118823 100755
--- a/loading/network_cpu_activity_view.py
+++ b/loading/network_cpu_activity_view.py
@@ -27,12 +27,14 @@ def _CpuActivityTimeline(cpu_lens, start_msec, end_msec, granularity):
   return (cpu_timestamps[:-1], np.array(busy_percentage))
 
 
-def GraphTimelines(trace, output_filename):
-  """Creates and saves a graph of Network and CPU activity for a trace.
+def GraphTimelines(trace):
+  """Creates a figure of Network and CPU activity for a trace.
 
   Args:
     trace: (LoadingTrace)
-    output_filename: (str) Path of the output graph.
+
+  Returns:
+    A matplotlib.pylab.figure.
   """
   cpu_lens = activity_lens.ActivityLens(trace)
   network_lens = network_activity_lens.NetworkActivityLens(trace)
@@ -56,13 +58,15 @@ def GraphTimelines(trace, output_filename):
   cpu.set_ylim(ymin=0, ymax=100)
   cpu.set_xlabel('Time (ms)')
   cpu.set_ylabel('Main Renderer Thread Busyness (%)')
-  figure.savefig(output_filename, dpi=300)
+  return figure
 
 
 def main():
   filename = sys.argv[1]
   trace = loading_trace.LoadingTrace.FromJsonFile(filename)
-  GraphTimelines(trace, filename + '.pdf')
+  figure = GraphTimelines(trace, filename + '.pdf')
+  output_filename = filename + '.pdf'
+  figure.savefig(output_filename, dpi=300)
 
 
 if __name__ == '__main__':

commit c382b512efb7c6c8f4e1cf8286302fa85a5a40c7
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 07:26:05 2016 -0700

    tools/android/loading Fix logging crash in main.py
    
    trace.ToJsonDict() was sometimes trying to write to a closed log file,
    resulting in crashes.
    Some exceptions were also not caught in the log file, and the
    exception handling code was itself crashing.
    
    This CL reorganizes the handling of log files and exceptions handling.
    
    NO_DEPENDENCY_CHECKS=true
    
    Review URL: https://codereview.chromium.org/1871023002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386067}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e4bb66b3be2c5f84b5c48b1d99e1a77f5bfbbca2

diff --git a/loading/gce/main.py b/loading/gce/main.py
index ad4410c..2988113 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -134,8 +134,9 @@ class ServerApp(object):
     old_stderr = sys.stderr
 
     trace_metadata = { 'succeeded' : False, 'url' : url }
-    try:
-      with open(log_filename, 'w') as sys.stdout:
+    trace = None
+    with open(log_filename, 'w') as sys.stdout:
+      try:
         sys.stderr = sys.stdout
 
         # Set up the controller.
@@ -154,15 +155,16 @@ class ServerApp(object):
               url, connection, chrome_ctl.ChromeMetadata())
           trace_metadata['succeeded'] = True
           trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
-    except Exception as e:
-      sys.stderr.write(e)
+      except Exception as e:
+        sys.stderr.write(str(e))
+
+      if trace:
+        with open(filename, 'w') as f:
+          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
 
     sys.stdout = old_stdout
     sys.stderr = old_stderr
 
-    with open(filename, 'w') as f:
-      json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
-
     return trace_metadata
 
   def _GetCurrentTaskCount(self):

commit 024c76a86a903c7b8f36fbfd108719681d17b835
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 07:23:16 2016 -0700

    tools/android/loading Improve GCE status reporting
    
    There is now only one way to check the status of the application.
    Previously, there was one used by the /status request and another used
    by the /set_tasks request.
    This was leading to inconsistencies where /status reported that the app
    is idle, but then /set_tasks failed because it considered the app is
    still busy.
    
    NO_DEPENDENCY_CHECKS=true
    
    Review URL: https://codereview.chromium.org/1874673002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386066}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9a1f6d3bb92a29c23652c48724cdc269173608ab

diff --git a/loading/gce/main.py b/loading/gce/main.py
index dc1da8e..ad4410c 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -63,6 +63,10 @@ class ServerApp(object):
     options.OPTIONS.ParseArgs([])
     options.OPTIONS.local_binary = config['chrome_path']
 
+  def _IsProcessingTasks(self):
+    """Returns True if the application is currently processing tasks."""
+    return self._thread is not None and self._thread.is_alive()
+
   def _GetStorageClient(self):
     return storage.Client(project = self._project_name,
                           credentials = self._credentials)
@@ -240,7 +244,7 @@ class ServerApp(object):
       A string to be sent back to the client, describing the success status of
       the request.
     """
-    if self._thread is not None and self._thread.is_alive():
+    if self._IsProcessingTasks():
       return 'Error: Already running\n'
 
     load_parameters = json.loads(http_body)
@@ -281,12 +285,15 @@ class ServerApp(object):
     elif path == '/test':
       data = 'hello\n'
     elif path == '/status':
-      task_count = self._GetCurrentTaskCount()
-      if task_count == 0:
+      if not self._IsProcessingTasks():
         data = 'Idle\n'
       else:
-        data = 'Remaining tasks: %s / %s\n' % (
-            task_count, self._initial_task_count)
+        task_count = self._GetCurrentTaskCount()
+        if task_count == 0:
+          data = '%s tasks complete. Finalizing.\n' % self._initial_task_count
+        else:
+          data = 'Remaining tasks: %s / %s\n' % (
+              task_count, self._initial_task_count)
         elapsed = time.time() - self._start_time
         data += 'Elapsed time: %s seconds\n' % str(elapsed)
         self._tasks_lock.acquire()

commit e2edaa9d5b784a97b30854073bd004adfb43b29f
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 8 02:55:56 2016 -0700

    tools/android/loading: Add WPR logging support in the ChromeController
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1865803005
    
    Cr-Original-Commit-Position: refs/heads/master@{#386030}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5b980b6c398d38685a82957cb0205aaa4eac5de3

diff --git a/loading/controller.py b/loading/controller.py
index 567bcb7..a0f6dff 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -152,7 +152,8 @@ class ChromeControllerBase(object):
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
     """Opens a Web Page Replay host context.
 
     Args:
@@ -162,6 +163,7 @@ class ChromeControllerBase(object):
           emulation.NETWORK_CONDITIONS.
       disable_script_injection: Disable JavaScript file injections that is
         fighting against resources name entropy.
+      out_log_path: Path of the WPR host's log.
     """
     raise NotImplementedError
 
@@ -261,13 +263,15 @@ class RemoteChromeController(ChromeControllerBase):
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
     """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
     assert not self._chrome_wpr_specific_args, 'WPR is already running.'
     with device_setup.RemoteWprHost(self._device, wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
-        disable_script_injection=disable_script_injection) as additional_flags:
+        disable_script_injection=disable_script_injection,
+        out_log_path=out_log_path) as additional_flags:
       self._chrome_wpr_specific_args = additional_flags
       yield
     self._chrome_wpr_specific_args = []
@@ -368,14 +372,15 @@ class LocalChromeController(ChromeControllerBase):
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
     """Override for WPR context."""
     assert not self._chrome_wpr_specific_args, 'WPR is already running.'
     with device_setup.LocalWprHost(wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
-        disable_script_injection=disable_script_injection
-        ) as additional_flags:
+        disable_script_injection=disable_script_injection,
+        out_log_path=out_log_path) as additional_flags:
       self._chrome_wpr_specific_args = additional_flags
       yield
     self._chrome_wpr_specific_args = []
diff --git a/loading/device_setup.py b/loading/device_setup.py
index d2c466d..d1abaff 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -131,8 +131,16 @@ def ForwardPort(device, local, remote):
 def _WprHost(wpr_archive_path, record=False,
              network_condition_name=None,
              disable_script_injection=False,
-             wpr_ca_cert_path=None):
+             wpr_ca_cert_path=None,
+             out_log_path=None):
   assert wpr_archive_path
+
+  def PathWorkaround(path):
+    # webpagereplay.ReplayServer is doing a os.path.exist(os.path.dirname(p))
+    # that fails if p = 'my_file.txt' because os.path.dirname(p) = '' != '.'.
+    # This workaround just sends absolute path to work around this bug.
+    return os.path.abspath(path)
+
   wpr_server_args = ['--use_closest_match']
   if record:
     wpr_server_args.append('--record')
@@ -157,10 +165,17 @@ def _WprHost(wpr_archive_path, record=False,
     wpr_server_args.extend(['--inject_scripts', ''])
   if wpr_ca_cert_path:
     wpr_server_args.extend(['--should_generate_certs',
-                            '--https_root_ca_cert_path=' + wpr_ca_cert_path])
+        '--https_root_ca_cert_path=' + PathWorkaround(wpr_ca_cert_path)])
+  if out_log_path:
+    # --log_level debug to extract the served URLs requests from the log.
+    wpr_server_args.extend(['--log_level', 'debug',
+                            '--log_file', PathWorkaround(out_log_path)])
+    # Don't append to previously existing log.
+    if os.path.exists(out_log_path):
+      os.remove(out_log_path)
 
   # Set up WPR server and device forwarder.
-  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
+  wpr_server = webpagereplay.ReplayServer(PathWorkaround(wpr_archive_path),
       '127.0.0.1', 0, 0, None, wpr_server_args)
   http_port, https_port = wpr_server.StartServer()[:-1]
 
@@ -193,7 +208,8 @@ def _FormatWPRRelatedChromeArgumentFor(http_port, https_port, escape):
 @contextlib.contextmanager
 def LocalWprHost(wpr_archive_path, record=False,
                  network_condition_name=None,
-                 disable_script_injection=False):
+                 disable_script_injection=False,
+                 out_log_path=None):
   """Launches web page replay host.
 
   Args:
@@ -203,6 +219,7 @@ def LocalWprHost(wpr_archive_path, record=False,
         emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
+    out_log_path: Path of the WPR host's log.
 
   Returns:
     Additional flags list that may be used for chromium to load web page through
@@ -216,8 +233,8 @@ def LocalWprHost(wpr_archive_path, record=False,
       wpr_archive_path,
       record=record,
       network_condition_name=network_condition_name,
-      disable_script_injection=disable_script_injection
-      ) as (http_port, https_port):
+      disable_script_injection=disable_script_injection,
+      out_log_path=out_log_path) as (http_port, https_port):
     chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
                                                      escape=False)
     # Certification authority is handled only available on Android.
@@ -228,7 +245,8 @@ def LocalWprHost(wpr_archive_path, record=False,
 @contextlib.contextmanager
 def RemoteWprHost(device, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
   """Launches web page replay host.
 
   Args:
@@ -239,6 +257,7 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
         emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
+    out_log_path: Path of the WPR host's log.
 
   Returns:
     Additional flags list that may be used for chromium to load web page through
@@ -264,8 +283,8 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection,
-        wpr_ca_cert_path=wpr_ca_cert_path
-        ) as (http_port, https_port):
+        wpr_ca_cert_path=wpr_ca_cert_path,
+        out_log_path=out_log_path) as (http_port, https_port):
       # Set up the forwarder.
       forwarder.Forwarder.Map([(0, http_port), (0, https_port)], device)
       device_http_port = forwarder.Forwarder.DevicePortForHostPort(http_port)

commit e76e5d0324e5ad1e7070bc60638b3d6d70ce3179
Author: iceman <iceman@yandex-team.ru>
Date:   Thu Apr 7 12:36:33 2016 -0700

    Use StringPiece as an argument for Base64Encode() instead of std::string.
    
    It is not necessary to make a copy of characters when calling
    Base64Encode() funcion.
    
    No behavior changes.
    
    BUG=
    CQ_INCLUDE_TRYBOTS=tryserver.blink:linux_blink_rel
    
    Review URL: https://codereview.chromium.org/1841703002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385840}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 35ea753e261641df335585e8a4dc060cb1e456e7

diff --git a/memdump/memdump.cc b/memdump/memdump.cc
index 7f135ee..572a838 100644
--- a/memdump/memdump.cc
+++ b/memdump/memdump.cc
@@ -56,9 +56,8 @@ class BitSet {
     size_t end = data_.size();
     while (end > 0 && data_[end - 1] == '\0')
       --end;
-    std::string bits(&data_[0], end);
     std::string b64_string;
-    base::Base64Encode(bits, &b64_string);
+    base::Base64Encode(base::StringPiece(data_.data(), end), &b64_string);
     return b64_string;
   }
 

commit 73e0d89654b2dc09c544a08e5aaa0ba8e843d0d8
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 7 08:57:06 2016 -0700

    Sandwich: Implement filter-cache's --subresource-discoverer flag.
    
    Before this CL, sandwich's cache filtering operation was whitelisting
    resources only discoverable by the parser. This CL add different
    strategies to white-list resources to keep in the cache for benchmark
    metrics comparison.
    
    Moreover, this CL also add the record-test-trace sandwich sub-command
    generating sandwich compatible loading traces using the
    webserver_test module, and also add the loading_trace_analyzer.py's
    prune subcommand to make loading traces thinner such as the
    scanner_vs_parser.trace in this CL.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1859553002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385771}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0c6569f5a9cd03222930efa4593f9fe2c8ff9800

diff --git a/loading/common_util.py b/loading/common_util.py
index 9384df0..ca06da2 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -2,7 +2,10 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import contextlib
 import logging
+import shutil
+import tempfile
 import time
 
 
@@ -61,3 +64,15 @@ def DeserializeAttributesFromJsonDict(json_dict, instance, attributes):
     getattr(instance, attr) # To raise AttributeError if attr doesn't exist.
     setattr(instance, attr, json_dict[attr])
   return instance
+
+
+@contextlib.contextmanager
+def TemporaryDirectory():
+  """Returns a freshly-created directory that gets automatically deleted after
+  usage.
+  """
+  name = tempfile.mkdtemp()
+  try:
+    yield name
+  finally:
+    shutil.rmtree(name)
diff --git a/loading/loading_trace_analyzer.py b/loading/loading_trace_analyzer.py
index 1821001..3c7882d 100755
--- a/loading/loading_trace_analyzer.py
+++ b/loading/loading_trace_analyzer.py
@@ -33,6 +33,23 @@ def _ArgumentParser():
       dest='where_statement', type=str,
       nargs=2, metavar=('FORMAT', 'REGEX'), default=[],
       help='Where statement to filter such as: --where "{protocol}" "https?"')
+
+  # requests listing subcommand.
+  prune_parser = subparsers.add_parser('prune',
+      help='Prunes some stuff from traces to make them small.')
+  prune_parser.add_argument('loading_trace', type=file,
+      help='Input path of the loading trace.')
+  prune_parser.add_argument('-t', '--trace-filters',
+      type=str, nargs='+', metavar='REGEX', default=[],
+      help='Regex filters to whitelist trace events.')
+  prune_parser.add_argument('-r', '--request-member-filter',
+      type=str, nargs='+', metavar='REGEX', default=[],
+      help='Regex filters to whitelist requests\' members.')
+  prune_parser.add_argument('-i', '--indent', type=int, default=2,
+      help='Number of space to indent the output.')
+  prune_parser.add_argument('-o', '--output',
+      type=argparse.FileType('w'), default=sys.stdout,
+      help='Output destination path if different from stdout.')
   return parser
 
 
@@ -70,9 +87,70 @@ def ListRequests(loading_trace_path,
     yield output_format.format(**request_event_json)
 
 
-def main(command_line_args):
-  """Command line tool entry point.
+def _PruneMain(args):
+  """`loading_trace_analyzer.py requests` Command line tool entry point.
+
+  Args:
+    args: Command line parsed arguments.
+
+  Example:
+    Keep only blink.net trace event category:
+      ... prune -t "blink.net"
+
+    Keep only requestStart trace events:
+      ... prune -t "requestStart"
+
+    Keep only requestStart trace events of the blink.user_timing category:
+      ... prune -t "blink.user_timing:requestStart"
+
+    Keep only all blink trace event categories:
+      ... prune -t "^blink\.*"
+
+    Keep only requests' url member:
+      ... prune -r "^url$"
+
+    Keep only requests' url and document_url members:
+      ... prune -r "^./url$"
+
+    Keep only requests' url, document_url and initiator members:
+      ... prune -r "^./url$" "initiator"
   """
+  trace_json = json.load(args.loading_trace)
+
+  # Filter trace events.
+  regexes = [re.compile(f) for f in args.trace_filters]
+  events = []
+  for event in trace_json['tracing_track']['events']:
+    prune = True
+    for cat in event['cat'].split(','):
+      event_name = cat + ':' + event['name']
+      for regex in regexes:
+        if regex.search(event_name):
+          prune = False
+          break
+      if not prune:
+        events.append(event)
+        break
+  trace_json['tracing_track']['events'] = events
+
+  # Filter members of requests.
+  regexes = [re.compile(f) for f in args.request_member_filter]
+  for request in trace_json['request_track']['events']:
+    for key in request.keys():
+      prune = True
+      for regex in regexes:
+        if regex.search(key):
+          prune = False
+          break
+      if prune:
+        del request[key]
+
+  json.dump(trace_json, args.output, indent=args.indent)
+  return 0
+
+
+def main(command_line_args):
+  """Command line tool entry point."""
   args = _ArgumentParser().parse_args(command_line_args)
   if args.subcommand == 'requests':
     try:
@@ -91,6 +169,8 @@ def main(command_line_args):
       sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
           where_statement[1], str(e)))
     return 1
+  elif args.subcommand == 'prune':
+    return _PruneMain(args)
   assert False
 
 
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 85dd48d..057ac0f 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -56,7 +56,7 @@ class PrefetchSimulationView(object):
     self.postload_msec = user_lens.PostloadTimeMsec()
     self.graph = dependency_graph.RequestDependencyGraph(
         requests, dependencies_lens, node_class=RequestNode)
-    preloaded_requests = [r.request_id for r in self._PreloadedRequests(
+    preloaded_requests = [r.request_id for r in self.PreloadedRequests(
         requests[0], dependencies_lens, trace)]
     self._AnnotateNodes(self.graph.graph.Nodes(), preloaded_requests,
                         critical_requests_ids)
@@ -102,8 +102,8 @@ class PrefetchSimulationView(object):
       node.before = node.request.request_id in critical_requests_ids
 
   @classmethod
-  def _ParserDiscoverableRequests(
-      cls, dependencies_lens, request, recurse=False):
+  def ParserDiscoverableRequests(
+      cls, request, dependencies_lens, recurse=False):
     """Returns a list of requests IDs dicovered by the parser.
 
     Args:
@@ -128,7 +128,7 @@ class PrefetchSimulationView(object):
         [dependencies_lens.GetRedirectChain(r) for r in requests]))
 
   @classmethod
-  def _PreloadedRequests(cls, request, dependencies_lens, trace):
+  def PreloadedRequests(cls, request, dependencies_lens, trace):
     """Returns the requests that have been preloaded from a given request.
 
     This list is the set of request that are:
@@ -142,7 +142,7 @@ class PrefetchSimulationView(object):
 
     Returns:
       A list of Request. Does not include the root request. This list is a
-      subset of the one returned by _ParserDiscoverableRequests().
+      subset of the one returned by ParserDiscoverableRequests().
     """
     # Preload step events are emitted in ResourceFetcher::preloadStarted().
     resource_events = trace.tracing_track.Filter(
@@ -155,8 +155,8 @@ class PrefetchSimulationView(object):
       preload_event = resource_events.EventFromStep(preload_step_event)
       if preload_event:
         preloaded_urls.add(preload_event.args['url'])
-    parser_requests = cls._ParserDiscoverableRequests(
-        dependencies_lens, request)
+    parser_requests = cls.ParserDiscoverableRequests(
+        request, dependencies_lens)
     preloaded_root_requests = filter(
         lambda r: r.url in preloaded_urls, parser_requests)
     # We can actually fetch the whole redirect chain.
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index e222bf5..be97809 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -25,8 +25,8 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
 
   def testParserDiscoverableRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    discovered_requests = PrefetchSimulationView._ParserDiscoverableRequests(
-        self.dependencies_lens, first_request)
+    discovered_requests = PrefetchSimulationView.ParserDiscoverableRequests(
+        first_request, self.dependencies_lens)
     self.assertListEqual(
         [TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
@@ -34,7 +34,7 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
 
   def testPreloadedRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+    preloaded_requests = PrefetchSimulationView.PreloadedRequests(
         first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([first_request], preloaded_requests)
     self._SetUp(
@@ -44,7 +44,7 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
          {'args': {'step': 'Preload'}, 'cat': 'blink.net',
           'id': '0xaf9f14fa9dd6c314', 'name': 'Resource', 'ph': 'T',
           'ts': 12, 'pid': 12, 'tid': 12}])
-    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+    preloaded_requests = PrefetchSimulationView.PreloadedRequests(
         first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
diff --git a/loading/request_track.py b/loading/request_track.py
index 47b874f..bf19fbb 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -333,6 +333,9 @@ class RequestTrack(devtools_monitor.Track):
                       % len(self._requests_in_flight))
     return self._requests
 
+  def GetFirstResourceRequest(self):
+    return self.GetEvents()[0]
+
   def GetFirstRequestMillis(self):
     """Find the canonical start time for this track.
 
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 3fac221..5f61aac 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -15,6 +15,7 @@ import argparse
 import csv
 import logging
 import os
+import shutil
 import sys
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -28,11 +29,13 @@ from pylib import constants
 import devil_chromium
 
 import chrome_cache
+import common_util
 import emulation
 import options
 import sandwich_metrics
 import sandwich_misc
 from sandwich_runner import SandwichRunner
+from trace_test.webserver_test import WebServer
 
 
 # Use options layer to access constants.
@@ -137,6 +140,10 @@ def _ArgumentParser():
   filter_cache_parser.add_argument('--cache-archive', type=str, required=True,
                                    dest='cache_archive_path',
                                    help='Path of the cache archive to filter.')
+  filter_cache_parser.add_argument('--subresource-discoverer', required=True,
+      help='Strategy for populating the cache with a subset of resources, '
+           'according to the way they can be discovered',
+      choices=sandwich_misc.SUBRESOURCE_DISCOVERERS)
   filter_cache_parser.add_argument('--output', type=str, required=True,
                                    dest='output_cache_archive_path',
                                    help='Path of filtered cache archive.')
@@ -147,11 +154,24 @@ def _ArgumentParser():
           'list the ones discoverable by the HTML pre-scanner for that given ' +
           'url.')
 
+  # Record test trace subcommand.
+  record_trace_parser = subparsers.add_parser('record-test-trace',
+      help='Record a test trace using the trace_test.webserver_test.')
+  record_trace_parser.add_argument('--source-dir', type=str, required=True,
+                                   help='Base path where the files are opened'
+                                        'by the web server.')
+  record_trace_parser.add_argument('--page', type=str, required=True,
+                                   help='Source page in source-dir to navigate '
+                                        'to.')
+  record_trace_parser.add_argument('-o', '--output', type=str, required=True,
+                                   help='Output path of the generated trace.')
+
   return parser
 
 
 def _RecordWprMain(args):
-  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner = SandwichRunner()
+  sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.wpr_record = True
   sandwich_runner.PrintConfig()
@@ -162,7 +182,8 @@ def _RecordWprMain(args):
 
 
 def _CreateCacheMain(args):
-  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner = SandwichRunner()
+  sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.cache_operation = 'save'
   sandwich_runner.PrintConfig()
@@ -173,7 +194,8 @@ def _CreateCacheMain(args):
 
 
 def _RunJobMain(args):
-  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner = SandwichRunner()
+  sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.PrintConfig()
   sandwich_runner.Run()
@@ -196,8 +218,8 @@ def _ExtractMetricsMain(args):
 def _FilterCacheMain(args):
   whitelisted_urls = set()
   for loading_trace_path in args.loading_trace_paths:
-    whitelisted_urls.update(
-        sandwich_misc.ExtractParserDiscoverableResources(loading_trace_path))
+    whitelisted_urls.update(sandwich_misc.ExtractDiscoverableUrls(
+        loading_trace_path, args.subresource_discoverer))
   if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
     os.makedirs(os.path.dirname(args.output_cache_archive_path))
   chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
@@ -206,6 +228,22 @@ def _FilterCacheMain(args):
   return 0
 
 
+def _RecordWebServerTestTrace(args):
+  with common_util.TemporaryDirectory() as out_path:
+    sandwich_runner = SandwichRunner()
+    # Reuse the WPR's forwarding to access the webpage from Android.
+    sandwich_runner.wpr_record = True
+    sandwich_runner.wpr_archive_path = os.path.join(out_path, 'wpr')
+    sandwich_runner.trace_output_directory = os.path.join(out_path, 'run')
+    with WebServer.Context(
+        source_dir=args.source_dir, communication_dir=out_path) as server:
+      address = server.Address()
+      sandwich_runner.urls = ['http://%s/%s' % (address, args.page)]
+      sandwich_runner.Run()
+    shutil.copy(os.path.join(out_path, 'run', '0', 'trace.json'), args.output)
+  return 0
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -226,6 +264,8 @@ def main(command_line_args):
     return _ExtractMetricsMain(args)
   if args.subcommand == 'filter-cache':
     return _FilterCacheMain(args)
+  if args.subcommand == 'record-test-trace':
+    return _RecordWebServerTestTrace(args)
   assert False
 
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index e30232c..1a9e96d 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -29,7 +29,16 @@ import loading_trace as loading_trace_module
 import tracing
 
 
-CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
+# List of selected trace event categories when running chrome.
+CATEGORIES = [
+    # Need blink network trace events for prefetch_view.PrefetchSimulationView
+    'blink.net',
+
+    # Need to get mark trace events for _GetWebPageTrackedEvents()
+    'blink.user_timing',
+
+    # Need to memory dump trace event for _GetBrowserDumpEvents()
+    'disabled-by-default-memory-infra']
 
 CSV_FIELD_NAMES = [
     'id',
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index a15f5ab..13988c3 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -4,9 +4,27 @@
 
 import logging
 
+from loading_trace import LoadingTrace
+from prefetch_view import PrefetchSimulationView
+from request_dependencies_lens import RequestDependencyLens
+from user_satisfied_lens import FirstContentfulPaintLens
 import wpr_backend
-import loading_trace
-import request_dependencies_lens
+
+
+# Prefetches the first resource following the redirection chain.
+REDIRECTED_MAIN_DISCOVERER = 'redirected-main'
+
+# All resources which are fetched from the main document and their redirections.
+PARSER_DISCOVERER = 'parser',
+
+# Simulation of HTMLPreloadScanner on the main document and their redirections.
+HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner',
+
+SUBRESOURCE_DISCOVERERS = set([
+  REDIRECTED_MAIN_DISCOVERER,
+  PARSER_DISCOVERER,
+  HTML_PRELOAD_SCANNER_DISCOVERER
+])
 
 
 def PatchWpr(wpr_archive_path):
@@ -44,38 +62,55 @@ def PatchWpr(wpr_archive_path):
   wpr_archive.Persist()
 
 
-def ExtractParserDiscoverableResources(loading_trace_path):
-  """Extracts the parser discoverable resources from a loading trace.
+def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
+  """Extracts discoverable resource urls from a loading trace according to a
+  sub-resource discoverer.
 
   Args:
     loading_trace_path: The loading trace's path.
+    subresource_discoverer: The sub-resources discoverer that should white-list
+      the resources to keep in cache for the NoState-Prefetch benchmarks.
 
   Returns:
     A set of urls.
   """
-  whitelisted_urls = set()
+  assert subresource_discoverer in SUBRESOURCE_DISCOVERERS, \
+      'unknown prefetch simulation {}'.format(subresource_discoverer)
+
+  # Load trace and related infos.
   logging.info('loading %s' % loading_trace_path)
-  trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
-  requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
-  deps = requests_lens.GetRequestDependencies()
-
-  main_resource_request = deps[0][0]
-  logging.info('white-listing %s' % main_resource_request.url)
-  whitelisted_urls.add(main_resource_request.url)
-  for (first, second, reason) in deps:
+  trace = LoadingTrace.FromJsonFile(loading_trace_path)
+  dependencies_lens = RequestDependencyLens(trace)
+  first_resource_request = trace.request_track.GetFirstResourceRequest()
+
+  # Build the list of discovered requests according to the desired simulation.
+  discovered_requests = []
+  if subresource_discoverer == REDIRECTED_MAIN_DISCOVERER:
+    discovered_requests = \
+        [dependencies_lens.GetRedirectChain(first_resource_request)[-1]]
+  elif subresource_discoverer == PARSER_DISCOVERER:
+    discovered_requests = PrefetchSimulationView.ParserDiscoverableRequests(
+        first_resource_request, dependencies_lens)
+  elif subresource_discoverer == HTML_PRELOAD_SCANNER_DISCOVERER:
+    discovered_requests = PrefetchSimulationView.PreloadedRequests(
+        first_resource_request, dependencies_lens, trace)
+  else:
+    assert False
+
+  # Prune out data:// requests.
+  whitelisted_urls = set()
+  logging.info('white-listing %s' % first_resource_request.url)
+  whitelisted_urls.add(first_resource_request.url)
+  for request in discovered_requests:
     # Work-around where the protocol may be none for an unclear reason yet.
     # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
     #   this work-around.
-    if not second.protocol:
-      logging.info('ignoring %s (no protocol)' % second.url)
+    if not request.protocol:
+      logging.warning('ignoring %s (no protocol)' % request.url)
       continue
     # Ignore data protocols.
-    if not second.protocol.startswith('http'):
-      logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
-          second.url, second.protocol))
+    if not request.protocol.startswith('http'):
       continue
-    if (first.request_id == main_resource_request.request_id and
-        reason == 'parser' and second.url not in whitelisted_urls):
-      logging.info('white-listing %s' % second.url)
-      whitelisted_urls.add(second.url)
+    logging.info('white-listing %s' % request.url)
+    whitelisted_urls.add(request.url)
   return whitelisted_urls
diff --git a/loading/sandwich_misc_unittest.py b/loading/sandwich_misc_unittest.py
new file mode 100644
index 0000000..f75288c
--- /dev/null
+++ b/loading/sandwich_misc_unittest.py
@@ -0,0 +1,45 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import unittest
+import urlparse
+
+import sandwich_misc
+
+
+LOADING_DIR = os.path.dirname(__file__)
+TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
+
+
+class SandwichMiscTest(unittest.TestCase):
+  _TRACE_PATH = os.path.join(TEST_DATA_DIR, 'scanner_vs_parser.trace')
+
+  def GetResourceUrl(self, path):
+    return urlparse.urljoin('http://l/', path)
+
+  def testRedirectedMainWhitelisting(self):
+    urls_set_ref = set([self.GetResourceUrl('./')])
+    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.REDIRECTED_MAIN_DISCOVERER)
+    self.assertEquals(urls_set_ref, urls_set)
+
+  def testParserDiscoverableWhitelisting(self):
+    urls_set_ref = set([self.GetResourceUrl('./'),
+                        self.GetResourceUrl('0.png'),
+                        self.GetResourceUrl('1.png')])
+    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.PARSER_DISCOVERER)
+    self.assertEquals(urls_set_ref, urls_set)
+
+  def testHTMLPreloadScannerWhitelisting(self):
+    urls_set_ref = set([self.GetResourceUrl('./'),
+                        self.GetResourceUrl('0.png')])
+    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.HTML_PRELOAD_SCANNER_DISCOVERER)
+    self.assertEquals(urls_set_ref, urls_set)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 5dd7ba2..ebef772 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -78,7 +78,7 @@ class SandwichRunner(object):
   command line flags have `dest` parameter set to existing runner members.
   """
 
-  def __init__(self, job_name):
+  def __init__(self):
     """Configures a sandwich runner out of the box.
 
     Public members are meant to be configured as wished before calling Run().
@@ -97,7 +97,7 @@ class SandwichRunner(object):
     self.disable_wpr_script_injection = False
 
     # The job name. Is str.
-    self.job_name = job_name
+    self.job_name = '__unknown_job'
 
     # Number of times to repeat the job.
     self.job_repeat = 1
@@ -112,7 +112,7 @@ class SandwichRunner(object):
     self.trace_output_directory = None
 
     # List of urls to run.
-    self.urls = _ReadUrlsFromJobDescription(job_name)
+    self.urls = []
 
     # Configures whether to record speed-index video.
     self.record_video = False
@@ -126,6 +126,10 @@ class SandwichRunner(object):
     self._chrome_ctl = None
     self._local_cache_directory_path = None
 
+  def LoadJob(self, job_name):
+    self.job_name = job_name
+    self.urls = _ReadUrlsFromJobDescription(job_name)
+
   def PullConfigFromArgs(self, args):
     """Configures the sandwich runner from parsed command line argument.
 
diff --git a/loading/testdata/scanner_vs_parser.trace b/loading/testdata/scanner_vs_parser.trace
new file mode 100644
index 0000000..427cdc8
--- /dev/null
+++ b/loading/testdata/scanner_vs_parser.trace
@@ -0,0 +1,159 @@
+{
+  "metadata": {},
+  "page_track": {
+    "events": []
+  },
+  "request_track": {
+    "events": [
+      {
+        "initiator": {
+          "type": "other"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/"
+      },
+      {
+        "initiator": {
+          "lineNumber": 28,
+          "type": "parser",
+          "url": "http://l/"
+        },
+        "protocol": "data",
+        "url": "data:image/png;base64,iVBO[PRUNED]"
+      },
+      {
+        "initiator": {
+          "lineNumber": 21,
+          "type": "parser",
+          "url": "http://l/"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/0.png"
+      },
+      {
+        "initiator": {
+          "type": "parser",
+          "url": "http://l/"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/1.png"
+      },
+      {
+        "initiator": {
+          "type": "other"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/favicon.ico"
+      }
+    ],
+    "metadata": {
+      "duplicates_count": 0,
+      "inconsistent_initiators": 0
+    }
+  },
+  "tracing_track": {
+    "events": [
+      {
+        "args": {
+          "priority": 4,
+          "url": "http://l/"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "S",
+        "pid": 3,
+        "ts": 1213697828839
+      },
+      {
+        "args": {},
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "F",
+        "pid": 3,
+        "ts": 1213697889955
+      },
+      {
+        "args": {
+          "priority": 1,
+          "url": "http://l/0.png"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "S",
+        "pid": 3,
+        "ts": 1213697891911
+      },
+      {
+        "args": {
+          "step": "Preload"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697892658
+      },
+      {
+        "args": {
+          "priority": 1,
+          "url": "http://l/1.png"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "S",
+        "pid": 3,
+        "ts": 1213697934273
+      },
+      {
+        "args": {},
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "F",
+        "pid": 3,
+        "ts": 1213697943810
+      },
+      {
+        "args": {
+          "priority": 3,
+          "step": "ChangePriority"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697984606
+      },
+      {
+        "args": {
+          "priority": 3,
+          "step": "ChangePriority"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697984875
+      },
+      {
+        "args": {
+          "priority": 3,
+          "step": "ChangePriority"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697985346
+      },
+      {
+        "args": {},
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "F",
+        "pid": 3,
+        "ts": 1213698035637
+      }
+    ]
+  },
+  "url": "http://l/"
+}

commit 51d74298761b0430ce97a3a2c50496704d6eeebc
Author: blundell <blundell@chromium.org>
Date:   Thu Apr 7 07:03:15 2016 -0700

    tools/android/loading: Write trace database when generating traces on GCE
    
    This CL adds the generation of a trace database file when generating traces
    via GCE. The trace database file is located in
    "gs://path/to/traces/trace_database.json" and indexes all of the
    successfully-generated traces in "gs://path/to/traces". To access the
    database in Python, do the following:
    
    from loading_trace_database import LoadingTraceDatabase
    db = LoadingTraceDatabase.FromJsonFileInGoogleStorage(
        "gs://path/to/traces/trace_database.json")
    
    Review URL: https://codereview.chromium.org/1866103003
    
    Cr-Original-Commit-Position: refs/heads/master@{#385745}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f7564ce44cc27d4a39890c2bca6e073c710dfe4c

diff --git a/loading/gce/main.py b/loading/gce/main.py
index c20f0ae..dc1da8e 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -22,6 +22,7 @@ sys.path.insert(0,
 import controller
 import loading_trace
 import options
+from loading_trace_database import LoadingTraceDatabase
 
 
 class ServerApp(object):
@@ -114,7 +115,8 @@ class ServerApp(object):
       log_filename: Name of the file where standard output and errors are logged
 
     Returns:
-      True if the trace was generated successfully.
+      A dictionary of metadata about the trace, including a 'succeeded' field
+      indicating whether the trace was successfully generated.
     """
     try:
       os.remove(filename)  # Remove any existing trace for this URL.
@@ -127,7 +129,7 @@ class ServerApp(object):
     old_stdout = sys.stdout
     old_stderr = sys.stderr
 
-    succeeded = True
+    trace_metadata = { 'succeeded' : False, 'url' : url }
     try:
       with open(log_filename, 'w') as sys.stdout:
         sys.stderr = sys.stdout
@@ -146,8 +148,9 @@ class ServerApp(object):
           connection.ClearCache()
           trace = loading_trace.LoadingTrace.RecordUrlNavigation(
               url, connection, chrome_ctl.ChromeMetadata())
+          trace_metadata['succeeded'] = True
+          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
     except Exception as e:
-      succeeded = False
       sys.stderr.write(e)
 
     sys.stdout = old_stdout
@@ -156,7 +159,7 @@ class ServerApp(object):
     with open(filename, 'w') as f:
       json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
 
-    return succeeded
+    return trace_metadata
 
   def _GetCurrentTaskCount(self):
     """Returns the number of remaining tasks. Thread safe."""
@@ -183,6 +186,8 @@ class ServerApp(object):
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
 
+    trace_database = LoadingTraceDatabase({})
+
     # TODO(blundell): Fix this up.
     logs_dir = self._base_path_in_bucket + 'analyze_logs/'
     log_filename = 'analyze.log'
@@ -194,10 +199,15 @@ class ServerApp(object):
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
         remote_filename = local_filename + '/' + str(repeat)
-        if self._GenerateTrace(
-            url, emulate_device, emulate_network, local_filename, log_filename):
+        trace_metadata = self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename)
+        if trace_metadata['succeeded']:
           print 'Uploading: %s' % remote_filename
-          self._UploadFile(local_filename, traces_dir + remote_filename)
+          remote_trace_location = traces_dir + remote_filename
+          self._UploadFile(local_filename, remote_trace_location)
+          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
+              remote_trace_location)
+          trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
         else:
           print 'Trace generation failed for URL: %s' % url
           self._tasks_lock.acquire()
@@ -212,6 +222,9 @@ class ServerApp(object):
       url = self._tasks.pop()
       self._tasks_lock.release()
 
+    self._UploadString(json.dumps(trace_database.ToJsonDict(), indent=2),
+                       traces_dir + 'trace_database.json')
+
     if len(self._failed_tasks) > 0:
       print 'Uploading failing URLs'
       self._UploadString(json.dumps(self._failed_tasks, indent=2),
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index ac8f855..2ef33b0 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -14,6 +14,11 @@ class LoadingTraceDatabase(object):
        about those traces."""
     self._traces_dict = traces_dict
 
+  def AddTrace(self, filename, trace_dict):
+    """Adds a mapping from |filename| to |trace_dict| into the database."""
+    assert filename not in self._traces_dict
+    self._traces_dict[filename] = trace_dict
+
   def GetTraceFilesForURL(self, url):
     """Given a URL, returns the set of filenames of traces that were generated
        for this URL."""
diff --git a/loading/loading_trace_database_unittest.py b/loading/loading_trace_database_unittest.py
index 775638c..31dc9b3 100644
--- a/loading/loading_trace_database_unittest.py
+++ b/loading/loading_trace_database_unittest.py
@@ -32,6 +32,14 @@ class LoadingTraceDatabaseUnittest(unittest.TestCase):
     self.assertEqual(
         self._JSON_DATABASE, self.database.ToJsonDict())
 
+  def testAddTrace(self):
+    dummy_url = "http://dummy.com"
+    new_trace_file = "traces/new_trace.json"
+    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url), [])
+    self.database.AddTrace(new_trace_file, {"url" : dummy_url})
+    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url),
+                     [new_trace_file])
+
 
 if __name__ == '__main__':
   unittest.main()

commit 831e96e58edf8af2eaa0ead5fd1d4cd49163da8e
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 7 05:17:59 2016 -0700

    clovis: Fix a typo (redundant self parameter).
    
    Also make a class inherit from object.
    
    Review URL: https://codereview.chromium.org/1868923004
    
    Cr-Original-Commit-Position: refs/heads/master@{#385721}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2a9c63b81cd26207ddfd14674f7c492887583eec

diff --git a/loading/controller.py b/loading/controller.py
index b6616f9..567bcb7 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -346,7 +346,7 @@ class LocalChromeController(ChromeControllerBase):
     the value of stdout/stderr based on the value of OPTIONS.local_noisy."""
     stdout = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     stderr = stdout
-    return self.OpenWithRedirection(self, stdout, stderr)
+    return self.OpenWithRedirection(stdout, stderr)
 
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index d2e4910..ac8f855 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -7,8 +7,8 @@
 import json
 from google_storage_util import ReadFromGoogleStorage
 
-class LoadingTraceDatabase:
 
+class LoadingTraceDatabase(object):
   def __init__(self, traces_dict):
     """traces_dict is a dictionary mapping filenames of traces to metadata
        about those traces."""

commit c99ddca2895e08071438c31fd9b6f6983e4cad42
Author: droger <droger@chromium.org>
Date:   Thu Apr 7 03:37:44 2016 -0700

    tools/android/loading Experimental scripts for clovis validation
    
    This CL adds experimental scripts to collect and compare traces on a
    device and on GCE.
    
    Review URL: https://codereview.chromium.org/1865233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385705}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4af98546321dcd74207f781c9e37415ce4264e79

diff --git a/loading/unmaintained/README.md b/loading/unmaintained/README.md
new file mode 100644
index 0000000..295323b
--- /dev/null
+++ b/loading/unmaintained/README.md
@@ -0,0 +1,2 @@
+This directory contains unmaintained code that still has value, such as
+experimental or temporary scripts.
diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
new file mode 100755
index 0000000..41d8417
--- /dev/null
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Takes a list of URLs (infile), and runs analyse.py on them in  parallel on a
+# device and on GCE, in a sychronized manner (the task is started on both
+# platforms at the same time).
+
+infile=$1
+outdir=$2
+instance_ip=$3
+repeat_count=$4
+
+for site in $(< $infile); do
+ echo $site
+ output_subdir=$(echo "$site"|tr "/:" "_")
+ echo 'Start remote task'
+ cat >urls.json << EOF
+ {
+  "urls" : [
+    "$site"
+  ],
+  "repeat_count" : "$repeat_count",
+  "emulate_device" : "Nexus 4"
+ }
+EOF
+
+ while [ "$(curl http://$instance_ip:8080/status)" != "Idle" ]; do
+   echo 'Waiting for instance to be ready, retry in 5s'
+   sleep 5
+ done
+ curl -X POST -d @urls.json http://$instance_ip:8080/set_tasks
+
+ echo 'Run on device'
+ mkdir $outdir/$output_subdir
+ for ((run=0;run<$repeat_count;++run)); do
+   echo '****'  $run
+   tools/android/loading/analyze.py log_requests \
+      --clear_cache \
+      --devtools_port 9222 \
+      --url $site \
+      --output $outdir/${output_subdir}/${run}
+ done
+done
diff --git a/loading/unmaintained/gce_validation_compare.sh b/loading/unmaintained/gce_validation_compare.sh
new file mode 100755
index 0000000..88e26df
--- /dev/null
+++ b/loading/unmaintained/gce_validation_compare.sh
@@ -0,0 +1,75 @@
+#!/bin/bash
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# gce_validation_compare.sh rootdir compare_filename
+#   root_dir: root directory for the experiment.
+#   compare_filename: file where the comparison breakdown is output.
+#
+# Computes core sets from GCE and device experiment resutls, and compare them.
+# The expected directory structure is:
+#
+# root_dir/
+#   cloud/
+#     url1/         # Can be any name as long as it is mirrored under device/.
+#       run1.trace  # Can be any name.
+#       run2.trace
+#       ...
+#     url2/
+#     ...
+#   device/
+#     url1/
+#       run1.trace
+#       run2.trace
+#       ...
+#     url2/
+#     ...
+
+root_dir=$1
+compare_filename=$2
+
+rm $compare_filename
+
+# Check directory structure.
+if [ ! -d $root_dir/cloud ]; then
+  echo "$root_dir/cloud missing!"
+  exit 1
+fi
+
+if [ ! -d $root_dir/device ]; then
+  echo "$root_dir/device missing!"
+  exit 1
+fi
+
+for device_file in $root_dir/device/*/  ; do
+  cloud_file=$root_dir/cloud/$(basename $device_file)
+  if [ ! -d $cloud_file ]; then
+    echo "$cloud_file not found"
+  fi
+done
+
+for cloud_file in $root_dir/cloud/*/  ; do
+  device_file=$root_dir/device/$(basename $device_file)
+  if [ ! -d $device_file ]; then
+    echo "$device_file not found"
+  fi
+done
+
+# Loop through all the subdirectories, compute the core sets and compare them.
+for device_file in $root_dir/device/*/  ; do
+  base_name=$(basename $device_file)
+  python tools/android/loading/core_set.py page_core --sets device/$base_name \
+    --output $device_file/core_set.json --prefix $device_file
+
+  cloud_file=$root_dir/cloud/$base_name
+  if [ -d $cloud_file ]; then
+    python tools/android/loading/core_set.py page_core --sets cloud/$base_name \
+      --output $cloud_file/core_set.json --prefix $cloud_file
+
+    compare_result=$(python tools/android/loading/core_set.py compare \
+      --a $cloud_file/core_set.json --b $device_file/core_set.json)
+    compare_result+=" $base_name"
+    echo $compare_result >> $compare_filename
+  fi
+done

commit f01c7a0340c2a838d42611f69530fc228aa79ad8
Author: lizeb <lizeb@chromium.org>
Date:   Wed Apr 6 09:22:26 2016 -0700

    clovis: Make PrefetchSimulationView serializable.
    
    PrefetchSimulationView keeps a reference to the trace, making it heavy
    in memory, and costly to instanciate (hundreds of MB, and up to
    10s). This CL only keeps the relevant data, that can then be stored in a
    JSON file.
    
    Review URL: https://codereview.chromium.org/1863613002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385474}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9faed16cc5a57221063baff2d1bad04a8cf396b1

diff --git a/loading/common_util.py b/loading/common_util.py
index 5b62ce0..9384df0 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -24,3 +24,40 @@ def PollFor(condition, condition_name, interval=5):
     if result:
       return result
     time.sleep(interval)
+
+
+def SerializeAttributesToJsonDict(json_dict, instance, attributes):
+  """Adds the |attributes| from |instance| to a |json_dict|.
+
+  Args:
+    json_dict: (dict) Dict to update.
+    instance: (object) instance to take the values from.
+    attributes: ([str]) List of attributes to serialize.
+
+  Returns:
+    json_dict
+  """
+  json_dict.update({attr: getattr(instance, attr) for attr in attributes})
+  return json_dict
+
+
+def DeserializeAttributesFromJsonDict(json_dict, instance, attributes):
+  """Sets a list of |attributes| in |instance| according to their value in
+    |json_dict|.
+
+  Args:
+    json_dict: (dict) Dict containing values dumped by
+               SerializeAttributesToJsonDict.
+    instance: (object) instance to modify.
+    attributes: ([str]) List of attributes to set.
+
+  Raises:
+    AttributeError if one of the attribute doesn't exist in |instance|.
+
+  Returns:
+    instance
+  """
+  for attr in attributes:
+    getattr(instance, attr) # To raise AttributeError if attr doesn't exist.
+    setattr(instance, attr, json_dict[attr])
+  return instance
diff --git a/loading/common_util_unittest.py b/loading/common_util_unittest.py
new file mode 100644
index 0000000..8984881
--- /dev/null
+++ b/loading/common_util_unittest.py
@@ -0,0 +1,51 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import common_util
+
+
+class SerializeAttributesTestCase(unittest.TestCase):
+  class Foo(object):
+    def __init__(self, foo_fighters, whisky_bar):
+      # Pylint doesn't like foo and bar, but I guess musical references are OK.
+      self.foo_fighters = foo_fighters
+      self.whisky_bar = whisky_bar
+
+  def testSerialization(self):
+    foo_fighters = self.Foo('1', 2)
+    json_dict = common_util.SerializeAttributesToJsonDict(
+        {}, foo_fighters, ['foo_fighters', 'whisky_bar'])
+    self.assertDictEqual({'foo_fighters': '1', 'whisky_bar': 2}, json_dict)
+    # Partial update
+    json_dict = common_util.SerializeAttributesToJsonDict(
+        {'baz': 42}, foo_fighters, ['whisky_bar'])
+    self.assertDictEqual({'baz': 42, 'whisky_bar': 2}, json_dict)
+    # Non-existing attribute.
+    with self.assertRaises(AttributeError):
+      json_dict = common_util.SerializeAttributesToJsonDict(
+          {}, foo_fighters, ['foo_fighters', 'whisky_bar', 'baz'])
+
+  def testDeserialization(self):
+    foo_fighters = self.Foo('hello', 'world')
+    json_dict = {'foo_fighters': 12, 'whisky_bar': 42}
+    # Partial.
+    foo_fighters = common_util.DeserializeAttributesFromJsonDict(
+        json_dict, foo_fighters, ['foo_fighters'])
+    self.assertEqual(12, foo_fighters.foo_fighters)
+    self.assertEqual('world', foo_fighters.whisky_bar)
+    # Complete.
+    foo_fighters = common_util.DeserializeAttributesFromJsonDict(
+        json_dict, foo_fighters, ['foo_fighters', 'whisky_bar'])
+    self.assertEqual(42, foo_fighters.whisky_bar)
+    # Non-existing attribute.
+    with self.assertRaises(AttributeError):
+      json_dict['baz'] = 'bad'
+      foo_fighters = common_util.DeserializeAttributesFromJsonDict(
+          json_dict, foo_fighters, ['foo_fighters', 'whisky_bar', 'baz'])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index 955177c..da55ad2 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -7,25 +7,54 @@
 import logging
 import sys
 
+import common_util
 import graph
 import request_track
 
 
 class RequestNode(graph.Node):
-  def __init__(self, request):
+  def __init__(self, request=None):
     super(RequestNode, self).__init__()
     self.request = request
-    self.cost = request.Cost()
+    self.cost = request.Cost() if request else None # Deserialization.
+
+  def ToJsonDict(self):
+    json_dict = super(RequestNode, self).ToJsonDict()
+    json_dict.update({'request': self.request.ToJsonDict()})
+    return json_dict
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = super(RequestNode, cls).FromJsonDict(json_dict)
+    result.request = request_track.Request.FromJsonDict(json_dict['request'])
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['cost'])
 
 
 class Edge(graph.Edge):
-  def __init__(self, from_node, to_node, reason):
+  def __init__(self, from_node, to_node, reason=None):
     super(Edge, self).__init__(from_node, to_node)
     self.reason = reason
+    self.cost = None
+    self.is_timing = None
+    if from_node is None: # Deserialization.
+      return
+    self.reason = reason
     self.cost = request_track.TimeBetween(
         self.from_node.request, self.to_node.request, self.reason)
     self.is_timing = False
 
+  def ToJsonDict(self):
+    result = {}
+    return common_util.SerializeAttributesToJsonDict(
+        result, self, ['reason', 'cost', 'is_timing'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = cls(None, None, None)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['reason', 'cost', 'is_timing'])
+
 
 class RequestDependencyGraph(object):
   """Request dependency graph."""
@@ -45,6 +74,12 @@ class RequestDependencyGraph(object):
       node_class: (subclass of RequestNode)
       edge_class: (subclass of Edge)
     """
+    self._requests = None
+    self._first_request_node = None
+    self._deps_graph = None
+    self._nodes_by_id = None
+    if requests is None: # Deserialization.
+      return
     assert issubclass(node_class, RequestNode)
     assert issubclass(edge_class, Edge)
     self._requests = requests
@@ -175,3 +210,22 @@ class RequestDependencyGraph(object):
         self._deps_graph.UpdateEdge(
             current, edges_by_end_time[end_mark].to_node,
             current.to_node)
+
+  def ToJsonDict(self):
+    result = {'graph': self.graph.ToJsonDict()}
+    result['requests'] = [r.ToJsonDict() for r in self._requests]
+    return result
+
+  @classmethod
+  def FromJsonDict(cls, json_dict, node_class, edge_class):
+    result = cls(None, None)
+    graph_dict = json_dict['graph']
+    g = graph.DirectedGraph.FromJsonDict(graph_dict, node_class, edge_class)
+    result._requests = [request_track.Request.FromJsonDict(r)
+                        for r in json_dict['requests']]
+    result._nodes_by_id = {node.request.request_id: node
+                           for node in g.Nodes()}
+    result._first_request_node = result._nodes_by_id[
+        result._requests[0].request_id]
+    result._deps_graph = g
+    return result
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
index 4f60995..68433f5 100644
--- a/loading/dependency_graph_unittest.py
+++ b/loading/dependency_graph_unittest.py
@@ -16,13 +16,15 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     super(RequestDependencyGraphTestCase, self).setUp()
     self.trace = TestRequests.CreateLoadingTrace()
 
-  def testUpdateRequestCost(self):
+  def testUpdateRequestCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
     requests[0].timing = request_track.TimingFromDict(
         {'requestTime': 12, 'loadingFinished': 10})
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
     g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     self.assertEqual(10, g.Cost())
     request_id = requests[0].request_id
     g.UpdateRequestsCost({request_id: 100})
@@ -30,7 +32,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     g.UpdateRequestsCost({'unrelated_id': 1000})
     self.assertEqual(100, g.Cost())
 
-  def testCost(self):
+  def testCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
     for (index, request) in enumerate(requests):
       request.timing = request_track.TimingFromDict(
@@ -39,6 +41,8 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
     g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     # First redirect -> Second redirect -> Redirected Request -> Request ->
     # JS Request 2
     self.assertEqual(7010, g.Cost())
@@ -50,7 +54,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     g.UpdateRequestsCost({TestRequests.SECOND_REDIRECT_REQUEST.request_id: 0})
     self.assertEqual(6990, g.Cost())
 
-  def testHandleTimingDependencies(self):
+  def testHandleTimingDependencies(self, serialize=False):
     # Timing adds node 1 as a parent to 2 but not 3.
     requests = [
         test_utils.MakeRequest(0, 'null', 100, 110, 110,
@@ -65,6 +69,8 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
         test_utils.MakeRequest(5, 2, 122, 126, 126)]
 
     g = self._GraphFromRequests(requests)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     self.assertSetEqual(
         self._Successors(g, requests[0]), set([requests[1], requests[3]]))
     self.assertSetEqual(
@@ -106,7 +112,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     self.assertSetEqual(self._Successors(g, requests[5]), set())
     self.assertSetEqual(self._Successors(g, requests[6]), set([requests[3]]))
 
-  def testHandleTimingDependenciesImages(self):
+  def testHandleTimingDependenciesImages(self, serialize=False):
     # If we're all image types, then we shouldn't split by timing.
     requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
                 test_utils.MakeRequest(1, 0, 115, 120, 120),
@@ -117,6 +123,8 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     for r in requests:
       r.response_headers['Content-Type'] = 'image/gif'
     g = self._GraphFromRequests(requests)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     self.assertSetEqual(self._Successors(g, requests[0]),
                         set([requests[1], requests[2], requests[3]]))
     self.assertSetEqual(self._Successors(g, requests[1]), set())
@@ -126,6 +134,19 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     self.assertSetEqual(self._Successors(g, requests[4]), set())
     self.assertSetEqual(self._Successors(g, requests[5]), set())
 
+  def testSerializeDeserialize(self):
+    # Redo the tests, with a graph that has been serialized / deserialized.
+    self.testUpdateRequestCost(True)
+    self.testCost(True)
+    self.testHandleTimingDependencies(True)
+    self.testHandleTimingDependenciesImages(True)
+
+  @classmethod
+  def _SerializeDeserialize(cls, g):
+    json_dict = g.ToJsonDict()
+    return dependency_graph.RequestDependencyGraph.FromJsonDict(
+        json_dict, dependency_graph.RequestNode, dependency_graph.Edge)
+
   @classmethod
   def _GraphFromRequests(cls, requests):
     trace = test_utils.LoadingTraceFromEvents(requests)
diff --git a/loading/graph.py b/loading/graph.py
index d051d58..7c2d275 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -6,6 +6,8 @@
 
 import collections
 
+import common_util
+
 
 class Node(object):
   """A node in a Graph.
@@ -16,6 +18,14 @@ class Node(object):
     """Create a new node."""
     self.cost = 0
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict({}, self, ['cost'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, cls(), ['cost'])
+
 
 class Edge(object):
   """Represents an edge in a graph."""
@@ -30,6 +40,16 @@ class Edge(object):
     self.to_node = to_node
     self.cost = 0
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict(
+        {}, self, ['from_node', 'to_node', 'cost'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = cls(None, None)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['from_node', 'to_node', 'cost'])
+
 
 class DirectedGraph(object):
   """Directed graph.
@@ -37,6 +57,10 @@ class DirectedGraph(object):
   A graph is identified by a list of nodes and a list of edges. It does not need
   to be acyclic, but then some methods will fail.
   """
+  __GRAPH_NODE_INDEX = '__graph_node_index'
+  __TO_NODE_INDEX = '__to_node_index'
+  __FROM_NODE_INDEX = '__from_node_index'
+
   def __init__(self, nodes, edges):
     """Builds a graph from a set of node and edges.
 
@@ -185,3 +209,40 @@ class DirectedGraph(object):
             else costliest_node, predecessors)
         path_list.insert(0, node)
     return max_cost
+
+  def ToJsonDict(self):
+    node_dicts = []
+    node_to_index = {node: index for (index, node) in enumerate(self._nodes)}
+    for (node, index) in node_to_index.items():
+      node_dict = node.ToJsonDict()
+      assert self.__GRAPH_NODE_INDEX not in node_dict
+      node_dict.update({self.__GRAPH_NODE_INDEX: index})
+      node_dicts.append(node_dict)
+    edge_dicts = []
+    for edge in self._edges:
+      edge_dict = edge.ToJsonDict()
+      assert self.__TO_NODE_INDEX not in edge_dict
+      assert self.__FROM_NODE_INDEX not in edge_dict
+      edge_dict.update({self.__TO_NODE_INDEX: node_to_index[edge.to_node],
+                        self.__FROM_NODE_INDEX: node_to_index[edge.from_node]})
+      edge_dicts.append(edge_dict)
+    return {'nodes': node_dicts, 'edges': edge_dicts}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict, node_class, edge_class):
+    """Returns an instance from a dict.
+
+    Note that the classes of the nodes and edges need to be specified here.
+    This is done to reduce the likelihood of error.
+    """
+    index_to_node = {
+        node_dict[cls.__GRAPH_NODE_INDEX]: node_class.FromJsonDict(node_dict)
+        for node_dict in json_dict['nodes']}
+    edges = []
+    for edge_dict in json_dict['edges']:
+      edge = edge_class.FromJsonDict(edge_dict)
+      edge.from_node = index_to_node[edge_dict[cls.__FROM_NODE_INDEX]]
+      edge.to_node = index_to_node[edge_dict[cls.__TO_NODE_INDEX]]
+      edges.append(edge)
+    result = DirectedGraph(index_to_node.values(), edges)
+    return result
diff --git a/loading/graph_unittest.py b/loading/graph_unittest.py
index e8ef761..e0e5f5b 100644
--- a/loading/graph_unittest.py
+++ b/loading/graph_unittest.py
@@ -7,42 +7,61 @@ import os
 import sys
 import unittest
 
+import common_util
 import graph
 
 
 class _IndexedNode(graph.Node):
-  def __init__(self, index):
+  def __init__(self, index=None):
     super(_IndexedNode, self).__init__()
     self.index = index
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict(
+        super(_IndexedNode, self).ToJsonDict(), self, ['index'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = super(_IndexedNode, cls).FromJsonDict(json_dict)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['index'])
+
 
 class GraphTestCase(unittest.TestCase):
   @classmethod
-  def MakeGraph(cls, count, edge_tuples):
+  def MakeGraph(cls, count, edge_tuples, serialize=False):
     """Makes a graph from a list of edges.
 
     Args:
       count: Number of nodes.
       edge_tuples: (from_index, to_index). Both indices must be in [0, count),
-                   and uniquely identify a node.
+                   and uniquely identify a node. Must be sorted
+                   lexicographically by node indices.
     """
     nodes = [_IndexedNode(i) for i in xrange(count)]
     edges = [graph.Edge(nodes[from_index], nodes[to_index])
              for (from_index, to_index) in edge_tuples]
-    return (nodes, edges, graph.DirectedGraph(nodes, edges))
+    g = graph.DirectedGraph(nodes, edges)
+    if serialize:
+      g = graph.DirectedGraph.FromJsonDict(
+          g.ToJsonDict(), _IndexedNode, graph.Edge)
+      nodes = sorted(g.Nodes(), key=operator.attrgetter('index'))
+      edges = sorted(g.Edges(), key=operator.attrgetter(
+          'from_node.index', 'to_node.index'))
+    return (nodes, edges, g)
 
   @classmethod
   def _NodesIndices(cls, g):
     return map(operator.attrgetter('index'), g.Nodes())
 
-  def testBuildGraph(self):
+  def testBuildGraph(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
     self.assertSetEqual(set(edges), set(g.Edges()))
 
@@ -72,14 +91,14 @@ class GraphTestCase(unittest.TestCase):
     self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
     self.assertEqual(5, len(g.Edges()))
 
-  def testUpdateEdge(self):
+  def testUpdateEdge(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     edge = edges[1]
     self.assertTrue(edge in g.OutEdges(nodes[0]))
     self.assertTrue(edge in g.InEdges(nodes[2]))
@@ -89,28 +108,28 @@ class GraphTestCase(unittest.TestCase):
     self.assertTrue(edge in g.OutEdges(nodes[2]))
     self.assertTrue(edge in g.InEdges(nodes[3]))
 
-  def testTopologicalSort(self):
+  def testTopologicalSort(self, serialize=False):
     (_, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     sorted_nodes = g.TopologicalSort()
     node_to_sorted_index = dict(zip(sorted_nodes, xrange(len(sorted_nodes))))
     for e in edges:
       self.assertTrue(
           node_to_sorted_index[e.from_node] < node_to_sorted_index[e.to_node])
 
-  def testReachableNodes(self):
+  def testReachableNodes(self, serialize=False):
     (nodes, _, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     self.assertSetEqual(
         set([0, 1, 2, 3, 4]),
         set(n.index for n in g.ReachableNodes([nodes[0]])))
@@ -124,14 +143,14 @@ class GraphTestCase(unittest.TestCase):
         set([6]),
         set(n.index for n in g.ReachableNodes([nodes[6]])))
 
-  def testCost(self):
+  def testCost(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     for (i, node) in enumerate(nodes):
       node.cost = i + 1
     nodes[6].cost = 6
@@ -146,14 +165,14 @@ class GraphTestCase(unittest.TestCase):
     g.Cost(path_list=path_list)
     self.assertListEqual([nodes[i] for i in (5, 6)], path_list)
 
-  def testCostWithRoots(self):
+  def testCostWithRoots(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     for (i, node) in enumerate(nodes):
       node.cost = i + 1
     nodes[6].cost = 9
@@ -165,6 +184,15 @@ class GraphTestCase(unittest.TestCase):
     self.assertEqual(15, g.Cost(roots=[nodes[0]], path_list=path_list))
     self.assertListEqual([nodes[i] for i in (0, 1, 3, 4)], path_list)
 
+  def testSerialize(self):
+    # Re-do tests with a deserialized graph.
+    self.testBuildGraph(True)
+    self.testUpdateEdge(True)
+    self.testTopologicalSort(True)
+    self.testReachableNodes(True)
+    self.testCost(True)
+    self.testCostWithRoots(True)
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 8a172bd..85dd48d 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -8,47 +8,103 @@ For example, this can be used to evaluate NoState Prefetch
 (https://goo.gl/B3nRUR).
 
 When executed as a script, takes a trace as a command-line arguments and shows
-how many requests were prefetched.
+statistics about it.
 """
 
 import itertools
 import operator
 
+import common_util
 import dependency_graph
+import graph
 import loading_trace
 import user_satisfied_lens
 import request_dependencies_lens
 import request_track
 
 
-class PrefetchSimulationView(object):
+class RequestNode(dependency_graph.RequestNode):
   """Simulates the effect of prefetching resources discoverable by the preload
   scanner.
   """
+  _ATTRS = ['preloaded', 'before']
+  def __init__(self, request=None):
+    super(RequestNode, self).__init__(request)
+    self.preloaded = False
+    self.before = False
+
+  def ToJsonDict(self):
+    result = super(RequestNode, self).ToJsonDict()
+    return common_util.SerializeAttributesToJsonDict(result, self, self._ATTRS)
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = super(RequestNode, cls).FromJsonDict(json_dict)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, cls._ATTRS)
+
+
+class PrefetchSimulationView(object):
+  """Simulates the effect of prefetch."""
   def __init__(self, trace, dependencies_lens, user_lens):
-    """Initializes an instance of PrefetchSimulationView.
+    self.postload_msec = None
+    self.graph = None
+    if trace is None:
+      return
+    requests = trace.request_track.GetEvents()
+    critical_requests_ids = user_lens.CriticalRequests()
+    self.postload_msec = user_lens.PostloadTimeMsec()
+    self.graph = dependency_graph.RequestDependencyGraph(
+        requests, dependencies_lens, node_class=RequestNode)
+    preloaded_requests = [r.request_id for r in self._PreloadedRequests(
+        requests[0], dependencies_lens, trace)]
+    self._AnnotateNodes(self.graph.graph.Nodes(), preloaded_requests,
+                        critical_requests_ids)
+
+  def Cost(self):
+    """Returns the cost of the graph, restricted to the critical requests."""
+    pruned_graph = self._PrunedGraph()
+    return pruned_graph.Cost() + self.postload_msec
+
+  def UpdateNodeCosts(self, node_to_cost):
+    """Updates the cost of nodes, according to |node_to_cost|.
 
     Args:
-      trace: (LoadingTrace) a loading trace.
-      dependencies_lens: (RequestDependencyLens) request dependencies.
-      user_lens: (UserSatisfiedLens) Lens used to compute costs.
+      node_to_cost: (Callable) RequestNode -> float. Callable returning the cost
+                    of a node.
     """
-    self.trace = trace
-    self.dependencies_lens = dependencies_lens
-    self._resource_events = self.trace.tracing_track.Filter(
-        categories=set([u'blink.net']))
-    assert len(self._resource_events.GetEvents()) > 0,\
-            'Was the "blink.net" category enabled at trace collection time?"'
-    self._user_lens = user_lens
-    request_ids = self._user_lens.CriticalRequests()
-    all_requests = self.trace.request_track.GetEvents()
-    self._first_request_node = all_requests[0].request_id
-    requests = [r for r in all_requests if r.request_id in request_ids]
-    self.graph = dependency_graph.RequestDependencyGraph(
-        requests, self.dependencies_lens)
-
-  def ParserDiscoverableRequests(self, request, recurse=False):
-    """Returns a list of requests discovered by the parser from a given request.
+    pruned_graph = self._PrunedGraph()
+    for node in pruned_graph.Nodes():
+      node.cost = node_to_cost(node)
+
+  def ToJsonDict(self):
+    """Returns a dict representing this instance."""
+    result = {'graph': self.graph.ToJsonDict()}
+    return common_util.SerializeAttributesToJsonDict(
+        result, self, ['postload_msec'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns an instance of PrefetchSimulationView from a dict dumped by
+    ToJSonDict().
+    """
+    result = cls(None, None, None)
+    result.graph = dependency_graph.RequestDependencyGraph.FromJsonDict(
+        json_dict['graph'], RequestNode, dependency_graph.Edge)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['postload_msec'])
+
+  @classmethod
+  def _AnnotateNodes(cls, nodes, preloaded_requests_ids,
+                     critical_requests_ids,):
+    for node in nodes:
+      node.preloaded = node.request.request_id in preloaded_requests_ids
+      node.before = node.request.request_id in critical_requests_ids
+
+  @classmethod
+  def _ParserDiscoverableRequests(
+      cls, dependencies_lens, request, recurse=False):
+    """Returns a list of requests IDs dicovered by the parser.
 
     Args:
       request: (Request) Root request.
@@ -59,18 +115,20 @@ class PrefetchSimulationView(object):
     # TODO(lizeb): handle the recursive case.
     assert not recurse
     discoverable_requests = [request]
-    first_request = self.dependencies_lens.GetRedirectChain(request)[-1]
-    deps = self.dependencies_lens.GetRequestDependencies()
+    first_request = dependencies_lens.GetRedirectChain(request)[-1]
+    deps = dependencies_lens.GetRequestDependencies()
     for (first, second, reason) in deps:
       if first.request_id == first_request.request_id and reason == 'parser':
         discoverable_requests.append(second)
     return discoverable_requests
 
-  def ExpandRedirectChains(self, requests):
+  @classmethod
+  def _ExpandRedirectChains(cls, requests, dependencies_lens):
     return list(itertools.chain.from_iterable(
-        [self.dependencies_lens.GetRedirectChain(r) for r in requests]))
+        [dependencies_lens.GetRedirectChain(r) for r in requests]))
 
-  def PreloadedRequests(self, request):
+  @classmethod
+  def _PreloadedRequests(cls, request, dependencies_lens, trace):
     """Returns the requests that have been preloaded from a given request.
 
     This list is the set of request that are:
@@ -84,51 +142,48 @@ class PrefetchSimulationView(object):
 
     Returns:
       A list of Request. Does not include the root request. This list is a
-      subset of the one returned by ParserDiscoverableRequests().
+      subset of the one returned by _ParserDiscoverableRequests().
     """
     # Preload step events are emitted in ResourceFetcher::preloadStarted().
+    resource_events = trace.tracing_track.Filter(
+        categories=set([u'blink.net']))
     preload_step_events = filter(
         lambda e:  e.args.get('step') == 'Preload',
-        self._resource_events.GetEvents())
+        resource_events.GetEvents())
     preloaded_urls = set()
     for preload_step_event in preload_step_events:
-      preload_event = self._resource_events.EventFromStep(preload_step_event)
+      preload_event = resource_events.EventFromStep(preload_step_event)
       if preload_event:
         preloaded_urls.add(preload_event.args['url'])
-    parser_requests = self.ParserDiscoverableRequests(request)
+    parser_requests = cls._ParserDiscoverableRequests(
+        dependencies_lens, request)
     preloaded_root_requests = filter(
         lambda r: r.url in preloaded_urls, parser_requests)
     # We can actually fetch the whole redirect chain.
     return [request] + list(itertools.chain.from_iterable(
-        [self.dependencies_lens.GetRedirectChain(r)
+        [dependencies_lens.GetRedirectChain(r)
          for r in preloaded_root_requests]))
 
+  def _PrunedGraph(self):
+    roots = self.graph.graph.RootNodes()
+    nodes = self.graph.graph.ReachableNodes(
+        roots, should_stop=lambda n: not n.before)
+    return graph.DirectedGraph(nodes, self.graph.graph.Edges())
 
-def _PrintSummary(prefetch_view, user_lens):
-  requests = prefetch_view.trace.request_track.GetEvents()
-  first_request = prefetch_view.trace.request_track.GetEvents()[0]
-  parser_requests = prefetch_view.ExpandRedirectChains(
-      prefetch_view.ParserDiscoverableRequests(first_request))
-  preloaded_requests = prefetch_view.ExpandRedirectChains(
-      prefetch_view.PreloadedRequests(first_request))
-  print '%d requests, %d parser from the main request, %d preloaded' % (
-      len(requests), len(parser_requests), len(preloaded_requests))
-  print 'Time to user satisfaction: %.02fms' % (
-      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
 
-  print 'With 0-cost prefetched resources...'
-  new_costs = {r.request_id: 0. for r in preloaded_requests}
-  prefetch_view.graph.UpdateRequestsCost(new_costs)
-  print 'Time to user satisfaction: %.02fms' % (
-      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
+def _PrintSumamry(trace, dependencies_lens, user_lens):
+  prefetch_view = PrefetchSimulationView(trace, dependencies_lens, user_lens)
+  print 'Time to First Contentful Paint = %.02fms' % prefetch_view.Cost()
+  print 'Set costs of prefetched requests to 0.'
+  prefetch_view.UpdateNodeCosts(lambda n: 0 if n.preloaded else n.cost)
+  print 'Time to First Contentful Paint = %.02fms' % prefetch_view.Cost()
 
 
 def main(filename):
   trace = loading_trace.LoadingTrace.FromJsonFile(filename)
   dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
   user_lens = user_satisfied_lens.FirstContentfulPaintLens(trace)
-  prefetch_view = PrefetchSimulationView(trace, dependencies_lens, user_lens)
-  _PrintSummary(prefetch_view, user_lens)
+  _PrintSumamry(trace, dependencies_lens, user_lens)
 
 
 if __name__ == '__main__':
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index ca8897d..e222bf5 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -4,7 +4,7 @@
 
 import unittest
 
-import prefetch_view
+from prefetch_view import PrefetchSimulationView
 import request_dependencies_lens
 from request_dependencies_lens_unittest import TestRequests
 import request_track
@@ -20,13 +20,13 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
     self.assertListEqual(
         [TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST],
-        self.prefetch_view.ExpandRedirectChains(
-            [TestRequests.FIRST_REDIRECT_REQUEST]))
+        PrefetchSimulationView._ExpandRedirectChains(
+            [TestRequests.FIRST_REDIRECT_REQUEST], self.dependencies_lens))
 
   def testParserDiscoverableRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    discovered_requests = self.prefetch_view.ParserDiscoverableRequests(
-        first_request)
+    discovered_requests = PrefetchSimulationView._ParserDiscoverableRequests(
+        self.dependencies_lens, first_request)
     self.assertListEqual(
         [TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
@@ -34,7 +34,8 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
 
   def testPreloadedRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+        first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([first_request], preloaded_requests)
     self._SetUp(
         [{'args': {'url': 'http://bla.com/nyancat.js'},
@@ -43,22 +44,51 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
          {'args': {'step': 'Preload'}, 'cat': 'blink.net',
           'id': '0xaf9f14fa9dd6c314', 'name': 'Resource', 'ph': 'T',
           'ts': 12, 'pid': 12, 'tid': 12}])
-    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+        first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
          TestRequests.JS_REQUEST_UNRELATED_FRAME], preloaded_requests)
 
+  def testCost(self):
+    self.assertEqual(40 + 12, self.prefetch_view.Cost())
+
+  def testUpdateNodeCosts(self):
+    self.prefetch_view.UpdateNodeCosts(lambda _: 100)
+    self.assertEqual(500 + 40 + 12, self.prefetch_view.Cost())
+
+  def testUpdateNodeCostsPartial(self):
+    self.prefetch_view.UpdateNodeCosts(
+        lambda n: 100 if (n.request.request_id
+                          == TestRequests.REDIRECTED_REQUEST.request_id) else 0)
+    self.assertEqual(100 + 40 + 12, self.prefetch_view.Cost())
+
+  def testToFromJsonDict(self):
+    self.assertEqual(40 + 12, self.prefetch_view.Cost())
+    json_dict = self.prefetch_view.ToJsonDict()
+    new_view = PrefetchSimulationView.FromJsonDict(json_dict)
+    self.assertEqual(40 + 12, new_view.Cost())
+    # Updated Costs.
+    self.prefetch_view.UpdateNodeCosts(lambda _: 100)
+    self.assertEqual(500 + 40 + 12, self.prefetch_view.Cost())
+    json_dict = self.prefetch_view.ToJsonDict()
+    new_view = PrefetchSimulationView.FromJsonDict(json_dict)
+    self.assertEqual(500 + 40 + 12, new_view.Cost())
+
   def _SetUp(self, added_trace_events=None):
     trace_events = [
         {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'blink.net'}]
     if added_trace_events is not None:
       trace_events += added_trace_events
     self.trace = TestRequests.CreateLoadingTrace(trace_events)
-    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+    self.dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
     self.user_satisfied_lens = test_utils.MockUserSatisfiedLens(self.trace)
-    self.prefetch_view = prefetch_view.PrefetchSimulationView(
-        self.trace, dependencies_lens, self.user_satisfied_lens)
+    self.user_satisfied_lens._postload_msec = 12
+    self.prefetch_view = PrefetchSimulationView(
+        self.trace, self.dependencies_lens, self.user_satisfied_lens)
+    for e in self.prefetch_view.graph.graph.Edges():
+      e.cost = 10
 
 
 if __name__ == '__main__':
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 0115f8c..89ce7a1 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -10,6 +10,8 @@ FirstSignificantPaintLens.
 import logging
 import operator
 
+import common_util
+
 
 class _UserSatisfiedLens(object):
   """A base class for all user satisfaction metrics.
@@ -19,6 +21,9 @@ class _UserSatisfiedLens(object):
   event. Subclasses need only provide the time computation. The base class will
   use that to construct the request ids.
   """
+  _ATTRS = ['_satisfied_msec', '_event_msec', '_postload_msec',
+            '_critical_request_ids']
+
   def __init__(self, trace):
     """Initialize the lens.
 
@@ -27,6 +32,10 @@ class _UserSatisfiedLens(object):
     """
     self._satisfied_msec = None
     self._event_msec = None
+    self._postload_msec = None
+    self._critical_request_ids = None
+    if trace is None:
+      return
     self._CalculateTimes(trace.tracing_track)
     critical_requests = self._RequestsBefore(
         trace.request_track, self._satisfied_msec)
@@ -57,6 +66,15 @@ class _UserSatisfiedLens(object):
     """
     return self._postload_msec
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict({}, self, self._ATTRS)
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = cls(None)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, cls._ATTRS)
+
   def _CalculateTimes(self, tracing_track):
     """Subclasses should implement to set _satisfied_msec and _event_msec."""
     raise NotImplementedError

commit d14970e46ed5efc717475c20ce29a554f5e78c71
Author: blundell <blundell@chromium.org>
Date:   Wed Apr 6 05:55:41 2016 -0700

    tools/android/loading: Avoid using analyze.py in GCE code
    
    This CL changes tools/android/loading/gce/main.py to use the relevant
    tools/android/loading libraries directly rather than invoking analyze.py
    via subprocess. The concrete motivation is to be able to directly access
    LoadingTrace objects after they're generated.
    
    Review URL: https://codereview.chromium.org/1863603002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385443}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 178ce7d179e5dff0218e68ad826b25c0ae9bc4ca

diff --git a/loading/controller.py b/loading/controller.py
index 053e062..b6616f9 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -299,8 +299,9 @@ class LocalChromeController(ChromeControllerBase):
     self._headless = headless
 
   @contextlib.contextmanager
-  def Open(self):
-    """Override for connection context."""
+  def OpenWithRedirection(self, stdout, stderr):
+    """Override for connection context. stdout and stderr are passed to the
+       child processes used to run Chrome and XVFB."""
     chrome_cmd = [OPTIONS.local_binary]
     chrome_cmd.extend(self._GetChromeArguments())
     chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
@@ -309,16 +310,16 @@ class LocalChromeController(ChromeControllerBase):
     #   - To find the correct target descriptor at devtool connection;
     #   - To avoid cache and WPR pollution by the NTP.
     chrome_cmd.append('about:blank')
-    chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     environment = os.environ.copy()
     if self._headless:
       environment['DISPLAY'] = 'localhost:99'
       xvfb_process = subprocess.Popen(
           ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
-          stderr=chrome_out)
+          stdout=stdout, stderr=stderr)
     logging.debug(subprocess.list2cmdline(chrome_cmd))
     chrome_process = subprocess.Popen(chrome_cmd, shell=False,
-                                      stderr=chrome_out, env=environment)
+                                      stdout=stdout, stderr=stderr,
+                                      env=environment)
     connection = None
     try:
       time.sleep(10)
@@ -340,6 +341,13 @@ class LocalChromeController(ChromeControllerBase):
       if self._headless:
         xvfb_process.kill()
 
+  def Open(self):
+    """Wrapper around the more-specialized version of Open() above that sets
+    the value of stdout/stderr based on the value of OPTIONS.local_noisy."""
+    stdout = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+    stderr = stdout
+    return self.OpenWithRedirection(self, stdout, stderr)
+
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
     self._EnsureProfileDirectory()
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 86d24a6..c20f0ae 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -8,11 +8,22 @@ import re
 import threading
 import time
 import subprocess
+import sys
 
 from gcloud import storage
 from gcloud.exceptions import NotFound
 from oauth2client.client import GoogleCredentials
 
+# NOTE: The parent directory needs to be first in sys.path to avoid conflicts
+# with catapult modules that have colliding names, as catapult inserts itself
+# into the path as the second element. This is an ugly and fragile hack.
+sys.path.insert(0,
+    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
+import controller
+import loading_trace
+import options
+
+
 class ServerApp(object):
   """Simple web server application, collecting traces and writing them in
   Google Cloud Storage.
@@ -45,9 +56,11 @@ class ServerApp(object):
          if not self._base_path_in_bucket.endswith('/'):
            self._base_path_in_bucket += '/'
 
-       self._chrome_path = config['chrome_path']
        self._src_path = config['src_path']
 
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs([])
+    options.OPTIONS.local_binary = config['chrome_path']
 
   def _GetStorageClient(self):
     return storage.Client(project = self._project_name,
@@ -91,8 +104,7 @@ class ServerApp(object):
 
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
-    """ Generates a trace using analyze.py
-    Runs on _thread.
+    """ Generates a trace on _thread.
 
     Args:
       url: URL as a string.
@@ -108,18 +120,43 @@ class ServerApp(object):
       os.remove(filename)  # Remove any existing trace for this URL.
     except OSError:
       pass  # Nothing to remove.
-    analyze_path = self._src_path + '/tools/android/loading/analyze.py'
-    command_line = ['python', analyze_path, 'log_requests', '--local_noisy',
-        '--clear_cache', '--local', '--headless', '--local_binary',
-        self._chrome_path, '--url', url, '--output', filename]
-    if len(emulate_device):
-      command_line += ['--emulate_device', emulate_device]
-    if len(emulate_network):
-      command_line += ['--emulate_network', emulate_network]
-    with open(log_filename, 'w') as log_file:
-      ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
-                            stdout = log_file)
-    return ret == 0
+
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
+
+    old_stdout = sys.stdout
+    old_stderr = sys.stderr
+
+    succeeded = True
+    try:
+      with open(log_filename, 'w') as sys.stdout:
+        sys.stderr = sys.stdout
+
+        # Set up the controller.
+        chrome_ctl = controller.LocalChromeController()
+        chrome_ctl.SetHeadless(True)
+        if emulate_device:
+          chrome_ctl.SetDeviceEmulation(emulate_device)
+        if emulate_network:
+          chrome_ctl.SetNetworkEmulation(emulate_network)
+
+        # Record and write the trace.
+        with chrome_ctl.OpenWithRedirection(sys.stdout,
+                                            sys.stderr) as connection:
+          connection.ClearCache()
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url, connection, chrome_ctl.ChromeMetadata())
+    except Exception as e:
+      succeeded = False
+      sys.stderr.write(e)
+
+    sys.stdout = old_stdout
+    sys.stderr = old_stderr
+
+    with open(filename, 'w') as f:
+      json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+
+    return succeeded
 
   def _GetCurrentTaskCount(self):
     """Returns the number of remaining tasks. Thread safe."""
@@ -129,9 +166,8 @@ class ServerApp(object):
     return task_count
 
   def _ProcessTasks(self, tasks, repeat_count, emulate_device, emulate_network):
-    """Iterates over _tasks and runs analyze.py on each of them. Uploads the
-    resulting traces to Google Cloud Storage.
-    Runs on _thread.
+    """Iterates over _task, generating a trace for each of them. Uploads the
+    resulting traces to Google Cloud Storage.  Runs on _thread.
 
     Args:
       tasks: The list of URLs to process.
@@ -146,6 +182,8 @@ class ServerApp(object):
     self._tasks_lock.release()
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
+
+    # TODO(blundell): Fix this up.
     logs_dir = self._base_path_in_bucket + 'analyze_logs/'
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
@@ -161,13 +199,13 @@ class ServerApp(object):
           print 'Uploading: %s' % remote_filename
           self._UploadFile(local_filename, traces_dir + remote_filename)
         else:
-          print 'analyze.py failed for URL: %s' % url
+          print 'Trace generation failed for URL: %s' % url
           self._tasks_lock.acquire()
           self._failed_tasks.append({ "url": url, "repeat": repeat})
           self._tasks_lock.release()
           if os.path.isfile(local_filename):
             self._UploadFile(local_filename, failures_dir + remote_filename)
-        print 'Uploading analyze log'
+        print 'Uploading log'
         self._UploadFile(log_filename, logs_dir + remote_filename)
       # Pop once task is finished, for accurate status tracking.
       self._tasks_lock.acquire()

commit 15a7665e8d3063d783c6b22c5f7daea752a56c83
Author: droger <droger@chromium.org>
Date:   Tue Apr 5 07:54:53 2016 -0700

    tools/android/loading Ignore 'p' trace events
    
    These events can happen in practice, and thus the analysis should not
    crash in that case. Instead, these events are now ignored.
    
    The error was:
    
    __init__ at tools/android/loading/tracing.py:318
      'Deprecated event: %s' % tracing_event)
    
    DevToolsConnectionException: Deprecated event: {u'name': u'EmbeddedWorkerInstance::Start', u'tts': 163020, u'args': {u'step': u'OnProcessAllocated', u'Is New Process': False}, u'pid': 13875, u'ts': 517201903, u'cat': u'ServiceWorker', u'tid': 13898, u'ph': u'p', u'id': u'0xaf5594c526a89982'}
    
    Review URL: https://codereview.chromium.org/1853753004
    
    Cr-Original-Commit-Position: refs/heads/master@{#385174}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6c2cfd7db70f6686439c78d8829a603c3605fb6e

diff --git a/loading/tracing.py b/loading/tracing.py
index 5d0a647..fb54d70 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -207,6 +207,7 @@ class TracingTrack(devtools_monitor.Track):
           'M': self._Ignore,
           'X': self._Ignore,
           'R': self._Ignore,
+          'p': self._Ignore,
           '(': self._Ignore, # Context events.
           ')': self._Ignore, # Ditto.
           None: self._Ignore,
@@ -313,9 +314,6 @@ class Event(object):
     if not synthetic and tracing_event['ph'] in ['s', 't', 'f']:
       raise devtools_monitor.DevToolsConnectionException(
           'Unsupported event: %s' % tracing_event)
-    if not synthetic and tracing_event['ph'] in ['p']:
-      raise devtools_monitor.DevToolsConnectionException(
-          'Deprecated event: %s' % tracing_event)
 
     self._tracing_event = tracing_event
     # Note tracing event times are in microseconds.

commit c0fa12c6235e61a83a7272ee7ea83b1fb1a49200
Author: droger <droger@chromium.org>
Date:   Tue Apr 5 07:45:44 2016 -0700

    tools/android/loading Fix crash when popping task in main.py
    
    The function popleft does not exist on list.
    Popping from the other end instead.
    
    Review URL: https://codereview.chromium.org/1858223002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385172}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 67d4b25201f8dd0b32b815b68372bf64085dbffd

diff --git a/loading/gce/main.py b/loading/gce/main.py
index 66bd7dd..86d24a6 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -151,7 +151,7 @@ class ServerApp(object):
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
     while len(self._tasks) > 0:
-      url = self._tasks[0]
+      url = self._tasks[-1]
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
@@ -171,7 +171,7 @@ class ServerApp(object):
         self._UploadFile(log_filename, logs_dir + remote_filename)
       # Pop once task is finished, for accurate status tracking.
       self._tasks_lock.acquire()
-      url = self._tasks.popleft()
+      url = self._tasks.pop()
       self._tasks_lock.release()
 
     if len(self._failed_tasks) > 0:

commit ad4e8ff5457b683afe3612b60132f659a594d4f9
Author: dgn <dgn@chromium.org>
Date:   Tue Apr 5 07:07:41 2016 -0700

    [tool] SpnegoAuth: add multi account, credential confirmation
    
    Add more features to the SpnegoAuthenticator:
    
     -  Set up up to 2 accounts.
     -  Account 1 will start authenticated.
     -  Account 2 will start unauthenticated. The first token request
        will require an additional confirmation step.
     -  Accounts can be added and removed from the Android account
        settings screen
    
    BUG=534293
    
    Review URL: https://codereview.chromium.org/1416443003
    
    Cr-Original-Commit-Position: refs/heads/master@{#385165}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e4cefacb5bc08b92c284a8d7a3deaaaa2545a04d

diff --git a/kerberos/README.md b/kerberos/README.md
index c8aecb2..4daa473 100644
--- a/kerberos/README.md
+++ b/kerberos/README.md
@@ -62,14 +62,22 @@ $CHROMIUM_SRC/tools/android/kerberos/negotiate_test_server.py
 
 ### SpnegoAuthenticator
 
-This app declares and sets up an account type to be used for Negotiate auth.
-When Chrome makes a request for the `org.chromium.tools.SpnegoAuthenticator`
-account type, it returns the dummy account, allowing Chrome to continue the
-request.
+This app declares and sets up an accounts to be used for Negotiate auth, as
+described in the chromium.org wiki
+([Writing a SPNEGO Authenticator for Chrome on Android][crwiki]).
+Those accounts use the type `org.chromium.tools.SpnegoAuthenticator`.
 
-See [Writing a SPNEGO Authenticator for Chrome on Android][crwiki] on
-chromium.org for more information.
+![Account administration activity preview][screenshot]
+
+Features:
+
+ -  Set up up to 2 accounts.
+ -  Account 1 will start authenticated.
+ -  Account 2 will start unauthenticated. The first token request will require
+    an additional confirmation step.
+ -  Accounts can be added and removed from the Android account settings screen
 
 [testdpc-play]: https://play.google.com/store/apps/details?id=com.sample.android.testdpc
 [testdpc-gh]: https://github.com/googlesamples/android-testdpc
 [crwiki]:https://sites.google.com/a/chromium.org/dev/developers/design-documents/http-authentication/writing-a-spnego-authenticator-for-chrome-on-android
+[screenshot]:SpnegoAuthenticator/preview.png
diff --git a/kerberos/SpnegoAuthenticator/AndroidManifest.xml b/kerberos/SpnegoAuthenticator/AndroidManifest.xml
index efd4f33..bb48308 100644
--- a/kerberos/SpnegoAuthenticator/AndroidManifest.xml
+++ b/kerberos/SpnegoAuthenticator/AndroidManifest.xml
@@ -9,6 +9,10 @@
 
     <uses-sdk android:minSdkVersion="21" android:targetSdkVersion="23" />
 
+    <!--
+      Deprecated permissions. Normal protection level, autogranted. Needed
+      for API level 22 and before.
+    -->
     <uses-permission android:name="android.permission.AUTHENTICATE_ACCOUNTS" />
     <uses-permission android:name="android.permission.MANAGE_ACCOUNTS" />
 
@@ -27,6 +31,14 @@
                 android:name="android.accounts.AccountAuthenticator"
                 android:resource="@xml/spnego_authenticator" />
         </service>
+
+        <!-- exported=true needed so that chrome can start the activity -->
+        <activity
+           android:exported="true"
+           android:label="@string/title_activity_account_authenticator"
+           android:name=".SpnegoAuthenticatorActivity"
+           android:noHistory="true">
+       </activity>
     </application>
 
 </manifest>
diff --git a/kerberos/SpnegoAuthenticator/BUILD.gn b/kerberos/SpnegoAuthenticator/BUILD.gn
index 3ce7ea4..e6e8f89 100644
--- a/kerberos/SpnegoAuthenticator/BUILD.gn
+++ b/kerberos/SpnegoAuthenticator/BUILD.gn
@@ -18,7 +18,10 @@ android_apk("spnego_authenticator_apk") {
   ]
 
   java_files = [
+    "src/org/chromium/tools/spnegoauthenticator/AccountData.java",
+    "src/org/chromium/tools/spnegoauthenticator/Constants.java",
     "src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java",
+    "src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java",
     "src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorService.java",
   ]
 }
diff --git a/kerberos/SpnegoAuthenticator/preview.png b/kerberos/SpnegoAuthenticator/preview.png
new file mode 100644
index 0000000..82fd331
Binary files /dev/null and b/kerberos/SpnegoAuthenticator/preview.png differ
diff --git a/kerberos/SpnegoAuthenticator/res/layout/activity_account_authenticator.xml b/kerberos/SpnegoAuthenticator/res/layout/activity_account_authenticator.xml
new file mode 100644
index 0000000..275384d
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/res/layout/activity_account_authenticator.xml
@@ -0,0 +1,53 @@
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+  xmlns:tools="http://schemas.android.com/tools"
+  android:layout_width="match_parent"
+  android:layout_height="match_parent"
+  android:paddingTop="16dp"
+  android:paddingBottom="16dp"
+  android:paddingStart="16dp"
+  android:paddingEnd="16dp"
+  android:gravity="center_horizontal"
+  android:orientation="vertical"
+  android:theme="@android:style/Theme.Material"
+  tools:context=".SpnegoAuthenticatorActivity">
+
+  <ScrollView
+    android:layout_width="match_parent"
+    android:layout_height="match_parent">
+
+    <LinearLayout
+      android:id="@+id/login_form"
+      android:layout_width="match_parent"
+      android:layout_height="wrap_content"
+      android:orientation="vertical">
+
+      <Button
+        android:id="@+id/sign_in_button_1"
+        style="?android:textAppearanceSmall"
+        android:textStyle="bold"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_marginTop="16dp"
+        android:text="@string/action_sign_in_1"/>
+
+      <Button
+        android:id="@+id/sign_in_button_2"
+        style="?android:textAppearanceSmall"
+        android:textStyle="bold"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_marginTop="16dp"
+        android:text="@string/action_sign_in_2"/>
+
+      <Button
+        android:id="@+id/confirm_credentials_button"
+        style="?android:textAppearanceSmall"
+        android:textStyle="bold"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_marginTop="16dp"
+        android:text="@string/action_confirm_credentials"/>
+
+    </LinearLayout>
+  </ScrollView>
+</LinearLayout>
diff --git a/kerberos/SpnegoAuthenticator/res/values/strings.xml b/kerberos/SpnegoAuthenticator/res/values/strings.xml
index 1fb5555..2427630 100644
--- a/kerberos/SpnegoAuthenticator/res/values/strings.xml
+++ b/kerberos/SpnegoAuthenticator/res/values/strings.xml
@@ -1,4 +1,8 @@
 <resources>
     <string name="app_name">SpnegoAuthenticator</string>
+    <string name="action_sign_in_1">Sign In Account 1</string>
+    <string name="action_sign_in_2">Sign In Account 2</string>
+    <string name="action_confirm_credentials">Confirm Credentials</string>
+    <string name="title_activity_account_authenticator">Spnego Authenticator Sign In</string>
     <string name="account_type">org.chromium.tools.SpnegoAuthenticator</string>
 </resources>
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/AccountData.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/AccountData.java
new file mode 100644
index 0000000..5c68378
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/AccountData.java
@@ -0,0 +1,100 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.spnegoauthenticator;
+
+import android.accounts.Account;
+import android.accounts.AccountManager;
+import android.content.Context;
+import android.content.Intent;
+import android.text.TextUtils;
+
+import org.chromium.base.Log;
+import org.chromium.net.HttpNegotiateConstants;
+
+/** Utility class to get and set account data. */
+class AccountData {
+    private static final String TAG = Constants.TAG;
+    private static final String OPT_KEY_AUTH = "isAuthenticated";
+    private static final String OPT_VALUE_AUTH = "YES";
+    private final Account mAccount;
+    private final String mPassword;
+    private boolean mIsAuthenticated;
+
+    private AccountData(Account account, boolean isAuthenticated) {
+        Log.d(TAG, "AccountData(name=%s, isAuthenticated=%s", account.name, isAuthenticated);
+        mAccount = account;
+        mIsAuthenticated = isAuthenticated;
+        mPassword = "userPass";
+    }
+
+    /** Creates some new account data. */
+    public static AccountData create(String name, Context context) {
+        Account account = new Account(name, context.getString(R.string.account_type));
+        boolean isAuthenticated = Constants.ACCOUNT_1_NAME.equals(name);
+
+        return new AccountData(account, isAuthenticated);
+    }
+
+    /**
+     * Creates a new {@link AccountData} object, looking at previously saved data to
+     * initialize it.
+     */
+    public static AccountData get(String accountName, Context context) {
+        Account account = new Account(accountName, context.getString(R.string.account_type));
+
+        AccountManager am = AccountManager.get(context);
+        String authValue = am.getUserData(account, OPT_KEY_AUTH);
+        boolean isAuthenticated = TextUtils.equals(authValue, OPT_VALUE_AUTH);
+
+        return new AccountData(account, isAuthenticated);
+    }
+
+    /**
+     * Saves the account data with the AccountManager. If the account did not previously
+     * exist, it will be created.
+     */
+    public void save(Context context) {
+        AccountManager am = AccountManager.get(context);
+
+        // Does nothing if the account already exists
+        am.addAccountExplicitly(mAccount, mPassword, null);
+
+        am.setUserData(mAccount, OPT_KEY_AUTH, mIsAuthenticated ? OPT_VALUE_AUTH : null);
+
+        // Is supposed to be send by AccountsService when accounts are modified, but it looks like
+        // the authenticator has to do it itself.
+        context.sendBroadcast(new Intent(AccountManager.LOGIN_ACCOUNTS_CHANGED_ACTION));
+    }
+
+    /** Returns an intent as expected for answers to {@link SpnegoAuthenticator#addAccount}. */
+    public Intent getAccountAddedIntent() {
+        Intent intent = new Intent();
+        intent.putExtra(AccountManager.KEY_ACCOUNT_NAME, mAccount.name);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_TYPE, mAccount.type);
+        return intent;
+    }
+
+    /** Returns an intent as expected for answers to {@link SpnegoAuthenticator#getAuthToken}. */
+    public Intent getCredentialsConfirmedIntent() {
+        Intent intent = new Intent();
+        intent.putExtra(AccountManager.KEY_ACCOUNT_NAME, mAccount.name);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_TYPE, mAccount.type);
+        intent.putExtra(AccountManager.KEY_AUTHTOKEN, Constants.AUTH_TOKEN);
+        intent.putExtra(HttpNegotiateConstants.KEY_SPNEGO_RESULT, 0);
+        return intent;
+    }
+
+    public boolean isAuthenticated() {
+        return mIsAuthenticated;
+    }
+
+    public void setIsAuthenticated(boolean value) {
+        mIsAuthenticated = value;
+    }
+
+    public Account getAccount() {
+        return mAccount;
+    }
+}
\ No newline at end of file
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/Constants.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/Constants.java
new file mode 100644
index 0000000..126bed0
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/Constants.java
@@ -0,0 +1,13 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.spnegoauthenticator;
+
+class Constants {
+    static final String TAG = "tools_SpnegoAuth";
+    static final String AUTH_TOKEN = "DummyAuthToken";
+    static final int CONFIRM_CREDENTIAL_NOTIFICATION_ID = 42;
+    static final String ACCOUNT_1_NAME = "Dummy SpnegoAccount 1";
+    static final String ACCOUNT_2_NAME = "Dummy SpnegoAccount 2";
+}
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java
index 16133ad..7d2c084 100644
--- a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java
@@ -9,7 +9,11 @@ import android.accounts.Account;
 import android.accounts.AccountAuthenticatorResponse;
 import android.accounts.AccountManager;
 import android.accounts.NetworkErrorException;
+import android.app.Notification;
+import android.app.NotificationManager;
+import android.app.PendingIntent;
 import android.content.Context;
+import android.content.Intent;
 import android.os.Bundle;
 
 import org.chromium.base.Log;
@@ -18,66 +22,85 @@ import org.chromium.net.HttpNegotiateConstants;
 import java.util.Arrays;
 
 /**
- * AccountAuthenticator implementation that automatically creates a dummy account and returns a
- * dummy token when asked.
+ * AccountAuthenticator implementation
  */
 public class SpnegoAuthenticator extends AbstractAccountAuthenticator {
-    private static final String TAG = "tools_SpnegoAuth";
-    private static final String ACCOUNT_NAME = "DummySpnegoAccount";
+
+    private static final String TAG = Constants.TAG;
+    private final Context mContext;
 
     /**
      * @param context
      */
     public SpnegoAuthenticator(Context context) {
         super(context);
-        ensureTestAccountExists(context);
+        mContext = context;
     }
 
     @Override
-    public Bundle addAccount(AccountAuthenticatorResponse arg0, String accountType, String arg2,
-            String[] arg3, Bundle arg4) throws NetworkErrorException {
-        Log.w(TAG, "addAccount():  Not supported.");
-        Bundle result = new Bundle();
-        result.putInt(AccountManager.KEY_ERROR_CODE, AccountManager.ERROR_CODE_BAD_REQUEST);
-        result.putString(AccountManager.KEY_ERROR_MESSAGE, "Can't add new SPNEGO accounts");
-        return result;
+    public Bundle addAccount(AccountAuthenticatorResponse response, String accountType,
+            String authTokenType, String[] requiredFeatures, Bundle options)
+            throws NetworkErrorException {
+        Log.d(TAG, "addAccount()");
+
+        // Delegate to the activity to get the account information from the user.
+        Bundle bundle = new Bundle();
+        bundle.putParcelable(AccountManager.KEY_INTENT,
+                SpnegoAuthenticatorActivity.getAddAccountIntent(mContext, response));
+        return bundle;
     }
 
     @Override
-    public Bundle confirmCredentials(AccountAuthenticatorResponse arg0, Account arg1, Bundle arg2)
-            throws NetworkErrorException {
-        Bundle result = new Bundle();
-        result.putBoolean(AccountManager.KEY_BOOLEAN_RESULT, true);
-        return result;
+    public Bundle confirmCredentials(AccountAuthenticatorResponse response, Account account,
+            Bundle options) throws NetworkErrorException {
+        Log.d(TAG, "confirmCredentials(%s)", account.name);
+        return unsupportedOperationBundle("confirmCredentials");
     }
 
     @Override
-    public Bundle editProperties(AccountAuthenticatorResponse arg0, String arg1) {
-        return new Bundle();
+    public Bundle editProperties(AccountAuthenticatorResponse response, String accountType) {
+        Log.d(TAG, "editProperties(%s)", accountType);
+        return unsupportedOperationBundle("editProperties");
     }
 
     @Override
     public Bundle getAuthToken(AccountAuthenticatorResponse response, Account account,
             String authTokenType, Bundle options) throws NetworkErrorException {
+        Log.d(TAG, "getAuthToken(%s)", account.name);
+
         Bundle result = new Bundle();
-        result.putString(AccountManager.KEY_ACCOUNT_NAME, account.name);
-        result.putString(AccountManager.KEY_ACCOUNT_TYPE, account.type);
-        result.putString(AccountManager.KEY_AUTHTOKEN, "DummyAuthToken");
-        result.putInt(HttpNegotiateConstants.KEY_SPNEGO_RESULT, 0);
-        Log.d(TAG, "getAuthToken(): Returning dummy SPNEGO auth token");
+        if (AccountData.get(account.name, mContext).isAuthenticated()) {
+            Log.d(TAG, "getAuthToken(): Returning dummy SPNEGO auth token");
+            result.putString(AccountManager.KEY_ACCOUNT_NAME, account.name);
+            result.putString(AccountManager.KEY_ACCOUNT_TYPE, account.type);
+            result.putString(AccountManager.KEY_AUTHTOKEN, Constants.AUTH_TOKEN);
+            result.putInt(HttpNegotiateConstants.KEY_SPNEGO_RESULT, 0);
+        } else {
+            Log.d(TAG, "getAuthToken(): Asking for credentials confirmation");
+            Intent intent = SpnegoAuthenticatorActivity.getConfirmCredentialsIntent(
+                    mContext, account.name, response);
+            result.putParcelable(AccountManager.KEY_INTENT, intent);
+
+            // We need to show a notification in case the caller can't use the intent directly.
+            showConfirmCredentialsNotification(mContext, intent);
+        }
+
         return result;
     }
 
     @Override
     public String getAuthTokenLabel(String authTokenType) {
+        Log.d(TAG, "getAuthTokenLabel(%s)", authTokenType);
         return "Spnego " + authTokenType;
     }
 
     @Override
-    public Bundle hasFeatures(AccountAuthenticatorResponse arg0, Account arg1, String[] features)
-            throws NetworkErrorException {
+    public Bundle hasFeatures(AccountAuthenticatorResponse response, Account account,
+            String[] features) throws NetworkErrorException {
         Log.d(TAG, "hasFeatures(%s)", Arrays.asList(features));
         Bundle result = new Bundle();
+
+        // All our accounts only have the SPNEGO feature, other features are not supported.
         for (String feature : features) {
             if (!feature.equals(HttpNegotiateConstants.SPNEGO_FEATURE)) {
                 result.putBoolean(AccountManager.KEY_BOOLEAN_RESULT, false);
@@ -89,18 +112,34 @@ public class SpnegoAuthenticator extends AbstractAccountAuthenticator {
     }
 
     @Override
-    public Bundle updateCredentials(AccountAuthenticatorResponse arg0, Account arg1, String arg2,
-            Bundle arg3) throws NetworkErrorException {
-        Log.w(TAG, "updateCredentials(): Not supported.");
-        Bundle result = new Bundle();
-        result.putInt(AccountManager.KEY_ERROR_CODE, AccountManager.ERROR_CODE_BAD_REQUEST);
-        result.putString(AccountManager.KEY_ERROR_MESSAGE, "Can't update credentials.");
-        return result;
+    public Bundle updateCredentials(AccountAuthenticatorResponse response, Account account,
+            String authTokenType, Bundle options) throws NetworkErrorException {
+        Log.d(TAG, "updateCredentials(%s)", account.name);
+        return unsupportedOperationBundle("updateCredentials");
     }
 
-    private void ensureTestAccountExists(Context context) {
-        AccountManager am = AccountManager.get(context);
-        Account account = new Account(ACCOUNT_NAME, context.getString(R.string.account_type));
-        am.addAccountExplicitly(account, null, null);
+    private void showConfirmCredentialsNotification(Context context, Intent intent) {
+        PendingIntent notificationAction =
+                PendingIntent.getActivity(context, 0, intent, PendingIntent.FLAG_UPDATE_CURRENT);
+        Notification notification = new Notification.Builder(context)
+                .setContentTitle("Authentication required")
+                .setContentText("Credential confirmation required for the Spnego account")
+                .setSmallIcon(android.R.drawable.stat_sys_warning)
+                .setContentIntent(notificationAction)
+                .setAutoCancel(true).build();
+
+        NotificationManager notificationManager =
+                (NotificationManager) context.getSystemService(Context.NOTIFICATION_SERVICE);
+
+        notificationManager.notify(Constants.CONFIRM_CREDENTIAL_NOTIFICATION_ID, notification);
+    }
+
+    /** Returns a bundle containing a standard error response. */
+    private Bundle unsupportedOperationBundle(String operationName) {
+        Bundle result = new Bundle();
+        result.putInt(
+                AccountManager.KEY_ERROR_CODE, AccountManager.ERROR_CODE_UNSUPPORTED_OPERATION);
+        result.putString(AccountManager.KEY_ERROR_MESSAGE, "Unsupported method: " + operationName);
+        return result;
     }
 }
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java
new file mode 100644
index 0000000..1172a1e
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java
@@ -0,0 +1,131 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.spnegoauthenticator;
+
+import android.accounts.AccountAuthenticatorActivity;
+import android.accounts.AccountAuthenticatorResponse;
+import android.accounts.AccountManager;
+import android.app.NotificationManager;
+import android.content.Context;
+import android.content.Intent;
+import android.os.Bundle;
+import android.view.View;
+import android.view.View.OnClickListener;
+import android.widget.Button;
+
+import org.chromium.base.Log;
+
+/** Provides a UI to administrate the Spnego accounts. */
+public class SpnegoAuthenticatorActivity extends AccountAuthenticatorActivity {
+    private static final String TAG = Constants.TAG;
+
+    // Constants for passing information via intents.
+    private static final String KEY_MODE = "mode";
+    private static final String KEY_ACCOUNT = "account";
+    private static final int MODE_INVALID = 0;
+    private static final int MODE_ADD_ACCOUNT = 1;
+    private static final int MODE_CONFIRM_CREDENTIALS = 2;
+
+    @Override
+    protected void onCreate(Bundle savedInstanceState) {
+        super.onCreate(savedInstanceState);
+        setContentView(R.layout.activity_account_authenticator);
+
+        Intent intent = getIntent();
+        initUi(intent.getIntExtra(KEY_MODE, MODE_INVALID), intent.getStringExtra(KEY_ACCOUNT));
+    }
+
+    /** Returns an intent that can be used to start the activity in AddAcount mode */
+    public static Intent getAddAccountIntent(
+            Context context, AccountAuthenticatorResponse response) {
+        Intent intent = new Intent(context, SpnegoAuthenticatorActivity.class);
+        intent.putExtra(KEY_MODE, MODE_ADD_ACCOUNT);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_AUTHENTICATOR_RESPONSE, response);
+        return intent;
+    }
+
+    /** Returns an intent that can be used to start the activity in ConfirmCredentials mode */
+    public static Intent getConfirmCredentialsIntent(
+            Context context, String accountName, AccountAuthenticatorResponse response) {
+        Intent intent = new Intent(context, SpnegoAuthenticatorActivity.class);
+        intent.putExtra(KEY_MODE, MODE_CONFIRM_CREDENTIALS);
+        intent.putExtra(KEY_ACCOUNT, accountName);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_AUTHENTICATOR_RESPONSE, response);
+        intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK | Intent.FLAG_ACTIVITY_EXCLUDE_FROM_RECENTS
+                | Intent.FLAG_ACTIVITY_NO_HISTORY);
+        return intent;
+    }
+
+    private void addAccount(String accountName) {
+        Log.d(TAG, "Adding account '%s'", accountName);
+
+        AccountData accountData = AccountData.create(accountName, this);
+        accountData.save(this);
+        Intent intent = accountData.getAccountAddedIntent();
+        intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK | Intent.FLAG_ACTIVITY_EXCLUDE_FROM_RECENTS
+                | Intent.FLAG_ACTIVITY_NO_HISTORY);
+        setAccountAuthenticatorResult(intent.getExtras());
+        setResult(RESULT_OK, intent);
+        finish();
+    }
+
+    private void confirmCredentials(String accountName) {
+        Log.d(TAG, "Confirming credentials for account '%s'", accountName);
+
+        NotificationManager nm = (NotificationManager) getSystemService(NOTIFICATION_SERVICE);
+        nm.cancel(Constants.CONFIRM_CREDENTIAL_NOTIFICATION_ID);
+
+        AccountData accountData = AccountData.get(accountName, this);
+        accountData.setIsAuthenticated(true);
+        accountData.save(this);
+
+        Intent intent = accountData.getCredentialsConfirmedIntent();
+        setAccountAuthenticatorResult(intent.getExtras());
+        setResult(RESULT_OK, intent);
+        finish();
+    }
+
+    private void initUi(final int mode, final String account) {
+        Button signInButton1 = (Button) findViewById(R.id.sign_in_button_1);
+        Button signInButton2 = (Button) findViewById(R.id.sign_in_button_2);
+        Button confirmCredentialsButton = (Button) findViewById(R.id.confirm_credentials_button);
+
+        switch (mode) {
+            case MODE_ADD_ACCOUNT:
+                signInButton1.setOnClickListener(new OnClickListener() {
+                    @Override
+                    public void onClick(View view) {
+                        addAccount(Constants.ACCOUNT_1_NAME);
+                    }
+                });
+                signInButton2.setOnClickListener(new OnClickListener() {
+                    @Override
+                    public void onClick(View view) {
+                        addAccount(Constants.ACCOUNT_2_NAME);
+                    }
+                });
+                confirmCredentialsButton.setEnabled(false);
+                break;
+
+            case MODE_CONFIRM_CREDENTIALS:
+                signInButton1.setEnabled(false);
+                signInButton2.setEnabled(false);
+                confirmCredentialsButton.setOnClickListener(new OnClickListener() {
+                    @Override
+                    public void onClick(View view) {
+                        confirmCredentials(account);
+                    }
+                });
+                break;
+
+            default:
+                Log.w(TAG, "Opened the activity in an invalid mode: %d", mode);
+                signInButton1.setEnabled(false);
+                signInButton2.setEnabled(false);
+                confirmCredentialsButton.setEnabled(false);
+                break;
+        }
+    }
+}

commit 3fecfb79db47fb118c534beadfcf8ade8f48a11a
Author: droger <droger@chromium.org>
Date:   Tue Apr 5 04:43:36 2016 -0700

    tools/android/loading Add a /status URL
    
    This URL can be used to track the progress of a job.
    
    Review URL: https://codereview.chromium.org/1857653003
    
    Cr-Original-Commit-Position: refs/heads/master@{#385152}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cf2eac75c6c4241c1cab6bb2423ab1c941f783b4

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 0399de0..2bbd06b 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -77,6 +77,8 @@ where `urls.json` is a JSON dictionary with the keys:
 *   `emulate_device`: Name of the device to emulate. Optional.
 *   `emulate_network`: Type of network emulation. Optional.
 
+You can follow the progress at `http://<instance-ip>:8080/status`.
+
 ## Stop the app in the cloud
 
 ```shell
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 95c18c5..66bd7dd 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -6,6 +6,7 @@ import json
 import os
 import re
 import threading
+import time
 import subprocess
 
 from gcloud import storage
@@ -21,8 +22,12 @@ class ServerApp(object):
     """|configuration_file| is a path to a file containing JSON as described in
     README.md.
     """
-    self._tasks = []
+    self._tasks = []  # List of remaining tasks, only modified by _thread.
+    self._failed_tasks = []  # Failed tasks, only modified by _thread.
     self._thread = None
+    self._tasks_lock = threading.Lock()  # Protects _tasks and _failed_tasks.
+    self._initial_task_count = -1
+    self._start_time = None
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
     print 'Reading configuration'
@@ -87,6 +92,7 @@ class ServerApp(object):
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
     """ Generates a trace using analyze.py
+    Runs on _thread.
 
     Args:
       url: URL as a string.
@@ -115,24 +121,37 @@ class ServerApp(object):
                             stdout = log_file)
     return ret == 0
 
-  def _ProcessTasks(self, repeat_count, emulate_device, emulate_network):
+  def _GetCurrentTaskCount(self):
+    """Returns the number of remaining tasks. Thread safe."""
+    self._tasks_lock.acquire()
+    task_count = len(self._tasks)
+    self._tasks_lock.release()
+    return task_count
+
+  def _ProcessTasks(self, tasks, repeat_count, emulate_device, emulate_network):
     """Iterates over _tasks and runs analyze.py on each of them. Uploads the
     resulting traces to Google Cloud Storage.
+    Runs on _thread.
 
     Args:
+      tasks: The list of URLs to process.
       repeat_count: The number of traces generated for each URL.
       emulate_device: Name of the device to emulate. Empty for no emulation.
       emulate_network: Type of network emulation. Empty for no emulation.
     """
+    # The main thread might be reading the task lists, take the lock to modify.
+    self._tasks_lock.acquire()
+    self._tasks = tasks
+    self._failed_tasks = []
+    self._tasks_lock.release()
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
     logs_dir = self._base_path_in_bucket + 'analyze_logs/'
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
-    failed_tasks = []
     while len(self._tasks) > 0:
-      url = self._tasks.pop()
+      url = self._tasks[0]
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
@@ -143,15 +162,21 @@ class ServerApp(object):
           self._UploadFile(local_filename, traces_dir + remote_filename)
         else:
           print 'analyze.py failed for URL: %s' % url
-          failed_tasks.append({ "url": url, "repeat": repeat})
+          self._tasks_lock.acquire()
+          self._failed_tasks.append({ "url": url, "repeat": repeat})
+          self._tasks_lock.release()
           if os.path.isfile(local_filename):
             self._UploadFile(local_filename, failures_dir + remote_filename)
         print 'Uploading analyze log'
         self._UploadFile(log_filename, logs_dir + remote_filename)
+      # Pop once task is finished, for accurate status tracking.
+      self._tasks_lock.acquire()
+      url = self._tasks.popleft()
+      self._tasks_lock.release()
 
-    if len(failed_tasks) > 0:
+    if len(self._failed_tasks) > 0:
       print 'Uploading failing URLs'
-      self._UploadString(json.dumps(failed_tasks),
+      self._UploadString(json.dumps(self._failed_tasks, indent=2),
                          failures_dir + 'failures.json')
 
   def _SetTaskList(self, http_body):
@@ -164,29 +189,32 @@ class ServerApp(object):
       A string to be sent back to the client, describing the success status of
       the request.
     """
+    if self._thread is not None and self._thread.is_alive():
+      return 'Error: Already running\n'
+
     load_parameters = json.loads(http_body)
     try:
-      self._tasks = load_parameters['urls']
+      tasks = load_parameters['urls']
     except KeyError:
-      return 'Error: invalid urls'
+      return 'Error: invalid urls\n'
     # Optional parameters.
     try:
       repeat_count = int(load_parameters.get('repeat_count', '1'))
     except ValueError:
-      return 'Error: invalid repeat_count'
+      return 'Error: invalid repeat_count\n'
     emulate_device = load_parameters.get('emulate_device', '')
     emulate_network = load_parameters.get('emulate_network', '')
 
-    if len(self._tasks) == 0:
-      return 'Error: Empty task list'
-    elif self._thread is not None and self._thread.is_alive():
-      return 'Error: Already running'
+    if len(tasks) == 0:
+      return 'Error: Empty task list\n'
     else:
+      self._initial_task_count = len(tasks)
+      self._start_time = time.time()
       self._thread = threading.Thread(
           target = self._ProcessTasks,
-          args = (repeat_count, emulate_device, emulate_network))
+          args = (tasks, repeat_count, emulate_device, emulate_network))
       self._thread.start()
-      return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
+      return 'Starting generation of %s tasks\n' % str(self._initial_task_count)
 
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
@@ -200,7 +228,21 @@ class ServerApp(object):
       body = environ['wsgi.input'].read(body_size)
       data = self._SetTaskList(body)
     elif path == '/test':
-      data = 'hello'
+      data = 'hello\n'
+    elif path == '/status':
+      task_count = self._GetCurrentTaskCount()
+      if task_count == 0:
+        data = 'Idle\n'
+      else:
+        data = 'Remaining tasks: %s / %s\n' % (
+            task_count, self._initial_task_count)
+        elapsed = time.time() - self._start_time
+        data += 'Elapsed time: %s seconds\n' % str(elapsed)
+        self._tasks_lock.acquire()
+        failed_tasks = self._failed_tasks
+        self._tasks_lock.release()
+        data += '%s failed tasks:\n' % len(failed_tasks)
+        data += json.dumps(failed_tasks, indent=2)
     else:
       start_response('404 NOT FOUND', [('Content-Length', '0')])
       return iter([''])

commit e53369781ea73d371a995c76dc4f1e5a6a53b9ac
Author: mattcary <mattcary@chromium.org>
Date:   Tue Apr 5 01:26:24 2016 -0700

    Clovis: Update core set output to use full URL instead of shortened label.
    
    This also includes a generalization for doing approximate name matching which is currently unused (that is, we still identify resources by their URLs).
    
    Review URL: https://codereview.chromium.org/1859563002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385133}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 581720eb88d9a3b0b973c5fe33e3894e2f7164a0

diff --git a/loading/core_set.py b/loading/core_set.py
index 3bbb66f..910c280 100644
--- a/loading/core_set.py
+++ b/loading/core_set.py
@@ -42,7 +42,15 @@ def _PageCore(prefix, graph_set_names, output):
       sack.ConsumeGraph(graph)
       name_graphs.append(graph)
     graph_sets.append(name_graphs)
-  json.dump({'page_core': [l for l in sack.CoreSet(*graph_sets)],
+  core = sack.CoreSet(*graph_sets)
+  json.dump({'page_core': [{'label': b.label,
+                            'name': b.name,
+                            'count': b.num_nodes}
+                           for b in core],
+             'non_core': [{'label': b.label,
+                           'name': b.name,
+                           'count': b.num_nodes}
+                          for b in sack.bags if b not in core],
              'threshold': sack.CORE_THRESHOLD},
             output, sort_keys=True, indent=2)
   output.write('\n')
@@ -80,39 +88,9 @@ def _Spawn(site_list_file, graph_sets, input_dir, output_dir, workers):
                              for s in sites])
 
 
-def _AllCores(prefix, graph_set_names, output, threshold):
-  """Compute all core sets (per-set and overall page core) for a site."""
-  core_sets = []
-  _Progress('Using threshold %s' % threshold)
-  big_sack = resource_sack.GraphSack()
-  graph_sets = []
-  for name in graph_set_names:
-    _Progress('Finding core set for %s' % name)
-    sack = resource_sack.GraphSack()
-    sack.CORE_THRESHOLD = threshold
-    this_set = []
-    for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
-      _Progress('Reading %s' % filename)
-      trace = loading_trace.LoadingTrace.FromJsonFile(filename)
-      graph = dependency_graph.RequestDependencyGraph(
-          trace.request_track.GetEvents(),
-          request_dependencies_lens.RequestDependencyLens(trace))
-      sack.ConsumeGraph(graph)
-      big_sack.ConsumeGraph(graph)
-      this_set.append(graph)
-    core_sets.append({
-        'set_name': name,
-        'core_set': [l for l in sack.CoreSet()]
-    })
-    graph_sets.append(this_set)
-  json.dump({'core_sets': core_sets,
-             'page_core': [l for l in big_sack.CoreSet(*graph_sets)]},
-            output, sort_keys=True, indent=2)
-
-
 def _ReadCoreSet(filename):
   data = json.load(open(filename))
-  return set(data['page_core'])
+  return set(page['name'] for page in data['page_core'])
 
 
 def _Compare(a_name, b_name, csv):
@@ -172,28 +150,9 @@ if __name__ == '__main__':
                            help='trace file prefix')
   page_core.add_argument('--output', required=True,
                            help='JSON output file name')
-  page_core.set_defaults(
-      executor=lambda args:
-      _PageCore(args.prefix, args.sets.split(','), file(args.output, 'w')))
-
-  all_cores = subparsers.add_parser(
-      'all_cores',
-      help=('compute core and page core sets. Computes the core for each set '
-            'in --sets and then the overall page core using trace files '
-            'of form {--prefix}{set}*.trace. Outputs all the sets as JSON'))
-  all_cores.add_argument('--sets', required=True,
-                         help='sets to combine, comma-separated')
-  all_cores.add_argument('--prefix', required=True,
-                         help='input file prefix')
-  all_cores.add_argument('--output', required=True,
-                         help='JSON output file name')
-  all_cores.add_argument('--threshold',
-                         default=resource_sack.GraphSack.CORE_THRESHOLD,
-                         type=float, help='core set threshold')
-  all_cores.set_defaults(
-      executor=lambda args:
-      _AllCores(args.prefix, args.sets.split(','), file(args.output, 'w'),
-                args.threshold))
+  page_core.set_defaults(executor=lambda args:
+                         _PageCore(args.prefix, args.sets.split(','),
+                                   file(args.output, 'w')))
 
   compare = subparsers.add_parser(
       'compare',
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 563bb77..d7fe331 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -47,9 +47,23 @@ class GraphSack(object):
     # Maps graph -> _GraphInfo structures for each graph we've consumed.
     self._graph_info = {}
 
+    # How we generate names.
+    self._name_generator = lambda n: n.request.url
+
     # Our graph, updated after each ConsumeGraph.
     self._graph = None
 
+  def SetNameGenerator(self, generator):
+    """Set the generator we use for names.
+
+    This will define the equivalence class of requests we use to define sacks.
+
+    Args:
+      generator: a function taking a RequestDependencyGraph node and returning a
+        string.
+    """
+    self._name_generator = generator
+
   def ConsumeGraph(self, request_graph):
     """Add a graph and process.
 
@@ -66,6 +80,10 @@ class GraphSack(object):
     # explicit graph creation from the client.
     self._graph = graph.DirectedGraph(self.bags, self._edges.itervalues())
 
+  def GetBag(self, node):
+    """Find the bag for a node, or None if not found."""
+    return self._name_to_bag.get(self._name_generator(node), None)
+
   def AddNode(self, request_graph, node):
     """Add a node to our collection.
 
@@ -76,7 +94,7 @@ class GraphSack(object):
     Returns:
       The Bag containing the node.
     """
-    sack_name = self._GetSackName(node)
+    sack_name = self._name_generator(node)
     if sack_name not in self._name_to_bag:
       self._name_to_bag[sack_name] = Bag(self, sack_name)
     bag = self._name_to_bag[sack_name]
@@ -107,7 +125,7 @@ class GraphSack(object):
         computed.
 
     Returns:
-      A set of bag labels (as strings) in the core set.
+      A set of bags in the core set.
     """
     if not graph_sets:
       graph_sets = [self._graph_info.keys()]
@@ -151,12 +169,9 @@ class GraphSack(object):
     for b in self.bags:
       count = sum([g in graph_set for g in b.graphs])
       if float(count) / num_graphs > self.CORE_THRESHOLD:
-        core.add(b.label)
+        core.add(b)
     return core
 
-  def _GetSackName(self, node):
-    return self._MakeShortname(node.request.url)
-
   @classmethod
   def _MakeShortname(cls, url):
     # TODO(lizeb): Move this method to a convenient common location.
@@ -173,14 +188,19 @@ class GraphSack(object):
 
 
 class Bag(graph.Node):
-  def __init__(self, sack, label):
+  def __init__(self, sack, name):
     super(Bag, self).__init__()
     self._sack = sack
-    self._label = label
+    self._name = name
+    self._label = GraphSack._MakeShortname(name)
     # Maps a ResourceGraph to its Nodes contained in this Bag.
     self._graphs = defaultdict(set)
 
   @property
+  def name(self):
+    return self._name
+
+  @property
   def label(self):
     return self._label
 
@@ -192,6 +212,9 @@ class Bag(graph.Node):
   def num_nodes(self):
     return sum(len(g) for g in self._graphs.itervalues())
 
+  def GraphNodes(self, g):
+    return self._graphs.get(g, set())
+
   def AddNode(self, request_graph, node):
     if node in self._graphs[request_graph]:
       return  # Already added.
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index 5c3e54d..30c1d0e 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -72,7 +72,9 @@ class ResourceSackTestCase(unittest.TestCase):
 
   def test_Core(self):
     # We will use a core threshold of 0.5 to make it easier to define
-    # graphs. Resources 0 and 1 are core and others are not.
+    # graphs. Resources 0 and 1 are core and others are not. We check full names
+    # and node counts as we output that for core set analysis. In subsequent
+    # tests we just check labels to make the tests easier to read.
     graphs = [self.SimpleGraph([0, 1, 2]),
               self.SimpleGraph([0, 1, 3]),
               self.SimpleGraph([0, 1, 4]),
@@ -81,7 +83,8 @@ class ResourceSackTestCase(unittest.TestCase):
     sack.CORE_THRESHOLD = 0.5
     for g in graphs:
       sack.ConsumeGraph(g)
-    self.assertEqual(set(['0/', '1/']), sack.CoreSet())
+    self.assertEqual(set([('http://0', 4), ('http://1', 3)]),
+                     set((b.name, b.num_nodes) for b in sack.CoreSet()))
 
   def test_IntersectingCore(self):
     # Graph set A has core set {0, 1} and B {0, 2} so the final core set should
@@ -96,10 +99,13 @@ class ResourceSackTestCase(unittest.TestCase):
     for g in set_A + set_B + set_C:
       sack.ConsumeGraph(g)
     self.assertEqual(set(), sack.CoreSet())
-    self.assertEqual(set(['0/', '1/']), sack.CoreSet(set_A))
-    self.assertEqual(set(['0/', '2/']), sack.CoreSet(set_B))
+    self.assertEqual(set(['0/', '1/']),
+                     set(b.label for b in sack.CoreSet(set_A)))
+    self.assertEqual(set(['0/', '2/']),
+                     set(b.label for b in sack.CoreSet(set_B)))
     self.assertEqual(set(), sack.CoreSet(set_C))
-    self.assertEqual(set(['0/']), sack.CoreSet(set_A, set_B))
+    self.assertEqual(set(['0/']),
+                     set(b.label for b in sack.CoreSet(set_A, set_B)))
     self.assertEqual(set(), sack.CoreSet(set_A, set_B, set_C))
 
   def test_Simililarity(self):

commit 0d9e17938a76dcd6742051b2b80f799f539656fe
Author: agrieve <agrieve@chromium.org>
Date:   Mon Apr 4 19:03:45 2016 -0700

    Reland 2 of GN: Make breakpad_unittests & sandbox_linux_unittests use test()
    
    This simplifies build rules for native tests, and allows us to get rid
    of ${target}_deps targets (once recipes are updated).
    
    This change fixes the generated wrapper scripts, which didn't work.
    
    TBR=jbudorick
    BUG=589318
    
    Review URL: https://codereview.chromium.org/1854233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385084}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 62ab0028a0165eecd60568783531d1ad3d031a26

diff --git a/forwarder2/BUILD.gn b/forwarder2/BUILD.gn
index a118d65..6899a7e 100644
--- a/forwarder2/BUILD.gn
+++ b/forwarder2/BUILD.gn
@@ -54,7 +54,6 @@ if (current_toolchain == default_toolchain) {
   create_native_executable_dist("device_forwarder_prepare_dist") {
     dist_dir = "$root_build_dir/forwarder_dist"
     binary = "$root_build_dir/device_forwarder"
-    include_main_binary = true
     deps = [
       ":device_forwarder",
     ]
diff --git a/md5sum/BUILD.gn b/md5sum/BUILD.gn
index 391ffb8..7cce4c9 100644
--- a/md5sum/BUILD.gn
+++ b/md5sum/BUILD.gn
@@ -38,7 +38,6 @@ if (current_toolchain == default_toolchain) {
   create_native_executable_dist("md5sum_prepare_dist") {
     dist_dir = "$root_build_dir/md5sum_dist"
     binary = "$root_build_dir/md5sum_bin"
-    include_main_binary = true
     deps = [
       ":md5sum_bin",
     ]

commit 531b53710ce2d434a738fca06565807e7147fbd9
Author: blundell <blundell@chromium.org>
Date:   Mon Apr 4 08:09:59 2016 -0700

    tools/android/loading: Initialize loading trace DB from Google Storage path
    
    This CL adds the ability to initialize a LoadingTraceDatabase instance from
    a JSON file stored in Google Storage (as opposed to locally). This will be
    needed in order to access the serialized database files that will be generated
    at the time of generating traces.
    
    Review URL: https://codereview.chromium.org/1856743002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384910}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 352303571a8842aa7a25b072d68be5535e7c316d

diff --git a/loading/google_storage_util.py b/loading/google_storage_util.py
new file mode 100644
index 0000000..82a59a5
--- /dev/null
+++ b/loading/google_storage_util.py
@@ -0,0 +1,19 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Contains utility functions for interacting with Google Storage."""
+
+import subprocess
+
+def ReadFromGoogleStorage(path):
+  """Given a Google Storage path, returns the contents of the file at that path
+     as a string. Will fail if the user does not have authorization to access
+     the path or if the path does not exist. To gain authorization, follow the
+     instructions for installing gsutil and setting up credentials to access
+     protected data that are on this page:
+     https://cloud.google.com/storage/docs/gsutil_install"""
+  # TODO(blundell): Change this to use the gcloud Python module once
+  # https://github.com/GoogleCloudPlatform/gcloud-python/issues/14360 is fixed.
+  contents = subprocess.check_output(["gsutil", "cat", path])
+  return contents
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index 89d608a..d2e4910 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -5,7 +5,7 @@
 """Represents a database of on-disk traces."""
 
 import json
-
+from google_storage_util import ReadFromGoogleStorage
 
 class LoadingTraceDatabase:
 
@@ -26,7 +26,7 @@ class LoadingTraceDatabase:
     return self._traces_dict
 
   def ToJsonFile(self, json_path):
-    """Save a json file representing this instance."""
+    """Saves a json file representing this instance."""
     json_dict = self.ToJsonDict()
     with open(json_path, 'w') as output_file:
        json.dump(json_dict, output_file, indent=2)
@@ -41,3 +41,10 @@ class LoadingTraceDatabase:
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:
       return cls.FromJsonDict(json.load(input_file))
+
+  @classmethod
+  def FromJsonFileInGoogleStorage(cls, json_google_storage_path):
+    """Returns an instance from a json file in Google Storage whose contents
+       were generated by ToJsonFile()."""
+    json_string = ReadFromGoogleStorage(json_google_storage_path)
+    return cls.FromJsonDict(json.loads(json_string))

commit 8e655f52e41b128165b2c9ca48cfaaf65834d3db
Author: dcheng <dcheng@chromium.org>
Date:   Sat Apr 2 11:35:46 2016 -0700

    Fix IWYU violators that don't include scoped_ptr.h in Android build.
    
    This blocks the conversion of //base from scoped_ptr to std::unique_ptr.
    
    BUG=554298
    R=avi@chromium.org
    TBR=brettw@chromium.org
    
    Review URL: https://codereview.chromium.org/1847373005
    
    Cr-Original-Commit-Position: refs/heads/master@{#384813}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: af7ea1d22dca21a645f58d3c649aa9fd2b2d4e04

diff --git a/md5sum/md5sum.cc b/md5sum/md5sum.cc
index dacf8f4..94efa10 100644
--- a/md5sum/md5sum.cc
+++ b/md5sum/md5sum.cc
@@ -17,6 +17,7 @@
 #include "base/files/file_util.h"
 #include "base/logging.h"
 #include "base/md5.h"
+#include "base/memory/scoped_ptr.h"
 
 namespace {
 

commit 8ab51a0c0ea715e0d817a1b5056fc5e39f7b015b
Author: blundell <blundell@chromium.org>
Date:   Fri Apr 1 08:29:51 2016 -0700

    tools/android/loading: Add simple database for trace files
    
    This CL adds a class whose purpose will be to allow for search/indexing
    over a set of tracefiles along various dimensions of interest (e.g.,
    "give me all the tracefiles for a given domain"). This class can be
    initialized from and serialized to an on-disk representation of the
    database.
    
    Followup CLs will generate this on-disk representation as part of
    generating traces in the cloud and add support for initializing the
    database from a serialized file stored in the cloud.
    
    Review URL: https://codereview.chromium.org/1850203002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384592}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6a6bec0e71f127adafe56bf659769fb9aa97edf5

diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
new file mode 100644
index 0000000..89d608a
--- /dev/null
+++ b/loading/loading_trace_database.py
@@ -0,0 +1,43 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Represents a database of on-disk traces."""
+
+import json
+
+
+class LoadingTraceDatabase:
+
+  def __init__(self, traces_dict):
+    """traces_dict is a dictionary mapping filenames of traces to metadata
+       about those traces."""
+    self._traces_dict = traces_dict
+
+  def GetTraceFilesForURL(self, url):
+    """Given a URL, returns the set of filenames of traces that were generated
+       for this URL."""
+    trace_files = [f for f in self._traces_dict.keys()
+        if self._traces_dict[f]["url"] == url]
+    return trace_files
+
+  def ToJsonDict(self):
+    """Returns a dict representing this instance."""
+    return self._traces_dict
+
+  def ToJsonFile(self, json_path):
+    """Save a json file representing this instance."""
+    json_dict = self.ToJsonDict()
+    with open(json_path, 'w') as output_file:
+       json.dump(json_dict, output_file, indent=2)
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns an instance from a dict returned by ToJsonDict()."""
+    return LoadingTraceDatabase(json_dict)
+
+  @classmethod
+  def FromJsonFile(cls, json_path):
+    """Returns an instance from a json file saved by ToJsonFile()."""
+    with open(json_path) as input_file:
+      return cls.FromJsonDict(json.load(input_file))
diff --git a/loading/loading_trace_database_unittest.py b/loading/loading_trace_database_unittest.py
new file mode 100644
index 0000000..775638c
--- /dev/null
+++ b/loading/loading_trace_database_unittest.py
@@ -0,0 +1,37 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from loading_trace_database import LoadingTraceDatabase
+
+
+class LoadingTraceDatabaseUnittest(unittest.TestCase):
+  _JSON_DATABASE = {
+    "traces/trace1.json" : { "url" : "http://bar.html", },
+    "traces/trace2.json" : { "url" : "http://bar.html", },
+    "traces/trace3.json" : { "url" : "http://qux.html", },
+  }
+
+  def setUp(self):
+    self.database = LoadingTraceDatabase.FromJsonDict(self._JSON_DATABASE)
+
+  def testGetTraceFilesForURL(self):
+    # Test a URL with no matching traces.
+    self.assertEqual(
+        self.database.GetTraceFilesForURL("http://foo.html"),
+        [])
+
+    # Test a URL with matching traces.
+    self.assertEqual(
+        set(self.database.GetTraceFilesForURL("http://bar.html")),
+        set(["traces/trace1.json", "traces/trace2.json"]))
+
+  def testSerialization(self):
+    self.assertEqual(
+        self._JSON_DATABASE, self.database.ToJsonDict())
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 19ca7df295331bf924064a50b973936faf93ba6d
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 08:11:08 2016 -0700

    tools/android/loading Deploy module.json for device emulation on GCE
    
    Review URL: https://codereview.chromium.org/1847323003
    
    Cr-Original-Commit-Position: refs/heads/master@{#384586}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 076f05ee140ffeb19aec751590c4a434bb12fb94

diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 0a3a531..2362414 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -28,7 +28,7 @@ cp -r tools/android/loading/gce $tmp_src_dir/tools/android/loading
 # Copy other dependencies.
 mkdir $tmp_src_dir/third_party
 rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
-  --delete third_party/catapult $tmp_src_dir/third_party
+  third_party/catapult $tmp_src_dir/third_party
 mkdir $tmp_src_dir/tools/perf
 cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
 mkdir -p $tmp_src_dir/build/android
@@ -36,6 +36,10 @@ cp build/android/devil_chromium.py $tmp_src_dir/build/android/
 cp build/android/video_recorder.py $tmp_src_dir/build/android/
 cp build/android/devil_chromium.json $tmp_src_dir/build/android/
 cp -r build/android/pylib $tmp_src_dir/build/android/
+mkdir -p \
+  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
+cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \
+  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices/
 
 # Tar up the source and copy it to Google Cloud Storage.
 source_tarball=$tmpdir/source.tgz

commit 92d05b8d0d1aed43a1ce5dd6e84d23f89733a24c
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 05:40:05 2016 -0700

    tools/android/loading Add GCE support for device and network emulation
    
    Review URL: https://codereview.chromium.org/1853653003
    
    Cr-Original-Commit-Position: refs/heads/master@{#384564}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cc06b8b66a7be3dfdb0421a4de864f4b55091d3b

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 52e0c82..0399de0 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -73,7 +73,9 @@ where `urls.json` is a JSON dictionary with the keys:
 
 *   `urls`: array of URLs
 *   `repeat_count`: Number of times each URL will be loaded. Each load of a URL
-    generates a separate trace file.
+    generates a separate trace file. Optional.
+*   `emulate_device`: Name of the device to emulate. Optional.
+*   `emulate_network`: Type of network emulation. Optional.
 
 ## Stop the app in the cloud
 
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 9b74ec6..95c18c5 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -84,13 +84,16 @@ class ServerApp(object):
     blob.upload_from_string(data_string)
     return blob.public_url
 
-  def _GenerateTrace(self, url, filename, log_filename):
+  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
+                     log_filename):
     """ Generates a trace using analyze.py
 
     Args:
-      url: url as a string.
-      filename: name of the file where the trace is saved.
-      log_filename: name of the file where standard output and errors are logged
+      url: URL as a string.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
+      filename: Name of the file where the trace is saved.
+      log_filename: Name of the file where standard output and errors are logged
 
     Returns:
       True if the trace was generated successfully.
@@ -103,17 +106,23 @@ class ServerApp(object):
     command_line = ['python', analyze_path, 'log_requests', '--local_noisy',
         '--clear_cache', '--local', '--headless', '--local_binary',
         self._chrome_path, '--url', url, '--output', filename]
+    if len(emulate_device):
+      command_line += ['--emulate_device', emulate_device]
+    if len(emulate_network):
+      command_line += ['--emulate_network', emulate_network]
     with open(log_filename, 'w') as log_file:
       ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
                             stdout = log_file)
     return ret == 0
 
-  def _ProcessTasks(self, repeat_count):
+  def _ProcessTasks(self, repeat_count, emulate_device, emulate_network):
     """Iterates over _tasks and runs analyze.py on each of them. Uploads the
     resulting traces to Google Cloud Storage.
 
     Args:
       repeat_count: The number of traces generated for each URL.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
     """
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
@@ -128,7 +137,8 @@ class ServerApp(object):
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
         remote_filename = local_filename + '/' + str(repeat)
-        if self._GenerateTrace(url, local_filename, log_filename):
+        if self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename):
           print 'Uploading: %s' % remote_filename
           self._UploadFile(local_filename, traces_dir + remote_filename)
         else:
@@ -159,18 +169,22 @@ class ServerApp(object):
       self._tasks = load_parameters['urls']
     except KeyError:
       return 'Error: invalid urls'
+    # Optional parameters.
     try:
-      repeat_count = int(load_parameters['repeat_count'])
-    except (KeyError, ValueError):
+      repeat_count = int(load_parameters.get('repeat_count', '1'))
+    except ValueError:
       return 'Error: invalid repeat_count'
+    emulate_device = load_parameters.get('emulate_device', '')
+    emulate_network = load_parameters.get('emulate_network', '')
 
     if len(self._tasks) == 0:
       return 'Error: Empty task list'
     elif self._thread is not None and self._thread.is_alive():
       return 'Error: Already running'
     else:
-      self._thread = threading.Thread(target = self._ProcessTasks,
-                                      args = (repeat_count,))
+      self._thread = threading.Thread(
+          target = self._ProcessTasks,
+          args = (repeat_count, emulate_device, emulate_network))
       self._thread.start()
       return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
 

commit 9a7fce0251cf143569be8cbb5ea4138b66b9357c
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 1 04:58:00 2016 -0700

    clovis: Replace loading_model with loading_graph_view.
    
    This completes the refactoring of loading_model.py.
    
    Review URL: https://codereview.chromium.org/1850683002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384556}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 09d7bc4db813cc546a5a890fb72a1f0c106f1568

diff --git a/loading/analyze.py b/loading/analyze.py
index fd1ba4e..db0106f 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -29,11 +29,12 @@ import content_classification_lens
 import controller
 import device_setup
 import frame_load_lens
-import loading_model
+import loading_graph_view
+import loading_graph_view_visualization
 import loading_trace
-import model_graph
 import options
-
+import request_dependencies_lens
+import request_track
 
 # TODO(mattcary): logging.info isn't that useful, as the whole (tools) world
 # uses logging info; we need to introduce logging modules to get finer-grained
@@ -68,11 +69,11 @@ def _WriteJson(output, json_data):
   json.dump(json_data, output, sort_keys=True, indent=2)
 
 
-def _GetPrefetchHtml(graph, name=None):
+def _GetPrefetchHtml(graph_view, name=None):
   """Generate prefetch page for the resources in resource graph.
 
   Args:
-    graph: a ResourceGraph.
+    graph_view: (LoadingGraphView)
     name: optional string used in the generated page.
 
   Returns:
@@ -89,8 +90,8 @@ def _GetPrefetchHtml(graph, name=None):
 <head>
 <title>%s</title>
 """ % title)
-  for info in graph.ResourceInfo():
-    output.append('<link rel="prefetch" href="%s">\n' % info.Url())
+  for node in graph_view.deps_graph.graph.Nodes():
+    output.append('<link rel="prefetch" href="%s">\n' % node.request.url)
   output.append("""</head>
 <body>%s</body>
 </html>
@@ -139,8 +140,7 @@ def _FullFetch(url, json_output, prefetch):
   if prefetch:
     assert not OPTIONS.local
     logging.warning('Generating prefetch')
-    prefetch_html = _GetPrefetchHtml(
-        loading_model.ResourceGraph(cold_data), name=url)
+    prefetch_html = _GetPrefetchHtml(_ProcessJsonTrace(cold_data), name=url)
     tmp = tempfile.NamedTemporaryFile()
     tmp.write(prefetch_html)
     tmp.flush()
@@ -165,19 +165,24 @@ def _FullFetch(url, json_output, prefetch):
     logging.warning('Wrote ' + json_output)
 
 
-def _ProcessRequests(filename):
+def _ProcessTraceFile(filename):
   with open(filename) as f:
-    trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
+    return _ProcessJsonTrace(json.load(f))
+
+
+def _ProcessJsonTrace(json_dict):
+    trace = loading_trace.LoadingTrace.FromJsonDict(json_dict)
     content_lens = (
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
             trace, OPTIONS.ad_rules, OPTIONS.tracking_rules))
     frame_lens = frame_load_lens.FrameLoadLens(trace)
     activity = activity_lens.ActivityLens(trace)
-    graph = loading_model.ResourceGraph(
-        trace, content_lens, frame_lens, activity)
+    deps_lens = request_dependencies_lens.RequestDependencyLens(trace)
+    graph_view = loading_graph_view.LoadingGraphView(
+        trace, deps_lens, content_lens, frame_lens, activity)
     if OPTIONS.noads:
-      graph.Set(node_filter=graph.FilterAds)
-    return graph
+      graph_view.RemoveAds()
+    return graph_view
 
 
 def InvalidCommand(cmd):
@@ -189,8 +194,10 @@ def DoPng(arg_str):
   OPTIONS.ParseArgs(arg_str, description='Generates a PNG from a trace',
                     extra=['request_json', ('--png_output', ''),
                            ('--eog', False)])
-  graph = _ProcessRequests(OPTIONS.request_json)
-  visualization = model_graph.GraphVisualization(graph)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
+  visualization = (
+      loading_graph_view_visualization.LoadingGraphViewVisualization(
+          graph_view))
   tmp = tempfile.NamedTemporaryFile()
   visualization.OutputDot(tmp)
   tmp.flush()
@@ -208,26 +215,13 @@ def DoPng(arg_str):
   tmp.close()
 
 
-def DoCompare(arg_str):
-  OPTIONS.ParseArgs(arg_str, description='Compares two traces',
-                    extra=['g1_json', 'g2_json'])
-  g1 = _ProcessRequests(OPTIONS.g1_json)
-  g2 = _ProcessRequests(OPTIONS.g2_json)
-  discrepancies = loading_model.ResourceGraph.CheckImageLoadConsistency(g1, g2)
-  if discrepancies:
-    print '%d discrepancies' % len(discrepancies)
-    print '\n'.join([str(r) for r in discrepancies])
-  else:
-    print 'Consistent!'
-
-
 def DoPrefetchSetup(arg_str):
   OPTIONS.ParseArgs(arg_str, description='Sets up prefetch',
                     extra=['request_json', 'target_html', ('--upload', False)])
-  graph = _ProcessRequests(OPTIONS.request_json)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
   with open(OPTIONS.target_html, 'w') as html:
     html.write(_GetPrefetchHtml(
-        graph, name=os.path.basename(OPTIONS.request_json)))
+        graph_view, name=os.path.basename(OPTIONS.request_json)))
   if OPTIONS.upload:
     device = device_setup.GetFirstDevice()
     destination = os.path.join('/sdcard/Download',
@@ -265,35 +259,34 @@ def DoFetch(arg_str):
 def DoLongPole(arg_str):
   OPTIONS.ParseArgs(arg_str, description='Calculates long pole',
                     extra='request_json')
-  graph = _ProcessRequests(OPTIONS.request_json)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
   path_list = []
-  cost = graph.Cost(path_list=path_list)
-  print '%s (%s)' % (path_list[-1], cost)
+  cost = graph_view.deps_graph.Cost(path_list=path_list)
+  print '%s (%s)' % (path_list[-1].request.url, cost)
 
 
 def DoNodeCost(arg_str):
   OPTIONS.ParseArgs(arg_str,
                     description='Calculates node cost',
                     extra='request_json')
-  graph = _ProcessRequests(OPTIONS.request_json)
-  print sum((n.NodeCost() for n in graph.Nodes()))
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
+  print sum((n.cost for n in graph_view.deps_graph.graph.Nodes()))
 
 
 def DoCost(arg_str):
   OPTIONS.ParseArgs(arg_str,
                     description='Calculates total cost',
                     extra=['request_json', ('--path', False)])
-  graph = _ProcessRequests(OPTIONS.request_json)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
   path_list = []
-  print 'Graph cost: %s' % graph.Cost(path_list)
+  print 'Graph cost: %s' % graph_view.deps_graph.Cost(path_list=path_list)
   if OPTIONS.path:
-    for p in path_list:
-      print '  ' + p.ShortName()
+    for n in path_list:
+      print '  ' + request_track.ShortName(n.request.url)
 
 
 COMMAND_MAP = {
     'png': DoPng,
-    'compare': DoCompare,
     'prefetch_setup': DoPrefetchSetup,
     'log_requests': DoLogRequests,
     'longpole': DoLongPole,
diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index 13f5485..955177c 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -11,19 +11,20 @@ import graph
 import request_track
 
 
-class _RequestNode(graph.Node):
+class RequestNode(graph.Node):
   def __init__(self, request):
-    super(_RequestNode, self).__init__()
+    super(RequestNode, self).__init__()
     self.request = request
     self.cost = request.Cost()
 
 
-class _Edge(graph.Edge):
+class Edge(graph.Edge):
   def __init__(self, from_node, to_node, reason):
-    super(_Edge, self).__init__(from_node, to_node)
+    super(Edge, self).__init__(from_node, to_node)
     self.reason = reason
     self.cost = request_track.TimeBetween(
         self.from_node.request, self.to_node.request, self.reason)
+    self.is_timing = False
 
 
 class RequestDependencyGraph(object):
@@ -34,16 +35,21 @@ class RequestDependencyGraph(object):
   _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
   _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
 
-  def __init__(self, requests, dependencies_lens):
+  def __init__(self, requests, dependencies_lens,
+               node_class=RequestNode, edge_class=Edge):
     """Creates a request dependency graph.
 
     Args:
       requests: ([Request]) a list of requests.
       dependencies_lens: (RequestDependencyLens)
+      node_class: (subclass of RequestNode)
+      edge_class: (subclass of Edge)
     """
+    assert issubclass(node_class, RequestNode)
+    assert issubclass(edge_class, Edge)
     self._requests = requests
     deps = dependencies_lens.GetRequestDependencies()
-    self._nodes_by_id = {r.request_id : _RequestNode(r) for r in self._requests}
+    self._nodes_by_id = {r.request_id : node_class(r) for r in self._requests}
     edges = []
     for (parent_request, child_request, reason) in deps:
       if (parent_request.request_id not in self._nodes_by_id
@@ -51,7 +57,7 @@ class RequestDependencyGraph(object):
         continue
       parent_node = self._nodes_by_id[parent_request.request_id]
       child_node = self._nodes_by_id[child_request.request_id]
-      edges.append(_Edge(parent_node, child_node, reason))
+      edges.append(edge_class(parent_node, child_node, reason))
     self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
     self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
     self._HandleTimingDependencies()
@@ -73,17 +79,20 @@ class RequestDependencyGraph(object):
       if request_id in request_id_to_cost:
         node.cost = request_id_to_cost[request_id]
 
-  def Cost(self, from_first_request=True):
+  def Cost(self, from_first_request=True, path_list=None, costs_out=None):
     """Returns the cost of the graph, that is the costliest path.
 
     Args:
       from_first_request: (boolean) If True, only considers paths that originate
                           from the first request node.
+      path_list: (list) See graph.Cost().
+      costs_out: (list) See graph.Cost().
     """
     if from_first_request:
-      return self._deps_graph.Cost([self._first_request_node])
+      return self._deps_graph.Cost(
+          [self._first_request_node], path_list, costs_out)
     else:
-      return self._deps_graph.Cost()
+      return self._deps_graph.Cost(path_list=path_list, costs_out=costs_out)
 
   def _HandleTimingDependencies(self):
     try:
@@ -162,6 +171,7 @@ class RequestDependencyGraph(object):
                   # eligible.
       if (edges_by_end_time[end_mark].to_node.request.end_msec
           <= current.to_node.request.start_msec):
+        current.is_timing = True
         self._deps_graph.UpdateEdge(
             current, edges_by_end_time[end_mark].to_node,
             current.to_node)
diff --git a/loading/graph.py b/loading/graph.py
index 789d917..d051d58 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -75,6 +75,10 @@ class DirectedGraph(object):
     """Returns the set of edges of this graph."""
     return self._edges
 
+  def RootNodes(self):
+    """Returns an iterable of nodes that have no incoming edges."""
+    return filter(lambda n: not self.InEdges(n), self._nodes)
+
   def UpdateEdge(self, edge, new_from_node, new_to_node):
     """Updates an edge.
 
@@ -125,15 +129,23 @@ class DirectedGraph(object):
           sources.append(successor)
     return sorted_nodes
 
-  def ReachableNodes(self, roots):
-    """Returns a list of nodes from a set of root nodes."""
+  def ReachableNodes(self, roots, should_stop=lambda n: False):
+    """Returns a list of nodes from a set of root nodes.
+
+    Args:
+      roots: ([Node]) List of roots to start from.
+      should_stop: (callable) Returns True when a node should stop the
+                   exploration and be skipped.
+    """
     visited = set()
-    fifo = collections.deque(roots)
+    fifo = collections.deque([n for n in roots if not should_stop(n)])
     while len(fifo) != 0:
       node = fifo.pop()
+      if should_stop(node):
+        continue
       visited.add(node)
       for e in self.OutEdges(node):
-        if e.to_node not in visited:
+        if e.to_node not in visited and not should_stop(e.to_node):
           visited.add(e.to_node)
         fifo.appendleft(e.to_node)
     return list(visited)
diff --git a/loading/loading_graph_view.py b/loading/loading_graph_view.py
new file mode 100644
index 0000000..c312af0
--- /dev/null
+++ b/loading/loading_graph_view.py
@@ -0,0 +1,88 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Views a trace as an annotated request dependency graph."""
+
+import dependency_graph
+
+
+class RequestNode(dependency_graph.RequestNode):
+  """Represents a request in the graph.
+
+  is_ad and is_tracking are set according to the ContentClassificationLens
+  passed to LoadingGraphView.
+  """
+  def __init__(self, request):
+    super(RequestNode, self).__init__(request)
+    self.is_ad = False
+    self.is_tracking = False
+
+
+class Edge(dependency_graph.Edge):
+  """Represents a dependency between two nodes.
+
+  activity is set according to the ActivityLens passed to LoadingGraphView.
+  """
+  def __init__(self, from_node, to_node, reason):
+    super(Edge, self).__init__(from_node, to_node, reason)
+    self.activity = {}
+
+
+class LoadingGraphView(object):
+  """Represents a trace as a dependency graph. The graph is annotated using
+     optional lenses passed to it.
+  """
+  def __init__(self, trace, dependencies_lens, content_lens=None,
+               frame_lens=None, activity=None):
+    """Initalizes a LoadingGraphView instance.
+
+    Args:
+      trace: (LoadingTrace) a loading trace.
+      dependencies_lens: (RequestDependencyLens)
+      content_lens: (ContentClassificationLens)
+      frame_lens: (FrameLoadLens)
+      activity: (ActivityLens)
+    """
+    self._requests = trace.request_track.GetEvents()
+    self._deps_lens = dependencies_lens
+    self._content_lens = content_lens
+    self._frame_lens = frame_lens
+    self._activity_lens = activity
+    self._graph = None
+    self._BuildGraph()
+
+  def RemoveAds(self):
+    """Updates the graph to remove the Ads.
+
+    Nodes that are only reachable through ad nodes are excluded as well.
+    """
+    roots = self._graph.graph.RootNodes()
+    self._requests = [n.request for n in self._graph.graph.ReachableNodes(
+        roots, should_stop=lambda n: n.is_ad or n.is_tracking)]
+    self._BuildGraph()
+
+  @property
+  def deps_graph(self):
+    return self._graph
+
+  def _BuildGraph(self):
+    self._graph = dependency_graph.RequestDependencyGraph(
+        self._requests, self._deps_lens, RequestNode, Edge)
+    self._AnnotateNodes()
+    self._AnnotateEdges()
+
+  def _AnnotateNodes(self):
+    if self._content_lens is None:
+      return
+    for node in self._graph.graph.Nodes():
+      node.is_ad = self._content_lens.IsAdRequest(node.request)
+      node.is_tracking = self._content_lens.IsTrackingRequest(node.request)
+
+  def _AnnotateEdges(self):
+    if self._activity_lens is None:
+      return
+    for edge in self._graph.graph.Edges():
+      dep = (edge.from_node.request, edge.to_node.request, edge.reason)
+      activity = self._activity_lens.BreakdownEdgeActivityByInitiator(dep)
+      edge.activity = activity
diff --git a/loading/loading_graph_view_unittest.py b/loading/loading_graph_view_unittest.py
new file mode 100644
index 0000000..b1a93c0
--- /dev/null
+++ b/loading/loading_graph_view_unittest.py
@@ -0,0 +1,84 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import loading_graph_view
+import request_dependencies_lens
+from request_dependencies_lens_unittest import TestRequests
+
+
+class MockContentClassificationLens(object):
+  def __init__(self, ad_request_ids, tracking_request_ids):
+    self._ad_requests_ids = ad_request_ids
+    self._tracking_request_ids = tracking_request_ids
+
+  def IsAdRequest(self, request):
+    return request.request_id in self._ad_requests_ids
+
+  def IsTrackingRequest(self, request):
+    return request.request_id in self._tracking_request_ids
+
+
+class LoadingGraphViewTestCase(unittest.TestCase):
+  def setUp(self):
+    super(LoadingGraphViewTestCase, self).setUp()
+    self.trace = TestRequests.CreateLoadingTrace()
+    self.deps_lens = request_dependencies_lens.RequestDependencyLens(self.trace)
+
+  def testAnnotateNodesNoLenses(self):
+    graph_view = loading_graph_view.LoadingGraphView(self.trace, self.deps_lens)
+    for node in graph_view.deps_graph.graph.Nodes():
+      self.assertFalse(node.is_ad)
+      self.assertFalse(node.is_tracking)
+    for edge in graph_view.deps_graph.graph.Edges():
+      self.assertFalse(edge.is_timing)
+
+  def testAnnotateNodesContentLens(self):
+    ad_request_ids = set([TestRequests.JS_REQUEST_UNRELATED_FRAME.request_id])
+    tracking_request_ids = set([TestRequests.JS_REQUEST.request_id])
+    content_lens = MockContentClassificationLens(
+        ad_request_ids, tracking_request_ids)
+    graph_view = loading_graph_view.LoadingGraphView(self.trace, self.deps_lens,
+                                                     content_lens)
+    for node in graph_view.deps_graph.graph.Nodes():
+      request_id = node.request.request_id
+      self.assertEqual(request_id in ad_request_ids, node.is_ad)
+      self.assertEqual(request_id in tracking_request_ids, node.is_tracking)
+
+  def testRemoveAds(self):
+    ad_request_ids = set([TestRequests.JS_REQUEST_UNRELATED_FRAME.request_id])
+    tracking_request_ids = set([TestRequests.JS_REQUEST.request_id])
+    content_lens = MockContentClassificationLens(
+        ad_request_ids, tracking_request_ids)
+    graph_view = loading_graph_view.LoadingGraphView(self.trace, self.deps_lens,
+                                                     content_lens)
+    graph_view.RemoveAds()
+    request_ids = set([n.request.request_id
+                       for n in graph_view.deps_graph.graph.Nodes()])
+    expected_request_ids = set([r.request_id for r in [
+        TestRequests.FIRST_REDIRECT_REQUEST,
+        TestRequests.SECOND_REDIRECT_REQUEST,
+        TestRequests.REDIRECTED_REQUEST,
+        TestRequests.REQUEST,
+        TestRequests.JS_REQUEST_OTHER_FRAME]])
+    self.assertSetEqual(expected_request_ids, request_ids)
+
+  def testRemoveAdsPruneGraph(self):
+    ad_request_ids = set([TestRequests.SECOND_REDIRECT_REQUEST.request_id])
+    tracking_request_ids = set([])
+    content_lens = MockContentClassificationLens(
+        ad_request_ids, tracking_request_ids)
+    graph_view = loading_graph_view.LoadingGraphView(
+        self.trace, self.deps_lens, content_lens)
+    graph_view.RemoveAds()
+    request_ids = set([n.request.request_id
+                       for n in graph_view.deps_graph.graph.Nodes()])
+    expected_request_ids = set(
+        [TestRequests.FIRST_REDIRECT_REQUEST.request_id])
+    self.assertSetEqual(expected_request_ids, request_ids)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/loading_graph_view_visualization.py b/loading/loading_graph_view_visualization.py
new file mode 100644
index 0000000..d083575
--- /dev/null
+++ b/loading/loading_graph_view_visualization.py
@@ -0,0 +1,169 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Visualize a loading_graph_view.LoadingGraphView."""
+
+import request_track
+
+
+class LoadingGraphViewVisualization(object):
+  """Manipulate visual representations of a request graph.
+
+  Currently only DOT output is supported.
+  """
+  _LONG_EDGE_THRESHOLD_MS = 2000  # Time in milliseconds.
+
+  _CONTENT_KIND_TO_COLOR = {
+      'application':     'blue',      # Scripts.
+      'font':            'grey70',
+      'image':           'orange',    # This probably catches gifs?
+      'video':           'hotpink1',
+      'audio':           'hotpink2',
+      }
+
+  _CONTENT_TYPE_TO_COLOR = {
+      'html':            'red',
+      'css':             'green',
+      'script':          'blue',
+      'javascript':      'blue',
+      'json':            'purple',
+      'gif':             'grey',
+      'image':           'orange',
+      'jpeg':            'orange',
+      'ping':            'cyan',  # Empty response
+      'redirect':        'forestgreen',
+      'png':             'orange',
+      'plain':           'brown3',
+      'octet-stream':    'brown3',
+      'other':           'white',
+      }
+
+  _EDGE_REASON_TO_COLOR = {
+    'redirect': 'black',
+    'parser': 'red',
+    'script': 'blue',
+    'script_inferred': 'purple',
+  }
+
+  _ACTIVITY_TYPE_LABEL = (
+      ('idle', 'I'), ('unrelated_work', 'W'), ('script', 'S'),
+      ('parsing', 'P'), ('other_url', 'O'), ('unknown_url', 'U'))
+
+  def __init__(self, graph_view):
+    """Initialize.
+
+    Args:
+      graph_view: (loading_graph_view.LoadingGraphView) the graph to visualize.
+    """
+    self._graph_view = graph_view
+    self._global_start = None
+
+  def OutputDot(self, output):
+    """Output DOT (graphviz) representation.
+
+    Args:
+      output: a file-like output stream to receive the dot file.
+    """
+    nodes = self._graph_view.deps_graph.graph.Nodes()
+    self._global_start = min(n.request.start_msec for n in nodes)
+    g = self._graph_view.deps_graph.graph
+
+    output.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+
+    isolated_nodes = [
+        n for n in nodes if (
+            len(g.InEdges(n)) == 0 and len(g.OutEdges(n)) == 0)]
+    if isolated_nodes:
+      output.write("""subgraph cluster_isolated {
+                        color=black;
+                        label="Isolated Nodes";
+                   """)
+      for n in isolated_nodes:
+        output.write(self._DotNode(n))
+      output.write('}\n')
+
+    output.write("""subgraph cluster_nodes {
+                      color=invis;
+                 """)
+    for n in nodes:
+      if n in isolated_nodes:
+        continue
+      output.write(self._DotNode(n))
+
+    edges = g.Edges()
+    for edge in edges:
+      output.write(self._DotEdge(edge))
+
+    output.write('}\n')
+    output.write('}\n')
+
+  def _ContentTypeToColor(self, content_type):
+    if not content_type:
+      type_str = 'other'
+    elif '/' in content_type:
+      kind, type_str = content_type.split('/', 1)
+      if kind in self._CONTENT_KIND_TO_COLOR:
+        return self._CONTENT_KIND_TO_COLOR[kind]
+    else:
+      type_str = content_type
+    return self._CONTENT_TYPE_TO_COLOR[type_str]
+
+  def _DotNode(self, node):
+    """Returns a graphviz node description for a given node.
+
+    Args:
+      node: (RequestNode)
+
+    Returns:
+      A string describing the resource in graphviz format.
+      The resource is color-coded according to its content type, and its shape
+      is oval if its max-age is less than 300s (or if it's not cacheable).
+    """
+    color = self._ContentTypeToColor(node.request.GetContentType())
+    request = node.request
+    max_age = request.MaxAge()
+    shape = 'polygon' if max_age > 300 else 'oval'
+    styles = ['filled']
+    if node.is_ad or node.is_tracking:
+      styles += ['bold', 'diagonals']
+    return ('"%s" [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
+            'fillcolor = %s; shape = %s];\n'
+            % (request.request_id, request_track.ShortName(request.url),
+               request.start_msec - self._global_start,
+               request.end_msec - self._global_start,
+               request.end_msec - request.start_msec,
+               ','.join(styles), color, shape))
+
+  def _DotEdge(self, edge):
+    """Returns a graphviz edge description for a given edge.
+
+    Args:
+      edge: (Edge)
+
+    Returns:
+      A string encoding the graphviz representation of the edge.
+    """
+    style = {'color': 'orange'}
+    label = '%.02f' % edge.cost
+    if edge.is_timing:
+      style['style'] = 'dashed'
+    style['color'] = self._EDGE_REASON_TO_COLOR[edge.reason]
+    if edge.cost > self._LONG_EDGE_THRESHOLD_MS:
+      style['penwidth'] = '5'
+      style['weight'] = '2'
+    style_str = '; '.join('%s=%s' % (k, v) for (k, v) in style.items())
+
+    label = '%.02f' % edge.cost
+    if edge.activity:
+      separator = ' - '
+      for activity_type, activity_label in self._ACTIVITY_TYPE_LABEL:
+        label += '%s%s:%.02f ' % (
+            separator, activity_label, edge.activity[activity_type])
+        separator = ' '
+    arrow = '[%s; label="%s"]' % (style_str, label)
+    from_request_id = edge.from_node.request.request_id
+    to_request_id = edge.to_node.request.request_id
+    return '"%s" -> "%s" %s;\n' % (from_request_id, to_request_id, arrow)
diff --git a/loading/loading_model.py b/loading/loading_model.py
deleted file mode 100644
index b7f8002..0000000
--- a/loading/loading_model.py
+++ /dev/null
@@ -1,610 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Models for loading in chrome.
-
-(Redirect the following to the general model module once we have one)
-A model is an object with the following methods.
-  CostMs(): return the cost of the model in milliseconds.
-  Set(): set model-specific parameters.
-
-ResourceGraph
-  This creates a DAG of resource dependencies from loading.log_requests to model
-  loading time. The model may be parameterized by changing the loading time of
-  a particular or all resources.
-"""
-
-import logging
-import os
-
-import sys
-
-import activity_lens
-import dag
-import loading_trace
-import request_dependencies_lens
-import request_track
-
-class ResourceGraph(object):
-  """A model of loading by a DAG of resource dependencies.
-
-  See model parameters in Set().
-  """
-  # The lens to build request dependencies. Exposed here for subclasses in
-  # unittesting.
-  REQUEST_LENS = request_dependencies_lens.RequestDependencyLens
-
-  EDGE_KIND_KEY = 'edge_kind'
-  EDGE_KINDS = request_track.Request.INITIATORS + (
-      'script_inferred', 'after-load', 'before-load', 'timing')
-
-  def __init__(self, trace, content_lens=None, frame_lens=None,
-               activity=None):
-    """Create from a LoadingTrace (or json of a trace).
-
-    Args:
-      trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
-      content_lens: (ContentClassificationLens) Lens used to annotate the
-                    nodes, or None.
-      frame_lens: (FrameLoadLens) Lens used to augment graph with load nodes.
-      activity:   (ActivityLens) Lens used to augment the edges with the
-                   activity.
-    """
-    if type(trace) == dict:
-      trace = loading_trace.LoadingTrace.FromJsonDict(trace)
-    self._trace = trace
-    self._content_lens = content_lens
-    self._frame_lens = frame_lens
-    self._activity_lens = activity
-    self._BuildDag(trace)
-    # Sort before splitting children so that we can correctly dectect if a
-    # reparented child is actually a dependency for a child of its new parent.
-    try:
-      for n in dag.TopologicalSort(self._nodes):
-        self._SplitChildrenByTime(self._node_info[n.Index()])
-    except AssertionError as exc:
-      sys.stderr.write('Bad topological sort: %s\n'
-                       'Skipping child split\n' % str(exc))
-    self._cache_all = False
-    self._node_filter = lambda _: True
-
-  @classmethod
-  def CheckImageLoadConsistency(cls, g1, g2):
-    """Check that images have the same dependencies between ResourceGraphs.
-
-    Image resources are identified by their short names.
-
-    Args:
-      g1: a ResourceGraph instance
-      g2: a ResourceGraph instance
-
-    Returns:
-      A list of discrepancy tuples. If this list is empty, g1 and g2 are
-      consistent with respect to image load dependencies. Otherwise, each tuple
-      is of the form:
-        ( g1 resource short name or str(list of short names),
-          g2 resource short name or str(list of short names),
-          human-readable discrepancy reason )
-      Either or both of the g1 and g2 image resource short names may be None if
-      it's not applicable for the discrepancy reason.
-    """
-    discrepancies = []
-    g1_image_to_info = g1._ExtractImages()
-    g2_image_to_info = g2._ExtractImages()
-    for image in set(g1_image_to_info.keys()) - set(g2_image_to_info.keys()):
-      discrepancies.append((image, None, 'Missing in g2'))
-    for image in set(g2_image_to_info.keys()) - set(g1_image_to_info.keys()):
-      discrepancies.append((None, image, 'Missing in g1'))
-
-    for image in set(g1_image_to_info.keys()) & set(g2_image_to_info.keys()):
-      def PredecessorInfo(g, n):
-        info = [g._ShortName(p) for p in n.Node().Predecessors()]
-        info.sort()
-        return str(info)
-      g1_pred = PredecessorInfo(g1, g1_image_to_info[image])
-      g2_pred = PredecessorInfo(g2, g2_image_to_info[image])
-      if g1_pred != g2_pred:
-        discrepancies.append((g1_pred, g2_pred,
-                              'Predecessor mismatch for ' + image))
-
-    return discrepancies
-
-  def Set(self, cache_all=None, node_filter=None):
-    """Set model parameters.
-
-    TODO(mattcary): add parameters for caching certain types of resources (just
-    scripts, just cacheable, etc).
-
-    Args:
-      cache_all: boolean that if true ignores empirical resource load times for
-        all resources.
-      node_filter: a Node->boolean used to restrict the graph for most
-        operations.
-    """
-    if self._cache_all is not None:
-      self._cache_all = cache_all
-    if node_filter is not None:
-      self._node_filter = node_filter
-
-  def Nodes(self, sort=False):
-    """Return iterable of all nodes via their NodeInfos.
-
-    Args:
-      sort: if true, return nodes in sorted order. This may prune additional
-        nodes from the unsorted list (eg, non-root, non-ad nodes reachable only
-        by ad nodes)
-
-    Returns:
-      Iterable of node infos.
-
-    """
-    if sort:
-      return (self._node_info[n.Index()]
-              for n in dag.TopologicalSort(self._nodes, self._node_filter))
-    return (n for n in self._node_info if self._node_filter(n.Node()))
-
-  def EdgeCosts(self, node_filter=None):
-    """Edge costs.
-
-    Args:
-      node_filter: if not none, a Node->boolean filter to use instead of the
-      current one from Set.
-
-    Returns:
-      The total edge costs of our graph.
-
-    """
-    node_filter = self._node_filter if node_filter is None else node_filter
-    total = 0
-    for n in self._node_info:
-      if not node_filter(n.Node()):
-        continue
-      for s in n.Node().Successors():
-        if node_filter(s):
-          total += self.EdgeCost(n.Node(), s)
-    return total
-
-  def Intersect(self, other_nodes):
-    """Return iterable of nodes that intersect with another graph.
-
-    Args:
-      other_nodes: iterable of the nodes of another graph, eg from Nodes().
-
-    Returns:
-      an iterable of (mine, other) pairs for all nodes for which the URL is
-      identical.
-    """
-    other_map = {n.Url(): n for n in other_nodes}
-    for n in self._node_info:
-      if self._node_filter(n.Node()) and n.Url() in other_map:
-        yield(n, other_map[n.Url()])
-
-  def Cost(self, path_list=None, costs_out=None):
-    """Compute cost of current model.
-
-    Args:
-      path_list: if not None, gets a list of NodeInfo in the longest path.
-      costs_out: if not None, gets a vector of node costs by node index. Any
-        filtered nodes will have zero cost.
-
-    Returns:
-      Cost of the longest path.
-
-    """
-    costs = [0] * len(self._nodes)
-    for n in dag.TopologicalSort(self._nodes, self._node_filter):
-      cost = 0
-      if n.Predecessors():
-        cost = max([costs[p.Index()] + self.EdgeCost(p, n)
-                    for p in n.Predecessors()])
-      if not self._cache_all:
-        cost += self.NodeCost(n)
-      costs[n.Index()] = cost
-    max_cost = max(costs)
-    if costs_out is not None:
-      del costs_out[:]
-      costs_out.extend(costs)
-    assert max_cost > 0  # Otherwise probably the filter went awry.
-    if path_list is not None:
-      del path_list[:]
-      n = (i for i in self._nodes if costs[i.Index()] == max_cost).next()
-      path_list.append(self._node_info[n.Index()])
-      while n.Predecessors():
-        n = reduce(lambda costliest, next:
-                   next if (self._node_filter(next) and
-                            costs[next.Index()] > costs[costliest.Index()])
-                        else costliest,
-                   n.Predecessors())
-        path_list.insert(0, self._node_info[n.Index()])
-    return max_cost
-
-  def FilterAds(self, node):
-    """A filter for use in eg, Cost, to remove advertising nodes.
-
-    Args:
-      node: A dag.Node.
-
-    Returns:
-      True if the node is not ad-related.
-    """
-    node_info = self._node_info[node.Index()]
-    return not (node_info.IsAd() or node_info.IsTracking())
-
-  def ResourceInfo(self):
-    """Get resource info.
-
-    Returns:
-      A list of NodeInfo objects that describe the resources fetched.
-    """
-    return [n for n in self._node_info if n.Request() is not None]
-
-  def DebugString(self):
-    """Graph structure for debugging.
-
-    TODO(mattcary): this fails for graphs with more than one component or where
-    self._nodes[0] is not a root.
-
-    Returns:
-      A human-readable string of the graph.
-    """
-    output = []
-    queue = [self._nodes[0]]
-    visited = set()
-    while queue:
-      n = queue.pop(0)
-      assert n not in visited
-      visited.add(n)
-      children = n.SortedSuccessors()
-      output.append('%d -> [%s]' %
-                    (n.Index(), ' '.join([str(c.Index()) for c in children])))
-      for c in children:
-        assert n in c.Predecessors()  # Integrity checking
-        queue.append(c)
-    assert len(visited) == len(self._nodes)
-    return '\n'.join(output)
-
-  def NodeInfo(self, node):
-    """Return the node info for a graph node.
-
-    Args:
-      node: (int, dag.Node or NodeInfo) a node representation. An int is taken
-      to be the node's index.
-
-    Returns:
-      The NodeInfo instance corresponding to the node.
-    """
-    if type(node) is self._NodeInfo:
-      return node
-    elif type(node) is int:
-      return self._node_info[node]
-    return self._node_info[node.Index()]
-
-  def ShortName(self, node):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(node).ShortName()
-
-  def Url(self, node):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(node).Url()
-
-  def NodeCost(self, node):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(node).NodeCost()
-
-  def EdgeCost(self, parent, child):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(parent).EdgeCost(self.NodeInfo(child))
-
-  def EdgeAnnotations(self, parent, child):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(parent).EdgeAnnotations(self.NodeInfo(child))
-
-  ##
-  ## Internal items
-  ##
-
-  # This resource type may induce a timing dependency. See _SplitChildrenByTime
-  # for details.
-  # TODO(mattcary): are these right?
-  _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
-  _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
-
-  class _NodeInfo(object):
-    """Our internal class that adds cost and other information to nodes.
-
-    Costs are stored on the node as well as edges. Edge information is only
-    stored on successor edges and not predecessor, that is, you get them from
-    the parent and not the child.
-
-    We also store the request on the node, and expose request-derived
-    information like content type.
-    """
-    def __init__(self, node, request):
-      """Create a new node info.
-
-      Args:
-        node: The node to augment.
-        request: The request associated with this node, or an (index, msec)
-          tuple.
-      """
-      self._node = node
-      self._is_ad = False
-      self._is_tracking = False
-      self._edge_costs = {}
-      self._edge_annotations = {}
-
-      if type(request) == tuple:
-        self._request = None
-        self._node_cost = 0
-        self._shortname = 'LOAD %s' % request[0]
-        self._start_time = request[1]
-      else:
-        self._shortname = None
-        self._start_time = None
-        self._request = request
-        # All fields in timing are millis relative to request_time.
-        self._node_cost = max(
-            [0] + [t for f, t in request.timing._asdict().iteritems()
-                   if f != 'request_time'])
-
-    def __str__(self):
-      return self.ShortName()
-
-    def Node(self):
-      return self._node
-
-    def Index(self):
-      return self._node.Index()
-
-    def SetRequestContent(self, is_ad, is_tracking):
-      """Sets the kind of content the request relates to.
-
-      Args:
-        is_ad: (bool) Whether the request is an Ad.
-        is_tracking: (bool) Whether the request is related to tracking.
-      """
-      (self._is_ad, self._is_tracking) = (is_ad, is_tracking)
-
-    def IsAd(self):
-      return self._is_ad
-
-    def IsTracking(self):
-      return self._is_tracking
-
-    def Request(self):
-      return self._request
-
-    def NodeCost(self):
-      return self._node_cost
-
-    def EdgeCost(self, s):
-      return self._edge_costs.get(s, 0)
-
-    def StartTime(self):
-      if self._start_time:
-        return self._start_time
-      return self._request.timing.request_time * 1000
-
-    def EndTime(self):
-      return self.StartTime() + self._node_cost
-
-    def EdgeAnnotations(self, s):
-      assert s.Node() in self.Node().Successors()
-      return self._edge_annotations.get(s, {})
-
-    def ContentType(self):
-      if self._request is None:
-        return 'synthetic'
-      return self._request.GetContentType()
-
-    def ShortName(self):
-      """Returns either the hostname of the resource, or the filename,
-      or the end of the path. Tries to include the domain as much as possible.
-      """
-      if self._shortname:
-        return self._shortname
-      return request_track.ShortName(self._request.url)
-
-    def Url(self):
-      return self._request.url
-
-    def SetEdgeCost(self, child, cost):
-      assert child.Node() in self._node.Successors()
-      self._edge_costs[child] = cost
-
-    def AddEdgeAnnotations(self, s, annotations):
-      assert s.Node() in self._node.Successors()
-      self._edge_annotations.setdefault(s, {}).update(annotations)
-
-    def ReparentTo(self, old_parent, new_parent):
-      """Move costs and annotatations from old_parent to new_parent.
-
-      Also updates the underlying node connections, ie, do not call
-      old_parent.RemoveSuccessor(), etc.
-
-      Args:
-        old_parent: the NodeInfo of a current parent of self. We assert this
-          is actually a parent.
-        new_parent: the NodeInfo of the new parent. We assert it is not already
-          a parent.
-      """
-      assert old_parent.Node() in self.Node().Predecessors()
-      assert new_parent.Node() not in self.Node().Predecessors()
-      edge_annotations = old_parent._edge_annotations.pop(self, {})
-      edge_cost =  old_parent._edge_costs.pop(self)
-      old_parent.Node().RemoveSuccessor(self.Node())
-      new_parent.Node().AddSuccessor(self.Node())
-      new_parent.SetEdgeCost(self, edge_cost)
-      new_parent.AddEdgeAnnotations(self, edge_annotations)
-
-    def __eq__(self, o):
-      """Note this works whether o is a Node or a NodeInfo."""
-      return self.Index() == o.Index()
-
-    def __hash__(self):
-      return hash(self.Node().Index())
-
-  def _BuildDag(self, trace):
-    """Build DAG of resources.
-
-    Build a DAG from our requests and augment with NodeInfo (see above) in a
-    parallel array indexed by Node.Index().
-
-    Creates self._nodes and self._node_info.
-
-    Args:
-      trace: A LoadingTrace.
-    """
-    self._nodes = []
-    self._node_info = []
-    index_by_request = {}
-    for request in trace.request_track.GetEvents():
-      next_index = len(self._nodes)
-      assert request not in index_by_request
-      index_by_request[request] = next_index
-      node = dag.Node(next_index)
-      node_info = self._NodeInfo(node, request)
-      if self._content_lens:
-        node_info.SetRequestContent(
-            self._content_lens.IsAdRequest(request),
-            self._content_lens.IsTrackingRequest(request))
-      self._nodes.append(node)
-      self._node_info.append(node_info)
-
-    dependencies = self.REQUEST_LENS(trace).GetRequestDependencies()
-    for dep in dependencies:
-      (parent_rq, child_rq, reason) = dep
-      parent = self._node_info[index_by_request[parent_rq]]
-      child = self._node_info[index_by_request[child_rq]]
-      edge_cost = request_track.TimeBetween(parent_rq, child_rq, reason)
-      if edge_cost < 0:
-        edge_cost = 0
-        if child.StartTime() < parent.StartTime():
-          logging.error('Inverted dependency: %s->%s',
-                        parent.ShortName(), child.ShortName())
-          # Note that child.StartTime() < parent.EndTime() appears to happen a
-          # fair amount in practice.
-      parent.Node().AddSuccessor(child.Node())
-      parent.SetEdgeCost(child, edge_cost)
-      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: reason})
-      if self._activity_lens:
-        activity = self._activity_lens.BreakdownEdgeActivityByInitiator(dep)
-        parent.AddEdgeAnnotations(child, {'activity': activity})
-
-    self._AugmentFrameLoads(index_by_request)
-
-  def _AugmentFrameLoads(self, index_by_request):
-    if not self._frame_lens:
-      return
-    loads = self._frame_lens.GetFrameLoadInfo()
-    load_index_to_node = {}
-    for l in loads:
-      next_index = len(self._nodes)
-      node = dag.Node(next_index)
-      node_info = self._NodeInfo(node, (l.index, l.msec))
-      load_index_to_node[l.index] = next_index
-      self._nodes.append(node)
-      self._node_info.append(node_info)
-    frame_deps = self._frame_lens.GetFrameLoadDependencies()
-    for load_idx, rq in frame_deps[0]:
-      parent = self._node_info[load_index_to_node[load_idx]]
-      child = self._node_info[index_by_request[rq]]
-      parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'after-load'})
-    for rq, load_idx in frame_deps[1]:
-      child = self._node_info[load_index_to_node[load_idx]]
-      parent = self._node_info[index_by_request[rq]]
-      parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'before-load'})
-
-  def _SplitChildrenByTime(self, parent):
-    """Split children of a node by request times.
-
-    The initiator of a request may not be the true dependency of a request. For
-    example, a script may appear to load several resources independently, but in
-    fact one of them may be a JSON data file, and the remaining resources assets
-    described in the JSON. The assets should be dependent upon the JSON data
-    file, and not the original script.
-
-    This function approximates that by rearranging the children of a node
-    according to their request times. The predecessor of each child is made to
-    be the node with the greatest finishing time, that is before the start time
-    of the child.
-
-    We do this by sorting the nodes twice, once by start time and once by end
-    time. We mark the earliest end time, and then we walk the start time list,
-    advancing the end time mark when it is less than our current start time.
-
-    This is refined by only considering assets which we believe actually create
-    a dependency. We only split if the original parent is a script, and the new
-    parent a data file. We confirm these relationships heuristically by loading
-    pages multiple times and ensuring that dependencies do not change; see
-    CheckImageLoadConsistency() for details.
-
-    We incorporate this heuristic by skipping over any non-script/json resources
-    when moving the end mark.
-
-    TODO(mattcary): More heuristics, like incorporating cachability somehow, and
-    not just picking arbitrarily if there are two nodes with the same end time
-    (does that ever really happen?)
-
-    Args:
-      parent: the NodeInfo whose children we are going to rearrange.
-
-    """
-    if parent.ContentType() not in self._CAN_BE_TIMING_PARENT:
-      return  # No dependency changes.
-    children_by_start_time = [self._node_info[s.Index()]
-                              for s in parent.Node().Successors()]
-    children_by_start_time.sort(key=lambda c: c.StartTime())
-    children_by_end_time = [self._node_info[s.Index()]
-                            for s in parent.Node().Successors()]
-    children_by_end_time.sort(key=lambda c: c.EndTime())
-    end_mark = 0
-    for current in children_by_start_time:
-      if current.StartTime() < parent.EndTime() - 1e-5:
-        logging.warning('Child loaded before parent finished: %s -> %s',
-                        parent.ShortName(), current.ShortName())
-      go_to_next_child = False
-      while end_mark < len(children_by_end_time):
-        if children_by_end_time[end_mark] == current:
-          go_to_next_child = True
-          break
-        elif (children_by_end_time[end_mark].ContentType() not in
-            self._CAN_MAKE_TIMING_DEPENDENCE):
-          end_mark += 1
-        elif (end_mark < len(children_by_end_time) - 1 and
-              children_by_end_time[end_mark + 1].EndTime() <
-                  current.StartTime()):
-          end_mark += 1
-        else:
-          break
-      if end_mark >= len(children_by_end_time):
-        break  # It's not possible to rearrange any more children.
-      if go_to_next_child:
-        continue  # We can't rearrange this child, but the next child may be
-                  # eligible.
-      if children_by_end_time[end_mark].EndTime() <= current.StartTime():
-        current.ReparentTo(parent, children_by_end_time[end_mark])
-        children_by_end_time[end_mark].AddEdgeAnnotations(
-            current, {self.EDGE_KIND_KEY: 'timing'})
-
-  def _ExtractImages(self):
-    """Return interesting image resources.
-
-    Uninteresting image resources are things like ads that we don't expect to be
-    constant across fetches.
-
-    Returns:
-      Dict of image url + short name to NodeInfo.
-    """
-    image_to_info = {}
-    for n in self._node_info:
-      if (n.ContentType() is not None and
-          n.ContentType().startswith('image') and
-          self.FilterAds(n)):
-        key = str((n.Url(), n.ShortName(), n.StartTime()))
-        assert key not in image_to_info, n.Url()
-        image_to_info[key] = n
-    return image_to_info
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
deleted file mode 100644
index ee5e84d..0000000
--- a/loading/loading_model_unittest.py
+++ /dev/null
@@ -1,154 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import os
-import sys
-import unittest
-
-import dag
-import loading_model
-import request_track
-import request_dependencies_lens
-import test_utils
-
-
-class LoadingModelTestCase(unittest.TestCase):
-  def SortedIndicies(self, graph):
-    return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
-
-  def SuccessorIndicies(self, node):
-    return [c.Index() for c in node.SortedSuccessors()]
-
-  def test_DictConstruction(self):
-    graph = test_utils.TestResourceGraph(
-        {'request_track': {
-            'events': [
-                test_utils.MakeRequest(0, 'null', 100, 100.5, 101).ToJsonDict(),
-                test_utils.MakeRequest(1, 0, 102, 102.5, 103).ToJsonDict(),
-                test_utils.MakeRequest(2, 0, 102, 102.5, 103).ToJsonDict(),
-                test_utils.MakeRequest(3, 2, 104, 114.5, 105).ToJsonDict()],
-            'metadata': {
-                request_track.RequestTrack._DUPLICATES_KEY: 0,
-                request_track.RequestTrack._INCONSISTENT_INITIATORS_KEY: 0}},
-         'url': 'foo.com',
-         'tracing_track': {'events': []},
-         'page_track': {'events': []},
-         'metadata': {}})
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
-
-  def test_Costing(self):
-    requests = [test_utils.MakeRequest(0, 'null', 100, 105, 110),
-                test_utils.MakeRequest(1, 0, 115, 117, 120),
-                test_utils.MakeRequest(2, 0, 112, 116, 120),
-                test_utils.MakeRequest(3, 1, 122, 124, 126),
-                test_utils.MakeRequest(4, 3, 127, 127.5, 128),
-                test_utils.MakeRequest(5, 'null', 100, 103, 105),
-                test_utils.MakeRequest(6, 5, 105, 107, 110)]
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [4])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [6])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 5, 1, 2, 6, 3, 4])
-    self.assertEqual(28, graph.Cost())
-    graph.Set(cache_all=True)
-    self.assertEqual(8, graph.Cost())
-
-  def test_MaxPath(self):
-    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 111),
-                test_utils.MakeRequest(1, 0, 115, 120, 121),
-                test_utils.MakeRequest(2, 0, 112, 120, 121),
-                test_utils.MakeRequest(3, 1, 122, 126, 127),
-                test_utils.MakeRequest(4, 3, 127, 128, 129),
-                test_utils.MakeRequest(5, 'null', 100, 105, 106),
-                test_utils.MakeRequest(6, 5, 105, 110, 111)]
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    path_list = []
-    self.assertEqual(29, graph.Cost(path_list))
-    self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
-
-    # More interesting would be a test when a node has multiple predecessors,
-    # but it's not possible for us to construct such a graph from requests yet.
-
-  def test_TimingSplit(self):
-    # Timing adds node 1 as a parent to 2 but not 3.
-    requests = [
-        test_utils.MakeRequest(0, 'null', 100, 110, 110,
-                               magic_content_type=True),
-        test_utils.MakeRequest(1, 0, 115, 120, 120,
-                               magic_content_type=True),
-        test_utils.MakeRequest(2, 0, 121, 122, 122,
-                               magic_content_type=True),
-        test_utils.MakeRequest(3, 0, 112, 119, 119,
-                               magic_content_type=True),
-        test_utils.MakeRequest(4, 2, 122, 126, 126),
-        test_utils.MakeRequest(5, 2, 122, 126, 126)]
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
-
-    # Change node 1 so it is a parent of 3, which becomes the parent of 2.
-    requests[1] = test_utils.MakeRequest(
-        1, 0, 110, 111, 111, magic_content_type=True)
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
-
-    # Add an initiator dependence to 1 that will become the parent of 3.
-    requests[1] = test_utils.MakeRequest(
-        1, 0, 110, 111, 111, magic_content_type=True)
-    requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    # Check it doesn't change until we change the content type of 6.
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
-    requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
-                                         magic_content_type=True)
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [3])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 6, 3, 2, 4, 5])
-
-  def test_TimingSplitImage(self):
-    # If we're all image types, then we shouldn't split by timing.
-    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
-                test_utils.MakeRequest(1, 0, 115, 120, 120),
-                test_utils.MakeRequest(2, 0, 121, 122, 122),
-                test_utils.MakeRequest(3, 0, 112, 119, 119),
-                test_utils.MakeRequest(4, 2, 122, 126, 126),
-                test_utils.MakeRequest(5, 2, 122, 126, 126)]
-    for r in requests:
-      r.response_headers['Content-Type'] = 'image/gif'
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5])
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/model_graph.py b/loading/model_graph.py
deleted file mode 100644
index eaded4a..0000000
--- a/loading/model_graph.py
+++ /dev/null
@@ -1,184 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Visualize a loading_model.ResourceGraph."""
-
-import dag
-import itertools
-
-import loading_model
-
-
-class GraphVisualization(object):
-  """Manipulate visual representations of a resource graph.
-
-  The output will change as the ResourceGraph is changed, for example by setting
-  filters.
-
-  Currently only DOT output is supported.
-  """
-  _LONG_EDGE_THRESHOLD_MS = 2000  # Time in milliseconds.
-
-  _CONTENT_KIND_TO_COLOR = {
-      'application':     'blue',      # Scripts.
-      'font':            'grey70',
-      'image':           'orange',    # This probably catches gifs?
-      'video':           'hotpink1',
-      'audio':           'hotpink2',
-      }
-
-  _CONTENT_TYPE_TO_COLOR = {
-      'html':            'red',
-      'css':             'green',
-      'script':          'blue',
-      'javascript':      'blue',
-      'json':            'purple',
-      'gif':             'grey',
-      'image':           'orange',
-      'jpeg':            'orange',
-      'ping':            'cyan',  # Empty response
-      'redirect':        'forestgreen',
-      'png':             'orange',
-      'plain':           'brown3',
-      'octet-stream':    'brown3',
-      'other':           'white',
-      'synthetic':       'yellow',
-      }
-
-  _EDGE_KIND_TO_COLOR = {
-    'redirect': 'black',
-    'parser': 'red',
-    'script': 'blue',
-    'script_inferred': 'purple',
-    'after-load': 'forestgreen',
-    'before-load': 'forestgreen',
-  }
-
-  _ACTIVITY_TYPE_LABEL = (
-      ('idle', 'I'), ('unrelated_work', 'W'), ('script', 'S'),
-      ('parsing', 'P'), ('other_url', 'O'), ('unknown_url', 'U'))
-
-  def __init__(self, graph):
-    """Initialize.
-
-    Args:
-      graph: (loading_model.ResourceGraph) the graph to visualize.
-    """
-    self._graph = graph
-    self._global_start = None
-
-  def OutputDot(self, output):
-    """Output DOT (graphviz) representation.
-
-    Args:
-      output: a file-like output stream to receive the dot file.
-    """
-    sorted_nodes = [n for n in self._graph.Nodes(sort=True)]
-    self._global_start = min([n.StartTime() for n in sorted_nodes])
-    visited_nodes = set([n for n in sorted_nodes])
-
-    output.write("""digraph dependencies {
-    rankdir = LR;
-    """)
-
-    orphans = set()
-    for n in sorted_nodes:
-      for s in itertools.chain(n.Node().Successors(),
-                               n.Node().Predecessors()):
-        if s in visited_nodes:
-          break
-      else:
-        orphans.add(n)
-    if orphans:
-      output.write("""subgraph cluster_orphans {
-                        color=black;
-                        label="Orphans";
-                   """)
-      for n in orphans:
-        # Ignore synthetic nodes for orphan display.
-        if not self._graph.NodeInfo(n).Request():
-          continue
-        output.write(self.DotNode(n))
-      output.write('}\n')
-
-    output.write("""subgraph cluster_nodes {
-                      color=invis;
-                 """)
-
-    for n in sorted_nodes:
-      if n in orphans:
-        continue
-      output.write(self.DotNode(n))
-
-    for n in visited_nodes:
-      for s in n.Node().Successors():
-        if s not in visited_nodes:
-          continue
-        style = 'color = orange'
-        label = '%.02f' % self._graph.EdgeCost(n, s)
-        annotations = self._graph.EdgeAnnotations(n, s)
-        edge_kind = annotations.get(
-            loading_model.ResourceGraph.EDGE_KIND_KEY, None)
-        assert ((edge_kind is None)
-                or (edge_kind in loading_model.ResourceGraph.EDGE_KINDS))
-        style = 'color = %s' % self._EDGE_KIND_TO_COLOR[edge_kind]
-        if edge_kind == 'timing':
-          style += '; style=dashed'
-        if self._graph.EdgeCost(n, s) > self._LONG_EDGE_THRESHOLD_MS:
-          style += '; penwidth=5; weight=2'
-
-        label = '%.02f' % self._graph.EdgeCost(n, s)
-        if 'activity' in annotations:
-          activity = annotations['activity']
-          separator = ' - '
-          for activity_type, activity_label in self._ACTIVITY_TYPE_LABEL:
-            label += '%s%s:%.02f ' % (
-                separator, activity_label, activity[activity_type])
-            separator = ' '
-        arrow = '[%s; label="%s"]' % (style, label)
-        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
-    output.write('}\n')
-
-    output.write('}\n')
-
-  def _ContentTypeToColor(self, content_type):
-    if not content_type:
-      type_str = 'other'
-    elif '/' in content_type:
-      kind, type_str = content_type.split('/', 1)
-      if kind in self._CONTENT_KIND_TO_COLOR:
-        return self._CONTENT_KIND_TO_COLOR[kind]
-    else:
-      type_str = content_type
-    return self._CONTENT_TYPE_TO_COLOR[type_str]
-
-  def DotNode(self, node):
-    """Returns a graphviz node description for a given node.
-
-    Args:
-      node: a dag.Node or ResourceGraph node info.
-
-    Returns:
-      A string describing the resource in graphviz format.
-      The resource is color-coded according to its content type, and its shape
-      is oval if its max-age is less than 300s (or if it's not cacheable).
-    """
-    if type(node) is dag.Node:
-      node = self._graph.NodeInfo(node)
-    color = self._ContentTypeToColor(node.ContentType())
-    if node.Request():
-      max_age = node.Request().MaxAge()
-      shape = 'polygon' if max_age > 300 else 'oval'
-    else:
-      shape = 'doubleoctagon'
-    styles = ['filled']
-    if node.IsAd() or node.IsTracking():
-      styles += ['bold', 'diagonals']
-    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
-            'fillcolor = %s; shape = %s];\n'
-            % (node.Index(), node.ShortName(),
-               node.StartTime() - self._global_start,
-               node.EndTime() - self._global_start,
-               node.EndTime() - node.StartTime(),
-               ','.join(styles), color, shape))
diff --git a/loading/model_graph_unittest.py b/loading/model_graph_unittest.py
deleted file mode 100644
index 831b1e1..0000000
--- a/loading/model_graph_unittest.py
+++ /dev/null
@@ -1,33 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import gzip
-import json
-import os.path
-import tempfile
-import unittest
-
-import frame_load_lens
-import loading_model
-import loading_trace
-import model_graph
-
-TEST_DATA_DIR = os.path.join(os.path.dirname(__file__), 'testdata')
-
-class ModelGraphTestCase(unittest.TestCase):
-  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
-
-  def test_EndToEnd(self):
-    # Test that we don't crash. This also runs through frame_load_lens.
-    tmp = tempfile.NamedTemporaryFile()
-    with gzip.GzipFile(self._ROLLING_STONE) as f:
-      trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
-      frame_lens = frame_load_lens.FrameLoadLens(trace)
-      graph = loading_model.ResourceGraph(trace=trace, frame_lens=frame_lens)
-      visualization = model_graph.GraphVisualization(graph)
-      visualization.OutputDot(tmp)
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 8e07826..a0b5b16 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -6,7 +6,6 @@
 
 import dependency_graph
 import devtools_monitor
-import loading_model
 import loading_trace
 import page_track
 import request_track
@@ -158,15 +157,6 @@ class SimpleLens(object):
     return deps
 
 
-class TestResourceGraph(loading_model.ResourceGraph):
-  """Replace the default request lens in a ResourceGraph with our SimpleLens."""
-  REQUEST_LENS = SimpleLens
-
-  @classmethod
-  def FromRequestList(cls, requests, page_events=None, trace_events=None):
-    return cls(LoadingTraceFromEvents(requests, page_events, trace_events))
-
-
 class TestDependencyGraph(dependency_graph.RequestDependencyGraph):
   """A dependency graph created from requests using a simple lens."""
   def __init__(self, requests):

commit 2db8db644141af35bb5cbae4c2a72988cbc6b08c
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 1 04:32:37 2016 -0700

    clovis: Add a method to slim a trace by dropping large objects.
    
    JSON is less expensive than objects for trace events. A ~45MB trace file
    expands to more than 500MB of memory. This CL adds a method to return
    the trace to its JSON format. This increases the peak memory usage when
    processing a single trace (and the processing time, since serializing
    the trace is not free), but reduces the total memory cost when holding
    several traces at once.
    
    Review URL: https://codereview.chromium.org/1848203003
    
    Cr-Original-Commit-Position: refs/heads/master@{#384548}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 65f484b3674d177e98ed04aa2ec30482e05a6c3a

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 6dbd13d..7630bcc 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -34,7 +34,8 @@ class LoadingTrace(object):
     self.metadata = metadata
     self.page_track = page
     self.request_track = request
-    self.tracing_track = tracing_track
+    self._tracing_track = tracing_track
+    self._tracing_json_str = None
 
   def ToJsonDict(self):
     """Returns a dictionary representing this instance."""
@@ -94,3 +95,23 @@ class LoadingTrace(object):
                     else categories))
     connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
     return cls(url, chrome_metadata, page, request, trace)
+
+  @property
+  def tracing_track(self):
+    if not self._tracing_track:
+      self._RestoreTracingTrack()
+    return self._tracing_track
+
+  def Slim(self):
+    """Slims the memory usage of a trace by dropping the TraceEvents from it.
+
+    The tracing track is restored on-demand when accessed.
+    """
+    self._tracing_json_str = json.dumps(self._tracing_track.ToJsonDict())
+    self._tracing_track = None
+
+  def _RestoreTracingTrack(self):
+    assert self._tracing_json_str
+    self._tracing_track = tracing.TracingTrack.FromJsonDict(
+        json.loads(self._tracing_json_str))
+    self._tracing_json_str = None

commit fb8ed3b74b67e421d947a50da71b604a66d65988
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 04:22:14 2016 -0700

    tools/android/loading Use '--local_noisy' for trace collection
    
    This options might be useful to diagnose failures
    
    Review URL: https://codereview.chromium.org/1847383002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384545}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 81a4b6ff340a9544a45690982a61e13ed190e3a7

diff --git a/loading/gce/main.py b/loading/gce/main.py
index b950c54..9b74ec6 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -84,23 +84,6 @@ class ServerApp(object):
     blob.upload_from_string(data_string)
     return blob.public_url
 
-  def _DeleteFile(self, filename):
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    try:
-      bucket.delete_blob(filename)
-      return True
-    except NotFound:
-      return False
-
-  def _ReadFile(self, filename):
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.get_blob(filename)
-    if not blob:
-      return None
-    return blob.download_as_string()
-
   def _GenerateTrace(self, url, filename, log_filename):
     """ Generates a trace using analyze.py
 
@@ -117,7 +100,7 @@ class ServerApp(object):
     except OSError:
       pass  # Nothing to remove.
     analyze_path = self._src_path + '/tools/android/loading/analyze.py'
-    command_line = ['python', analyze_path, 'log_requests',
+    command_line = ['python', analyze_path, 'log_requests', '--local_noisy',
         '--clear_cache', '--local', '--headless', '--local_binary',
         self._chrome_path, '--url', url, '--output', filename]
     with open(log_filename, 'w') as log_file:

commit 38c7aea54ceee18a2765565dc5f3361c2c466c33
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 02:45:07 2016 -0700

    tools/android/loading Factor code to query metadata as a function
    
    Review URL: https://codereview.chromium.org/1847853002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384533}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fc66f57b93009510d05efa5943d6b5052fc9a3ca

diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index cf1fe03..94edb22 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -7,6 +7,11 @@
 
 set -v
 
+get_instance_metadata() {
+  curl -fs http://metadata/computeMetadata/v1/instance/attributes/$1 \
+      -H "Metadata-Flavor: Google"
+}
+
 # Talk to the metadata server to get the project id
 PROJECTID=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
@@ -34,9 +39,7 @@ pip install --upgrade pip virtualenv
 # Download the Clovis deployment from Google Cloud Storage and unzip it.
 # It is expected that the contents of the deployment have been generated using
 # the tools/android/loading/gce/deploy.sh script.
-CLOUD_STORAGE_PATH=$(curl -s \
-    "http://metadata/computeMetadata/v1/instance/attributes/cloud-storage-path" \
-    -H "Metadata-Flavor: Google")
+CLOUD_STORAGE_PATH=`get_instance_metadata cloud-storage-path`
 DEPLOYMENT_PATH=$CLOUD_STORAGE_PATH/deployment
 
 mkdir -p /opt/app/clovis
@@ -73,9 +76,7 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
 EOF
 
 # Check if auto-start is enabled
-AUTO_START=$(curl -s \
-    "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
-    -H "Metadata-Flavor: Google")
+AUTO_START=`get_instance_metadata auto-start`
 
 # Exit early if auto start is not enabled.
 if [ "$AUTO_START" != "true" ]; then

commit 8f1c84f3020b2af91fe89fe07ab1c99bb6e8fa0c
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 01:50:34 2016 -0700

    tools/android/loading Run main.py from an arbitrary directory
    
    The path to analyze.py was defined as a relative path to the current
    directory.
    This CL changes this to get the path from the configuration instead.
    
    The goal is to be able to launch the app from a directory that is not
    under the Chromium checkout, and thus avoid polluting that checkout
    
    Review URL: https://codereview.chromium.org/1848773002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384523}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b75ab05b606f449a858c2cc421994cf1fd2a228b

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 1d0332c..52e0c82 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -12,7 +12,7 @@ Install the [gcloud command line tool][1].
 ## Deploy the code
 
 ```shell
-# Build Chrome
+# Build Chrome (do not use the component build).
 BUILD_DIR=out/Release
 ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
 
@@ -32,11 +32,10 @@ gcloud compute instances create clovis-tracer-1 \
  --machine-type n1-standard-1 \
  --image ubuntu-14-04 \
  --zone europe-west1-c \
- --tags clovis-http-server \
  --scopes cloud-platform \
- --metadata cloud-storage-path=$CLOUD_STORAGE_PATH
- --metadata auto-start=true \
- --metadata-from-file startup-script=tools/android/loading/gce/startup-script.sh
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,auto-start=true \
+ --metadata-from-file \
+     startup-script=$CHROMIUM_SRC/tools/android/loading/gce/startup-script.sh
 ```
 
 **Note:** To start an instance without automatically starting the app on it,
@@ -90,12 +89,12 @@ gcloud compute ssh clovis-tracer-1
 
 ## Use the app locally
 
-Set up the local environment:
+From a new directory, set up a local environment:
 
 ```shell
 virtualenv env
 source env/bin/activate
-pip install -r pip_requirements.txt
+pip install -r $CHROMIUM_SRC/tools/android/loading/gce/pip_requirements.txt
 ```
 
 Create a JSON file describing the deployment configuration:
@@ -106,11 +105,13 @@ Create a JSON file describing the deployment configuration:
 # CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
 # be stored.
 # CHROME_PATH is the path to the Chrome executable on the host.
+# CHROMIUM_SRC is the Chromium src directory.
 cat >$CONFIG_FILE << EOF
 {
   "project_name" : "$PROJECT_NAME",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "$CHROME_PATH"
+  "chrome_path" : "$CHROME_PATH",
+  "src_path" : "$CHROMIUM_SRC"
 }
 EOF
 ```
@@ -118,7 +119,9 @@ EOF
 Launch the app, passing the path to the deployment configuration file:
 
 ```shell
-gunicorn --workers=1 --bind 127.0.0.1:8080 'main:StartApp('\"$CONFIG_FILE\"')'
+gunicorn --workers=1 --bind 127.0.0.1:8080 \
+    --pythonpath $CHROMIUM_SRC/tools/android/loading/gce \
+    'main:StartApp('\"$CONFIG_FILE\"')'
 ```
 
 You can now [use the app][2], which is located at http://localhost:8080.
@@ -129,28 +132,5 @@ Tear down the local environment:
 deactivate
 ```
 
-## Project-wide settings
-
-This is already setup, no need to do this again.
-Kept here for reference.
-
-### Firewall rule
-
-Firewall rule to allow access to the instance HTTP server from the outside:
-
-```shell
-gcloud compute firewall-rules create default-allow-http-8080 \
-    --allow tcp:8080 \
-    --source-ranges 0.0.0.0/0 \
-    --target-tags clovis-http-server \
-    --description "Allow port 8080 access to http-server"
-```
-
-The firewall rule can be disabled with:
-
-```shell
-gcloud compute firewall-rules delete default-allow-http-8080
-```
-
 [1]: https://cloud.google.com/sdk
 [2]: #Use-the-app
diff --git a/loading/gce/main.py b/loading/gce/main.py
index b926cad..b950c54 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -41,6 +41,7 @@ class ServerApp(object):
            self._base_path_in_bucket += '/'
 
        self._chrome_path = config['chrome_path']
+       self._src_path = config['src_path']
 
 
   def _GetStorageClient(self):
@@ -115,9 +116,10 @@ class ServerApp(object):
       os.remove(filename)  # Remove any existing trace for this URL.
     except OSError:
       pass  # Nothing to remove.
-    command_line = ['python', '../analyze.py', 'log_requests', '--clear_cache',
-        '--local', '--headless', '--local_binary', self._chrome_path, '--url',
-        url, '--output', filename]
+    analyze_path = self._src_path + '/tools/android/loading/analyze.py'
+    command_line = ['python', analyze_path, 'log_requests',
+        '--clear_cache', '--local', '--headless', '--local_binary',
+        self._chrome_path, '--url', url, '--output', filename]
     with open(log_filename, 'w') as log_file:
       ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
                             stdout = log_file)
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index ef3d24f..cf1fe03 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -67,7 +67,8 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "/opt/app/clovis/binaries/chrome"
+  "chrome_path" : "/opt/app/clovis/binaries/chrome",
+  "src_path" : "/opt/app/clovis/src"
 }
 EOF
 

commit 7aeb4b6dad4606929aa4be0a1ba7f0cde9e40325
Author: droger <droger@chromium.org>
Date:   Thu Mar 31 07:31:16 2016 -0700

    tools/android/loading Add a repeat_count parameter
    
    This CL adds support for generating multiple traces for the same URL.
    
    Review URL: https://codereview.chromium.org/1839053002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384272}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 34f3f023894244d92cfcd2395d6a14f767c8459e

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 5176d00..1d0332c 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -70,7 +70,11 @@ To send a list of URLs to process:
 curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
 ```
 
-where `urls.json` is a file containing URLs as a JSON array.
+where `urls.json` is a JSON dictionary with the keys:
+
+*   `urls`: array of URLs
+*   `repeat_count`: Number of times each URL will be loaded. Each load of a URL
+    generates a separate trace file.
 
 ## Stop the app in the cloud
 
diff --git a/loading/gce/main.py b/loading/gce/main.py
index f89c536..b926cad 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -123,9 +123,12 @@ class ServerApp(object):
                             stdout = log_file)
     return ret == 0
 
-  def _ProcessTasks(self):
+  def _ProcessTasks(self, repeat_count):
     """Iterates over _tasks and runs analyze.py on each of them. Uploads the
     resulting traces to Google Cloud Storage.
+
+    Args:
+      repeat_count: The number of traces generated for each URL.
     """
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
@@ -136,17 +139,20 @@ class ServerApp(object):
     failed_tasks = []
     while len(self._tasks) > 0:
       url = self._tasks.pop()
-      print 'Generating trace for URL: %s' % url
-      filename = pattern.sub('_', url)
-      if self._GenerateTrace(url, filename, log_filename):
-        self._UploadFile(filename, traces_dir + filename)
-      else:
-        print 'analyze.py failed'
-        failed_tasks.append(url)
-        if os.path.isfile(filename):
-          self._UploadFile(filename, failures_dir + filename)
-      print 'Uploading analyze log'
-      self._UploadFile(log_filename, logs_dir + filename)
+      local_filename = pattern.sub('_', url)
+      for repeat in range(repeat_count):
+        print 'Generating trace for URL: %s' % url
+        remote_filename = local_filename + '/' + str(repeat)
+        if self._GenerateTrace(url, local_filename, log_filename):
+          print 'Uploading: %s' % remote_filename
+          self._UploadFile(local_filename, traces_dir + remote_filename)
+        else:
+          print 'analyze.py failed for URL: %s' % url
+          failed_tasks.append({ "url": url, "repeat": repeat})
+          if os.path.isfile(local_filename):
+            self._UploadFile(local_filename, failures_dir + remote_filename)
+        print 'Uploading analyze log'
+        self._UploadFile(log_filename, logs_dir + remote_filename)
 
     if len(failed_tasks) > 0:
       print 'Uploading failing URLs'
@@ -157,19 +163,29 @@ class ServerApp(object):
     """Sets the list of tasks and starts processing them
 
     Args:
-      http_body: List of URLs as a string representing a JSON array.
+      http_body: JSON dictionary. See README.md for a description of the format.
 
     Returns:
       A string to be sent back to the client, describing the success status of
       the request.
     """
-    self._tasks = json.loads(http_body)
+    load_parameters = json.loads(http_body)
+    try:
+      self._tasks = load_parameters['urls']
+    except KeyError:
+      return 'Error: invalid urls'
+    try:
+      repeat_count = int(load_parameters['repeat_count'])
+    except (KeyError, ValueError):
+      return 'Error: invalid repeat_count'
+
     if len(self._tasks) == 0:
       return 'Error: Empty task list'
     elif self._thread is not None and self._thread.is_alive():
       return 'Error: Already running'
     else:
-      self._thread = threading.Thread(target = self._ProcessTasks)
+      self._thread = threading.Thread(target = self._ProcessTasks,
+                                      args = (repeat_count,))
       self._thread.start()
       return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
 

commit d36e479b9434cd70eaa79b805b78707860e1241b
Author: droger <droger@chromium.org>
Date:   Thu Mar 31 06:28:18 2016 -0700

    tools/android/loading Re-enable autostart in the GCE startup script.
    
    The test checking for the autostart parameter was assuming that GCE
    would return the empty string when a metadata was not defined.
    In fact, some error string is returned instead, and thus the value of
    the string has to be explicitly checked.
    
    Review URL: https://codereview.chromium.org/1844533002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384261}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 76aa326bb5c62e334d2cc508f70bbde56532e56b

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 60c29b2..5176d00 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -50,6 +50,15 @@ Otherwise the IP address can be retrieved by doing:
 gcloud compute instances list
 ```
 
+**Note:** It can take a few minutes for the instance to start. You can follow
+the progress of the startup script on the gcloud console web interface (menu
+"Compute Engine" > "VM instances" then click on your instance and scroll down to
+see the "Serial console output") or from the command line using:
+
+```shell
+gcloud compute instances get-serial-port-output clovis-tracer-1
+```
+
 ## Use the app
 
 Check that `http://<instance-ip>:8080/test` prints `hello` when opened in a
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index 9ea5fa4..ef3d24f 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -57,7 +57,6 @@ unzip /opt/app/clovis/binaries/linux.zip -d /opt/app/clovis/binaries/
 cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
 chown root:root /usr/local/sbin/chrome-devel-sandbox
 chmod 4755 /usr/local/sbin/chrome-devel-sandbox
-export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
 
 # Make sure the pythonapp user owns the application code
 chown -R pythonapp:pythonapp /opt/app
@@ -77,12 +76,8 @@ AUTO_START=$(curl -s \
     "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
     -H "Metadata-Flavor: Google")
 
-# TODO(droger): Figure out how to correctly restore check for auto-startup
-# as well as auto-startup code.
-exit 1
-
 # Exit early if auto start is not enabled.
-if [ -z "$AUTO_START" ]; then
+if [ "$AUTO_START" != "true" ]; then
   exit 1
 fi
 
@@ -100,7 +95,8 @@ user=pythonapp
 # configured virtualenv.
 environment=VIRTUAL_ENV="/opt/app/clovis/env", \
     PATH="/opt/app/clovis/env/bin:/usr/bin", \
-    HOME="/home/pythonapp",USER="pythonapp"
+    HOME="/home/pythonapp",USER="pythonapp", \
+    CHROME_DEVEL_SANDBOX="/usr/local/sbin/chrome-devel-sandbox"
 stdout_logfile=syslog
 stderr_logfile=syslog
 EOF

commit d4baaced2a9db67c6d1e35a542f9bea4a15b6b72
Author: droger <droger@chromium.org>
Date:   Thu Mar 31 05:38:54 2016 -0700

    tools/android/loading Cleanup main.py
    
    This CL merges the set_tasks and start API endpoints
    and cleanups the flow.
    
    It also addresses a TODO by uploading the list of
    failed tasks.
    
    Review URL: https://codereview.chromium.org/1831763004
    
    Cr-Original-Commit-Position: refs/heads/master@{#384256}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a0137660f2c93f7eba72e747ad5f3e730b0fe6fb

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 5302aaa..60c29b2 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -63,13 +63,6 @@ curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
 
 where `urls.json` is a file containing URLs as a JSON array.
 
-Start the processing by sending a request to `http://<instance-ip>:8080/start`,
-for example:
-
-```shell
-curl http://<instance-ip>:8080/start
-```
-
 ## Stop the app in the cloud
 
 ```shell
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 80f7f50..f89c536 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import json
+import os
 import re
 import threading
 import subprocess
@@ -37,7 +38,7 @@ class ServerApp(object):
        if len(storage_path_components) > 1:
          self._base_path_in_bucket = '/'.join(storage_path_components[1:])
          if not self._base_path_in_bucket.endswith('/'):
-           self._base_path_in_bucket.append('/')
+           self._base_path_in_bucket += '/'
 
        self._chrome_path = config['chrome_path']
 
@@ -57,15 +58,30 @@ class ServerApp(object):
       filename_dest: name of the file in Google Cloud Storage
 
     Returns:
-      The URL of the new file in Google Cloud Storage.
+      The URL of the file in Google Cloud Storage.
     """
     client = self._GetStorageClient()
     bucket = self._GetStorageBucket(client)
     blob = bucket.blob(filename_dest)
     with open(filename_src) as file_src:
       blob.upload_from_file(file_src)
-    url = blob.public_url
-    return url
+    return blob.public_url
+
+  def _UploadString(self, data_string, filename_dest):
+    """Uploads a string to Google Cloud Storage
+
+    Args:
+      data_string: the contents of the file to be uploaded
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    blob.upload_from_string(data_string)
+    return blob.public_url
 
   def _DeleteFile(self, filename):
     client = self._GetStorageClient()
@@ -84,40 +100,78 @@ class ServerApp(object):
       return None
     return blob.download_as_string()
 
-  def _SetTasks(self, task_list):
-    if len(self._tasks) > 0:
-      return False  # There are tasks already.
-    self._tasks = json.loads(task_list)
-    return len(self._tasks) != 0
-
-  def _GenerateTrace(self, url, filename):
+  def _GenerateTrace(self, url, filename, log_filename):
     """ Generates a trace using analyze.py
 
     Args:
       url: url as a string.
-      filename: name of the file where the output is saved.
+      filename: name of the file where the trace is saved.
+      log_filename: name of the file where standard output and errors are logged
 
     Returns:
       True if the trace was generated successfully.
     """
-    ret = subprocess.call(
-        ['python', '../analyze.py', 'log_requests', '--clear_cache', '--local',
-         '--headless', '--local_binary', self._chrome_path, '--url',
-         url, '--output', filename])
+    try:
+      os.remove(filename)  # Remove any existing trace for this URL.
+    except OSError:
+      pass  # Nothing to remove.
+    command_line = ['python', '../analyze.py', 'log_requests', '--clear_cache',
+        '--local', '--headless', '--local_binary', self._chrome_path, '--url',
+        url, '--output', filename]
+    with open(log_filename, 'w') as log_file:
+      ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
+                            stdout = log_file)
     return ret == 0
 
   def _ProcessTasks(self):
+    """Iterates over _tasks and runs analyze.py on each of them. Uploads the
+    resulting traces to Google Cloud Storage.
+    """
+    failures_dir = self._base_path_in_bucket + 'failures/'
+    traces_dir = self._base_path_in_bucket + 'traces/'
+    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
+    log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
+    failed_tasks = []
     while len(self._tasks) > 0:
       url = self._tasks.pop()
+      print 'Generating trace for URL: %s' % url
       filename = pattern.sub('_', url)
-      if self._GenerateTrace(url, filename):
-        self._UploadFile(filename, self._base_path_in_bucket + 'traces/'
-                         + filename)
+      if self._GenerateTrace(url, filename, log_filename):
+        self._UploadFile(filename, traces_dir + filename)
       else:
-        # TODO(droger): Upload the list of urls that failed.
         print 'analyze.py failed'
+        failed_tasks.append(url)
+        if os.path.isfile(filename):
+          self._UploadFile(filename, failures_dir + filename)
+      print 'Uploading analyze log'
+      self._UploadFile(log_filename, logs_dir + filename)
+
+    if len(failed_tasks) > 0:
+      print 'Uploading failing URLs'
+      self._UploadString(json.dumps(failed_tasks),
+                         failures_dir + 'failures.json')
+
+  def _SetTaskList(self, http_body):
+    """Sets the list of tasks and starts processing them
+
+    Args:
+      http_body: List of URLs as a string representing a JSON array.
+
+    Returns:
+      A string to be sent back to the client, describing the success status of
+      the request.
+    """
+    self._tasks = json.loads(http_body)
+    if len(self._tasks) == 0:
+      return 'Error: Empty task list'
+    elif self._thread is not None and self._thread.is_alive():
+      return 'Error: Already running'
+    else:
+      self._thread = threading.Thread(target = self._ProcessTasks)
+      self._thread.start()
+      return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
 
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
@@ -129,19 +183,7 @@ class ServerApp(object):
       except (ValueError):
         body_size = 0
       body = environ['wsgi.input'].read(body_size)
-      if self._SetTasks(body):
-        data = 'Set tasks: ' + str(len(self._tasks))
-      else:
-        data = 'Something went wrong'
-    elif path == '/start':
-      if len(self._tasks) == 0 :
-        data = 'Nothing to do!'
-      elif self._thread is not None and self._thread.is_alive():
-        data = 'Already running!'
-      else:
-        data = 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
-        self._thread = threading.Thread(target = self._ProcessTasks)
-        self._thread.start()
+      data = self._SetTaskList(body)
     elif path == '/test':
       data = 'hello'
     else:
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index 74b049a..9ea5fa4 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -98,7 +98,8 @@ autorestart=true
 user=pythonapp
 # Environment variables ensure that the application runs inside of the
 # configured virtualenv.
-environment=VIRTUAL_ENV="/opt/app/clovis/env",PATH="/opt/app/clovis/env/bin",\
+environment=VIRTUAL_ENV="/opt/app/clovis/env", \
+    PATH="/opt/app/clovis/env/bin:/usr/bin", \
     HOME="/home/pythonapp",USER="pythonapp"
 stdout_logfile=syslog
 stderr_logfile=syslog

commit 50d2611774ae0a45d25469a55a004a0c9536ac07
Author: blundell <blundell@chromium.org>
Date:   Thu Mar 31 03:41:49 2016 -0700

    tools/android/loading: Eliminate need for distinct GCE repo
    
    This CL eliminates the need to have a separate git repo for the
    GCE-related code. Instead, the code needed to run Clovis in the cloud
    is uploaded to Google Storage along with the binaries.
    
    Review URL: https://codereview.chromium.org/1836503002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384243}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 19a7554f5cc09950603cdff94b2ff4a518a49737

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 453ca86..5302aaa 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -9,46 +9,20 @@ Engine.
 
 Install the [gcloud command line tool][1].
 
-Checkout the source:
-
-```shell
-mkdir clovis
-cd clovis
-gcloud init
-```
-
-When offered, accept to clone the Google Cloud repo.
-
-## Update or Change the code
-
-Make changes to the code, or copy the latest version from Chromium into your
-local Google Cloud repository:
+## Deploy the code
 
 ```shell
 # Build Chrome
 BUILD_DIR=out/Release
 ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
 
-GCE_DIR=~/dev/clovis/default
-
 # Deploy to GCE
-# CHROME_BUCKET_NAME is the name of the Google Cloud Storage bucket where the
-# Chrome build artifacts will be uploaded, and matches the value of
-# 'bucket_name' in server_config.json.
-./tools/android/loading/gce/deploy.sh $BUILD_DIR $GCE_DIR $CHROME_BUCKET_NAME
-
-cd $GCE_DIR
-
-# git add the relevant files
+# CLOUD_STORAGE_PATH is the path in Google Cloud Storage under which the
+# Clovis deployment will be uploaded.
 
-# commit and push:
-git commit
-git push -u origin master
+./tools/android/loading/gce/deploy.sh $BUILD_DIR $CLOUD_STORAGE_PATH
 ```
 
-If there are instances already running, they need to be restarted for this to
-take effect.
-
 ## Start the app in the cloud
 
 Create an instance using latest ubuntu LTS:
@@ -60,6 +34,7 @@ gcloud compute instances create clovis-tracer-1 \
  --zone europe-west1-c \
  --tags clovis-http-server \
  --scopes cloud-platform \
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH
  --metadata auto-start=true \
  --metadata-from-file startup-script=tools/android/loading/gce/startup-script.sh
 ```
@@ -109,7 +84,7 @@ gcloud compute ssh clovis-tracer-1
 
 ## Use the app locally
 
-Setup the local environment:
+Set up the local environment:
 
 ```shell
 virtualenv env
@@ -117,11 +92,27 @@ source env/bin/activate
 pip install -r pip_requirements.txt
 ```
 
-Launch the app, passing the path to the Chrome executable on the host:
+Create a JSON file describing the deployment configuration:
+
+```shell
+# CONFIG_FILE is the output json file.
+# PROJECT_NAME is the Google Cloud project.
+# CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
+# be stored.
+# CHROME_PATH is the path to the Chrome executable on the host.
+cat >$CONFIG_FILE << EOF
+{
+  "project_name" : "$PROJECT_NAME",
+  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
+  "chrome_path" : "$CHROME_PATH"
+}
+EOF
+```
+
+Launch the app, passing the path to the deployment configuration file:
 
 ```shell
-gunicorn --workers=1 --bind 127.0.0.1:8080 \
-    'main:StartApp("/path/to/chrome")'
+gunicorn --workers=1 --bind 127.0.0.1:8080 'main:StartApp('\"$CONFIG_FILE\"')'
 ```
 
 You can now [use the app][2], which is located at http://localhost:8080.
@@ -137,15 +128,6 @@ deactivate
 This is already setup, no need to do this again.
 Kept here for reference.
 
-### Server configuration file
-
-`main.py` expects to find a `server_config.json` file, which is a dictionary
-with the keys:
-
-*   `project_name`: the name of the Google Compute project,
-*   `bucket_name`: the name of the Google Storage bucket used to store the
-    results.
-
 ### Firewall rule
 
 Firewall rule to allow access to the instance HTTP server from the outside:
diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 8e937ed..0a3a531 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -5,48 +5,55 @@
 
 # This script copies all dependencies required for trace collection.
 # Usage:
-#   deploy.sh builddir outdir bucket
+#   deploy.sh builddir gcs_path
 #
 # Where:
 #   builddir is the build directory for Chrome
-#   outdir is the directory where files are deployed
-#   bucket is the Google Storage bucket where Chrome is uploaded
+#   gcs_path is the Google Storage bucket under which the deployment is
+#   installed
 
 builddir=$1
-outdir=$2
-bucket=$3
+tmpdir=`mktemp -d`
+deployment_gcs_path=$2/deployment
 
-# Copy files from tools/android/loading
-mkdir -p $outdir/tools/android/loading
-cp tools/android/loading/*.py $outdir/tools/android/loading
-cp -r tools/android/loading/gce $outdir/tools/android/loading
+# Extract needed sources.
+src_suffix=src
+tmp_src_dir=$tmpdir/$src_suffix
 
-# Copy other dependencies
-mkdir $outdir/third_party
-# Use rsync to exclude unwanted files (e.g. the .git directory).
+# Copy files from tools/android/loading.
+mkdir -p $tmp_src_dir/tools/android/loading
+cp tools/android/loading/*.py $tmp_src_dir/tools/android/loading
+cp -r tools/android/loading/gce $tmp_src_dir/tools/android/loading
+
+# Copy other dependencies.
+mkdir $tmp_src_dir/third_party
 rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
-  --delete third_party/catapult $outdir/third_party
-mkdir $outdir/tools/perf
-cp -r tools/perf/chrome_telemetry_build $outdir/tools/perf
-mkdir -p $outdir/build/android
-cp build/android/devil_chromium.py $outdir/build/android/
-cp build/android/video_recorder.py $outdir/build/android/
-cp build/android/devil_chromium.json $outdir/build/android/
-cp -r build/android/pylib $outdir/build/android/
-
-# Copy the chrome executable to Google Cloud Storage
+  --delete third_party/catapult $tmp_src_dir/third_party
+mkdir $tmp_src_dir/tools/perf
+cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
+mkdir -p $tmp_src_dir/build/android
+cp build/android/devil_chromium.py $tmp_src_dir/build/android/
+cp build/android/video_recorder.py $tmp_src_dir/build/android/
+cp build/android/devil_chromium.json $tmp_src_dir/build/android/
+cp -r build/android/pylib $tmp_src_dir/build/android/
+
+# Tar up the source and copy it to Google Cloud Storage.
+source_tarball=$tmpdir/source.tgz
+tar -cvzf $source_tarball -C $tmpdir $src_suffix
+gsutil cp $source_tarball gs://$deployment_gcs_path/source/
+
+# Copy the chrome executable to Google Cloud Storage.
 chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
-  /tmp/linux.zip
-gsutil cp /tmp/linux.zip gs://$bucket/chrome/linux.zip
-rm /tmp/linux.zip
+  $tmpdir/linux.zip
+gsutil cp $tmpdir/linux.zip gs://$deployment_gcs_path/binaries/linux.zip
 
-# Upload Chromium revision
+# Generate and upload metadata about this deployment.
 CHROMIUM_REV=$(git merge-base HEAD origin/master)
-cat >/tmp/build_metadata.json << EOF
+cat >$tmpdir/build_metadata.json << EOF
 {
   "chromium_rev": "$CHROMIUM_REV"
 }
 EOF
-gsutil cp /tmp/build_metadata.json gs://$bucket/chrome/build_metadata.json
-rm /tmp/build_metadata.json
-
+gsutil cp $tmpdir/build_metadata.json \
+  gs://$deployment_gcs_path/deployment_metadata.json
+rm -rf $tmpdir
diff --git a/loading/gce/main.py b/loading/gce/main.py
index cddc903..80f7f50 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -16,25 +16,38 @@ class ServerApp(object):
   Google Cloud Storage.
   """
 
-  def __init__(self, chrome_path):
-    """The chrome_path argument is the path to the Chrome executable as a
-    string.
+  def __init__(self, configuration_file):
+    """|configuration_file| is a path to a file containing JSON as described in
+    README.md.
     """
     self._tasks = []
     self._thread = None
-    self._chrome_path = chrome_path
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
-    print 'Reading server configuration'
-    with open('server_config.json') as configuration_file:
-       self._config = json.load(configuration_file)
+    print 'Reading configuration'
+    with open(configuration_file) as config_json:
+       config = json.load(config_json)
+       self._project_name = config['project_name']
+
+       # Separate the cloud storage path into the bucket and the base path under
+       # the bucket.
+       storage_path_components = config['cloud_storage_path'].split('/')
+       self._bucket_name = storage_path_components[0]
+       self._base_path_in_bucket = ''
+       if len(storage_path_components) > 1:
+         self._base_path_in_bucket = '/'.join(storage_path_components[1:])
+         if not self._base_path_in_bucket.endswith('/'):
+           self._base_path_in_bucket.append('/')
+
+       self._chrome_path = config['chrome_path']
+
 
   def _GetStorageClient(self):
-    return storage.Client(project = self._config['project_name'],
+    return storage.Client(project = self._project_name,
                           credentials = self._credentials)
 
   def _GetStorageBucket(self, storage_client):
-    return storage_client.get_bucket(self._config['bucket_name'])
+    return storage_client.get_bucket(self._bucket_name)
 
   def _UploadFile(self, filename_src, filename_dest):
     """Uploads a file to Google Cloud Storage
@@ -89,8 +102,8 @@ class ServerApp(object):
     """
     ret = subprocess.call(
         ['python', '../analyze.py', 'log_requests', '--clear_cache', '--local',
-         '--headless', '--local_binary', self._chrome_path, '--url', url,
-         '--output', filename])
+         '--headless', '--local_binary', self._chrome_path, '--url',
+         url, '--output', filename])
     return ret == 0
 
   def _ProcessTasks(self):
@@ -100,7 +113,8 @@ class ServerApp(object):
       url = self._tasks.pop()
       filename = pattern.sub('_', url)
       if self._GenerateTrace(url, filename):
-        self._UploadFile(filename, 'traces/' + filename)
+        self._UploadFile(filename, self._base_path_in_bucket + 'traces/'
+                         + filename)
       else:
         # TODO(droger): Upload the list of urls that failed.
         print 'analyze.py failed'
@@ -142,5 +156,5 @@ class ServerApp(object):
     return iter([data])
 
 
-def StartApp(chrome_path):
-  return ServerApp(chrome_path)
+def StartApp(configuration_file):
+  return ServerApp(configuration_file)
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index bb1f94a..74b049a 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -31,37 +31,30 @@ useradd -m -d /home/pythonapp pythonapp
 # pip from apt is out of date, so make it update itself and install virtualenv.
 pip install --upgrade pip virtualenv
 
-# Get the source code from the Google Cloud Repository.
-# It is expected that the contents of this repository have been generated using
+# Download the Clovis deployment from Google Cloud Storage and unzip it.
+# It is expected that the contents of the deployment have been generated using
 # the tools/android/loading/gce/deploy.sh script.
-# git requires $HOME and it's not set during the startup script.
-export HOME=/root
-git config --global credential.helper gcloud.sh
-git clone --depth 1 https://source.developers.google.com/p/$PROJECTID \
-    /opt/app/clovis
+CLOUD_STORAGE_PATH=$(curl -s \
+    "http://metadata/computeMetadata/v1/instance/attributes/cloud-storage-path" \
+    -H "Metadata-Flavor: Google")
+DEPLOYMENT_PATH=$CLOUD_STORAGE_PATH/deployment
+
+mkdir -p /opt/app/clovis
+gsutil cp gs://$DEPLOYMENT_PATH/source/source.tgz /opt/app/clovis/source.tgz
+tar xvf /opt/app/clovis/source.tgz -C /opt/app/clovis
+rm /opt/app/clovis/source.tgz
 
 # Install app dependencies
 virtualenv /opt/app/clovis/env
 /opt/app/clovis/env/bin/pip install \
-    -r /opt/app/clovis/tools/android/loading/gce/pip_requirements.txt
-
-# Download Chrome from Google Cloud Storage and unzip it.
-# It is expected that the contents of the bucket have been generated using the
-# tools/android/loading/gce/deploy.sh script.
-STORAGE_BUCKET=`python - <<EOF
-import json
-config_file = "/opt/app/clovis/tools/android/loading/gce/server_config.json"
-with open(config_file) as config:
-  obj=json.load(config);
-  print obj["bucket_name"]
-EOF`
-
-mkdir /opt/app/clovis/out
-gsutil cp gs://$STORAGE_BUCKET/chrome/* /opt/app/clovis/out/
-unzip /opt/app/clovis/out/linux.zip -d /opt/app/clovis/out/
+    -r /opt/app/clovis/src/tools/android/loading/gce/pip_requirements.txt
+
+mkdir /opt/app/clovis/binaries
+gsutil cp gs://$DEPLOYMENT_PATH/binaries/* /opt/app/clovis/binaries/
+unzip /opt/app/clovis/binaries/linux.zip -d /opt/app/clovis/binaries/
 
 # Install the Chrome sandbox
-cp /opt/app/clovis/out/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
+cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
 chown root:root /usr/local/sbin/chrome-devel-sandbox
 chmod 4755 /usr/local/sbin/chrome-devel-sandbox
 export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
@@ -69,6 +62,16 @@ export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
 # Make sure the pythonapp user owns the application code
 chown -R pythonapp:pythonapp /opt/app
 
+# Create the configuration file for this deployment.
+DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
+cat >$DEPLOYMENT_CONFIG_PATH << EOF
+{
+  "project_name" : "$PROJECTID",
+  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
+  "chrome_path" : "/opt/app/clovis/binaries/chrome"
+}
+EOF
+
 # Check if auto-start is enabled
 AUTO_START=$(curl -s \
     "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
@@ -87,9 +90,9 @@ fi
 # applicaiton.
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
-directory=/opt/app/clovis/tools/android/loading/gce
+directory=/opt/app/clovis/src/tools/android/loading/gce
 command=/opt/app/clovis/env/bin/gunicorn --workers=1 --bind 0.0.0.0:8080 \
-    'main:StartApp("/opt/app/clovis/out/chrome")'
+    'main:StartApp('\"$DEPLOYMENT_CONFIG_PATH\"')'
 autostart=true
 autorestart=true
 user=pythonapp

commit d4219e74cbe5de58e402464e41042631ae2f9097
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 30 06:44:18 2016 -0700

    clovis: Add the timing heuristic to request_dependency_graph.
    
    This heuristic is taken from loading_model, and is a required step to
    complete the refactoring of loading_model. It also provides better
    results for prefetch_view, which uses dependency_graph.
    
    Review URL: https://codereview.chromium.org/1848443002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383968}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4393a4e251c93a2ffe59f37b0a09853c85b32043

diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index be2c5e0..13f5485 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -4,6 +4,9 @@
 
 """Request dependency graph."""
 
+import logging
+import sys
+
 import graph
 import request_track
 
@@ -25,6 +28,12 @@ class _Edge(graph.Edge):
 
 class RequestDependencyGraph(object):
   """Request dependency graph."""
+  # This resource type may induce a timing dependency. See _SplitChildrenByTime
+  # for details.
+  # TODO(lizeb,mattcary): are these right?
+  _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
+  _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
+
   def __init__(self, requests, dependencies_lens):
     """Creates a request dependency graph.
 
@@ -45,6 +54,7 @@ class RequestDependencyGraph(object):
       edges.append(_Edge(parent_node, child_node, reason))
     self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
     self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
+    self._HandleTimingDependencies()
 
   @property
   def graph(self):
@@ -55,9 +65,8 @@ class RequestDependencyGraph(object):
     """Updates the cost of the nodes identified by their request ID.
 
     Args:
-      request_id_to_cost: {request_id: new_cost} Can be a superset of the
+      request_id_to_cost: ({request_id: new_cost}) Can be a superset of the
                           requests actually present in the graph.
-
     """
     for node in self._deps_graph.Nodes():
       request_id = node.request.request_id
@@ -75,3 +84,84 @@ class RequestDependencyGraph(object):
       return self._deps_graph.Cost([self._first_request_node])
     else:
       return self._deps_graph.Cost()
+
+  def _HandleTimingDependencies(self):
+    try:
+      for n in self._deps_graph.TopologicalSort():
+        self._SplitChildrenByTime(n)
+    except AssertionError as exc:
+      sys.stderr.write('Bad topological sort: %s\n'
+                       'Skipping child split\n' % str(exc))
+
+  def _SplitChildrenByTime(self, parent):
+    """Splits children of a node by request times.
+
+    The initiator of a request may not be the true dependency of a request. For
+    example, a script may appear to load several resources independently, but in
+    fact one of them may be a JSON data file, and the remaining resources assets
+    described in the JSON. The assets should be dependent upon the JSON data
+    file, and not the original script.
+
+    This function approximates that by rearranging the children of a node
+    according to their request times. The predecessor of each child is made to
+    be the node with the greatest finishing time, that is before the start time
+    of the child.
+
+    We do this by sorting the nodes twice, once by start time and once by end
+    time. We mark the earliest end time, and then we walk the start time list,
+    advancing the end time mark when it is less than our current start time.
+
+    This is refined by only considering assets which we believe actually create
+    a dependency. We only split if the original parent is a script, and the new
+    parent a data file.
+    We incorporate this heuristic by skipping over any non-script/json resources
+    when moving the end mark.
+
+    TODO(mattcary): More heuristics, like incorporating cachability somehow, and
+    not just picking arbitrarily if there are two nodes with the same end time
+    (does that ever really happen?)
+
+    Args:
+      parent: (_RequestNode) The children of this node are processed by this
+              function.
+    """
+    if parent.request.GetContentType() not in self._CAN_BE_TIMING_PARENT:
+      return
+    edges = self._deps_graph.OutEdges(parent)
+    edges_by_start_time = sorted(
+        edges, key=lambda e: e.to_node.request.start_msec)
+    edges_by_end_time = sorted(
+        edges, key=lambda e: e.to_node.request.end_msec)
+    end_mark = 0
+    for current in edges_by_start_time:
+      assert current.from_node is parent
+      if current.to_node.request.start_msec < parent.request.end_msec - 1e-5:
+        parent_url = parent.request.url
+        child_url = current.to_node.request.url
+        logging.warning('Child loaded before parent finished: %s -> %s',
+                        request_track.ShortName(parent_url),
+                        request_track.ShortName(child_url))
+      go_to_next_child = False
+      while end_mark < len(edges_by_end_time):
+        if edges_by_end_time[end_mark] == current:
+          go_to_next_child = True
+          break
+        elif (edges_by_end_time[end_mark].to_node.request.GetContentType()
+              not in self._CAN_MAKE_TIMING_DEPENDENCE):
+          end_mark += 1
+        elif (end_mark < len(edges_by_end_time) - 1 and
+              edges_by_end_time[end_mark + 1].to_node.request.end_msec
+              < current.to_node.request.start_msec):
+          end_mark += 1
+        else:
+          break
+      if end_mark >= len(edges_by_end_time):
+        break  # It's not possible to rearrange any more children.
+      if go_to_next_child:
+        continue  # We can't rearrange this child, but the next child may be
+                  # eligible.
+      if (edges_by_end_time[end_mark].to_node.request.end_msec
+          <= current.to_node.request.start_msec):
+        self._deps_graph.UpdateEdge(
+            current, edges_by_end_time[end_mark].to_node,
+            current.to_node)
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
index 448a234..4f60995 100644
--- a/loading/dependency_graph_unittest.py
+++ b/loading/dependency_graph_unittest.py
@@ -8,6 +8,7 @@ import dependency_graph
 import request_dependencies_lens
 from request_dependencies_lens_unittest import TestRequests
 import request_track
+import test_utils
 
 
 class RequestDependencyGraphTestCase(unittest.TestCase):
@@ -49,6 +50,94 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     g.UpdateRequestsCost({TestRequests.SECOND_REDIRECT_REQUEST.request_id: 0})
     self.assertEqual(6990, g.Cost())
 
+  def testHandleTimingDependencies(self):
+    # Timing adds node 1 as a parent to 2 but not 3.
+    requests = [
+        test_utils.MakeRequest(0, 'null', 100, 110, 110,
+                               magic_content_type=True),
+        test_utils.MakeRequest(1, 0, 115, 120, 120,
+                               magic_content_type=True),
+        test_utils.MakeRequest(2, 0, 121, 122, 122,
+                               magic_content_type=True),
+        test_utils.MakeRequest(3, 0, 112, 119, 119,
+                               magic_content_type=True),
+        test_utils.MakeRequest(4, 2, 122, 126, 126),
+        test_utils.MakeRequest(5, 2, 122, 126, 126)]
+
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(
+        self._Successors(g, requests[0]), set([requests[1], requests[3]]))
+    self.assertSetEqual(
+        self._Successors(g, requests[1]), set([requests[2]]))
+    self.assertSetEqual(
+        self._Successors(g, requests[2]), set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set())
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+
+    # Change node 1 so it is a parent of 3, which becomes the parent of 2.
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(self._Successors(g, requests[0]), set([requests[1]]))
+    self.assertSetEqual(self._Successors(g, requests[1]), set([requests[3]]))
+    self.assertSetEqual(self._Successors(g, requests[2]),
+                        set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set([requests[2]]))
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+
+    # Add an initiator dependence to 1 that will become the parent of 3.
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
+    requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
+    g = self._GraphFromRequests(requests)
+    # Check it doesn't change until we change the content type of 6.
+    self.assertEqual(self._Successors(g, requests[6]), set())
+    requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
+                                         magic_content_type=True)
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(self._Successors(g, requests[0]), set([requests[1]]))
+    self.assertSetEqual(self._Successors(g, requests[1]), set([requests[6]]))
+    self.assertSetEqual(self._Successors(g, requests[2]),
+                        set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set([requests[2]]))
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+    self.assertSetEqual(self._Successors(g, requests[6]), set([requests[3]]))
+
+  def testHandleTimingDependenciesImages(self):
+    # If we're all image types, then we shouldn't split by timing.
+    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
+                test_utils.MakeRequest(1, 0, 115, 120, 120),
+                test_utils.MakeRequest(2, 0, 121, 122, 122),
+                test_utils.MakeRequest(3, 0, 112, 119, 119),
+                test_utils.MakeRequest(4, 2, 122, 126, 126),
+                test_utils.MakeRequest(5, 2, 122, 126, 126)]
+    for r in requests:
+      r.response_headers['Content-Type'] = 'image/gif'
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(self._Successors(g, requests[0]),
+                        set([requests[1], requests[2], requests[3]]))
+    self.assertSetEqual(self._Successors(g, requests[1]), set())
+    self.assertSetEqual(self._Successors(g, requests[2]),
+                        set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set())
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+
+  @classmethod
+  def _GraphFromRequests(cls, requests):
+    trace = test_utils.LoadingTraceFromEvents(requests)
+    deps_lens = test_utils.SimpleLens(trace)
+    return dependency_graph.RequestDependencyGraph(requests, deps_lens)
+
+  @classmethod
+  def _Successors(cls, g, parent_request):
+    parent_node = g._nodes_by_id[parent_request.request_id]
+    edges = g._deps_graph.OutEdges(parent_node)
+    return set(e.to_node.request for e in edges)
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/loading_model.py b/loading/loading_model.py
index d929ac4..b7f8002 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -17,7 +17,7 @@ ResourceGraph
 
 import logging
 import os
-import urlparse
+
 import sys
 
 import activity_lens
@@ -404,22 +404,7 @@ class ResourceGraph(object):
       """
       if self._shortname:
         return self._shortname
-      parsed = urlparse.urlparse(self._request.url)
-      path = parsed.path
-      hostname = parsed.hostname if parsed.hostname else '?.?.?'
-      if path != '' and path != '/':
-        last_path = parsed.path.split('/')[-1]
-        if len(last_path) < 10:
-          if len(path) < 10:
-            return hostname + '/' + path
-          else:
-            return hostname + '/..' + parsed.path[-10:]
-        elif len(last_path) > 10:
-          return hostname + '/..' + last_path[:5]
-        else:
-          return hostname + '/..' + last_path
-      else:
-        return hostname
+      return request_track.ShortName(self._request.url)
 
     def Url(self):
       return self._request.url
diff --git a/loading/request_track.py b/loading/request_track.py
index 2c083c7..47b874f 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -13,6 +13,7 @@ import copy
 import json
 import logging
 import re
+import urlparse
 
 import devtools_monitor
 
@@ -29,6 +30,24 @@ _TIMING_NAMES_MAPPING = {
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
 
+def ShortName(url):
+  """Returns a shortened version of a URL."""
+  parsed = urlparse.urlparse(url)
+  path = parsed.path
+  hostname = parsed.hostname if parsed.hostname else '?.?.?'
+  if path != '' and path != '/':
+    last_path = parsed.path.split('/')[-1]
+    if len(last_path) < 10:
+      if len(path) < 10:
+        return hostname + '/' + path
+      else:
+        return hostname + '/..' + parsed.path[-10:]
+    else:
+        return hostname + '/..' + last_path[:5]
+  else:
+    return hostname
+
+
 def IntervalBetween(first, second, reason):
   """Returns the start and end of the inteval between two requests, in ms.
 

commit c261f3ca8ef04d92e35abe547f59ccefec612d10
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 30 06:04:47 2016 -0700

    clovis: Remove the deprecated directory.
    
    Review URL: https://codereview.chromium.org/1839373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383958}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7bed61994bd05f0f48d64de84fc4ea23a616c152

diff --git a/loading/deprecated/log_parser.py b/loading/deprecated/log_parser.py
deleted file mode 100644
index a58bd55..0000000
--- a/loading/deprecated/log_parser.py
+++ /dev/null
@@ -1,218 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Parses a JSON request log created by log_requests.py."""
-
-import collections
-import json
-import operator
-import urlparse
-
-Timing = collections.namedtuple(
-    'Timing',
-    ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
-     'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
-     'sslEnd', 'sslStart', 'workerReady', 'workerStart', 'loadingFinished'])
-
-
-class Resource(object):
-  """Describes a resource."""
-
-  def __init__(self, url, content_type):
-    """Creates an instance of Resource.
-
-    Args:
-      url: URL of the resource
-      content_type: Content-Type of the resources.
-    """
-    self.url = url
-    self.content_type = content_type
-
-  def GetShortName(self):
-    """Returns either the hostname of the resource, or the filename,
-    or the end of the path. Tries to include the domain as much as possible.
-    """
-    parsed = urlparse.urlparse(self.url)
-    path = parsed.path
-    if path != '' and path != '/':
-      last_path = parsed.path.split('/')[-1]
-      if len(last_path) < 10:
-        if len(path) < 10:
-          return parsed.hostname + '/' + path
-        else:
-          return parsed.hostname + '/..' + parsed.path[-10:]
-      elif len(last_path) > 10:
-        return parsed.hostname + '/..' + last_path[:5]
-      else:
-        return parsed.hostname + '/..' + last_path
-    else:
-      return parsed.hostname
-
-  def GetContentType(self):
-    mime = self.content_type
-    if 'magic-debug-content' in mime:
-      # A silly hack to make the unittesting easier.
-      return 'magic-debug-content'
-    elif mime == 'text/html':
-      return 'html'
-    elif mime == 'text/css':
-      return 'css'
-    elif mime in ('application/x-javascript', 'text/javascript',
-                  'application/javascript'):
-      return 'script'
-    elif mime == 'application/json':
-      return 'json'
-    elif mime == 'image/gif':
-      return 'gif_image'
-    elif mime.startswith('image/'):
-      return 'image'
-    else:
-      return 'other'
-
-  @classmethod
-  def FromRequest(cls, request):
-    """Creates a Resource from an instance of RequestData."""
-    return Resource(request.url, request.GetContentType())
-
-  def __Fields(self):
-    return (self.url, self.content_type)
-
-  def __eq__(self, o):
-    return  self.__Fields() == o.__Fields()
-
-  def __hash__(self):
-    return hash(self.__Fields())
-
-
-class RequestData(object):
-  """Represents a request, as dumped by log_requests.py."""
-
-  def __init__(self, status, headers, request_headers, timestamp, timing, url,
-               served_from_cache, initiator):
-    self.status = status
-    self.headers = headers
-    self.request_headers = request_headers
-    self.timestamp = timestamp
-    self.timing = Timing(**timing) if timing else None
-    self.url = url
-    self.served_from_cache = served_from_cache
-    self.initiator = initiator
-
-  def IsDataUrl(self):
-    return self.url.startswith('data:')
-
-  def GetContentType(self):
-    content_type = self.headers['Content-Type']
-    if ';' in content_type:
-      return content_type[:content_type.index(';')]
-    else:
-      return content_type
-
-  @classmethod
-  def FromDict(cls, r):
-    """Creates a RequestData object from a dict."""
-    return RequestData(r['status'], r['headers'], r['request_headers'],
-                       r['timestamp'], r['timing'], r['url'],
-                       r['served_from_cache'], r['initiator'])
-
-
-def ParseJsonFile(filename):
-  """Converts a JSON file to a sequence of RequestData."""
-  with open(filename) as f:
-    json_data = json.load(f)
-    return [RequestData.FromDict(r) for r in json_data]
-
-
-def FilterRequests(requests):
-  """Filters a list of requests.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    A list of requests that are not data URL, have a Content-Type, and are
-    not served from the cache.
-  """
-  return [r for r in requests if not r.IsDataUrl()
-          and 'Content-Type' in r.headers and not r.served_from_cache]
-
-
-def ResourceToRequestMap(requests):
-  """Returns a Resource -> Request map.
-
-  A resource can be requested several times in a single page load. Keeps the
-  first request in this case.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    [Resource, ...]
-  """
-  # reversed(requests) because we want the first one to win.
-  return dict([(Resource.FromRequest(r), r) for r in reversed(requests)])
-
-
-def GetResources(requests):
-  """Returns an ordered list of resources from a list of requests.
-
-  The same resource can be requested several time for a single page load. This
-  keeps only the first request.
-
-  Args:
-    requests: [RequestData]
-
-  Returns:
-    [Resource]
-  """
-  resources = []
-  known_resources = set()
-  for r in requests:
-    resource = Resource.FromRequest(r)
-    if r in known_resources:
-      continue
-    known_resources.add(resource)
-    resources.append(resource)
-  return resources
-
-
-def ParseCacheControl(headers):
-  """Parses the "Cache-Control" header and returns a dict representing it.
-
-  Args:
-    headers: (dict) Response headers.
-
-  Returns:
-    {Directive: Value, ...}
-  """
-  # TODO(lizeb): Handle the "Expires" header as well.
-  result = {}
-  cache_control = headers.get('Cache-Control', None)
-  if cache_control is None:
-    return result
-  directives = [s.strip() for s in cache_control.split(',')]
-  for directive in directives:
-    parts = [s.strip() for s in directive.split('=')]
-    if len(parts) == 1:
-      result[parts[0]] = True
-    else:
-      result[parts[0]] = parts[1]
-  return result
-
-
-def MaxAge(request):
-  """Returns the max-age of a resource, or -1."""
-  cache_control = ParseCacheControl(request.headers)
-  if (u'no-store' in cache_control
-      or u'no-cache' in cache_control
-      or len(cache_control) == 0):
-    return -1
-  if 'max-age' in cache_control:
-    return int(cache_control['max-age'])
-  return -1
-
-
-def SortedByCompletion(requests):
-  """Returns the requests, sorted by completion time."""
-  return sorted(requests, key=operator.attrgetter('timestamp'))
diff --git a/loading/deprecated/log_requests.py b/loading/deprecated/log_requests.py
deleted file mode 100755
index 12ebb66..0000000
--- a/loading/deprecated/log_requests.py
+++ /dev/null
@@ -1,239 +0,0 @@
-#! /usr/bin/python
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Loads a URL on an Android device, logging all the requests made to do it
-to a JSON file using DevTools.
-"""
-
-import contextlib
-import httplib
-import json
-import logging
-import optparse
-import os
-import sys
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-import devil_chromium
-
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
-from chrome_telemetry_build import chromium_config
-sys.path.append(chromium_config.GetTelemetryDir())
-from telemetry.internal.backends.chrome_inspector import inspector_websocket
-from telemetry.internal.backends.chrome_inspector import websocket
-
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'chrome_proxy'))
-from common import inspector_network
-
-import device_setup
-
-
-class AndroidRequestsLogger(object):
-  """Logs all the requests made to load a page on a device."""
-
-  def __init__(self, device):
-    """If device is None, we connect to a local chrome session."""
-    self.device = device
-    self._please_stop = False
-    self._main_frame_id = None
-    self._tracing_data = []
-
-  def _PageDataReceived(self, msg):
-    """Called when a Page event is received.
-
-    Records the main frame, and stops the recording once it has finished
-    loading.
-
-    Args:
-      msg: (dict) Message sent by DevTools.
-    """
-    if 'params' not in msg:
-      return
-    params = msg['params']
-    method = msg.get('method', None)
-    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
-      self._main_frame_id = params['frameId']
-    elif (method == 'Page.frameStoppedLoading'
-          and params['frameId'] == self._main_frame_id):
-      self._please_stop = True
-
-  def _TracingDataReceived(self, msg):
-    self._tracing_data.append(msg)
-
-  def _LogPageLoadInternal(self, url, clear_cache):
-    """Returns the collection of requests made to load a given URL.
-
-    Assumes that DevTools is available on http://localhost:DEVTOOLS_PORT.
-
-    Args:
-      url: URL to load.
-      clear_cache: Whether to clear the HTTP cache.
-
-    Returns:
-      [inspector_network.InspectorNetworkResponseData, ...]
-    """
-    self._main_frame_id = None
-    self._please_stop = False
-    r = httplib.HTTPConnection(
-        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      logging.error('Cannot connect to the remote target.')
-      return None
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    inspector = inspector_network.InspectorNetwork(ws)
-    if clear_cache:
-      inspector.ClearCache()
-    ws.SyncRequest({'method': 'Page.enable'})
-    ws.RegisterDomain('Page', self._PageDataReceived)
-    inspector.StartMonitoringNetwork()
-    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
-                              'params': {'url': url}})
-    while not self._please_stop:
-      try:
-        ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException as e:
-        logging.warning('Exception: ' + str(e))
-        break
-    if not self._please_stop:
-      logging.warning('Finished with timeout instead of page load')
-    inspector.StopMonitoringNetwork()
-    return inspector.GetResponseData()
-
-  def _LogTracingInternal(self, url):
-    self._main_frame_id = None
-    self._please_stop = False
-    r = httplib.HTTPConnection('localhost', device_setup.DEVTOOLS_PORT)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      logging.error('Cannot connect to the remote target.')
-      return None
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    ws.RegisterDomain('Tracing', self._TracingDataReceived)
-    logging.warning('Tracing.start: ' +
-                    str(ws.SyncRequest({'method': 'Tracing.start',
-                                        'options': 'zork'})))
-    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
-                              'params': {'url': url}})
-    while not self._please_stop:
-      try:
-        ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException:
-        break
-    if not self._please_stop:
-      logging.warning('Finished with timeout instead of page load')
-    return {'events': self._tracing_data,
-            'end': ws.SyncRequest({'method': 'Tracing.end'})}
-
-
-  def LogPageLoad(self, url, clear_cache, package):
-    """Returns the collection of requests made to load a given URL on a device.
-
-    Args:
-      url: (str) URL to load on the device.
-      clear_cache: (bool) Whether to clear the HTTP cache.
-
-    Returns:
-      See _LogPageLoadInternal().
-    """
-    return device_setup.SetUpAndExecute(
-        self.device, package,
-        lambda: self._LogPageLoadInternal(url, clear_cache))
-
-  def LogTracing(self, url):
-    """Log tracing events from a load of the given URL.
-
-    TODO(mattcary): This doesn't work. It would be best to log tracing
-    simultaneously with network requests, but as that wasn't working the tracing
-    logging was broken out separately. It still doesn't work...
-    """
-    return device_setup.SetUpAndExecute(
-        self.device, 'chrome', lambda: self._LogTracingInternal(url))
-
-
-def _ResponseDataToJson(data):
-  """Converts a list of inspector_network.InspectorNetworkResponseData to JSON.
-
-  Args:
-    data: as returned by _LogPageLoad()
-
-  Returns:
-    A JSON file with the following format:
-    [request1, request2, ...], and a request is:
-    {'status': str, 'headers': dict, 'request_headers': dict,
-     'timestamp': double, 'timing': dict, 'url': str,
-      'served_from_cache': bool, 'initiator': str})
-  """
-  result = []
-  for r in data:
-    result.append({'status': r.status,
-                   'headers': r.headers,
-                   'request_headers': r.request_headers,
-                   'timestamp': r.timestamp,
-                   'timing': r.timing,
-                   'url': r.url,
-                   'served_from_cache': r.served_from_cache,
-                   'initiator': r.initiator})
-  return json.dumps(result)
-
-
-def _CreateOptionParser():
-  """Returns the option parser for this tool."""
-  parser = optparse.OptionParser(description='Starts a browser on an Android '
-                                 'device, gathers the requests made to load a '
-                                 'page and dumps it to a JSON file.')
-  parser.add_option('--url', help='URL to load.',
-                    default='https://www.google.com', metavar='URL')
-  parser.add_option('--output', help='Output file.', default='result.json')
-  parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
-                                              'before loading the URL.'),
-                    default=True, action='store_false', dest='clear_cache')
-  parser.add_option('--package', help='Package info for chrome build. '
-                                      'See build/android/pylib/constants.',
-                    default='chrome')
-  parser.add_option('--local', action='store_true', default=False,
-                    help='Connect to local chrome session rather than android.')
-  return parser
-
-
-def main():
-  logging.basicConfig(level=logging.WARNING)
-  parser = _CreateOptionParser()
-  options, _ = parser.parse_args()
-
-  devil_chromium.Initialize()
-
-  if options.local:
-    device = None
-  else:
-    devices = device_utils.DeviceUtils.HealthyDevices()
-    device = devices[0]
-
-  request_logger = AndroidRequestsLogger(device)
-  response_data = request_logger.LogPageLoad(
-      options.url, options.clear_cache, options.package)
-  json_data = _ResponseDataToJson(response_data)
-  with open(options.output, 'w') as f:
-    f.write(json_data)
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/deprecated/process_request_log.py b/loading/deprecated/process_request_log.py
deleted file mode 100755
index bbec0a8..0000000
--- a/loading/deprecated/process_request_log.py
+++ /dev/null
@@ -1,189 +0,0 @@
-#! /usr/bin/python
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Creates a Graphviz file visualizing the resource dependencies from a JSON
-file dumped by log_requests.py.
-"""
-
-import collections
-import sys
-import urlparse
-
-import log_parser
-from log_parser import Resource
-
-
-def _BuildResourceDependencyGraph(requests):
-  """Builds the graph of resource dependencies.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    A tuple ([Resource], [(resource1, resource2, reason), ...])
-  """
-  resources = log_parser.GetResources(requests)
-  resources_from_url = {resource.url: resource for resource in resources}
-  requests_by_completion = log_parser.SortedByCompletion(requests)
-  deps = []
-  for r in requests:
-    resource = Resource.FromRequest(r)
-    initiator = r.initiator
-    initiator_type = initiator['type']
-    dep = None
-    if initiator_type == 'parser':
-      url = initiator['url']
-      blocking_resource = resources_from_url.get(url, None)
-      if blocking_resource is None:
-        continue
-      dep = (blocking_resource, resource, 'parser')
-    elif initiator_type == 'script' and 'stackTrace' in initiator:
-      for frame in initiator['stackTrace']:
-        url = frame['url']
-        blocking_resource = resources_from_url.get(url, None)
-        if blocking_resource is None:
-          continue
-        dep = (blocking_resource, resource, 'stack')
-        break
-    else:
-      # When the initiator is a script without a stackTrace, infer that it comes
-      # from the most recent script from the same hostname.
-      # TLD+1 might be better, but finding what is a TLD requires a database.
-      request_hostname = urlparse.urlparse(r.url).hostname
-      sorted_script_requests_from_hostname = [
-          r for r in requests_by_completion
-          if (resource.GetContentType() in ('script', 'html', 'json')
-              and urlparse.urlparse(r.url).hostname == request_hostname)]
-      most_recent = None
-      # Linear search is bad, but this shouldn't matter here.
-      for request in sorted_script_requests_from_hostname:
-        if request.timestamp < r.timing.requestTime:
-          most_recent = request
-        else:
-          break
-      if most_recent is not None:
-        blocking = resources_from_url.get(most_recent.url, None)
-        if blocking is not None:
-          dep = (blocking, resource, 'script_inferred')
-    if dep is not None:
-      deps.append(dep)
-  return (resources, deps)
-
-
-def PrefetchableResources(requests):
-  """Returns a list of resources that are discoverable without JS.
-
-  Args:
-    requests: List of requests.
-
-  Returns:
-    List of discoverable resources, with their initial request.
-  """
-  resource_to_request = log_parser.ResourceToRequestMap(requests)
-  (_, all_deps) = _BuildResourceDependencyGraph(requests)
-  # Only keep "parser" arcs
-  deps = [(first, second) for (first, second, reason) in all_deps
-          if reason == 'parser']
-  deps_per_resource = collections.defaultdict(list)
-  for (first, second) in deps:
-    deps_per_resource[first].append(second)
-  result = []
-  visited = set()
-  to_visit = [deps[0][0]]
-  while len(to_visit) != 0:
-    r = to_visit.pop()
-    visited.add(r)
-    to_visit += deps_per_resource[r]
-    result.append(resource_to_request[r])
-  return result
-
-
-_CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
-                          'json': 'purple', 'gif_image': 'grey',
-                          'image': 'orange', 'other': 'white'}
-
-
-def _ResourceGraphvizNode(resource, request, resource_to_index):
-  """Returns the node description for a given resource.
-
-  Args:
-    resource: Resource.
-    request: RequestData associated with the resource.
-    resource_to_index: {Resource: int}.
-
-  Returns:
-    A string describing the resource in graphviz format.
-    The resource is color-coded according to its content type, and its shape is
-    oval if its max-age is less than 300s (or if it's not cacheable).
-  """
-  color = _CONTENT_TYPE_TO_COLOR[resource.GetContentType()]
-  max_age = log_parser.MaxAge(request)
-  shape = 'polygon' if max_age > 300 else 'oval'
-  return ('%d [label = "%s"; style = "filled"; fillcolor = %s; shape = %s];\n'
-          % (resource_to_index[resource], resource.GetShortName(), color,
-             shape))
-
-
-def _GraphvizFileFromDeps(resources, requests, deps, output_filename):
-  """Writes a graphviz file from a set of resource dependencies.
-
-  Args:
-    resources: [Resource, ...]
-    requests: list of requests
-    deps: [(resource1, resource2, reason), ...]
-    output_filename: file to write the graph to.
-  """
-  with open(output_filename, 'w') as f:
-    f.write("""digraph dependencies {
-    rankdir = LR;
-    """)
-    resource_to_request = log_parser.ResourceToRequestMap(requests)
-    resource_to_index = {r: i for (i, r) in enumerate(resources)}
-    resources_with_edges = set()
-    for (first, second, reason) in deps:
-      resources_with_edges.add(first)
-      resources_with_edges.add(second)
-    if len(resources_with_edges) != len(resources):
-      f.write("""subgraph cluster_orphans {
-  color=black;
-  label="Orphans";
-""")
-      for resource in resources:
-        if resource not in resources_with_edges:
-          request = resource_to_request[resource]
-          f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
-      f.write('}\n')
-
-    f.write("""subgraph cluster_nodes {
-  color=invis;
-""")
-    for resource in resources:
-      request = resource_to_request[resource]
-      print resource.url
-      if resource in resources_with_edges:
-        f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
-    for (first, second, reason) in deps:
-      arrow = ''
-      if reason == 'parser':
-        arrow = '[color = red]'
-      elif reason == 'stack':
-        arrow = '[color = blue]'
-      elif reason == 'script_inferred':
-        arrow = '[color = blue; style=dotted]'
-      f.write('%d -> %d %s;\n' % (
-          resource_to_index[first], resource_to_index[second], arrow))
-    f.write('}\n}\n')
-
-
-def main():
-  filename = sys.argv[1]
-  requests = log_parser.ParseJsonFile(filename)
-  requests = log_parser.FilterRequests(requests)
-  (resources, deps) = _BuildResourceDependencyGraph(requests)
-  _GraphvizFileFromDeps(resources, requests, deps, filename + '.dot')
-
-
-if __name__ == '__main__':
-  main()

commit 2385e6fe21b15547360ba6ec24c42ed9a4182462
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 29 07:39:00 2016 -0700

    Clovis: update resource sack to use new dependency graph.
    
    This is a refactoring with (hopefully) no functional change.
    
    Review URL: https://codereview.chromium.org/1837193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383719}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6341256079ecdb84a47165c1fda47ed308ccb87e

diff --git a/loading/core_set.py b/loading/core_set.py
index 8ab9493..3bbb66f 100644
--- a/loading/core_set.py
+++ b/loading/core_set.py
@@ -15,8 +15,9 @@ import multiprocessing
 import os
 import sys
 
-import loading_model
+import dependency_graph
 import loading_trace
+import request_dependencies_lens
 import resource_sack
 
 
@@ -34,8 +35,10 @@ def _PageCore(prefix, graph_set_names, output):
     _Progress('Processing %s' % name)
     for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
       _Progress('Reading %s' % filename)
-      graph = loading_model.ResourceGraph(
-          loading_trace.LoadingTrace.FromJsonFile(filename))
+      trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+      graph = dependency_graph.RequestDependencyGraph(
+          trace.request_track.GetEvents(),
+          request_dependencies_lens.RequestDependencyLens(trace))
       sack.ConsumeGraph(graph)
       name_graphs.append(graph)
     graph_sets.append(name_graphs)
@@ -90,8 +93,10 @@ def _AllCores(prefix, graph_set_names, output, threshold):
     this_set = []
     for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
       _Progress('Reading %s' % filename)
-      graph = loading_model.ResourceGraph(
-          loading_trace.LoadingTrace.FromJsonDict(json.load(open(filename))))
+      trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+      graph = dependency_graph.RequestDependencyGraph(
+          trace.request_track.GetEvents(),
+          request_dependencies_lens.RequestDependencyLens(trace))
       sack.ConsumeGraph(graph)
       big_sack.ConsumeGraph(graph)
       this_set.append(graph)
diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index 23285cc..be2c5e0 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -46,6 +46,11 @@ class RequestDependencyGraph(object):
     self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
     self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
 
+  @property
+  def graph(self):
+    """Return the Graph we're based on."""
+    return self._deps_graph
+
   def UpdateRequestsCost(self, request_id_to_cost):
     """Updates the cost of the nodes identified by their request ID.
 
diff --git a/loading/graph.py b/loading/graph.py
index a665439..789d917 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -43,15 +43,15 @@ class DirectedGraph(object):
     Note that the edges referencing a node not in the provided list are dropped.
 
     Args:
-      nodes: ([Node]) List of nodes.
-      edges: ([Edge]) List of Edges.
+      nodes: ([Node]) Sequence of Nodes.
+      edges: ([Edge]) Sequence of Edges.
     """
-    assert all(isinstance(node, Node) for node in nodes)
-    assert all(isinstance(edge, Edge) for edge in edges)
     self._nodes = set(nodes)
     self._edges = set(filter(
         lambda e: e.from_node in self._nodes and e.to_node in self._nodes,
         edges))
+    assert all(isinstance(node, Node) for node in self._nodes)
+    assert all(isinstance(edge, Edge) for edge in self._edges)
     self._in_edges = {n: [] for n in self._nodes}
     self._out_edges = {n: [] for n in self._nodes}
     for edge in self._edges:
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 90ced38..563bb77 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -17,16 +17,16 @@ import urlparse
 from collections import defaultdict
 
 import content_classification_lens
-import dag
+import graph
 import user_satisfied_lens
 
 class GraphSack(object):
-  """Aggreate of ResourceGraphs.
+  """Aggreate of RequestDependencyGraphs.
 
-  Collects ResourceGraph nodes into bags, where each bag contains the nodes with
-  common urls. Dependency edges are tracked between bags (so that each bag may
-  be considered as a node of a graph). This graph of bags is referred to as a
-  sack.
+  Collects RequestDependencyGraph nodes into bags, where each bag contains the
+  nodes with common urls. Dependency edges are tracked between bags (so that
+  each bag may be considered as a node of a graph). This graph of bags is
+  referred to as a sack.
 
   Each bag is associated with a dag.Node, even though the bag graph may not be a
   DAG. The edges are annotated with list of graphs and nodes that generated
@@ -37,55 +37,56 @@ class GraphSack(object):
 
   _GraphInfo = collections.namedtuple('_GraphInfo', (
       'cost',   # The graph cost (aka critical path length).
-      'total_costs',  # A vector by node index of total cost of each node.
       ))
 
   def __init__(self):
-    # A bag is a node in our combined graph.
-    self._bags = []
-    # Each bag in our sack corresponds to a url, as expressed by this map.
-    self._url_to_bag = {}
+    # Each bag in our sack is named as indicated by this map.
+    self._name_to_bag = {}
+    # List our edges by bag pairs: (from_bag, to_bag) -> graph.Edge.
+    self._edges = {}
     # Maps graph -> _GraphInfo structures for each graph we've consumed.
     self._graph_info = {}
 
-  def ConsumeGraph(self, graph):
+    # Our graph, updated after each ConsumeGraph.
+    self._graph = None
+
+  def ConsumeGraph(self, request_graph):
     """Add a graph and process.
 
     Args:
-      graph: (ResourceGraph) the graph to add. The graph is processed sorted
-        according to its current filter.
+      graph: (RequestDependencyGraph) the graph to add.
     """
     assert graph not in self._graph_info
-    critical_path = []
-    total_costs = []
-    cost = graph.Cost(path_list=critical_path,
-                      costs_out=total_costs)
-    self._graph_info[graph] = self._GraphInfo(
-        cost=cost, total_costs=total_costs)
-    for n in graph.Nodes(sort=True):
-      assert graph._node_filter(n.Node())
-      self.AddNode(graph, n)
-    for node in critical_path:
-      self._url_to_bag[node.Url()].MarkCritical()
-
-  def AddNode(self, graph, node):
+    cost = request_graph.Cost()
+    self._graph_info[request_graph] = self._GraphInfo(cost=cost)
+    for n in request_graph.graph.Nodes():
+      self.AddNode(request_graph, n)
+
+    # TODO(mattcary): this is inefficient but our current API doesn't require an
+    # explicit graph creation from the client.
+    self._graph = graph.DirectedGraph(self.bags, self._edges.itervalues())
+
+  def AddNode(self, request_graph, node):
     """Add a node to our collection.
 
     Args:
-      graph: (ResourceGraph) the graph in which the node lives.
-      node: (NodeInfo) the node to add.
+      graph: (RequestDependencyGraph) the graph in which the node lives.
+      node: (RequestDependencyGraph node) the node to add.
 
     Returns:
       The Bag containing the node.
     """
-    if not graph._node_filter(node):
-      return
-    if node.Url() not in self._url_to_bag:
-      new_index = len(self._bags)
-      self._bags.append(Bag(self, new_index, node.Url()))
-      self._url_to_bag[node.Url()] = self._bags[-1]
-    self._url_to_bag[node.Url()].AddNode(graph, node)
-    return self._url_to_bag[node.Url()]
+    sack_name = self._GetSackName(node)
+    if sack_name not in self._name_to_bag:
+      self._name_to_bag[sack_name] = Bag(self, sack_name)
+    bag = self._name_to_bag[sack_name]
+    bag.AddNode(request_graph, node)
+    return bag
+
+  def AddEdge(self, from_bag, to_bag):
+    """Add an edge between two bags."""
+    if (from_bag, to_bag) not in self._edges:
+      self._edges[(from_bag, to_bag)] = graph.Edge(from_bag, to_bag)
 
   def CoreSet(self, *graph_sets):
     """Compute the core set of this sack.
@@ -141,7 +142,7 @@ class GraphSack(object):
 
   @property
   def bags(self):
-    return self._bags
+    return self._name_to_bag.values()
 
   def _SingleCore(self, graph_set):
     core = set()
@@ -153,31 +154,31 @@ class GraphSack(object):
         core.add(b.label)
     return core
 
+  def _GetSackName(self, node):
+    return self._MakeShortname(node.request.url)
+
+  @classmethod
+  def _MakeShortname(cls, url):
+    # TODO(lizeb): Move this method to a convenient common location.
+    parsed = urlparse.urlparse(url)
+    if parsed.scheme == 'data':
+      if ';' in parsed.path:
+        kind, _ = parsed.path.split(';', 1)
+      else:
+        kind, _ = parsed.path.split(',', 1)
+      return 'data:' + kind
+    path = parsed.path[:10]
+    hostname = parsed.hostname if parsed.hostname else '?.?.?'
+    return hostname + '/' + path
+
 
-class Bag(dag.Node):
-  def __init__(self, sack, index, url):
-    super(Bag, self).__init__(index)
+class Bag(graph.Node):
+  def __init__(self, sack, label):
+    super(Bag, self).__init__()
     self._sack = sack
-    self._url = url
-    self._label = self._MakeShortname(url)
+    self._label = label
     # Maps a ResourceGraph to its Nodes contained in this Bag.
     self._graphs = defaultdict(set)
-    # Maps each successor bag to the set of (graph, node, graph-successor)
-    # tuples that generated it.
-    self._successor_sources = defaultdict(set)
-    # Maps each successor bag to a set of edge costs. This is just used to
-    # track min and max; if we want more statistics we'd have to count the
-    # costs with multiplicity.
-    self._successor_edge_costs = defaultdict(set)
-
-    # Miscellaneous counts and costs used in display.
-    self._total_costs = []
-    self._relative_costs = []
-    self._num_critical = 0
-
-  @property
-  def url(self):
-    return self._url
 
   @property
   def label(self):
@@ -185,62 +186,16 @@ class Bag(dag.Node):
 
   @property
   def graphs(self):
-    return self._graphs
-
-  @property
-  def successor_sources(self):
-    return self._successor_sources
-
-  @property
-  def successor_edge_costs(self):
-    return self._successor_edge_costs
-
-  @property
-  def total_costs(self):
-    return self._total_costs
-
-  @property
-  def relative_costs(self):
-    return self._relative_costs
-
-  @property
-  def num_critical(self):
-    return self._num_critical
+    return self._graphs.iterkeys()
 
   @property
   def num_nodes(self):
-    return len(self._total_costs)
-
-  def MarkCritical(self):
-    self._num_critical += 1
+    return sum(len(g) for g in self._graphs.itervalues())
 
-  def AddNode(self, graph, node):
-    if node in self._graphs[graph]:
+  def AddNode(self, request_graph, node):
+    if node in self._graphs[request_graph]:
       return  # Already added.
-    graph_info = self._sack.graph_info[graph]
-    self._graphs[graph].add(node)
-    node_total_cost = graph_info.total_costs[node.Index()]
-    self._total_costs.append(node_total_cost)
-    self._relative_costs.append(
-        float(node_total_cost) / graph_info.cost)
-    for s in node.Node().Successors():
-      if not graph._node_filter(s):
-        continue
-      node_info = graph.NodeInfo(s)
-      successor_bag = self._sack.AddNode(graph, node_info)
-      self.AddSuccessor(successor_bag)
-      self._successor_sources[successor_bag].add((graph, node, s))
-      self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
-
-  @classmethod
-  def _MakeShortname(cls, url):
-    parsed = urlparse.urlparse(url)
-    if parsed.scheme == 'data':
-      if ';' in parsed.path:
-        kind, _ = parsed.path.split(';', 1)
-      else:
-        kind, _ = parsed.path.split(',', 1)
-      return 'data:' + kind
-    path = parsed.path[:10]
-    hostname = parsed.hostname if parsed.hostname else '?.?.?'
-    return hostname + '/' + path
+    self._graphs[request_graph].add(node)
+    for edge in request_graph.graph.OutEdges(node):
+      out_bag = self._sack.AddNode(request_graph, edge.to_node)
+      self._sack.AddEdge(self, out_bag)
diff --git a/loading/resource_sack_display.py b/loading/resource_sack_display.py
deleted file mode 100644
index 210f4da..0000000
--- a/loading/resource_sack_display.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities for displaying a ResourceSack.
-
-When run standalone, takes traces on the command line and produces a dot file to
-stdout.
-"""
-
-
-def ToDot(sack, output, prune=-1, long_edge_msec=2000):
-  """Output as a dot file.
-
-  Args:
-    sack: (ResourceSack) the sack to convert to dot.
-    output: a file-like output stream.
-    prune: if positive, prune & coalesce nodes under the specified threshold
-      of repeated views, as fraction node views / total graphs. All pruned
-      nodes are represented by a single node, and an edge is connected only if
-      the view count is greater than 1.
-    long_edge_msec: if positive, the definition of a long edge. Long edges are
-      distinguished in graph.
-  """
-  output.write("""digraph dependencies {
-  rankdir = LR;
-  """)
-
-  pruned = set()
-  num_graphs = len(sack.graph_info)
-  for bag in sack.bags:
-    if prune > 0 and float(len(bag.graphs)) / num_graphs < prune:
-      pruned.add(bag)
-      continue
-    output.write('%d [label="%s (%d)\n(%d, %d)\n(%.2f, %.2f)" shape=%s; '
-                 'style=filled; fillcolor=%s];\n' % (
-        bag.Index(), bag.label, len(bag.graphs),
-        min(bag.total_costs), max(bag.total_costs),
-        min(bag.relative_costs), max(bag.relative_costs),
-        _CriticalToShape(bag),
-        _AmountToNodeColor(len(bag.graphs), num_graphs)))
-
-  if pruned:
-    pruned_index = num_graphs
-    output.write('%d [label="Pruned at %.0f%%\n(%d)"; '
-                 'shape=polygon; style=dotted];\n' %
-                 (pruned_index, 100 * prune, len(pruned)))
-
-  for bag in sack.bags:
-    if bag in pruned:
-      for succ in bag.Successors():
-        if succ not in pruned:
-          output.write('%d -> %d [style=dashed];\n' % (
-              pruned_index, succ.Index()))
-    for succ in bag.Successors():
-      if succ in pruned:
-        if len(bag.successor_sources[succ]) > 1:
-          output.write('%d -> %d [label="%d"; style=dashed];\n' % (
-              bag.Index(), pruned_index, len(bag.successor_sources[succ])))
-      else:
-        num_succ = len(bag.successor_sources[succ])
-        num_long = 0
-        for graph, source, target in bag.successor_sources[succ]:
-          if graph.EdgeCost(source, target) > long_edge_msec:
-            num_long += 1
-        if num_long > 0:
-          long_frac = float(num_long) / num_succ
-          long_edge_style = '; penwidth=%f' % (2 + 6.0 * long_frac)
-          if long_frac < 0.75:
-            long_edge_style += '; style=dashed'
-        else:
-          long_edge_style = ''
-        min_edge = min(bag.successor_edge_costs[succ])
-        max_edge = max(bag.successor_edge_costs[succ])
-        output.write('%d -> %d [label="%d\n(%f,%f)"; color=%s %s];\n' % (
-            bag.Index(), succ.Index(), num_succ, min_edge, max_edge,
-            _AmountToEdgeColor(num_succ, len(bag.graphs)),
-            long_edge_style))
-
-  output.write('}')
-
-
-def _CriticalToShape(bag):
-  frac = float(bag.num_critical) / bag.num_nodes
-  if frac < 0.4:
-    return 'oval'
-  elif frac < 0.7:
-    return 'polygon'
-  elif frac < 0.9:
-    return 'trapezium'
-  return 'box'
-
-
-def _AmountToNodeColor(numer, denom):
-  if denom <= 0:
-    return 'grey72'
-  ratio = 1.0 * numer / denom
-  if ratio < .3:
-    return 'white'
-  elif ratio < .6:
-    return 'yellow'
-  elif ratio < .8:
-    return 'orange'
-  return 'green'
-
-
-def _AmountToEdgeColor(numer, denom):
-  color = _AmountToNodeColor(numer, denom)
-  if color == 'white' or color == 'grey72':
-    return 'black'
-  return color
-
-
-def _Main():
-  import json
-  import logging
-  import sys
-
-  import loading_model
-  import loading_trace
-  import resource_sack
-
-  sack = resource_sack.GraphSack()
-  for fname in sys.argv[1:]:
-    trace = loading_trace.LoadingTrace.FromJsonDict(
-      json.load(open(fname)))
-    logging.info('Making graph from %s', fname)
-    model = loading_model.ResourceGraph(trace, content_lens=None)
-    sack.ConsumeGraph(model)
-    logging.info('Finished %s', fname)
-  ToDot(sack, sys.stdout, prune=.1)
-
-
-if __name__ == '__main__':
-  _Main()
diff --git a/loading/resource_sack_display_unittest.py b/loading/resource_sack_display_unittest.py
deleted file mode 100644
index 65b53ff..0000000
--- a/loading/resource_sack_display_unittest.py
+++ /dev/null
@@ -1,42 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import re
-from StringIO import StringIO
-import unittest
-
-import resource_sack
-import resource_sack_display
-from test_utils import (MakeRequest,
-                        TestResourceGraph)
-
-
-class ResourceSackDispayTestCase(unittest.TestCase):
-  def test_SimpleOutput(self):
-    g1 = TestResourceGraph.FromRequestList([
-        MakeRequest(0, 'null'),
-        MakeRequest(1, 0),
-        MakeRequest(2, 0),
-        MakeRequest(3, 1)])
-    g2 = TestResourceGraph.FromRequestList([
-        MakeRequest(0, 'null'),
-        MakeRequest(1, 0),
-        MakeRequest(2, 0),
-        MakeRequest(4, 2)])
-    sack = resource_sack.GraphSack()
-    sack.ConsumeGraph(g1)
-    sack.ConsumeGraph(g2)
-    buf = StringIO()
-    resource_sack_display.ToDot(sack, buf,
-                                long_edge_msec=1000)
-    dot = buf.getvalue()
-    # Short edge.
-    self.assertTrue(re.search(r'0 -> 1[^]]+color=green \]', dot, re.MULTILINE))
-    # Long edge.
-    self.assertTrue(re.search(r'0 -> 3[^]]+penwidth=8', dot))
-
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index a4b1a30..5c3e54d 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -6,7 +6,7 @@ import unittest
 
 import resource_sack
 from test_utils import (MakeRequest,
-                        TestResourceGraph)
+                        TestDependencyGraph)
 
 
 class ResourceSackTestCase(unittest.TestCase):
@@ -15,15 +15,15 @@ class ResourceSackTestCase(unittest.TestCase):
     requests = [MakeRequest(node_names[0], 'null')]
     for n in node_names[1:]:
       requests.append(MakeRequest(n, node_names[0]))
-    return TestResourceGraph.FromRequestList(requests)
+    return TestDependencyGraph(requests)
 
   def test_NodeMerge(self):
-    g1 = TestResourceGraph.FromRequestList([
+    g1 = TestDependencyGraph([
         MakeRequest(0, 'null'),
         MakeRequest(1, 0),
         MakeRequest(2, 0),
         MakeRequest(3, 1)])
-    g2 = TestResourceGraph.FromRequestList([
+    g2 = TestDependencyGraph([
         MakeRequest(0, 'null'),
         MakeRequest(1, 0),
         MakeRequest(2, 0),
@@ -39,10 +39,10 @@ class ResourceSackTestCase(unittest.TestCase):
         self.assertEqual(1, bag.num_nodes)
 
   def test_MultiParents(self):
-    g1 = TestResourceGraph.FromRequestList([
+    g1 = TestDependencyGraph([
         MakeRequest(0, 'null'),
         MakeRequest(2, 0)])
-    g2 = TestResourceGraph.FromRequestList([
+    g2 = TestDependencyGraph([
         MakeRequest(1, 'null'),
         MakeRequest(2, 1)])
     sack = resource_sack.GraphSack()
@@ -50,17 +50,21 @@ class ResourceSackTestCase(unittest.TestCase):
     sack.ConsumeGraph(g2)
     self.assertEqual(3, len(sack.bags))
     labels = {bag.label: bag for bag in sack.bags}
+    def Predecessors(label):
+      bag = labels['%s/' % label]
+      return [e.from_node
+              for e in bag._sack._graph.InEdges(bag)]
     self.assertEqual(
         set(['0/', '1/']),
-        set([bag.label for bag in labels['2/'].Predecessors()]))
-    self.assertFalse(labels['0/'].Predecessors())
-    self.assertFalse(labels['1/'].Predecessors())
+        set([bag.label for bag in Predecessors(2)]))
+    self.assertFalse(Predecessors(0))
+    self.assertFalse(Predecessors(1))
 
   def test_Shortname(self):
     root = MakeRequest(0, 'null')
     shortname = MakeRequest(1, 0)
     shortname.url = 'data:fake/content;' + 'lotsand' * 50 + 'lotsofdata'
-    g1 = TestResourceGraph.FromRequestList([root, shortname])
+    g1 = TestDependencyGraph([root, shortname])
     sack = resource_sack.GraphSack()
     sack.ConsumeGraph(g1)
     self.assertEqual(set(['0/', 'data:fake/content']),
diff --git a/loading/test_utils.py b/loading/test_utils.py
index a221385..8e07826 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -4,6 +4,7 @@
 
 """Common utilities used in unit tests, within this directory."""
 
+import dependency_graph
 import devtools_monitor
 import loading_model
 import loading_trace
@@ -166,6 +167,13 @@ class TestResourceGraph(loading_model.ResourceGraph):
     return cls(LoadingTraceFromEvents(requests, page_events, trace_events))
 
 
+class TestDependencyGraph(dependency_graph.RequestDependencyGraph):
+  """A dependency graph created from requests using a simple lens."""
+  def __init__(self, requests):
+    lens = SimpleLens(LoadingTraceFromEvents(requests))
+    super(TestDependencyGraph, self).__init__(requests, lens)
+
+
 class MockConnection(object):
   """Mock out connection for testing.
 

commit 88ea38781b6ec94423c9ee7fee1885b704436683
Author: lizeb <lizeb@chromium.org>
Date:   Fri Mar 25 11:33:56 2016 -0700

    clovis: Estimate cost of a load, until a given point.
    
    Review URL: https://codereview.chromium.org/1827813005
    
    Cr-Original-Commit-Position: refs/heads/master@{#383318}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4a5dd80de434040fa64d2d2d4b35474f8c9f5f70

diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
new file mode 100644
index 0000000..23285cc
--- /dev/null
+++ b/loading/dependency_graph.py
@@ -0,0 +1,72 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Request dependency graph."""
+
+import graph
+import request_track
+
+
+class _RequestNode(graph.Node):
+  def __init__(self, request):
+    super(_RequestNode, self).__init__()
+    self.request = request
+    self.cost = request.Cost()
+
+
+class _Edge(graph.Edge):
+  def __init__(self, from_node, to_node, reason):
+    super(_Edge, self).__init__(from_node, to_node)
+    self.reason = reason
+    self.cost = request_track.TimeBetween(
+        self.from_node.request, self.to_node.request, self.reason)
+
+
+class RequestDependencyGraph(object):
+  """Request dependency graph."""
+  def __init__(self, requests, dependencies_lens):
+    """Creates a request dependency graph.
+
+    Args:
+      requests: ([Request]) a list of requests.
+      dependencies_lens: (RequestDependencyLens)
+    """
+    self._requests = requests
+    deps = dependencies_lens.GetRequestDependencies()
+    self._nodes_by_id = {r.request_id : _RequestNode(r) for r in self._requests}
+    edges = []
+    for (parent_request, child_request, reason) in deps:
+      if (parent_request.request_id not in self._nodes_by_id
+          or child_request.request_id not in self._nodes_by_id):
+        continue
+      parent_node = self._nodes_by_id[parent_request.request_id]
+      child_node = self._nodes_by_id[child_request.request_id]
+      edges.append(_Edge(parent_node, child_node, reason))
+    self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
+    self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
+
+  def UpdateRequestsCost(self, request_id_to_cost):
+    """Updates the cost of the nodes identified by their request ID.
+
+    Args:
+      request_id_to_cost: {request_id: new_cost} Can be a superset of the
+                          requests actually present in the graph.
+
+    """
+    for node in self._deps_graph.Nodes():
+      request_id = node.request.request_id
+      if request_id in request_id_to_cost:
+        node.cost = request_id_to_cost[request_id]
+
+  def Cost(self, from_first_request=True):
+    """Returns the cost of the graph, that is the costliest path.
+
+    Args:
+      from_first_request: (boolean) If True, only considers paths that originate
+                          from the first request node.
+    """
+    if from_first_request:
+      return self._deps_graph.Cost([self._first_request_node])
+    else:
+      return self._deps_graph.Cost()
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
new file mode 100644
index 0000000..448a234
--- /dev/null
+++ b/loading/dependency_graph_unittest.py
@@ -0,0 +1,54 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import dependency_graph
+import request_dependencies_lens
+from request_dependencies_lens_unittest import TestRequests
+import request_track
+
+
+class RequestDependencyGraphTestCase(unittest.TestCase):
+  def setUp(self):
+    super(RequestDependencyGraphTestCase, self).setUp()
+    self.trace = TestRequests.CreateLoadingTrace()
+
+  def testUpdateRequestCost(self):
+    requests = self.trace.request_track.GetEvents()
+    requests[0].timing = request_track.TimingFromDict(
+        {'requestTime': 12, 'loadingFinished': 10})
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        self.trace)
+    g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    self.assertEqual(10, g.Cost())
+    request_id = requests[0].request_id
+    g.UpdateRequestsCost({request_id: 100})
+    self.assertEqual(100, g.Cost())
+    g.UpdateRequestsCost({'unrelated_id': 1000})
+    self.assertEqual(100, g.Cost())
+
+  def testCost(self):
+    requests = self.trace.request_track.GetEvents()
+    for (index, request) in enumerate(requests):
+      request.timing = request_track.TimingFromDict(
+          {'requestTime': index, 'receiveHeadersEnd': 10,
+           'loadingFinished': 10})
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        self.trace)
+    g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    # First redirect -> Second redirect -> Redirected Request -> Request ->
+    # JS Request 2
+    self.assertEqual(7010, g.Cost())
+    # Not on the critical path
+    g.UpdateRequestsCost({TestRequests.JS_REQUEST.request_id: 0})
+    self.assertEqual(7010, g.Cost())
+    g.UpdateRequestsCost({TestRequests.FIRST_REDIRECT_REQUEST.request_id: 0})
+    self.assertEqual(7000, g.Cost())
+    g.UpdateRequestsCost({TestRequests.SECOND_REDIRECT_REQUEST.request_id: 0})
+    self.assertEqual(6990, g.Cost())
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/graph.py b/loading/graph.py
new file mode 100644
index 0000000..a665439
--- /dev/null
+++ b/loading/graph.py
@@ -0,0 +1,175 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Support for graphs."""
+
+import collections
+
+
+class Node(object):
+  """A node in a Graph.
+
+  Nodes are identified within a graph using object identity.
+  """
+  def __init__(self):
+    """Create a new node."""
+    self.cost = 0
+
+
+class Edge(object):
+  """Represents an edge in a graph."""
+  def __init__(self, from_node, to_node):
+    """Creates an Edge.
+
+    Args:
+      from_node: (Node) Start node.
+      to_node: (Node) End node.
+    """
+    self.from_node = from_node
+    self.to_node = to_node
+    self.cost = 0
+
+
+class DirectedGraph(object):
+  """Directed graph.
+
+  A graph is identified by a list of nodes and a list of edges. It does not need
+  to be acyclic, but then some methods will fail.
+  """
+  def __init__(self, nodes, edges):
+    """Builds a graph from a set of node and edges.
+
+    Note that the edges referencing a node not in the provided list are dropped.
+
+    Args:
+      nodes: ([Node]) List of nodes.
+      edges: ([Edge]) List of Edges.
+    """
+    assert all(isinstance(node, Node) for node in nodes)
+    assert all(isinstance(edge, Edge) for edge in edges)
+    self._nodes = set(nodes)
+    self._edges = set(filter(
+        lambda e: e.from_node in self._nodes and e.to_node in self._nodes,
+        edges))
+    self._in_edges = {n: [] for n in self._nodes}
+    self._out_edges = {n: [] for n in self._nodes}
+    for edge in self._edges:
+      self._out_edges[edge.from_node].append(edge)
+      self._in_edges[edge.to_node].append(edge)
+
+  def OutEdges(self, node):
+    """Returns a list of edges starting from a node.
+    """
+    return self._out_edges[node]
+
+  def InEdges(self, node):
+    """Returns a list of edges ending at a node."""
+    return self._in_edges[node]
+
+  def Nodes(self):
+    """Returns the set of nodes of this graph."""
+    return self._nodes
+
+  def Edges(self):
+    """Returns the set of edges of this graph."""
+    return self._edges
+
+  def UpdateEdge(self, edge, new_from_node, new_to_node):
+    """Updates an edge.
+
+    Args:
+      edge:
+      new_from_node:
+      new_to_node:
+    """
+    assert edge in self._edges
+    assert new_from_node in self._nodes
+    assert new_to_node in self._nodes
+    self._in_edges[edge.to_node].remove(edge)
+    self._out_edges[edge.from_node].remove(edge)
+    edge.from_node = new_from_node
+    edge.to_node = new_to_node
+    # TODO(lizeb): Check for duplicate edges?
+    self._in_edges[edge.to_node].append(edge)
+    self._out_edges[edge.from_node].append(edge)
+
+  def TopologicalSort(self, roots=None):
+    """Returns a list of nodes, in topological order.
+
+      Args:
+        roots: ([Node]) If set, the topological sort will only consider nodes
+                        reachable from this list of sources.
+    """
+    sorted_nodes = []
+    if roots is None:
+      nodes_subset = self._nodes
+    else:
+      nodes_subset = self.ReachableNodes(roots)
+    remaining_in_edges = {n: 0 for n in nodes_subset}
+    for edge in self._edges:
+      if edge.from_node in nodes_subset and edge.to_node in nodes_subset:
+        remaining_in_edges[edge.to_node] += 1
+    sources = [node for (node, count) in remaining_in_edges.items()
+               if count == 0]
+    while sources:
+      node = sources.pop(0)
+      sorted_nodes.append(node)
+      for e in self.OutEdges(node):
+        successor = e.to_node
+        if successor not in nodes_subset:
+          continue
+        assert remaining_in_edges[successor] > 0
+        remaining_in_edges[successor] -= 1
+        if remaining_in_edges[successor] == 0:
+          sources.append(successor)
+    return sorted_nodes
+
+  def ReachableNodes(self, roots):
+    """Returns a list of nodes from a set of root nodes."""
+    visited = set()
+    fifo = collections.deque(roots)
+    while len(fifo) != 0:
+      node = fifo.pop()
+      visited.add(node)
+      for e in self.OutEdges(node):
+        if e.to_node not in visited:
+          visited.add(e.to_node)
+        fifo.appendleft(e.to_node)
+    return list(visited)
+
+  def Cost(self, roots=None, path_list=None, costs_out=None):
+    """Compute the cost of the graph.
+
+    Args:
+      roots: ([Node]) If set, only compute the cost of the paths reachable
+             from this list of nodes.
+      path_list: if not None, gets a list of nodes in the longest path.
+      costs_out: if not None, gets a vector of node costs by node.
+
+    Returns:
+      Cost of the longest path.
+    """
+    costs = {n: 0 for n in self._nodes}
+    for node in self.TopologicalSort(roots):
+      cost = 0
+      if self.InEdges(node):
+        cost = max([costs[e.from_node] + e.cost for e in self.InEdges(node)])
+      costs[node] = cost + node.cost
+    max_cost = max(costs.values())
+    if costs_out is not None:
+      del costs_out[:]
+      costs_out.extend(costs)
+    assert max_cost > 0
+    if path_list is not None:
+      del path_list[:]
+      node = (i for i in self._nodes if costs[i] == max_cost).next()
+      path_list.append(node)
+      while self.InEdges(node):
+        predecessors = [e.from_node for e in self.InEdges(node)]
+        node = reduce(
+            lambda costliest_node, next_node:
+            next_node if costs[next_node] > costs[costliest_node]
+            else costliest_node, predecessors)
+        path_list.insert(0, node)
+    return max_cost
diff --git a/loading/graph_unittest.py b/loading/graph_unittest.py
new file mode 100644
index 0000000..e8ef761
--- /dev/null
+++ b/loading/graph_unittest.py
@@ -0,0 +1,170 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import operator
+import os
+import sys
+import unittest
+
+import graph
+
+
+class _IndexedNode(graph.Node):
+  def __init__(self, index):
+    super(_IndexedNode, self).__init__()
+    self.index = index
+
+
+class GraphTestCase(unittest.TestCase):
+  @classmethod
+  def MakeGraph(cls, count, edge_tuples):
+    """Makes a graph from a list of edges.
+
+    Args:
+      count: Number of nodes.
+      edge_tuples: (from_index, to_index). Both indices must be in [0, count),
+                   and uniquely identify a node.
+    """
+    nodes = [_IndexedNode(i) for i in xrange(count)]
+    edges = [graph.Edge(nodes[from_index], nodes[to_index])
+             for (from_index, to_index) in edge_tuples]
+    return (nodes, edges, graph.DirectedGraph(nodes, edges))
+
+  @classmethod
+  def _NodesIndices(cls, g):
+    return map(operator.attrgetter('index'), g.Nodes())
+
+  def testBuildGraph(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
+    self.assertSetEqual(set(edges), set(g.Edges()))
+
+    self.assertSetEqual(set([edges[0], edges[1]]), set(g.OutEdges(nodes[0])))
+    self.assertFalse(g.InEdges(nodes[0]))
+    self.assertSetEqual(set([edges[2]]), set(g.OutEdges(nodes[1])))
+    self.assertSetEqual(set([edges[0]]), set(g.InEdges(nodes[1])))
+    self.assertFalse(g.OutEdges(nodes[2]))
+    self.assertSetEqual(set([edges[1]]), set(g.InEdges(nodes[2])))
+    self.assertSetEqual(set([edges[3]]), set(g.OutEdges(nodes[3])))
+    self.assertSetEqual(set([edges[2]]), set(g.InEdges(nodes[3])))
+    self.assertFalse(g.OutEdges(nodes[4]))
+    self.assertSetEqual(set([edges[3]]), set(g.InEdges(nodes[4])))
+    self.assertSetEqual(set([edges[4]]), set(g.OutEdges(nodes[5])))
+    self.assertFalse(g.InEdges(nodes[5]))
+    self.assertFalse(g.OutEdges(nodes[6]))
+    self.assertSetEqual(set([edges[4]]), set(g.InEdges(nodes[6])))
+
+  def testIgnoresUnknownEdges(self):
+    nodes = [_IndexedNode(i) for i in xrange(7)]
+    edges = [graph.Edge(nodes[from_index], nodes[to_index])
+             for (from_index, to_index) in [
+                 (0, 1), (0, 2), (1, 3), (3, 4), (5, 6)]]
+    edges.append(graph.Edge(nodes[4], _IndexedNode(42)))
+    edges.append(graph.Edge(_IndexedNode(42), nodes[5]))
+    g = graph.DirectedGraph(nodes, edges)
+    self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
+    self.assertEqual(5, len(g.Edges()))
+
+  def testUpdateEdge(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    edge = edges[1]
+    self.assertTrue(edge in g.OutEdges(nodes[0]))
+    self.assertTrue(edge in g.InEdges(nodes[2]))
+    g.UpdateEdge(edge, nodes[2], nodes[3])
+    self.assertFalse(edge in g.OutEdges(nodes[0]))
+    self.assertFalse(edge in g.InEdges(nodes[2]))
+    self.assertTrue(edge in g.OutEdges(nodes[2]))
+    self.assertTrue(edge in g.InEdges(nodes[3]))
+
+  def testTopologicalSort(self):
+    (_, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    sorted_nodes = g.TopologicalSort()
+    node_to_sorted_index = dict(zip(sorted_nodes, xrange(len(sorted_nodes))))
+    for e in edges:
+      self.assertTrue(
+          node_to_sorted_index[e.from_node] < node_to_sorted_index[e.to_node])
+
+  def testReachableNodes(self):
+    (nodes, _, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    self.assertSetEqual(
+        set([0, 1, 2, 3, 4]),
+        set(n.index for n in g.ReachableNodes([nodes[0]])))
+    self.assertSetEqual(
+        set([0, 1, 2, 3, 4]),
+        set(n.index for n in g.ReachableNodes([nodes[0], nodes[1]])))
+    self.assertSetEqual(
+        set([5, 6]),
+        set(n.index for n in g.ReachableNodes([nodes[5]])))
+    self.assertSetEqual(
+        set([6]),
+        set(n.index for n in g.ReachableNodes([nodes[6]])))
+
+  def testCost(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    for (i, node) in enumerate(nodes):
+      node.cost = i + 1
+    nodes[6].cost = 6
+    for edge in edges:
+      edge.cost = 1
+    self.assertEqual(15, g.Cost())
+    path_list = []
+    g.Cost(path_list=path_list)
+    self.assertListEqual([nodes[i] for i in (0, 1, 3, 4)], path_list)
+    nodes[6].cost = 9
+    self.assertEqual(16, g.Cost())
+    g.Cost(path_list=path_list)
+    self.assertListEqual([nodes[i] for i in (5, 6)], path_list)
+
+  def testCostWithRoots(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    for (i, node) in enumerate(nodes):
+      node.cost = i + 1
+    nodes[6].cost = 9
+    for edge in edges:
+      edge.cost = 1
+    path_list = []
+    self.assertEqual(16, g.Cost(path_list=path_list))
+    self.assertListEqual([nodes[i] for i in (5, 6)], path_list)
+    self.assertEqual(15, g.Cost(roots=[nodes[0]], path_list=path_list))
+    self.assertListEqual([nodes[i] for i in (0, 1, 3, 4)], path_list)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 76f7723..8a172bd 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -14,20 +14,24 @@ how many requests were prefetched.
 import itertools
 import operator
 
+import dependency_graph
 import loading_trace
+import user_satisfied_lens
 import request_dependencies_lens
+import request_track
 
 
 class PrefetchSimulationView(object):
   """Simulates the effect of prefetching resources discoverable by the preload
   scanner.
   """
-  def __init__(self, trace, dependencies_lens):
+  def __init__(self, trace, dependencies_lens, user_lens):
     """Initializes an instance of PrefetchSimulationView.
 
     Args:
       trace: (LoadingTrace) a loading trace.
       dependencies_lens: (RequestDependencyLens) request dependencies.
+      user_lens: (UserSatisfiedLens) Lens used to compute costs.
     """
     self.trace = trace
     self.dependencies_lens = dependencies_lens
@@ -35,6 +39,13 @@ class PrefetchSimulationView(object):
         categories=set([u'blink.net']))
     assert len(self._resource_events.GetEvents()) > 0,\
             'Was the "blink.net" category enabled at trace collection time?"'
+    self._user_lens = user_lens
+    request_ids = self._user_lens.CriticalRequests()
+    all_requests = self.trace.request_track.GetEvents()
+    self._first_request_node = all_requests[0].request_id
+    requests = [r for r in all_requests if r.request_id in request_ids]
+    self.graph = dependency_graph.RequestDependencyGraph(
+        requests, self.dependencies_lens)
 
   def ParserDiscoverableRequests(self, request, recurse=False):
     """Returns a list of requests discovered by the parser from a given request.
@@ -93,7 +104,7 @@ class PrefetchSimulationView(object):
          for r in preloaded_root_requests]))
 
 
-def _PrintSummary(prefetch_view):
+def _PrintSummary(prefetch_view, user_lens):
   requests = prefetch_view.trace.request_track.GetEvents()
   first_request = prefetch_view.trace.request_track.GetEvents()[0]
   parser_requests = prefetch_view.ExpandRedirectChains(
@@ -102,13 +113,22 @@ def _PrintSummary(prefetch_view):
       prefetch_view.PreloadedRequests(first_request))
   print '%d requests, %d parser from the main request, %d preloaded' % (
       len(requests), len(parser_requests), len(preloaded_requests))
+  print 'Time to user satisfaction: %.02fms' % (
+      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
+
+  print 'With 0-cost prefetched resources...'
+  new_costs = {r.request_id: 0. for r in preloaded_requests}
+  prefetch_view.graph.UpdateRequestsCost(new_costs)
+  print 'Time to user satisfaction: %.02fms' % (
+      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
 
 
 def main(filename):
   trace = loading_trace.LoadingTrace.FromJsonFile(filename)
   dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
-  prefetch_view = PrefetchSimulationView(trace, dependencies_lens)
-  _PrintSummary(prefetch_view)
+  user_lens = user_satisfied_lens.FirstContentfulPaintLens(trace)
+  prefetch_view = PrefetchSimulationView(trace, dependencies_lens, user_lens)
+  _PrintSummary(prefetch_view, user_lens)
 
 
 if __name__ == '__main__':
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index f7210bc..ca8897d 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -7,6 +7,8 @@ import unittest
 import prefetch_view
 import request_dependencies_lens
 from request_dependencies_lens_unittest import TestRequests
+import request_track
+import test_utils
 
 
 class PrefetchSimulationViewTestCase(unittest.TestCase):
@@ -54,8 +56,9 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
     self.trace = TestRequests.CreateLoadingTrace(trace_events)
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
+    self.user_satisfied_lens = test_utils.MockUserSatisfiedLens(self.trace)
     self.prefetch_view = prefetch_view.PrefetchSimulationView(
-        self.trace, dependencies_lens)
+        self.trace, dependencies_lens, self.user_satisfied_lens)
 
 
 if __name__ == '__main__':
diff --git a/loading/request_track.py b/loading/request_track.py
index bba015d..2c083c7 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -244,6 +244,14 @@ class Request(object):
       return int(age_match.group(1))
     return -1
 
+  def Cost(self):
+    """Returns the cost of this request in ms, defined as time between
+    request_time and the latest timing event.
+    """
+    # All fields in timing are millis relative to request_time.
+    return max([0] + [t for f, t in self.timing._asdict().iteritems()
+                      if f != 'request_time'])
+
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
 
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 38737c0..a221385 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -10,6 +10,7 @@ import loading_trace
 import page_track
 import request_track
 import tracing
+import user_satisfied_lens
 
 
 class FakeRequestTrack(devtools_monitor.Track):
@@ -207,3 +208,9 @@ class MockConnection(object):
       del self._expected_responses[method]
     self._test_case.assertEqual(expected_params, params)
     return response
+
+
+class MockUserSatisfiedLens(user_satisfied_lens._UserSatisfiedLens):
+  def _CalculateTimes(self, _):
+    self._satisfied_msec = float('inf')
+    self._event_msec = float('inf')

commit 849720169a3b57da2b02bc245cced9fec3d9cb6d
Author: droger <droger@chromium.org>
Date:   Fri Mar 25 07:44:10 2016 -0700

    tools/android/loading Run analyze.py on GCE
    
    Review URL: https://codereview.chromium.org/1831073002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383276}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c0164a8eff21b43ad24d8488424c0ad15d1211c3

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 522be17..453ca86 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -77,7 +77,8 @@ gcloud compute instances list
 
 ## Use the app
 
-Interact with the app on the port 8080 at `http://<instance-ip>:8080`.
+Check that `http://<instance-ip>:8080/test` prints `hello` when opened in a
+browser.
 
 To send a list of URLs to process:
 
@@ -116,13 +117,14 @@ source env/bin/activate
 pip install -r pip_requirements.txt
 ```
 
-Launch the app:
+Launch the app, passing the path to the Chrome executable on the host:
 
 ```shell
-gunicorn --workers=1 main:app --bind 127.0.0.1:8080
+gunicorn --workers=1 --bind 127.0.0.1:8080 \
+    'main:StartApp("/path/to/chrome")'
 ```
 
-In your browser, go to `http://localhost:8080` and use the app.
+You can now [use the app][2], which is located at http://localhost:8080.
 
 Tear down the local environment:
 
@@ -163,3 +165,4 @@ gcloud compute firewall-rules delete default-allow-http-8080
 ```
 
 [1]: https://cloud.google.com/sdk
+[2]: #Use-the-app
diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index fad1755..8e937ed 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -24,12 +24,13 @@ cp -r tools/android/loading/gce $outdir/tools/android/loading
 # Copy other dependencies
 mkdir $outdir/third_party
 # Use rsync to exclude unwanted files (e.g. the .git directory).
-rsync -av --exclude=".*" --exclude "*.pyc" --delete \
-  third_party/catapult $outdir/third_party
+rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
+  --delete third_party/catapult $outdir/third_party
 mkdir $outdir/tools/perf
 cp -r tools/perf/chrome_telemetry_build $outdir/tools/perf
 mkdir -p $outdir/build/android
 cp build/android/devil_chromium.py $outdir/build/android/
+cp build/android/video_recorder.py $outdir/build/android/
 cp build/android/devil_chromium.json $outdir/build/android/
 cp -r build/android/pylib $outdir/build/android/
 
@@ -38,7 +39,6 @@ chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
   /tmp/linux.zip
 gsutil cp /tmp/linux.zip gs://$bucket/chrome/linux.zip
 rm /tmp/linux.zip
-gsutil cp $builddir/chrome_sandbox gs://$bucket/chrome/chrome_sandbox
 
 # Upload Chromium revision
 CHROMIUM_REV=$(git merge-base HEAD origin/master)
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 2a0fd44..cddc903 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -5,6 +5,7 @@
 import json
 import re
 import threading
+import subprocess
 
 from gcloud import storage
 from gcloud.exceptions import NotFound
@@ -15,9 +16,13 @@ class ServerApp(object):
   Google Cloud Storage.
   """
 
-  def __init__(self):
+  def __init__(self, chrome_path):
+    """The chrome_path argument is the path to the Chrome executable as a
+    string.
+    """
     self._tasks = []
     self._thread = None
+    self._chrome_path = chrome_path
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
     print 'Reading server configuration'
@@ -31,11 +36,21 @@ class ServerApp(object):
   def _GetStorageBucket(self, storage_client):
     return storage_client.get_bucket(self._config['bucket_name'])
 
-  def _UploadFile(self, file_stream, filename):
+  def _UploadFile(self, filename_src, filename_dest):
+    """Uploads a file to Google Cloud Storage
+
+    Args:
+      filename_src: name of the local file
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the new file in Google Cloud Storage.
+    """
     client = self._GetStorageClient()
     bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename)
-    blob.upload_from_string(file_stream)
+    blob = bucket.blob(filename_dest)
+    with open(filename_src) as file_src:
+      blob.upload_from_file(file_src)
     url = blob.public_url
     return url
 
@@ -62,23 +77,36 @@ class ServerApp(object):
     self._tasks = json.loads(task_list)
     return len(self._tasks) != 0
 
+  def _GenerateTrace(self, url, filename):
+    """ Generates a trace using analyze.py
+
+    Args:
+      url: url as a string.
+      filename: name of the file where the output is saved.
+
+    Returns:
+      True if the trace was generated successfully.
+    """
+    ret = subprocess.call(
+        ['python', '../analyze.py', 'log_requests', '--clear_cache', '--local',
+         '--headless', '--local_binary', self._chrome_path, '--url', url,
+         '--output', filename])
+    return ret == 0
+
   def _ProcessTasks(self):
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
     while len(self._tasks) > 0:
       url = self._tasks.pop()
       filename = pattern.sub('_', url)
-      # TODO: compute the actual trace for url.
-      trace = '{}'
-      self._UploadFile(trace, filename)
+      if self._GenerateTrace(url, filename):
+        self._UploadFile(filename, 'traces/' + filename)
+      else:
+        # TODO(droger): Upload the list of urls that failed.
+        print 'analyze.py failed'
 
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
-    if path == '/favicon.ico':
-        start_response('404 NOT FOUND', [('Content-Length', '0')])
-        return iter([''])
-
-    status = '200 OK'
 
     if path == '/set_tasks':
       # Get the tasks from the HTTP body.
@@ -92,21 +120,27 @@ class ServerApp(object):
       else:
         data = 'Something went wrong'
     elif path == '/start':
-      if len(self._tasks) > 0:
-        data = 'Starting...'
+      if len(self._tasks) == 0 :
+        data = 'Nothing to do!'
+      elif self._thread is not None and self._thread.is_alive():
+        data = 'Already running!'
+      else:
+        data = 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
         self._thread = threading.Thread(target = self._ProcessTasks)
         self._thread.start()
-      else:
-        data = 'Nothing to do!'
+    elif path == '/test':
+      data = 'hello'
     else:
-      data = environ['PATH_INFO'] + '\n'
+      start_response('404 NOT FOUND', [('Content-Length', '0')])
+      return iter([''])
 
     response_headers = [
         ('Content-type','text/plain'),
         ('Content-Length', str(len(data)))
     ]
-    start_response(status, response_headers)
+    start_response('200 OK', response_headers)
     return iter([data])
 
 
-app = ServerApp()
+def StartApp(chrome_path):
+  return ServerApp(chrome_path)
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index dcad7f2..bb1f94a 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -74,8 +74,12 @@ AUTO_START=$(curl -s \
     "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
     -H "Metadata-Flavor: Google")
 
+# TODO(droger): Figure out how to correctly restore check for auto-startup
+# as well as auto-startup code.
+exit 1
+
 # Exit early if auto start is not enabled.
-if [-z "$AUTO_START"]; then
+if [ -z "$AUTO_START" ]; then
   exit 1
 fi
 
@@ -84,8 +88,8 @@ fi
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
 directory=/opt/app/clovis/tools/android/loading/gce
-command=/opt/app/clovis/env/bin/gunicorn --workers=1 main:app \
-  --bind 0.0.0.0:8080
+command=/opt/app/clovis/env/bin/gunicorn --workers=1 --bind 0.0.0.0:8080 \
+    'main:StartApp("/opt/app/clovis/out/chrome")'
 autostart=true
 autorestart=true
 user=pythonapp

commit ea403492aa471332438daa1041cf18191c6053c8
Author: mattcary <mattcary@chromium.org>
Date:   Thu Mar 24 06:23:41 2016 -0700

    Clovis: update documentation links. goo.gl short links can't be changed :(
    
    Review URL: https://codereview.chromium.org/1827033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383056}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f259d70d9f1a7866c9a6c0d4e1192ea7bb9c6e61

diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 08d5d14..90ced38 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -95,7 +95,7 @@ class GraphSack(object):
     appear with frequency at least CORE_THRESHOLD. For a collection of graph
     sets, for instance pulling the same page under different network
     connections, we intersect the core sets to produce a page core set that
-    describes the key resources used by the page. See https://goo.gl/F1BoEB for
+    describes the key resources used by the page. See https://goo.gl/LmqQRS for
     context and discussion.
 
     Args:
@@ -117,7 +117,7 @@ class GraphSack(object):
   def CoreSimilarity(cls, a, b):
     """Compute the similarity of two core sets.
 
-    We use the Jaccard index. See https://goo.gl/F1BoEB for discussion.
+    We use the Jaccard index. See https://goo.gl/LmqQRS for discussion.
 
     Args:
       a: The first core set, as a set of strings.

commit 73a6767a3ad69c7ae2c48d2fab42390e87f9cbe7
Author: agrieve <agrieve@chromium.org>
Date:   Wed Mar 23 12:54:45 2016 -0700

    Replace usages of DEPRECATED_java_in_dir with java_files
    
    BUG=484854
    
    Review URL: https://codereview.chromium.org/1829823002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382905}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 696e6a571b4db1a966ac7b49260eaa47a1ba9bd4

diff --git a/customtabs_benchmark/BUILD.gn b/customtabs_benchmark/BUILD.gn
index 41fb5e2..e1bd05e 100644
--- a/customtabs_benchmark/BUILD.gn
+++ b/customtabs_benchmark/BUILD.gn
@@ -5,7 +5,7 @@
 import("//build/config/android/rules.gni")
 
 android_apk("customtabs_benchmark_apk") {
-  DEPRECATED_java_in_dir = "java/src"
+  java_files = [ "java/src/org/chromium/customtabs/test/MainActivity.java" ]
   android_manifest = "java/AndroidManifest.xml"
   apk_name = "CustomTabsBenchmark"
   deps = [
diff --git a/memconsumer/BUILD.gn b/memconsumer/BUILD.gn
index af7e045..90b7fcf 100644
--- a/memconsumer/BUILD.gn
+++ b/memconsumer/BUILD.gn
@@ -12,7 +12,10 @@ android_resources("memconsumer_apk_resources") {
 android_apk("memconsumer_apk") {
   apk_name = "MemConsumer"
   android_manifest = "java/AndroidManifest.xml"
-  DEPRECATED_java_in_dir = "java/src"
+  java_files = [
+    "java/src/org/chromium/memconsumer/MemConsumer.java",
+    "java/src/org/chromium/memconsumer/ResidentService.java",
+  ]
   native_libs = [ "libmemconsumer.so" ]
 
   deps = [

commit b925141b88c6170fe1dab96a7783518737f0275c
Author: gabadie <gabadie@chromium.org>
Date:   Wed Mar 23 11:05:47 2016 -0700

    tools/android/loading: Remove old chrome API
    
    Currently, all the code has been migrated to the
    ChromeController API. This CL remove the no longer
    used legacy API, becoming dead code.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1825403002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382880}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a9415b6fd69350e6410726008c0843ee690baade

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
deleted file mode 100644
index 245246d..0000000
--- a/loading/chrome_setup.py
+++ /dev/null
@@ -1,187 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Handles Chrome's configuration. DEPRECATED!"""
-
-import contextlib
-import json
-import shutil
-import subprocess
-import tempfile
-import time
-
-import devtools_monitor
-from options import OPTIONS
-
-
-# Copied from
-# WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
-# Units:
-#   download/upload: byte/s
-#   latency: ms
-NETWORK_CONDITIONS = {
-    'GPRS': {
-        'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
-    'Regular 2G': {
-        'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
-    'Good 2G': {
-        'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
-    'Regular 3G': {
-        'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
-    'Good 3G': {
-        'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
-        'latency': 40},
-    'Regular 4G': {
-        'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
-        'latency': 20},
-    'DSL': {
-        'download': 2 * 1024 * 1024 / 8, 'upload': 1 * 1024 * 1024 / 8,
-        'latency': 5},
-    'WiFi': {
-        'download': 30 * 1024 * 1024 / 8, 'upload': 15 * 1024 * 1024 / 8,
-        'latency': 2}
-}
-
-
-def BandwidthToString(bandwidth):
-  """Converts a bandwidth to string.
-
-  Args:
-    bandwidth: The bandwidth to convert in byte/s. Must be a multiple of 1024/8.
-
-  Returns:
-    A string compatible with wpr --{up,down} command line flags.
-  """
-  assert type(bandwidth) == int
-  assert bandwidth % (1024/8) == 0
-  bandwidth_kbps = (bandwidth * 8) / 1024
-  if bandwidth_kbps % 1024:
-    return '{}Kbit/s'.format(bandwidth_kbps)
-  return '{}Mbit/s'.format(bandwidth_kbps / 1024)
-
-
-@contextlib.contextmanager
-def DevToolsConnectionForLocalBinary(flags):
-  """Returns a DevToolsConnection context manager for a local binary.
-
-  Args:
-    flags: ([str]) List of flags to pass to the browser.
-
-  Returns:
-    A DevToolsConnection context manager.
-  """
-  binary_filename = OPTIONS.local_binary
-  profile_dir = OPTIONS.local_profile_dir
-  using_temp_profile_dir = profile_dir is None
-  if using_temp_profile_dir:
-    profile_dir = tempfile.mkdtemp()
-  flags.append('--user-data-dir=%s' % profile_dir)
-  chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-  process = subprocess.Popen(
-      [binary_filename] + flags, shell=False, stderr=chrome_out)
-  try:
-    time.sleep(10)
-    yield devtools_monitor.DevToolsConnection(
-        OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-  finally:
-    process.kill()
-    if using_temp_profile_dir:
-      shutil.rmtree(profile_dir)
-
-
-def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
-                                    emulated_network_name):
-  """Sets up the device and network emulation and returns the trace metadata.
-
-  Args:
-    connection: (DevToolsConnection)
-    emulated_device_name: (str) Key in the dict returned by
-                          _LoadEmulatedDevices().
-    emulated_network_name: (str) Key in NETWORK_CONDITIONS.
-
-  Returns:
-    A metadata dict {'deviceEmulation': params, 'networkEmulation': params}.
-  """
-  result = {'deviceEmulation': {}, 'networkEmulation': {}}
-  if emulated_device_name:
-    devices = _LoadEmulatedDevices(OPTIONS.devices_file)
-    emulated_device = devices[emulated_device_name]
-    emulation_params = _SetUpDeviceEmulationAndReturnMetadata(
-        connection, emulated_device)
-    result['deviceEmulation'] = emulation_params
-  if emulated_network_name:
-    params = NETWORK_CONDITIONS[emulated_network_name]
-    _SetUpNetworkEmulation(
-        connection, params['latency'], params['download'], params['upload'])
-    result['networkEmulation'] = params
-  return result
-
-
-def _LoadEmulatedDevices(filename):
-  """Loads a list of emulated devices from the DevTools JSON registry.
-
-  Args:
-    filename: (str) Path to the JSON file.
-
-  Returns:
-    {'device_name': device}
-  """
-  json_dict = json.load(open(filename, 'r'))
-  devices = {}
-  for device in json_dict['extensions']:
-    device = device['device']
-    devices[device['title']] = device
-  return devices
-
-
-def _GetDeviceEmulationMetadata(device):
-  """Returns the metadata associated with a given device."""
-  return {'width': device['screen']['vertical']['width'],
-          'height': device['screen']['vertical']['height'],
-          'deviceScaleFactor': device['screen']['device-pixel-ratio'],
-          'mobile': 'mobile' in device['capabilities'],
-          'userAgent': device['user-agent']}
-
-
-def _SetUpDeviceEmulationAndReturnMetadata(connection, device):
-  """Configures an instance of Chrome for device emulation.
-
-  Args:
-    connection: (DevToolsConnection)
-    device: (dict) As returned by LoadEmulatedDevices().
-
-  Returns:
-    A dict containing the device emulation metadata.
-  """
-  print device
-  res = connection.SyncRequest('Emulation.canEmulate')
-  assert res['result'], 'Cannot set device emulation.'
-  data = _GetDeviceEmulationMetadata(device)
-  connection.SyncRequestNoResponse(
-      'Emulation.setDeviceMetricsOverride',
-      {'width': data['width'],
-       'height': data['height'],
-       'deviceScaleFactor': data['deviceScaleFactor'],
-       'mobile': data['mobile'],
-       'fitWindow': True})
-  connection.SyncRequestNoResponse('Network.setUserAgentOverride',
-                                   {'userAgent': data['userAgent']})
-  return data
-
-
-def _SetUpNetworkEmulation(connection, latency, download, upload):
-  """Configures an instance of Chrome for network emulation.
-
-  Args:
-    connection: (DevToolsConnection)
-    latency: (float) Latency in ms.
-    download: (float) Download speed (Bytes / s).
-    upload: (float) Upload speed (Bytes / s).
-  """
-  res = connection.SyncRequest('Network.canEmulateNetworkConditions')
-  assert res['result'], 'Cannot set network emulation.'
-  connection.SyncRequestNoResponse(
-      'Network.emulateNetworkConditions',
-      {'offline': False, 'latency': latency, 'downloadThroughput': download,
-       'uploadThroughput': upload})
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 787cd79..d2c466d 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -35,8 +35,8 @@ sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
 import adb_install_cert
 import certutils
 
-import chrome_setup
 import devtools_monitor
+import emulation
 import options
 
 
@@ -127,13 +127,6 @@ def ForwardPort(device, local, remote):
     device.adb.ForwardRemove(local)
 
 
-# Deprecated
-def _SetUpDevice(device, package_info):
-  """Enables root and closes Chrome on a device."""
-  device.EnableRoot()
-  device.KillAll(package_info.package, quiet=True)
-
-
 @contextlib.contextmanager
 def _WprHost(wpr_archive_path, record=False,
              network_condition_name=None,
@@ -148,13 +141,13 @@ def _WprHost(wpr_archive_path, record=False,
   else:
     assert os.path.exists(wpr_archive_path)
   if network_condition_name:
-    condition = chrome_setup.NETWORK_CONDITIONS[network_condition_name]
+    condition = emulation.NETWORK_CONDITIONS[network_condition_name]
     if record:
       logging.warning('WPR network condition is ignored when recording.')
     else:
       wpr_server_args.extend([
-          '--down', chrome_setup.BandwidthToString(condition['download']),
-          '--up', chrome_setup.BandwidthToString(condition['upload']),
+          '--down', emulation.BandwidthToString(condition['download']),
+          '--up', emulation.BandwidthToString(condition['upload']),
           '--delay_ms', str(condition['latency']),
           '--shaping_type', 'proxy'])
 
@@ -207,7 +200,7 @@ def LocalWprHost(wpr_archive_path, record=False,
     wpr_archive_path: host sided WPR archive's path.
     record: Enables or disables WPR archive recording.
     network_condition_name: Network condition name available in
-        chrome_setup.NETWORK_CONDITIONS.
+        emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
 
@@ -243,7 +236,7 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
     wpr_archive_path: host sided WPR archive's path.
     record: Enables or disables WPR archive recording.
     network_condition_name: Network condition name available in
-        chrome_setup.NETWORK_CONDITIONS.
+        emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
 
@@ -374,57 +367,3 @@ def RemoteSpeedIndexRecorder(device, connection, local_output_path):
       })();
     """)
     yield
-
-
-@contextlib.contextmanager
-def _DevToolsConnectionOnDevice(device, flags):
-  """Returns a DevToolsConnection context manager for a given device.
-
-  Args:
-    device: Device to connect to.
-    flags: ([str]) List of flags.
-
-  Returns:
-    A DevToolsConnection context manager.
-  """
-  package_info = OPTIONS.ChromePackage()
-  command_line_path = '/data/local/chrome-command-line'
-  _SetUpDevice(device, package_info)
-  with FlagReplacer(device, command_line_path, flags):
-    start_intent = intent.Intent(
-        package=package_info.package, activity=package_info.activity,
-        data='about:blank')
-    device.StartActivity(start_intent, blocking=True)
-    time.sleep(2)
-    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
-                     'localabstract:chrome_devtools_remote'):
-      yield devtools_monitor.DevToolsConnection(
-          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-
-
-# Deprecated, use *Controller.
-def DeviceConnection(device, additional_flags=None):
-  """Context for starting recording on a device.
-
-  Sets up and restores any device and tracing appropriately
-
-  Args:
-    device: Android device, or None for a local run (in which case chrome needs
-      to have been started with --remote-debugging-port=XXX).
-    additional_flags: Additional chromium arguments.
-
-  Returns:
-    A context manager type which evaluates to a DevToolsConnection.
-  """
-  new_flags = ['--disable-fre',
-               '--enable-test-events',
-               '--remote-debugging-port=%d' % OPTIONS.devtools_port]
-  if OPTIONS.no_sandbox:
-    new_flags.append('--no-sandbox')
-  if additional_flags != None:
-    new_flags.extend(additional_flags)
-
-  if device:
-    return _DevToolsConnectionOnDevice(device, new_flags)
-  else:
-    return chrome_setup.DevToolsConnectionForLocalBinary(new_flags)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
deleted file mode 100755
index 7c9e822..0000000
--- a/loading/trace_recorder.py
+++ /dev/null
@@ -1,85 +0,0 @@
-#! /usr/bin/python
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Loading trace recorder. DEPRECATED!"""
-
-import argparse
-import datetime
-import json
-import logging
-import os
-import sys
-import time
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-import devil_chromium
-
-import device_setup
-import devtools_monitor
-import loading_trace
-import page_track
-import request_track
-import tracing
-
-
-def MonitorUrl(connection, url, clear_cache=False,
-               categories=tracing.DEFAULT_CATEGORIES,
-               timeout=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
-  """Monitor a URL via a trace recorder.
-
-  DEPRECATED! Use LoadingTrace.FromUrlAndController instead.
-
-  Args:
-    connection: A devtools_monitor.DevToolsConnection instance.
-    url: url to navigate to as string.
-    clear_cache: boolean indicating if cache should be cleared before loading.
-    categories: List of tracing event categories to record.
-    timeout: Websocket timeout.
-
-  Returns:
-    loading_trace.LoadingTrace.
-  """
-  page = page_track.PageTrack(connection)
-  request = request_track.RequestTrack(connection)
-  trace = tracing.TracingTrack(connection, categories=categories)
-  connection.SetUpMonitoring()
-  if clear_cache:
-    connection.ClearCache()
-  connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-  connection.StartMonitoring(timeout=timeout)
-  metadata = {'date': datetime.datetime.utcnow().isoformat(),
-              'seconds_since_epoch': time.time()}
-  return loading_trace.LoadingTrace(url, metadata, page, request, trace)
-
-def RecordAndDumpTrace(device, url, output_filename):
-  with file(output_filename, 'w') as output,\
-        device_setup.DeviceConnection(device) as connection:
-    trace = MonitorUrl(connection, url)
-    json.dump(trace.ToJsonDict(), output)
-
-
-def main():
-  logging.basicConfig(level=logging.INFO)
-  devil_chromium.Initialize()
-
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--url', required=True)
-  parser.add_argument('--output', required=True)
-  args = parser.parse_args()
-  url = args.url
-  if not url.startswith('http'):
-    url = 'http://' + url
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  RecordAndDumpTrace(device, url, args.output)
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index aad36c1..1db5da7 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -44,7 +44,7 @@ sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 import controller
 import loading_trace
 import options
-import trace_recorder
+
 
 OPTIONS = options.OPTIONS
 WEBSERVER = os.path.join(os.path.dirname(__file__), 'test_server.py')

commit 17a094776de09c85980abfcbcb68e3e4730ec035
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 09:40:42 2016 -0700

    Clovis: remove occurrence counting in ResourceSack.
    
    Now that we've settled on CoreSet this is unused.
    
    Review URL: https://codereview.chromium.org/1830523004
    
    Cr-Original-Commit-Position: refs/heads/master@{#382865}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fdd7f55e4218083d927e8de55fb1621c64ab1641

diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 9c174c5..08d5d14 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -131,22 +131,6 @@ class GraphSack(object):
       return 0
     return float(len(a & b)) / len(a | b)
 
-  def FilterOccurrence(self, tag, filter_from_graph):
-    """Accumulate filter occurrences for each bag in the graph.
-
-    This can be retrieved under tag for each Bag in the graph. For example, if
-    FilterContentful marks the nodes of each graph before the first contentful
-    paint, then FilterOccurrence('contentful', FilterContentful) will count, for
-    each bag, the fraction of nodes that were before the first contentful paint.
-
-    Args:
-      tag: the tag to count the filter appearances under.
-      filter_from_graph: a function graph -> node filter, where node filter
-        takes a node to a boolean.
-    """
-    for bag in self.bags:
-      bag.MarkOccurrence(tag, filter_from_graph)
-
   @property
   def num_graphs(self):
     return len(self.graph_info)
@@ -191,12 +175,6 @@ class Bag(dag.Node):
     self._relative_costs = []
     self._num_critical = 0
 
-    # See MarkOccurrence and GetOccurrence, below. This maps an occurrence
-    # tag to a list of nodes matching the occurrence.
-    self._occurence_matches = {}
-    # Number of nodes seen for each occurrence.
-    self._occurence_count = {}
-
   @property
   def url(self):
     return self._url
@@ -254,37 +232,6 @@ class Bag(dag.Node):
       self._successor_sources[successor_bag].add((graph, node, s))
       self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
 
-  def MarkOccurrence(self, tag, filter_from_graph):
-    """Mark occurrences for nodes in this bag according to graph_filters.
-
-    Results can be querried by GetOccurrence().
-
-    Args:
-      tag: a label for this set of occurrences.
-      filter_from_graph: a function graph -> node filter, where node filter
-        takes a node to a boolean.
-    """
-    self._occurence_matches[tag] = 0
-    self._occurence_count[tag] = 0
-    for graph, nodes in self.graphs.iteritems():
-      for n in nodes:
-        self._occurence_count[tag] += 1
-        if filter_from_graph(graph)(n):
-          self._occurence_matches[tag] += 1
-
-  def GetOccurrence(self, tag):
-    """Retrieve the occurrence fraction of a tag.
-
-    Args:
-      tag: the tag under which the occurrence was counted. This must have been
-        previously added at least once via AddOccurrence.
-
-    Returns:
-      A fraction occurrence matches / occurrence node count.
-    """
-    assert self._occurence_count[tag] > 0
-    return float(self._occurence_matches[tag]) / self._occurence_count[tag]
-
   @classmethod
   def _MakeShortname(cls, url):
     parsed = urlparse.urlparse(url)
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index df0fdb7..a4b1a30 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -66,34 +66,6 @@ class ResourceSackTestCase(unittest.TestCase):
     self.assertEqual(set(['0/', 'data:fake/content']),
                      set([bag.label for bag in sack.bags]))
 
-  def test_Occurrence(self):
-    # There are two graph shapes. The first one is added to the sack three
-    # times, and the second once. The second graph has one sibling that doesn't
-    # appear in the first as well as a new child.
-    shape1 = [MakeRequest(0, 'null'), MakeRequest(1, 0), MakeRequest(2, 0)]
-    shape2 = [MakeRequest(0, 'null'), MakeRequest(1, 0),
-              MakeRequest(3, 0), MakeRequest(4, 1)]
-    graphs = [TestResourceGraph.FromRequestList(s)
-              for s in (shape1, shape1,  shape1, shape2)]
-    sack = resource_sack.GraphSack()
-    for g in graphs:
-      sack.ConsumeGraph(g)
-    # Map a graph to a list of nodes that are in its filter.
-    filter_sets = {
-        graphs[0]: set([0, 1, 2]),
-        graphs[1]: set([0, 1, 2]),
-        graphs[2]: set([0, 1]),
-        graphs[3]: set([0, 3])}
-    sack.FilterOccurrence(
-        'test', lambda graph: lambda node:
-            int(node.ShortName()) in filter_sets[graph])
-    labels = {bag.label: bag for bag in sack.bags}
-    self.assertAlmostEqual(1, labels['0/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(0.75, labels['1/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(0.667, labels['2/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(1, labels['3/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(0, labels['4/'].GetOccurrence('test'), 3)
-
   def test_Core(self):
     # We will use a core threshold of 0.5 to make it easier to define
     # graphs. Resources 0 and 1 are core and others are not.

commit ffd6a1ef1176bd1ad69104ea861339af47a1e7e8
Author: gabadie <gabadie@chromium.org>
Date:   Wed Mar 23 09:29:17 2016 -0700

    sandwich: Slice up sandwich.py into sandwich_{runner,misc}.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1822163002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382860}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 75fe2ef04b5ad5a3e91e34e20d236a7963da74bb

diff --git a/loading/sandwich.py b/loading/sandwich.py
index c989b6f..3fac221 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -13,13 +13,9 @@ TODO(pasko): implement cache preparation and WPR.
 
 import argparse
 import csv
-import json
 import logging
 import os
-import shutil
 import sys
-import tempfile
-import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -32,281 +28,16 @@ from pylib import constants
 import devil_chromium
 
 import chrome_cache
-import chrome_setup
-import controller
-import device_setup
-import devtools_monitor
-import frame_load_lens
-import loading_trace
+import emulation
 import options
-import page_track
-import pull_sandwich_metrics
-import request_dependencies_lens
-import trace_recorder
-import tracing
-import wpr_backend
+import sandwich_metrics
+import sandwich_misc
+from sandwich_runner import SandwichRunner
 
 
 # Use options layer to access constants.
 OPTIONS = options.OPTIONS
 
-_JOB_SEARCH_PATH = 'sandwich_jobs'
-
-# An estimate of time to wait for the device to become idle after expensive
-# operations, such as opening the launcher activity.
-_TIME_TO_DEVICE_IDLE_SECONDS = 2
-
-
-# Devtools timeout of 1 minute to avoid websocket timeout on slow
-# network condition.
-_DEVTOOLS_TIMEOUT = 60
-
-
-def _ReadUrlsFromJobDescription(job_name):
-  """Retrieves the list of URLs associated with the job name."""
-  try:
-    # Extra sugar: attempt to load from a relative path.
-    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
-        job_name)
-    with open(json_file_name) as f:
-      json_data = json.load(f)
-  except IOError:
-    # Attempt to read by regular file name.
-    with open(job_name) as f:
-      json_data = json.load(f)
-
-  key = 'urls'
-  if json_data and key in json_data:
-    url_list = json_data[key]
-    if isinstance(url_list, list) and len(url_list) > 0:
-      return url_list
-  raise Exception('Job description does not define a list named "urls"')
-
-
-def _CleanPreviousTraces(output_directories_path):
-  """Cleans previous traces from the output directory.
-
-  Args:
-    output_directories_path: The output directory path where to clean the
-        previous traces.
-  """
-  for dirname in os.listdir(output_directories_path):
-    directory_path = os.path.join(output_directories_path, dirname)
-    if not os.path.isdir(directory_path):
-      continue
-    try:
-      int(dirname)
-    except ValueError:
-      continue
-    shutil.rmtree(directory_path)
-
-
-class SandwichRunner(object):
-  """Sandwich runner.
-
-  This object is meant to be configured first and then run using the Run()
-  method. The runner can configure itself conveniently with parsed arguement
-  using the PullConfigFromArgs() method. The only job is to make sure that the
-  command line flags have `dest` parameter set to existing runner members.
-  """
-
-  def __init__(self, job_name):
-    """Configures a sandwich runner out of the box.
-
-    Public members are meant to be configured as wished before calling Run().
-
-    Args:
-      job_name: The job name to get the associated urls.
-    """
-    # Cache operation to do before doing the chrome navigation.
-    #   Can be: clear,save,push,reload
-    self.cache_operation = 'clear'
-
-    # The cache archive's path to save to or push from. Is str or None.
-    self.cache_archive_path = None
-
-    # Controls whether the WPR server should do script injection.
-    self.disable_wpr_script_injection = False
-
-    # The job name. Is str.
-    self.job_name = job_name
-
-    # Number of times to repeat the job.
-    self.job_repeat = 1
-
-    # Network conditions to emulate. None if no emulation.
-    self.network_condition = None
-
-    # Network condition emulator. Can be: browser,wpr
-    self.network_emulator = 'browser'
-
-    # Output directory where to save the traces. Is str or None.
-    self.trace_output_directory = None
-
-    # List of urls to run.
-    self.urls = _ReadUrlsFromJobDescription(job_name)
-
-    # Configures whether to record speed-index video.
-    self.record_video = False
-
-    # Path to the WPR archive to load or save. Is str or None.
-    self.wpr_archive_path = None
-
-    # Configures whether the WPR archive should be read or generated.
-    self.wpr_record = False
-
-    self._chrome_ctl = None
-    self._local_cache_directory_path = None
-
-  def PullConfigFromArgs(self, args):
-    """Configures the sandwich runner from parsed command line argument.
-
-    Args:
-      args: The command line parsed argument.
-    """
-    for config_name in self.__dict__.keys():
-      if config_name in args.__dict__:
-        self.__dict__[config_name] = args.__dict__[config_name]
-
-  def PrintConfig(self):
-    """Print the current sandwich runner configuration to stdout. """
-    for config_name in sorted(self.__dict__.keys()):
-      if config_name[0] != '_':
-        print '{} = {}'.format(config_name, self.__dict__[config_name])
-
-  def _CleanTraceOutputDirectory(self):
-    assert self.trace_output_directory
-    if not os.path.isdir(self.trace_output_directory):
-      try:
-        os.makedirs(self.trace_output_directory)
-      except OSError:
-        logging.error('Cannot create directory for results: %s',
-            self.trace_output_directory)
-        raise
-    else:
-      _CleanPreviousTraces(self.trace_output_directory)
-
-  def _SaveRunInfos(self, urls):
-    assert self.trace_output_directory
-    run_infos = {
-      'cache-op': self.cache_operation,
-      'job_name': self.job_name,
-      'urls': urls
-    }
-    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
-              'w') as file_output:
-      json.dump(run_infos, file_output, indent=2)
-
-  def _GetEmulatorNetworkCondition(self, emulator):
-    if self.network_emulator == emulator:
-      return self.network_condition
-    return None
-
-  def _RunNavigation(self, url, clear_cache, run_id=None):
-    """Run a page navigation to the given URL.
-
-    Args:
-      url: The URL to navigate to.
-      clear_cache: Whether if the cache should be cleared before navigation.
-      run_id: Id of the run in the output directory. If it is None, then no
-        trace or video will be saved.
-    """
-    run_path = None
-    if self.trace_output_directory is not None and run_id is not None:
-      run_path = os.path.join(self.trace_output_directory, str(run_id))
-      if os.path.isdir(run_path):
-        os.makedirs(run_path)
-    self._chrome_ctl.SetNetworkEmulation(
-        self._GetEmulatorNetworkCondition('browser'))
-    # TODO(gabadie): add a way to avoid recording a trace.
-    with self._chrome_ctl.Open() as connection:
-      if clear_cache:
-        connection.ClearCache()
-      if run_path is not None and self.record_video:
-        device = self._chrome_ctl.GetDevice()
-        assert device, 'Can only record video on a remote device.'
-        video_recording_path = os.path.join(run_path, 'video.mp4')
-        with device_setup.RemoteSpeedIndexRecorder(device, connection,
-                                                   video_recording_path):
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url=url,
-              connection=connection,
-              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              categories=pull_sandwich_metrics.CATEGORIES,
-              timeout_seconds=_DEVTOOLS_TIMEOUT)
-      else:
-        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-            url=url,
-            connection=connection,
-            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            categories=pull_sandwich_metrics.CATEGORIES,
-            timeout_seconds=_DEVTOOLS_TIMEOUT)
-    if run_path is not None:
-      trace_path = os.path.join(run_path, 'trace.json')
-      trace.ToJsonFile(trace_path)
-
-  def _RunUrl(self, url, run_id):
-    clear_cache = False
-    if self.cache_operation == 'clear':
-      clear_cache = True
-    elif self.cache_operation == 'push':
-      self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
-    elif self.cache_operation == 'reload':
-      self._RunNavigation(url, clear_cache=True)
-    elif self.cache_operation == 'save':
-      clear_cache = run_id == 0
-    self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
-
-  def _PullCacheFromDevice(self):
-    assert self.cache_operation == 'save'
-    assert self.cache_archive_path, 'Need to specify where to save the cache'
-
-    cache_directory_path = self._chrome_ctl.PullBrowserCache()
-    chrome_cache.ZipDirectoryContent(
-        cache_directory_path, self.cache_archive_path)
-    shutil.rmtree(cache_directory_path)
-
-  def Run(self):
-    """SandwichRunner main entry point meant to be called once configured."""
-    assert self._chrome_ctl == None
-    assert self._local_cache_directory_path == None
-    if self.trace_output_directory:
-      self._CleanTraceOutputDirectory()
-
-    # TODO(gabadie): Make sandwich working on desktop.
-    device = device_utils.DeviceUtils.HealthyDevices()[0]
-    self._chrome_ctl = controller.RemoteChromeController(device)
-    self._chrome_ctl.AddChromeArgument('--disable-infobars')
-    if self.cache_operation == 'save':
-      self._chrome_ctl.SetSlowDeath()
-
-    if self.cache_operation == 'push':
-      assert os.path.isfile(self.cache_archive_path)
-      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-      chrome_cache.UnzipDirectoryContent(
-          self.cache_archive_path, self._local_cache_directory_path)
-
-    ran_urls = []
-    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
-        record=self.wpr_record,
-        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
-        disable_script_injection=self.disable_wpr_script_injection
-        ):
-      for _ in xrange(self.job_repeat):
-        for url in self.urls:
-          self._RunUrl(url, run_id=len(ran_urls))
-          ran_urls.append(url)
-
-    if self._local_cache_directory_path:
-      shutil.rmtree(self._local_cache_directory_path)
-      self._local_cache_directory_path = None
-    if self.cache_operation == 'save':
-      self._PullCacheFromDevice()
-    if self.trace_output_directory:
-      self._SaveRunInfos(ran_urls)
-
-    self._chrome_ctl = None
-
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
@@ -371,7 +102,7 @@ def _ArgumentParser():
                               'overriding javascript\'s Math.random() and ' +
                               'Date() with deterministic implementations.')
   run_parser.add_argument('--network-condition', default=None,
-      choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
+      choices=sorted(emulation.NETWORK_CONDITIONS.keys()),
       help='Set a network profile.')
   run_parser.add_argument('--network-emulator', default='browser',
       choices=['browser', 'wpr'],
@@ -430,36 +161,6 @@ def _RecordWprMain(args):
   return 0
 
 
-def _PatchWprMain(args):
-  # Sets the resources cache max-age to 10 years.
-  MAX_AGE = 10 * 365 * 24 * 60 * 60
-  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
-
-  wpr_archive = wpr_backend.WprArchiveBackend(args.wpr_archive_path)
-  for url_entry in wpr_archive.ListUrlEntries():
-    response_headers = url_entry.GetResponseHeadersDict()
-    if 'cache-control' in response_headers and \
-        response_headers['cache-control'] == CACHE_CONTROL:
-      continue
-    logging.info('patching %s' % url_entry.url)
-    # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
-    # TODO(gabadie): may need to delete ETag.
-    # TODO(gabadie): may need to patch Vary.
-    # TODO(gabadie): may need to take care of x-cache.
-    #
-    # Override the cache-control header to set the resources max age to MAX_AGE.
-    #
-    # Important note: Some resources holding sensitive information might have
-    # cache-control set to no-store which allow the resource to be cached but
-    # not cached in the file system. NoState-Prefetch is going to take care of
-    # this case. But in here, to simulate NoState-Prefetch, we don't have other
-    # choices but save absolutely all cached resources on disk so they survive
-    # after killing chrome for cache save, modification and push.
-    url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
-  wpr_archive.Persist()
-  return 0
-
-
 def _CreateCacheMain(args):
   sandwich_runner = SandwichRunner(args.job)
   sandwich_runner.PullConfigFromArgs(args)
@@ -480,12 +181,12 @@ def _RunJobMain(args):
 
 
 def _ExtractMetricsMain(args):
-  trace_metrics_list = pull_sandwich_metrics.PullMetricsFromOutputDirectory(
+  trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
       args.trace_output_directory)
   trace_metrics_list.sort(key=lambda e: e['id'])
   with open(args.metrics_csv_path, 'w') as csv_file:
     writer = csv.DictWriter(csv_file,
-                            fieldnames=pull_sandwich_metrics.CSV_FIELD_NAMES)
+                            fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
     writer.writeheader()
     for trace_metrics in trace_metrics_list:
       writer.writerow(trace_metrics)
@@ -495,31 +196,8 @@ def _ExtractMetricsMain(args):
 def _FilterCacheMain(args):
   whitelisted_urls = set()
   for loading_trace_path in args.loading_trace_paths:
-    logging.info('loading %s' % loading_trace_path)
-    trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
-    requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
-    deps = requests_lens.GetRequestDependencies()
-
-    main_resource_request = deps[0][0]
-    logging.info('white-listing %s' % main_resource_request.url)
-    whitelisted_urls.add(main_resource_request.url)
-    for (first, second, reason) in deps:
-      # Work-around where the protocol may be none for an unclear reason yet.
-      # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
-      #   this work-around.
-      if not second.protocol:
-        logging.info('ignoring %s (no protocol)' % second.url)
-        continue
-      # Ignore data protocols.
-      if not second.protocol.startswith('http'):
-        logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
-            second.url, second.protocol))
-        continue
-      if (first.request_id == main_resource_request.request_id and
-          reason == 'parser' and second.url not in whitelisted_urls):
-        logging.info('white-listing %s' % second.url)
-        whitelisted_urls.add(second.url)
-
+    whitelisted_urls.update(
+        sandwich_misc.ExtractParserDiscoverableResources(loading_trace_path))
   if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
     os.makedirs(os.path.dirname(args.output_cache_archive_path))
   chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
@@ -538,7 +216,8 @@ def main(command_line_args):
   if args.subcommand == 'record-wpr':
     return _RecordWprMain(args)
   if args.subcommand == 'patch-wpr':
-    return _PatchWprMain(args)
+    sandwich_misc.PatchWpr(args.wpr_archive_path)
+    return 0
   if args.subcommand == 'create-cache':
     return _CreateCacheMain(args)
   if args.subcommand == 'run':
diff --git a/loading/pull_sandwich_metrics.py b/loading/sandwich_metrics.py
similarity index 100%
rename from loading/pull_sandwich_metrics.py
rename to loading/sandwich_metrics.py
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
similarity index 99%
rename from loading/pull_sandwich_metrics_unittest.py
rename to loading/sandwich_metrics_unittest.py
index 31519af..7cbe795 100644
--- a/loading/pull_sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -12,7 +12,7 @@ import unittest
 
 import loading_trace
 import page_track
-import pull_sandwich_metrics as puller
+import sandwich_metrics as puller
 import request_track
 import tracing
 
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
new file mode 100644
index 0000000..a15f5ab
--- /dev/null
+++ b/loading/sandwich_misc.py
@@ -0,0 +1,81 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+
+import wpr_backend
+import loading_trace
+import request_dependencies_lens
+
+
+def PatchWpr(wpr_archive_path):
+  """Patches a WPR archive to get all resources into the HTTP cache and avoid
+  invalidation and revalidations.
+
+  Args:
+    wpr_archive_path: Path of the WPR archive to patch.
+  """
+  # Sets the resources cache max-age to 10 years.
+  MAX_AGE = 10 * 365 * 24 * 60 * 60
+  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
+
+  wpr_archive = wpr_backend.WprArchiveBackend(wpr_archive_path)
+  for url_entry in wpr_archive.ListUrlEntries():
+    response_headers = url_entry.GetResponseHeadersDict()
+    if 'cache-control' in response_headers and \
+        response_headers['cache-control'] == CACHE_CONTROL:
+      continue
+    logging.info('patching %s' % url_entry.url)
+    # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
+    # TODO(gabadie): may need to delete ETag.
+    # TODO(gabadie): may need to patch Vary.
+    # TODO(gabadie): may need to take care of x-cache.
+    #
+    # Override the cache-control header to set the resources max age to MAX_AGE.
+    #
+    # Important note: Some resources holding sensitive information might have
+    # cache-control set to no-store which allow the resource to be cached but
+    # not cached in the file system. NoState-Prefetch is going to take care of
+    # this case. But in here, to simulate NoState-Prefetch, we don't have other
+    # choices but save absolutely all cached resources on disk so they survive
+    # after killing chrome for cache save, modification and push.
+    url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
+  wpr_archive.Persist()
+
+
+def ExtractParserDiscoverableResources(loading_trace_path):
+  """Extracts the parser discoverable resources from a loading trace.
+
+  Args:
+    loading_trace_path: The loading trace's path.
+
+  Returns:
+    A set of urls.
+  """
+  whitelisted_urls = set()
+  logging.info('loading %s' % loading_trace_path)
+  trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
+  requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
+  deps = requests_lens.GetRequestDependencies()
+
+  main_resource_request = deps[0][0]
+  logging.info('white-listing %s' % main_resource_request.url)
+  whitelisted_urls.add(main_resource_request.url)
+  for (first, second, reason) in deps:
+    # Work-around where the protocol may be none for an unclear reason yet.
+    # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
+    #   this work-around.
+    if not second.protocol:
+      logging.info('ignoring %s (no protocol)' % second.url)
+      continue
+    # Ignore data protocols.
+    if not second.protocol.startswith('http'):
+      logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
+          second.url, second.protocol))
+      continue
+    if (first.request_id == main_resource_request.request_id and
+        reason == 'parser' and second.url not in whitelisted_urls):
+      logging.info('white-listing %s' % second.url)
+      whitelisted_urls.add(second.url)
+  return whitelisted_urls
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
new file mode 100644
index 0000000..5dd7ba2
--- /dev/null
+++ b/loading/sandwich_runner.py
@@ -0,0 +1,276 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import logging
+import os
+import shutil
+import sys
+import tempfile
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+import chrome_cache
+import controller
+import devtools_monitor
+import device_setup
+import loading_trace
+import sandwich_metrics
+
+
+_JOB_SEARCH_PATH = 'sandwich_jobs'
+
+# Devtools timeout of 1 minute to avoid websocket timeout on slow
+# network condition.
+_DEVTOOLS_TIMEOUT = 60
+
+
+def _ReadUrlsFromJobDescription(job_name):
+  """Retrieves the list of URLs associated with the job name."""
+  try:
+    # Extra sugar: attempt to load from a relative path.
+    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
+        job_name)
+    with open(json_file_name) as f:
+      json_data = json.load(f)
+  except IOError:
+    # Attempt to read by regular file name.
+    with open(job_name) as f:
+      json_data = json.load(f)
+
+  key = 'urls'
+  if json_data and key in json_data:
+    url_list = json_data[key]
+    if isinstance(url_list, list) and len(url_list) > 0:
+      return url_list
+  raise Exception('Job description does not define a list named "urls"')
+
+
+def _CleanPreviousTraces(output_directories_path):
+  """Cleans previous traces from the output directory.
+
+  Args:
+    output_directories_path: The output directory path where to clean the
+        previous traces.
+  """
+  for dirname in os.listdir(output_directories_path):
+    directory_path = os.path.join(output_directories_path, dirname)
+    if not os.path.isdir(directory_path):
+      continue
+    try:
+      int(dirname)
+    except ValueError:
+      continue
+    shutil.rmtree(directory_path)
+
+
+class SandwichRunner(object):
+  """Sandwich runner.
+
+  This object is meant to be configured first and then run using the Run()
+  method. The runner can configure itself conveniently with parsed arguement
+  using the PullConfigFromArgs() method. The only job is to make sure that the
+  command line flags have `dest` parameter set to existing runner members.
+  """
+
+  def __init__(self, job_name):
+    """Configures a sandwich runner out of the box.
+
+    Public members are meant to be configured as wished before calling Run().
+
+    Args:
+      job_name: The job name to get the associated urls.
+    """
+    # Cache operation to do before doing the chrome navigation.
+    #   Can be: clear,save,push,reload
+    self.cache_operation = 'clear'
+
+    # The cache archive's path to save to or push from. Is str or None.
+    self.cache_archive_path = None
+
+    # Controls whether the WPR server should do script injection.
+    self.disable_wpr_script_injection = False
+
+    # The job name. Is str.
+    self.job_name = job_name
+
+    # Number of times to repeat the job.
+    self.job_repeat = 1
+
+    # Network conditions to emulate. None if no emulation.
+    self.network_condition = None
+
+    # Network condition emulator. Can be: browser,wpr
+    self.network_emulator = 'browser'
+
+    # Output directory where to save the traces. Is str or None.
+    self.trace_output_directory = None
+
+    # List of urls to run.
+    self.urls = _ReadUrlsFromJobDescription(job_name)
+
+    # Configures whether to record speed-index video.
+    self.record_video = False
+
+    # Path to the WPR archive to load or save. Is str or None.
+    self.wpr_archive_path = None
+
+    # Configures whether the WPR archive should be read or generated.
+    self.wpr_record = False
+
+    self._chrome_ctl = None
+    self._local_cache_directory_path = None
+
+  def PullConfigFromArgs(self, args):
+    """Configures the sandwich runner from parsed command line argument.
+
+    Args:
+      args: The command line parsed argument.
+    """
+    for config_name in self.__dict__.keys():
+      if config_name in args.__dict__:
+        self.__dict__[config_name] = args.__dict__[config_name]
+
+  def PrintConfig(self):
+    """Print the current sandwich runner configuration to stdout. """
+    for config_name in sorted(self.__dict__.keys()):
+      if config_name[0] != '_':
+        print '{} = {}'.format(config_name, self.__dict__[config_name])
+
+  def _CleanTraceOutputDirectory(self):
+    assert self.trace_output_directory
+    if not os.path.isdir(self.trace_output_directory):
+      try:
+        os.makedirs(self.trace_output_directory)
+      except OSError:
+        logging.error('Cannot create directory for results: %s',
+            self.trace_output_directory)
+        raise
+    else:
+      _CleanPreviousTraces(self.trace_output_directory)
+
+  def _SaveRunInfos(self, urls):
+    assert self.trace_output_directory
+    run_infos = {
+      'cache-op': self.cache_operation,
+      'job_name': self.job_name,
+      'urls': urls
+    }
+    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
+              'w') as file_output:
+      json.dump(run_infos, file_output, indent=2)
+
+  def _GetEmulatorNetworkCondition(self, emulator):
+    if self.network_emulator == emulator:
+      return self.network_condition
+    return None
+
+  def _RunNavigation(self, url, clear_cache, run_id=None):
+    """Run a page navigation to the given URL.
+
+    Args:
+      url: The URL to navigate to.
+      clear_cache: Whether if the cache should be cleared before navigation.
+      run_id: Id of the run in the output directory. If it is None, then no
+        trace or video will be saved.
+    """
+    run_path = None
+    if self.trace_output_directory is not None and run_id is not None:
+      run_path = os.path.join(self.trace_output_directory, str(run_id))
+      if not os.path.isdir(run_path):
+        os.makedirs(run_path)
+    self._chrome_ctl.SetNetworkEmulation(
+        self._GetEmulatorNetworkCondition('browser'))
+    # TODO(gabadie): add a way to avoid recording a trace.
+    with self._chrome_ctl.Open() as connection:
+      if clear_cache:
+        connection.ClearCache()
+      if run_path is not None and self.record_video:
+        device = self._chrome_ctl.GetDevice()
+        assert device, 'Can only record video on a remote device.'
+        video_recording_path = os.path.join(run_path, 'video.mp4')
+        with device_setup.RemoteSpeedIndexRecorder(device, connection,
+                                                   video_recording_path):
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url=url,
+              connection=connection,
+              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+              categories=sandwich_metrics.CATEGORIES,
+              timeout_seconds=_DEVTOOLS_TIMEOUT)
+      else:
+        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+            url=url,
+            connection=connection,
+            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+            categories=sandwich_metrics.CATEGORIES,
+            timeout_seconds=_DEVTOOLS_TIMEOUT)
+    if run_path is not None:
+      trace_path = os.path.join(run_path, 'trace.json')
+      trace.ToJsonFile(trace_path)
+
+  def _RunUrl(self, url, run_id):
+    clear_cache = False
+    if self.cache_operation == 'clear':
+      clear_cache = True
+    elif self.cache_operation == 'push':
+      self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
+    elif self.cache_operation == 'reload':
+      self._RunNavigation(url, clear_cache=True)
+    elif self.cache_operation == 'save':
+      clear_cache = run_id == 0
+    self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
+
+  def _PullCacheFromDevice(self):
+    assert self.cache_operation == 'save'
+    assert self.cache_archive_path, 'Need to specify where to save the cache'
+
+    cache_directory_path = self._chrome_ctl.PullBrowserCache()
+    chrome_cache.ZipDirectoryContent(
+        cache_directory_path, self.cache_archive_path)
+    shutil.rmtree(cache_directory_path)
+
+  def Run(self):
+    """SandwichRunner main entry point meant to be called once configured."""
+    assert self._chrome_ctl == None
+    assert self._local_cache_directory_path == None
+    if self.trace_output_directory:
+      self._CleanTraceOutputDirectory()
+
+    # TODO(gabadie): Make sandwich working on desktop.
+    device = device_utils.DeviceUtils.HealthyDevices()[0]
+    self._chrome_ctl = controller.RemoteChromeController(device)
+    self._chrome_ctl.AddChromeArgument('--disable-infobars')
+    if self.cache_operation == 'save':
+      self._chrome_ctl.SetSlowDeath()
+
+    if self.cache_operation == 'push':
+      assert os.path.isfile(self.cache_archive_path)
+      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+      chrome_cache.UnzipDirectoryContent(
+          self.cache_archive_path, self._local_cache_directory_path)
+
+    ran_urls = []
+    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
+        record=self.wpr_record,
+        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
+        disable_script_injection=self.disable_wpr_script_injection
+        ):
+      for _ in xrange(self.job_repeat):
+        for url in self.urls:
+          self._RunUrl(url, run_id=len(ran_urls))
+          ran_urls.append(url)
+
+    if self._local_cache_directory_path:
+      shutil.rmtree(self._local_cache_directory_path)
+      self._local_cache_directory_path = None
+    if self.cache_operation == 'save':
+      self._PullCacheFromDevice()
+    if self.trace_output_directory:
+      self._SaveRunInfos(ran_urls)
+
+    self._chrome_ctl = None

commit 984e6ba778aedb4eee85d4ccde1c778e66ab3358
Author: gabadie <gabadie@chromium.org>
Date:   Wed Mar 23 08:40:08 2016 -0700

    sandwich: Record a loading video on android and compute speed-index from it.
    
    Before, metrics in the CSV where fetched only from loading traces.
    This CL adds the hability to record a MP4 video of chrome loading
    a web page in sandwich, and then process that recorded video to
    output the loading speed-index into the metrics' CSV.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1782543002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382850}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0c661158cf00b7ea75807aa78787234d80305799

diff --git a/loading/analyze.py b/loading/analyze.py
index 58a4ace..fd1ba4e 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -117,12 +117,15 @@ def _LogRequests(url, clear_cache_override=None):
 
   clear_cache = (clear_cache_override if clear_cache_override is not None
                  else OPTIONS.clear_cache)
-  chrome_ctl.SetClearCache(clear_cache)
   if OPTIONS.emulate_device:
     chrome_ctl.SetDeviceEmulation(OPTIONS.emulate_device)
   if OPTIONS.emulate_network:
     chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_network)
-  trace = loading_trace.LoadingTrace.FromUrlAndController(url, chrome_ctl)
+  with chrome_ctl.Open() as connection:
+    if clear_cache:
+      connection.ClearCache()
+    trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+        url, connection, chrome_ctl.ChromeMetadata())
   return trace.ToJsonDict()
 
 
diff --git a/loading/common_util.py b/loading/common_util.py
new file mode 100644
index 0000000..5b62ce0
--- /dev/null
+++ b/loading/common_util.py
@@ -0,0 +1,26 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+import time
+
+
+def PollFor(condition, condition_name, interval=5):
+  """Polls for a function to return true.
+
+  Args:
+    condition: Function to wait its return to be True.
+    condition_name: The condition's name used for logging.
+    interval: Periods to wait between tries in seconds.
+
+  Returns:
+    What condition has returned to stop waiting.
+  """
+  while True:
+    result = condition()
+    logging.info('Polling condition %s is %s' % (
+        condition_name, 'met' if result else 'not met'))
+    if result:
+      return result
+    time.sleep(interval)
diff --git a/loading/controller.py b/loading/controller.py
index 7aa9bde..053e062 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -66,21 +66,12 @@ class ChromeControllerBase(object):
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
-    self._clear_cache = False
     self._slow_death = False
 
   def AddChromeArgument(self, arg):
     """Add command-line argument to the chrome execution."""
     self._chrome_args.append(arg)
 
-  def SetClearCache(self, clear_cache=True):
-    """Ensure cache is cleared before running.
-
-    Args:
-      clear_cache: true if cache should be cleared.
-    """
-    self._clear_cache = clear_cache
-
   @contextlib.contextmanager
   def Open(self):
     """Context that returns a connection/chrome instance.
@@ -182,11 +173,8 @@ class ChromeControllerBase(object):
     if self._emulated_network:
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
-
     self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
                           seconds_since_epoch=time.time())
-    if self._clear_cache:
-      connection.AddHook(connection.ClearCache)
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
@@ -216,6 +204,10 @@ class RemoteChromeController(ChromeControllerBase):
     self._device = device
     self._device.EnableRoot()
 
+  def GetDevice(self):
+    """Overridden android device."""
+    return self._device
+
   @contextlib.contextmanager
   def Open(self):
     """Overridden connection creation."""
diff --git a/loading/device_setup.py b/loading/device_setup.py
index fad2b23..787cd79 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -22,11 +22,13 @@ from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
+from video_recorder import video_recorder
 
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
 from chrome_telemetry_build import chromium_config
 
 sys.path.append(chromium_config.GetTelemetryDir())
+from telemetry.internal.image_processing import video
 from telemetry.internal.util import webpagereplay
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
@@ -40,6 +42,9 @@ import options
 
 OPTIONS = options.OPTIONS
 
+# The speed index's video recording's bit rate in Mb/s.
+_SPEED_INDEX_VIDEO_BITRATE = 4
+
 
 class DeviceSetupException(Exception):
   def __init__(self, msg):
@@ -288,6 +293,90 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
 
 # Deprecated
 @contextlib.contextmanager
+def _RemoteVideoRecorder(device, local_output_path, megabits_per_second):
+  """Record a video on Device.
+
+  Args:
+    device: (device_utils.DeviceUtils) Android device to connect to.
+    local_output_path: Output path were to save the video locally.
+    megabits_per_second: Video recorder Mb/s.
+
+  Yields:
+    None
+  """
+  assert device
+  if megabits_per_second > 100:
+    raise ValueError('Android video capture cannot capture at %dmbps. '
+                     'Max capture rate is 100mbps.' % megabits_per_second)
+  assert local_output_path.endswith('.mp4')
+  recorder = video_recorder.VideoRecorder(device, megabits_per_second)
+  recorder.Start()
+  try:
+    yield
+    recorder.Stop()
+    recorder.Pull(host_file=local_output_path)
+    recorder = None
+  finally:
+    if recorder:
+      recorder.Stop()
+
+
+@contextlib.contextmanager
+def RemoteSpeedIndexRecorder(device, connection, local_output_path):
+  """Records on a device a video compatible for speed-index computation.
+
+  Note:
+    Chrome should be opened with the --disable-infobars command line argument to
+    avoid web page viewport size to be changed, that can change speed-index
+    value.
+
+  Args:
+    device: (device_utils.DeviceUtils) Android device to connect to.
+    connection: devtools connection.
+    local_output_path: Output path were to save the video locally.
+
+  Yields:
+    None
+  """
+  # Paint the current HTML document with the ORANGE that video is detecting with
+  # the view-port position and size.
+  color = video.HIGHLIGHT_ORANGE_FRAME
+  connection.ExecuteJavaScript("""
+    (function() {
+      var screen = document.createElement('div');
+      screen.style.background = 'rgb(%d, %d, %d)';
+      screen.style.position = 'fixed';
+      screen.style.top = '0';
+      screen.style.left = '0';
+      screen.style.width = '100%%';
+      screen.style.height = '100%%';
+      screen.style.zIndex = '2147483638';
+      document.body.appendChild(screen);
+      requestAnimationFrame(function() {
+        requestAnimationFrame(function() {
+          window.__speedindex_screen = screen;
+        });
+      });
+    })();
+  """ % (color.r, color.g, color.b))
+  connection.PollForJavaScriptExpression('!!window.__speedindex_screen', 1)
+
+  with _RemoteVideoRecorder(device, local_output_path,
+                            megabits_per_second=_SPEED_INDEX_VIDEO_BITRATE):
+    # Paint the current HTML document with white so that it is not troubling the
+    # speed index measurement.
+    connection.ExecuteJavaScript("""
+      (function() {
+        requestAnimationFrame(function() {
+          var screen = window.__speedindex_screen;
+          screen.style.background = 'rgb(255, 255, 255)';
+        });
+      })();
+    """)
+    yield
+
+
+@contextlib.contextmanager
 def _DevToolsConnectionOnDevice(device, flags):
   """Returns a DevToolsConnection context manager for a given device.
 
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index b579918..ab78f8e 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -19,6 +19,8 @@ sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
+import common_util
+
 
 DEFAULT_TIMEOUT_SECONDS = 10 # seconds
 
@@ -100,7 +102,6 @@ class DevToolsConnection(object):
     self._domains_to_enable = set()
     self._tearing_down_tracing = False
     self._please_stop = False
-    self._hooks = []
     self._ws = None
     self._target_descriptor = None
 
@@ -212,14 +213,6 @@ class DevToolsConnection(object):
     assert res['result'], 'Cache clearing is not supported by this browser.'
     self.SyncRequest('Network.clearBrowserCache')
 
-  def AddHook(self, hook):
-    """Add hook to be run on monitoring start.
-
-    Args:
-      hook: a function.
-    """
-    self._hooks.append(hook)
-
   def MonitorUrl(self, url, timeout_seconds=DEFAULT_TIMEOUT_SECONDS):
     """Navigate to url and dispatch monitoring loop.
 
@@ -239,8 +232,6 @@ class DevToolsConnection(object):
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][0])
-    for hook in self._hooks:
-      hook()
     self._tearing_down_tracing = False
 
     self.SendAndIgnoreResponse('Page.navigate', {'url': url})
@@ -252,6 +243,37 @@ class DevToolsConnection(object):
     """Stops the monitoring."""
     self._please_stop = True
 
+  def ExecuteJavaScript(self, expression):
+    """Run JavaScript expression.
+
+    Args:
+      expression: JavaScript expression to run.
+
+    Returns:
+      The return value from the JavaScript expression.
+    """
+    response = self.SyncRequest('Runtime.evaluate', {
+        'expression': expression,
+        'returnByValue': True})
+    if 'error' in response:
+      raise Exception(response['error']['message'])
+    if 'wasThrown' in response['result'] and response['result']['wasThrown']:
+      raise Exception(response['error']['result']['description'])
+    if response['result']['result']['type'] == 'undefined':
+      return None
+    return response['result']['result']['value']
+
+  def PollForJavaScriptExpression(self, expression, interval):
+    """Wait until JavaScript expression is true.
+
+    Args:
+      expression: JavaScript expression to run.
+      interval: Period between expression evaluation in seconds.
+    """
+    common_util.PollFor(lambda: bool(self.ExecuteJavaScript(expression)),
+                        'JavaScript: {}'.format(expression),
+                        interval)
+
   def Close(self):
     """Cleanly close chrome by closing the only tab."""
     assert self._ws
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 50dbc23..6dbd13d 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -71,26 +71,26 @@ class LoadingTrace(object):
       return cls.FromJsonDict(json.load(input_file))
 
   @classmethod
-  def FromUrlAndController(
-      cls, url, controller, categories=None,
+  def RecordUrlNavigation(
+      cls, url, connection, chrome_metadata, categories=None,
       timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
     """Create a loading trace by using controller to fetch url.
 
     Args:
       url: (str) url to fetch.
-      controller: (ChromeControllerBase) controller to manage the connection.
+      connection: An opened devtools connection.
+      chrome_metadata: Dictionary of chrome metadata.
       categories: TracingTrack categories to capture.
       timeout_seconds: monitoring connection timeout in seconds.
 
     Returns:
       LoadingTrace instance.
     """
-    with controller.Open() as connection:
-      page = page_track.PageTrack(connection)
-      request = request_track.RequestTrack(connection)
-      trace = tracing.TracingTrack(
-          connection,
-          categories=(tracing.DEFAULT_CATEGORIES if categories is None
-                      else categories))
-      connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
-      return cls(url, controller.ChromeMetadata(), page, request, trace)
+    page = page_track.PageTrack(connection)
+    request = request_track.RequestTrack(connection)
+    trace = tracing.TracingTrack(
+        connection,
+        categories=(tracing.DEFAULT_CATEGORIES if categories is None
+                    else categories))
+    connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
+    return cls(url, chrome_metadata, page, request, trace)
diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
index ee37583..e30232c 100644
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -7,10 +7,23 @@
 python pull_sandwich_metrics.py -h
 """
 
-import json
+import collections
 import logging
 import os
+import shutil
 import sys
+import tempfile
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
+from chrome_telemetry_build import chromium_config
+
+sys.path.append(chromium_config.GetTelemetryDir())
+from telemetry.internal.image_processing import video
+from telemetry.util import image_util
+from telemetry.util import rgba_color
 
 import loading_trace as loading_trace_module
 import tracing
@@ -24,10 +37,20 @@ CSV_FIELD_NAMES = [
     'total_load',
     'onload',
     'browser_malloc_avg',
-    'browser_malloc_max']
+    'browser_malloc_max',
+    'speed_index']
 
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
+# Points of a completeness record.
+#
+# Members:
+#   |time| is in milliseconds,
+#   |frame_completeness| value representing how complete the frame is at a given
+#     |time|. Caution: this completeness might be negative.
+CompletenessPoint = collections.namedtuple('CompletenessPoint',
+    ('time', 'frame_completeness'))
+
 
 def _GetBrowserPID(tracing_track):
   """Get the browser PID from a trace.
@@ -136,6 +159,65 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   }
 
 
+def _ExtractCompletenessRecordFromVideo(video_path):
+  """Extracts the completeness record from a video.
+
+  The video must start with a filled rectangle of orange (RGB: 222, 100, 13), to
+  give the view-port size/location from where to compute the completeness.
+
+  Args:
+    video_path: Path of the video to extract the completeness list from.
+
+  Returns:
+    list(CompletenessPoint)
+  """
+  video_file = tempfile.NamedTemporaryFile()
+  shutil.copy(video_path, video_file.name)
+  video_capture = video.Video(video_file)
+
+  histograms = [
+      (time, image_util.GetColorHistogram(
+          image, ignore_color=rgba_color.WHITE, tolerance=8))
+      for time, image in video_capture.GetVideoFrameIter()
+  ]
+
+  start_histogram = histograms[1][1]
+  final_histogram = histograms[-1][1]
+  total_distance = start_histogram.Distance(final_histogram)
+
+  def FrameProgress(histogram):
+    if total_distance == 0:
+      if histogram.Distance(final_histogram) == 0:
+        return 1.0
+      else:
+        return 0.0
+    return 1 - histogram.Distance(final_histogram) / total_distance
+
+  return [(time, FrameProgress(hist)) for time, hist in histograms]
+
+
+def ComputeSpeedIndex(completeness_record):
+  """Computes the speed-index from a completeness record.
+
+  Args:
+    completeness_record: list(CompletenessPoint)
+
+  Returns:
+    Speed-index value.
+  """
+  speed_index = 0.0
+  last_time = completeness_record[0][0]
+  last_completness = completeness_record[0][1]
+  for time, completeness in completeness_record:
+    if time < last_time:
+      raise ValueError('Completeness record must be sorted by timestamps.')
+    elapsed = time - last_time
+    speed_index += elapsed * (1.0 - last_completness)
+    last_time = time
+    last_completness = completeness
+  return speed_index
+
+
 def PullMetricsFromOutputDirectory(output_directory_path):
   """Pulls all the metrics from all the traces of a sandwich run directory.
 
@@ -147,10 +229,6 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     List of dictionaries with all CSV_FIELD_NAMES's field set.
   """
   assert os.path.isdir(output_directory_path)
-  run_infos = None
-  with open(os.path.join(output_directory_path, 'run_infos.json')) as f:
-    run_infos = json.load(f)
-  assert run_infos
   metrics = []
   for node_name in os.listdir(output_directory_path):
     if not os.path.isdir(os.path.join(output_directory_path, node_name)):
@@ -159,15 +237,22 @@ def PullMetricsFromOutputDirectory(output_directory_path):
       page_id = int(node_name)
     except ValueError:
       continue
-    trace_path = os.path.join(output_directory_path, node_name, 'trace.json')
+    run_path = os.path.join(output_directory_path, node_name)
+    trace_path = os.path.join(run_path, 'trace.json')
     if not os.path.isfile(trace_path):
       continue
     logging.info('processing \'%s\'' % trace_path)
     loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
-    trace_metrics = _PullMetricsFromLoadingTrace(loading_trace)
-    trace_metrics['id'] = page_id
-    trace_metrics['url'] = run_infos['urls'][page_id]
-    metrics.append(trace_metrics)
+    row_metrics = {key: 'unavailable' for key in CSV_FIELD_NAMES}
+    row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
+    row_metrics['id'] = page_id
+    row_metrics['url'] = loading_trace.url
+    video_path = os.path.join(run_path, 'video.mp4')
+    if os.path.isfile(video_path):
+      logging.info('processing \'%s\'' % video_path)
+      completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
+      row_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+    metrics.append(row_metrics)
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
   return metrics
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
index 518607c..31519af 100644
--- a/loading/pull_sandwich_metrics_unittest.py
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -194,10 +194,41 @@ class PageTrackTest(unittest.TestCase):
     self.assertEquals(30971, metrics['browser_malloc_avg'])
     self.assertEquals(55044, metrics['browser_malloc_max'])
 
+  def testComputeSpeedIndex(self):
+    def point(time, frame_completeness):
+      return puller.CompletenessPoint(time=time,
+                                      frame_completeness=frame_completeness)
+    completness_record = [
+      point(0, 0.0),
+      point(120, 0.4),
+      point(190, 0.75),
+      point(280, 1.0),
+      point(400, 1.0),
+    ]
+    self.assertEqual(120 + 70 * 0.6 + 90 * 0.25,
+                     puller.ComputeSpeedIndex(completness_record))
+
+    completness_record = [
+      point(70, 0.0),
+      point(150, 0.3),
+      point(210, 0.6),
+      point(220, 0.9),
+      point(240, 1.0),
+    ]
+    self.assertEqual(80 + 60 * 0.7 + 10 * 0.4 + 20 * 0.1,
+                     puller.ComputeSpeedIndex(completness_record))
+
+    completness_record = [
+      point(90, 0.0),
+      point(200, 0.6),
+      point(150, 0.3),
+      point(230, 1.0),
+    ]
+    with self.assertRaises(ValueError):
+      puller.ComputeSpeedIndex(completness_record)
+
   def testCommandLine(self):
     tmp_dir = tempfile.mkdtemp()
-    with open(os.path.join(tmp_dir, 'run_infos.json'), 'w') as out_file:
-      json.dump({'urls': ['a.com', 'b.com', 'c.org']}, out_file)
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
       LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 1739578..c989b6f 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -146,6 +146,9 @@ class SandwichRunner(object):
     # List of urls to run.
     self.urls = _ReadUrlsFromJobDescription(job_name)
 
+    # Configures whether to record speed-index video.
+    self.record_video = False
+
     # Path to the WPR archive to load or save. Is str or None.
     self.wpr_archive_path = None
 
@@ -199,23 +202,50 @@ class SandwichRunner(object):
       return self.network_condition
     return None
 
-  def _RunNavigation(self, url, clear_cache, trace_id=None):
-    self._chrome_ctl.SetClearCache(clear_cache)
+  def _RunNavigation(self, url, clear_cache, run_id=None):
+    """Run a page navigation to the given URL.
+
+    Args:
+      url: The URL to navigate to.
+      clear_cache: Whether if the cache should be cleared before navigation.
+      run_id: Id of the run in the output directory. If it is None, then no
+        trace or video will be saved.
+    """
+    run_path = None
+    if self.trace_output_directory is not None and run_id is not None:
+      run_path = os.path.join(self.trace_output_directory, str(run_id))
+      if os.path.isdir(run_path):
+        os.makedirs(run_path)
     self._chrome_ctl.SetNetworkEmulation(
         self._GetEmulatorNetworkCondition('browser'))
     # TODO(gabadie): add a way to avoid recording a trace.
-    trace = loading_trace.LoadingTrace.FromUrlAndController(
-        url=url,
-        controller=self._chrome_ctl,
-        categories=pull_sandwich_metrics.CATEGORIES,
-        timeout_seconds=_DEVTOOLS_TIMEOUT)
-    if trace_id != None and self.trace_output_directory:
-      trace_path = os.path.join(
-          self.trace_output_directory, str(trace_id), 'trace.json')
-      os.makedirs(os.path.dirname(trace_path))
+    with self._chrome_ctl.Open() as connection:
+      if clear_cache:
+        connection.ClearCache()
+      if run_path is not None and self.record_video:
+        device = self._chrome_ctl.GetDevice()
+        assert device, 'Can only record video on a remote device.'
+        video_recording_path = os.path.join(run_path, 'video.mp4')
+        with device_setup.RemoteSpeedIndexRecorder(device, connection,
+                                                   video_recording_path):
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url=url,
+              connection=connection,
+              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+              categories=pull_sandwich_metrics.CATEGORIES,
+              timeout_seconds=_DEVTOOLS_TIMEOUT)
+      else:
+        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+            url=url,
+            connection=connection,
+            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+            categories=pull_sandwich_metrics.CATEGORIES,
+            timeout_seconds=_DEVTOOLS_TIMEOUT)
+    if run_path is not None:
+      trace_path = os.path.join(run_path, 'trace.json')
       trace.ToJsonFile(trace_path)
 
-  def _RunUrl(self, url, trace_id=0):
+  def _RunUrl(self, url, run_id):
     clear_cache = False
     if self.cache_operation == 'clear':
       clear_cache = True
@@ -224,8 +254,8 @@ class SandwichRunner(object):
     elif self.cache_operation == 'reload':
       self._RunNavigation(url, clear_cache=True)
     elif self.cache_operation == 'save':
-      clear_cache = trace_id == 0
-    self._RunNavigation(url, clear_cache=clear_cache, trace_id=trace_id)
+      clear_cache = run_id == 0
+    self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
 
   def _PullCacheFromDevice(self):
     assert self.cache_operation == 'save'
@@ -264,7 +294,7 @@ class SandwichRunner(object):
         ):
       for _ in xrange(self.job_repeat):
         for url in self.urls:
-          self._RunUrl(url, trace_id=len(ran_urls))
+          self._RunUrl(url, run_id=len(ran_urls))
           ran_urls.append(url)
 
     if self._local_cache_directory_path:
@@ -350,6 +380,9 @@ def _ArgumentParser():
           ' to be set.')
   run_parser.add_argument('--job-repeat', default=1, type=int,
                           help='How many times to run the job.')
+  run_parser.add_argument('--record-video', action='store_true',
+                          help='Configures either to record or not a video of '
+                              +'chrome loading the web pages.')
   run_parser.add_argument('--wpr-archive', default=None, type=str,
                           dest='wpr_archive_path',
                           help='Web page replay archive to load job\'s urls ' +
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index 17c7560..aad36c1 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -229,9 +229,12 @@ def RunTest(webserver, test_page, expected):
   url = 'http://%s/%s' % (webserver.Address(), test_page)
   sys.stdout.write('Testing %s...' % url)
   chrome_controller = controller.LocalChromeController()
-  chrome_controller.SetClearCache()
-  observed_seq = InitiatorSequence(
-      loading_trace.LoadingTrace.FromUrlAndController(url, chrome_controller))
+
+  with chrome_controller.Open() as connection:
+    connection.ClearCache()
+    observed_seq = InitiatorSequence(
+        loading_trace.LoadingTrace.RecordUrlNavigation(
+            url, connection, chrome_controller.ChromeMetadata()))
   if observed_seq == expected:
     sys.stdout.write(' ok\n')
     return True

commit 95080f4fe2de285a508e38fa00d447e8568e55bc
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 07:38:24 2016 -0700

    Clovis: stopgap for webserver test race.
    
    Return test server files in single chunk rather than as individual bytes. This
    affects what seems to be an internal race in chrome or chrome devtools that
    causes different reported initiators.
    
    Review URL: https://codereview.chromium.org/1824343002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382846}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 710720cfe33cf8135aa539e949736cc88357cb89

diff --git a/loading/trace_test/test_server.py b/loading/trace_test/test_server.py
index 5463667..45517f7 100755
--- a/loading/trace_test/test_server.py
+++ b/loading/trace_test/test_server.py
@@ -86,7 +86,7 @@ class ServerApp(object):
       for header in self._response_headers[path]:
         headers.append((str(header[0]), str(header[1])))
     start_response('200 OK', headers)
-    return file(filename).read()
+    return [file(filename).read()]
 
 
 if __name__ == '__main__':

commit 6598f4ed6a8d662d57455d5853d068f6c0bca96f
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 05:21:35 2016 -0700

    Clovis: Update webtest to new controller to make it run.
    
    This incorporates the controller refactor, which incidentally fixes some
    bitrotted code as well as adds --test_filter to make debugging a specific test
    easier. Also updates a test which had missed comments shifting around.
    
    1.html is still flaky, I'm looking into it.
    
    Review URL: https://codereview.chromium.org/1829633002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382832}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b9f18314c220fec5b9b91409db88556234462f9b

diff --git a/loading/analyze.py b/loading/analyze.py
index 2aed605..58a4ace 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -110,6 +110,7 @@ def _LogRequests(url, clear_cache_override=None):
   """
   if OPTIONS.local:
     chrome_ctl = controller.LocalChromeController()
+    chrome_ctl.SetHeadless(OPTIONS.headless)
   else:
     chrome_ctl = controller.RemoteChromeController(
         device_setup.GetFirstDevice())
diff --git a/loading/controller.py b/loading/controller.py
index 1a55085..7aa9bde 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -74,6 +74,11 @@ class ChromeControllerBase(object):
     self._chrome_args.append(arg)
 
   def SetClearCache(self, clear_cache=True):
+    """Ensure cache is cleared before running.
+
+    Args:
+      clear_cache: true if cache should be cleared.
+    """
     self._clear_cache = clear_cache
 
   @contextlib.contextmanager
@@ -287,11 +292,20 @@ class LocalChromeController(ChromeControllerBase):
     self._using_temp_profile_dir = self._profile_dir is None
     if self._using_temp_profile_dir:
       self._profile_dir = tempfile.mkdtemp(suffix='.profile')
+    self._headless = False
 
   def __del__(self):
     if self._using_temp_profile_dir:
       shutil.rmtree(self._profile_dir)
 
+  def SetHeadless(self, headless=True):
+    """Set a headless run.
+
+    Args:
+      headless: true if the chrome instance should be headless.
+    """
+    self._headless = headless
+
   @contextlib.contextmanager
   def Open(self):
     """Override for connection context."""
@@ -305,7 +319,7 @@ class LocalChromeController(ChromeControllerBase):
     chrome_cmd.append('about:blank')
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     environment = os.environ.copy()
-    if OPTIONS.headless:
+    if self._headless:
       environment['DISPLAY'] = 'localhost:99'
       xvfb_process = subprocess.Popen(
           ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
@@ -331,7 +345,7 @@ class LocalChromeController(ChromeControllerBase):
     finally:
       if connection:
         chrome_process.kill()
-      if OPTIONS.headless:
+      if self._headless:
         xvfb_process.kill()
 
   def PushBrowserCache(self, cache_path):
diff --git a/loading/trace_test/results/3.result b/loading/trace_test/results/3.result
index bcebf63..196a88d 100644
--- a/loading/trace_test/results/3.result
+++ b/loading/trace_test/results/3.result
@@ -1,6 +1,6 @@
 parser (no stack) 3a.js
 parser (no stack) 3c.js
-script (3a.js:10/3a.js:14/3.html:23) 3a.jpg
+script (3a.js:10/3a.js:14/3.html:20) 3a.jpg
 script (3a.js:20) 3b.js
 script (3b.js:9) 3b.jpg
-script (3c.js:7/3.html:24) 3c.jpg
+script (3c.js:7/3.html:21) 3c.jpg
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index 800b8d6..17c7560 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -41,7 +41,7 @@ import urlparse
 
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 
-from device_setup import DeviceConnection
+import controller
 import loading_trace
 import options
 import trace_recorder
@@ -211,7 +211,7 @@ class InitiatorSequence(object):
     return short
 
 
-def RunTest(webserver, connection, test_page, expected):
+def RunTest(webserver, test_page, expected):
   """Run an webserver test.
 
   The expected result can be None, in which case --failed_trace_dir can be set
@@ -220,7 +220,6 @@ def RunTest(webserver, connection, test_page, expected):
   Args:
     webserver [WebServer]: the webserver to use for the test. It must be
       started.
-    connection [DevToolsConnection]: the connection to trace against.
     test_page: the name of the page to load.
     expected [InitiatorSequence]: expected initiator sequence.
 
@@ -229,8 +228,10 @@ def RunTest(webserver, connection, test_page, expected):
   """
   url = 'http://%s/%s' % (webserver.Address(), test_page)
   sys.stdout.write('Testing %s...' % url)
-  observed_seq = InitiatorSequence(trace_recorder.MonitorUrl(
-      connection, url, clear_cache=True))
+  chrome_controller = controller.LocalChromeController()
+  chrome_controller.SetClearCache()
+  observed_seq = InitiatorSequence(
+      loading_trace.LoadingTrace.FromUrlAndController(url, chrome_controller))
   if observed_seq == expected:
     sys.stdout.write(' ok\n')
     return True
@@ -251,12 +252,16 @@ def RunAllTests():
   All tests must have a corresponding result in RESULTDIR unless
   --failed_trace_dir is set.
   """
+  test_filter = set(OPTIONS.test_filter.split(',')) \
+      if OPTIONS.test_filter else None
+
   with TemporaryDirectory() as temp_dir, \
-       WebServer.Context(TESTDIR, temp_dir) as webserver, \
-       DeviceConnection(None) as connection:
+       WebServer.Context(TESTDIR, temp_dir) as webserver:
     failure = False
     for test in sorted(os.listdir(TESTDIR)):
       if test.endswith('.html'):
+        if test_filter and test not in test_filter:
+          continue
         result = os.path.join(RESULTDIR, test[:test.rfind('.')] + '.result')
         assert OPTIONS.failed_trace_dir or os.path.exists(result), \
             'No result found for test'
@@ -264,7 +269,7 @@ def RunAllTests():
         if os.path.exists(result):
           with file(result) as result_file:
             expected = InitiatorSequence.ReadFromFile(result_file)
-        if not RunTest(webserver, connection, test, expected):
+        if not RunTest(webserver, test, expected):
           failure = True
   if failure:
     print 'FAILED!'
@@ -276,5 +281,6 @@ if __name__ == '__main__':
   OPTIONS.ParseArgs(sys.argv[1:],
                     description='Run webserver integration test',
                     extra=[('--failed_trace_dir', ''),
-                           ('--noisy', False)])
+                           ('--noisy', False),
+                           ('--test_filter', None)])
   RunAllTests()

commit 104b1dc04ac92a85f172f36435cdff59f738c435
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 03:47:37 2016 -0700

    Clovis: Improve --local_binary option.
    
    Make --local_binary smart enough to be relative to code directory no matter where a script is being run from.
    
    Review URL: https://codereview.chromium.org/1825333002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382823}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 23f2d9a9a21c608c5008e8cb0e00d0dcf8d0647c

diff --git a/loading/options.py b/loading/options.py
index 0490ac5..021377c 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -26,7 +26,7 @@ class Options(object):
              'hostname for devtools websocket connection'),
             ('devtools_port', 9222,
              'port for devtools websocket connection'),
-            ('local_binary', 'out/Release/chrome',
+            ('local_binary', os.path.join(_SRC_DIR, 'out/Release/chrome'),
              'chrome binary for local runs'),
             ('local_noisy', False,
              'Enable local chrome console output'),

commit 29d75b6b48431a4f57000ce12cba70fb0a9c3540
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 22 12:15:31 2016 -0700

    tools/android/loading: Launch chrome on devices more reliably.
    
    Currently, we are giving only 2s for chrome to start its devtools
    HTTP server causing flakyness. This CL attempts to connect to
    chrome's devtools every one seconds instead.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1825123002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382628}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a8893e338e26dd9dfcf9c00f2635a025759bd16f

diff --git a/loading/controller.py b/loading/controller.py
index 25fefae..1a55085 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -14,6 +14,7 @@ import datetime
 import logging
 import os
 import shutil
+import socket
 import subprocess
 import sys
 import tempfile
@@ -33,10 +34,6 @@ from devil.android.sdk import intent
 
 OPTIONS = options.OPTIONS
 
-# An estimate of time to wait for the device to become idle after expensive
-# operations, such as opening the launcher activity.
-_TIME_TO_DEVICE_IDLE_SECONDS = 2
-
 
 class ChromeControllerBase(object):
   """Base class for all controllers.
@@ -193,8 +190,15 @@ class ChromeControllerBase(object):
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
-  # Seconds to sleep after starting chrome activity.
-  POST_ACTIVITY_SLEEP_SECONDS = 2
+  # Number of connection attempt to chrome's devtools.
+  DEVTOOLS_CONNECTION_ATTEMPTS = 10
+
+  # Time interval in seconds between chrome's devtools connection attempts.
+  DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
+
+  # An estimate of time to wait for the device to become idle after expensive
+  # operations, such as opening the launcher activity.
+  TIME_TO_IDLE_SECONDS = 2
 
   def __init__(self, device):
     """Initialize the controller.
@@ -203,38 +207,50 @@ class RemoteChromeController(ChromeControllerBase):
       device: an andriod device.
     """
     assert device is not None, 'Should you be using LocalController instead?'
-    self._device = device
     super(RemoteChromeController, self).__init__()
+    self._device = device
+    self._device.EnableRoot()
 
   @contextlib.contextmanager
   def Open(self):
     """Overridden connection creation."""
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
-
-    self._device.EnableRoot()
     self._device.KillAll(package_info.package, quiet=True)
-
+    chrome_args = self._GetChromeArguments()
+    logging.info('Launching %s with flags: %s' % (package_info.package,
+        subprocess.list2cmdline(chrome_args)))
     with device_setup.FlagReplacer(
         self._device, command_line_path, self._GetChromeArguments()):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
       self._device.StartActivity(start_intent, blocking=True)
-      time.sleep(self.POST_ACTIVITY_SLEEP_SECONDS)
-      with device_setup.ForwardPort(
-          self._device, 'tcp:%d' % OPTIONS.devtools_port,
-          'localabstract:chrome_devtools_remote'):
-        connection = devtools_monitor.DevToolsConnection(
-            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-        self._StartConnection(connection)
-        yield connection
-    if self._slow_death:
-      self._device.adb.Shell('am start com.google.android.launcher')
-      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    self._device.KillAll(package_info.package, quiet=True)
+      try:
+        for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS + 1):
+          if attempt_id == self.DEVTOOLS_CONNECTION_ATTEMPTS:
+            raise RuntimeError('Failed to connect to chrome devtools after {} '
+                               'attempts.'.format(attempt_id))
+          logging.info('Devtools connection attempt %d' % attempt_id)
+          with device_setup.ForwardPort(
+              self._device, 'tcp:%d' % OPTIONS.devtools_port,
+              'localabstract:chrome_devtools_remote'):
+            try:
+              connection = devtools_monitor.DevToolsConnection(
+                  OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+              self._StartConnection(connection)
+            except socket.error as e:
+              assert str(e).startswith('[Errno 104] Connection reset by peer')
+              time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
+              continue
+            logging.info('Devtools connection success')
+            yield connection
+            if self._slow_death:
+              self._device.adb.Shell('am start com.google.android.launcher')
+              time.sleep(self.TIME_TO_IDLE_SECONDS)
+            break
+      finally:
+        self._device.KillAll(package_info.package, quiet=True)
 
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""

commit 00e15989d6cb9c5e536902972f8c7400803e9787
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 22 12:07:34 2016 -0700

    tools/android/loading: Implements CacheBackend.GetDecodedContentForKey()
    
    HTTP cache is storing into key's index stream 1 the transport layer
    resource binary. However, the resources might be encoded using a
    compression algorithm specified in the Content-Encoding response
    header. This new method takes care of returning decoded binary
    content of the resource if necessary.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1765033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382625}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f5b1e432220b3d8596912dca53bba2fef17841dd

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 1df3010..9d486e9 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -8,6 +8,7 @@
 from datetime import datetime
 import json
 import os
+import re
 import shutil
 import subprocess
 import sys
@@ -37,6 +38,13 @@ OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
 # Default cachetool binary location.
 CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
 
+# Default content_decoder_tool binary location.
+CONTENT_DECODER_TOOL_BIN_PATH = os.path.join(OUT_DIRECTORY,
+                                             'content_decoder_tool')
+
+# Regex used to parse HTTP headers line by line.
+HEADER_PARSING_REGEX = re.compile(r'^(?P<header>\S+):(?P<value>.*)$')
+
 
 def _EnsureCleanCacheDirectory(directory_dest_path):
   """Ensure that a cache directory is created and clean.
@@ -306,6 +314,42 @@ class CacheBackend(object):
     assert process.returncode == 0
     return stdout_data
 
+  def GetDecodedContentForKey(self, key):
+    """Gets a key's decoded content.
+
+    HTTP cache is storing into key's index stream 1 the transport layer resource
+    binary. However, the resources might be encoded using a compression
+    algorithm specified in the Content-Encoding response header. This method
+    takes care of returning decoded binary content of the resource.
+
+    Args:
+      key: The key to access the decoded content.
+
+    Returns:
+      String holding binary content.
+    """
+    response_headers = self.GetStreamForKey(key, 0)
+    content_encoding = None
+    for response_header_line in response_headers.split('\n'):
+      match = HEADER_PARSING_REGEX.match(response_header_line)
+      if not match:
+        continue
+      if match.group('header').lower() == 'content-encoding':
+        content_encoding = match.group('value')
+        break
+    encoded_content = self.GetStreamForKey(key, 1)
+    if content_encoding == None:
+      return encoded_content
+
+    cmd = [CONTENT_DECODER_TOOL_BIN_PATH]
+    cmd.extend([s.strip() for s in content_encoding.split(',')])
+    process = subprocess.Popen(cmd,
+                               stdin=subprocess.PIPE,
+                               stdout=subprocess.PIPE)
+    decoded_content, _ = process.communicate(input=encoded_content)
+    assert process.returncode == 0
+    return decoded_content
+
 
 def ApplyUrlWhitelistToCacheArchive(cache_archive_path,
                                     whitelisted_urls,
@@ -333,18 +377,33 @@ def ApplyUrlWhitelistToCacheArchive(cache_archive_path,
     shutil.rmtree(cache_temp_directory)
 
 
-if __name__ == '__main__':
+def ManualTestMain():
   import argparse
   parser = argparse.ArgumentParser(description='Tests cache back-end.')
-  parser.add_argument('cache_path', type=str)
+  parser.add_argument('cache_archive_path', type=str)
   parser.add_argument('backend_type', type=str, choices=BACKEND_TYPES)
   command_line_args = parser.parse_args()
 
+  cache_path = tempfile.mkdtemp()
+  UnzipDirectoryContent(command_line_args.cache_archive_path, cache_path)
+
   cache_backend = CacheBackend(
-      cache_directory_path=command_line_args.cache_path,
+      cache_directory_path=cache_path,
       cache_backend_type=command_line_args.backend_type)
-  keys = cache_backend.ListKeys()
-  print '{}\'s HTTP response header:'.format(keys[0])
-  print cache_backend.GetStreamForKey(keys[0], 0)
+  keys = sorted(cache_backend.ListKeys())
+  selected_key = None
+  for key in keys:
+    if key.endswith('.js'):
+      selected_key = key
+      break
+  assert selected_key
+  print '{}\'s HTTP response header:'.format(selected_key)
+  print cache_backend.GetStreamForKey(selected_key, 0)
+  print cache_backend.GetDecodedContentForKey(selected_key)
   cache_backend.DeleteKey(keys[1])
   assert keys[1] not in cache_backend.ListKeys()
+  shutil.rmtree(cache_path)
+
+
+if __name__ == '__main__':
+  ManualTestMain()

commit a963dc7c135a18c0f2a7cc3b6b7efb99cd5961b0
Author: droger <droger@chromium.org>
Date:   Tue Mar 22 08:55:13 2016 -0700

    tools/android/loading Add script to deploy chrome
    
    This CL adds a deploy.sh script that packages the
    analyse.py and Chrome for deployment on GCE.
    
    It also updates the startup script and the pip requirements
    to add required dependencies.
    
    Review URL: https://codereview.chromium.org/1809133002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382578}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c84d04ac9fdf9752bd3ea79180df7ba5cca2a6cb

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 6de07d2..522be17 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -21,10 +21,27 @@ When offered, accept to clone the Google Cloud repo.
 
 ## Update or Change the code
 
-Make changes to the code, or simply copy the latest version from Chromium into
-your local Google Cloud repository. Then commit and push:
+Make changes to the code, or copy the latest version from Chromium into your
+local Google Cloud repository:
 
 ```shell
+# Build Chrome
+BUILD_DIR=out/Release
+ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
+
+GCE_DIR=~/dev/clovis/default
+
+# Deploy to GCE
+# CHROME_BUCKET_NAME is the name of the Google Cloud Storage bucket where the
+# Chrome build artifacts will be uploaded, and matches the value of
+# 'bucket_name' in server_config.json.
+./tools/android/loading/gce/deploy.sh $BUILD_DIR $GCE_DIR $CHROME_BUCKET_NAME
+
+cd $GCE_DIR
+
+# git add the relevant files
+
+# commit and push:
 git commit
 git push -u origin master
 ```
@@ -43,9 +60,14 @@ gcloud compute instances create clovis-tracer-1 \
  --zone europe-west1-c \
  --tags clovis-http-server \
  --scopes cloud-platform \
- --metadata-from-file startup-script=default/startup-script.sh
+ --metadata auto-start=true \
+ --metadata-from-file startup-script=tools/android/loading/gce/startup-script.sh
 ```
 
+**Note:** To start an instance without automatically starting the app on it,
+remove the `--metadata auto-start=true` argument. This can be useful when doing
+iterative development on the instance, to be able to restart the app manually.
+
 This should output the IP address of the instance.
 Otherwise the IP address can be retrieved by doing:
 
@@ -53,10 +75,6 @@ Otherwise the IP address can be retrieved by doing:
 gcloud compute instances list
 ```
 
-TODO: allow starting the instance in the cloud without Supervisor. This enables
-iterative development on the instance using SSH, manually starting and stopping
-the app. This can be done using [instance metadata][2].
-
 ## Use the app
 
 Interact with the app on the port 8080 at `http://<instance-ip>:8080`.
@@ -67,7 +85,7 @@ To send a list of URLs to process:
 curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
 ```
 
-where `urls.txt` is a file containing URLs (one per line).
+where `urls.json` is a file containing URLs as a JSON array.
 
 Start the processing by sending a request to `http://<instance-ip>:8080/start`,
 for example:
@@ -101,10 +119,10 @@ pip install -r pip_requirements.txt
 Launch the app:
 
 ```shell
-gunicorn --workers=1 main:app --bind 127.0.0.1:8000
+gunicorn --workers=1 main:app --bind 127.0.0.1:8080
 ```
 
-In your browser, go to `http://localhost:8000` and use the app.
+In your browser, go to `http://localhost:8080` and use the app.
 
 Tear down the local environment:
 
@@ -145,4 +163,3 @@ gcloud compute firewall-rules delete default-allow-http-8080
 ```
 
 [1]: https://cloud.google.com/sdk
-[2]: https://cloud.google.com/compute/docs/startupscript#custom
diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
new file mode 100755
index 0000000..fad1755
--- /dev/null
+++ b/loading/gce/deploy.sh
@@ -0,0 +1,52 @@
+#!/bin/bash
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# This script copies all dependencies required for trace collection.
+# Usage:
+#   deploy.sh builddir outdir bucket
+#
+# Where:
+#   builddir is the build directory for Chrome
+#   outdir is the directory where files are deployed
+#   bucket is the Google Storage bucket where Chrome is uploaded
+
+builddir=$1
+outdir=$2
+bucket=$3
+
+# Copy files from tools/android/loading
+mkdir -p $outdir/tools/android/loading
+cp tools/android/loading/*.py $outdir/tools/android/loading
+cp -r tools/android/loading/gce $outdir/tools/android/loading
+
+# Copy other dependencies
+mkdir $outdir/third_party
+# Use rsync to exclude unwanted files (e.g. the .git directory).
+rsync -av --exclude=".*" --exclude "*.pyc" --delete \
+  third_party/catapult $outdir/third_party
+mkdir $outdir/tools/perf
+cp -r tools/perf/chrome_telemetry_build $outdir/tools/perf
+mkdir -p $outdir/build/android
+cp build/android/devil_chromium.py $outdir/build/android/
+cp build/android/devil_chromium.json $outdir/build/android/
+cp -r build/android/pylib $outdir/build/android/
+
+# Copy the chrome executable to Google Cloud Storage
+chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
+  /tmp/linux.zip
+gsutil cp /tmp/linux.zip gs://$bucket/chrome/linux.zip
+rm /tmp/linux.zip
+gsutil cp $builddir/chrome_sandbox gs://$bucket/chrome/chrome_sandbox
+
+# Upload Chromium revision
+CHROMIUM_REV=$(git merge-base HEAD origin/master)
+cat >/tmp/build_metadata.json << EOF
+{
+  "chromium_rev": "$CHROMIUM_REV"
+}
+EOF
+gsutil cp /tmp/build_metadata.json gs://$bucket/chrome/build_metadata.json
+rm /tmp/build_metadata.json
+
diff --git a/loading/gce/pip_requirements.txt b/loading/gce/pip_requirements.txt
index e2d68b7..7d97e12 100644
--- a/loading/gce/pip_requirements.txt
+++ b/loading/gce/pip_requirements.txt
@@ -1,2 +1,3 @@
 gunicorn==19.4.5
 gcloud==0.10.1
+psutil==4.1.0
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index b6ddde1..dcad7f2 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -14,7 +14,16 @@ PROJECTID=$(curl -s \
 
 # Install dependencies from apt
 apt-get update
-apt-get install -yq git supervisor python-pip python-dev libffi-dev libssl-dev
+# Basic dependencies
+apt-get install -yq git supervisor python-pip python-dev unzip
+# Web server dependencies
+apt-get install -yq libffi-dev libssl-dev
+# Chrome dependencies
+apt-get install -yq libpangocairo-1.0-0 libXcomposite1 libXcursor1 libXdamage1 \
+    libXi6 libXtst6 libnss3 libcups2 libgconf2-4 libXss1 libXrandr2 \
+    libatk1.0-0 libasound2 libgtk2.0-0
+# Trace collection dependencies
+apt-get install -yq xvfb
 
 # Create a pythonapp user. The application will run as this user.
 useradd -m -d /home/pythonapp pythonapp
@@ -22,24 +31,59 @@ useradd -m -d /home/pythonapp pythonapp
 # pip from apt is out of date, so make it update itself and install virtualenv.
 pip install --upgrade pip virtualenv
 
-# Get the source code from the Google Cloud Repository
+# Get the source code from the Google Cloud Repository.
+# It is expected that the contents of this repository have been generated using
+# the tools/android/loading/gce/deploy.sh script.
 # git requires $HOME and it's not set during the startup script.
 export HOME=/root
 git config --global credential.helper gcloud.sh
-git clone https://source.developers.google.com/p/$PROJECTID /opt/app/clovis
+git clone --depth 1 https://source.developers.google.com/p/$PROJECTID \
+    /opt/app/clovis
 
 # Install app dependencies
 virtualenv /opt/app/clovis/env
-/opt/app/clovis/env/bin/pip install -r /opt/app/clovis/pip_requirements.txt
+/opt/app/clovis/env/bin/pip install \
+    -r /opt/app/clovis/tools/android/loading/gce/pip_requirements.txt
+
+# Download Chrome from Google Cloud Storage and unzip it.
+# It is expected that the contents of the bucket have been generated using the
+# tools/android/loading/gce/deploy.sh script.
+STORAGE_BUCKET=`python - <<EOF
+import json
+config_file = "/opt/app/clovis/tools/android/loading/gce/server_config.json"
+with open(config_file) as config:
+  obj=json.load(config);
+  print obj["bucket_name"]
+EOF`
+
+mkdir /opt/app/clovis/out
+gsutil cp gs://$STORAGE_BUCKET/chrome/* /opt/app/clovis/out/
+unzip /opt/app/clovis/out/linux.zip -d /opt/app/clovis/out/
+
+# Install the Chrome sandbox
+cp /opt/app/clovis/out/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
+chown root:root /usr/local/sbin/chrome-devel-sandbox
+chmod 4755 /usr/local/sbin/chrome-devel-sandbox
+export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
 
 # Make sure the pythonapp user owns the application code
 chown -R pythonapp:pythonapp /opt/app
 
+# Check if auto-start is enabled
+AUTO_START=$(curl -s \
+    "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
+    -H "Metadata-Flavor: Google")
+
+# Exit early if auto start is not enabled.
+if [-z "$AUTO_START"]; then
+  exit 1
+fi
+
 # Configure supervisor to start gunicorn inside of our virtualenv and run the
 # applicaiton.
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
-directory=/opt/app/clovis
+directory=/opt/app/clovis/tools/android/loading/gce
 command=/opt/app/clovis/env/bin/gunicorn --workers=1 main:app \
   --bind 0.0.0.0:8080
 autostart=true
@@ -47,7 +91,7 @@ autorestart=true
 user=pythonapp
 # Environment variables ensure that the application runs inside of the
 # configured virtualenv.
-environment=VIRTUAL_ENV="/opt/app/env/clovis",PATH="/opt/app/clovis/env/bin",\
+environment=VIRTUAL_ENV="/opt/app/clovis/env",PATH="/opt/app/clovis/env/bin",\
     HOME="/home/pythonapp",USER="pythonapp"
 stdout_logfile=syslog
 stderr_logfile=syslog

commit 9fc49a72b0f12a31992b5170838133fe15fbba35
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 22 04:40:44 2016 -0700

    Clovis: rewrite user satisfaction lens.
    
    This rewrites the existing user satisfaction lens to output a list of request
    ids rather than the old graph filter approach. In addition, we create a
    hierarchy of lenses for all of the various first paint metrics.
    
    Review URL: https://codereview.chromium.org/1814973004
    
    Cr-Original-Commit-Position: refs/heads/master@{#382548}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 49220fb35e2fc3ae4ce364bef2405e0e029e5fa5

diff --git a/loading/tracing.py b/loading/tracing.py
index 8a07a2b..5d0a647 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -366,6 +366,21 @@ class Event(object):
     return ''.join([str(self._tracing_event),
                     '[%s,%s]' % (self.start_msec, self.end_msec)])
 
+  def Matches(self, category, name):
+    """Match tracing events.
+
+    Args:
+      category: a tracing category (event['cat']).
+      name: the tracing event name (event['name']).
+
+    Returns:
+      True if the event matches and False otherwise.
+    """
+    if name != self.name:
+      return False
+    categories = self.category.split(',')
+    return category in categories
+
   def IsIndexable(self):
     """True iff the event can be indexed by time."""
     return self._synthetic or self.type not in [
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index a3bea3f..1bf0273 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -382,6 +382,23 @@ class IntervalTreeTestCase(unittest.TestCase):
     self.assertEquals(3 + 10, len(tree.OverlappingEvents(450, 550)))
     self.assertEquals(8 + 10, len(tree.OverlappingEvents(450, 800)))
 
+  def testEventMatches(self):
+    event = Event({'name': 'foo',
+                   'cat': 'bar',
+                   'ph': 'X',
+                   'ts': 0, 'dur': 0})
+    self.assertTrue(event.Matches('bar', 'foo'))
+    self.assertFalse(event.Matches('bar', 'biz'))
+    self.assertFalse(event.Matches('biz', 'foo'))
+
+    event = Event({'name': 'foo',
+                   'cat': 'bar,baz,bizbiz',
+                   'ph': 'X',
+                   'ts': 0, 'dur': 0})
+    self.assertTrue(event.Matches('bar', 'foo'))
+    self.assertTrue(event.Matches('baz', 'foo'))
+    self.assertFalse(event.Matches('bar', 'biz'))
+    self.assertFalse(event.Matches('biz', 'foo'))
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 0b23342..0115f8c 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -2,50 +2,136 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Identifies a user satisfied event, and marks all the relationship of all
-model events accordingly.
-"""
+"""Identifies key events related to user satisfaction.
 
+Several lenses are defined, for example FirstTextPaintLens and
+FirstSignificantPaintLens.
+"""
 import logging
+import operator
+
 
+class _UserSatisfiedLens(object):
+  """A base class for all user satisfaction metrics.
 
-class UserSatisfiedLens(object):
-  def __init__(self, trace, graph):
+  All of these work by identifying a user satisfaction event from the trace, and
+  then building a set of request ids whose loading is needed to achieve that
+  event. Subclasses need only provide the time computation. The base class will
+  use that to construct the request ids.
+  """
+  def __init__(self, trace):
     """Initialize the lens.
 
     Args:
       trace: (LoadingTrace) the trace to use in the analysis.
-      graph: (ResourceGraph) the graph to annotate, using the current filter set
-        on the graph.
     """
-    satisfied_time = self._GetFirstContentfulPaintTime(trace.tracing_track)
-    self._satisfied_nodes = set(self._GetNodesAfter(
-        graph.Nodes(), satisfied_time))
+    self._satisfied_msec = None
+    self._event_msec = None
+    self._CalculateTimes(trace.tracing_track)
+    critical_requests = self._RequestsBefore(
+        trace.request_track, self._satisfied_msec)
+    self._critical_request_ids = set(rq.request_id for rq in critical_requests)
+    if critical_requests:
+      last_load = max(rq.end_msec for rq in critical_requests)
+    else:
+      last_load = float('inf')
+    self._postload_msec = self._event_msec - last_load
 
-  def Filter(self, node):
-    """A ResourceGraph filter.
+  def CriticalRequests(self):
+    """Request ids of critical requests.
 
-    Meant to be used in some_graph.Set(node_filter=this_lens.Filter).
+    Returns:
+      A set of request ids (as strings) of an estimate of all requests that are
+      necessary for the user satisfaction defined by this class.
+    """
+    return self._critical_request_ids
 
-    Args:
-      node: a ResourceGraph NodeInfo.
+  def PostloadTimeMsec(self):
+    """Return postload time.
+
+    The postload time is an estimate of the amount of time needed by chrome to
+    transform the critical results into the satisfying event.
 
     Returns:
-      True iff the node occurred before user satisfaction occurred.
+      Postload time in milliseconds.
     """
-    return node not in self._satisfied_nodes
+    return self._postload_msec
+
+  def _CalculateTimes(self, tracing_track):
+    """Subclasses should implement to set _satisfied_msec and _event_msec."""
+    raise NotImplementedError
+
+  @classmethod
+  def _RequestsBefore(cls, request_track, time_ms):
+    return [rq for rq in request_track.GetEvents()
+            if rq.end_msec <= time_ms]
+
+
+class _FirstEventLens(_UserSatisfiedLens):
+  """Helper abstract subclass that defines users first event manipulations."""
+  # pylint can't handle abstract subclasses.
+  # pylint: disable=abstract-method
 
-  # TODO(mattcary): hoist to base class?
   @classmethod
-  def _GetNodesAfter(cls, nodes, time):
-    return (n for n in nodes if n.StartTime() >= time)
+  def _ExtractFirstTiming(cls, times):
+    if not times:
+      return float('inf')
+    if len(times) != 1:
+      # TODO(mattcary): in some cases a trace has two first paint events. Why?
+      logging.error('%d %s with spread of %s', len(times),
+                    str(cls), max(times) - min(times))
+    return float(min(times))
+
+
+class FirstTextPaintLens(_FirstEventLens):
+  """Define satisfaction by the first text paint.
 
-  def _GetFirstContentfulPaintTime(self, tracing_track):
+  This event is taken directly from a trace.
+  """
+  def _CalculateTimes(self, tracing_track):
     first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if (e.tracing_event['name'] == 'firstContentfulPaint' and
-                        'blink.user_timing' in e.tracing_event['cat'])]
-    if len(first_paints) != 1:
-      # TODO(mattcary): in some cases a trace has two contentful paints. Why?
-      logging.error('%d first paints with spread of %s', len(first_paints),
-                    max(first_paints) - min(first_paints))
-    return min(first_paints)
+                    if e.Matches('blink.user_timing', 'firstPaint')]
+    self._satisfied_msec = self._event_msec = \
+        self._ExtractFirstTiming(first_paints)
+
+
+class FirstContentfulPaintLens(_FirstEventLens):
+  """Define satisfaction by the first contentful paint.
+
+  This event is taken directly from a trace. Internally to chrome it's computed
+  by filtering out things like background paint from firstPaint.
+  """
+  def _CalculateTimes(self, tracing_track):
+    first_paints = [e.start_msec for e in tracing_track.GetEvents()
+                    if e.Matches('blink.user_timing', 'firstContentfulPaint')]
+    self._satisfied_msec = self._event_msec = \
+       self._ExtractFirstTiming(first_paints)
+
+
+class FirstSignificantPaintLens(_FirstEventLens):
+  """Define satisfaction by the first paint after a big layout change.
+
+  Our satisfaction time is that of the layout change, as all resources must have
+  been loaded to compute the layout. Our event time is that of the next paint as
+  that is the observable event.
+  """
+  FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
+
+  def _CalculateTimes(self, tracing_track):
+    sync_paint_times = []
+    layouts = []  # (layout item count, msec).
+    for e in tracing_track.GetEvents():
+      # TODO(mattcary): is this the right paint event? Check if synchronized
+      # paints appear at the same time as the first*Paint events, above.
+      if e.Matches('blink', 'FrameView::SynchronizedPaint'):
+        sync_paint_times.append(e.start_msec)
+      if ('counters' in e.args and
+          self.FIRST_LAYOUT_COUNTER in e.args['counters']):
+        layouts.append((e.args['counters'][self.FIRST_LAYOUT_COUNTER],
+                        e.start_msec))
+    assert layouts, ('No layout events, was the disabled-by-default-blink'
+                     '.debug.layout category enabled?')
+    layouts.sort(key=operator.itemgetter(0), reverse=True)
+    self._satisfied_msec = layouts[0][1]
+    self._event_msec = self._ExtractFirstTiming([
+        min(t for t in sync_paint_times if t > self._satisfied_msec)])
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 7b16f23..29cf3d5 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -4,12 +4,15 @@
 
 import unittest
 
-import loading_model
 import request_track
 import test_utils
 import user_satisfied_lens
 
 class UserSatisfiedLensTestCase(unittest.TestCase):
+  # We track all times in milliseconds, but raw trace events are in
+  # microseconds.
+  MILLI_TO_MICRO = 1000
+
   def setUp(self):
     super(UserSatisfiedLensTestCase, self).setUp()
     self._request_index = 1
@@ -18,7 +21,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     timestamp_sec = float(timestamp_msec) / 1000
     rq = request_track.Request.FromJsonDict({
         'url': 'http://bla-%s-.com' % timestamp_msec,
-        'request_id': '1234.%s' % self._request_index,
+        'request_id': '0.%s' % self._request_index,
         'frame_id': '123.%s' % timestamp_msec,
         'initiator': {'type': 'other'},
         'timestamp': timestamp_sec,
@@ -29,28 +32,86 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     self._request_index += 1
     return rq
 
-  def testUserSatisfiedLens(self):
-    # We track all times in milliseconds, but raw trace events are in
-    # microseconds.
-    MILLI_TO_MICRO = 1000
+  def testFirstContentfulPaintLens(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'blink.some_other_user_timing',
                        'name': 'firstContentfulPaint'},
-                      {'ts': 9 * MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstDiscontentPaint'},
-                      {'ts': 12 * MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstContentfulPaint'},
-                      {'ts': 22 * MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstContentfulPaint'}])
-    graph = loading_model.ResourceGraph(loading_trace)
-    lens = user_satisfied_lens.UserSatisfiedLens(loading_trace, graph)
-    for n in graph.Nodes():
-      if n.Request().frame_id == '123.20':
-        self.assertFalse(lens.Filter(n))
-      else:
-        self.assertTrue(lens.Filter(n))
+    lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(1, lens.PostloadTimeMsec())
+
+  def testCantGetNoSatisfaction(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'not_my_cat',
+                       'name': 'someEvent'}])
+    lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequests())
+    self.assertEqual(float('inf'), lens.PostloadTimeMsec())
+
+  def testFirstTextPaintLens(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink.some_other_user_timing',
+                       'name': 'firstPaint'},
+                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstishPaint'},
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstPaint'},
+                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstPaint'}])
+    lens = user_satisfied_lens.FirstTextPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(1, lens.PostloadTimeMsec())
+
+  def testFirstSignificantPaintLens(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10),
+         self._RequestAt(15), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink',
+                       'name': 'firstPaint'},
+                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'FrameView::SynchronizedPaint'},
+                      {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink',
+                       'name': 'FrameView::SynchronizedPaint'},
+                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink',
+                       'name': 'FrameView::SynchronizedPaint'},
+
+                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'foobar', 'name': 'biz',
+                       'args': {'counters': {
+                           'LayoutObjectsThatHadNeverHadLayout': 10
+                       } } },
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'foobar', 'name': 'biz',
+                       'args': {'counters': {
+                           'LayoutObjectsThatHadNeverHadLayout': 12
+                       } } },
+                      {'ts': 15 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'foobar', 'name': 'biz',
+                       'args': {'counters': {
+                           'LayoutObjectsThatHadNeverHadLayout': 10
+                       } } } ])
+    lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(7, lens.PostloadTimeMsec())

commit 244714577a0d69d32ca5bdc152c145b81ae8a95d
Author: lizeb <lizeb@chromium.org>
Date:   Tue Mar 22 03:18:19 2016 -0700

    clovis: Identify prefetchable resources from dependencies and tracing.
    
    Resources that are discoverable by the preload scanner are behind a
    "parser" dependency. However, this is a superset of the actual
    discoverable resources. Use tracing to find the actual resources, that
    are then matched to requests.
    
    Review URL: https://codereview.chromium.org/1813723002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382542}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7f2bd1dc11f2e24416a0c0380bf4d908d4e82b62

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index f6165d9..bf6610d 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -29,8 +29,8 @@ class ActivityLens(object):
     self._trace = trace
     events = trace.tracing_track.GetEvents()
     self._renderer_main_pid_tid = self._GetRendererMainThreadId(events)
-    self._tracing = self._trace.tracing_track.TracingTrackForThread(
-        self._renderer_main_pid_tid)
+    self._tracing = self._trace.tracing_track.Filter(
+        *self._renderer_main_pid_tid)
 
   @classmethod
   def _GetRendererMainThreadId(cls, events):
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
new file mode 100644
index 0000000..76f7723
--- /dev/null
+++ b/loading/prefetch_view.py
@@ -0,0 +1,116 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Models the effect of prefetching resources from a loading trace.
+
+For example, this can be used to evaluate NoState Prefetch
+(https://goo.gl/B3nRUR).
+
+When executed as a script, takes a trace as a command-line arguments and shows
+how many requests were prefetched.
+"""
+
+import itertools
+import operator
+
+import loading_trace
+import request_dependencies_lens
+
+
+class PrefetchSimulationView(object):
+  """Simulates the effect of prefetching resources discoverable by the preload
+  scanner.
+  """
+  def __init__(self, trace, dependencies_lens):
+    """Initializes an instance of PrefetchSimulationView.
+
+    Args:
+      trace: (LoadingTrace) a loading trace.
+      dependencies_lens: (RequestDependencyLens) request dependencies.
+    """
+    self.trace = trace
+    self.dependencies_lens = dependencies_lens
+    self._resource_events = self.trace.tracing_track.Filter(
+        categories=set([u'blink.net']))
+    assert len(self._resource_events.GetEvents()) > 0,\
+            'Was the "blink.net" category enabled at trace collection time?"'
+
+  def ParserDiscoverableRequests(self, request, recurse=False):
+    """Returns a list of requests discovered by the parser from a given request.
+
+    Args:
+      request: (Request) Root request.
+
+    Returns:
+      [Request]
+    """
+    # TODO(lizeb): handle the recursive case.
+    assert not recurse
+    discoverable_requests = [request]
+    first_request = self.dependencies_lens.GetRedirectChain(request)[-1]
+    deps = self.dependencies_lens.GetRequestDependencies()
+    for (first, second, reason) in deps:
+      if first.request_id == first_request.request_id and reason == 'parser':
+        discoverable_requests.append(second)
+    return discoverable_requests
+
+  def ExpandRedirectChains(self, requests):
+    return list(itertools.chain.from_iterable(
+        [self.dependencies_lens.GetRedirectChain(r) for r in requests]))
+
+  def PreloadedRequests(self, request):
+    """Returns the requests that have been preloaded from a given request.
+
+    This list is the set of request that are:
+    - Discoverable by the parser
+    - Found in the trace log.
+
+    Before looking for dependencies, this follows the redirect chain.
+
+    Args:
+      request: (Request) Root request.
+
+    Returns:
+      A list of Request. Does not include the root request. This list is a
+      subset of the one returned by ParserDiscoverableRequests().
+    """
+    # Preload step events are emitted in ResourceFetcher::preloadStarted().
+    preload_step_events = filter(
+        lambda e:  e.args.get('step') == 'Preload',
+        self._resource_events.GetEvents())
+    preloaded_urls = set()
+    for preload_step_event in preload_step_events:
+      preload_event = self._resource_events.EventFromStep(preload_step_event)
+      if preload_event:
+        preloaded_urls.add(preload_event.args['url'])
+    parser_requests = self.ParserDiscoverableRequests(request)
+    preloaded_root_requests = filter(
+        lambda r: r.url in preloaded_urls, parser_requests)
+    # We can actually fetch the whole redirect chain.
+    return [request] + list(itertools.chain.from_iterable(
+        [self.dependencies_lens.GetRedirectChain(r)
+         for r in preloaded_root_requests]))
+
+
+def _PrintSummary(prefetch_view):
+  requests = prefetch_view.trace.request_track.GetEvents()
+  first_request = prefetch_view.trace.request_track.GetEvents()[0]
+  parser_requests = prefetch_view.ExpandRedirectChains(
+      prefetch_view.ParserDiscoverableRequests(first_request))
+  preloaded_requests = prefetch_view.ExpandRedirectChains(
+      prefetch_view.PreloadedRequests(first_request))
+  print '%d requests, %d parser from the main request, %d preloaded' % (
+      len(requests), len(parser_requests), len(preloaded_requests))
+
+
+def main(filename):
+  trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+  dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
+  prefetch_view = PrefetchSimulationView(trace, dependencies_lens)
+  _PrintSummary(prefetch_view)
+
+
+if __name__ == '__main__':
+  import sys
+  main(sys.argv[1])
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
new file mode 100644
index 0000000..f7210bc
--- /dev/null
+++ b/loading/prefetch_view_unittest.py
@@ -0,0 +1,62 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import prefetch_view
+import request_dependencies_lens
+from request_dependencies_lens_unittest import TestRequests
+
+
+class PrefetchSimulationViewTestCase(unittest.TestCase):
+  def setUp(self):
+    super(PrefetchSimulationViewTestCase, self).setUp()
+    self._SetUp()
+
+  def testExpandRedirectChains(self):
+    self.assertListEqual(
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST],
+        self.prefetch_view.ExpandRedirectChains(
+            [TestRequests.FIRST_REDIRECT_REQUEST]))
+
+  def testParserDiscoverableRequests(self):
+    first_request = TestRequests.FIRST_REDIRECT_REQUEST
+    discovered_requests = self.prefetch_view.ParserDiscoverableRequests(
+        first_request)
+    self.assertListEqual(
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_UNRELATED_FRAME], discovered_requests)
+
+  def testPreloadedRequests(self):
+    first_request = TestRequests.FIRST_REDIRECT_REQUEST
+    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    self.assertListEqual([first_request], preloaded_requests)
+    self._SetUp(
+        [{'args': {'url': 'http://bla.com/nyancat.js'},
+          'cat': 'blink.net', 'id': '0xaf9f14fa9dd6c314', 'name': 'Resource',
+          'ph': 'X', 'ts': 1, 'dur': 120, 'pid': 12, 'tid': 12},
+         {'args': {'step': 'Preload'}, 'cat': 'blink.net',
+          'id': '0xaf9f14fa9dd6c314', 'name': 'Resource', 'ph': 'T',
+          'ts': 12, 'pid': 12, 'tid': 12}])
+    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    self.assertListEqual([TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_UNRELATED_FRAME], preloaded_requests)
+
+  def _SetUp(self, added_trace_events=None):
+    trace_events = [
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'blink.net'}]
+    if added_trace_events is not None:
+      trace_events += added_trace_events
+    self.trace = TestRequests.CreateLoadingTrace(trace_events)
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        self.trace)
+    self.prefetch_view = prefetch_view.PrefetchSimulationView(
+        self.trace, dependencies_lens)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 4a72a4f..1689547 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -30,6 +30,7 @@ class RequestDependencyLens(object):
     self._requests = self.loading_trace.request_track.GetEvents()
     self._requests_by_id = {r.request_id: r for r in self._requests}
     self._requests_by_url = collections.defaultdict(list)
+    self._deps = None
     for request in self._requests:
       self._requests_by_url[request.url].append(request)
     self._frame_to_parent = {}
@@ -45,12 +46,36 @@ class RequestDependencyLens(object):
       request_track.Request, and reason is in DEPENDENCIES. The second request
       depends on the first one, with the listed reason.
     """
-    deps = []
+    self._ComputeRequestDependencies()
+    return copy.copy(self._deps)
+
+  def GetRedirectChain(self, request):
+    """Returns the whole redirect chain for a given request.
+
+    Note that this misses some JS-based redirects.
+
+    Returns:
+      A list of request, containing the request passed as a parameter.
+    """
+    self._ComputeRequestDependencies()
+    chain = [request]
+    while True:
+      for (first_request, second_request, why) in self._deps:
+        if first_request == request and why == 'redirect':
+          chain.append(second_request)
+          request = second_request
+          break
+      else:
+        return chain
+
+  def _ComputeRequestDependencies(self):
+    if self._deps is not None:
+      return
+    self._deps = []
     for request in self._requests:
       dependency = self._GetDependency(request)
       if dependency:
-        deps.append(dependency)
-    return deps
+        self._deps.append(dependency)
 
   def _GetDependency(self, request):
     """Returns (first, second, reason), or None.
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 1429a0e..773af44 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -11,77 +11,122 @@ from request_track import (Request, TimingFromDict)
 import test_utils
 
 
-class RequestDependencyLensTestCase(unittest.TestCase):
-  _REDIRECT_REQUEST = Request.FromJsonDict(
+class TestRequests(object):
+  FIRST_REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com', 'request_id': '1234.redirect.1',
        'initiator': {'type': 'other'},
+       'timestamp': 0.5, 'timing': TimingFromDict({})})
+  SECOND_REDIRECT_REQUEST = Request.FromJsonDict(
+      {'url': 'http://bla.com/redirect1', 'request_id': '1234.redirect.2',
+       'initiator': {'type': 'redirect',
+                     'initiating_request': '1234.redirect.1'},
        'timestamp': 1, 'timing': TimingFromDict({})})
-  _REDIRECTED_REQUEST = Request.FromJsonDict({
-      'url': 'http://bla.com',
+  REDIRECTED_REQUEST = Request.FromJsonDict({
+      'url': 'http://bla.com/index.html',
       'request_id': '1234.1',
       'frame_id': '123.1',
       'initiator': {'type': 'redirect',
-                    'initiating_request': '1234.redirect.1'},
+                    'initiating_request': '1234.redirect.2'},
       'timestamp': 2,
       'timing': TimingFromDict({})})
-  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
-                                   'request_id': '1234.1',
-                                   'frame_id': '123.1',
-                                   'initiator': {'type': 'other'},
-                                   'timestamp': 2,
-                                   'timing': TimingFromDict({})})
-  _JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
-                                      'request_id': '1234.12',
-                                      'frame_id': '123.1',
-                                      'initiator': {'type': 'parser',
-                                                    'url': 'http://bla.com'},
-                                      'timestamp': 3,
-                                      'timing': TimingFromDict({})})
-  _JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
+  REQUEST = Request.FromJsonDict({'url': 'http://bla.com/index.html',
+                                  'request_id': '1234.1',
+                                  'frame_id': '123.1',
+                                  'initiator': {'type': 'other'},
+                                  'timestamp': 2,
+                                  'timing': TimingFromDict({})})
+  JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
+                                     'request_id': '1234.12',
+                                     'frame_id': '123.123',
+                                     'initiator': {
+                                         'type': 'parser',
+                                         'url': 'http://bla.com/index.html'},
+                                     'timestamp': 3,
+                                     'timing': TimingFromDict({})})
+  JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
        'request_id': '1234.42',
        'frame_id': '123.13',
        'initiator': {'type': 'parser',
-                     'url': 'http://bla.com'},
+                     'url': 'http://bla.com/index.html'},
        'timestamp': 4, 'timing': TimingFromDict({})})
-  _JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
+  JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
-       'request_id': '1234.42',
+       'request_id': '1234.56',
        'frame_id': '123.99',
        'initiator': {'type': 'parser',
-                     'url': 'http://bla.com'},
+                     'url': 'http://bla.com/index.html'},
        'timestamp': 5, 'timing': TimingFromDict({})})
-  _JS_REQUEST_2 = Request.FromJsonDict(
+  JS_REQUEST_2 = Request.FromJsonDict(
       {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
-       'frame_id': '123.1',
+       'frame_id': '123.123',
        'initiator': {'type': 'script',
                      'stack': {'callFrames': [
                          {'url': 'unknown'},
                          {'url': 'http://bla.com/nyancat.js'}]}},
        'timestamp': 10, 'timing': TimingFromDict({})})
-  _PAGE_EVENTS = [{'method': 'Page.frameAttached',
-                   'frame_id': '123.13', 'parent_frame_id': '123.1'}]
+  PAGE_EVENTS = [{'method': 'Page.frameAttached',
+                   'frame_id': '123.13', 'parent_frame_id': '123.1'},
+                 {'method': 'Page.frameAttached',
+                  'frame_id': '123.123', 'parent_frame_id': '123.1'}]
+
+  @classmethod
+  def CreateLoadingTrace(cls, trace_events=None):
+    return test_utils.LoadingTraceFromEvents(
+        [cls.FIRST_REDIRECT_REQUEST, cls.SECOND_REDIRECT_REQUEST,
+         cls.REDIRECTED_REQUEST, cls.REQUEST, cls.JS_REQUEST, cls.JS_REQUEST_2,
+         cls.JS_REQUEST_OTHER_FRAME, cls.JS_REQUEST_UNRELATED_FRAME],
+        cls.PAGE_EVENTS, trace_events)
 
+
+class RequestDependencyLensTestCase(unittest.TestCase):
   def testRedirectDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST])
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
-    self.assertEquals(1, len(deps))
+    self.assertEquals(2, len(deps))
     (first, second, reason) = deps[0]
     self.assertEquals('redirect', reason)
-    self.assertEquals(self._REDIRECT_REQUEST.request_id, first.request_id)
-    self.assertEquals(self._REQUEST.request_id, second.request_id)
+    self.assertEquals(TestRequests.FIRST_REDIRECT_REQUEST.request_id,
+                      first.request_id)
+    self.assertEquals(TestRequests.SECOND_REDIRECT_REQUEST.request_id,
+                      second.request_id)
+    (first, second, reason) = deps[1]
+    self.assertEquals('redirect', reason)
+    self.assertEquals(TestRequests.SECOND_REDIRECT_REQUEST.request_id,
+                      first.request_id)
+    self.assertEquals(TestRequests.REQUEST.request_id, second.request_id)
+
+  def testGetRedirectChain(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST])
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    whole_chain = [TestRequests.FIRST_REDIRECT_REQUEST,
+                   TestRequests.SECOND_REDIRECT_REQUEST,
+                   TestRequests.REDIRECTED_REQUEST]
+    chain = request_dependencies_lens.GetRedirectChain(
+        TestRequests.FIRST_REDIRECT_REQUEST)
+    self.assertListEqual(whole_chain, chain)
+    chain = request_dependencies_lens.GetRedirectChain(
+        TestRequests.SECOND_REDIRECT_REQUEST)
+    self.assertListEqual(whole_chain[1:], chain)
+    chain = request_dependencies_lens.GetRedirectChain(
+        TestRequests.REDIRECTED_REQUEST)
+    self.assertEquals(whole_chain[2:], chain)
 
   def testScriptDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST, self._JS_REQUEST_2])
+        [TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.JS_REQUEST.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def testAsyncScriptDependency(self):
     JS_REQUEST_WITH_ASYNC_STACK = Request.FromJsonDict(
@@ -93,65 +138,76 @@ class RequestDependencyLensTestCase(unittest.TestCase):
                                       {'url': 'http://bla.com/nyancat.js'}]}}},
          'timestamp': 10, 'timing': TimingFromDict({})})
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
+        [TestRequests.JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
-        deps[0], self._JS_REQUEST.request_id,
+        deps[0], TestRequests.JS_REQUEST.request_id,
         JS_REQUEST_WITH_ASYNC_STACK.request_id, 'script')
 
   def testParserDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REQUEST, self._JS_REQUEST])
+        [TestRequests.REQUEST, TestRequests.JS_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+        TestRequests.REQUEST.request_id, TestRequests.JS_REQUEST.request_id,
+        'parser')
 
   def testSeveralDependencies(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST, self._JS_REQUEST,
-         self._JS_REQUEST_2])
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST,
+         TestRequests.REDIRECTED_REQUEST,
+         TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
-    self.assertEquals(3, len(deps))
+    self.assertEquals(4, len(deps))
     self._AssertDependencyIs(
-        deps[0], self._REDIRECT_REQUEST.request_id, self._REQUEST.request_id,
-        'redirect')
+        deps[0], TestRequests.FIRST_REDIRECT_REQUEST.request_id,
+        TestRequests.SECOND_REDIRECT_REQUEST.request_id, 'redirect')
     self._AssertDependencyIs(
-        deps[1],
-        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+        deps[1], TestRequests.SECOND_REDIRECT_REQUEST.request_id,
+        TestRequests.REQUEST.request_id, 'redirect')
     self._AssertDependencyIs(
         deps[2],
-        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.REQUEST.request_id, TestRequests.JS_REQUEST.request_id,
+        'parser')
+    self._AssertDependencyIs(
+        deps[3],
+        TestRequests.JS_REQUEST.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def testDependencyDifferentFrame(self):
     """Checks that a more recent request from another frame is ignored."""
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
+        [TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.JS_REQUEST.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def testDependencySameParentFrame(self):
     """Checks that a more recent request from an unrelated frame is ignored
     if there is one from a related frame."""
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
-         self._JS_REQUEST_2], self._PAGE_EVENTS)
+        [TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_UNRELATED_FRAME, TestRequests.JS_REQUEST_2],
+        TestRequests.PAGE_EVENTS)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._JS_REQUEST_OTHER_FRAME.request_id,
-        self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.JS_REQUEST_OTHER_FRAME.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def _AssertDependencyIs(
       self, dep, first_request_id, second_request_id, reason):
diff --git a/loading/test_utils.py b/loading/test_utils.py
index f1c80c2..38737c0 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -125,7 +125,7 @@ def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
   request = FakeRequestTrack(requests)
   page_event_track = FakePageTrack(page_events if page_events else [])
-  if trace_events:
+  if trace_events is not None:
     tracing_track = tracing.TracingTrack(None)
     tracing_track.Handle('Tracing.dataCollected',
                          {'params': {'value': [e for e in trace_events]}})
diff --git a/loading/tracing.py b/loading/tracing.py
index 7403a9d..8a07a2b 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -91,19 +91,24 @@ class TracingTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
-  def TracingTrackForThread(self, pid_tid):
-    """Returns a new TracingTrack with only the events from a given thread.
+  def Filter(self, pid=None, tid=None, categories=None):
+    """Returns a new TracingTrack with a subset of the events.
 
     Args:
-      pid_tid: ((int, int) PID and TID.
-
-    Returns:
-      A new instance of TracingTrack.
+      pid: (int or None) Selects events from this PID.
+      tid: (int or None) Selects events from this TID.
+      categories: (set([str]) or None) Selects events belonging to one of the
+                  categories.
     """
-    (pid, tid) = pid_tid
-    events = [e for e in self._events
-              if (e.tracing_event['pid'] == pid
-                  and e.tracing_event['tid'] == tid)]
+    events = self._events
+    if pid is not None:
+      events = filter(lambda e : e.tracing_event['pid'] == pid, events)
+    if tid is not None:
+      events = filter(lambda e : e.tracing_event['tid'] == tid, events)
+    if categories is not None:
+      events = filter(
+          lambda e : set(e.category.split(',')).intersection(categories),
+          events)
     tracing_track = TracingTrack(None)
     tracing_track._events = events
     return tracing_track
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 8d946b7..a3bea3f 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -287,14 +287,35 @@ class TracingTrackTestCase(unittest.TestCase):
     with self.assertRaises(AssertionError):
       self.track.EventFromStep(no_step)
 
-  def testTracingTrackForThread(self):
+  def testFilterPidTid(self):
     self._HandleEvents(self._EVENTS)
-    tracing_track = self.track.TracingTrackForThread((2, 1))
+    tracing_track = self.track.Filter(2, 1)
     self.assertTrue(tracing_track is not self.track)
     self.assertEquals(4, len(tracing_track.GetEvents()))
-    tracing_track = self.track.TracingTrackForThread((2, 42))
+    tracing_track = self.track.Filter(2, 42)
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
+  def testFilterCategories(self):
+    events = [
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'A'},
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'B'},
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'C,D'},
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'A,B,C,D'}]
+    self._HandleEvents(events)
+    tracing_events = self.track.GetEvents()
+    self.assertEquals(4, len(tracing_events))
+    filtered_events = self.track.Filter(categories=None).GetEvents()
+    self.assertListEqual(tracing_events, filtered_events)
+    filtered_events = self.track.Filter(categories=set(['A'])).GetEvents()
+    self.assertEquals(2, len(filtered_events))
+    self.assertListEqual([tracing_events[0], tracing_events[3]],
+                         filtered_events)
+    filtered_events = self.track.Filter(categories=set(['Z'])).GetEvents()
+    self.assertEquals(0, len(filtered_events))
+    filtered_events = self.track.Filter(categories=set(['B', 'C'])).GetEvents()
+    self.assertEquals(3, len(filtered_events))
+    self.assertListEqual(tracing_events[1:], filtered_events)
+
   def _HandleEvents(self, events):
     self.track.Handle('Tracing.dataCollected', {'params': {'value': [
         self.EventToMicroseconds(e) for e in events]}})

commit a47e8ad4e144934870395036d13cdff39148c98d
Author: gabadie <gabadie@chromium.org>
Date:   Mon Mar 21 13:19:00 2016 -0700

    sandwich: Make it work on desktop.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1812053002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382368}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f1d603801466887e509cb7c5b3fd2fcdd5da4b61

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index bdbe1e4..1df3010 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -38,6 +38,19 @@ OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
 CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
 
 
+def _EnsureCleanCacheDirectory(directory_dest_path):
+  """Ensure that a cache directory is created and clean.
+
+  Args:
+    directory_dest_path: Path of the cache directory to ensure cleanliness.
+  """
+  if os.path.isdir(directory_dest_path):
+    shutil.rmtree(directory_dest_path)
+  elif not os.path.isdir(os.path.dirname(directory_dest_path)):
+    os.makedirs(os.path.dirname(directory_dest_path))
+  assert not os.path.exists(directory_dest_path)
+
+
 def _RemoteCacheDirectory():
   """Returns the path of the cache directory's on the remote device."""
   return '/data/data/{}/cache/Cache'.format(
@@ -182,9 +195,7 @@ def UnzipDirectoryContent(archive_path, directory_dest_path):
     archive_path: Archive's path to unzip.
     directory_dest_path: Directory destination path.
   """
-  if not os.path.exists(directory_dest_path):
-    os.makedirs(directory_dest_path)
-
+  _EnsureCleanCacheDirectory(directory_dest_path)
   with zipfile.ZipFile(archive_path) as zip_input:
     timestamps = None
     for file_archive_name in zip_input.namelist():
@@ -207,6 +218,19 @@ def UnzipDirectoryContent(archive_path, directory_dest_path):
       os.utime(output_path, (stats['atime'], stats['mtime']))
 
 
+def CopyCacheDirectory(directory_src_path, directory_dest_path):
+  """Copies a cache directory recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    directory_src_path: Path of the cache directory source.
+    directory_dest_path: Path of the cache directory destination.
+  """
+  assert os.path.isdir(directory_src_path)
+  _EnsureCleanCacheDirectory(directory_dest_path)
+  shutil.copytree(directory_src_path, directory_dest_path)
+
+
 class CacheBackend(object):
   """Takes care of reading and deleting cached keys.
   """
diff --git a/loading/chrome_cache_unittest.py b/loading/chrome_cache_unittest.py
new file mode 100644
index 0000000..051a938
--- /dev/null
+++ b/loading/chrome_cache_unittest.py
@@ -0,0 +1,121 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import shutil
+import tempfile
+import unittest
+
+import chrome_cache
+
+
+LOADING_DIR = os.path.dirname(os.path.abspath(__file__))
+THIS_BASEMAME = os.path.basename(__file__)
+
+
+class CacheDirectoryTest(unittest.TestCase):
+  def setUp(self):
+    self._temp_dir = tempfile.mkdtemp()
+
+  def tearDown(self):
+    shutil.rmtree(self._temp_dir)
+
+  def GetTempPath(self, temp_name):
+    return os.path.join(self._temp_dir, temp_name)
+
+  def CreateNewGarbageFile(self, file_path):
+    assert not os.path.exists(file_path)
+    with open(file_path, 'w') as f:
+      f.write('garbage content')
+    assert os.path.isfile(file_path)
+
+  @classmethod
+  def CompareDirectories(cls, reference_path, generated_path):
+    def CompareNode(relative_path):
+      reference_stat = os.stat(os.path.join(reference_path, relative_path))
+      generated_stat = os.stat(os.path.join(generated_path, relative_path))
+      assert int(reference_stat.st_mtime) == int(generated_stat.st_mtime), \
+          "{}: invalid mtime.".format(relative_path)
+    for reference_parent_path, dir_names, file_names in os.walk(reference_path):
+      parent_path = os.path.relpath(reference_parent_path, reference_path)
+      reference_nodes = sorted(dir_names + file_names)
+      generated_nodes = sorted(os.listdir(
+          os.path.join(generated_path, parent_path)))
+      assert reference_nodes == generated_nodes, \
+          '{}: directory entries don\'t match.'.format(parent_path)
+      for node in file_names:
+        CompareNode(os.path.join(parent_path, node))
+      CompareNode(parent_path)
+
+  def testCompareDirectories(self):
+    generated_path = self.GetTempPath('dir0')
+    shutil.copytree(LOADING_DIR, generated_path)
+    self.CompareDirectories(LOADING_DIR, generated_path)
+
+    generated_path = self.GetTempPath('dir1')
+    shutil.copytree(LOADING_DIR, generated_path)
+    self.CreateNewGarbageFile(os.path.join(generated_path, 'garbage'))
+    assert 'garbage' in os.listdir(generated_path)
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+
+    generated_path = self.GetTempPath('dir2')
+    shutil.copytree(LOADING_DIR, generated_path)
+    self.CreateNewGarbageFile(os.path.join(generated_path, 'testdata/garbage'))
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+
+    generated_path = self.GetTempPath('dir3')
+    shutil.copytree(LOADING_DIR, generated_path)
+    os.remove(os.path.join(generated_path, THIS_BASEMAME))
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+    self.CreateNewGarbageFile(os.path.join(generated_path, 'garbage'))
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+
+    def TouchHelper(temp_name, relative_name, timestamps):
+      generated_path = self.GetTempPath(temp_name)
+      shutil.copytree(LOADING_DIR, generated_path)
+      os.utime(os.path.join(generated_path, relative_name), timestamps)
+      with self.assertRaisesRegexp(AssertionError, r'^.* invalid mtime\.$'):
+        self.CompareDirectories(LOADING_DIR, generated_path)
+
+    TouchHelper('dir4', THIS_BASEMAME, (1256925858, 1256463122))
+    TouchHelper('dir5', 'testdata', (1256918318, 1256568641))
+    TouchHelper('dir6', 'trace_test/test_server.py', (1255116211, 1256156632))
+    TouchHelper('dir7', './', (1255115332, 1256251864))
+
+  def testCacheArchive(self):
+    zip_dest = self.GetTempPath('cache.zip')
+    chrome_cache.ZipDirectoryContent(LOADING_DIR, zip_dest)
+
+    unzip_dest = self.GetTempPath('cache')
+    chrome_cache.UnzipDirectoryContent(zip_dest, unzip_dest)
+    self.CompareDirectories(LOADING_DIR, unzip_dest)
+
+    self.CreateNewGarbageFile(os.path.join(unzip_dest, 'garbage'))
+    chrome_cache.UnzipDirectoryContent(zip_dest, unzip_dest)
+    self.CompareDirectories(LOADING_DIR, unzip_dest)
+
+    unzip_dest = self.GetTempPath('foo/bar/cache')
+    chrome_cache.UnzipDirectoryContent(zip_dest, unzip_dest)
+    self.CompareDirectories(LOADING_DIR, unzip_dest)
+
+  def testCopyCacheDirectory(self):
+    copy_dest = self.GetTempPath('cache')
+    chrome_cache.CopyCacheDirectory(LOADING_DIR, copy_dest)
+    self.CompareDirectories(LOADING_DIR, copy_dest)
+
+    self.CreateNewGarbageFile(os.path.join(copy_dest, 'garbage'))
+    chrome_cache.CopyCacheDirectory(LOADING_DIR, copy_dest)
+    self.CompareDirectories(LOADING_DIR, copy_dest)
+
+    copy_dest = self.GetTempPath('foo/bar/cache')
+    chrome_cache.CopyCacheDirectory(LOADING_DIR, copy_dest)
+    self.CompareDirectories(LOADING_DIR, copy_dest)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/controller.py b/loading/controller.py
index 9226ab3..25fefae 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -45,7 +45,23 @@ class ChromeControllerBase(object):
   """
   def __init__(self):
     self._chrome_args = [
+        # Disable backgound network requests that may pollute WPR archive,
+        # pollute HTTP cache generation, and introduce noise in loading
+        # performance.
+        '--disable-background-networking',
+        '--disable-default-apps',
+        '--no-proxy-server',
+        # TODO(gabadie): Remove once crbug.com/354743 done.
+        '--safebrowsing-disable-auto-update',
+
+        # Disables actions that chrome performs only on first run or each
+        # launches, which can interfere with page load performance, or even
+        # block its execution by waiting for user input.
         '--disable-fre',
+        '--no-default-browser-check',
+        '--no-first-run',
+
+        # Tests & dev-tools related stuff.
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
     ]
@@ -54,6 +70,7 @@ class ChromeControllerBase(object):
     self._emulated_device = None
     self._emulated_network = None
     self._clear_cache = False
+    self._slow_death = False
 
   def AddChromeArgument(self, arg):
     """Add command-line argument to the chrome execution."""
@@ -97,13 +114,64 @@ class ChromeControllerBase(object):
     """Set network emulation.
 
     Args:
-      network_name: (str) Key from emulation.NETWORK_CONDITIONS.
+      network_name: (str) Key from emulation.NETWORK_CONDITIONS or None to
+        disable network emulation.
     """
     if network_name:
       self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
     else:
       self._emulated_network = None
 
+  def PushBrowserCache(self, cache_path):
+    """Pushes the HTTP chrome cache to the profile directory.
+
+    Caution:
+      The chrome cache backend type differ according to the platform. On
+      desktop, the cache backend type is `blockfile` versus `simple` on Android.
+      This method assumes that your are pushing a cache with the correct backend
+      type, and will NOT verify for you.
+
+    Args:
+      cache_path: The directory's path containing the cache locally.
+    """
+    raise NotImplementedError
+
+  def PullBrowserCache(self):
+    """Pulls the HTTP chrome cache from the profile directory.
+
+    Returns:
+      Temporary directory containing all the browser cache. Caller will need to
+      remove this directory manually.
+    """
+    raise NotImplementedError
+
+  def SetSlowDeath(self, slow_death=True):
+    """Set to pause before final kill of chrome.
+
+    Gives time for caches to write.
+
+    Args:
+      slow_death: (bool) True if you want that which comes to all who live, to
+        be slow.
+    """
+    self._slow_death = slow_death
+
+  @contextlib.contextmanager
+  def OpenWprHost(self, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+    """Opens a Web Page Replay host context.
+
+    Args:
+      wpr_archive_path: host sided WPR archive's path.
+      record: Enables or disables WPR archive recording.
+      network_condition_name: Network condition name available in
+          emulation.NETWORK_CONDITIONS.
+      disable_script_injection: Disable JavaScript file injections that is
+        fighting against resources name entropy.
+    """
+    raise NotImplementedError
+
   def _StartConnection(self, connection):
     """This should be called after opening an appropriate connection."""
     if self._emulated_device:
@@ -137,7 +205,6 @@ class RemoteChromeController(ChromeControllerBase):
     assert device is not None, 'Should you be using LocalController instead?'
     self._device = device
     super(RemoteChromeController, self).__init__()
-    self._slow_death = False
 
   @contextlib.contextmanager
   def Open(self):
@@ -170,40 +237,21 @@ class RemoteChromeController(ChromeControllerBase):
     self._device.KillAll(package_info.package, quiet=True)
 
   def PushBrowserCache(self, cache_path):
-    """Push a chrome cache.
-
-    Args:
-      cache_path: The directory's path containing the cache locally.
-    """
+    """Override for chrome cache pushing."""
     chrome_cache.PushBrowserCache(self._device, cache_path)
 
   def PullBrowserCache(self):
-    """Pull a chrome cache.
-
-    Returns:
-      Temporary directory containing all the browser cache. Caller will need to
-      remove this directory manually.
-    """
+    """Override for chrome cache pulling."""
+    assert self._slow_death, 'Must do SetSlowDeath() before opening chrome.'
     return chrome_cache.PullBrowserCache(self._device)
 
-  def SetSlowDeath(self, slow_death=True):
-    """Set to pause before final kill of chrome.
-
-    Gives time for caches to write.
-
-    Args:
-      slow_death: (bool) True if you want that which comes to all who live, to
-        be slow.
-    """
-    self._slow_death = slow_death
-
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
                   disable_script_injection=False):
     """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
     assert not self._chrome_wpr_specific_args, 'WPR is already running.'
-    with device_setup.WprHost(self._device, wpr_archive_path,
+    with device_setup.RemoteWprHost(self._device, wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection) as additional_flags:
@@ -213,25 +261,32 @@ class RemoteChromeController(ChromeControllerBase):
 
 
 class LocalChromeController(ChromeControllerBase):
-  """Controller for a local (desktop) chrome instance.
+  """Controller for a local (desktop) chrome instance."""
 
-  TODO(gabadie): implement cache push/pull and declare up in base class.
-  """
   def __init__(self):
     super(LocalChromeController, self).__init__()
     if OPTIONS.no_sandbox:
       self.AddChromeArgument('--no-sandbox')
+    self._profile_dir = OPTIONS.local_profile_dir
+    self._using_temp_profile_dir = self._profile_dir is None
+    if self._using_temp_profile_dir:
+      self._profile_dir = tempfile.mkdtemp(suffix='.profile')
+
+  def __del__(self):
+    if self._using_temp_profile_dir:
+      shutil.rmtree(self._profile_dir)
 
   @contextlib.contextmanager
   def Open(self):
     """Override for connection context."""
-    binary_filename = OPTIONS.local_binary
-    profile_dir = OPTIONS.local_profile_dir
-    using_temp_profile_dir = profile_dir is None
-    flags = self._GetChromeArguments()
-    if using_temp_profile_dir:
-      profile_dir = tempfile.mkdtemp()
-    flags = ['--user-data-dir=%s' % profile_dir] + flags
+    chrome_cmd = [OPTIONS.local_binary]
+    chrome_cmd.extend(self._GetChromeArguments())
+    chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
+    chrome_cmd.extend(['--enable-logging=stderr', '--v=1'])
+    # Navigates to about:blank for couples of reasons:
+    #   - To find the correct target descriptor at devtool connection;
+    #   - To avoid cache and WPR pollution by the NTP.
+    chrome_cmd.append('about:blank')
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     environment = os.environ.copy()
     if OPTIONS.headless:
@@ -239,9 +294,10 @@ class LocalChromeController(ChromeControllerBase):
       xvfb_process = subprocess.Popen(
           ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
           stderr=chrome_out)
-    chrome_process = subprocess.Popen(
-        [binary_filename] + flags, shell=False, stderr=chrome_out,
-        env=environment)
+    logging.debug(subprocess.list2cmdline(chrome_cmd))
+    chrome_process = subprocess.Popen(chrome_cmd, shell=False,
+                                      stderr=chrome_out, env=environment)
+    connection = None
     try:
       time.sleep(10)
       process_result = chrome_process.poll()
@@ -252,9 +308,57 @@ class LocalChromeController(ChromeControllerBase):
             OPTIONS.devtools_hostname, OPTIONS.devtools_port)
         self._StartConnection(connection)
         yield connection
+        if self._slow_death:
+          connection.Close()
+          connection = None
+          chrome_process.wait()
     finally:
-      chrome_process.kill()
+      if connection:
+        chrome_process.kill()
       if OPTIONS.headless:
         xvfb_process.kill()
-      if using_temp_profile_dir:
-        shutil.rmtree(profile_dir)
+
+  def PushBrowserCache(self, cache_path):
+    """Override for chrome cache pushing."""
+    self._EnsureProfileDirectory()
+    profile_cache_path = self._GetCacheDirectoryPath()
+    logging.info('Copy cache directory from %s to %s.' % (
+        cache_path, profile_cache_path))
+    chrome_cache.CopyCacheDirectory(cache_path, profile_cache_path)
+
+  def PullBrowserCache(self):
+    """Override for chrome cache pulling."""
+    cache_path = tempfile.mkdtemp()
+    profile_cache_path = self._GetCacheDirectoryPath()
+    logging.info('Copy cache directory from %s to %s.' % (
+        profile_cache_path, cache_path))
+    chrome_cache.CopyCacheDirectory(profile_cache_path, cache_path)
+    return cache_path
+
+  @contextlib.contextmanager
+  def OpenWprHost(self, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+    """Override for WPR context."""
+    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    with device_setup.LocalWprHost(wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection
+        ) as additional_flags:
+      self._chrome_wpr_specific_args = additional_flags
+      yield
+    self._chrome_wpr_specific_args = []
+
+  def _EnsureProfileDirectory(self):
+    if (not os.path.isdir(self._profile_dir) or
+        os.listdir(self._profile_dir) == []):
+      # Launch chrome so that it populates the profile directory.
+      with self.Open():
+        pass
+      print os.listdir(self._profile_dir + '/Default')
+    assert os.path.isdir(self._profile_dir)
+    assert os.path.isdir(os.path.dirname(self._GetCacheDirectoryPath()))
+
+  def _GetCacheDirectoryPath(self):
+    return os.path.join(self._profile_dir, 'Default', 'Cache')
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 4ffbb51..fad2b23 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -130,30 +130,11 @@ def _SetUpDevice(device, package_info):
 
 
 @contextlib.contextmanager
-def WprHost(device, wpr_archive_path, record=False,
-            network_condition_name=None,
-            disable_script_injection=False):
-  """Launches web page replay host.
-
-  Args:
-    device: Android device.
-    wpr_archive_path: host sided WPR archive's path.
-    network_condition_name: Network condition name available in
-        chrome_setup.NETWORK_CONDITIONS.
-    record: Enables or disables WPR archive recording.
-
-  Returns:
-    Additional flags list that may be used for chromium to load web page through
-    the running web page replay host.
-  """
-  assert device
-  if wpr_archive_path == None:
-    assert not record, 'WPR cannot record without a specified archive.'
-    assert not network_condition_name, ('WPR cannot emulate network condition' +
-                                        ' without a specified archive.')
-    yield []
-    return
-
+def _WprHost(wpr_archive_path, record=False,
+             network_condition_name=None,
+             disable_script_injection=False,
+             wpr_ca_cert_path=None):
+  assert wpr_archive_path
   wpr_server_args = ['--use_closest_match']
   if record:
     wpr_server_args.append('--record')
@@ -176,40 +157,130 @@ def WprHost(device, wpr_archive_path, record=False,
     # Remove default WPR injected scripts like deterministic.js which
     # overrides Math.random.
     wpr_server_args.extend(['--inject_scripts', ''])
+  if wpr_ca_cert_path:
+    wpr_server_args.extend(['--should_generate_certs',
+                            '--https_root_ca_cert_path=' + wpr_ca_cert_path])
 
+  # Set up WPR server and device forwarder.
+  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
+      '127.0.0.1', 0, 0, None, wpr_server_args)
+  http_port, https_port = wpr_server.StartServer()[:-1]
+
+  logging.info('WPR server listening on HTTP=%s, HTTPS=%s (options=%s)' % (
+      http_port, https_port, wpr_server_args))
+  try:
+    yield http_port, https_port
+  finally:
+    wpr_server.StopServer()
+
+
+def _VerifySilentWprHost(record, network_condition_name):
+  assert not record, 'WPR cannot record without a specified archive.'
+  assert not network_condition_name, ('WPR cannot emulate network condition' +
+                                      ' without a specified archive.')
+
+
+def _FormatWPRRelatedChromeArgumentFor(http_port, https_port, escape):
+  HOST_RULES='MAP * 127.0.0.1,EXCLUDE localhost'
+  chrome_args = [
+      '--testing-fixed-http-port={}'.format(http_port),
+      '--testing-fixed-https-port={}'.format(https_port)]
+  if escape:
+    chrome_args.append('--host-resolver-rules="{}"'.format(HOST_RULES))
+  else:
+    chrome_args.append('--host-resolver-rules={}'.format(HOST_RULES))
+  return chrome_args
+
+
+@contextlib.contextmanager
+def LocalWprHost(wpr_archive_path, record=False,
+                 network_condition_name=None,
+                 disable_script_injection=False):
+  """Launches web page replay host.
+
+  Args:
+    wpr_archive_path: host sided WPR archive's path.
+    record: Enables or disables WPR archive recording.
+    network_condition_name: Network condition name available in
+        chrome_setup.NETWORK_CONDITIONS.
+    disable_script_injection: Disable JavaScript file injections that is
+      fighting against resources name entropy.
+
+  Returns:
+    Additional flags list that may be used for chromium to load web page through
+    the running web page replay host.
+  """
+  if wpr_archive_path == None:
+    _VerifySilentWprHost(record, network_condition_name)
+    yield []
+    return
+  with _WprHost(
+      wpr_archive_path,
+      record=record,
+      network_condition_name=network_condition_name,
+      disable_script_injection=disable_script_injection
+      ) as (http_port, https_port):
+    chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
+                                                     escape=False)
+    # Certification authority is handled only available on Android.
+    chrome_args.append('--ignore-certificate-errors')
+    yield chrome_args
+
+
+@contextlib.contextmanager
+def RemoteWprHost(device, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+  """Launches web page replay host.
+
+  Args:
+    device: Android device.
+    wpr_archive_path: host sided WPR archive's path.
+    record: Enables or disables WPR archive recording.
+    network_condition_name: Network condition name available in
+        chrome_setup.NETWORK_CONDITIONS.
+    disable_script_injection: Disable JavaScript file injections that is
+      fighting against resources name entropy.
+
+  Returns:
+    Additional flags list that may be used for chromium to load web page through
+    the running web page replay host.
+  """
+  assert device
+  if wpr_archive_path == None:
+    _VerifySilentWprHost(record, network_condition_name)
+    yield []
+    return
   # Deploy certification authority to the device.
   temp_certificate_dir = tempfile.mkdtemp()
   wpr_ca_cert_path = os.path.join(temp_certificate_dir, 'testca.pem')
   certutils.write_dummy_ca_cert(*certutils.generate_dummy_ca_cert(),
                                 cert_path=wpr_ca_cert_path)
-
   device_cert_util = adb_install_cert.AndroidCertInstaller(
       device.adb.GetDeviceSerial(), None, wpr_ca_cert_path)
   device_cert_util.install_cert(overwrite_cert=True)
-  wpr_server_args.extend(['--should_generate_certs',
-                          '--https_root_ca_cert_path=' + wpr_ca_cert_path])
-
-  # Set up WPR server and device forwarder.
-  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
-      '127.0.0.1', 0, 0, None, wpr_server_args)
-  ports = wpr_server.StartServer()[:-1]
-  host_http_port = ports[0]
-  host_https_port = ports[1]
-
-  forwarder.Forwarder.Map([(0, host_http_port), (0, host_https_port)], device)
-  device_http_port = forwarder.Forwarder.DevicePortForHostPort(host_http_port)
-  device_https_port = forwarder.Forwarder.DevicePortForHostPort(host_https_port)
-
   try:
-    yield [
-      '--host-resolver-rules="MAP * 127.0.0.1,EXCLUDE localhost"',
-      '--testing-fixed-http-port={}'.format(device_http_port),
-      '--testing-fixed-https-port={}'.format(device_https_port)]
+    # Set up WPR server
+    with _WprHost(
+        wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection,
+        wpr_ca_cert_path=wpr_ca_cert_path
+        ) as (http_port, https_port):
+      # Set up the forwarder.
+      forwarder.Forwarder.Map([(0, http_port), (0, https_port)], device)
+      device_http_port = forwarder.Forwarder.DevicePortForHostPort(http_port)
+      device_https_port = forwarder.Forwarder.DevicePortForHostPort(https_port)
+      try:
+        yield _FormatWPRRelatedChromeArgumentFor(device_http_port,
+                                                 device_https_port,
+                                                 escape=True)
+      finally:
+        # Tear down the forwarder.
+        forwarder.Forwarder.UnmapDevicePort(device_http_port, device)
+        forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
   finally:
-    forwarder.Forwarder.UnmapDevicePort(device_http_port, device)
-    forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
-    wpr_server.StopServer()
-
     # Remove certification authority from the device.
     device_cert_util.remove_cert()
     shutil.rmtree(temp_certificate_dir)
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 1ea5d91..b579918 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -92,7 +92,8 @@ class DevToolsConnection(object):
       hostname: server hostname.
       port: port number.
     """
-    self._ws = self._Connect(hostname, port)
+    self._http_hostname = hostname
+    self._http_port = port
     self._event_listeners = {}
     self._domain_listeners = {}
     self._scoped_states = {}
@@ -100,6 +101,10 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self._please_stop = False
     self._hooks = []
+    self._ws = None
+    self._target_descriptor = None
+
+    self._Connect()
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -247,6 +252,13 @@ class DevToolsConnection(object):
     """Stops the monitoring."""
     self._please_stop = True
 
+  def Close(self):
+    """Cleanly close chrome by closing the only tab."""
+    assert self._ws
+    response = self._HttpRequest('/close/' + self._target_descriptor['id'])
+    assert response == 'Target is closing'
+    self._ws = None
+
   def _Dispatch(self, timeout, kind='Monitoring'):
     self._please_stop = False
     while not self._please_stop:
@@ -313,25 +325,30 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self.StopMonitoring()
 
-  @classmethod
-  def _GetWebSocketUrl(cls, hostname, port):
-    r = httplib.HTTPConnection(hostname, port)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      raise DevToolsConnectionException(
-          'Cannot connect to DevTools, reponse code %d' % response.status)
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    return websocket_url
-
-  @classmethod
-  def _Connect(cls, hostname, port):
-    websocket_url = cls._GetWebSocketUrl(hostname, port)
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    return ws
+  def _HttpRequest(self, path):
+    assert path[0] == '/'
+    r = httplib.HTTPConnection(self._http_hostname, self._http_port)
+    try:
+      r.request('GET', '/json' + path)
+      response = r.getresponse()
+      if response.status != 200:
+        raise DevToolsConnectionException(
+            'Cannot connect to DevTools, reponse code %d' % response.status)
+      raw_response = response.read()
+    finally:
+      r.close()
+    return raw_response
+
+  def _Connect(self):
+    assert not self._ws
+    assert not self._target_descriptor
+    for target_descriptor in json.loads(self._HttpRequest('/list')):
+      if target_descriptor['type'] == 'page':
+        self._target_descriptor = target_descriptor
+        break
+    assert self._target_descriptor['url'] == 'about:blank'
+    self._ws = inspector_websocket.InspectorWebsocket()
+    self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'])
 
 
 class Listener(object):
diff --git a/loading/options.py b/loading/options.py
index ec295ac..0490ac5 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -118,7 +118,7 @@ class Options(object):
     for arg, default, help_str in self._ARGS:
       # All global options are named.
       arg = '--' + arg
-      self._AddArg(parser, arg, default, help_str=help_str)
+      self._AddArg(container, arg, default, help_str=help_str)
     if extra is not None:
       if type(extra) is not list:
         extra = [extra]
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 4657eaf..1739578 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -246,6 +246,7 @@ class SandwichRunner(object):
     # TODO(gabadie): Make sandwich working on desktop.
     device = device_utils.DeviceUtils.HealthyDevices()[0]
     self._chrome_ctl = controller.RemoteChromeController(device)
+    self._chrome_ctl.AddChromeArgument('--disable-infobars')
     if self.cache_operation == 'save':
       self._chrome_ctl.SetSlowDeath()
 
@@ -284,8 +285,11 @@ def _ArgumentParser():
   common_job_parser.add_argument('--job', required=True,
                                  help='JSON file with job description.')
 
+  # Plumbing parser to configure OPTIONS.
+  plumbing_parser = OPTIONS.GetParentParser('plumbing options')
+
   # Main parser
-  parser = argparse.ArgumentParser()
+  parser = argparse.ArgumentParser(parents=[plumbing_parser])
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
   # Record WPR subcommand.
@@ -495,11 +499,8 @@ def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
 
-  # Don't give the argument yet. All we are interested in for now is accessing
-  # the default values of OPTIONS.
-  OPTIONS.ParseArgs([])
-
   args = _ArgumentParser().parse_args(command_line_args)
+  OPTIONS.SetParsedArgs(args)
 
   if args.subcommand == 'record-wpr':
     return _RecordWprMain(args)

commit 542cedab0274708011685cd4123226a04bc37f1e
Author: gabadie <gabadie@chromium.org>
Date:   Mon Mar 21 07:43:20 2016 -0700

    tools/android/loading: Lets test_server.py handling custom response headers.
    
    This CL let the test_server.py looking for a RESPONSE_HEADERS.json
    in the source directory to add response headers on resources. This
    will be used for non-regression tests of sandwich's WPR patching.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1759093002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382284}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0869819900ada55ac050c31400665679b01fc8e5

diff --git a/loading/trace_test/test_server.py b/loading/trace_test/test_server.py
index 5898c66..5463667 100755
--- a/loading/trace_test/test_server.py
+++ b/loading/trace_test/test_server.py
@@ -13,6 +13,7 @@ via a named pipe at --fifo. Sources are served from the tree named at
 
 import argparse
 import cgi
+import json
 import os.path
 import logging
 import re
@@ -20,6 +21,27 @@ import time
 import wsgiref.simple_server
 
 
+_CONTENT_TYPE_FOR_SUFFIX = {
+    'css': 'text/css',
+    'html': 'text/html',
+    'jpg': 'image/jpeg',
+    'js': 'text/javascript',
+    'json': 'application/json',
+    'png': 'image/png',
+    'ttf': 'font/ttf',}
+
+# Name of the JSON file containing per file custom response headers located in
+# the --source_dir.
+# This file should structured like:
+#   {
+#     'mydocument.html': [
+#       ['Cache-Control', 'max-age=3600'],
+#       ['Content-Encoding', 'gzip'],
+#     ]
+#   }
+RESPONSE_HEADERS_PATH = 'RESPONSE_HEADERS.json'
+
+
 class ServerApp(object):
   """WSGI App.
 
@@ -27,6 +49,11 @@ class ServerApp(object):
   """
   def __init__(self, source_dir):
     self._source_dir = source_dir
+    self._response_headers = {}
+    response_header_path = os.path.join(source_dir, RESPONSE_HEADERS_PATH)
+    if os.path.exists(response_header_path):
+      with open(response_header_path) as response_headers_file:
+        self._response_headers = json.load(response_headers_file)
 
   def __call__(self, environ, start_response):
     """WSGI dispatch.
@@ -54,14 +81,11 @@ class ServerApp(object):
 
     logging.info('responding with %s', filename)
     suffix = path[path.rfind('.') + 1:]
-    start_response('200 OK', [('Content-Type',
-                               {'css': 'text/css',
-                                'html': 'text/html',
-                                'jpg': 'image/jpeg',
-                                'js': 'text/javascript',
-                                'png': 'image/png',
-                                'ttf': 'font/ttf',
-                              }[suffix])])
+    headers = [('Content-Type', _CONTENT_TYPE_FOR_SUFFIX[suffix])]
+    if path in self._response_headers:
+      for header in self._response_headers[path]:
+        headers.append((str(header[0]), str(header[1])))
+    start_response('200 OK', headers)
     return file(filename).read()
 
 
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index 2daa271..d687b13 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -2,9 +2,12 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import httplib
+import json
 import os
-import socket
+import shutil
 import sys
+import tempfile
 import unittest
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -12,41 +15,101 @@ _SRC_DIR = os.path.abspath(os.path.join(
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'android', 'loading'))
 
 import options
+from trace_test import test_server
 from trace_test import webserver_test
 
 
 OPTIONS = options.OPTIONS
 
 
-class TracingTrackTestCase(unittest.TestCase):
+class WebServerTestCase(unittest.TestCase):
   def setUp(self):
-    OPTIONS.ParseArgs('', extra=[('--noisy', False)])
-
-  def testWebserver(self):
-    with webserver_test.TemporaryDirectory() as temp_dir:
-      test_html = file(os.path.join(temp_dir, 'test.html'), 'w')
-      test_html.write('<!DOCTYPE html><html><head><title>Test</title></head>'
-                      '<body><h1>Test Page</h1></body></html>')
-      test_html.close()
-
-      server = webserver_test.WebServer(temp_dir, temp_dir)
-      server.Start()
-      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-      host, port = server.Address().split(':')
-      sock.connect((host, int(port)))
-      sock.sendall('GET null HTTP/1.1\n\n')
-      data = sock.recv(4096)
-      self.assertTrue(data.startswith('HTTP/1.0 404 Not Found'))
-      sock.close()
-
-      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-      sock.connect((host, int(port)))
-      sock.sendall('GET test.html HTTP/1.1\n\n')
-      data = sock.recv(4096)
-      self.assertTrue('HTTP/1.0 200 OK' in data)
-
-      sock.close()
-      self.assertTrue(server.Stop())
+    if not OPTIONS._parsed_args:
+      OPTIONS.ParseArgs('', extra=[('--noisy', False)])
+    self._temp_dir = tempfile.mkdtemp()
+    self._server = webserver_test.WebServer(self._temp_dir, self._temp_dir)
+
+  def tearDown(self):
+    self.assertTrue(self._server.Stop())
+    shutil.rmtree(self._temp_dir)
+
+  def StartServer(self):
+    self._server.Start()
+
+  def WriteFile(self, path, file_content):
+    with open(os.path.join(self._temp_dir, path), 'w') as file_output:
+      file_output.write(file_content)
+
+  def Request(self, path):
+    host, port = self._server.Address().split(':')
+    connection = httplib.HTTPConnection(host, int(port))
+    connection.request('GET', path)
+    response = connection.getresponse()
+    connection.close()
+    return response
+
+  def testWebserverBasic(self):
+    self.WriteFile('test.html',
+               '<!DOCTYPE html><html><head><title>Test</title></head>'
+               '<body><h1>Test Page</h1></body></html>')
+    self.StartServer()
+
+    response = self.Request('test.html')
+    self.assertEqual(200, response.status)
+
+    response = self.Request('/test.html')
+    self.assertEqual(200, response.status)
+
+    response = self.Request('///test.html')
+    self.assertEqual(200, response.status)
+
+  def testWebserver404(self):
+    self.StartServer()
+
+    response = self.Request('null')
+    self.assertEqual(404, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+
+  def testContentType(self):
+    self.WriteFile('test.html',
+               '<!DOCTYPE html><html><head><title>Test</title></head>'
+               '<body><h1>Test Page</h1></body></html>')
+    self.WriteFile('blobfile',
+               'whatever')
+    self.StartServer()
+
+    response = self.Request('test.html')
+    self.assertEqual(200, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+
+    response = self.Request('blobfile')
+    self.assertEqual(500, response.status)
+
+  def testCustomResponseHeader(self):
+    self.WriteFile('test.html',
+               '<!DOCTYPE html><html><head><title>Test</title></head>'
+               '<body><h1>Test Page</h1></body></html>')
+    self.WriteFile('test2.html',
+               '<!DOCTYPE html><html><head><title>Test 2</title></head>'
+               '<body><h1>Test Page 2</h1></body></html>')
+    self.WriteFile(test_server.RESPONSE_HEADERS_PATH,
+               json.dumps({'test2.html': [['Cache-Control', 'no-store']]}))
+    self.StartServer()
+
+    response = self.Request('test.html')
+    self.assertEqual(200, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+    self.assertEqual(None, response.getheader('cache-control'))
+
+    response = self.Request('test2.html')
+    self.assertEqual(200, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+    self.assertEqual('no-store', response.getheader('cache-control'))
+
+    response = self.Request(test_server.RESPONSE_HEADERS_PATH)
+    self.assertEqual(200, response.status)
+    self.assertEqual('application/json', response.getheader('content-type'))
+    self.assertEqual(None, response.getheader('cache-control'))
 
 
 if __name__ == '__main__':

commit 0a75bdcee248f512eae24f754ebfa963d1ded098
Author: droger <droger@chromium.org>
Date:   Mon Mar 21 03:26:20 2016 -0700

    tools/android/loading Add support for headless mode in analyse.py
    
    Review URL: https://codereview.chromium.org/1814863003
    
    Cr-Original-Commit-Position: refs/heads/master@{#382263}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 63512b75fc78ecb89e3740224ddf902879ee374d

diff --git a/loading/analyze.py b/loading/analyze.py
index 4b72ec6..2aed605 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -314,6 +314,8 @@ def main():
       'prefetch_delay_seconds', 5,
       'delay after requesting load of prefetch page '
       '(only when running full fetch)')
+  OPTIONS.AddGlobalArgument(
+      'headless', False, 'Do not display Chrome UI (only works in local mode).')
 
   parser = argparse.ArgumentParser(description='Analyzes loading')
   parser.add_argument('command', help=' '.join(COMMAND_MAP.keys()))
diff --git a/loading/controller.py b/loading/controller.py
index 2d9c3bd..9226ab3 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -233,11 +233,18 @@ class LocalChromeController(ChromeControllerBase):
       profile_dir = tempfile.mkdtemp()
     flags = ['--user-data-dir=%s' % profile_dir] + flags
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-    process = subprocess.Popen(
-        [binary_filename] + flags, shell=False, stderr=chrome_out)
+    environment = os.environ.copy()
+    if OPTIONS.headless:
+      environment['DISPLAY'] = 'localhost:99'
+      xvfb_process = subprocess.Popen(
+          ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
+          stderr=chrome_out)
+    chrome_process = subprocess.Popen(
+        [binary_filename] + flags, shell=False, stderr=chrome_out,
+        env=environment)
     try:
       time.sleep(10)
-      process_result = process.poll()
+      process_result = chrome_process.poll()
       if process_result is not None:
         logging.error('Unexpected process exit: %s', process_result)
       else:
@@ -246,6 +253,8 @@ class LocalChromeController(ChromeControllerBase):
         self._StartConnection(connection)
         yield connection
     finally:
-      process.kill()
+      chrome_process.kill()
+      if OPTIONS.headless:
+        xvfb_process.kill()
       if using_temp_profile_dir:
         shutil.rmtree(profile_dir)

commit 9b26136d503f4f7ede2bc95432377de85629d214
Author: gabadie <gabadie@chromium.org>
Date:   Fri Mar 18 09:44:35 2016 -0700

    sandwich: Use the ChromeController API.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1814023002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381992}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: dac49a12229b0e2b799ef40f706ca604c30d6700

diff --git a/loading/controller.py b/loading/controller.py
index 09c794b..2d9c3bd 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -49,6 +49,7 @@ class ChromeControllerBase(object):
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
     ]
+    self._chrome_wpr_specific_args = []
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
@@ -98,7 +99,10 @@ class ChromeControllerBase(object):
     Args:
       network_name: (str) Key from emulation.NETWORK_CONDITIONS.
     """
-    self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
+    if network_name:
+      self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
+    else:
+      self._emulated_network = None
 
   def _StartConnection(self, connection):
     """This should be called after opening an appropriate connection."""
@@ -114,6 +118,10 @@ class ChromeControllerBase(object):
     if self._clear_cache:
       connection.AddHook(connection.ClearCache)
 
+  def _GetChromeArguments(self):
+    """Get command-line arguments for the chrome execution."""
+    return self._chrome_args + self._chrome_wpr_specific_args
+
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
@@ -141,7 +149,7 @@ class RemoteChromeController(ChromeControllerBase):
     self._device.KillAll(package_info.package, quiet=True)
 
     with device_setup.FlagReplacer(
-        self._device, command_line_path, self._chrome_args):
+        self._device, command_line_path, self._GetChromeArguments()):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
@@ -189,6 +197,20 @@ class RemoteChromeController(ChromeControllerBase):
     """
     self._slow_death = slow_death
 
+  @contextlib.contextmanager
+  def OpenWprHost(self, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+    """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
+    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    with device_setup.WprHost(self._device, wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection) as additional_flags:
+      self._chrome_wpr_specific_args = additional_flags
+      yield
+    self._chrome_wpr_specific_args = []
+
 
 class LocalChromeController(ChromeControllerBase):
   """Controller for a local (desktop) chrome instance.
@@ -206,7 +228,7 @@ class LocalChromeController(ChromeControllerBase):
     binary_filename = OPTIONS.local_binary
     profile_dir = OPTIONS.local_profile_dir
     using_temp_profile_dir = profile_dir is None
-    flags = self._chrome_args
+    flags = self._GetChromeArguments()
     if using_temp_profile_dir:
       profile_dir = tempfile.mkdtemp()
     flags = ['--user-data-dir=%s' % profile_dir] + flags
diff --git a/loading/sandwich.py b/loading/sandwich.py
index b4e2590..4657eaf 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -33,6 +33,7 @@ import devil_chromium
 
 import chrome_cache
 import chrome_setup
+import controller
 import device_setup
 import devtools_monitor
 import frame_load_lens
@@ -151,8 +152,7 @@ class SandwichRunner(object):
     # Configures whether the WPR archive should be read or generated.
     self.wpr_record = False
 
-    self._device = None
-    self._chrome_additional_flags = []
+    self._chrome_ctl = None
     self._local_cache_directory_path = None
 
   def PullConfigFromArgs(self, args):
@@ -200,35 +200,27 @@ class SandwichRunner(object):
     return None
 
   def _RunNavigation(self, url, clear_cache, trace_id=None):
-    with device_setup.DeviceConnection(
-        device=self._device,
-        additional_flags=self._chrome_additional_flags) as connection:
-      additional_metadata = {}
-      if self._GetEmulatorNetworkCondition('browser'):
-        additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
-            connection=connection,
-            emulated_device_name=None,
-            emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
-      trace = trace_recorder.MonitorUrl(
-          connection, url,
-          clear_cache=clear_cache,
-          categories=pull_sandwich_metrics.CATEGORIES,
-          timeout=_DEVTOOLS_TIMEOUT)
-      trace.metadata.update(additional_metadata)
-      if trace_id != None and self.trace_output_directory:
-        trace_path = os.path.join(
-            self.trace_output_directory, str(trace_id), 'trace.json')
-        os.makedirs(os.path.dirname(trace_path))
-        trace.ToJsonFile(trace_path)
+    self._chrome_ctl.SetClearCache(clear_cache)
+    self._chrome_ctl.SetNetworkEmulation(
+        self._GetEmulatorNetworkCondition('browser'))
+    # TODO(gabadie): add a way to avoid recording a trace.
+    trace = loading_trace.LoadingTrace.FromUrlAndController(
+        url=url,
+        controller=self._chrome_ctl,
+        categories=pull_sandwich_metrics.CATEGORIES,
+        timeout_seconds=_DEVTOOLS_TIMEOUT)
+    if trace_id != None and self.trace_output_directory:
+      trace_path = os.path.join(
+          self.trace_output_directory, str(trace_id), 'trace.json')
+      os.makedirs(os.path.dirname(trace_path))
+      trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, trace_id=0):
     clear_cache = False
     if self.cache_operation == 'clear':
       clear_cache = True
     elif self.cache_operation == 'push':
-      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-      chrome_cache.PushBrowserCache(self._device,
-                                    self._local_cache_directory_path)
+      self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
     elif self.cache_operation == 'reload':
       self._RunNavigation(url, clear_cache=True)
     elif self.cache_operation == 'save':
@@ -239,27 +231,24 @@ class SandwichRunner(object):
     assert self.cache_operation == 'save'
     assert self.cache_archive_path, 'Need to specify where to save the cache'
 
-    # Move Chrome to background to allow it to flush the index.
-    self._device.adb.Shell('am start com.google.android.launcher')
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-
-    cache_directory_path = chrome_cache.PullBrowserCache(self._device)
+    cache_directory_path = self._chrome_ctl.PullBrowserCache()
     chrome_cache.ZipDirectoryContent(
         cache_directory_path, self.cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
   def Run(self):
-    """SandwichRunner main entry point meant to be called once configured.
-    """
+    """SandwichRunner main entry point meant to be called once configured."""
+    assert self._chrome_ctl == None
+    assert self._local_cache_directory_path == None
     if self.trace_output_directory:
       self._CleanTraceOutputDirectory()
 
-    self._device = device_utils.DeviceUtils.HealthyDevices()[0]
-    self._chrome_additional_flags = []
+    # TODO(gabadie): Make sandwich working on desktop.
+    device = device_utils.DeviceUtils.HealthyDevices()[0]
+    self._chrome_ctl = controller.RemoteChromeController(device)
+    if self.cache_operation == 'save':
+      self._chrome_ctl.SetSlowDeath()
 
-    assert self._local_cache_directory_path == None
     if self.cache_operation == 'push':
       assert os.path.isfile(self.cache_archive_path)
       self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
@@ -267,12 +256,11 @@ class SandwichRunner(object):
           self.cache_archive_path, self._local_cache_directory_path)
 
     ran_urls = []
-    with device_setup.WprHost(self._device, self.wpr_archive_path,
+    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
         record=self.wpr_record,
         network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
         disable_script_injection=self.disable_wpr_script_injection
-        ) as additional_flags:
-      self._chrome_additional_flags.extend(additional_flags)
+        ):
       for _ in xrange(self.job_repeat):
         for url in self.urls:
           self._RunUrl(url, trace_id=len(ran_urls))
@@ -286,6 +274,8 @@ class SandwichRunner(object):
     if self.trace_output_directory:
       self._SaveRunInfos(ran_urls)
 
+    self._chrome_ctl = None
+
 
 def _ArgumentParser():
   """Build a command line argument's parser."""

commit bd92f85e7d36c0058e26ac88daf7b8752d44007b
Author: mattcary <mattcary@chromium.org>
Date:   Fri Mar 18 06:17:08 2016 -0700

    Clovis: add core sets.
    
    This adds the core set computation to resource_sack.py (computing the common resources over a set of runs) as well as a script for computing and comparing the core sets of traces.
    
    Review URL: https://codereview.chromium.org/1804053002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381948}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8e34fbee0755ce47548153ef4fec89d754e78f4a

diff --git a/loading/core_set.py b/loading/core_set.py
new file mode 100644
index 0000000..8ab9493
--- /dev/null
+++ b/loading/core_set.py
@@ -0,0 +1,205 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Compute core set for a page.
+
+This script is a collection of utilities for working with core sets.
+"""
+
+import argparse
+import glob
+import json
+import logging
+import multiprocessing
+import os
+import sys
+
+import loading_model
+import loading_trace
+import resource_sack
+
+
+def _Progress(x):
+  sys.stderr.write(x + '\n')
+
+
+def _PageCore(prefix, graph_set_names, output):
+  """Compute the page core over sets defined by graph_set_names."""
+  assert graph_set_names
+  graph_sets = []
+  sack = resource_sack.GraphSack()
+  for name in graph_set_names:
+    name_graphs = []
+    _Progress('Processing %s' % name)
+    for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
+      _Progress('Reading %s' % filename)
+      graph = loading_model.ResourceGraph(
+          loading_trace.LoadingTrace.FromJsonFile(filename))
+      sack.ConsumeGraph(graph)
+      name_graphs.append(graph)
+    graph_sets.append(name_graphs)
+  json.dump({'page_core': [l for l in sack.CoreSet(*graph_sets)],
+             'threshold': sack.CORE_THRESHOLD},
+            output, sort_keys=True, indent=2)
+  output.write('\n')
+
+
+def _DoSite(site, graph_sets, input_dir, output_dir):
+  """Compute the appropriate page core for a site.
+
+  Used by _Spawn.
+  """
+  _Progress('Doing %s on %s' % (site, '/'.join(graph_sets)))
+  prefix = os.path.join(input_dir, site)
+  with file(os.path.join(output_dir,
+                         '%s-%s.json' % (site, '.'.join(graph_sets))),
+            'w') as output:
+    _PageCore(prefix, graph_sets, output)
+
+
+def _DoSiteRedirect(t):
+  """Unpack arguments for map call.
+
+  Note that multiprocessing.Pool.map cannot use a lambda (as it needs to be
+  serialized into the executing process).
+  """
+  _DoSite(*t)
+
+
+def _Spawn(site_list_file, graph_sets, input_dir, output_dir, workers):
+  """Spool site computation out to a multiprocessing pool."""
+  with file(site_list_file) as site_file:
+    sites = [l.strip() for l in site_file.readlines()]
+  _Progress('Using sites:\n %s' % '\n '.join(sites))
+  pool = multiprocessing.Pool(workers, maxtasksperchild=1)
+  pool.map(_DoSiteRedirect, [(s, graph_sets, input_dir, output_dir)
+                             for s in sites])
+
+
+def _AllCores(prefix, graph_set_names, output, threshold):
+  """Compute all core sets (per-set and overall page core) for a site."""
+  core_sets = []
+  _Progress('Using threshold %s' % threshold)
+  big_sack = resource_sack.GraphSack()
+  graph_sets = []
+  for name in graph_set_names:
+    _Progress('Finding core set for %s' % name)
+    sack = resource_sack.GraphSack()
+    sack.CORE_THRESHOLD = threshold
+    this_set = []
+    for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
+      _Progress('Reading %s' % filename)
+      graph = loading_model.ResourceGraph(
+          loading_trace.LoadingTrace.FromJsonDict(json.load(open(filename))))
+      sack.ConsumeGraph(graph)
+      big_sack.ConsumeGraph(graph)
+      this_set.append(graph)
+    core_sets.append({
+        'set_name': name,
+        'core_set': [l for l in sack.CoreSet()]
+    })
+    graph_sets.append(this_set)
+  json.dump({'core_sets': core_sets,
+             'page_core': [l for l in big_sack.CoreSet(*graph_sets)]},
+            output, sort_keys=True, indent=2)
+
+
+def _ReadCoreSet(filename):
+  data = json.load(open(filename))
+  return set(data['page_core'])
+
+
+def _Compare(a_name, b_name, csv):
+  """Compare two core sets."""
+  a = _ReadCoreSet(a_name)
+  b = _ReadCoreSet(b_name)
+  result = (resource_sack.GraphSack.CoreSimilarity(a, b),
+            '  Equal' if a == b else 'UnEqual',
+            'a<=b' if a <= b else 'a!<b',
+            'a>=b' if b <= a else 'a!>b')
+  if csv:
+    print '%s,%s,%s,%s' % result
+  else:
+    print '%.2f %s %s %s' % result
+
+
+if __name__ == '__main__':
+  logging.basicConfig(level=logging.ERROR)
+  parser = argparse.ArgumentParser()
+  subparsers = parser.add_subparsers()
+
+  spawn = subparsers.add_parser(
+      'spawn', help=('spawn page core set computation from a sites list.\n'
+                     'A core set will be computed for each site by '
+                     'combining all run indicies from site traces for each '
+                     '--set, then computing the page core over the sets. '
+                     'Assumes trace file names in form {input-dir}/'
+                     '{site}-{set}-{run index}.trace'))
+  spawn.add_argument('--sets', required=True,
+                     help='sets to combine, comma-separated')
+  spawn.add_argument('--sites', required=True, help='file containing sites')
+  spawn.add_argument('--workers', default=8, type=int,
+                     help=('number of parallel workers. Each worker seems to '
+                           'use about 0.5-1G/trace when processing. Total '
+                           'memory usage should be kept less than physical '
+                           'memory for the job to run in a reasonable time'))
+  spawn.add_argument('--input_dir', required=True,
+                     help='trace input directory')
+  spawn.add_argument('--output_dir', required=True,
+                     help=('core set output directory. Each site will have one '
+                           'JSON file generated listing the core set as well '
+                           'as some metadata like the threshold used'))
+  spawn.set_defaults(executor=lambda args:
+                     _Spawn(site_list_file=args.sites,
+                            graph_sets=args.sets.split(','),
+                            input_dir=args.input_dir,
+                            output_dir=args.output_dir,
+                            workers=args.workers))
+
+  page_core = subparsers.add_parser(
+      'page_core',
+      help=('compute page core set for a group of files of form '
+            '{--prefix}{set}*.trace over each set in --sets'))
+  page_core.add_argument('--sets', required=True,
+                       help='sets to combine, comma-separated')
+  page_core.add_argument('--prefix', required=True,
+                           help='trace file prefix')
+  page_core.add_argument('--output', required=True,
+                           help='JSON output file name')
+  page_core.set_defaults(
+      executor=lambda args:
+      _PageCore(args.prefix, args.sets.split(','), file(args.output, 'w')))
+
+  all_cores = subparsers.add_parser(
+      'all_cores',
+      help=('compute core and page core sets. Computes the core for each set '
+            'in --sets and then the overall page core using trace files '
+            'of form {--prefix}{set}*.trace. Outputs all the sets as JSON'))
+  all_cores.add_argument('--sets', required=True,
+                         help='sets to combine, comma-separated')
+  all_cores.add_argument('--prefix', required=True,
+                         help='input file prefix')
+  all_cores.add_argument('--output', required=True,
+                         help='JSON output file name')
+  all_cores.add_argument('--threshold',
+                         default=resource_sack.GraphSack.CORE_THRESHOLD,
+                         type=float, help='core set threshold')
+  all_cores.set_defaults(
+      executor=lambda args:
+      _AllCores(args.prefix, args.sets.split(','), file(args.output, 'w'),
+                args.threshold))
+
+  compare = subparsers.add_parser(
+      'compare',
+      help=('compare two core sets (as output by spawn, page_core or '
+            'all_cores) using Jaccard index. Outputs on stdout'))
+  compare.add_argument('--a', required=True, help='the first core set JSON')
+  compare.add_argument('--b', required=True, help='the second core set JSON')
+  compare.add_argument('--csv', action='store_true', help='output as CSV')
+  compare.set_defaults(
+      executor=lambda args:
+      _Compare(args.a, args.b, args.csv))
+
+  args = parser.parse_args()
+  args.executor(args)
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index f1182e5..9c174c5 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -32,6 +32,9 @@ class GraphSack(object):
   DAG. The edges are annotated with list of graphs and nodes that generated
   them.
   """
+  # See CoreSet().
+  CORE_THRESHOLD = 0.8
+
   _GraphInfo = collections.namedtuple('_GraphInfo', (
       'cost',   # The graph cost (aka critical path length).
       'total_costs',  # A vector by node index of total cost of each node.
@@ -84,6 +87,50 @@ class GraphSack(object):
     self._url_to_bag[node.Url()].AddNode(graph, node)
     return self._url_to_bag[node.Url()]
 
+  def CoreSet(self, *graph_sets):
+    """Compute the core set of this sack.
+
+    The core set of a sack is the set of resource that are common to most of the
+    graphs in the sack. A core set of a set of graphs are the resources that
+    appear with frequency at least CORE_THRESHOLD. For a collection of graph
+    sets, for instance pulling the same page under different network
+    connections, we intersect the core sets to produce a page core set that
+    describes the key resources used by the page. See https://goo.gl/F1BoEB for
+    context and discussion.
+
+    Args:
+      graph_sets: one or more collection of graphs to compute core sets. If one
+        graph set is given, its core set is computed. If more than one set is
+        given, the page core set of all sets is computed (the intersection of
+        core sets). If no graph set is given, the core of all graphs is
+        computed.
+
+    Returns:
+      A set of bag labels (as strings) in the core set.
+    """
+    if not graph_sets:
+      graph_sets = [self._graph_info.keys()]
+    return reduce(lambda a, b: a & b,
+                  (self._SingleCore(s) for s in graph_sets))
+
+  @classmethod
+  def CoreSimilarity(cls, a, b):
+    """Compute the similarity of two core sets.
+
+    We use the Jaccard index. See https://goo.gl/F1BoEB for discussion.
+
+    Args:
+      a: The first core set, as a set of strings.
+      b: The second core set, as a set of strings.
+
+    Returns:
+      A similarity score between zero and one. If both sets are empty the
+      similarity is zero.
+    """
+    if not a and not b:
+      return 0
+    return float(len(a & b)) / len(a | b)
+
   def FilterOccurrence(self, tag, filter_from_graph):
     """Accumulate filter occurrences for each bag in the graph.
 
@@ -101,6 +148,10 @@ class GraphSack(object):
       bag.MarkOccurrence(tag, filter_from_graph)
 
   @property
+  def num_graphs(self):
+    return len(self.graph_info)
+
+  @property
   def graph_info(self):
     return self._graph_info
 
@@ -108,6 +159,17 @@ class GraphSack(object):
   def bags(self):
     return self._bags
 
+  def _SingleCore(self, graph_set):
+    core = set()
+    graph_set = set(graph_set)
+    num_graphs = len(graph_set)
+    for b in self.bags:
+      count = sum([g in graph_set for g in b.graphs])
+      if float(count) / num_graphs > self.CORE_THRESHOLD:
+        core.add(b.label)
+    return core
+
+
 class Bag(dag.Node):
   def __init__(self, sack, index, url):
     super(Bag, self).__init__(index)
@@ -227,7 +289,10 @@ class Bag(dag.Node):
   def _MakeShortname(cls, url):
     parsed = urlparse.urlparse(url)
     if parsed.scheme == 'data':
-      kind, _ = parsed.path.split(';', 1)
+      if ';' in parsed.path:
+        kind, _ = parsed.path.split(';', 1)
+      else:
+        kind, _ = parsed.path.split(',', 1)
       return 'data:' + kind
     path = parsed.path[:10]
     hostname = parsed.hostname if parsed.hostname else '?.?.?'
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index 2034eba..df0fdb7 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -10,6 +10,13 @@ from test_utils import (MakeRequest,
 
 
 class ResourceSackTestCase(unittest.TestCase):
+  def SimpleGraph(self, node_names):
+    """Create a simple graph from a list of nodes."""
+    requests = [MakeRequest(node_names[0], 'null')]
+    for n in node_names[1:]:
+      requests.append(MakeRequest(n, node_names[0]))
+    return TestResourceGraph.FromRequestList(requests)
+
   def test_NodeMerge(self):
     g1 = TestResourceGraph.FromRequestList([
         MakeRequest(0, 'null'),
@@ -66,10 +73,8 @@ class ResourceSackTestCase(unittest.TestCase):
     shape1 = [MakeRequest(0, 'null'), MakeRequest(1, 0), MakeRequest(2, 0)]
     shape2 = [MakeRequest(0, 'null'), MakeRequest(1, 0),
               MakeRequest(3, 0), MakeRequest(4, 1)]
-    graphs = [TestResourceGraph.FromRequestList(shape1),
-              TestResourceGraph.FromRequestList(shape1),
-              TestResourceGraph.FromRequestList(shape1),
-              TestResourceGraph.FromRequestList(shape2)]
+    graphs = [TestResourceGraph.FromRequestList(s)
+              for s in (shape1, shape1,  shape1, shape2)]
     sack = resource_sack.GraphSack()
     for g in graphs:
       sack.ConsumeGraph(g)
@@ -89,6 +94,46 @@ class ResourceSackTestCase(unittest.TestCase):
     self.assertAlmostEqual(1, labels['3/'].GetOccurrence('test'), 3)
     self.assertAlmostEqual(0, labels['4/'].GetOccurrence('test'), 3)
 
+  def test_Core(self):
+    # We will use a core threshold of 0.5 to make it easier to define
+    # graphs. Resources 0 and 1 are core and others are not.
+    graphs = [self.SimpleGraph([0, 1, 2]),
+              self.SimpleGraph([0, 1, 3]),
+              self.SimpleGraph([0, 1, 4]),
+              self.SimpleGraph([0, 5])]
+    sack = resource_sack.GraphSack()
+    sack.CORE_THRESHOLD = 0.5
+    for g in graphs:
+      sack.ConsumeGraph(g)
+    self.assertEqual(set(['0/', '1/']), sack.CoreSet())
+
+  def test_IntersectingCore(self):
+    # Graph set A has core set {0, 1} and B {0, 2} so the final core set should
+    # be {0}. Set C makes sure we restrict core computation to tags A and B.
+    set_A = [self.SimpleGraph([0, 1, 2]),
+             self.SimpleGraph([0, 1, 3])]
+    set_B = [self.SimpleGraph([0, 2, 3]),
+             self.SimpleGraph([0, 2, 1])]
+    set_C = [self.SimpleGraph([2 * i + 4, 2 * i + 5]) for i in xrange(5)]
+    sack = resource_sack.GraphSack()
+    sack.CORE_THRESHOLD = 0.5
+    for g in set_A + set_B + set_C:
+      sack.ConsumeGraph(g)
+    self.assertEqual(set(), sack.CoreSet())
+    self.assertEqual(set(['0/', '1/']), sack.CoreSet(set_A))
+    self.assertEqual(set(['0/', '2/']), sack.CoreSet(set_B))
+    self.assertEqual(set(), sack.CoreSet(set_C))
+    self.assertEqual(set(['0/']), sack.CoreSet(set_A, set_B))
+    self.assertEqual(set(), sack.CoreSet(set_A, set_B, set_C))
+
+  def test_Simililarity(self):
+    self.assertAlmostEqual(
+        0.5,
+        resource_sack.GraphSack.CoreSimilarity(
+            set([1, 2, 3]), set([1, 3, 4])))
+    self.assertEqual(
+        0, resource_sack.GraphSack.CoreSimilarity(set(), set()))
+
 
 if __name__ == '__main__':
   unittest.main()

commit d5706494eeee1285ff5b96ecc5af0065a2f69789
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 16 07:12:33 2016 -0700

    clovis: Add step to event matching to the tracing track.
    
    Steps are used in traces to annotate aync events. One such annotation is
    "Preload", tagging resource fetches. It looks like this in the JSON:
    
            {
                "cat": "blink.net",
                "id": "0xaf9f4a094d99b1a0",
                "name": "Resource",
                "ph": "S",
                "pid": 32197,
                "tid": 1,
                "ts": 2252598472497,
                "tts": 52249
            },
            {
                "args": {
                    "step": "Preload"
                },
                "cat": "blink.net",
                "id": "0xaf9f4a094d99b1a0",
                "name": "Resource",
                "ph": "T",
                "pid": 32197,
                "tid": 1,
                "ts": 2252598472528,
                "tts": 52280
            },
    
    Review URL: https://codereview.chromium.org/1802973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381449}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b852db43b8cbc2f2477b45a344605660a2b7f7be

diff --git a/loading/tracing.py b/loading/tracing.py
index 5798b1e..7403a9d 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -123,6 +123,43 @@ class TracingTrack(devtools_monitor.Track):
         tracing_track._base_msec = e.start_msec
     return tracing_track
 
+  def OverlappingEvents(self, start_msec, end_msec):
+    self._IndexEvents()
+    return self._interval_tree.OverlappingEvents(start_msec, end_msec)
+
+  def EventsEndingBetween(self, start_msec, end_msec):
+    """Gets the list of events ending within an interval.
+
+    Args:
+      start_msec: the start of the range to query, in milliseconds, inclusive.
+      end_msec: the end of the range to query, in milliseconds, inclusive.
+
+    Returns:
+      See OverlappingEvents() above.
+    """
+    overlapping_events = self.OverlappingEvents(start_msec, end_msec)
+    return [e for e in overlapping_events
+            if start_msec <= e.end_msec <= end_msec]
+
+  def EventFromStep(self, step_event):
+    """Returns the Event associated with a step event, or None.
+
+    Args:
+      step_event: (Event) Step event.
+
+    Returns:
+      an Event that matches the step event, or None.
+    """
+    self._IndexEvents()
+    assert 'step' in step_event.args and step_event.tracing_event['ph'] == 'T'
+    candidates = self._interval_tree.EventsAt(step_event.start_msec)
+    for event in candidates:
+      # IDs are only unique within a process (often they are pointers).
+      if (event.pid == step_event.pid and event.tracing_event['ph'] != 'T'
+          and event.name == step_event.name and event.id == step_event.id):
+        return event
+    return None
+
   def _IndexEvents(self, strict=False):
     if self._interval_tree:
       return
@@ -144,24 +181,6 @@ class TracingTrack(devtools_monitor.Track):
           'Pending spanning events: %s' %
           '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
 
-  def OverlappingEvents(self, start_msec, end_msec):
-    self._IndexEvents()
-    return self._interval_tree.OverlappingEvents(start_msec, end_msec)
-
-  def EventsEndingBetween(self, start_msec, end_msec):
-    """Gets the list of events ending within an interval.
-
-    Args:
-      start_msec: the start of the range to query, in milliseconds, inclusive.
-      end_msec: the end of the range to query, in milliseconds, inclusive.
-
-    Returns:
-      See OverlappingEvents() above.
-    """
-    overlapping_events = self.OverlappingEvents(start_msec, end_msec)
-    return [e for e in overlapping_events
-            if start_msec <= e.end_msec <= end_msec]
-
   def _GetEvents(self):
     self._IndexEvents()
     return self._interval_tree.GetEvents()
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index ade5f8d..8d946b7 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -214,10 +214,7 @@ class TracingTrackTestCase(unittest.TestCase):
           event.tracing_event, deserialized_event.tracing_event)
 
   def testTracingTrackSerialization(self):
-    events = self._MIXED_EVENTS
-    self.track.Handle('Tracing.dataCollected',
-                      {'params': {'value': [self.EventToMicroseconds(e)
-                                            for e in events]}})
+    self._HandleEvents(self._MIXED_EVENTS)
     json_dict = self.track.ToJsonDict()
     self.assertTrue('events' in json_dict)
     deserialized_track = TracingTrack.FromJsonDict(json_dict)
@@ -227,9 +224,7 @@ class TracingTrackTestCase(unittest.TestCase):
       self.assertEquals(e1.tracing_event, e2.tracing_event)
 
   def testEventsEndingBetween(self):
-    self.track.Handle(
-        'Tracing.dataCollected', {'params': {'value': [
-            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self._HandleEvents(self._EVENTS)
     self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
                           for e in self.track.EventsEndingBetween(0, 100)]))
@@ -242,9 +237,7 @@ class TracingTrackTestCase(unittest.TestCase):
                           for e in self.track.EventsEndingBetween(3, 6)]))
 
   def testOverlappingEvents(self):
-    self.track.Handle(
-        'Tracing.dataCollected', {'params': {'value': [
-            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self._HandleEvents(self._EVENTS)
     self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
                           for e in self.track.OverlappingEvents(0, 100)]))
@@ -257,16 +250,55 @@ class TracingTrackTestCase(unittest.TestCase):
                      set([e.args['name']
                           for e in self.track.OverlappingEvents(6, 10.1)]))
 
+  def testEventFromStep(self):
+    events = [
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x123',
+         'name': 'B'},
+        {'ts': 5, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 1, 'id': '0x12343',
+        'name': 'A'}]
+    step_events = [{'ts': 6, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 4, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'T', 'pid': 12, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x1234',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'A', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'n', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'n', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {}}]
+    self._HandleEvents(events + step_events)
+    trace_events = self.track.GetEvents()
+    self.assertEquals(9, len(trace_events))
+    # pylint: disable=unbalanced-tuple-unpacking
+    (event, _, step_event, outside, wrong_pid, wrong_id, wrong_name,
+     wrong_phase, no_step) = trace_events
+    self.assertEquals(event, self.track.EventFromStep(step_event))
+    self.assertIsNone(self.track.EventFromStep(outside))
+    self.assertIsNone(self.track.EventFromStep(wrong_pid))
+    self.assertIsNone(self.track.EventFromStep(wrong_id))
+    self.assertIsNone(self.track.EventFromStep(wrong_name))
+    # Invalid events
+    with self.assertRaises(AssertionError):
+      self.track.EventFromStep(wrong_phase)
+    with self.assertRaises(AssertionError):
+      self.track.EventFromStep(no_step)
+
   def testTracingTrackForThread(self):
-    self.track.Handle(
-        'Tracing.dataCollected', {'params': {'value': [
-            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self._HandleEvents(self._EVENTS)
     tracing_track = self.track.TracingTrackForThread((2, 1))
     self.assertTrue(tracing_track is not self.track)
     self.assertEquals(4, len(tracing_track.GetEvents()))
     tracing_track = self.track.TracingTrackForThread((2, 42))
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
+  def _HandleEvents(self, events):
+    self.track.Handle('Tracing.dataCollected', {'params': {'value': [
+        self.EventToMicroseconds(e) for e in events]}})
+
 
 class IntervalTreeTestCase(unittest.TestCase):
   class FakeEvent(object):

commit 7b500502789727b5cf6fbdbad4de5e65d588636c
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 16 06:31:49 2016 -0700

    clovis: Ignore trace context events in tracing.py.
    
    Per https://goo.gl/Qabkqk, the '(' and ')' phases relate to Context
    Events. We don't process them currently, so we need to ignore them to
    prevent tracing.py from failing when indexing a trace.
    
    Review URL: https://codereview.chromium.org/1804413002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381446}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e7736901158ad527b98db7bcf4fc130aa300778e

diff --git a/loading/tracing.py b/loading/tracing.py
index 54ff3d0..5798b1e 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -183,6 +183,8 @@ class TracingTrack(devtools_monitor.Track):
           'M': self._Ignore,
           'X': self._Ignore,
           'R': self._Ignore,
+          '(': self._Ignore, # Context events.
+          ')': self._Ignore, # Ditto.
           None: self._Ignore,
           }
 

commit 8f0715dc597531828509a53cc565b7e6375bed81
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 16 06:06:46 2016 -0700

    tools/android/loading: Remove spaces from network emulation names to make scriptability easier.
    
    Review URL: https://codereview.chromium.org/1802743003
    
    Cr-Original-Commit-Position: refs/heads/master@{#381439}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e5aa4f8b6b4b06e0a163098f31748d048ff50b98

diff --git a/loading/emulation.py b/loading/emulation.py
index fa2fd89..d7f05c6 100644
--- a/loading/emulation.py
+++ b/loading/emulation.py
@@ -14,16 +14,16 @@ import json
 NETWORK_CONDITIONS = {
     'GPRS': {
         'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
-    'Regular 2G': {
+    'Regular2G': {
         'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
-    'Good 2G': {
+    'Good2G': {
         'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
-    'Regular 3G': {
+    'Regular3G': {
         'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
-    'Good 3G': {
+    'Good3G': {
         'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
         'latency': 40},
-    'Regular 4G': {
+    'Regular4G': {
         'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
         'latency': 20},
     'DSL': {

commit fd2ab36728f25059f35d42a7f7aac8385b7ded97
Author: newt <newt@chromium.org>
Date:   Tue Mar 15 16:35:11 2016 -0700

    Discourage use of android.app.AlertDialog and StringBuffer.
    
    android.support.v7.app.AlertDialog and StringBuilder should be used
    instead.
    
    BUG=4562
    
    Review URL: https://codereview.chromium.org/1804293002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381350}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6e35db99271ad9312089a3bc0d3474acff9ceef8

diff --git a/checkstyle/chromium-style-5.0.xml b/checkstyle/chromium-style-5.0.xml
index c8748ad..6557f61 100644
--- a/checkstyle/chromium-style-5.0.xml
+++ b/checkstyle/chromium-style-5.0.xml
@@ -124,7 +124,7 @@
       <property name="tokens" value="ASSIGN, BAND, BAND_ASSIGN, BOR, BOR_ASSIGN, BSR, BSR_ASSIGN, BXOR, BXOR_ASSIGN, COLON, DIV, DIV_ASSIGN, EQUAL, GE, GT, LAND, LE, LITERAL_ASSERT, LITERAL_CATCH, LITERAL_DO, LITERAL_ELSE, LITERAL_FINALLY, LITERAL_FOR, LITERAL_IF, LITERAL_RETURN, LITERAL_SYNCHRONIZED, LITERAL_TRY, LITERAL_WHILE, LOR, LT, MINUS, MINUS_ASSIGN, MOD, MOD_ASSIGN, NOT_EQUAL, PLUS, PLUS_ASSIGN, QUESTION, SL, SLIST, SL_ASSIGN, SR, SR_ASSIGN, STAR, STAR_ASSIGN, TYPE_EXTENSION_AND" />
       <property name="allowEmptyConstructors" value="true"/>
       <property name="allowEmptyMethods" value="true"/>
-   </module>
+    </module>
     <module name="WhitespaceAfter">
       <property name="severity" value="error"/>
       <property name="tokens" value="COMMA, SEMI, TYPECAST"/>
@@ -196,7 +196,21 @@
       <property name="tokens" value="COMMA"/>
       <property name="option" value="EOL"/>
     </module>
+    <module name="RegexpSinglelineJava">
+      <property name="severity" value="error"/>
+      <property name="format" value="StringBuffer"/>
+      <property name="ignoreComments" value="true"/>
+      <property name="message" value="Avoid StringBuffer; use StringBuilder instead, which is faster (it's not thread-safe, but this is almost never needed)"/>
+    </module>
+    <module name="RegexpSinglelineJava">
+      <property name="severity" value="warning"/>
+      <property name="format" value="android\.app\.AlertDialog"/>
+      <property name="ignoreComments" value="true"/>
+      <property name="message" value="Avoid android.app.AlertDialog; if possible, use android.support.v7.app.AlertDialog instead, which has a Material look on all devices. (Some parts of the codebase cant depend on the support library, in which case android.app.AlertDialog is the only option)"/>
+    </module>
   </module>
+
+  <!-- Non-TreeWalker modules -->
   <module name="FileTabCharacter">
     <property name="severity" value="error"/>
   </module>
@@ -204,7 +218,7 @@
     <property name="severity" value="error"/>
     <property name="format" value="[ \t]+$"/>
     <property name="message" value="Trailing whitespace"/>
-    </module>
+  </module>
   <module name="RegexpHeader">
     <property name="severity" value="error"/>
     <property name="header" value="^// Copyright 20\d\d The Chromium Authors. All rights reserved.$\n^// Use of this source code is governed by a BSD-style license that can be$\n^// found in the LICENSE file.$"/>

commit 4f11846d37bd1ca293ab789fd152ceaeddf956a6
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 15 02:48:52 2016 -0700

    tools/android/loading: Optimize chrome_cache.{Pull,Push}BrowserCache()
    
    Some websites have a lot of resources, slowing down sandwich's
    cache pulling and pushing. This CL optimizes these operations
    by pulling the cache directory recursively, but also by
    introducing the commands queue that are written to a temporary
    shell file and then pushed and ran on the device.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1753343002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381200}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 752434c777d2e7b445e37629bf89ecc7c38c7c5c

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 40d15bb..bdbe1e4 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -20,6 +20,7 @@ _SRC_DIR = os.path.abspath(os.path.join(
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 
+import device_setup
 import options
 
 
@@ -43,21 +44,10 @@ def _RemoteCacheDirectory():
       constants.PACKAGE_INFO[OPTIONS.chrome_package_name].package)
 
 
-def _UpdateTimestampFromAdbStat(filename, stat):
-  os.utime(filename, (stat.st_time, stat.st_time))
-
-
 def _AdbShell(adb, cmd):
   adb.Shell(subprocess.list2cmdline(cmd))
 
 
-def _AdbUtime(adb, filename, timestamp):
-  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
-  """
-  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
-  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
-
-
 def PullBrowserCache(device):
   """Pulls the browser cache from the device and saves it locally.
 
@@ -71,8 +61,16 @@ def PullBrowserCache(device):
   _REAL_INDEX_FILE_NAME = 'the-real-index'
 
   remote_cache_directory = _RemoteCacheDirectory()
-  print remote_cache_directory
   save_target = tempfile.mkdtemp(suffix='.cache')
+
+  # Pull the cache recursively.
+  device.adb.Pull(remote_cache_directory, save_target)
+
+  # Update the modification time stamp on the local cache copy.
+  def _UpdateTimestampFromAdbStat(filename, stat):
+    assert os.path.exists(filename)
+    os.utime(filename, (stat.st_time, stat.st_time))
+
   for filename, stat in device.adb.Ls(remote_cache_directory):
     if filename == '..':
       continue
@@ -81,7 +79,6 @@ def PullBrowserCache(device):
       continue
     original_file = os.path.join(remote_cache_directory, filename)
     saved_file = os.path.join(save_target, filename)
-    device.adb.Pull(original_file, saved_file)
     _UpdateTimestampFromAdbStat(saved_file, stat)
     if filename == _INDEX_DIRECTORY_NAME:
       # The directory containing the index was pulled recursively, update the
@@ -119,11 +116,16 @@ def PushBrowserCache(device, local_cache_path):
   # Push cache content.
   device.adb.Push(local_cache_path, remote_cache_directory)
 
+  # Command queue to touch all files with correct timestamp.
+  command_queue = []
+
   # Walk through the local cache to update mtime on the device.
   def MirrorMtime(local_path):
     cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
     remote_path = os.path.join(remote_cache_directory, cache_relative_path)
-    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
+    timestamp = os.stat(local_path).st_mtime
+    touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
+    command_queue.append(['touch', '-t', touch_stamp, remote_path])
 
   for local_directory_path, dirnames, filenames in os.walk(
         local_cache_path, topdown=False):
@@ -133,6 +135,8 @@ def PushBrowserCache(device, local_cache_path):
       MirrorMtime(os.path.join(local_directory_path, dirname))
   MirrorMtime(local_cache_path)
 
+  device_setup.DeviceSubmitShellCommandQueue(device, command_queue)
+
 
 def ZipDirectoryContent(root_directory_path, archive_dest_path):
   """Zip a directory's content recursively with all the directories'
diff --git a/loading/device_setup.py b/loading/device_setup.py
index bb2b4d7..4ffbb51 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -59,6 +59,31 @@ def GetFirstDevice():
   return devices[0]
 
 
+def DeviceSubmitShellCommandQueue(device, command_queue):
+  """Executes on the device a command queue.
+
+  Args:
+    device: The device to execute the shell commands to.
+    command_queue: a list of commands to be executed in that order.
+  """
+  REMOTE_COMMAND_FILE_PATH = '/data/local/tmp/adb_command_file.sh'
+  if not command_queue:
+    return
+  with tempfile.NamedTemporaryFile(prefix='adb_command_file_',
+                                   suffix='.sh') as command_file:
+    command_file.write('#!/bin/sh\n')
+    command_file.write('# Shell file generated by {}\'s {}\n'.format(
+        __file__, DeviceSubmitShellCommandQueue.__name__))
+    command_file.write('set -e\n')
+    for command in command_queue:
+      command_file.write(subprocess.list2cmdline(command) + ' ;\n')
+    command_file.write('exit 0;\n'.format(
+        REMOTE_COMMAND_FILE_PATH))
+    command_file.flush()
+    device.adb.Push(command_file.name, REMOTE_COMMAND_FILE_PATH)
+    device.adb.Shell('sh {p} && rm {p}'.format(p=REMOTE_COMMAND_FILE_PATH))
+
+
 @contextlib.contextmanager
 def FlagReplacer(device, command_line_path, new_flags):
   """Replaces chrome flags in a context, restores them afterwards.

commit 9d1e0288072a2066ed27c26ae42d9f9e983c76aa
Author: droger <droger@chromium.org>
Date:   Mon Mar 14 07:36:33 2016 -0700

    tools/android/loading Send a list of URLs with a POST request
    
    Review URL: https://codereview.chromium.org/1781403002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380973}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 04e1b2e7d54c290c2d8aa5570949cded14e2fd5f

diff --git a/loading/gce/README.md b/loading/gce/README.md
index eda5520..6de07d2 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -53,13 +53,29 @@ Otherwise the IP address can be retrieved by doing:
 gcloud compute instances list
 ```
 
-Interact with the app on the port 8080 in your browser at
-`http://<instance-ip>:8080`.
-
 TODO: allow starting the instance in the cloud without Supervisor. This enables
 iterative development on the instance using SSH, manually starting and stopping
 the app. This can be done using [instance metadata][2].
 
+## Use the app
+
+Interact with the app on the port 8080 at `http://<instance-ip>:8080`.
+
+To send a list of URLs to process:
+
+```shell
+curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
+```
+
+where `urls.txt` is a file containing URLs (one per line).
+
+Start the processing by sending a request to `http://<instance-ip>:8080/start`,
+for example:
+
+```shell
+curl http://<instance-ip>:8080/start
+```
+
 ## Stop the app in the cloud
 
 ```shell
@@ -85,7 +101,7 @@ pip install -r pip_requirements.txt
 Launch the app:
 
 ```shell
-gunicorn --workers=2 main:app --bind 127.0.0.1:8000
+gunicorn --workers=1 main:app --bind 127.0.0.1:8000
 ```
 
 In your browser, go to `http://localhost:8000` and use the app.
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 67fe3d1..2a0fd44 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -3,6 +3,8 @@
 # found in the LICENSE file.
 
 import json
+import re
+import threading
 
 from gcloud import storage
 from gcloud.exceptions import NotFound
@@ -14,6 +16,8 @@ class ServerApp(object):
   """
 
   def __init__(self):
+    self._tasks = []
+    self._thread = None
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
     print 'Reading server configuration'
@@ -52,6 +56,22 @@ class ServerApp(object):
       return None
     return blob.download_as_string()
 
+  def _SetTasks(self, task_list):
+    if len(self._tasks) > 0:
+      return False  # There are tasks already.
+    self._tasks = json.loads(task_list)
+    return len(self._tasks) != 0
+
+  def _ProcessTasks(self):
+    # Avoid special characters in storage object names
+    pattern = re.compile(r"[#\?\[\]\*/]")
+    while len(self._tasks) > 0:
+      url = self._tasks.pop()
+      filename = pattern.sub('_', url)
+      # TODO: compute the actual trace for url.
+      trace = '{}'
+      self._UploadFile(trace, filename)
+
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
     if path == '/favicon.ico':
@@ -60,19 +80,24 @@ class ServerApp(object):
 
     status = '200 OK'
 
-    if path == '/write':
-      url = self._UploadFile('foo', 'test.txt')
-      data = 'Writing file at\n' + url + '\n'
-    elif path == '/read':
-      data = self._ReadFile('test.txt')
-      if not data:
-        data = ''
-        status = '404 NOT FOUND'
-    elif path == '/delete':
-      if self._DeleteFile('test.txt'):
-        data = 'Success\n'
+    if path == '/set_tasks':
+      # Get the tasks from the HTTP body.
+      try:
+        body_size = int(environ.get('CONTENT_LENGTH', 0))
+      except (ValueError):
+        body_size = 0
+      body = environ['wsgi.input'].read(body_size)
+      if self._SetTasks(body):
+        data = 'Set tasks: ' + str(len(self._tasks))
+      else:
+        data = 'Something went wrong'
+    elif path == '/start':
+      if len(self._tasks) > 0:
+        data = 'Starting...'
+        self._thread = threading.Thread(target = self._ProcessTasks)
+        self._thread.start()
       else:
-        data = 'Failed\n'
+        data = 'Nothing to do!'
     else:
       data = environ['PATH_INFO'] + '\n'
 
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index 4f2cafa..b6ddde1 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -40,7 +40,7 @@ chown -R pythonapp:pythonapp /opt/app
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
 directory=/opt/app/clovis
-command=/opt/app/clovis/env/bin/gunicorn --workers=2 main:app \
+command=/opt/app/clovis/env/bin/gunicorn --workers=1 main:app \
   --bind 0.0.0.0:8080
 autostart=true
 autorestart=true

commit 38e5e580df90a95bb98c3b9944357a4cc65fdfc9
Author: tedchoc <tedchoc@chromium.org>
Date:   Fri Mar 11 11:04:44 2016 -0800

    Update eclipse .classpath to include other android device/ directories.
    
    TBR=newt@chromium.org
    BUG=
    
    Review URL: https://codereview.chromium.org/1783943003
    
    Cr-Original-Commit-Position: refs/heads/master@{#380690}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a0a0219dde77a6b75a57e439d43664e978878ef4

diff --git a/eclipse/.classpath b/eclipse/.classpath
index d9a4516..0495984 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -59,6 +59,9 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="content/shell/android/linker_test_apk/src"/>
     <classpathentry kind="src" path="content/shell/android/shell_apk/src"/>
     <classpathentry kind="src" path="device/battery/android/java/src"/>
+    <classpathentry kind="src" path="device/bluetooth/android/java/src"/>
+    <classpathentry kind="src" path="device/usb/android/java/src"/>
+    <classpathentry kind="src" path="device/vibration/android/java/src"/>
     <classpathentry kind="src" path="media/base/android/java/src"/>
     <classpathentry kind="src" path="mojo/android/system/src"/>
     <classpathentry kind="src" path="mojo/android/javatests/src"/>

commit 43b3c0a84f95e28e873c4174802e907c7b7e5c35
Author: mikecase <mikecase@chromium.org>
Date:   Fri Mar 11 10:38:34 2016 -0800

    Add an Android sample app for Telemetry tests.
    
    The purpose of this app is to be launched and push other apps to
    the background during Android telemetry tests. It is just a
    super simple Android app.
    
    BUG=586148
    
    Review URL: https://codereview.chromium.org/1744423002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380680}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f5e98301e0ee9a7a7388df01a1dfa366a95840ed

diff --git a/BUILD.gn b/BUILD.gn
index bcadd0f..99f9a01 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -79,3 +79,11 @@ group("audio_focus_grabber") {
     "//tools/android/audio_focus_grabber:audio_focus_grabber_apk",
   ]
 }
+
+# GYP: //tools/android/android_tools.gyp:push_apps_to_background
+group("push_apps_to_background") {
+  testonly = true
+  deps = [
+    "//tools/android/push_apps_to_background:push_apps_to_background_apk",
+  ]
+}
diff --git a/android_tools.gyp b/android_tools.gyp
index 065c804..3e1589f 100644
--- a/android_tools.gyp
+++ b/android_tools.gyp
@@ -85,5 +85,13 @@
         'audio_focus_grabber/audio_focus_grabber.gyp:audio_focus_grabber_apk',
       ],
     },
+    {
+      # GN: //tools/android:push_apps_to_background
+      'target_name': 'push_apps_to_background',
+      'type': 'none',
+      'dependencies': [
+        'push_apps_to_background/push_apps_to_background.gyp:push_apps_to_background_apk',
+      ],
+    },
   ],
 }
diff --git a/push_apps_to_background/AndroidManifest.xml b/push_apps_to_background/AndroidManifest.xml
new file mode 100644
index 0000000..92753f3
--- /dev/null
+++ b/push_apps_to_background/AndroidManifest.xml
@@ -0,0 +1,29 @@
+<!--
+ * Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+-->
+
+<manifest
+    xmlns:android="http://schemas.android.com/apk/res/android"
+    package="org.chromium.push_apps_to_background"
+    android:versionCode="1"
+    android:versionName="1.0" >
+
+    <uses-sdk android:minSdkVersion="16" android:targetSdkVersion="23" />
+
+    <application
+        android:icon="@drawable/ic_launcher"
+        android:label="@string/app_name"
+        android:theme="@android:style/Theme.Light" >
+        <activity
+            android:name="org.chromium.push_apps_to_background.PushAppsToBackgroundActivity"
+            android:label="@string/title_activity_push_apps_to_background"
+            android:exported="true">
+        <intent-filter>
+            <action android:name="android.intent.action.MAIN" />
+            <category android:name="android.intent.category.LAUNCHER" />
+        </intent-filter>
+        </activity>
+    </application>
+</manifest>
\ No newline at end of file
diff --git a/push_apps_to_background/BUILD.gn b/push_apps_to_background/BUILD.gn
new file mode 100644
index 0000000..cd90aa3
--- /dev/null
+++ b/push_apps_to_background/BUILD.gn
@@ -0,0 +1,23 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import("//build/config/android/rules.gni")
+import("//testing/test.gni")
+
+# Mark all targets as test only.
+testonly = true
+
+android_apk("push_apps_to_background_apk") {
+  apk_name = "PushAppsToBackground"
+  java_files = [ "src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java" ]
+  android_manifest = "AndroidManifest.xml"
+  deps = [
+    ":push_apps_to_background_apk_resources",
+  ]
+}
+
+android_resources("push_apps_to_background_apk_resources") {
+  resource_dirs = [ "res" ]
+  custom_package = "org.chromium.push_apps_to_background"
+}
diff --git a/push_apps_to_background/OWNERS b/push_apps_to_background/OWNERS
new file mode 100644
index 0000000..56fcbb8
--- /dev/null
+++ b/push_apps_to_background/OWNERS
@@ -0,0 +1,2 @@
+mikecase@chromium.org
+perezju@chromium.org
\ No newline at end of file
diff --git a/push_apps_to_background/push_apps_to_background.gyp b/push_apps_to_background/push_apps_to_background.gyp
new file mode 100644
index 0000000..25fca1f
--- /dev/null
+++ b/push_apps_to_background/push_apps_to_background.gyp
@@ -0,0 +1,21 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+{
+  'targets': [
+    # GN: //tools/android/push_apps_to_background:push_apps_to_background_apk
+    {
+      'target_name': 'push_apps_to_background_apk',
+      'type': 'none',
+      'variables': {
+        'apk_name': 'PushAppsToBackground',
+        'java_in_dir': '.',
+        'resource_dir': 'res',
+        'android_manifest_path': 'AndroidManifest.xml',
+      },
+      'includes': [
+        '../../../build/java_apk.gypi',
+      ],
+    },
+  ],
+}
diff --git a/push_apps_to_background/res/drawable-mdpi/ic_launcher.png b/push_apps_to_background/res/drawable-mdpi/ic_launcher.png
new file mode 100644
index 0000000..96a442e
Binary files /dev/null and b/push_apps_to_background/res/drawable-mdpi/ic_launcher.png differ
diff --git a/push_apps_to_background/res/layout/activity_push_apps_to_background.xml b/push_apps_to_background/res/layout/activity_push_apps_to_background.xml
new file mode 100644
index 0000000..9d292ca
--- /dev/null
+++ b/push_apps_to_background/res/layout/activity_push_apps_to_background.xml
@@ -0,0 +1,19 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!-- Copyright 2016 The Chromium Authors. All rights reserved.
+     Use of this source code is governed by a BSD-style license that can be
+     found in the LICENSE file.
+-->
+
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+    xmlns:tools="http://schemas.android.com/tools"
+    android:id="@+id/container"
+    android:layout_width="match_parent"
+    android:layout_height="match_parent"
+    android:orientation="vertical"
+    android:gravity="center">
+    <TextView
+        android:id="@+id/text_view"
+        android:layout_width="match_parent"
+        android:layout_height="match_parent"
+        android:text="@string/push_apps_to_background_message" />
+</LinearLayout>
\ No newline at end of file
diff --git a/push_apps_to_background/res/values/strings.xml b/push_apps_to_background/res/values/strings.xml
new file mode 100644
index 0000000..99f76a2
--- /dev/null
+++ b/push_apps_to_background/res/values/strings.xml
@@ -0,0 +1,15 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!-- Copyright 2016 The Chromium Authors. All rights reserved.
+     Use of this source code is governed by a BSD-style license that can be
+     found in the LICENSE file.
+-->
+
+<resources>
+    <string name="app_name">Push Apps To Background</string>
+    <string name="push_apps_to_background_message">
+        "<b>What is my purpose?</b>\n
+         <i>You push apps to the background.</i>\n
+         <b>Oh my God</b>."
+    </string>
+    <string name="title_activity_push_apps_to_background">Push Apps To Background</string>
+</resources>
\ No newline at end of file
diff --git a/push_apps_to_background/src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java b/push_apps_to_background/src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java
new file mode 100644
index 0000000..95e9a99
--- /dev/null
+++ b/push_apps_to_background/src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java
@@ -0,0 +1,21 @@
+// Copyright 2016 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.push_apps_to_background;
+
+import android.app.Activity;
+import android.os.Bundle;
+
+/**
+ * This activity is used in performance tests to push other apps
+ * to the background while running automated user stories.
+ */
+public class PushAppsToBackgroundActivity extends Activity {
+
+    @Override
+    public void onCreate(Bundle savedInstanceState) {
+        super.onCreate(savedInstanceState);
+        setContentView(R.layout.activity_push_apps_to_background);
+    }
+}
\ No newline at end of file

commit c6f7be5a1307420cc37d71a8226cbb1aa81f1d79
Author: droger <droger@chromium.org>
Date:   Thu Mar 10 07:00:03 2016 -0800

    tools/android/loading Initial infrastructure for trace collection on GCE
    
    This CL adds some configuration and a http server in prevision to
    running the trace collection on GCE.
    It is far from complete, and does not collect any traces yet.
    
    Review URL: https://codereview.chromium.org/1777523002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380404}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4962c0925d49b909e90ff6b6569b104abbdd9ed8

diff --git a/loading/gce/README.md b/loading/gce/README.md
new file mode 100644
index 0000000..eda5520
--- /dev/null
+++ b/loading/gce/README.md
@@ -0,0 +1,132 @@
+# Clovis in the Cloud: Developer Guide
+
+This document describes how to collect Chromium traces using Google Compute
+Engine.
+
+[TOC]
+
+## Initial setup
+
+Install the [gcloud command line tool][1].
+
+Checkout the source:
+
+```shell
+mkdir clovis
+cd clovis
+gcloud init
+```
+
+When offered, accept to clone the Google Cloud repo.
+
+## Update or Change the code
+
+Make changes to the code, or simply copy the latest version from Chromium into
+your local Google Cloud repository. Then commit and push:
+
+```shell
+git commit
+git push -u origin master
+```
+
+If there are instances already running, they need to be restarted for this to
+take effect.
+
+## Start the app in the cloud
+
+Create an instance using latest ubuntu LTS:
+
+```shell
+gcloud compute instances create clovis-tracer-1 \
+ --machine-type n1-standard-1 \
+ --image ubuntu-14-04 \
+ --zone europe-west1-c \
+ --tags clovis-http-server \
+ --scopes cloud-platform \
+ --metadata-from-file startup-script=default/startup-script.sh
+```
+
+This should output the IP address of the instance.
+Otherwise the IP address can be retrieved by doing:
+
+```shell
+gcloud compute instances list
+```
+
+Interact with the app on the port 8080 in your browser at
+`http://<instance-ip>:8080`.
+
+TODO: allow starting the instance in the cloud without Supervisor. This enables
+iterative development on the instance using SSH, manually starting and stopping
+the app. This can be done using [instance metadata][2].
+
+## Stop the app in the cloud
+
+```shell
+gcloud compute instances delete clovis-tracer-1
+```
+
+## Connect to the instance with SSH
+
+```shell
+gcloud compute ssh clovis-tracer-1
+```
+
+## Use the app locally
+
+Setup the local environment:
+
+```shell
+virtualenv env
+source env/bin/activate
+pip install -r pip_requirements.txt
+```
+
+Launch the app:
+
+```shell
+gunicorn --workers=2 main:app --bind 127.0.0.1:8000
+```
+
+In your browser, go to `http://localhost:8000` and use the app.
+
+Tear down the local environment:
+
+```shell
+deactivate
+```
+
+## Project-wide settings
+
+This is already setup, no need to do this again.
+Kept here for reference.
+
+### Server configuration file
+
+`main.py` expects to find a `server_config.json` file, which is a dictionary
+with the keys:
+
+*   `project_name`: the name of the Google Compute project,
+*   `bucket_name`: the name of the Google Storage bucket used to store the
+    results.
+
+### Firewall rule
+
+Firewall rule to allow access to the instance HTTP server from the outside:
+
+```shell
+gcloud compute firewall-rules create default-allow-http-8080 \
+    --allow tcp:8080 \
+    --source-ranges 0.0.0.0/0 \
+    --target-tags clovis-http-server \
+    --description "Allow port 8080 access to http-server"
+```
+
+The firewall rule can be disabled with:
+
+```shell
+gcloud compute firewall-rules delete default-allow-http-8080
+```
+
+[1]: https://cloud.google.com/sdk
+[2]: https://cloud.google.com/compute/docs/startupscript#custom
diff --git a/loading/gce/main.py b/loading/gce/main.py
new file mode 100644
index 0000000..67fe3d1
--- /dev/null
+++ b/loading/gce/main.py
@@ -0,0 +1,87 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+
+from gcloud import storage
+from gcloud.exceptions import NotFound
+from oauth2client.client import GoogleCredentials
+
+class ServerApp(object):
+  """Simple web server application, collecting traces and writing them in
+  Google Cloud Storage.
+  """
+
+  def __init__(self):
+    print 'Initializing credentials'
+    self._credentials = GoogleCredentials.get_application_default()
+    print 'Reading server configuration'
+    with open('server_config.json') as configuration_file:
+       self._config = json.load(configuration_file)
+
+  def _GetStorageClient(self):
+    return storage.Client(project = self._config['project_name'],
+                          credentials = self._credentials)
+
+  def _GetStorageBucket(self, storage_client):
+    return storage_client.get_bucket(self._config['bucket_name'])
+
+  def _UploadFile(self, file_stream, filename):
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename)
+    blob.upload_from_string(file_stream)
+    url = blob.public_url
+    return url
+
+  def _DeleteFile(self, filename):
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    try:
+      bucket.delete_blob(filename)
+      return True
+    except NotFound:
+      return False
+
+  def _ReadFile(self, filename):
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.get_blob(filename)
+    if not blob:
+      return None
+    return blob.download_as_string()
+
+  def __call__(self, environ, start_response):
+    path = environ['PATH_INFO']
+    if path == '/favicon.ico':
+        start_response('404 NOT FOUND', [('Content-Length', '0')])
+        return iter([''])
+
+    status = '200 OK'
+
+    if path == '/write':
+      url = self._UploadFile('foo', 'test.txt')
+      data = 'Writing file at\n' + url + '\n'
+    elif path == '/read':
+      data = self._ReadFile('test.txt')
+      if not data:
+        data = ''
+        status = '404 NOT FOUND'
+    elif path == '/delete':
+      if self._DeleteFile('test.txt'):
+        data = 'Success\n'
+      else:
+        data = 'Failed\n'
+    else:
+      data = environ['PATH_INFO'] + '\n'
+
+    response_headers = [
+        ('Content-type','text/plain'),
+        ('Content-Length', str(len(data)))
+    ]
+    start_response(status, response_headers)
+    return iter([data])
+
+
+app = ServerApp()
diff --git a/loading/gce/pip_requirements.txt b/loading/gce/pip_requirements.txt
new file mode 100644
index 0000000..e2d68b7
--- /dev/null
+++ b/loading/gce/pip_requirements.txt
@@ -0,0 +1,2 @@
+gunicorn==19.4.5
+gcloud==0.10.1
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
new file mode 100644
index 0000000..4f2cafa
--- /dev/null
+++ b/loading/gce/startup-script.sh
@@ -0,0 +1,58 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Script executed at instance startup. It installs the required dependencies,
+# downloads the source code, and starts a web server.
+
+set -v
+
+# Talk to the metadata server to get the project id
+PROJECTID=$(curl -s \
+    "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
+    -H "Metadata-Flavor: Google")
+
+# Install dependencies from apt
+apt-get update
+apt-get install -yq git supervisor python-pip python-dev libffi-dev libssl-dev
+
+# Create a pythonapp user. The application will run as this user.
+useradd -m -d /home/pythonapp pythonapp
+
+# pip from apt is out of date, so make it update itself and install virtualenv.
+pip install --upgrade pip virtualenv
+
+# Get the source code from the Google Cloud Repository
+# git requires $HOME and it's not set during the startup script.
+export HOME=/root
+git config --global credential.helper gcloud.sh
+git clone https://source.developers.google.com/p/$PROJECTID /opt/app/clovis
+
+# Install app dependencies
+virtualenv /opt/app/clovis/env
+/opt/app/clovis/env/bin/pip install -r /opt/app/clovis/pip_requirements.txt
+
+# Make sure the pythonapp user owns the application code
+chown -R pythonapp:pythonapp /opt/app
+
+# Configure supervisor to start gunicorn inside of our virtualenv and run the
+# applicaiton.
+cat >/etc/supervisor/conf.d/python-app.conf << EOF
+[program:pythonapp]
+directory=/opt/app/clovis
+command=/opt/app/clovis/env/bin/gunicorn --workers=2 main:app \
+  --bind 0.0.0.0:8080
+autostart=true
+autorestart=true
+user=pythonapp
+# Environment variables ensure that the application runs inside of the
+# configured virtualenv.
+environment=VIRTUAL_ENV="/opt/app/env/clovis",PATH="/opt/app/clovis/env/bin",\
+    HOME="/home/pythonapp",USER="pythonapp"
+stdout_logfile=syslog
+stderr_logfile=syslog
+EOF
+
+supervisorctl reread
+supervisorctl update
+

commit cf4dfdfa4a1aa8cb1aaec93e68c405198b5b1593
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 07:05:26 2016 -0800

    Clovis: improve method name and typo in analyze.py.
    
    Review URL: https://codereview.chromium.org/1775393002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380139}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d596348857f6d10658e8c4d62414120e8cec501a

diff --git a/loading/analyze.py b/loading/analyze.py
index 7712123..4b72ec6 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -120,8 +120,8 @@ def _LogRequests(url, clear_cache_override=None):
   if OPTIONS.emulate_device:
     chrome_ctl.SetDeviceEmulation(OPTIONS.emulate_device)
   if OPTIONS.emulate_network:
-    chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_device)
-  trace = loading_trace.LoadingTrace.FromUrlController(url, chrome_ctl)
+    chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_network)
+  trace = loading_trace.LoadingTrace.FromUrlAndController(url, chrome_ctl)
   return trace.ToJsonDict()
 
 
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index feb6c38..50dbc23 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -71,7 +71,7 @@ class LoadingTrace(object):
       return cls.FromJsonDict(json.load(input_file))
 
   @classmethod
-  def FromUrlController(
+  def FromUrlAndController(
       cls, url, controller, categories=None,
       timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
     """Create a loading trace by using controller to fetch url.
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 31fee71..7c9e822 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -35,7 +35,7 @@ def MonitorUrl(connection, url, clear_cache=False,
                timeout=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
   """Monitor a URL via a trace recorder.
 
-  DEPRECATED! Use LoadingTrace.FromUrlController instead.
+  DEPRECATED! Use LoadingTrace.FromUrlAndController instead.
 
   Args:
     connection: A devtools_monitor.DevToolsConnection instance.

commit e5125daf176bced4813ac0e1570ef49592ef6954
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 06:06:01 2016 -0800

    Use new chrome controller for analyze.py.
    
    Also updates the controller to track cache clearing.
    
    Review URL: https://codereview.chromium.org/1778543002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380131}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b6e375669d7dadecd6bbb252a56196ecee8843b5

diff --git a/loading/analyze.py b/loading/analyze.py
index 17ab770..7712123 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -25,15 +25,14 @@ import devil_chromium
 from pylib import constants
 
 import activity_lens
-import chrome_setup
 import content_classification_lens
+import controller
 import device_setup
 import frame_load_lens
 import loading_model
 import loading_trace
 import model_graph
 import options
-import trace_recorder
 
 
 # TODO(mattcary): logging.info isn't that useful, as the whole (tools) world
@@ -109,18 +108,21 @@ def _LogRequests(url, clear_cache_override=None):
   Returns:
     JSON dict of logged information (ie, a dict that describes JSON).
   """
-  device = device_setup.GetFirstDevice() if not OPTIONS.local else None
+  if OPTIONS.local:
+    chrome_ctl = controller.LocalChromeController()
+  else:
+    chrome_ctl = controller.RemoteChromeController(
+        device_setup.GetFirstDevice())
+
   clear_cache = (clear_cache_override if clear_cache_override is not None
                  else OPTIONS.clear_cache)
-
-  with device_setup.DeviceConnection(device) as connection:
-    additional_metadata = {}
-    if OPTIONS.local:
-      additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
-          connection, OPTIONS.emulate_device, OPTIONS.emulate_network)
-    trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
-    trace.metadata.update(additional_metadata)
-    return trace.ToJsonDict()
+  chrome_ctl.SetClearCache(clear_cache)
+  if OPTIONS.emulate_device:
+    chrome_ctl.SetDeviceEmulation(OPTIONS.emulate_device)
+  if OPTIONS.emulate_network:
+    chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_device)
+  trace = loading_trace.LoadingTrace.FromUrlController(url, chrome_ctl)
+  return trace.ToJsonDict()
 
 
 def _FullFetch(url, json_output, prefetch):
diff --git a/loading/controller.py b/loading/controller.py
index 28b3e97..09c794b 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,6 +10,8 @@ desktop-specific versions.
 """
 
 import contextlib
+import datetime
+import logging
 import os
 import shutil
 import subprocess
@@ -50,14 +52,23 @@ class ChromeControllerBase(object):
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
+    self._clear_cache = False
 
   def AddChromeArgument(self, arg):
     """Add command-line argument to the chrome execution."""
     self._chrome_args.append(arg)
 
+  def SetClearCache(self, clear_cache=True):
+    self._clear_cache = clear_cache
+
   @contextlib.contextmanager
   def Open(self):
-    """Context that returns a connection/chrome instance."""
+    """Context that returns a connection/chrome instance.
+
+    Returns:
+      DevToolsConnection instance for which monitoring has been set up but not
+      started.
+    """
     raise NotImplementedError
 
   def ChromeMetadata(self):
@@ -98,6 +109,11 @@ class ChromeControllerBase(object):
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
 
+    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
+                          seconds_since_epoch=time.time())
+    if self._clear_cache:
+      connection.AddHook(connection.ClearCache)
+
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
@@ -125,7 +141,7 @@ class RemoteChromeController(ChromeControllerBase):
     self._device.KillAll(package_info.package, quiet=True)
 
     with device_setup.FlagReplacer(
-        self._device, command_line_path, self._chrome_flags):
+        self._device, command_line_path, self._chrome_args):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
@@ -190,19 +206,23 @@ class LocalChromeController(ChromeControllerBase):
     binary_filename = OPTIONS.local_binary
     profile_dir = OPTIONS.local_profile_dir
     using_temp_profile_dir = profile_dir is None
-    flags = self._chrome_flags
+    flags = self._chrome_args
     if using_temp_profile_dir:
       profile_dir = tempfile.mkdtemp()
-      flags = '--user-data-dir=%s' + flags
+    flags = ['--user-data-dir=%s' % profile_dir] + flags
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     process = subprocess.Popen(
         [binary_filename] + flags, shell=False, stderr=chrome_out)
     try:
       time.sleep(10)
-      connection =  devtools_monitor.DevToolsConnection(
-          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-      self._StartConnection(connection)
-      yield connection
+      process_result = process.poll()
+      if process_result is not None:
+        logging.error('Unexpected process exit: %s', process_result)
+      else:
+        connection = devtools_monitor.DevToolsConnection(
+            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+        self._StartConnection(connection)
+        yield connection
     finally:
       process.kill()
       if using_temp_profile_dir:
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 110aa79..1ea5d91 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -20,7 +20,7 @@ from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
 
-DEFAULT_TIMEOUT = 10 # seconds
+DEFAULT_TIMEOUT_SECONDS = 10 # seconds
 
 
 class DevToolsConnectionException(Exception):
@@ -98,7 +98,6 @@ class DevToolsConnection(object):
     self._scoped_states = {}
     self._domains_to_enable = set()
     self._tearing_down_tracing = False
-    self._set_up = False
     self._please_stop = False
     self._hooks = []
 
@@ -209,14 +208,23 @@ class DevToolsConnection(object):
     self.SyncRequest('Network.clearBrowserCache')
 
   def AddHook(self, hook):
-    """Add hook to be run on monitoring setup.
+    """Add hook to be run on monitoring start.
 
     Args:
       hook: a function.
     """
     self._hooks.append(hook)
 
-  def SetUpMonitoring(self):
+  def MonitorUrl(self, url, timeout_seconds=DEFAULT_TIMEOUT_SECONDS):
+    """Navigate to url and dispatch monitoring loop.
+
+    Unless you have registered a listener that will call StopMonitoring, this
+    will run until timeout from chrome.
+
+    Args:
+      url: (str) a URL to navigate to before starting monitoring loop.\
+      timeout_seconds: timeout in seconds for monitoring loop.
+    """
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
       if domain != self.TRACING_DOMAIN:
@@ -226,28 +234,20 @@ class DevToolsConnection(object):
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][0])
-
     for hook in self._hooks:
       hook()
-
     self._tearing_down_tracing = False
-    self._set_up = True
 
-  def StartMonitoring(self, timeout=DEFAULT_TIMEOUT):
-    """Starts monitoring.
+    self.SendAndIgnoreResponse('Page.navigate', {'url': url})
 
-    DevToolsConnection.SetUpMonitoring() has to be called first.
-    """
-    assert self._set_up, 'DevToolsConnection.SetUpMonitoring not called.'
-    self._Dispatch(timeout=timeout)
+    self._Dispatch(timeout=timeout_seconds)
     self._TearDownMonitoring()
 
   def StopMonitoring(self):
     """Stops the monitoring."""
     self._please_stop = True
 
-  def _Dispatch(self, kind='Monitoring',
-                timeout=DEFAULT_TIMEOUT):
+  def _Dispatch(self, timeout, kind='Monitoring'):
     self._please_stop = False
     while not self._please_stop:
       try:
@@ -262,7 +262,7 @@ class DevToolsConnection(object):
       logging.info('Fetching tracing')
       self.SyncRequestNoResponse(self.TRACING_END_METHOD)
       self._tearing_down_tracing = True
-      self._Dispatch(kind='Tracing', timeout=self.TRACING_TIMEOUT)
+      self._Dispatch(timeout=self.TRACING_TIMEOUT, kind='Tracing')
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][1])
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 344baeb..feb6c38 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -6,6 +6,7 @@
 
 import json
 
+import devtools_monitor
 import page_track
 import request_track
 import tracing
@@ -68,3 +69,28 @@ class LoadingTrace(object):
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:
       return cls.FromJsonDict(json.load(input_file))
+
+  @classmethod
+  def FromUrlController(
+      cls, url, controller, categories=None,
+      timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
+    """Create a loading trace by using controller to fetch url.
+
+    Args:
+      url: (str) url to fetch.
+      controller: (ChromeControllerBase) controller to manage the connection.
+      categories: TracingTrack categories to capture.
+      timeout_seconds: monitoring connection timeout in seconds.
+
+    Returns:
+      LoadingTrace instance.
+    """
+    with controller.Open() as connection:
+      page = page_track.PageTrack(connection)
+      request = request_track.RequestTrack(connection)
+      trace = tracing.TracingTrack(
+          connection,
+          categories=(tracing.DEFAULT_CATEGORIES if categories is None
+                      else categories))
+      connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
+      return cls(url, controller.ChromeMetadata(), page, request, trace)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index de59b35..31fee71 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -3,7 +3,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Loading trace recorder."""
+"""Loading trace recorder. DEPRECATED!"""
 
 import argparse
 import datetime
@@ -32,9 +32,11 @@ import tracing
 
 def MonitorUrl(connection, url, clear_cache=False,
                categories=tracing.DEFAULT_CATEGORIES,
-               timeout=devtools_monitor.DEFAULT_TIMEOUT):
+               timeout=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
   """Monitor a URL via a trace recorder.
 
+  DEPRECATED! Use LoadingTrace.FromUrlController instead.
+
   Args:
     connection: A devtools_monitor.DevToolsConnection instance.
     url: url to navigate to as string.

commit 9a3cfc8151998ec3a1563691787fd42e70d0eafb
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 02:41:12 2016 -0800

    Add hooks to devtools monitor connection to enable configuration from a controller.
    
    Review URL: https://codereview.chromium.org/1779433002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380115}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e14c5dbb245da627c253528352570adf206de019

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 3ea8eea..110aa79 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -100,6 +100,7 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self._set_up = False
     self._please_stop = False
+    self._hooks = []
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -207,6 +208,14 @@ class DevToolsConnection(object):
     assert res['result'], 'Cache clearing is not supported by this browser.'
     self.SyncRequest('Network.clearBrowserCache')
 
+  def AddHook(self, hook):
+    """Add hook to be run on monitoring setup.
+
+    Args:
+      hook: a function.
+    """
+    self._hooks.append(hook)
+
   def SetUpMonitoring(self):
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
@@ -217,6 +226,10 @@ class DevToolsConnection(object):
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][0])
+
+    for hook in self._hooks:
+      hook()
+
     self._tearing_down_tracing = False
     self._set_up = True
 

commit 271606fbb4039c9a27e73600ec1cab6a5e855044
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 01:59:33 2016 -0800

    Ignore missing timings for about protocol.
    
    Review URL: https://codereview.chromium.org/1774993002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380111}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ee641ea1c18485b7b05dc226a9473b4ded2a0b83

diff --git a/loading/request_track.py b/loading/request_track.py
index 685ab0d..bba015d 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -495,7 +495,7 @@ class RequestTrack(devtools_monitor.Track):
     # data URLs don't have a timing dict, and timings for cached requests are
     # stale.
     # TODO(droger): the timestamp is inacurate, get the real timings instead.
-    if r.protocol == 'data' or r.served_from_cache:
+    if r.protocol in ('data', 'about') or r.served_from_cache:
       timing_dict = {'requestTime': r.timestamp}
     else:
       timing_dict = response['timing']

commit d61c96c356da892c6b385c49ddb07ef4e4aee0f7
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 8 03:28:32 2016 -0800

    Device controller refactor.
    
    First step in refactoring connection setup and chrome launching in a way that's
    friendly to sandwich, clovis, pre* and any others.
    
    Review URL: https://codereview.chromium.org/1731973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379807}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d996937e3918d382fddf3f53a862225f49d7d683

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
index 9bf2908..245246d 100644
--- a/loading/chrome_setup.py
+++ b/loading/chrome_setup.py
@@ -2,7 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Handles Chrome's configuration."""
+"""Handles Chrome's configuration. DEPRECATED!"""
 
 import contextlib
 import json
diff --git a/loading/controller.py b/loading/controller.py
new file mode 100644
index 0000000..28b3e97
--- /dev/null
+++ b/loading/controller.py
@@ -0,0 +1,209 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Controller objects that control the context in which chrome runs.
+
+This is responsible for the setup necessary for launching chrome, and for
+creating a DevToolsConnection. There are remote device and local
+desktop-specific versions.
+"""
+
+import contextlib
+import os
+import shutil
+import subprocess
+import sys
+import tempfile
+import time
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+import chrome_cache
+import device_setup
+import devtools_monitor
+import emulation
+import options
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android.sdk import intent
+
+OPTIONS = options.OPTIONS
+
+# An estimate of time to wait for the device to become idle after expensive
+# operations, such as opening the launcher activity.
+_TIME_TO_DEVICE_IDLE_SECONDS = 2
+
+
+class ChromeControllerBase(object):
+  """Base class for all controllers.
+
+  Defines common operations but should not be created directly.
+  """
+  def __init__(self):
+    self._chrome_args = [
+        '--disable-fre',
+        '--enable-test-events',
+        '--remote-debugging-port=%d' % OPTIONS.devtools_port,
+    ]
+    self._metadata = {}
+    self._emulated_device = None
+    self._emulated_network = None
+
+  def AddChromeArgument(self, arg):
+    """Add command-line argument to the chrome execution."""
+    self._chrome_args.append(arg)
+
+  @contextlib.contextmanager
+  def Open(self):
+    """Context that returns a connection/chrome instance."""
+    raise NotImplementedError
+
+  def ChromeMetadata(self):
+    """Return metadata such as emulation information.
+
+    Returns:
+      Metadata as JSON dictionary.
+    """
+    return self._metadata
+
+  def GetDevice(self):
+    """Returns an android device, or None if chrome is local."""
+    return None
+
+  def SetDeviceEmulation(self, device_name):
+    """Set device emulation.
+
+    Args:
+      device_name: (str) Key from --devices_file.
+    """
+    devices = emulation.LoadEmulatedDevices(file(OPTIONS.devices_file))
+    self._emulated_device = devices[device_name]
+
+  def SetNetworkEmulation(self, network_name):
+    """Set network emulation.
+
+    Args:
+      network_name: (str) Key from emulation.NETWORK_CONDITIONS.
+    """
+    self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
+
+  def _StartConnection(self, connection):
+    """This should be called after opening an appropriate connection."""
+    if self._emulated_device:
+      self._metadata.update(emulation.SetUpDeviceEmulationAndReturnMetadata(
+          connection, self._emulated_device))
+    if self._emulated_network:
+      emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
+      self._metadata.update(self._emulated_network)
+
+
+class RemoteChromeController(ChromeControllerBase):
+  """A controller for an android device, aka remote chrome instance."""
+  # Seconds to sleep after starting chrome activity.
+  POST_ACTIVITY_SLEEP_SECONDS = 2
+
+  def __init__(self, device):
+    """Initialize the controller.
+
+    Args:
+      device: an andriod device.
+    """
+    assert device is not None, 'Should you be using LocalController instead?'
+    self._device = device
+    super(RemoteChromeController, self).__init__()
+    self._slow_death = False
+
+  @contextlib.contextmanager
+  def Open(self):
+    """Overridden connection creation."""
+    package_info = OPTIONS.ChromePackage()
+    command_line_path = '/data/local/chrome-command-line'
+
+    self._device.EnableRoot()
+    self._device.KillAll(package_info.package, quiet=True)
+
+    with device_setup.FlagReplacer(
+        self._device, command_line_path, self._chrome_flags):
+      start_intent = intent.Intent(
+          package=package_info.package, activity=package_info.activity,
+          data='about:blank')
+      self._device.StartActivity(start_intent, blocking=True)
+      time.sleep(self.POST_ACTIVITY_SLEEP_SECONDS)
+      with device_setup.ForwardPort(
+          self._device, 'tcp:%d' % OPTIONS.devtools_port,
+          'localabstract:chrome_devtools_remote'):
+        connection = devtools_monitor.DevToolsConnection(
+            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+        self._StartConnection(connection)
+        yield connection
+    if self._slow_death:
+      self._device.adb.Shell('am start com.google.android.launcher')
+      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    self._device.KillAll(package_info.package, quiet=True)
+
+  def PushBrowserCache(self, cache_path):
+    """Push a chrome cache.
+
+    Args:
+      cache_path: The directory's path containing the cache locally.
+    """
+    chrome_cache.PushBrowserCache(self._device, cache_path)
+
+  def PullBrowserCache(self):
+    """Pull a chrome cache.
+
+    Returns:
+      Temporary directory containing all the browser cache. Caller will need to
+      remove this directory manually.
+    """
+    return chrome_cache.PullBrowserCache(self._device)
+
+  def SetSlowDeath(self, slow_death=True):
+    """Set to pause before final kill of chrome.
+
+    Gives time for caches to write.
+
+    Args:
+      slow_death: (bool) True if you want that which comes to all who live, to
+        be slow.
+    """
+    self._slow_death = slow_death
+
+
+class LocalChromeController(ChromeControllerBase):
+  """Controller for a local (desktop) chrome instance.
+
+  TODO(gabadie): implement cache push/pull and declare up in base class.
+  """
+  def __init__(self):
+    super(LocalChromeController, self).__init__()
+    if OPTIONS.no_sandbox:
+      self.AddChromeArgument('--no-sandbox')
+
+  @contextlib.contextmanager
+  def Open(self):
+    """Override for connection context."""
+    binary_filename = OPTIONS.local_binary
+    profile_dir = OPTIONS.local_profile_dir
+    using_temp_profile_dir = profile_dir is None
+    flags = self._chrome_flags
+    if using_temp_profile_dir:
+      profile_dir = tempfile.mkdtemp()
+      flags = '--user-data-dir=%s' + flags
+    chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+    process = subprocess.Popen(
+        [binary_filename] + flags, shell=False, stderr=chrome_out)
+    try:
+      time.sleep(10)
+      connection =  devtools_monitor.DevToolsConnection(
+          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+      self._StartConnection(connection)
+      yield connection
+    finally:
+      process.kill()
+      if using_temp_profile_dir:
+        shutil.rmtree(profile_dir)
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 62b99b4..bb2b4d7 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -97,6 +97,7 @@ def ForwardPort(device, local, remote):
     device.adb.ForwardRemove(local)
 
 
+# Deprecated
 def _SetUpDevice(device, package_info):
   """Enables root and closes Chrome on a device."""
   device.EnableRoot()
@@ -189,6 +190,7 @@ def WprHost(device, wpr_archive_path, record=False,
     shutil.rmtree(temp_certificate_dir)
 
 
+# Deprecated
 @contextlib.contextmanager
 def _DevToolsConnectionOnDevice(device, flags):
   """Returns a DevToolsConnection context manager for a given device.
@@ -215,6 +217,7 @@ def _DevToolsConnectionOnDevice(device, flags):
           OPTIONS.devtools_hostname, OPTIONS.devtools_port)
 
 
+# Deprecated, use *Controller.
 def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
 
diff --git a/loading/emulation.py b/loading/emulation.py
new file mode 100644
index 0000000..fa2fd89
--- /dev/null
+++ b/loading/emulation.py
@@ -0,0 +1,124 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Device and network emulation utilities via devtools."""
+
+import json
+
+# Copied from
+# WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
+# Units:
+#   download/upload: byte/s
+#   latency: ms
+NETWORK_CONDITIONS = {
+    'GPRS': {
+        'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
+    'Regular 2G': {
+        'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
+    'Good 2G': {
+        'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
+    'Regular 3G': {
+        'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
+    'Good 3G': {
+        'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
+        'latency': 40},
+    'Regular 4G': {
+        'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
+        'latency': 20},
+    'DSL': {
+        'download': 2 * 1024 * 1024 / 8, 'upload': 1 * 1024 * 1024 / 8,
+        'latency': 5},
+    'WiFi': {
+        'download': 30 * 1024 * 1024 / 8, 'upload': 15 * 1024 * 1024 / 8,
+        'latency': 2}
+}
+
+
+def LoadEmulatedDevices(registry):
+  """Loads a list of emulated devices from the DevTools JSON registry.
+
+  See, for example, third_party/WebKit/Source/devtools/front_end
+  /emulated_devices/module.json.
+
+  Args:
+    registry: A file-like object for the device registry (should be JSON).
+
+  Returns:
+    {'device_name': device}
+  """
+  json_dict = json.load(registry)
+  devices = {}
+  for device in json_dict['extensions']:
+    device = device['device']
+    devices[device['title']] = device
+  return devices
+
+
+def SetUpDeviceEmulationAndReturnMetadata(connection, device):
+  """Configures an instance of Chrome for device emulation.
+
+  Args:
+    connection: (DevToolsConnection)
+    device: (dict) An entry from LoadEmulatedDevices().
+
+  Returns:
+    A dict containing the device emulation metadata.
+  """
+  res = connection.SyncRequest('Emulation.canEmulate')
+  assert res['result'], 'Cannot set device emulation.'
+  data = _GetDeviceEmulationMetadata(device)
+  connection.SyncRequestNoResponse(
+      'Emulation.setDeviceMetricsOverride',
+      {'width': data['width'],
+       'height': data['height'],
+       'deviceScaleFactor': data['deviceScaleFactor'],
+       'mobile': data['mobile'],
+       'fitWindow': True})
+  connection.SyncRequestNoResponse('Network.setUserAgentOverride',
+                                   {'userAgent': data['userAgent']})
+  return data
+
+
+def SetUpNetworkEmulation(connection, latency, download, upload):
+  """Configures an instance of Chrome for network emulation.
+
+  See NETWORK_CONDITIONS for example (or valid?) emulation options.
+
+  Args:
+    connection: (DevToolsConnection)
+    latency: (float) Latency in ms.
+    download: (float) Download speed (Bytes / s).
+    upload: (float) Upload speed (Bytes / s).
+  """
+  res = connection.SyncRequest('Network.canEmulateNetworkConditions')
+  assert res['result'], 'Cannot set network emulation.'
+  connection.SyncRequestNoResponse(
+      'Network.emulateNetworkConditions',
+      {'offline': False, 'latency': latency, 'downloadThroughput': download,
+       'uploadThroughput': upload})
+
+
+def BandwidthToString(bandwidth):
+  """Converts a bandwidth to string.
+
+  Args:
+    bandwidth: The bandwidth to convert in byte/s. Must be a multiple of 1024/8.
+
+  Returns:
+    A string compatible with wpr --{up,down} command line flags.
+  """
+  assert bandwidth % (1024/8) == 0
+  bandwidth_kbps = (int(bandwidth) * 8) / 1024
+  if bandwidth_kbps % 1024:
+    return '{}Kbit/s'.format(bandwidth_kbps)
+  return '{}Mbit/s'.format(bandwidth_kbps / 1024)
+
+
+def _GetDeviceEmulationMetadata(device):
+  """Returns the metadata associated with a given device."""
+  return {'width': device['screen']['vertical']['width'],
+          'height': device['screen']['vertical']['height'],
+          'deviceScaleFactor': device['screen']['device-pixel-ratio'],
+          'mobile': 'mobile' in device['capabilities'],
+          'userAgent': device['user-agent']}
diff --git a/loading/emulation_unittest.py b/loading/emulation_unittest.py
new file mode 100644
index 0000000..9777e58
--- /dev/null
+++ b/loading/emulation_unittest.py
@@ -0,0 +1,81 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+from StringIO import StringIO
+import unittest
+
+import emulation
+import test_utils
+
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+
+class EmulationTestCase(unittest.TestCase):
+  def testLoadDevices(self):
+    devices = emulation.LoadEmulatedDevices(file(os.path.join(
+        _SRC_DIR, 'third_party/WebKit/Source/devtools/front_end',
+        'emulated_devices/module.json')))
+    # Just check we have something. We'll assume that if we were able to read
+    # the file without dying we must be ok.
+    self.assertTrue(devices)
+
+  def testSetUpDevice(self):
+    registry = StringIO("""{
+      "extensions": [
+          {
+              "type": "emulated-device",
+              "device": {
+                  "show-by-default": false,
+                  "title": "mattPhone" ,
+                  "screen": {
+                      "horizontal": {
+                          "width": 480,
+                          "height": 320
+                      },
+                      "device-pixel-ratio": 2,
+                      "vertical": {
+                          "width": 320,
+                          "height": 480
+                      }
+                  },
+                  "capabilities": [
+                      "touch",
+                      "mobile"
+                  ],
+                  "user-agent": "James Bond"
+              }
+          } ]}""")
+    devices = emulation.LoadEmulatedDevices(registry)
+    connection = test_utils.MockConnection(self)
+    connection.ExpectSyncRequest({'result': True}, 'Emulation.canEmulate')
+    metadata = emulation.SetUpDeviceEmulationAndReturnMetadata(
+        connection, devices['mattPhone'])
+    self.assertEqual(320, metadata['width'])
+    self.assertEqual('James Bond', metadata['userAgent'])
+    self.assertTrue(connection.AllExpectationsUsed())
+    self.assertEqual('Emulation.setDeviceMetricsOverride',
+                     connection.no_response_requests_seen[0][0])
+
+  def testSetUpNetwork(self):
+    connection = test_utils.MockConnection(self)
+    connection.ExpectSyncRequest({'result': True},
+                                 'Network.canEmulateNetworkConditions')
+    emulation.SetUpNetworkEmulation(connection, 120, 2048, 1024)
+    self.assertTrue(connection.AllExpectationsUsed())
+    self.assertEqual('Network.emulateNetworkConditions',
+                     connection.no_response_requests_seen[0][0])
+    self.assertEqual(
+        1024,
+        connection.no_response_requests_seen[0][1]['uploadThroughput'])
+
+  def testBandwidthToString(self):
+    self.assertEqual('16Kbit/s', emulation.BandwidthToString(2048))
+    self.assertEqual('8Mbit/s', emulation.BandwidthToString(1024 * 1024))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 5d2b617..f1c80c2 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -163,3 +163,47 @@ class TestResourceGraph(loading_model.ResourceGraph):
   @classmethod
   def FromRequestList(cls, requests, page_events=None, trace_events=None):
     return cls(LoadingTraceFromEvents(requests, page_events, trace_events))
+
+
+class MockConnection(object):
+  """Mock out connection for testing.
+
+  Use Expect* for requests expecting a repsonse. SyncRequestNoResponse puts
+  requests into no_response_requests_seen.
+
+  TODO(mattcary): use a standard mock system (the back-ported python3
+  unittest.mock? devil.utils.mock_calls?)
+
+  """
+  def __init__(self, test_case):
+    # List of (method, params) tuples.
+    self.no_response_requests_seen = []
+
+    self._test_case = test_case
+    self._expected_responses = {}
+
+  def ExpectSyncRequest(self, response, method, params=None):
+    """Test method when the connection is expected to make a SyncRequest.
+
+    Args:
+      response: (dict) the response to generate.
+      method: (str) the expected method in the call.
+      params: (dict) the expected params in the call.
+    """
+    self._expected_responses.setdefault(method, []).append((params, response))
+
+  def AllExpectationsUsed(self):
+    """Returns true when all expectations where used."""
+    return not self._expected_responses
+
+  def SyncRequestNoResponse(self, method, params):
+    """Mocked method."""
+    self.no_response_requests_seen.append((method, params))
+
+  def SyncRequest(self, method, params=None):
+    """Mocked method."""
+    expected_params, response = self._expected_responses[method].pop(0)
+    if not self._expected_responses[method]:
+      del self._expected_responses[method]
+    self._test_case.assertEqual(expected_params, params)
+    return response

commit 28e497bdfd82ad22e30f7d590adea21d1c6311dc
Author: tedchoc <tedchoc@chromium.org>
Date:   Mon Mar 7 16:48:10 2016 -0800

    Update eclipse classpath to include updated enum paths
    
    For the removed targets, I did a git grep and did not
    see any references.
    
    Command I used to generate the list:
    for enumpath in `ls -w1 out/Debug/gen/enums/`; do echo "    <classpathentry kind=\"src\" path=\"out/Debug/gen/enums/$enumpath\"/>"; done
    
    I built the following targets:
    chrome_apk chrome_test_apk custom_tabs_client_example_apk cronet_sample_apk
    
    I suspect the two actual targets that are required are:
    chrome_apk cronet_sample_apk
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1767303002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379689}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5e5d53e4e9e7361a2fd390c9a49b71be4dc27d46

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 62d3c39..d9a4516 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -98,8 +98,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/accessibility_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/activity_type_ids_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/android_resource_type_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_java/"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_type_java/"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/ax_enumerations_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_application_state"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_library_load_from_apk_status_codes"/>
@@ -107,46 +107,63 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_memory_pressure_level"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/bitmap_format_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/bookmark_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/browsing_data_time_period_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/browsing_data_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/certificate_mime_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/cert_verify_status_android_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/connection_security_levels_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/chromium_url_request_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/connectivity_check_result_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/console_message_level_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_gamepad_mapping"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_setting_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_settings_type_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/cronet_url_request_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/data_use_ui_message_enum_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/device_sensors_consts_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/gesture_event_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/http_cache_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/infobar_action_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/infobar_delegate_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/invalidate_types_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/investigated_scenario_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/media_android_captureapitype"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/media_android_imageformat"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/model_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/most_visited_tile_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/navigation_controller_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/net_request_priority_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/network_change_notifier_android_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/network_change_notifier_types_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/network_quality_observations_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/offline_page_feature_enums_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/offline_page_model_enums_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/page_info_connection_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/page_transition_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/popup_item_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/private_key_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/profile_account_management_metrics_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/profile_sync_service_model_type_selection_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/readback_response_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/result_codes_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/screen_orientation_values_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/security_state_enums_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/selection_event_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/sensor_manager_android_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/shortcut_source_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/signin_metrics_enum_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/speech_recognition_error_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/stop_source_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/system_ui_resource_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/tab_load_status_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/text_input_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/top_controls_state_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/touch_device_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/touch_handle_orientation_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/traffic_stats_error_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/url_request_error_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/url_request_failed_job_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/usb_descriptors_javagen"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_display_mode"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_input_event_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/website_settings_action_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_text_input_type"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/window_open_disposition_java"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/base_build_config_gen"/>

commit d00baca1150220a5e5443c26b48daa0b1be26ce8
Author: lizeb <lizeb@chromium.org>
Date:   Mon Mar 7 09:00:26 2016 -0800

    tools/android/loading: Network Activity lens, and CPU/network activity graphs.
    
    Review URL: https://codereview.chromium.org/1732413002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379565}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d40c3b58207b7bbc0cb7f6379f526d93417101ac

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index c99d47a..f6165d9 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -76,7 +76,7 @@ class ActivityLens(object):
                      - max(start_msec, event.start_msec)))
 
   @classmethod
-  def _ThreadBusiness(cls, events, start_msec, end_msec):
+  def _ThreadBusyness(cls, events, start_msec, end_msec):
     """Amount of time a thread spent executing from the message loop."""
     busy_duration = 0
     message_loop_events = [
@@ -179,7 +179,7 @@ class ActivityLens(object):
     assert end_msec - start_msec >= 0.
     events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
     result = {'edge_cost': end_msec - start_msec,
-              'busy': self._ThreadBusiness(events, start_msec, end_msec),
+              'busy': self._ThreadBusyness(events, start_msec, end_msec),
               'parsing': self._Parsing(events, start_msec, end_msec),
               'script': self._ScriptsExecuting(events, start_msec, end_msec)}
     return result
@@ -219,6 +219,16 @@ class ActivityLens(object):
         breakdown[x] for x in ('script', 'parsing', 'other_url', 'unknown_url'))
     return breakdown
 
+  def MainRendererThreadBusyness(self, start_msec, end_msec):
+    """Returns the amount of time the main renderer thread was busy.
+
+    Args:
+      start_msec: (float) Start of the interval.
+      end_msec: (float) End of the interval.
+    """
+    events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
+    return self._ThreadBusyness(events, start_msec, end_msec)
+
 
 class _EventsTree(object):
   """Builds the hierarchy of events from a list of fully nested events."""
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index 5abc172..f0c1275 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -11,7 +11,7 @@ import test_utils
 import tracing
 
 
-class ActivityLensTestCast(unittest.TestCase):
+class ActivityLensTestCase(unittest.TestCase):
   @classmethod
   def _EventsFromRawEvents(cls, raw_events):
     tracing_track = tracing.TracingTrack(None)
@@ -83,7 +83,7 @@ class ActivityLensTestCast(unittest.TestCase):
     self.assertEquals((1, second_renderer_tid),
                       ActivityLens._GetRendererMainThreadId(events))
 
-  def testThreadBusiness(self):
+  def testThreadBusyness(self):
     raw_events =  [
         {u'args': {},
          u'cat': u'toplevel',
@@ -104,10 +104,10 @@ class ActivityLensTestCast(unittest.TestCase):
          u'ts': 0,
          u'tts': 0}]
     events = self._EventsFromRawEvents(raw_events)
-    self.assertEquals(200, ActivityLens._ThreadBusiness(events, 0, 1000))
+    self.assertEquals(200, ActivityLens._ThreadBusyness(events, 0, 1000))
     # Clamping duration.
-    self.assertEquals(100, ActivityLens._ThreadBusiness(events, 0, 100))
-    self.assertEquals(50, ActivityLens._ThreadBusiness(events, 25, 75))
+    self.assertEquals(100, ActivityLens._ThreadBusyness(events, 0, 100))
+    self.assertEquals(50, ActivityLens._ThreadBusyness(events, 25, 75))
 
   def testScriptExecuting(self):
     url = u'http://example.com/script.js'
@@ -249,6 +249,53 @@ class ActivityLensTestCast(unittest.TestCase):
          'other_url': 6., 'unknown_url': 7.},
         activity.BreakdownEdgeActivityByInitiator(dep))
 
+  def testMainRendererThreadBusyness(self):
+    raw_events =  [
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 1,
+         u'tid': 12,
+         u'ts': 0},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 200 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 12,
+         u'ts': 0,
+         u'tts': 56485},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 8 * 200,
+         u'name': u'MessageLoop::NestedSomething',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 12,
+         u'ts': 0,
+         u'tts': 0},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 500 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 12,
+         u'tid': 12,
+         u'ts': 0,
+         u'tts': 56485}]
+    lens = self._ActivityLens([], raw_events)
+    # Ignore events from another PID.
+    self.assertEquals(200, lens.MainRendererThreadBusyness(0, 1000))
+    # Clamping duration.
+    self.assertEquals(100, lens.MainRendererThreadBusyness(0, 100))
+    self.assertEquals(50, lens.MainRendererThreadBusyness(25, 75))
+    # Other PID.
+    raw_events[0]['pid'] = 12
+    lens = self._ActivityLens([], raw_events)
+    self.assertEquals(500, lens.MainRendererThreadBusyness(0, 1000))
+
   def _ActivityLens(self, requests, raw_events):
     loading_trace = test_utils.LoadingTraceFromEvents(
         requests, None, raw_events)
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index d383857..344baeb 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -5,6 +5,7 @@
 """Represents the trace of a page load."""
 
 import json
+
 import page_track
 import request_track
 import tracing
diff --git a/loading/network_activity_lens.py b/loading/network_activity_lens.py
new file mode 100644
index 0000000..2040093
--- /dev/null
+++ b/loading/network_activity_lens.py
@@ -0,0 +1,210 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gives a picture of the network activity between timestamps."""
+
+import bisect
+import collections
+import itertools
+import operator
+
+
+class NetworkActivityLens(object):
+  """Reconstructs the network activity during a trace.
+
+  The {uploaded,downloaded}_bytes_timeline timelines are:
+  ([timestamp_msec], [value_at_timestamp]). Bytes are counted when a
+  network event completes.
+
+  The rate timelines are:
+  ([timestamp_msec], [rate]), where the rate is computed over the time
+  period ending at timestamp_msec.
+
+  For all the timelines, the list of timestamps are identical.
+  """
+  def __init__(self, trace):
+    """Initializes a NetworkActivityLens instance.
+
+    Args:
+      trace: (LoadingTrace)
+    """
+    self._trace = trace
+    self._start_end_times = []
+    self._active_events_list = []
+    self._uploaded_bytes_timeline = []
+    self._downloaded_bytes_timeline = []
+    self._upload_rate_timeline = []
+    self._download_rate_timeline = []
+    requests = trace.request_track.GetEvents()
+    self._network_events = list(itertools.chain.from_iterable(
+        NetworkEvent.EventsFromRequest(request) for request in requests))
+    self._IndexEvents()
+    self._CreateTimelines()
+
+  @property
+  def uploaded_bytes_timeline(self): # (timestamps, data)
+    return (self._start_end_times, self._uploaded_bytes_timeline)
+
+  @property
+  def downloaded_bytes_timeline(self):
+    return (self._start_end_times, self._downloaded_bytes_timeline)
+
+  @property
+  def upload_rate_timeline(self):
+    return (self._start_end_times, self._upload_rate_timeline)
+
+  @property
+  def download_rate_timeline(self):
+    return (self._start_end_times, self._download_rate_timeline)
+
+  def _IndexEvents(self):
+    start_end_times_set = set()
+    for event in self._network_events:
+      start_end_times_set.add(event.start_msec)
+      start_end_times_set.add(event.end_msec)
+    self._start_end_times = sorted(list(start_end_times_set))
+    self._active_events_list = [[] for _ in self._start_end_times]
+    for event in self._network_events:
+      start_index = bisect.bisect_right(
+          self._start_end_times, event.start_msec) - 1
+      end_index = bisect.bisect_right(
+          self._start_end_times, event.end_msec)
+      for index in range(start_index, end_index):
+        self._active_events_list[index].append(event)
+
+  def _CreateTimelines(self):
+    for (index, timestamp) in enumerate(self._start_end_times):
+      upload_rate = sum(
+          e.UploadRate() for e in self._active_events_list[index]
+          if timestamp != e.end_msec)
+      download_rate = sum(
+          e.DownloadRate() for e in self._active_events_list[index]
+          if timestamp != e.end_msec)
+      uploaded_bytes = sum(
+          e.UploadedBytes() for e in self._active_events_list[index]
+          if timestamp == e.end_msec)
+      downloaded_bytes = sum(
+          e.DownloadedBytes() for e in self._active_events_list[index]
+          if timestamp == e.end_msec)
+      self._uploaded_bytes_timeline.append(uploaded_bytes)
+      self._downloaded_bytes_timeline.append(downloaded_bytes)
+      self._upload_rate_timeline.append(upload_rate)
+      self._download_rate_timeline.append(download_rate)
+
+
+class NetworkEvent(object):
+  """Represents a network event."""
+  KINDS = set(
+      ('dns', 'connect', 'send', 'receive_headers', 'receive_body'))
+  def __init__(self, request, kind, start_msec, end_msec, chunk_index=None):
+    """Creates a NetworkEvent."""
+    self._request = request
+    self._kind = kind
+    self.start_msec = start_msec
+    self.end_msec = end_msec
+    self._chunk_index = chunk_index
+
+  @classmethod
+  def _GetStartEndOffsetsMsec(cls, request, kind, index=None):
+    start_offset, end_offset = (0, 0)
+    r = request
+    if kind == 'dns':
+      start_offset = r.timing.dns_start
+      end_offset = r.timing.dns_end
+    elif kind == 'connect':
+      start_offset = r.timing.connect_start
+      end_offset = r.timing.connect_end
+    elif kind == 'send':
+      start_offset = r.timing.send_start
+      end_offset = r.timing.send_end
+    elif kind == 'receive_headers': # There is no responseReceived timing.
+      start_offset = r.timing.send_end
+      end_offset = r.timing.receive_headers_end
+    elif kind == 'receive_body':
+      if index is None:
+        start_offset = r.timing.receive_headers_end
+        end_offset = r.timing.loading_finished
+      else:
+        # Some chunks can correspond to no data.
+        i = index - 1
+        while i >= 0:
+          (offset, size) = r.data_chunks[i]
+          if size != 0:
+            previous_chunk_start = offset
+            break
+          i -= 1
+        else:
+          previous_chunk_start = r.timing.receive_headers_end
+        start_offset = previous_chunk_start
+        end_offset = r.data_chunks[index][0]
+    return (start_offset, end_offset)
+
+  @classmethod
+  def EventsFromRequest(cls, request):
+    # TODO(lizeb): This ignore forced revalidations.
+    if (request.from_disk_cache or request.served_from_cache
+        or request.IsDataRequest()):
+      return []
+    events = []
+    for kind in cls.KINDS - set(['receive_body']):
+      event = cls._EventWithKindFromRequest(request, kind)
+      if event:
+        events.append(event)
+    kind = 'receive_body'
+    if request.data_chunks:
+      for (index, chunk) in enumerate(request.data_chunks):
+        if chunk[0] != 0:
+          event = cls._EventWithKindFromRequest(request, kind, index)
+          if event:
+            events.append(event)
+    else:
+      event = cls._EventWithKindFromRequest(request, kind, None)
+      if event:
+        events.append(event)
+    return events
+
+  @classmethod
+  def _EventWithKindFromRequest(cls, request, kind, index=None):
+    (start_offset, end_offset) = cls._GetStartEndOffsetsMsec(
+        request, kind, index)
+    event = cls(request, kind, request.start_msec + start_offset,
+                request.start_msec + end_offset, index)
+    if start_offset == -1 or end_offset == -1:
+      return None
+    return event
+
+  def UploadedBytes(self):
+    """Returns the number of bytes uploaded during this event."""
+    if self._kind not in ('send'):
+      return 0
+    # Headers are not compressed (ignoring SPDY / HTTP/2)
+    if not self._request.request_headers:
+      return 0
+    return sum(len(k) + len(str(v)) for (k, v)
+               in self._request.request_headers.items())
+
+  def DownloadedBytes(self):
+    """Returns the number of bytes downloaded during this event."""
+    if self._kind not in ('receive_headers', 'receive_body'):
+      return 0
+    if self._kind == 'receive_headers':
+      return sum(len(k) + len(str(v)) for (k, v)
+                 in self._request.response_headers.items())
+    else:
+      if self._chunk_index is None:
+        return self._request.encoded_data_length
+      else:
+        return self._request.data_chunks[self._chunk_index][1]
+
+  def UploadRate(self):
+    """Returns the upload rate of this event in Bytes / s."""
+    return 1000 * self.UploadedBytes() / float(self.end_msec - self.start_msec)
+
+  def DownloadRate(self):
+    """Returns the download rate of this event in Bytes / s."""
+    downloaded_bytes = self.DownloadedBytes()
+    value = 1000 * downloaded_bytes / float(self.end_msec - self.start_msec)
+    if value > 1e6:
+      print self._kind, downloaded_bytes, self.end_msec - self.start_msec
+    return value
diff --git a/loading/network_activity_lens_unittest.py b/loading/network_activity_lens_unittest.py
new file mode 100644
index 0000000..e857679
--- /dev/null
+++ b/loading/network_activity_lens_unittest.py
@@ -0,0 +1,111 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import unittest
+
+from network_activity_lens import NetworkActivityLens
+import test_utils
+
+
+class NetworkActivityLensTestCase(unittest.TestCase):
+  def testTimeline(self):
+    timing_dict = {
+        'requestTime': 1.2,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    lens = self._NetworkActivityLens([request])
+    start_end_times = lens.uploaded_bytes_timeline[0]
+    expected_start_times = [
+        1220., 1230., 1250., 1260., 1270., 1280., 1290., 1300.]
+    self.assertListEqual(expected_start_times, start_end_times)
+    timing_dict = copy.copy(timing_dict)
+    timing_dict['requestTime'] += .005
+    second_request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    lens = self._NetworkActivityLens([request, second_request])
+    start_end_times = lens.uploaded_bytes_timeline[0]
+    expected_start_times = sorted(
+        expected_start_times + [x + 5. for x in expected_start_times])
+    for (expected, actual) in zip(expected_start_times, start_end_times):
+      self.assertAlmostEquals(expected, actual)
+
+  def testTransferredBytes(self):
+    timing_dict = {
+        'requestTime': 1.2,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    request.request_headers = {'a': 'b'}
+    request.response_headers = {'c': 'def'}
+    lens = self._NetworkActivityLens([request])
+    # Upload
+    upload_timeline = lens.uploaded_bytes_timeline
+    self.assertEquals(1270, upload_timeline[0][4])
+    self.assertEquals(1280, upload_timeline[0][5])
+    self.assertEquals(0, upload_timeline[1][4])
+    self.assertEquals(2, upload_timeline[1][5])
+    self.assertEquals(0, upload_timeline[1][6])
+    upload_rate = lens.upload_rate_timeline
+    self.assertEquals(2 / 10e-3, upload_rate[1][4])
+    self.assertEquals(0, upload_rate[1][5])
+    # Download
+    download_timeline = lens.downloaded_bytes_timeline
+    download_rate = lens.download_rate_timeline
+    self.assertEquals(1280, download_timeline[0][5])
+    self.assertEquals(1290, download_timeline[0][6])
+    self.assertEquals(0, download_timeline[1][5])
+    self.assertEquals(4, download_timeline[1][6])
+    self.assertEquals(0, download_timeline[1][7])
+    download_rate = lens.download_rate_timeline
+    self.assertEquals(4 / 10e-3, download_rate[1][5])
+    self.assertEquals(0, download_rate[1][6])
+
+  def testLongRequest(self):
+    timing_dict = {
+        'requestTime': 1200,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    request.response_headers = {}
+    timing_dict = {
+        'requestTime': 1200,
+        'dnsStart': 2, 'dnsEnd': 3,
+        'connectStart': 5, 'connectEnd': 6,
+        'sendStart': 7, 'sendEnd': 8,
+        'receiveHeadersEnd': 10,
+        'loadingFinished': 1000}
+    long_request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    long_request.response_headers = {}
+    long_request.encoded_data_length = 1000
+    lens = self._NetworkActivityLens([request, long_request])
+    (timestamps, downloaded_bytes) = lens.downloaded_bytes_timeline
+    (_, download_rate) = lens.download_rate_timeline
+    start_receive = (long_request.start_msec
+                     + long_request.timing.receive_headers_end)
+    end_receive = (long_request.start_msec
+                   + long_request.timing.loading_finished)
+    self.assertEquals(1000, downloaded_bytes[-1])
+    for (index, timestamp) in enumerate(timestamps):
+      if start_receive < timestamp < end_receive:
+        self.assertAlmostEqual(1000 / 990e-3, download_rate[index])
+        self.assertEquals(0, downloaded_bytes[index])
+    self.assertEquals(1000, downloaded_bytes[-1])
+
+  def _NetworkActivityLens(self, requests):
+    trace = test_utils.LoadingTraceFromEvents(requests)
+    return NetworkActivityLens(trace)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/network_cpu_activity_view.py b/loading/network_cpu_activity_view.py
new file mode 100755
index 0000000..09450e1
--- /dev/null
+++ b/loading/network_cpu_activity_view.py
@@ -0,0 +1,69 @@
+#!/usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Graphs the CPU and network activity during a load."""
+
+import numpy as np
+import matplotlib
+from matplotlib import pylab as plt
+import sys
+
+import activity_lens
+import loading_trace
+import network_activity_lens
+
+
+def _CpuActivityTimeline(cpu_lens, start_msec, end_msec, granularity):
+  cpu_timestamps = np.arange(start_msec, end_msec, granularity)
+  busy_percentage = []
+  print len(cpu_timestamps)
+  for i in range(len(cpu_timestamps) - 1):
+    (start, end) = (cpu_timestamps[i], cpu_timestamps[i + 1])
+    duration = end - start
+    busy_ms = cpu_lens.MainRendererThreadBusyness(start, end)
+    busy_percentage.append(100 * busy_ms / float(duration))
+  return (cpu_timestamps[:-1], np.array(busy_percentage))
+
+
+def GraphTimelines(trace, output_filename):
+  """Creates and saves a graph of Network and CPU activity for a trace.
+
+  Args:
+    trace: (LoadingTrace)
+    output_filename: (str) Path of the output graph.
+  """
+  cpu_lens = activity_lens.ActivityLens(trace)
+  network_lens = network_activity_lens.NetworkActivityLens(trace)
+  matplotlib.rc('font', size=14)
+  figure, (network, cpu) = plt.subplots(2, sharex = True, figsize=(14, 10))
+  figure.suptitle('Network and CPU Activity - %s' % trace.url)
+  upload_timeline = network_lens.uploaded_bytes_timeline
+  download_timeline = network_lens.downloaded_bytes_timeline
+  start_time = upload_timeline[0][0]
+  end_time = upload_timeline[0][-1]
+  times = np.array(upload_timeline[0]) - start_time
+  network.step(times, np.cumsum(download_timeline[1]) / 1e6, label='Download')
+  network.step(times, np.cumsum(upload_timeline[1]) / 1e6, label='Upload')
+  network.legend(loc='lower right')
+  network.set_xlabel('Time (ms)')
+  network.set_ylabel('Total Data Transferred (MB)')
+
+  (cpu_timestamps, cpu_busyness) = _CpuActivityTimeline(
+      cpu_lens, start_time, end_time, 100)
+  cpu.step(cpu_timestamps - start_time, cpu_busyness)
+  cpu.set_ylim(ymin=0, ymax=100)
+  cpu.set_xlabel('Time (ms)')
+  cpu.set_ylabel('Main Renderer Thread Busyness (%)')
+  figure.savefig(output_filename, dpi=300)
+
+
+def main():
+  filename = sys.argv[1]
+  trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+  GraphTimelines(trace, filename + '.pdf')
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 2d80d1a..5d2b617 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -42,6 +42,41 @@ class FakePageTrack(devtools_monitor.Track):
     return event['frame_id']
 
 
+def MakeRequestWithTiming(
+    url, source_url, timing_dict, magic_content_type=False,
+    initiator_type='other'):
+  """Make a dependent request.
+
+  Args:
+    url: a url, or number which will be used as a url.
+    source_url: a url or number which will be used as the source (initiating)
+      url. If the source url is not present, then url will be a root. The
+      convention in tests is to use a source_url of 'null' in this case.
+    timing_dict: (dict) Suitable to be passed to request_track.TimingFromDict().
+    initiator_type: the initiator type to use.
+
+  Returns:
+    A request_track.Request.
+  """
+  assert initiator_type in ('other', 'parser')
+  timing = request_track.TimingFromDict(timing_dict)
+  rq = request_track.Request.FromJsonDict({
+      'timestamp': timing.request_time,
+      'request_id': str(MakeRequestWithTiming._next_request_id),
+      'url': 'http://' + str(url),
+      'initiator': {'type': initiator_type, 'url': 'http://' + str(source_url)},
+      'response_headers': {'Content-Type':
+                           'null' if not magic_content_type
+                           else 'magic-debug-content' },
+      'timing': request_track.TimingAsList(timing)
+  })
+  MakeRequestWithTiming._next_request_id += 1
+  return rq
+
+
+MakeRequestWithTiming._next_request_id = 0
+
+
 def MakeRequest(
     url, source_url, start_time=None, headers_time=None, end_time=None,
     magic_content_type=False, initiator_type='other'):
@@ -63,7 +98,6 @@ def MakeRequest(
 
   Returns:
     A request_track.Request.
-
   """
   assert ((start_time is None and
            headers_time is None and
@@ -75,29 +109,16 @@ def MakeRequest(
   if start_time is None:
     # Use the request id in seconds for timestamps. This guarantees increasing
     # times which makes request dependencies behave as expected.
-    start_time = headers_time = end_time = MakeRequest._next_request_id * 1000
-  assert initiator_type in ('other', 'parser')
-  timing = request_track.TimingAsList(request_track.TimingFromDict({
+    start_time = headers_time = end_time = (
+        MakeRequestWithTiming._next_request_id * 1000)
+  timing_dict = {
       # connectEnd should be ignored.
       'connectEnd': (end_time - start_time) / 2,
       'receiveHeadersEnd': headers_time - start_time,
       'loadingFinished': end_time - start_time,
-      'requestTime': start_time / 1000.0}))
-  rq = request_track.Request.FromJsonDict({
-      'timestamp': start_time / 1000.0,
-      'request_id': str(MakeRequest._next_request_id),
-      'url': 'http://' + str(url),
-      'initiator': {'type': initiator_type, 'url': 'http://' + str(source_url)},
-      'response_headers': {'Content-Type':
-                           'null' if not magic_content_type
-                           else 'magic-debug-content' },
-      'timing': timing
-  })
-  MakeRequest._next_request_id += 1
-  return rq
-
-
-MakeRequest._next_request_id = 0
+      'requestTime': start_time / 1000.0}
+  return MakeRequestWithTiming(
+      url, source_url, timing_dict, magic_content_type, initiator_type)
 
 
 def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):

commit 45c0613331c70dccf8edd9cf32171175cc769c8d
Author: tfarina <tfarina@chromium.org>
Date:   Mon Mar 7 08:44:20 2016 -0800

    forwarder2: fix documentation typo in Socket::InitUnixSocket() function
    
    This patch fixes, in a documentation comment, the filename to a file
    that does not exist anymore. The code that it is referring to is from
    UnixDomainClientSocket::FillAddress() function from
    net/socket/unix_domain_client_socket_posix.cc.
    
    BUG=None
    R=yfriedman@chromium.org
    
    Review URL: https://codereview.chromium.org/1766913002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379564}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c9a4f315495334f23d2f5e52ac1b3b17f4bd21c3

diff --git a/forwarder2/socket.cc b/forwarder2/socket.cc
index c6073c1..669764f 100644
--- a/forwarder2/socket.cc
+++ b/forwarder2/socket.cc
@@ -136,7 +136,7 @@ bool Socket::InitUnixSocket(const std::string& path) {
   }
   family_ = PF_UNIX;
   addr_.addr_un.sun_family = family_;
-  // Copied from net/socket/unix_domain_socket_posix.cc
+  // Copied from net/socket/unix_domain_client_socket_posix.cc.
   // Convert the path given into abstract socket name. It must start with
   // the '\0' character, so we are adding it. |addr_len| must specify the
   // length of the structure exactly, as potentially the socket name may

commit f94ab6225606e9976077b6bd9718f6be9e4277e4
Author: gabadie <gabadie@chromium.org>
Date:   Mon Mar 7 02:45:08 2016 -0800

    Sandwich: Adds a work-around in the cache filter when a ressource protocol is unknown.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1759183004
    
    Cr-Original-Commit-Position: refs/heads/master@{#379534}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f9afa9fbba5fa8ba1fad78491f12b178a2eee27b

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 86d730f..b4e2590 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -477,8 +477,16 @@ def _FilterCacheMain(args):
     logging.info('white-listing %s' % main_resource_request.url)
     whitelisted_urls.add(main_resource_request.url)
     for (first, second, reason) in deps:
+      # Work-around where the protocol may be none for an unclear reason yet.
+      # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
+      #   this work-around.
+      if not second.protocol:
+        logging.info('ignoring %s (no protocol)' % second.url)
+        continue
       # Ignore data protocols.
       if not second.protocol.startswith('http'):
+        logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
+            second.url, second.protocol))
         continue
       if (first.request_id == main_resource_request.request_id and
           reason == 'parser' and second.url not in whitelisted_urls):

commit 14315674915204bf3be56f5f27b3145369e15b38
Author: gabadie <gabadie@chromium.org>
Date:   Thu Mar 3 11:40:07 2016 -0800

    tools/android/loading: Makes WprUrlEntry ready for case in-sensitive headers.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1752793002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379059}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a1437c0f5983ce20e7a7e67b37cd46688f1536c0

diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
index c3a84a1..e983572 100644
--- a/loading/wpr_backend.py
+++ b/loading/wpr_backend.py
@@ -39,7 +39,7 @@ class WprUrlEntry(object):
     """
     headers = collections.defaultdict(list)
     for (key, value) in self._wpr_response.headers:
-      headers[key].append(value)
+      headers[key.lower()].append(value)
     return {k: ','.join(v) for (k, v) in headers.items()}
 
   def SetResponseHeader(self, name, value):
@@ -53,14 +53,15 @@ class WprUrlEntry(object):
       name: The name of the response header to set.
       value: The value of the response header to set.
     """
+    assert name.islower()
     new_headers = []
     new_header_set = False
     for header in self._wpr_response.headers:
-      if header[0] != name:
+      if header[0].lower() != name:
         new_headers.append(header)
       elif not new_header_set:
         new_header_set = True
-        new_headers.append((name, value))
+        new_headers.append((header[0], value))
     if new_header_set:
       self._wpr_response.headers = new_headers
     else:
@@ -76,8 +77,9 @@ class WprUrlEntry(object):
     Args:
       name: The name of the response header field to delete.
     """
+    assert name.islower()
     self._wpr_response.headers = \
-        [x for x in self._wpr_response.headers if x[0] != name]
+        [x for x in self._wpr_response.headers if x[0].lower() != name]
 
   @classmethod
   def _ExtractUrl(cls, request_string):
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
index e38070c..3cdf581 100644
--- a/loading/wpr_backend_unittest.py
+++ b/loading/wpr_backend_unittest.py
@@ -37,12 +37,14 @@ class WprUrlEntryTest(unittest.TestCase):
                                      ('header1', 'value1'),
                                      ('header0', 'value2'),
                                      ('header2', 'value3'),
-                                     ('header0', 'value4')])
+                                     ('header0', 'value4'),
+                                     ('HEadEr3', 'VaLue4')])
     headers = entry.GetResponseHeadersDict()
-    self.assertEquals(3, len(headers))
+    self.assertEquals(4, len(headers))
     self.assertEquals('value0,value2,value4', headers['header0'])
     self.assertEquals('value1', headers['header1'])
     self.assertEquals('value3', headers['header2'])
+    self.assertEquals('VaLue4', headers['header3'])
 
   def test_SetResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
@@ -63,10 +65,20 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('header1', entry._wpr_response.headers[1][0])
 
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('hEADEr1', 'value1'),
+                                     ('header2', 'value1'),])
+    entry.SetResponseHeader('header1', 'new_value1')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('new_value1', headers['header1'])
+    self.assertEquals('hEADEr1', entry._wpr_response.headers[1][0])
+
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
                                      ('header2', 'value2'),
                                      ('header1', 'value3'),
-                                     ('header3', 'value4')])
+                                     ('header3', 'value4'),
+                                     ('heADer1', 'value5')])
     entry.SetResponseHeader('header1', 'new_value2')
     headers = entry.GetResponseHeadersDict()
     self.assertEquals(4, len(headers))
@@ -75,6 +87,20 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('header3', entry._wpr_response.headers[3][0])
     self.assertEquals('value4', entry._wpr_response.headers[3][1])
 
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('heADer1', 'value1'),
+                                     ('header2', 'value2'),
+                                     ('HEader1', 'value3'),
+                                     ('header3', 'value4'),
+                                     ('header1', 'value5')])
+    entry.SetResponseHeader('header1', 'new_value2')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(4, len(headers))
+    self.assertEquals('new_value2', headers['header1'])
+    self.assertEquals('heADer1', entry._wpr_response.headers[1][0])
+    self.assertEquals('header3', entry._wpr_response.headers[3][0])
+    self.assertEquals('value4', entry._wpr_response.headers[3][1])
+
   def test_DeleteResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
@@ -87,6 +113,14 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertNotIn('header0', entry.GetResponseHeadersDict())
     self.assertEquals(1, len(entry.GetResponseHeadersDict()))
 
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('hEAder1', 'value1'),
+                                     ('header0', 'value2'),
+                                     ('heaDEr2', 'value3')])
+    entry.DeleteResponseHeader('header1')
+    self.assertNotIn('header1', entry.GetResponseHeadersDict())
+    self.assertEquals(2, len(entry.GetResponseHeadersDict()))
+
 
 if __name__ == '__main__':
   unittest.main()

commit 61a25526e1c912a20edd2c8c5775c56f0ae43dde
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 11:44:05 2016 -0800

    tools/android/loading: Implements loading_trace_analyzer.py's unittest.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1738803003
    
    Cr-Original-Commit-Position: refs/heads/master@{#378531}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 77bd4a8ce7c24b6c8d8ddbc22130f351d6c01b95

diff --git a/loading/loading_trace_analyzer.py b/loading/loading_trace_analyzer.py
index e703360..1821001 100755
--- a/loading/loading_trace_analyzer.py
+++ b/loading/loading_trace_analyzer.py
@@ -21,10 +21,10 @@ def _ArgumentParser():
   # requests listing subcommand.
   requests_parser = subparsers.add_parser('requests',
       help='Lists all request from the loading trace.')
-  requests_parser.add_argument('loading_trace', type=file,
+  requests_parser.add_argument('loading_trace', type=str,
       help='Input loading trace to see the cache usage from.')
   requests_parser.add_argument('--output',
-      type=argparse.FileType(),
+      type=argparse.FileType('w'),
       default=sys.stdout,
       help='Output destination path if different from stdout.')
   requests_parser.add_argument('--output-format', type=str, default='{url}',
@@ -36,9 +36,21 @@ def _ArgumentParser():
   return parser
 
 
-def _RequestsSubcommand(args):
+def ListRequests(loading_trace_path,
+                 output_format='{url}',
+                 where_format='{url}',
+                 where_statement=None):
   """`loading_trace_analyzer.py requests` Command line tool entry point.
 
+  Args:
+    loading_trace_path: Path of the loading trace.
+    output_format: Output format of the generated strings.
+    where_format: String formated to be regex tested with <where_statement>
+    where_statement: Regex for selecting request event.
+
+  Yields:
+    Formated string of the selected request event.
+
   Example:
     Lists all request with timing:
       ... requests --output-format "{timing} {url}"
@@ -46,28 +58,16 @@ def _RequestsSubcommand(args):
     Lists  HTTP/HTTPS requests that have used the cache:
       ... requests --where "{protocol} {from_disk_cache}" "https?\S* True"
   """
-  where_format = None
-  where_statement = None
-  if args.where_statement:
-    where_format = args.where_statement[0]
-    try:
-      where_statement = re.compile(args.where_statement[1])
-    except re.error as e:
-      sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
-          args.where_statement[1], str(e)))
-      return 1
-
-  loading_trace = LoadingTrace.FromJsonDict(json.load(args.loading_trace))
+  if where_statement:
+    where_statement = re.compile(where_statement)
+  loading_trace = LoadingTrace.FromJsonFile(loading_trace_path)
   for request_event in loading_trace.request_track.GetEvents():
     request_event_json = request_event.ToJsonDict()
-
     if where_statement != None:
       where_in = where_format.format(**request_event_json)
       if not where_statement.match(where_in):
         continue
-
-    args.output.write(args.output_format.format(**request_event_json) + '\n')
-  return 0
+    yield output_format.format(**request_event_json)
 
 
 def main(command_line_args):
@@ -75,7 +75,22 @@ def main(command_line_args):
   """
   args = _ArgumentParser().parse_args(command_line_args)
   if args.subcommand == 'requests':
-    return _RequestsSubcommand(args)
+    try:
+      where_format = None
+      where_statement = None
+      if args.where_statement:
+        where_format = args.where_statement[0]
+        where_statement = args.where_statement[1]
+      for output_line in ListRequests(loading_trace_path=args.loading_trace,
+                                      output_format=args.output_format,
+                                      where_format=where_format,
+                                      where_statement=where_statement):
+        args.output.write(output_line + '\n')
+      return 0
+    except re.error as e:
+      sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
+          where_statement[1], str(e)))
+    return 1
   assert False
 
 
diff --git a/loading/loading_trace_analyzer_unittest.py b/loading/loading_trace_analyzer_unittest.py
new file mode 100644
index 0000000..b01aab2
--- /dev/null
+++ b/loading/loading_trace_analyzer_unittest.py
@@ -0,0 +1,59 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import gzip
+import os
+import re
+import shutil
+import subprocess
+import tempfile
+import unittest
+
+import loading_trace_analyzer
+
+LOADING_DIR = os.path.dirname(__file__)
+TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
+
+
+class LoadingTraceAnalyzerTest(unittest.TestCase):
+  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
+
+  def setUp(self):
+    self._temp_dir = tempfile.mkdtemp()
+    self.trace_path = self._TmpPath('trace.json')
+    with gzip.GzipFile(self._ROLLING_STONE) as f:
+      with open(self.trace_path, 'w') as g:
+        g.write(f.read())
+
+  def tearDown(self):
+    shutil.rmtree(self._temp_dir)
+
+  def _TmpPath(self, name):
+    return os.path.join(self._temp_dir, name)
+
+  def testRequestsCmd(self):
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path)]
+    self.assertNotEqual(0, len(lines))
+
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
+        output_format='hello {protocol} world {url}')]
+    self.assertNotEqual(0, len(lines))
+    for line in lines:
+      self.assertTrue(re.match(r'^hello \S+ world \S+$', line))
+
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
+        where_format='{url}', where_statement=r'^http://.*$')]
+    self.assertNotEqual(0, len(lines))
+    for line in lines:
+      self.assertTrue(line.startswith('http://'))
+
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
+        where_format='{url}', where_statement=r'^https://.*$')]
+    self.assertNotEqual(0, len(lines))
+    for line in lines:
+      self.assertTrue(line.startswith('https://'))
+
+
+if __name__ == '__main__':
+  unittest.main()

commit b19f10e38e8ac2cd173edeb3045e8e7763726ca7
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 10:28:07 2016 -0800

    sandwich: Implements filter-cache sub-command.
    
    NoState-Prefetch won't be able to know all the resources to fetch.
    This new sub-command creates a new cache archive by pruning all
    resources that can't be discovered by the HTML parser of the main
    HTML document.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1737103002
    
    Cr-Original-Commit-Position: refs/heads/master@{#378486}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: aeb69a7ef733515cc168e9cdd4edcb5642c1b3fc

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index d6d65a2..40d15bb 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -8,6 +8,7 @@
 from datetime import datetime
 import json
 import os
+import shutil
 import subprocess
 import sys
 import tempfile
@@ -278,6 +279,32 @@ class CacheBackend(object):
     return stdout_data
 
 
+def ApplyUrlWhitelistToCacheArchive(cache_archive_path,
+                                    whitelisted_urls,
+                                    output_cache_archive_path):
+  """Generate a new cache archive containing only whitelisted urls.
+
+  Args:
+    cache_archive_path: Path of the cache archive to apply the white listing.
+    whitelisted_urls: Set of url to keep in cache.
+    output_cache_archive_path: Destination path of cache archive containing only
+      white-listed urls.
+  """
+  cache_temp_directory = tempfile.mkdtemp(suffix='.cache')
+  try:
+    UnzipDirectoryContent(cache_archive_path, cache_temp_directory)
+    backend = CacheBackend(cache_temp_directory, 'simple')
+    cached_urls = backend.ListKeys()
+    for cached_url in cached_urls:
+      if cached_url not in whitelisted_urls:
+        backend.DeleteKey(cached_url)
+    for cached_url in backend.ListKeys():
+      assert cached_url in whitelisted_urls
+    ZipDirectoryContent(cache_temp_directory, output_cache_archive_path)
+  finally:
+    shutil.rmtree(cache_temp_directory)
+
+
 if __name__ == '__main__':
   import argparse
   parser = argparse.ArgumentParser(description='Tests cache back-end.')
diff --git a/loading/sandwich.py b/loading/sandwich.py
index a76c82b..86d730f 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -35,9 +35,12 @@ import chrome_cache
 import chrome_setup
 import device_setup
 import devtools_monitor
+import frame_load_lens
+import loading_trace
 import options
 import page_track
 import pull_sandwich_metrics
+import request_dependencies_lens
 import trace_recorder
 import tracing
 import wpr_backend
@@ -206,17 +209,17 @@ class SandwichRunner(object):
             connection=connection,
             emulated_device_name=None,
             emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
-      loading_trace = trace_recorder.MonitorUrl(
+      trace = trace_recorder.MonitorUrl(
           connection, url,
           clear_cache=clear_cache,
           categories=pull_sandwich_metrics.CATEGORIES,
           timeout=_DEVTOOLS_TIMEOUT)
-      loading_trace.metadata.update(additional_metadata)
+      trace.metadata.update(additional_metadata)
       if trace_id != None and self.trace_output_directory:
-        loading_trace_path = os.path.join(
+        trace_path = os.path.join(
             self.trace_output_directory, str(trace_id), 'trace.json')
-        os.makedirs(os.path.dirname(loading_trace_path))
-        loading_trace.ToJsonFile(loading_trace_path)
+        os.makedirs(os.path.dirname(trace_path))
+        trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, trace_id=0):
     clear_cache = False
@@ -369,6 +372,23 @@ def _ArgumentParser():
                                    help='Path where to save the metrics\'s '+
                                       'CSV.')
 
+  # Filter cache subcommand.
+  filter_cache_parser = subparsers.add_parser('filter-cache',
+      help='Cache filtering that keeps only resources discoverable by the HTML'+
+          ' document parser.')
+  filter_cache_parser.add_argument('--cache-archive', type=str, required=True,
+                                   dest='cache_archive_path',
+                                   help='Path of the cache archive to filter.')
+  filter_cache_parser.add_argument('--output', type=str, required=True,
+                                   dest='output_cache_archive_path',
+                                   help='Path of filtered cache archive.')
+  filter_cache_parser.add_argument('loading_trace_paths', type=str, nargs='+',
+      metavar='LOADING_TRACE',
+      help='A list of loading traces generated by a sandwich run for a given' +
+          ' url. This is used to have a resource dependency graph to white-' +
+          'list the ones discoverable by the HTML pre-scanner for that given ' +
+          'url.')
+
   return parser
 
 
@@ -445,6 +465,34 @@ def _ExtractMetricsMain(args):
   return 0
 
 
+def _FilterCacheMain(args):
+  whitelisted_urls = set()
+  for loading_trace_path in args.loading_trace_paths:
+    logging.info('loading %s' % loading_trace_path)
+    trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
+    requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
+    deps = requests_lens.GetRequestDependencies()
+
+    main_resource_request = deps[0][0]
+    logging.info('white-listing %s' % main_resource_request.url)
+    whitelisted_urls.add(main_resource_request.url)
+    for (first, second, reason) in deps:
+      # Ignore data protocols.
+      if not second.protocol.startswith('http'):
+        continue
+      if (first.request_id == main_resource_request.request_id and
+          reason == 'parser' and second.url not in whitelisted_urls):
+        logging.info('white-listing %s' % second.url)
+        whitelisted_urls.add(second.url)
+
+  if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
+    os.makedirs(os.path.dirname(args.output_cache_archive_path))
+  chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
+                                               whitelisted_urls,
+                                               args.output_cache_archive_path)
+  return 0
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -465,6 +513,8 @@ def main(command_line_args):
     return _RunJobMain(args)
   if args.subcommand == 'extract-metrics':
     return _ExtractMetricsMain(args)
+  if args.subcommand == 'filter-cache':
+    return _FilterCacheMain(args)
   assert False
 
 

commit e11859dd8239b140b19fe0d70767e55151f9c034
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 09:29:24 2016 -0800

    sandwich: Makes pull_sandwich_metrics.py a sub-command.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1726403005
    
    Cr-Original-Commit-Position: refs/heads/master@{#378467}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 04c9261d40089b8454379f35df33e908b2ddbbb0

diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
old mode 100755
new mode 100644
index 926a579..ee37583
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -1,4 +1,3 @@
-#! /usr/bin/env python
 # Copyright 2016 The Chromium Authors. All rights reserved.
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -8,8 +7,6 @@
 python pull_sandwich_metrics.py -h
 """
 
-import argparse
-import csv
 import json
 import logging
 import os
@@ -21,7 +18,7 @@ import tracing
 
 CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
 
-_CSV_FIELD_NAMES = [
+CSV_FIELD_NAMES = [
     'id',
     'url',
     'total_load',
@@ -114,7 +111,7 @@ def _PullMetricsFromLoadingTrace(loading_trace):
     loading_trace: loading_trace_module.LoadingTrace.
 
   Returns:
-    Dictionary with all _CSV_FIELD_NAMES's field set (except the 'id').
+    Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
   """
   browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   web_page_tracked_events = _GetWebPageTrackedEvents(
@@ -139,7 +136,7 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   }
 
 
-def _PullMetricsFromOutputDirectory(output_directory_path):
+def PullMetricsFromOutputDirectory(output_directory_path):
   """Pulls all the metrics from all the traces of a sandwich run directory.
 
   Args:
@@ -147,7 +144,7 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
         metrics from.
 
   Returns:
-    List of dictionaries with all _CSV_FIELD_NAMES's field set.
+    List of dictionaries with all CSV_FIELD_NAMES's field set.
   """
   assert os.path.isdir(output_directory_path)
   run_infos = None
@@ -174,26 +171,3 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
   return metrics
-
-
-def main():
-  logging.basicConfig(level=logging.INFO)
-
-  parser = argparse.ArgumentParser()
-  parser.add_argument('output', type=str,
-                      help='Output directory of run_sandwich.py command.')
-  args = parser.parse_args()
-
-  trace_metrics_list = _PullMetricsFromOutputDirectory(args.output)
-  trace_metrics_list.sort(key=lambda e: e['id'])
-  cs_file_path = os.path.join(args.output, 'trace_analysis.csv')
-  with open(cs_file_path, 'w') as csv_file:
-    writer = csv.DictWriter(csv_file, fieldnames=_CSV_FIELD_NAMES)
-    writer.writeheader()
-    for trace_metrics in trace_metrics_list:
-      writer.writerow(trace_metrics)
-  return 0
-
-
-if __name__ == '__main__':
-  sys.exit(main())
diff --git a/loading/sandwich.py b/loading/sandwich.py
index aa2f9b5..a76c82b 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -12,6 +12,7 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
+import csv
 import json
 import logging
 import os
@@ -285,13 +286,17 @@ class SandwichRunner(object):
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
+  # Command parser when dealing with jobs.
+  common_job_parser = argparse.ArgumentParser(add_help=False)
+  common_job_parser.add_argument('--job', required=True,
+                                 help='JSON file with job description.')
+
+  # Main parser
   parser = argparse.ArgumentParser()
-  parser.add_argument('--job', required=True,
-                      help='JSON file with job description.')
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
   # Record WPR subcommand.
-  record_wpr = subparsers.add_parser('record-wpr',
+  record_wpr = subparsers.add_parser('record-wpr', parents=[common_job_parser],
                                      help='Record WPR from sandwich job.')
   record_wpr.add_argument('--wpr-archive', required=True, type=str,
                           dest='wpr_archive_path',
@@ -302,10 +307,11 @@ def _ArgumentParser():
                                      help='Patch WPR response headers.')
   patch_wpr.add_argument('--wpr-archive', required=True, type=str,
                          dest='wpr_archive_path',
-                         help='Web page replay archive to generate.')
+                         help='Web page replay archive to patch.')
 
   # Create cache subcommand.
   create_cache_parser = subparsers.add_parser('create-cache',
+      parents=[common_job_parser],
       help='Create cache from sandwich job.')
   create_cache_parser.add_argument('--cache-archive', required=True, type=str,
                                    dest='cache_archive_path',
@@ -316,7 +322,8 @@ def _ArgumentParser():
                                        'the cache from.')
 
   # Run subcommand.
-  run_parser = subparsers.add_parser('run', help='Run sandwich benchmark.')
+  run_parser = subparsers.add_parser('run', parents=[common_job_parser],
+                                     help='Run sandwich benchmark.')
   run_parser.add_argument('--output', required=True, type=str,
                           dest='trace_output_directory',
                           help='Path of output directory to create.')
@@ -350,6 +357,18 @@ def _ArgumentParser():
                           dest='wpr_archive_path',
                           help='Web page replay archive to load job\'s urls ' +
                               'from.')
+
+  # Pull metrics subcommand.
+  create_cache_parser = subparsers.add_parser('extract-metrics',
+      help='Extracts metrics from a loading trace and saves as CSV.')
+  create_cache_parser.add_argument('--trace-directory', required=True,
+                                   dest='trace_output_directory', type=str,
+                                   help='Path of loading traces directory.')
+  create_cache_parser.add_argument('--out-metrics', default=None, type=str,
+                                   dest='metrics_csv_path',
+                                   help='Path where to save the metrics\'s '+
+                                      'CSV.')
+
   return parser
 
 
@@ -358,6 +377,8 @@ def _RecordWprMain(args):
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.wpr_record = True
   sandwich_runner.PrintConfig()
+  if not os.path.isdir(os.path.dirname(args.wpr_archive_path)):
+    os.makedirs(os.path.dirname(args.wpr_archive_path))
   sandwich_runner.Run()
   return 0
 
@@ -397,6 +418,8 @@ def _CreateCacheMain(args):
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.cache_operation = 'save'
   sandwich_runner.PrintConfig()
+  if not os.path.isdir(os.path.dirname(args.cache_archive_path)):
+    os.makedirs(os.path.dirname(args.cache_archive_path))
   sandwich_runner.Run()
   return 0
 
@@ -409,6 +432,19 @@ def _RunJobMain(args):
   return 0
 
 
+def _ExtractMetricsMain(args):
+  trace_metrics_list = pull_sandwich_metrics.PullMetricsFromOutputDirectory(
+      args.trace_output_directory)
+  trace_metrics_list.sort(key=lambda e: e['id'])
+  with open(args.metrics_csv_path, 'w') as csv_file:
+    writer = csv.DictWriter(csv_file,
+                            fieldnames=pull_sandwich_metrics.CSV_FIELD_NAMES)
+    writer.writeheader()
+    for trace_metrics in trace_metrics_list:
+      writer.writerow(trace_metrics)
+  return 0
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -427,6 +463,8 @@ def main(command_line_args):
     return _CreateCacheMain(args)
   if args.subcommand == 'run':
     return _RunJobMain(args)
+  if args.subcommand == 'extract-metrics':
+    return _ExtractMetricsMain(args)
   assert False
 
 

commit a9447881ef96d2c6ad4579697503ee6d5b4586eb
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 1 08:38:26 2016 -0800

    Add occurrence tracking to a multigraph.
    
    The idea is to annotate node bags with things like "75% of urls were after first
    contentful paint".
    
    Review URL: https://codereview.chromium.org/1735093002
    
    Cr-Original-Commit-Position: refs/heads/master@{#378453}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: de1d607e176ce97c3857d2c8231a2fd4a862f625

diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 493029c..f1182e5 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -84,6 +84,22 @@ class GraphSack(object):
     self._url_to_bag[node.Url()].AddNode(graph, node)
     return self._url_to_bag[node.Url()]
 
+  def FilterOccurrence(self, tag, filter_from_graph):
+    """Accumulate filter occurrences for each bag in the graph.
+
+    This can be retrieved under tag for each Bag in the graph. For example, if
+    FilterContentful marks the nodes of each graph before the first contentful
+    paint, then FilterOccurrence('contentful', FilterContentful) will count, for
+    each bag, the fraction of nodes that were before the first contentful paint.
+
+    Args:
+      tag: the tag to count the filter appearances under.
+      filter_from_graph: a function graph -> node filter, where node filter
+        takes a node to a boolean.
+    """
+    for bag in self.bags:
+      bag.MarkOccurrence(tag, filter_from_graph)
+
   @property
   def graph_info(self):
     return self._graph_info
@@ -113,6 +129,12 @@ class Bag(dag.Node):
     self._relative_costs = []
     self._num_critical = 0
 
+    # See MarkOccurrence and GetOccurrence, below. This maps an occurrence
+    # tag to a list of nodes matching the occurrence.
+    self._occurence_matches = {}
+    # Number of nodes seen for each occurrence.
+    self._occurence_count = {}
+
   @property
   def url(self):
     return self._url
@@ -170,6 +192,37 @@ class Bag(dag.Node):
       self._successor_sources[successor_bag].add((graph, node, s))
       self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
 
+  def MarkOccurrence(self, tag, filter_from_graph):
+    """Mark occurrences for nodes in this bag according to graph_filters.
+
+    Results can be querried by GetOccurrence().
+
+    Args:
+      tag: a label for this set of occurrences.
+      filter_from_graph: a function graph -> node filter, where node filter
+        takes a node to a boolean.
+    """
+    self._occurence_matches[tag] = 0
+    self._occurence_count[tag] = 0
+    for graph, nodes in self.graphs.iteritems():
+      for n in nodes:
+        self._occurence_count[tag] += 1
+        if filter_from_graph(graph)(n):
+          self._occurence_matches[tag] += 1
+
+  def GetOccurrence(self, tag):
+    """Retrieve the occurrence fraction of a tag.
+
+    Args:
+      tag: the tag under which the occurrence was counted. This must have been
+        previously added at least once via AddOccurrence.
+
+    Returns:
+      A fraction occurrence matches / occurrence node count.
+    """
+    assert self._occurence_count[tag] > 0
+    return float(self._occurence_matches[tag]) / self._occurence_count[tag]
+
   @classmethod
   def _MakeShortname(cls, url):
     parsed = urlparse.urlparse(url)
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index 417918d..2034eba 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -59,6 +59,36 @@ class ResourceSackTestCase(unittest.TestCase):
     self.assertEqual(set(['0/', 'data:fake/content']),
                      set([bag.label for bag in sack.bags]))
 
+  def test_Occurrence(self):
+    # There are two graph shapes. The first one is added to the sack three
+    # times, and the second once. The second graph has one sibling that doesn't
+    # appear in the first as well as a new child.
+    shape1 = [MakeRequest(0, 'null'), MakeRequest(1, 0), MakeRequest(2, 0)]
+    shape2 = [MakeRequest(0, 'null'), MakeRequest(1, 0),
+              MakeRequest(3, 0), MakeRequest(4, 1)]
+    graphs = [TestResourceGraph.FromRequestList(shape1),
+              TestResourceGraph.FromRequestList(shape1),
+              TestResourceGraph.FromRequestList(shape1),
+              TestResourceGraph.FromRequestList(shape2)]
+    sack = resource_sack.GraphSack()
+    for g in graphs:
+      sack.ConsumeGraph(g)
+    # Map a graph to a list of nodes that are in its filter.
+    filter_sets = {
+        graphs[0]: set([0, 1, 2]),
+        graphs[1]: set([0, 1, 2]),
+        graphs[2]: set([0, 1]),
+        graphs[3]: set([0, 3])}
+    sack.FilterOccurrence(
+        'test', lambda graph: lambda node:
+            int(node.ShortName()) in filter_sets[graph])
+    labels = {bag.label: bag for bag in sack.bags}
+    self.assertAlmostEqual(1, labels['0/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(0.75, labels['1/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(0.667, labels['2/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(1, labels['3/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(0, labels['4/'].GetOccurrence('test'), 3)
+
 
 if __name__ == '__main__':
   unittest.main()

commit ec75308f387bbb121f9d16ded0e7a407055b2db6
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 08:37:39 2016 -0800

    sandwich: Implements patch-wpr subcommand.
    
    The patch-wpr sub-command patches all resources response headers of a WPR archive, to make sure they all will be going into the chrome cache on disk if not already in, and also takes care of making sure that this cached resources will not be invalidated or re-validated in the next 10 years.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1740653002
    
    Cr-Original-Commit-Position: refs/heads/master@{#378452}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4bfeb32fdb5d16888edca6eeabb694496ccf6fe7

diff --git a/loading/sandwich.py b/loading/sandwich.py
index c337b8b..aa2f9b5 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -39,6 +39,7 @@ import page_track
 import pull_sandwich_metrics
 import trace_recorder
 import tracing
+import wpr_backend
 
 
 # Use options layer to access constants.
@@ -296,6 +297,13 @@ def _ArgumentParser():
                           dest='wpr_archive_path',
                           help='Web page replay archive to generate.')
 
+  # Patch WPR subcommand.
+  patch_wpr = subparsers.add_parser('patch-wpr',
+                                     help='Patch WPR response headers.')
+  patch_wpr.add_argument('--wpr-archive', required=True, type=str,
+                         dest='wpr_archive_path',
+                         help='Web page replay archive to generate.')
+
   # Create cache subcommand.
   create_cache_parser = subparsers.add_parser('create-cache',
       help='Create cache from sandwich job.')
@@ -354,6 +362,36 @@ def _RecordWprMain(args):
   return 0
 
 
+def _PatchWprMain(args):
+  # Sets the resources cache max-age to 10 years.
+  MAX_AGE = 10 * 365 * 24 * 60 * 60
+  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
+
+  wpr_archive = wpr_backend.WprArchiveBackend(args.wpr_archive_path)
+  for url_entry in wpr_archive.ListUrlEntries():
+    response_headers = url_entry.GetResponseHeadersDict()
+    if 'cache-control' in response_headers and \
+        response_headers['cache-control'] == CACHE_CONTROL:
+      continue
+    logging.info('patching %s' % url_entry.url)
+    # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
+    # TODO(gabadie): may need to delete ETag.
+    # TODO(gabadie): may need to patch Vary.
+    # TODO(gabadie): may need to take care of x-cache.
+    #
+    # Override the cache-control header to set the resources max age to MAX_AGE.
+    #
+    # Important note: Some resources holding sensitive information might have
+    # cache-control set to no-store which allow the resource to be cached but
+    # not cached in the file system. NoState-Prefetch is going to take care of
+    # this case. But in here, to simulate NoState-Prefetch, we don't have other
+    # choices but save absolutely all cached resources on disk so they survive
+    # after killing chrome for cache save, modification and push.
+    url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
+  wpr_archive.Persist()
+  return 0
+
+
 def _CreateCacheMain(args):
   sandwich_runner = SandwichRunner(args.job)
   sandwich_runner.PullConfigFromArgs(args)
@@ -383,6 +421,8 @@ def main(command_line_args):
 
   if args.subcommand == 'record-wpr':
     return _RecordWprMain(args)
+  if args.subcommand == 'patch-wpr':
+    return _PatchWprMain(args)
   if args.subcommand == 'create-cache':
     return _CreateCacheMain(args)
   if args.subcommand == 'run':

commit 20cababc773ba1d71689c7ce2e27e07cfd89f02a
Author: changwan <changwan@chromium.org>
Date:   Thu Feb 25 16:53:39 2016 -0800

    Introduce ThreadedInputConnection behind a switch
    
    Design doc: https://goo.gl/pcNRA5
    
    AdapterInputConnection is based on a replica model such that it has its
    own implementation and expects renderer to behave the same way. This model
    also required the replica editor to sync when there is a renderer-side
    change. The rationale behind this model was that it does not block the UI
    thread while allowing internal state to change before the next
    inputconnection call.
    
    Now renamed as ReplicaInputConnection, this model caused lots of issues in
    the past. (See design doc for details.)
    
    This CL proposes a whole new approach called ThreadedInputConnection. In
    this model, we create a fake view that proxies the container view such that
    InputConnection can be created on a separate thread (IME thread).
    Subsequent InputConnection method calls will also run on IME thread, so we
    do not block UI thread and can still wait for internal state change before
    returning InputConnection method calls, such as getTextBeforeCursor().
    
    Also note that ThreadedInputConnection does not inherit BaseInputConnection
    because we are not using Editable model.
    
    And there are other changes that followed this model:
    - 'From IME' bit will be set only when there is a blocking method call that
      awaits state update.
    - ImeTest tests behavior and not internal states any more.
    - RecreateInputConnectionTest is replaced by a new test in ImeTest.
    - Move mEditable from ImeAdapter to ReplicaInputConnection.
    - Remove DPAD hacks in RenderWidgetInputHandler.
    
    BUG=551193
    
    Review URL: https://codereview.chromium.org/1278593004
    
    Cr-Original-Commit-Position: refs/heads/master@{#377737}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8c3427482bac96138551530642843126da1b70a7

diff --git a/eclipse/.classpath b/eclipse/.classpath
index db88a17..62d3c39 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -50,6 +50,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="components/web_contents_delegate_android/android/java/src"/>
     <classpathentry kind="src" path="content/public/android/java/src"/>
     <classpathentry kind="src" path="content/public/android/javatests/src"/>
+    <classpathentry kind="src" path="content/public/android/junit/src"/>
     <classpathentry kind="src" path="content/public/test/android/javatests/src"/>
     <classpathentry kind="src" path="content/shell/android/browsertests/src"/>
     <classpathentry kind="src" path="content/shell/android/browsertests_apk/src"/>

commit 9b634f07887a544bb7b4b61008ebc65e2528664a
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 25 11:38:38 2016 -0800

    tools/android/loading: mv run_sandwich.py -> sandwich.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1732803002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377645}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9b51e3c81b3eb0033d3511dcfdff2f07032dc9b7

diff --git a/loading/run_sandwich.py b/loading/sandwich.py
similarity index 100%
rename from loading/run_sandwich.py
rename to loading/sandwich.py

commit efced8e1630c13b8bc1f09e0dd8f5ad78ae2bcca
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 25 09:57:07 2016 -0800

    sandwich: Refactors main() into SandwichRunner class.
    
    Sandwich's command line tools were getting fatter and
    fatter as the CLs were landing. This CL refactors main()
    into smaller methods of the newly introduced
    SandwichRunner class. It also adds new sub-commands
    using SandwichRunner.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1731113003
    
    Cr-Original-Commit-Position: refs/heads/master@{#377612}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: aa5436dba8ebd27e983596b93f0ab43a10eebe1c

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index ff98acb..c337b8b 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -95,41 +95,283 @@ def _CleanPreviousTraces(output_directories_path):
     shutil.rmtree(directory_path)
 
 
-def _ArgumentParser():
-  """Build a command line argument's parser.
+class SandwichRunner(object):
+  """Sandwich runner.
+
+  This object is meant to be configured first and then run using the Run()
+  method. The runner can configure itself conveniently with parsed arguement
+  using the PullConfigFromArgs() method. The only job is to make sure that the
+  command line flags have `dest` parameter set to existing runner members.
   """
+
+  def __init__(self, job_name):
+    """Configures a sandwich runner out of the box.
+
+    Public members are meant to be configured as wished before calling Run().
+
+    Args:
+      job_name: The job name to get the associated urls.
+    """
+    # Cache operation to do before doing the chrome navigation.
+    #   Can be: clear,save,push,reload
+    self.cache_operation = 'clear'
+
+    # The cache archive's path to save to or push from. Is str or None.
+    self.cache_archive_path = None
+
+    # Controls whether the WPR server should do script injection.
+    self.disable_wpr_script_injection = False
+
+    # The job name. Is str.
+    self.job_name = job_name
+
+    # Number of times to repeat the job.
+    self.job_repeat = 1
+
+    # Network conditions to emulate. None if no emulation.
+    self.network_condition = None
+
+    # Network condition emulator. Can be: browser,wpr
+    self.network_emulator = 'browser'
+
+    # Output directory where to save the traces. Is str or None.
+    self.trace_output_directory = None
+
+    # List of urls to run.
+    self.urls = _ReadUrlsFromJobDescription(job_name)
+
+    # Path to the WPR archive to load or save. Is str or None.
+    self.wpr_archive_path = None
+
+    # Configures whether the WPR archive should be read or generated.
+    self.wpr_record = False
+
+    self._device = None
+    self._chrome_additional_flags = []
+    self._local_cache_directory_path = None
+
+  def PullConfigFromArgs(self, args):
+    """Configures the sandwich runner from parsed command line argument.
+
+    Args:
+      args: The command line parsed argument.
+    """
+    for config_name in self.__dict__.keys():
+      if config_name in args.__dict__:
+        self.__dict__[config_name] = args.__dict__[config_name]
+
+  def PrintConfig(self):
+    """Print the current sandwich runner configuration to stdout. """
+    for config_name in sorted(self.__dict__.keys()):
+      if config_name[0] != '_':
+        print '{} = {}'.format(config_name, self.__dict__[config_name])
+
+  def _CleanTraceOutputDirectory(self):
+    assert self.trace_output_directory
+    if not os.path.isdir(self.trace_output_directory):
+      try:
+        os.makedirs(self.trace_output_directory)
+      except OSError:
+        logging.error('Cannot create directory for results: %s',
+            self.trace_output_directory)
+        raise
+    else:
+      _CleanPreviousTraces(self.trace_output_directory)
+
+  def _SaveRunInfos(self, urls):
+    assert self.trace_output_directory
+    run_infos = {
+      'cache-op': self.cache_operation,
+      'job_name': self.job_name,
+      'urls': urls
+    }
+    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
+              'w') as file_output:
+      json.dump(run_infos, file_output, indent=2)
+
+  def _GetEmulatorNetworkCondition(self, emulator):
+    if self.network_emulator == emulator:
+      return self.network_condition
+    return None
+
+  def _RunNavigation(self, url, clear_cache, trace_id=None):
+    with device_setup.DeviceConnection(
+        device=self._device,
+        additional_flags=self._chrome_additional_flags) as connection:
+      additional_metadata = {}
+      if self._GetEmulatorNetworkCondition('browser'):
+        additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
+            connection=connection,
+            emulated_device_name=None,
+            emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
+      loading_trace = trace_recorder.MonitorUrl(
+          connection, url,
+          clear_cache=clear_cache,
+          categories=pull_sandwich_metrics.CATEGORIES,
+          timeout=_DEVTOOLS_TIMEOUT)
+      loading_trace.metadata.update(additional_metadata)
+      if trace_id != None and self.trace_output_directory:
+        loading_trace_path = os.path.join(
+            self.trace_output_directory, str(trace_id), 'trace.json')
+        os.makedirs(os.path.dirname(loading_trace_path))
+        loading_trace.ToJsonFile(loading_trace_path)
+
+  def _RunUrl(self, url, trace_id=0):
+    clear_cache = False
+    if self.cache_operation == 'clear':
+      clear_cache = True
+    elif self.cache_operation == 'push':
+      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+      chrome_cache.PushBrowserCache(self._device,
+                                    self._local_cache_directory_path)
+    elif self.cache_operation == 'reload':
+      self._RunNavigation(url, clear_cache=True)
+    elif self.cache_operation == 'save':
+      clear_cache = trace_id == 0
+    self._RunNavigation(url, clear_cache=clear_cache, trace_id=trace_id)
+
+  def _PullCacheFromDevice(self):
+    assert self.cache_operation == 'save'
+    assert self.cache_archive_path, 'Need to specify where to save the cache'
+
+    # Move Chrome to background to allow it to flush the index.
+    self._device.adb.Shell('am start com.google.android.launcher')
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+
+    cache_directory_path = chrome_cache.PullBrowserCache(self._device)
+    chrome_cache.ZipDirectoryContent(
+        cache_directory_path, self.cache_archive_path)
+    shutil.rmtree(cache_directory_path)
+
+  def Run(self):
+    """SandwichRunner main entry point meant to be called once configured.
+    """
+    if self.trace_output_directory:
+      self._CleanTraceOutputDirectory()
+
+    self._device = device_utils.DeviceUtils.HealthyDevices()[0]
+    self._chrome_additional_flags = []
+
+    assert self._local_cache_directory_path == None
+    if self.cache_operation == 'push':
+      assert os.path.isfile(self.cache_archive_path)
+      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+      chrome_cache.UnzipDirectoryContent(
+          self.cache_archive_path, self._local_cache_directory_path)
+
+    ran_urls = []
+    with device_setup.WprHost(self._device, self.wpr_archive_path,
+        record=self.wpr_record,
+        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
+        disable_script_injection=self.disable_wpr_script_injection
+        ) as additional_flags:
+      self._chrome_additional_flags.extend(additional_flags)
+      for _ in xrange(self.job_repeat):
+        for url in self.urls:
+          self._RunUrl(url, trace_id=len(ran_urls))
+          ran_urls.append(url)
+
+    if self._local_cache_directory_path:
+      shutil.rmtree(self._local_cache_directory_path)
+      self._local_cache_directory_path = None
+    if self.cache_operation == 'save':
+      self._PullCacheFromDevice()
+    if self.trace_output_directory:
+      self._SaveRunInfos(ran_urls)
+
+
+def _ArgumentParser():
+  """Build a command line argument's parser."""
   parser = argparse.ArgumentParser()
   parser.add_argument('--job', required=True,
                       help='JSON file with job description.')
-  parser.add_argument('--output', required=True,
-                      help='Name of output directory to create.')
-  parser.add_argument('--repeat', default=1, type=int,
-                      help='How many times to run the job')
-  parser.add_argument('--cache-op',
-                      choices=['clear', 'save', 'push', 'reload'],
-                      default='clear',
-                      help='Configures cache operation to do before launching '
-                          +'Chrome. (Default is clear).')
-  parser.add_argument('--wpr-archive', default=None, type=str,
-                      help='Web page replay archive to load job\'s urls from.')
-  parser.add_argument('--wpr-record', default=False, action='store_true',
-                      help='Record web page replay archive.')
-  parser.add_argument('--disable-wpr-script-injection', default=False,
-                      action='store_true',
-                      help='Disable WPR default script injection such as ' +
-                          'overriding javascript\'s Math.random() and Date() ' +
-                          'with deterministic implementations.')
-  parser.add_argument('--network-condition', default=None,
+  subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
+
+  # Record WPR subcommand.
+  record_wpr = subparsers.add_parser('record-wpr',
+                                     help='Record WPR from sandwich job.')
+  record_wpr.add_argument('--wpr-archive', required=True, type=str,
+                          dest='wpr_archive_path',
+                          help='Web page replay archive to generate.')
+
+  # Create cache subcommand.
+  create_cache_parser = subparsers.add_parser('create-cache',
+      help='Create cache from sandwich job.')
+  create_cache_parser.add_argument('--cache-archive', required=True, type=str,
+                                   dest='cache_archive_path',
+                                   help='Cache archive destination path.')
+  create_cache_parser.add_argument('--wpr-archive', default=None, type=str,
+                                   dest='wpr_archive_path',
+                                   help='Web page replay archive to create ' +
+                                       'the cache from.')
+
+  # Run subcommand.
+  run_parser = subparsers.add_parser('run', help='Run sandwich benchmark.')
+  run_parser.add_argument('--output', required=True, type=str,
+                          dest='trace_output_directory',
+                          help='Path of output directory to create.')
+  run_parser.add_argument('--cache-archive', type=str,
+                          dest='cache_archive_path',
+                          help='Cache archive destination path.')
+  run_parser.add_argument('--cache-op',
+                          choices=['clear', 'push', 'reload'],
+                          dest='cache_operation',
+                          default='clear',
+                          help='Configures cache operation to do before '
+                              +'launching Chrome. (Default is clear). The push'
+                              +' cache operation requires --cache-archive to '
+                              +'set.')
+  run_parser.add_argument('--disable-wpr-script-injection',
+                          action='store_true',
+                          help='Disable WPR default script injection such as ' +
+                              'overriding javascript\'s Math.random() and ' +
+                              'Date() with deterministic implementations.')
+  run_parser.add_argument('--network-condition', default=None,
       choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
       help='Set a network profile.')
-  parser.add_argument('--network-emulator', default='browser',
+  run_parser.add_argument('--network-emulator', default='browser',
       choices=['browser', 'wpr'],
       help='Set which component is emulating the network condition.' +
-          ' (Default to browser)')
+          ' (Default to browser). Wpr network emulator requires --wpr-archive' +
+          ' to be set.')
+  run_parser.add_argument('--job-repeat', default=1, type=int,
+                          help='How many times to run the job.')
+  run_parser.add_argument('--wpr-archive', default=None, type=str,
+                          dest='wpr_archive_path',
+                          help='Web page replay archive to load job\'s urls ' +
+                              'from.')
   return parser
 
 
-def main():
+def _RecordWprMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.wpr_record = True
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def _CreateCacheMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.cache_operation = 'save'
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def _RunJobMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
 
@@ -137,102 +379,16 @@ def main():
   # the default values of OPTIONS.
   OPTIONS.ParseArgs([])
 
-  args = _ArgumentParser().parse_args()
-
-  if not os.path.isdir(args.output):
-    try:
-      os.makedirs(args.output)
-    except OSError:
-      logging.error('Cannot create directory for results: %s' % args.output)
-      raise
-  else:
-    _CleanPreviousTraces(args.output)
-
-  run_infos = {
-    'cache-op': args.cache_op,
-    'job': args.job,
-    'urls': []
-  }
-  job_urls = _ReadUrlsFromJobDescription(args.job)
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  local_cache_archive_path = os.path.join(args.output, 'cache.zip')
-  local_cache_directory_path = None
-  wpr_network_condition_name = None
-  browser_network_condition_name = None
-  if args.network_emulator == 'wpr':
-    wpr_network_condition_name = args.network_condition
-  elif args.network_emulator == 'browser':
-    browser_network_condition_name = args.network_condition
-  else:
-    assert False
-
-  if args.cache_op == 'push':
-    assert os.path.isfile(local_cache_archive_path)
-    local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-    chrome_cache.UnzipDirectoryContent(
-        local_cache_archive_path, local_cache_directory_path)
-
-  with device_setup.WprHost(device, args.wpr_archive,
-      record=args.wpr_record,
-      network_condition_name=wpr_network_condition_name,
-      disable_script_injection=args.disable_wpr_script_injection
-      ) as additional_flags:
-    def _RunNavigation(url, clear_cache, trace_id):
-      with device_setup.DeviceConnection(
-          device=device,
-          additional_flags=additional_flags) as connection:
-        additional_metadata = {}
-        if browser_network_condition_name:
-          additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
-              connection=connection,
-              emulated_device_name=None,
-              emulated_network_name=browser_network_condition_name)
-        loading_trace = trace_recorder.MonitorUrl(
-            connection, url,
-            clear_cache=clear_cache,
-            categories=pull_sandwich_metrics.CATEGORIES,
-            timeout=_DEVTOOLS_TIMEOUT)
-        loading_trace.metadata.update(additional_metadata)
-        if trace_id != None:
-          loading_trace_path = os.path.join(
-              args.output, str(trace_id), 'trace.json')
-          os.makedirs(os.path.dirname(loading_trace_path))
-          loading_trace.ToJsonFile(loading_trace_path)
-
-    for _ in xrange(args.repeat):
-      for url in job_urls:
-        clear_cache = False
-        if args.cache_op == 'clear':
-          clear_cache = True
-        elif args.cache_op == 'push':
-          device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-          chrome_cache.PushBrowserCache(device, local_cache_directory_path)
-        elif args.cache_op == 'reload':
-          _RunNavigation(url, clear_cache=True, trace_id=None)
-        elif args.cache_op == 'save':
-          clear_cache = not run_infos['urls']
-        _RunNavigation(url, clear_cache=clear_cache,
-                       trace_id=len(run_infos['urls']))
-        run_infos['urls'].append(url)
-
-  if local_cache_directory_path:
-    shutil.rmtree(local_cache_directory_path)
-
-  if args.cache_op == 'save':
-    # Move Chrome to background to allow it to flush the index.
-    device.adb.Shell('am start com.google.android.launcher')
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-
-    cache_directory_path = chrome_cache.PullBrowserCache(device)
-    chrome_cache.ZipDirectoryContent(
-        cache_directory_path, local_cache_archive_path)
-    shutil.rmtree(cache_directory_path)
+  args = _ArgumentParser().parse_args(command_line_args)
 
-  with open(os.path.join(args.output, 'run_infos.json'), 'w') as file_output:
-    json.dump(run_infos, file_output, indent=2)
+  if args.subcommand == 'record-wpr':
+    return _RecordWprMain(args)
+  if args.subcommand == 'create-cache':
+    return _CreateCacheMain(args)
+  if args.subcommand == 'run':
+    return _RunJobMain(args)
+  assert False
 
 
 if __name__ == '__main__':
-  sys.exit(main())
+  sys.exit(main(sys.argv[1:]))

commit a4b0fc00f331941e69992a5dcd5a7f11edb9c3fe
Author: mattcary <mattcary@chromium.org>
Date:   Thu Feb 25 09:42:15 2016 -0800

    Fix bug in my understanding of python slices.
    
    Review URL: https://codereview.chromium.org/1735953003
    
    Cr-Original-Commit-Position: refs/heads/master@{#377603}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: dc9bee425efc025e9a92a87dae5d29d84fa217b3

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 923183e..d929ac4 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -203,11 +203,11 @@ class ResourceGraph(object):
       costs[n.Index()] = cost
     max_cost = max(costs)
     if costs_out is not None:
-      del costs_out[:-1]
+      del costs_out[:]
       costs_out.extend(costs)
     assert max_cost > 0  # Otherwise probably the filter went awry.
     if path_list is not None:
-      del path_list[:-1]
+      del path_list[:]
       n = (i for i in self._nodes if costs[i.Index()] == max_cost).next()
       path_list.append(self._node_info[n.Index()])
       while n.Predecessors():

commit 3c8a865ae142634931d2d6909ac2c06f80ce073a
Author: mattcary <mattcary@chromium.org>
Date:   Thu Feb 25 08:37:48 2016 -0800

    Add ExtractArgs to options to enable it to play nicely with custom argument parsing.
    
    Review URL: https://codereview.chromium.org/1727263004
    
    Cr-Original-Commit-Position: refs/heads/master@{#377589}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9e0410c9fc70ddc3961678fca476d9de834208f2

diff --git a/loading/options.py b/loading/options.py
index a11b56c..ec295ac 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -62,11 +62,11 @@ class Options(object):
     """
     self._ARGS.append((arg_name, default, help_str))
 
-  def ParseArgs(self, arg_str, description=None, extra=None):
+  def ParseArgs(self, arg_list, description=None, extra=None):
     """Parse command line arguments.
 
     Args:
-      arg_str: command line argument string.
+      arg_list: command line argument list.
       description: description to use in argument parser.
       extra: additional required arguments to add. These will be exposed as
         instance attributes. This is either a list of extra arguments, or a
@@ -75,7 +75,46 @@ class Options(object):
         used as in argparse, ie those beginning with -- are named, and those
         without a dash are positional. Don't use a single dash.
     """
-    parser = argparse.ArgumentParser(description=description)
+    parser = self._MakeParser(description, extra)
+    self._parsed_args = parser.parse_args(arg_list)
+
+  def ExtractArgs(self, arg_list):
+    """Extract arguments from arg_str.
+
+    Args:
+      arg_list: command line argument list. It will be changed so that arguments
+        used by this options instance are removed.
+    """
+    parser = self._MakeParser()
+    (self._parsed_args, unused) = parser.parse_known_args(arg_list)
+    del arg_list[:]
+    arg_list.extend(unused)
+
+  def GetParentParser(self, group_name='Global'):
+    """Returns a parser suitable for passing in as a parent to argparse.
+
+    Args:
+      group_name: A group name for the parser (see argparse's
+        add_argument_group).
+
+    Returns:
+      An argparse parser instance.
+    """
+    return self._MakeParser(group=group_name)
+
+  def SetParsedArgs(self, parsed_args):
+    """Set parsed args. Used with GetParentParser.
+
+    Args:
+      parsed_args: the result of argparse.parse_args or similar.
+    """
+    self._parsed_args = parsed_args
+
+  def _MakeParser(self, description=None, extra=None, group=None):
+    add_help = True if group is None else False
+    parser = argparse.ArgumentParser(
+        description=description, add_help=add_help)
+    container = parser if group is None else parser.add_argument_group(group)
     for arg, default, help_str in self._ARGS:
       # All global options are named.
       arg = '--' + arg
@@ -86,12 +125,12 @@ class Options(object):
       for arg in extra:
         if type(arg) is tuple:
           argname, default = arg
-          self._AddArg(parser, argname, default)
+          self._AddArg(container, argname, default)
         else:
-          self._AddArg(parser, arg, None, required=True)
-    self._parsed_args = parser.parse_args(arg_str)
+          self._AddArg(container, arg, None, required=True)
+    return parser
 
-  def _AddArg(self, parser, arg, default, required=False, help_str=None):
+  def _AddArg(self, container, arg, default, required=False, help_str=None):
     assert not arg.startswith('-') or arg.startswith('--'), \
         "Single dash arguments aren't supported: %s" % arg
     arg_name = arg
@@ -117,7 +156,7 @@ class Options(object):
         kwargs['default'] = default
         kwargs['type'] = type(default)
 
-    parser.add_argument(arg, **kwargs)
+    container.add_argument(arg, **kwargs)
 
   def __getattr__(self, name):
     if name in self._arg_set:
diff --git a/loading/options_unittest.py b/loading/options_unittest.py
new file mode 100644
index 0000000..9ecca66
--- /dev/null
+++ b/loading/options_unittest.py
@@ -0,0 +1,32 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import unittest
+
+import options
+
+
+class OptionsTestCase(unittest.TestCase):
+  def testExtract(self):
+    args = ['--A', 'foo', '--devtools_port', '2000', '--B=20',
+            '--no_sandbox', '--C', '30', 'baz']
+    opts = options.Options()
+    opts.ExtractArgs(args)
+    self.assertEqual(['--A', 'foo', '--B=20', '--C', '30', 'baz'], args)
+    self.assertEqual(2000, opts.devtools_port)
+    self.assertTrue(opts.no_sandbox)
+
+  def testParent(self):
+    opts = options.Options()
+    parser = argparse.ArgumentParser(parents=[opts.GetParentParser()])
+    parser.add_argument('--foo', type=int)
+    parsed_args = parser.parse_args(['--foo=4', '--devtools_port', '2000'])
+    self.assertEqual(4, parsed_args.foo)
+    opts.SetParsedArgs(parsed_args)
+    self.assertEqual(2000, opts.devtools_port)
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 0bc21ed5d6d5d228d02a2d31422c3d3fef55783b
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 25 06:09:43 2016 -0800

    tools/android/loading: Implements WprBackend
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1722243002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377569}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c1adaaec9a61bb544d81d9218433990a1a159311

diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
new file mode 100644
index 0000000..c3a84a1
--- /dev/null
+++ b/loading/wpr_backend.py
@@ -0,0 +1,124 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Opens and modifies WPR archive.
+"""
+
+import collections
+import os
+import re
+import sys
+
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+_WEBPAGEREPLAY_DIR = os.path.join(_SRC_DIR, 'third_party', 'webpagereplay')
+_WEBPAGEREPLAY_HTTPARCHIVE = os.path.join(_WEBPAGEREPLAY_DIR, 'httparchive.py')
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
+import httparchive
+
+# Regex used to parse httparchive.py stdout's when listing all urls.
+_PARSE_WPR_REQUEST_REGEX = re.compile(r'^\S+\s+(?P<url>\S+)')
+
+
+class WprUrlEntry(object):
+  """Wpr url entry holding request and response infos. """
+
+  def __init__(self, wpr_request, wpr_response):
+    self._wpr_response = wpr_response
+    self.url = self._ExtractUrl(str(wpr_request))
+
+  def GetResponseHeadersDict(self):
+    """Get a copied dictionary of available headers.
+
+    Returns:
+      dict(name -> value)
+    """
+    headers = collections.defaultdict(list)
+    for (key, value) in self._wpr_response.headers:
+      headers[key].append(value)
+    return {k: ','.join(v) for (k, v) in headers.items()}
+
+  def SetResponseHeader(self, name, value):
+    """Set a header value.
+
+    In the case where the <name> response header is present more than once
+    in the response header list, then the given value is set only to the first
+    occurrence of that given headers, and the next ones are removed.
+
+    Args:
+      name: The name of the response header to set.
+      value: The value of the response header to set.
+    """
+    new_headers = []
+    new_header_set = False
+    for header in self._wpr_response.headers:
+      if header[0] != name:
+        new_headers.append(header)
+      elif not new_header_set:
+        new_header_set = True
+        new_headers.append((name, value))
+    if new_header_set:
+      self._wpr_response.headers = new_headers
+    else:
+      self._wpr_response.headers.append((name, value))
+
+  def DeleteResponseHeader(self, name):
+    """Delete a header.
+
+    In the case where the <name> response header is present more than once
+    in the response header list, this method takes care of removing absolutely
+    all them.
+
+    Args:
+      name: The name of the response header field to delete.
+    """
+    self._wpr_response.headers = \
+        [x for x in self._wpr_response.headers if x[0] != name]
+
+  @classmethod
+  def _ExtractUrl(cls, request_string):
+    match = _PARSE_WPR_REQUEST_REGEX.match(request_string)
+    assert match, 'Looks like there is an issue with: {}'.format(request_string)
+    return match.group('url')
+
+
+class WprArchiveBackend(object):
+  """WPR archive back-end able to read and modify. """
+
+  def __init__(self, wpr_archive_path):
+    """Constructor:
+
+    Args:
+      wpr_archive_path: The path of the WPR archive to read/modify.
+    """
+    self._wpr_archive_path = wpr_archive_path
+    self._http_archive = httparchive.HttpArchive.Load(wpr_archive_path)
+
+  def ListUrlEntries(self):
+    """Iterates over all url entries
+
+    Returns:
+      A list of WprUrlEntry.
+    """
+    return [WprUrlEntry(request, self._http_archive[request])
+            for request in self._http_archive.get_requests()]
+
+  def Persist(self):
+    """Persists the archive to disk. """
+    self._http_archive.Persist(self._wpr_archive_path)
+
+
+if __name__ == '__main__':
+  import argparse
+  parser = argparse.ArgumentParser(description='Tests cache back-end.')
+  parser.add_argument('wpr_archive', type=str)
+  command_line_args = parser.parse_args()
+
+  wpr_backend = WprArchiveBackend(command_line_args.wpr_archive)
+  url_entries = wpr_backend.ListUrlEntries()
+  print url_entries[0].url
+  wpr_backend.Persist()
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
new file mode 100644
index 0000000..e38070c
--- /dev/null
+++ b/loading/wpr_backend_unittest.py
@@ -0,0 +1,92 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from wpr_backend import WprUrlEntry
+
+
+class MockWprResponse(object):
+  def __init__(self, headers):
+    self.headers = headers
+
+class WprUrlEntryTest(unittest.TestCase):
+
+  @classmethod
+  def _CreateWprUrlEntry(cls, headers):
+    wpr_response = MockWprResponse(headers)
+    return WprUrlEntry('GET http://a.com/', wpr_response)
+
+  def test_ExtractUrl(self):
+    self.assertEquals('http://aa.bb/c',
+                      WprUrlEntry._ExtractUrl('GET http://aa.bb/c'))
+    self.assertEquals('http://aa.b/c',
+                      WprUrlEntry._ExtractUrl('POST http://aa.b/c'))
+    self.assertEquals('http://a.bb/c',
+                      WprUrlEntry._ExtractUrl('WHATEVER http://a.bb/c'))
+    self.assertEquals('https://aa.bb/c',
+                      WprUrlEntry._ExtractUrl('GET https://aa.bb/c'))
+    self.assertEquals('http://aa.bb',
+                      WprUrlEntry._ExtractUrl('GET http://aa.bb'))
+    self.assertEquals('http://aa.bb',
+                      WprUrlEntry._ExtractUrl('GET http://aa.bb FOO BAR'))
+
+  def test_GetResponseHeadersDict(self):
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header0', 'value2'),
+                                     ('header2', 'value3'),
+                                     ('header0', 'value4')])
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('value0,value2,value4', headers['header0'])
+    self.assertEquals('value1', headers['header1'])
+    self.assertEquals('value3', headers['header2'])
+
+  def test_SetResponseHeader(self):
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1')])
+    entry.SetResponseHeader('new_header0', 'new_value0')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('new_value0', headers['new_header0'])
+    self.assertEquals('new_header0', entry._wpr_response.headers[2][0])
+
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header2', 'value1'),])
+    entry.SetResponseHeader('header1', 'new_value1')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('new_value1', headers['header1'])
+    self.assertEquals('header1', entry._wpr_response.headers[1][0])
+
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header2', 'value2'),
+                                     ('header1', 'value3'),
+                                     ('header3', 'value4')])
+    entry.SetResponseHeader('header1', 'new_value2')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(4, len(headers))
+    self.assertEquals('new_value2', headers['header1'])
+    self.assertEquals('header1', entry._wpr_response.headers[1][0])
+    self.assertEquals('header3', entry._wpr_response.headers[3][0])
+    self.assertEquals('value4', entry._wpr_response.headers[3][1])
+
+  def test_DeleteResponseHeader(self):
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header0', 'value2'),
+                                     ('header2', 'value3')])
+    entry.DeleteResponseHeader('header1')
+    self.assertNotIn('header1', entry.GetResponseHeadersDict())
+    self.assertEquals(2, len(entry.GetResponseHeadersDict()))
+    entry.DeleteResponseHeader('header0')
+    self.assertNotIn('header0', entry.GetResponseHeadersDict())
+    self.assertEquals(1, len(entry.GetResponseHeadersDict()))
+
+
+if __name__ == '__main__':
+  unittest.main()

commit b53fdc4bfa7d6a7047354d3cf3372ac1423b0c8b
Author: mattcary <mattcary@chromium.org>
Date:   Wed Feb 24 04:22:56 2016 -0800

    DOT output of multi-graph ResourceSack analysis.
    
    Review URL: https://codereview.chromium.org/1726573002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377274}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 311d001586e703ed92e460a0ffafdf00b9006326

diff --git a/loading/resource_sack_display.py b/loading/resource_sack_display.py
new file mode 100644
index 0000000..210f4da
--- /dev/null
+++ b/loading/resource_sack_display.py
@@ -0,0 +1,135 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Utilities for displaying a ResourceSack.
+
+When run standalone, takes traces on the command line and produces a dot file to
+stdout.
+"""
+
+
+def ToDot(sack, output, prune=-1, long_edge_msec=2000):
+  """Output as a dot file.
+
+  Args:
+    sack: (ResourceSack) the sack to convert to dot.
+    output: a file-like output stream.
+    prune: if positive, prune & coalesce nodes under the specified threshold
+      of repeated views, as fraction node views / total graphs. All pruned
+      nodes are represented by a single node, and an edge is connected only if
+      the view count is greater than 1.
+    long_edge_msec: if positive, the definition of a long edge. Long edges are
+      distinguished in graph.
+  """
+  output.write("""digraph dependencies {
+  rankdir = LR;
+  """)
+
+  pruned = set()
+  num_graphs = len(sack.graph_info)
+  for bag in sack.bags:
+    if prune > 0 and float(len(bag.graphs)) / num_graphs < prune:
+      pruned.add(bag)
+      continue
+    output.write('%d [label="%s (%d)\n(%d, %d)\n(%.2f, %.2f)" shape=%s; '
+                 'style=filled; fillcolor=%s];\n' % (
+        bag.Index(), bag.label, len(bag.graphs),
+        min(bag.total_costs), max(bag.total_costs),
+        min(bag.relative_costs), max(bag.relative_costs),
+        _CriticalToShape(bag),
+        _AmountToNodeColor(len(bag.graphs), num_graphs)))
+
+  if pruned:
+    pruned_index = num_graphs
+    output.write('%d [label="Pruned at %.0f%%\n(%d)"; '
+                 'shape=polygon; style=dotted];\n' %
+                 (pruned_index, 100 * prune, len(pruned)))
+
+  for bag in sack.bags:
+    if bag in pruned:
+      for succ in bag.Successors():
+        if succ not in pruned:
+          output.write('%d -> %d [style=dashed];\n' % (
+              pruned_index, succ.Index()))
+    for succ in bag.Successors():
+      if succ in pruned:
+        if len(bag.successor_sources[succ]) > 1:
+          output.write('%d -> %d [label="%d"; style=dashed];\n' % (
+              bag.Index(), pruned_index, len(bag.successor_sources[succ])))
+      else:
+        num_succ = len(bag.successor_sources[succ])
+        num_long = 0
+        for graph, source, target in bag.successor_sources[succ]:
+          if graph.EdgeCost(source, target) > long_edge_msec:
+            num_long += 1
+        if num_long > 0:
+          long_frac = float(num_long) / num_succ
+          long_edge_style = '; penwidth=%f' % (2 + 6.0 * long_frac)
+          if long_frac < 0.75:
+            long_edge_style += '; style=dashed'
+        else:
+          long_edge_style = ''
+        min_edge = min(bag.successor_edge_costs[succ])
+        max_edge = max(bag.successor_edge_costs[succ])
+        output.write('%d -> %d [label="%d\n(%f,%f)"; color=%s %s];\n' % (
+            bag.Index(), succ.Index(), num_succ, min_edge, max_edge,
+            _AmountToEdgeColor(num_succ, len(bag.graphs)),
+            long_edge_style))
+
+  output.write('}')
+
+
+def _CriticalToShape(bag):
+  frac = float(bag.num_critical) / bag.num_nodes
+  if frac < 0.4:
+    return 'oval'
+  elif frac < 0.7:
+    return 'polygon'
+  elif frac < 0.9:
+    return 'trapezium'
+  return 'box'
+
+
+def _AmountToNodeColor(numer, denom):
+  if denom <= 0:
+    return 'grey72'
+  ratio = 1.0 * numer / denom
+  if ratio < .3:
+    return 'white'
+  elif ratio < .6:
+    return 'yellow'
+  elif ratio < .8:
+    return 'orange'
+  return 'green'
+
+
+def _AmountToEdgeColor(numer, denom):
+  color = _AmountToNodeColor(numer, denom)
+  if color == 'white' or color == 'grey72':
+    return 'black'
+  return color
+
+
+def _Main():
+  import json
+  import logging
+  import sys
+
+  import loading_model
+  import loading_trace
+  import resource_sack
+
+  sack = resource_sack.GraphSack()
+  for fname in sys.argv[1:]:
+    trace = loading_trace.LoadingTrace.FromJsonDict(
+      json.load(open(fname)))
+    logging.info('Making graph from %s', fname)
+    model = loading_model.ResourceGraph(trace, content_lens=None)
+    sack.ConsumeGraph(model)
+    logging.info('Finished %s', fname)
+  ToDot(sack, sys.stdout, prune=.1)
+
+
+if __name__ == '__main__':
+  _Main()
diff --git a/loading/resource_sack_display_unittest.py b/loading/resource_sack_display_unittest.py
new file mode 100644
index 0000000..65b53ff
--- /dev/null
+++ b/loading/resource_sack_display_unittest.py
@@ -0,0 +1,42 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import re
+from StringIO import StringIO
+import unittest
+
+import resource_sack
+import resource_sack_display
+from test_utils import (MakeRequest,
+                        TestResourceGraph)
+
+
+class ResourceSackDispayTestCase(unittest.TestCase):
+  def test_SimpleOutput(self):
+    g1 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(3, 1)])
+    g2 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(4, 2)])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    sack.ConsumeGraph(g2)
+    buf = StringIO()
+    resource_sack_display.ToDot(sack, buf,
+                                long_edge_msec=1000)
+    dot = buf.getvalue()
+    # Short edge.
+    self.assertTrue(re.search(r'0 -> 1[^]]+color=green \]', dot, re.MULTILINE))
+    # Long edge.
+    self.assertTrue(re.search(r'0 -> 3[^]]+penwidth=8', dot))
+
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 2549e4b2415e46fe789b490cca240e6a87e43658
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 23 12:45:03 2016 -0800

    tools/android/loading: Moves cache specific code to chrome_cache.py
    
    run_sandwich.py had some cache related code such as pulling/pushing
    code from/to android device and zip/unzip cache directory keeping
    track of all file timestamps.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1712193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377077}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: dca6c239da6ac53adce1d87a01c837a940b25a1b

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 9e282b6..d6d65a2 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -5,8 +5,24 @@
 """Takes care of manipulating the chrome's HTTP cache.
 """
 
+from datetime import datetime
+import json
 import os
 import subprocess
+import sys
+import tempfile
+import zipfile
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
+
+import options
+
+
+OPTIONS = options.OPTIONS
 
 
 # Cache back-end types supported by cachetool.
@@ -20,6 +36,172 @@ OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
 CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
 
 
+def _RemoteCacheDirectory():
+  """Returns the path of the cache directory's on the remote device."""
+  return '/data/data/{}/cache/Cache'.format(
+      constants.PACKAGE_INFO[OPTIONS.chrome_package_name].package)
+
+
+def _UpdateTimestampFromAdbStat(filename, stat):
+  os.utime(filename, (stat.st_time, stat.st_time))
+
+
+def _AdbShell(adb, cmd):
+  adb.Shell(subprocess.list2cmdline(cmd))
+
+
+def _AdbUtime(adb, filename, timestamp):
+  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
+  """
+  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
+  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
+
+
+def PullBrowserCache(device):
+  """Pulls the browser cache from the device and saves it locally.
+
+  Cache is saved with the same file structure as on the device. Timestamps are
+  important to preserve because indexing and eviction depends on them.
+
+  Returns:
+    Temporary directory containing all the browser cache.
+  """
+  _INDEX_DIRECTORY_NAME = 'index-dir'
+  _REAL_INDEX_FILE_NAME = 'the-real-index'
+
+  remote_cache_directory = _RemoteCacheDirectory()
+  print remote_cache_directory
+  save_target = tempfile.mkdtemp(suffix='.cache')
+  for filename, stat in device.adb.Ls(remote_cache_directory):
+    if filename == '..':
+      continue
+    if filename == '.':
+      cache_directory_stat = stat
+      continue
+    original_file = os.path.join(remote_cache_directory, filename)
+    saved_file = os.path.join(save_target, filename)
+    device.adb.Pull(original_file, saved_file)
+    _UpdateTimestampFromAdbStat(saved_file, stat)
+    if filename == _INDEX_DIRECTORY_NAME:
+      # The directory containing the index was pulled recursively, update the
+      # timestamps for known files. They are ignored by cache backend, but may
+      # be useful for debugging.
+      index_dir_stat = stat
+      saved_index_dir = os.path.join(save_target, _INDEX_DIRECTORY_NAME)
+      saved_index_file = os.path.join(saved_index_dir, _REAL_INDEX_FILE_NAME)
+      for sub_file, sub_stat in device.adb.Ls(original_file):
+        if sub_file == _REAL_INDEX_FILE_NAME:
+          _UpdateTimestampFromAdbStat(saved_index_file, sub_stat)
+          break
+      _UpdateTimestampFromAdbStat(saved_index_dir, index_dir_stat)
+
+  # Store the cache directory modification time. It is important to update it
+  # after all files in it have been written. The timestamp is compared with
+  # the contents of the index file when freshness is determined.
+  _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
+  return save_target
+
+
+def PushBrowserCache(device, local_cache_path):
+  """Pushes the browser cache saved locally to the device.
+
+  Args:
+    device: Android device.
+    local_cache_path: The directory's path containing the cache locally.
+  """
+  remote_cache_directory = _RemoteCacheDirectory()
+
+  # Clear previous cache.
+  _AdbShell(device.adb, ['rm', '-rf', remote_cache_directory])
+  _AdbShell(device.adb, ['mkdir', remote_cache_directory])
+
+  # Push cache content.
+  device.adb.Push(local_cache_path, remote_cache_directory)
+
+  # Walk through the local cache to update mtime on the device.
+  def MirrorMtime(local_path):
+    cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
+    remote_path = os.path.join(remote_cache_directory, cache_relative_path)
+    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
+
+  for local_directory_path, dirnames, filenames in os.walk(
+        local_cache_path, topdown=False):
+    for filename in filenames:
+      MirrorMtime(os.path.join(local_directory_path, filename))
+    for dirname in dirnames:
+      MirrorMtime(os.path.join(local_directory_path, dirname))
+  MirrorMtime(local_cache_path)
+
+
+def ZipDirectoryContent(root_directory_path, archive_dest_path):
+  """Zip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    root_directory_path: The directory's path to archive.
+    archive_dest_path: Archive destination's path.
+  """
+  with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
+    timestamps = {}
+    root_directory_stats = os.stat(root_directory_path)
+    timestamps['.'] = {
+        'atime': root_directory_stats.st_atime,
+        'mtime': root_directory_stats.st_mtime}
+    for directory_path, dirnames, filenames in os.walk(root_directory_path):
+      for dirname in dirnames:
+        subdirectory_path = os.path.join(directory_path, dirname)
+        subdirectory_relative_path = os.path.relpath(subdirectory_path,
+                                                     root_directory_path)
+        subdirectory_stats = os.stat(subdirectory_path)
+        timestamps[subdirectory_relative_path] = {
+            'atime': subdirectory_stats.st_atime,
+            'mtime': subdirectory_stats.st_mtime}
+      for filename in filenames:
+        file_path = os.path.join(directory_path, filename)
+        file_archive_name = os.path.join('content',
+            os.path.relpath(file_path, root_directory_path))
+        file_stats = os.stat(file_path)
+        timestamps[file_archive_name[8:]] = {
+            'atime': file_stats.st_atime,
+            'mtime': file_stats.st_mtime}
+        zip_output.write(file_path, arcname=file_archive_name)
+    zip_output.writestr('timestamps.json',
+                        json.dumps(timestamps, indent=2))
+
+
+def UnzipDirectoryContent(archive_path, directory_dest_path):
+  """Unzip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    archive_path: Archive's path to unzip.
+    directory_dest_path: Directory destination path.
+  """
+  if not os.path.exists(directory_dest_path):
+    os.makedirs(directory_dest_path)
+
+  with zipfile.ZipFile(archive_path) as zip_input:
+    timestamps = None
+    for file_archive_name in zip_input.namelist():
+      if file_archive_name == 'timestamps.json':
+        timestamps = json.loads(zip_input.read(file_archive_name))
+      elif file_archive_name.startswith('content/'):
+        file_relative_path = file_archive_name[8:]
+        file_output_path = os.path.join(directory_dest_path, file_relative_path)
+        file_parent_directory_path = os.path.dirname(file_output_path)
+        if not os.path.exists(file_parent_directory_path):
+          os.makedirs(file_parent_directory_path)
+        with open(file_output_path, 'w') as f:
+          f.write(zip_input.read(file_archive_name))
+
+    assert timestamps
+    for relative_path, stats in timestamps.iteritems():
+      output_path = os.path.join(directory_dest_path, relative_path)
+      if not os.path.exists(output_path):
+        os.makedirs(output_path)
+      os.utime(output_path, (stats['atime'], stats['mtime']))
+
+
 class CacheBackend(object):
   """Takes care of reading and deleting cached keys.
   """
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 0572d5b..ff98acb 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -12,16 +12,13 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
-from datetime import datetime
 import json
 import logging
 import os
 import shutil
-import subprocess
 import sys
 import tempfile
 import time
-import zipfile
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -33,6 +30,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 import devil_chromium
 
+import chrome_cache
 import chrome_setup
 import device_setup
 import devtools_monitor
@@ -48,24 +46,11 @@ OPTIONS = options.OPTIONS
 
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
-# Directory name under --output to save the cache from the device.
-_CACHE_DIRECTORY_NAME = 'cache'
-
-# Name of cache subdirectory on the device where the cache index is stored.
-_INDEX_DIRECTORY_NAME = 'index-dir'
-
-# Name of the file containing the cache index. This file is stored on the device
-# in the cache directory under _INDEX_DIRECTORY_NAME.
-_REAL_INDEX_FILE_NAME = 'the-real-index'
-
 # An estimate of time to wait for the device to become idle after expensive
 # operations, such as opening the launcher activity.
 _TIME_TO_DEVICE_IDLE_SECONDS = 2
 
 
-def _RemoteCacheDirectory():
-  return '/data/data/{}/cache/Cache'.format(OPTIONS.chrome_package_name)
-
 # Devtools timeout of 1 minute to avoid websocket timeout on slow
 # network condition.
 _DEVTOOLS_TIMEOUT = 60
@@ -92,159 +77,6 @@ def _ReadUrlsFromJobDescription(job_name):
   raise Exception('Job description does not define a list named "urls"')
 
 
-def _UpdateTimestampFromAdbStat(filename, stat):
-  os.utime(filename, (stat.st_time, stat.st_time))
-
-
-def _AdbShell(adb, cmd):
-  adb.Shell(subprocess.list2cmdline(cmd))
-
-
-def _AdbUtime(adb, filename, timestamp):
-  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
-  """
-  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
-  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
-
-
-def _PullBrowserCache(device):
-  """Pulls the browser cache from the device and saves it locally.
-
-  Cache is saved with the same file structure as on the device. Timestamps are
-  important to preserve because indexing and eviction depends on them.
-
-  Returns:
-    Temporary directory containing all the browser cache.
-  """
-  save_target = tempfile.mkdtemp(suffix='.cache')
-  for filename, stat in device.adb.Ls(_RemoteCacheDirectory()):
-    if filename == '..':
-      continue
-    if filename == '.':
-      cache_directory_stat = stat
-      continue
-    original_file = os.path.join(_RemoteCacheDirectory(), filename)
-    saved_file = os.path.join(save_target, filename)
-    device.adb.Pull(original_file, saved_file)
-    _UpdateTimestampFromAdbStat(saved_file, stat)
-    if filename == _INDEX_DIRECTORY_NAME:
-      # The directory containing the index was pulled recursively, update the
-      # timestamps for known files. They are ignored by cache backend, but may
-      # be useful for debugging.
-      index_dir_stat = stat
-      saved_index_dir = os.path.join(save_target, _INDEX_DIRECTORY_NAME)
-      saved_index_file = os.path.join(saved_index_dir, _REAL_INDEX_FILE_NAME)
-      for sub_file, sub_stat in device.adb.Ls(original_file):
-        if sub_file == _REAL_INDEX_FILE_NAME:
-          _UpdateTimestampFromAdbStat(saved_index_file, sub_stat)
-          break
-      _UpdateTimestampFromAdbStat(saved_index_dir, index_dir_stat)
-
-  # Store the cache directory modification time. It is important to update it
-  # after all files in it have been written. The timestamp is compared with
-  # the contents of the index file when freshness is determined.
-  _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
-  return save_target
-
-
-def _PushBrowserCache(device, local_cache_path):
-  """Pushes the browser cache saved locally to the device.
-
-  Args:
-    device: Android device.
-    local_cache_path: The directory's path containing the cache locally.
-  """
-  # Clear previous cache.
-  _AdbShell(device.adb, ['rm', '-rf', _RemoteCacheDirectory()])
-  _AdbShell(device.adb, ['mkdir', _RemoteCacheDirectory()])
-
-  # Push cache content.
-  device.adb.Push(local_cache_path, _RemoteCacheDirectory())
-
-  # Walk through the local cache to update mtime on the device.
-  def MirrorMtime(local_path):
-    cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
-    remote_path = os.path.join(_RemoteCacheDirectory(), cache_relative_path)
-    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
-
-  for local_directory_path, dirnames, filenames in os.walk(
-        local_cache_path, topdown=False):
-    for filename in filenames:
-      MirrorMtime(os.path.join(local_directory_path, filename))
-    for dirname in dirnames:
-      MirrorMtime(os.path.join(local_directory_path, dirname))
-  MirrorMtime(local_cache_path)
-
-
-def _ZipDirectoryContent(root_directory_path, archive_dest_path):
-  """Zip a directory's content recursively with all the directories'
-  timestamps preserved.
-
-  Args:
-    root_directory_path: The directory's path to archive.
-    archive_dest_path: Archive destination's path.
-  """
-  with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
-    timestamps = {}
-    root_directory_stats = os.stat(root_directory_path)
-    timestamps['.'] = {
-        'atime': root_directory_stats.st_atime,
-        'mtime': root_directory_stats.st_mtime}
-    for directory_path, dirnames, filenames in os.walk(root_directory_path):
-      for dirname in dirnames:
-        subdirectory_path = os.path.join(directory_path, dirname)
-        subdirectory_relative_path = os.path.relpath(subdirectory_path,
-                                                     root_directory_path)
-        subdirectory_stats = os.stat(subdirectory_path)
-        timestamps[subdirectory_relative_path] = {
-            'atime': subdirectory_stats.st_atime,
-            'mtime': subdirectory_stats.st_mtime}
-      for filename in filenames:
-        file_path = os.path.join(directory_path, filename)
-        file_archive_name = os.path.join('content',
-            os.path.relpath(file_path, root_directory_path))
-        file_stats = os.stat(file_path)
-        timestamps[file_archive_name[8:]] = {
-            'atime': file_stats.st_atime,
-            'mtime': file_stats.st_mtime}
-        zip_output.write(file_path, arcname=file_archive_name)
-    zip_output.writestr('timestamps.json',
-                        json.dumps(timestamps, indent=2))
-
-
-def _UnzipDirectoryContent(archive_path, directory_dest_path):
-  """Unzip a directory's content recursively with all the directories'
-  timestamps preserved.
-
-  Args:
-    archive_path: Archive's path to unzip.
-    directory_dest_path: Directory destination path.
-  """
-  if not os.path.exists(directory_dest_path):
-    os.makedirs(directory_dest_path)
-
-  with zipfile.ZipFile(archive_path) as zip_input:
-    timestamps = None
-    for file_archive_name in zip_input.namelist():
-      if file_archive_name == 'timestamps.json':
-        timestamps = json.loads(zip_input.read(file_archive_name))
-      elif file_archive_name.startswith('content/'):
-        file_relative_path = file_archive_name[8:]
-        file_output_path = os.path.join(directory_dest_path, file_relative_path)
-        file_parent_directory_path = os.path.dirname(file_output_path)
-        if not os.path.exists(file_parent_directory_path):
-          os.makedirs(file_parent_directory_path)
-        with open(file_output_path, 'w') as f:
-          f.write(zip_input.read(file_archive_name))
-
-    assert timestamps
-    for relative_path, stats in timestamps.iteritems():
-      output_path = os.path.join(directory_dest_path, relative_path)
-      if not os.path.exists(output_path):
-        os.makedirs(output_path)
-      os.utime(output_path, (stats['atime'], stats['mtime']))
-
-
 def _CleanPreviousTraces(output_directories_path):
   """Cleans previous traces from the output directory.
 
@@ -337,7 +169,8 @@ def main():
   if args.cache_op == 'push':
     assert os.path.isfile(local_cache_archive_path)
     local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-    _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
+    chrome_cache.UnzipDirectoryContent(
+        local_cache_archive_path, local_cache_directory_path)
 
   with device_setup.WprHost(device, args.wpr_archive,
       record=args.wpr_record,
@@ -373,7 +206,7 @@ def main():
           clear_cache = True
         elif args.cache_op == 'push':
           device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-          _PushBrowserCache(device, local_cache_directory_path)
+          chrome_cache.PushBrowserCache(device, local_cache_directory_path)
         elif args.cache_op == 'reload':
           _RunNavigation(url, clear_cache=True, trace_id=None)
         elif args.cache_op == 'save':
@@ -392,8 +225,9 @@ def main():
     device.KillAll(OPTIONS.chrome_package_name, quiet=True)
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
 
-    cache_directory_path = _PullBrowserCache(device)
-    _ZipDirectoryContent(cache_directory_path, local_cache_archive_path)
+    cache_directory_path = chrome_cache.PullBrowserCache(device)
+    chrome_cache.ZipDirectoryContent(
+        cache_directory_path, local_cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
   with open(os.path.join(args.output, 'run_infos.json'), 'w') as file_output:

commit 072cbb22690f1bdde2ff0a1f9f35264226256f0d
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 23 11:21:19 2016 -0800

    tools/android/loading: Implements chrome_cache.CacheBackend.
    
    chrome_cache.CacheBackend is a python wraper of the new cachetool
    command line tool that has the following features: list all key,
    access a key's stream, and delete a key.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1713973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377043}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: aa36ddf8dcdc576dc74c87cf014066c2a7c65d24

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
new file mode 100644
index 0000000..9e282b6
--- /dev/null
+++ b/loading/chrome_cache.py
@@ -0,0 +1,113 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Takes care of manipulating the chrome's HTTP cache.
+"""
+
+import os
+import subprocess
+
+
+# Cache back-end types supported by cachetool.
+BACKEND_TYPES = ['simple']
+
+# Default build output directory.
+OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
+    os.path.dirname(__file__), '../../../out/Release'))
+
+# Default cachetool binary location.
+CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
+
+
+class CacheBackend(object):
+  """Takes care of reading and deleting cached keys.
+  """
+
+  def __init__(self, cache_directory_path, cache_backend_type,
+               cachetool_bin_path=CACHETOOL_BIN_PATH):
+    """Chrome cache back-end constructor.
+
+    Args:
+      cache_directory_path: The directory path where the cache is locally
+        stored.
+      cache_backend_type: A cache back-end type in BACKEND_TYPES.
+      cachetool_bin_path: Path of the cachetool binary.
+    """
+    assert os.path.isdir(cache_directory_path)
+    assert cache_backend_type in BACKEND_TYPES
+    assert os.path.isfile(cachetool_bin_path), 'invalid ' + cachetool_bin_path
+    self._cache_directory_path = cache_directory_path
+    self._cache_backend_type = cache_backend_type
+    self._cachetool_bin_path = cachetool_bin_path
+    # Make sure cache_directory_path is a valid cache.
+    self._CachetoolCmd('validate')
+
+  def ListKeys(self):
+    """Lists cache's keys.
+
+    Returns:
+      A list of all keys stored in the cache.
+    """
+    return [k.strip() for k in self._CachetoolCmd('list_keys').split('\n')[:-1]]
+
+  def GetStreamForKey(self, key, index):
+    """Gets a key's stream.
+
+    Args:
+      key: The key to access the stream.
+      index: The stream index:
+          index=0 is the HTTP response header;
+          index=1 is the transport encoded content;
+          index=2 is the compiled content.
+
+    Returns:
+      String holding stream binary content.
+    """
+    return self._CachetoolCmd('get_stream', key, str(index))
+
+  def DeleteKey(self, key):
+    """Deletes a key from the cache.
+
+    Args:
+      key: The key delete.
+    """
+    self._CachetoolCmd('delete_key', key)
+
+  def _CachetoolCmd(self, operation, *args):
+    """Runs the cache editor tool and return the stdout.
+
+    Args:
+      operation: Cachetool operation.
+      *args: Additional operation argument to append to the command line.
+
+    Returns:
+      Cachetool's stdout string.
+    """
+    editor_tool_cmd = [
+        self._cachetool_bin_path,
+        self._cache_directory_path,
+        self._cache_backend_type,
+        operation]
+    editor_tool_cmd.extend(args)
+    process = subprocess.Popen(editor_tool_cmd, stdout=subprocess.PIPE)
+    stdout_data, _ = process.communicate()
+    assert process.returncode == 0
+    return stdout_data
+
+
+if __name__ == '__main__':
+  import argparse
+  parser = argparse.ArgumentParser(description='Tests cache back-end.')
+  parser.add_argument('cache_path', type=str)
+  parser.add_argument('backend_type', type=str, choices=BACKEND_TYPES)
+  command_line_args = parser.parse_args()
+
+  cache_backend = CacheBackend(
+      cache_directory_path=command_line_args.cache_path,
+      cache_backend_type=command_line_args.backend_type)
+  keys = cache_backend.ListKeys()
+  print '{}\'s HTTP response header:'.format(keys[0])
+  print cache_backend.GetStreamForKey(keys[0], 0)
+  cache_backend.DeleteKey(keys[1])
+  assert keys[1] not in cache_backend.ListKeys()

commit 3ca5499832089b3a2d4c992eadcfac64b023bddd
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 23 08:11:26 2016 -0800

    Core multi-graph analysis. Adds the ResourceSack object which is a graph of
    graphs. The display code (turning this into a DOT) will follow in a separate
    CL.
    
    This combines multiple loading_model.ResourceGraphs into a summary graph. It's intended to be used on multiple traces from the same web page, to figure out what the core part of a web page is. This CL doesn't accomplish that goal, and is instead the starting point of such analysis.
    
    Review URL: https://codereview.chromium.org/1722583002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377000}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 20de63ca4ca7ea72c7d18c9f299fde1467eeb8f6

diff --git a/loading/loading_model.py b/loading/loading_model.py
index f9905a0..923183e 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -38,6 +38,7 @@ class ResourceGraph(object):
   EDGE_KIND_KEY = 'edge_kind'
   EDGE_KINDS = request_track.Request.INITIATORS + (
       'script_inferred', 'after-load', 'before-load', 'timing')
+
   def __init__(self, trace, content_lens=None, frame_lens=None,
                activity=None):
     """Create from a LoadingTrace (or json of a trace).
@@ -179,11 +180,13 @@ class ResourceGraph(object):
       if self._node_filter(n.Node()) and n.Url() in other_map:
         yield(n, other_map[n.Url()])
 
-  def Cost(self, path_list=None):
+  def Cost(self, path_list=None, costs_out=None):
     """Compute cost of current model.
 
     Args:
       path_list: if not None, gets a list of NodeInfo in the longest path.
+      costs_out: if not None, gets a vector of node costs by node index. Any
+        filtered nodes will have zero cost.
 
     Returns:
       Cost of the longest path.
@@ -199,6 +202,9 @@ class ResourceGraph(object):
         cost += self.NodeCost(n)
       costs[n.Index()] = cost
     max_cost = max(costs)
+    if costs_out is not None:
+      del costs_out[:-1]
+      costs_out.extend(costs)
     assert max_cost > 0  # Otherwise probably the filter went awry.
     if path_list is not None:
       del path_list[:-1]
@@ -385,7 +391,7 @@ class ResourceGraph(object):
 
     def EdgeAnnotations(self, s):
       assert s.Node() in self.Node().Successors()
-      return self._edge_annotations.get(s, [])
+      return self._edge_annotations.get(s, {})
 
     def ContentType(self):
       if self._request is None:
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
new file mode 100644
index 0000000..493029c
--- /dev/null
+++ b/loading/resource_sack.py
@@ -0,0 +1,181 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""A collection of ResourceGraphs.
+
+Processes multiple ResourceGraphs, all presumably from requests to the same
+site. Common urls are collected in Bags and different statistics on the
+relationship between bags are collected.
+"""
+
+import collections
+import json
+import sys
+import urlparse
+
+from collections import defaultdict
+
+import content_classification_lens
+import dag
+import user_satisfied_lens
+
+class GraphSack(object):
+  """Aggreate of ResourceGraphs.
+
+  Collects ResourceGraph nodes into bags, where each bag contains the nodes with
+  common urls. Dependency edges are tracked between bags (so that each bag may
+  be considered as a node of a graph). This graph of bags is referred to as a
+  sack.
+
+  Each bag is associated with a dag.Node, even though the bag graph may not be a
+  DAG. The edges are annotated with list of graphs and nodes that generated
+  them.
+  """
+  _GraphInfo = collections.namedtuple('_GraphInfo', (
+      'cost',   # The graph cost (aka critical path length).
+      'total_costs',  # A vector by node index of total cost of each node.
+      ))
+
+  def __init__(self):
+    # A bag is a node in our combined graph.
+    self._bags = []
+    # Each bag in our sack corresponds to a url, as expressed by this map.
+    self._url_to_bag = {}
+    # Maps graph -> _GraphInfo structures for each graph we've consumed.
+    self._graph_info = {}
+
+  def ConsumeGraph(self, graph):
+    """Add a graph and process.
+
+    Args:
+      graph: (ResourceGraph) the graph to add. The graph is processed sorted
+        according to its current filter.
+    """
+    assert graph not in self._graph_info
+    critical_path = []
+    total_costs = []
+    cost = graph.Cost(path_list=critical_path,
+                      costs_out=total_costs)
+    self._graph_info[graph] = self._GraphInfo(
+        cost=cost, total_costs=total_costs)
+    for n in graph.Nodes(sort=True):
+      assert graph._node_filter(n.Node())
+      self.AddNode(graph, n)
+    for node in critical_path:
+      self._url_to_bag[node.Url()].MarkCritical()
+
+  def AddNode(self, graph, node):
+    """Add a node to our collection.
+
+    Args:
+      graph: (ResourceGraph) the graph in which the node lives.
+      node: (NodeInfo) the node to add.
+
+    Returns:
+      The Bag containing the node.
+    """
+    if not graph._node_filter(node):
+      return
+    if node.Url() not in self._url_to_bag:
+      new_index = len(self._bags)
+      self._bags.append(Bag(self, new_index, node.Url()))
+      self._url_to_bag[node.Url()] = self._bags[-1]
+    self._url_to_bag[node.Url()].AddNode(graph, node)
+    return self._url_to_bag[node.Url()]
+
+  @property
+  def graph_info(self):
+    return self._graph_info
+
+  @property
+  def bags(self):
+    return self._bags
+
+class Bag(dag.Node):
+  def __init__(self, sack, index, url):
+    super(Bag, self).__init__(index)
+    self._sack = sack
+    self._url = url
+    self._label = self._MakeShortname(url)
+    # Maps a ResourceGraph to its Nodes contained in this Bag.
+    self._graphs = defaultdict(set)
+    # Maps each successor bag to the set of (graph, node, graph-successor)
+    # tuples that generated it.
+    self._successor_sources = defaultdict(set)
+    # Maps each successor bag to a set of edge costs. This is just used to
+    # track min and max; if we want more statistics we'd have to count the
+    # costs with multiplicity.
+    self._successor_edge_costs = defaultdict(set)
+
+    # Miscellaneous counts and costs used in display.
+    self._total_costs = []
+    self._relative_costs = []
+    self._num_critical = 0
+
+  @property
+  def url(self):
+    return self._url
+
+  @property
+  def label(self):
+    return self._label
+
+  @property
+  def graphs(self):
+    return self._graphs
+
+  @property
+  def successor_sources(self):
+    return self._successor_sources
+
+  @property
+  def successor_edge_costs(self):
+    return self._successor_edge_costs
+
+  @property
+  def total_costs(self):
+    return self._total_costs
+
+  @property
+  def relative_costs(self):
+    return self._relative_costs
+
+  @property
+  def num_critical(self):
+    return self._num_critical
+
+  @property
+  def num_nodes(self):
+    return len(self._total_costs)
+
+  def MarkCritical(self):
+    self._num_critical += 1
+
+  def AddNode(self, graph, node):
+    if node in self._graphs[graph]:
+      return  # Already added.
+    graph_info = self._sack.graph_info[graph]
+    self._graphs[graph].add(node)
+    node_total_cost = graph_info.total_costs[node.Index()]
+    self._total_costs.append(node_total_cost)
+    self._relative_costs.append(
+        float(node_total_cost) / graph_info.cost)
+    for s in node.Node().Successors():
+      if not graph._node_filter(s):
+        continue
+      node_info = graph.NodeInfo(s)
+      successor_bag = self._sack.AddNode(graph, node_info)
+      self.AddSuccessor(successor_bag)
+      self._successor_sources[successor_bag].add((graph, node, s))
+      self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
+
+  @classmethod
+  def _MakeShortname(cls, url):
+    parsed = urlparse.urlparse(url)
+    if parsed.scheme == 'data':
+      kind, _ = parsed.path.split(';', 1)
+      return 'data:' + kind
+    path = parsed.path[:10]
+    hostname = parsed.hostname if parsed.hostname else '?.?.?'
+    return hostname + '/' + path
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
new file mode 100644
index 0000000..417918d
--- /dev/null
+++ b/loading/resource_sack_unittest.py
@@ -0,0 +1,64 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import resource_sack
+from test_utils import (MakeRequest,
+                        TestResourceGraph)
+
+
+class ResourceSackTestCase(unittest.TestCase):
+  def test_NodeMerge(self):
+    g1 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(3, 1)])
+    g2 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(4, 2)])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    sack.ConsumeGraph(g2)
+    self.assertEqual(5, len(sack.bags))
+    for bag in sack.bags:
+      if bag.label not in ('3/', '4/'):
+        self.assertEqual(2, bag.num_nodes)
+      else:
+        self.assertEqual(1, bag.num_nodes)
+
+  def test_MultiParents(self):
+    g1 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(2, 0)])
+    g2 = TestResourceGraph.FromRequestList([
+        MakeRequest(1, 'null'),
+        MakeRequest(2, 1)])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    sack.ConsumeGraph(g2)
+    self.assertEqual(3, len(sack.bags))
+    labels = {bag.label: bag for bag in sack.bags}
+    self.assertEqual(
+        set(['0/', '1/']),
+        set([bag.label for bag in labels['2/'].Predecessors()]))
+    self.assertFalse(labels['0/'].Predecessors())
+    self.assertFalse(labels['1/'].Predecessors())
+
+  def test_Shortname(self):
+    root = MakeRequest(0, 'null')
+    shortname = MakeRequest(1, 0)
+    shortname.url = 'data:fake/content;' + 'lotsand' * 50 + 'lotsofdata'
+    g1 = TestResourceGraph.FromRequestList([root, shortname])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    self.assertEqual(set(['0/', 'data:fake/content']),
+                     set([bag.label for bag in sack.bags]))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index aea73a7..2d80d1a 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -43,8 +43,39 @@ class FakePageTrack(devtools_monitor.Track):
 
 
 def MakeRequest(
-    url, source_url, start_time, headers_time, end_time,
+    url, source_url, start_time=None, headers_time=None, end_time=None,
     magic_content_type=False, initiator_type='other'):
+  """Make a dependent request.
+
+  Args:
+    url: a url, or number which will be used as a url.
+    source_url: a url or number which will be used as the source (initiating)
+      url. If the source url is not present, then url will be a root. The
+      convention in tests is to use a source_url of 'null' in this case.
+    start_time: The request start time in milliseconds. If None, this is set to
+      the current request id in seconds. If None, the two other time parameters
+      below must also be None.
+    headers_time: The timestamp when resource headers were received, or None.
+    end_time: The timestamp when the resource was finished, or None.
+    magic_content_type (bool): if true, set a magic content type that makes url
+      always be detected as a valid source and destination request.
+    initiator_type: the initiator type to use.
+
+  Returns:
+    A request_track.Request.
+
+  """
+  assert ((start_time is None and
+           headers_time is None and
+           end_time is None) or
+          (start_time is not None and
+           headers_time is not None and
+           end_time is not None)), \
+      'Need no time specified or all times specified'
+  if start_time is None:
+    # Use the request id in seconds for timestamps. This guarantees increasing
+    # times which makes request dependencies behave as expected.
+    start_time = headers_time = end_time = MakeRequest._next_request_id * 1000
   assert initiator_type in ('other', 'parser')
   timing = request_track.TimingAsList(request_track.TimingFromDict({
       # connectEnd should be ignored.

commit d8911ad8ff299f4dfb8a8865d4f5240ed13f3d1c
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 22 08:04:58 2016 -0800

    tools/android/loading: Implements loading_trace_analyzer.py requests
    
    Command line tool design to quickly analyse a loading trace to
    debug the upcoming cache modification tool.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1708253002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376737}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6ba6f666b3c485e56d4e005d59734c7ab5a64ff2

diff --git a/loading/loading_trace_analyzer.py b/loading/loading_trace_analyzer.py
new file mode 100755
index 0000000..e703360
--- /dev/null
+++ b/loading/loading_trace_analyzer.py
@@ -0,0 +1,83 @@
+#! /usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import json
+import re
+import sys
+
+from loading_trace import LoadingTrace
+import request_track
+
+
+def _ArgumentParser():
+  """Builds a command line argument's parser.
+  """
+  parser = argparse.ArgumentParser()
+  subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
+
+  # requests listing subcommand.
+  requests_parser = subparsers.add_parser('requests',
+      help='Lists all request from the loading trace.')
+  requests_parser.add_argument('loading_trace', type=file,
+      help='Input loading trace to see the cache usage from.')
+  requests_parser.add_argument('--output',
+      type=argparse.FileType(),
+      default=sys.stdout,
+      help='Output destination path if different from stdout.')
+  requests_parser.add_argument('--output-format', type=str, default='{url}',
+      help='Output line format (Default to "{url}")')
+  requests_parser.add_argument('--where',
+      dest='where_statement', type=str,
+      nargs=2, metavar=('FORMAT', 'REGEX'), default=[],
+      help='Where statement to filter such as: --where "{protocol}" "https?"')
+  return parser
+
+
+def _RequestsSubcommand(args):
+  """`loading_trace_analyzer.py requests` Command line tool entry point.
+
+  Example:
+    Lists all request with timing:
+      ... requests --output-format "{timing} {url}"
+
+    Lists  HTTP/HTTPS requests that have used the cache:
+      ... requests --where "{protocol} {from_disk_cache}" "https?\S* True"
+  """
+  where_format = None
+  where_statement = None
+  if args.where_statement:
+    where_format = args.where_statement[0]
+    try:
+      where_statement = re.compile(args.where_statement[1])
+    except re.error as e:
+      sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
+          args.where_statement[1], str(e)))
+      return 1
+
+  loading_trace = LoadingTrace.FromJsonDict(json.load(args.loading_trace))
+  for request_event in loading_trace.request_track.GetEvents():
+    request_event_json = request_event.ToJsonDict()
+
+    if where_statement != None:
+      where_in = where_format.format(**request_event_json)
+      if not where_statement.match(where_in):
+        continue
+
+    args.output.write(args.output_format.format(**request_event_json) + '\n')
+  return 0
+
+
+def main(command_line_args):
+  """Command line tool entry point.
+  """
+  args = _ArgumentParser().parse_args(command_line_args)
+  if args.subcommand == 'requests':
+    return _RequestsSubcommand(args)
+  assert False
+
+
+if __name__ == '__main__':
+  sys.exit(main(sys.argv[1:]))

commit 069855b8e84e9c475934ebb855c3e02bcd1ea6fa
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 22 07:36:22 2016 -0800

    sandwich: Implements network condition on WPR server and browser.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1707363002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376731}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 45605a80083f18230d5359890f6b8735310b25af

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
index a1f6849..9bf2908 100644
--- a/loading/chrome_setup.py
+++ b/loading/chrome_setup.py
@@ -17,9 +17,10 @@ from options import OPTIONS
 
 # Copied from
 # WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
-_NETWORK_CONDITIONS = {
-    'Offline': {
-        'download': 0 * 1024 / 8, 'upload': 0 * 1024 / 8, 'latency': 0},
+# Units:
+#   download/upload: byte/s
+#   latency: ms
+NETWORK_CONDITIONS = {
     'GPRS': {
         'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
     'Regular 2G': {
@@ -43,6 +44,23 @@ _NETWORK_CONDITIONS = {
 }
 
 
+def BandwidthToString(bandwidth):
+  """Converts a bandwidth to string.
+
+  Args:
+    bandwidth: The bandwidth to convert in byte/s. Must be a multiple of 1024/8.
+
+  Returns:
+    A string compatible with wpr --{up,down} command line flags.
+  """
+  assert type(bandwidth) == int
+  assert bandwidth % (1024/8) == 0
+  bandwidth_kbps = (bandwidth * 8) / 1024
+  if bandwidth_kbps % 1024:
+    return '{}Kbit/s'.format(bandwidth_kbps)
+  return '{}Mbit/s'.format(bandwidth_kbps / 1024)
+
+
 @contextlib.contextmanager
 def DevToolsConnectionForLocalBinary(flags):
   """Returns a DevToolsConnection context manager for a local binary.
@@ -80,7 +98,7 @@ def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
     connection: (DevToolsConnection)
     emulated_device_name: (str) Key in the dict returned by
                           _LoadEmulatedDevices().
-    emulated_network_name: (str) Key in _NETWORK_CONDITIONS.
+    emulated_network_name: (str) Key in NETWORK_CONDITIONS.
 
   Returns:
     A metadata dict {'deviceEmulation': params, 'networkEmulation': params}.
@@ -93,7 +111,7 @@ def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
         connection, emulated_device)
     result['deviceEmulation'] = emulation_params
   if emulated_network_name:
-    params = _NETWORK_CONDITIONS[emulated_network_name]
+    params = NETWORK_CONDITIONS[emulated_network_name]
     _SetUpNetworkEmulation(
         connection, params['latency'], params['download'], params['upload'])
     result['networkEmulation'] = params
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 0fe95fd..62b99b4 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -105,12 +105,15 @@ def _SetUpDevice(device, package_info):
 
 @contextlib.contextmanager
 def WprHost(device, wpr_archive_path, record=False,
+            network_condition_name=None,
             disable_script_injection=False):
   """Launches web page replay host.
 
   Args:
     device: Android device.
     wpr_archive_path: host sided WPR archive's path.
+    network_condition_name: Network condition name available in
+        chrome_setup.NETWORK_CONDITIONS.
     record: Enables or disables WPR archive recording.
 
   Returns:
@@ -119,6 +122,9 @@ def WprHost(device, wpr_archive_path, record=False,
   """
   assert device
   if wpr_archive_path == None:
+    assert not record, 'WPR cannot record without a specified archive.'
+    assert not network_condition_name, ('WPR cannot emulate network condition' +
+                                        ' without a specified archive.')
     yield []
     return
 
@@ -129,6 +135,16 @@ def WprHost(device, wpr_archive_path, record=False,
       os.remove(wpr_archive_path)
   else:
     assert os.path.exists(wpr_archive_path)
+  if network_condition_name:
+    condition = chrome_setup.NETWORK_CONDITIONS[network_condition_name]
+    if record:
+      logging.warning('WPR network condition is ignored when recording.')
+    else:
+      wpr_server_args.extend([
+          '--down', chrome_setup.BandwidthToString(condition['download']),
+          '--up', chrome_setup.BandwidthToString(condition['upload']),
+          '--delay_ms', str(condition['latency']),
+          '--shaping_type', 'proxy'])
 
   if disable_script_injection:
     # Remove default WPR injected scripts like deterministic.js which
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 68c785f..3ea8eea 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -20,6 +20,9 @@ from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
 
+DEFAULT_TIMEOUT = 10 # seconds
+
+
 class DevToolsConnectionException(Exception):
   def __init__(self, message):
     super(DevToolsConnectionException, self).__init__(message)
@@ -217,20 +220,21 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self._set_up = True
 
-  def StartMonitoring(self):
+  def StartMonitoring(self, timeout=DEFAULT_TIMEOUT):
     """Starts monitoring.
 
     DevToolsConnection.SetUpMonitoring() has to be called first.
     """
     assert self._set_up, 'DevToolsConnection.SetUpMonitoring not called.'
-    self._Dispatch()
+    self._Dispatch(timeout=timeout)
     self._TearDownMonitoring()
 
   def StopMonitoring(self):
     """Stops the monitoring."""
     self._please_stop = True
 
-  def _Dispatch(self, kind='Monitoring', timeout=10):
+  def _Dispatch(self, kind='Monitoring',
+                timeout=DEFAULT_TIMEOUT):
     self._please_stop = False
     while not self._please_stop:
       try:
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index def4214..0572d5b 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -33,6 +33,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 import devil_chromium
 
+import chrome_setup
 import device_setup
 import devtools_monitor
 import options
@@ -65,6 +66,10 @@ _TIME_TO_DEVICE_IDLE_SECONDS = 2
 def _RemoteCacheDirectory():
   return '/data/data/{}/cache/Cache'.format(OPTIONS.chrome_package_name)
 
+# Devtools timeout of 1 minute to avoid websocket timeout on slow
+# network condition.
+_DEVTOOLS_TIMEOUT = 60
+
 
 def _ReadUrlsFromJobDescription(job_name):
   """Retrieves the list of URLs associated with the job name."""
@@ -258,14 +263,9 @@ def _CleanPreviousTraces(output_directories_path):
     shutil.rmtree(directory_path)
 
 
-def main():
-  logging.basicConfig(level=logging.INFO)
-  devil_chromium.Initialize()
-
-  # Don't give the argument yet. All we are interested in for now is accessing
-  # the default values of OPTIONS.
-  OPTIONS.ParseArgs([])
-
+def _ArgumentParser():
+  """Build a command line argument's parser.
+  """
   parser = argparse.ArgumentParser()
   parser.add_argument('--job', required=True,
                       help='JSON file with job description.')
@@ -287,7 +287,25 @@ def main():
                       help='Disable WPR default script injection such as ' +
                           'overriding javascript\'s Math.random() and Date() ' +
                           'with deterministic implementations.')
-  args = parser.parse_args()
+  parser.add_argument('--network-condition', default=None,
+      choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
+      help='Set a network profile.')
+  parser.add_argument('--network-emulator', default='browser',
+      choices=['browser', 'wpr'],
+      help='Set which component is emulating the network condition.' +
+          ' (Default to browser)')
+  return parser
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+  devil_chromium.Initialize()
+
+  # Don't give the argument yet. All we are interested in for now is accessing
+  # the default values of OPTIONS.
+  OPTIONS.ParseArgs([])
+
+  args = _ArgumentParser().parse_args()
 
   if not os.path.isdir(args.output):
     try:
@@ -307,22 +325,41 @@ def main():
   device = device_utils.DeviceUtils.HealthyDevices()[0]
   local_cache_archive_path = os.path.join(args.output, 'cache.zip')
   local_cache_directory_path = None
+  wpr_network_condition_name = None
+  browser_network_condition_name = None
+  if args.network_emulator == 'wpr':
+    wpr_network_condition_name = args.network_condition
+  elif args.network_emulator == 'browser':
+    browser_network_condition_name = args.network_condition
+  else:
+    assert False
 
   if args.cache_op == 'push':
     assert os.path.isfile(local_cache_archive_path)
     local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
     _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
 
-  with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
-      args.disable_wpr_script_injection) as additional_flags:
+  with device_setup.WprHost(device, args.wpr_archive,
+      record=args.wpr_record,
+      network_condition_name=wpr_network_condition_name,
+      disable_script_injection=args.disable_wpr_script_injection
+      ) as additional_flags:
     def _RunNavigation(url, clear_cache, trace_id):
       with device_setup.DeviceConnection(
           device=device,
           additional_flags=additional_flags) as connection:
+        additional_metadata = {}
+        if browser_network_condition_name:
+          additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
+              connection=connection,
+              emulated_device_name=None,
+              emulated_network_name=browser_network_condition_name)
         loading_trace = trace_recorder.MonitorUrl(
             connection, url,
             clear_cache=clear_cache,
-            categories=pull_sandwich_metrics.CATEGORIES)
+            categories=pull_sandwich_metrics.CATEGORIES,
+            timeout=_DEVTOOLS_TIMEOUT)
+        loading_trace.metadata.update(additional_metadata)
         if trace_id != None:
           loading_trace_path = os.path.join(
               args.output, str(trace_id), 'trace.json')
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 0da057f..de59b35 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -31,14 +31,16 @@ import tracing
 
 
 def MonitorUrl(connection, url, clear_cache=False,
-               categories=tracing.DEFAULT_CATEGORIES):
+               categories=tracing.DEFAULT_CATEGORIES,
+               timeout=devtools_monitor.DEFAULT_TIMEOUT):
   """Monitor a URL via a trace recorder.
 
   Args:
-    connection: A device_monitor.DevToolsConnection instance.
+    connection: A devtools_monitor.DevToolsConnection instance.
     url: url to navigate to as string.
     clear_cache: boolean indicating if cache should be cleared before loading.
     categories: List of tracing event categories to record.
+    timeout: Websocket timeout.
 
   Returns:
     loading_trace.LoadingTrace.
@@ -50,7 +52,7 @@ def MonitorUrl(connection, url, clear_cache=False,
   if clear_cache:
     connection.ClearCache()
   connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-  connection.StartMonitoring()
+  connection.StartMonitoring(timeout=timeout)
   metadata = {'date': datetime.datetime.utcnow().isoformat(),
               'seconds_since_epoch': time.time()}
   return loading_trace.LoadingTrace(url, metadata, page, request, trace)

commit 45d313bd001d00996e20c1b688590a143f7240de
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 22 07:10:49 2016 -0800

    sandwich: Refactor to use more existing code.
    
    Also fixes a failures caused by 23bd22480abd2209f258d1dd8d3a8572ee17954a:
    "Refactor options in analyze.py to global structure"
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1707793002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376728}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 51f679843972a6f6592d871fea887b6435b7fe75

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 720fa1d..d383857 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -4,10 +4,12 @@
 
 """Represents the trace of a page load."""
 
+import json
 import page_track
 import request_track
 import tracing
 
+
 class LoadingTrace(object):
   """Represents the trace of a page load."""
   _URL_KEY = 'url'
@@ -40,6 +42,12 @@ class LoadingTrace(object):
               self._TRACING_KEY: self.tracing_track.ToJsonDict()}
     return result
 
+  def ToJsonFile(self, json_path):
+    """Save a json file representing this instance."""
+    json_dict = self.ToJsonDict()
+    with open(json_path, 'w') as output_file:
+       json.dump(json_dict, output_file, indent=2)
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     """Returns an instance from a dictionary returned by ToJsonDict()."""
@@ -53,3 +61,9 @@ class LoadingTrace(object):
         json_dict[cls._TRACING_KEY])
     return LoadingTrace(json_dict[cls._URL_KEY], json_dict[cls._METADATA_KEY],
                         page, request, tracing_track)
+
+  @classmethod
+  def FromJsonFile(cls, json_path):
+    """Returns an instance from a json file saved by ToJsonFile()."""
+    with open(json_path) as input_file:
+      return cls.FromJsonDict(json.load(input_file))
diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
index df24f1c..926a579 100755
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -15,6 +15,9 @@ import logging
 import os
 import sys
 
+import loading_trace as loading_trace_module
+import tracing
+
 
 CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
 
@@ -29,41 +32,41 @@ _CSV_FIELD_NAMES = [
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
 
-def _GetBrowserPID(trace):
+def _GetBrowserPID(tracing_track):
   """Get the browser PID from a trace.
 
   Args:
-    trace: The cached trace.
+    tracing_track: The tracing.TracingTrack.
 
   Returns:
     The browser's PID as an integer.
   """
-  for event in trace['traceEvents']:
-    if event['cat'] != '__metadata' or event['name'] != 'process_name':
+  for event in tracing_track.GetEvents():
+    if event.category != '__metadata' or event.name != 'process_name':
       continue
-    if event['args']['name'] == 'Browser':
-      return event['pid']
+    if event.args['name'] == 'Browser':
+      return event.pid
   raise ValueError('couldn\'t find browser\'s PID')
 
 
-def _GetBrowserDumpEvents(trace):
-  """Get the browser memory dump events from a trace.
+def _GetBrowserDumpEvents(tracing_track):
+  """Get the browser memory dump events from a tracing track.
 
   Args:
-    trace: The cached trace.
+    tracing_track: The tracing.TracingTrack.
 
   Returns:
     List of memory dump events.
   """
-  browser_pid = _GetBrowserPID(trace)
+  browser_pid = _GetBrowserPID(tracing_track)
   browser_dumps_events = []
-  for event in trace['traceEvents']:
-    if event['cat'] != 'disabled-by-default-memory-infra':
+  for event in tracing_track.GetEvents():
+    if event.category != 'disabled-by-default-memory-infra':
       continue
-    if event['ph'] != 'v' or event['name'] != 'periodic_interval':
+    if event.type != 'v' or event.name != 'periodic_interval':
       continue
     # Ignore dump events for processes other than the browser process
-    if event['pid'] != browser_pid:
+    if event.pid != browser_pid:
       continue
     browser_dumps_events.append(event)
   if len(browser_dumps_events) == 0:
@@ -71,31 +74,31 @@ def _GetBrowserDumpEvents(trace):
   return browser_dumps_events
 
 
-def _GetWebPageTrackedEvents(trace):
-  """Get the web page's tracked events from a trace.
+def _GetWebPageTrackedEvents(tracing_track):
+  """Get the web page's tracked events from a tracing track.
 
   Args:
-    trace: The cached trace.
+    tracing_track: The tracing.TracingTrack.
 
   Returns:
     Dictionary all tracked events.
   """
   main_frame = None
   tracked_events = {}
-  for event in trace['traceEvents']:
-    if event['cat'] != 'blink.user_timing':
+  for event in tracing_track.GetEvents():
+    if event.category != 'blink.user_timing':
       continue
-    event_name = event['name']
+    event_name = event.name
     # Ignore events until about:blank's unloadEventEnd that give the main
     # frame id.
     if not main_frame:
       if event_name == 'unloadEventEnd':
-        main_frame = event['args']['frame']
+        main_frame = event.args['frame']
         logging.info('found about:blank\'s event \'unloadEventEnd\'')
       continue
     # Ignore sub-frames events. requestStart don't have the frame set but it
     # is fine since tracking the first one after about:blank's unloadEventEnd.
-    if 'frame' in event['args'] and event['args']['frame'] != main_frame:
+    if 'frame' in event.args and event.args['frame'] != main_frame:
       continue
     if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
       logging.info('found url\'s event \'%s\'' % event_name)
@@ -104,32 +107,33 @@ def _GetWebPageTrackedEvents(trace):
   return tracked_events
 
 
-def _PullMetricsFromTrace(trace):
+def _PullMetricsFromLoadingTrace(loading_trace):
   """Pulls all the metrics from a given trace.
 
   Args:
-    trace: The cached trace.
+    loading_trace: loading_trace_module.LoadingTrace.
 
   Returns:
     Dictionary with all _CSV_FIELD_NAMES's field set (except the 'id').
   """
-  browser_dump_events = _GetBrowserDumpEvents(trace)
-  web_page_tracked_events = _GetWebPageTrackedEvents(trace)
+  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
+  web_page_tracked_events = _GetWebPageTrackedEvents(
+      loading_trace.tracing_track)
 
   browser_malloc_sum = 0
   browser_malloc_max = 0
   for dump_event in browser_dump_events:
-    attr = dump_event['args']['dumps']['allocators']['malloc']['attrs']['size']
+    attr = dump_event.args['dumps']['allocators']['malloc']['attrs']['size']
     assert attr['units'] == 'bytes'
     size = int(attr['value'], 16)
     browser_malloc_sum += size
     browser_malloc_max = max(browser_malloc_max, size)
 
   return {
-    'total_load': (web_page_tracked_events['loadEventEnd']['ts'] -
-                   web_page_tracked_events['requestStart']['ts']),
-    'onload': (web_page_tracked_events['loadEventEnd']['ts'] -
-               web_page_tracked_events['loadEventStart']['ts']),
+    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
+                   web_page_tracked_events['requestStart'].start_msec),
+    'onload': (web_page_tracked_events['loadEventEnd'].start_msec -
+               web_page_tracked_events['loadEventStart'].start_msec),
     'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
     'browser_malloc_max': browser_malloc_max
   }
@@ -162,12 +166,11 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
     if not os.path.isfile(trace_path):
       continue
     logging.info('processing \'%s\'' % trace_path)
-    with open(trace_path) as trace_file:
-      trace = json.load(trace_file)
-      trace_metrics = _PullMetricsFromTrace(trace)
-      trace_metrics['id'] = page_id
-      trace_metrics['url'] = run_infos['urls'][page_id]
-      metrics.append(trace_metrics)
+    loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
+    trace_metrics = _PullMetricsFromLoadingTrace(loading_trace)
+    trace_metrics['id'] = page_id
+    trace_metrics['url'] = run_infos['urls'][page_id]
+    metrics.append(trace_metrics)
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
   return metrics
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
index 3c98508..518607c 100644
--- a/loading/pull_sandwich_metrics_unittest.py
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -10,7 +10,12 @@ import subprocess
 import tempfile
 import unittest
 
+import loading_trace
+import page_track
 import pull_sandwich_metrics as puller
+import request_track
+import tracing
+
 
 _BLINK_CAT = 'blink.user_timing'
 _MEM_CAT = 'disabled-by-default-memory-infra'
@@ -19,40 +24,56 @@ _LOADS='loadEventStart'
 _LOADE='loadEventEnd'
 _UNLOAD='unloadEventEnd'
 
-_MINIMALIST_TRACE = {'traceEvents': [
-    {'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10, 'args': {'frame': '0'}},
-    {'cat': _BLINK_CAT, 'name': _START,  'ts': 20, 'args': {},           },
+_MINIMALIST_TRACE_EVENTS = [
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10000,
+        'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _START,  'ts': 20000,
+        'args': {}},
     {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
-        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
             'units': 'bytes', 'value': '1af2', }}}}}}},
-    {'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35, 'args': {'frame': '0'}},
-    {'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40, 'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35000,
+        'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40000,
+        'args': {'frame': '0'}},
     {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
-        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
             'units': 'bytes', 'value': 'd704', }}}}}}},
-    {'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'args': {
-        'name': 'Browser'}}]}
+    {'ph': 'M', 'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'ts': 1,
+        'args': {'name': 'Browser'}}]
+
+
+def TracingTrack(events):
+  return tracing.TracingTrack.FromJsonDict({'events': events})
+
+
+def LoadingTrace(events):
+  return loading_trace.LoadingTrace('http://a.com/', {},
+                                    page_track.PageTrack(None),
+                                    request_track.RequestTrack(None),
+                                    TracingTrack(events))
 
 
 class PageTrackTest(unittest.TestCase):
   def testGetBrowserPID(self):
-    def RunHelper(expected, trace):
-      self.assertEquals(expected, puller._GetBrowserPID(trace))
-
-    RunHelper(123, {'traceEvents': [
-        {'pid': 354, 'cat': 'whatever0'},
-        {'pid': 354, 'cat': 'whatever1'},
-        {'pid': 354, 'cat': '__metadata', 'name': 'thread_name'},
-        {'pid': 354, 'cat': '__metadata', 'name': 'process_name', 'args': {
-            'name': 'Renderer'}},
-        {'pid': 123, 'cat': '__metadata', 'name': 'process_name', 'args': {
-            'name': 'Browser'}},
-        {'pid': 354, 'cat': 'whatever0'}]})
+    def RunHelper(expected, events):
+      self.assertEquals(expected, puller._GetBrowserPID(TracingTrack(events)))
+
+    RunHelper(123, [
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
+            'name': 'thread_name'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
+            'name': 'process_name', 'args': {'name': 'Renderer'}},
+        {'ph': 'M', 'ts': 0, 'pid': 123, 'cat': '__metadata',
+            'name': 'process_name', 'args': {'name': 'Browser'}},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'}])
 
     with self.assertRaises(ValueError):
-      RunHelper(123, {'traceEvents': [
-          {'pid': 354, 'cat': 'whatever0'},
-          {'pid': 354, 'cat': 'whatever1'}]})
+      RunHelper(123, [
+          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
+          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'}])
 
   def testGetBrowserDumpEvents(self):
     NAME = 'periodic_interval'
@@ -63,39 +84,41 @@ class PageTrackTest(unittest.TestCase):
           'pid': browser_pid,
           'cat': '__metadata',
           'name': 'process_name',
+          'ph': 'M',
+          'ts': 0,
           'args': {'name': 'Browser'}})
-      return puller._GetBrowserDumpEvents({'traceEvents': trace_events})
+      return puller._GetBrowserDumpEvents(TracingTrack(trace_events))
 
     TRACE_EVENTS = [
-        {'pid': 354, 'ts':  1, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts':  2, 'cat': _MEM_CAT, 'ph': 'V'},
-        {'pid': 672, 'ts':  3, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts':  4, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
-        {'pid': 123, 'ts':  5, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts':  6, 'cat': _MEM_CAT, 'ph': 'V'},
-        {'pid': 672, 'ts':  7, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts':  8, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
-        {'pid': 123, 'ts':  9, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts': 10, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts': 11, 'cat': 'whatever0'},
-        {'pid': 672, 'ts': 12, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
+        {'pid': 354, 'ts':  1000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  2000, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  3000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  4000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  5000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  6000, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  7000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  8000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  9000, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts': 10000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
+        {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
     self.assertTrue(_MEM_CAT in puller.CATEGORIES)
 
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
-    self.assertEquals(5, bump_events[0]['ts'])
-    self.assertEquals(10, bump_events[1]['ts'])
+    self.assertEquals(5, bump_events[0].start_msec)
+    self.assertEquals(10, bump_events[1].start_msec)
 
     bump_events = RunHelper(TRACE_EVENTS, 354)
     self.assertEquals(1, len(bump_events))
-    self.assertEquals(1, bump_events[0]['ts'])
+    self.assertEquals(1, bump_events[0].start_msec)
 
     bump_events = RunHelper(TRACE_EVENTS, 672)
     self.assertEquals(3, len(bump_events))
-    self.assertEquals(3, bump_events[0]['ts'])
-    self.assertEquals(7, bump_events[1]['ts'])
-    self.assertEquals(12, bump_events[2]['ts'])
+    self.assertEquals(3, bump_events[0].start_msec)
+    self.assertEquals(7, bump_events[1].start_msec)
+    self.assertEquals(12, bump_events[2].start_msec)
 
     with self.assertRaises(ValueError):
       RunHelper(TRACE_EVENTS, 895)
@@ -103,41 +126,68 @@ class PageTrackTest(unittest.TestCase):
   def testGetWebPageTrackedEvents(self):
     self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
 
-    trace_events = puller._GetWebPageTrackedEvents({'traceEvents': [
-        {'ts':  0, 'args': {},             'cat': 'whatever', 'name': _START},
-        {'ts':  1, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
-        {'ts':  2, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
-        {'ts':  3, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts':  4, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts':  5, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts':  6, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _UNLOAD},
-        {'ts':  7, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts':  8, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts':  9, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts': 10, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _UNLOAD},
-        {'ts': 11, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _START},
-        {'ts': 12, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
-        {'ts': 13, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
-        {'ts': 14, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts': 15, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts': 16, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts': 17, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts': 18, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts': 19, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts': 20, 'args': {},             'cat': 'whatever', 'name': _START},
-        {'ts': 21, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
-        {'ts': 22, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
-        {'ts': 23, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts': 24, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts': 25, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE}]})
+    trace_events = puller._GetWebPageTrackedEvents(TracingTrack([
+        {'ph': 'R', 'ts':  0000, 'args': {},             'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts':  1000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  2000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts':  3000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts':  4000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  5000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts':  6000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _UNLOAD},
+        {'ph': 'R', 'ts':  7000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts':  8000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  9000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _UNLOAD},
+        {'ph': 'R', 'ts': 11000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts': 12000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 13000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 14000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 15000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 16000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 17000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 18000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 19000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 20000, 'args': {},             'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts': 21000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 22000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 23000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 24000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 25000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE}]))
 
     self.assertEquals(3, len(trace_events))
-    self.assertEquals(14, trace_events['requestStart']['ts'])
-    self.assertEquals(17, trace_events['loadEventStart']['ts'])
-    self.assertEquals(19, trace_events['loadEventEnd']['ts'])
+    self.assertEquals(14, trace_events['requestStart'].start_msec)
+    self.assertEquals(17, trace_events['loadEventStart'].start_msec)
+    self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
 
-  def testPullMetricsFromTrace(self):
-    metrics = puller._PullMetricsFromTrace(_MINIMALIST_TRACE)
+  def testPullMetricsFromLoadingTrace(self):
+    metrics = puller._PullMetricsFromLoadingTrace(LoadingTrace(
+        _MINIMALIST_TRACE_EVENTS))
     self.assertEquals(4, len(metrics))
     self.assertEquals(20, metrics['total_load'])
     self.assertEquals(5, metrics['onload'])
@@ -150,8 +200,8 @@ class PageTrackTest(unittest.TestCase):
       json.dump({'urls': ['a.com', 'b.com', 'c.org']}, out_file)
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
-      with open(os.path.join(tmp_dir, dirname, 'trace.json'), 'w') as out_file:
-        json.dump(_MINIMALIST_TRACE, out_file)
+      LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
+          os.path.join(tmp_dir, dirname, 'trace.json'))
 
     process = subprocess.Popen(['python', puller.__file__, tmp_dir])
     process.wait()
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index ef7fca8..def4214 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -35,12 +35,16 @@ import devil_chromium
 
 import device_setup
 import devtools_monitor
-import json
+import options
 import page_track
 import pull_sandwich_metrics
+import trace_recorder
 import tracing
 
 
+# Use options layer to access constants.
+OPTIONS = options.OPTIONS
+
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
 # Directory name under --output to save the cache from the device.
@@ -53,16 +57,13 @@ _INDEX_DIRECTORY_NAME = 'index-dir'
 # in the cache directory under _INDEX_DIRECTORY_NAME.
 _REAL_INDEX_FILE_NAME = 'the-real-index'
 
-# Name of the chrome package.
-_CHROME_PACKAGE = (
-    constants.PACKAGE_INFO[device_setup.DEFAULT_CHROME_PACKAGE].package)
-
 # An estimate of time to wait for the device to become idle after expensive
 # operations, such as opening the launcher activity.
 _TIME_TO_DEVICE_IDLE_SECONDS = 2
 
-# Cache directory's path on the device.
-_REMOTE_CACHE_DIRECTORY = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
+
+def _RemoteCacheDirectory():
+  return '/data/data/{}/cache/Cache'.format(OPTIONS.chrome_package_name)
 
 
 def _ReadUrlsFromJobDescription(job_name):
@@ -86,23 +87,6 @@ def _ReadUrlsFromJobDescription(job_name):
   raise Exception('Job description does not define a list named "urls"')
 
 
-def _SaveChromeTrace(events, target_directory):
-  """Saves the trace events, ignores IO errors.
-
-  Args:
-    events: a dict as returned by TracingTrack.ToJsonDict()
-    target_directory: Directory path where trace is created.
-  """
-  filename = os.path.join(target_directory, 'trace.json')
-  try:
-    os.makedirs(target_directory)
-    with open(filename, 'w') as f:
-      json.dump({'traceEvents': events['events'], 'metadata': {}}, f, indent=2)
-  except IOError:
-    logging.warning('Could not save a trace: %s' % filename)
-    # Swallow the exception.
-
-
 def _UpdateTimestampFromAdbStat(filename, stat):
   os.utime(filename, (stat.st_time, stat.st_time))
 
@@ -128,13 +112,13 @@ def _PullBrowserCache(device):
     Temporary directory containing all the browser cache.
   """
   save_target = tempfile.mkdtemp(suffix='.cache')
-  for filename, stat in device.adb.Ls(_REMOTE_CACHE_DIRECTORY):
+  for filename, stat in device.adb.Ls(_RemoteCacheDirectory()):
     if filename == '..':
       continue
     if filename == '.':
       cache_directory_stat = stat
       continue
-    original_file = os.path.join(_REMOTE_CACHE_DIRECTORY, filename)
+    original_file = os.path.join(_RemoteCacheDirectory(), filename)
     saved_file = os.path.join(save_target, filename)
     device.adb.Pull(original_file, saved_file)
     _UpdateTimestampFromAdbStat(saved_file, stat)
@@ -166,16 +150,16 @@ def _PushBrowserCache(device, local_cache_path):
     local_cache_path: The directory's path containing the cache locally.
   """
   # Clear previous cache.
-  _AdbShell(device.adb, ['rm', '-rf', _REMOTE_CACHE_DIRECTORY])
-  _AdbShell(device.adb, ['mkdir', _REMOTE_CACHE_DIRECTORY])
+  _AdbShell(device.adb, ['rm', '-rf', _RemoteCacheDirectory()])
+  _AdbShell(device.adb, ['mkdir', _RemoteCacheDirectory()])
 
   # Push cache content.
-  device.adb.Push(local_cache_path, _REMOTE_CACHE_DIRECTORY)
+  device.adb.Push(local_cache_path, _RemoteCacheDirectory())
 
   # Walk through the local cache to update mtime on the device.
   def MirrorMtime(local_path):
     cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
-    remote_path = os.path.join(_REMOTE_CACHE_DIRECTORY, cache_relative_path)
+    remote_path = os.path.join(_RemoteCacheDirectory(), cache_relative_path)
     _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
 
   for local_directory_path, dirnames, filenames in os.walk(
@@ -278,6 +262,10 @@ def main():
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
 
+  # Don't give the argument yet. All we are interested in for now is accessing
+  # the default values of OPTIONS.
+  OPTIONS.ParseArgs([])
+
   parser = argparse.ArgumentParser()
   parser.add_argument('--job', required=True,
                       help='JSON file with job description.')
@@ -331,17 +319,15 @@ def main():
       with device_setup.DeviceConnection(
           device=device,
           additional_flags=additional_flags) as connection:
-        if clear_cache:
-          connection.ClearCache()
-        page_track.PageTrack(connection)
-        tracing_track = tracing.TracingTrack(connection,
+        loading_trace = trace_recorder.MonitorUrl(
+            connection, url,
+            clear_cache=clear_cache,
             categories=pull_sandwich_metrics.CATEGORIES)
-        connection.SetUpMonitoring()
-        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-        connection.StartMonitoring()
         if trace_id != None:
-          trace_target_directory = os.path.join(args.output, str(trace_id))
-          _SaveChromeTrace(tracing_track.ToJsonDict(), trace_target_directory)
+          loading_trace_path = os.path.join(
+              args.output, str(trace_id), 'trace.json')
+          os.makedirs(os.path.dirname(loading_trace_path))
+          loading_trace.ToJsonFile(loading_trace_path)
 
     for _ in xrange(args.repeat):
       for url in job_urls:
@@ -349,7 +335,7 @@ def main():
         if args.cache_op == 'clear':
           clear_cache = True
         elif args.cache_op == 'push':
-          device.KillAll(_CHROME_PACKAGE, quiet=True)
+          device.KillAll(OPTIONS.chrome_package_name, quiet=True)
           _PushBrowserCache(device, local_cache_directory_path)
         elif args.cache_op == 'reload':
           _RunNavigation(url, clear_cache=True, trace_id=None)
@@ -366,7 +352,7 @@ def main():
     # Move Chrome to background to allow it to flush the index.
     device.adb.Shell('am start com.google.android.launcher')
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    device.KillAll(_CHROME_PACKAGE, quiet=True)
+    device.KillAll(OPTIONS.chrome_package_name, quiet=True)
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
 
     cache_directory_path = _PullBrowserCache(device)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 34293f8..0da057f 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -30,20 +30,22 @@ import request_track
 import tracing
 
 
-def MonitorUrl(connection, url, clear_cache=False):
+def MonitorUrl(connection, url, clear_cache=False,
+               categories=tracing.DEFAULT_CATEGORIES):
   """Monitor a URL via a trace recorder.
 
   Args:
     connection: A device_monitor.DevToolsConnection instance.
     url: url to navigate to as string.
     clear_cache: boolean indicating if cache should be cleared before loading.
+    categories: List of tracing event categories to record.
 
   Returns:
     loading_trace.LoadingTrace.
   """
   page = page_track.PageTrack(connection)
   request = request_track.RequestTrack(connection)
-  trace = tracing.TracingTrack(connection)
+  trace = tracing.TracingTrack(connection, categories=categories)
   connection.SetUpMonitoring()
   if clear_cache:
     connection.ClearCache()
diff --git a/loading/tracing.py b/loading/tracing.py
index cf79908..54ff3d0 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -12,12 +12,17 @@ import operator
 import devtools_monitor
 
 
+DEFAULT_CATEGORIES = None
+
+
 class TracingTrack(devtools_monitor.Track):
   """Grabs and processes trace event messages.
 
   See https://goo.gl/Qabkqk for details on the protocol.
   """
-  def __init__(self, connection, categories=None, fetch_stream=False):
+  def __init__(self, connection,
+               categories=DEFAULT_CATEGORIES,
+               fetch_stream=False):
     """Initialize this TracingTrack.
 
     Args:
@@ -304,6 +309,14 @@ class Event(object):
     return self._tracing_event['ph']
 
   @property
+  def category(self):
+    return self._tracing_event['cat']
+
+  @property
+  def pid(self):
+    return self._tracing_event['pid']
+
+  @property
   def args(self):
     return self._tracing_event.get('args', {})
 

commit 7fbfcd6ccf901a3267e0e82a2fa212984ffa633a
Author: droger <droger@chromium.org>
Date:   Mon Feb 22 01:06:14 2016 -0800

    tools/android/loading Add support for async stacks in webserver tests
    
    Review URL: https://codereview.chromium.org/1713063002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376705}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5133c8214f810865c65300ddc6ea2260895cdc2b

diff --git a/loading/trace_test/results/3.result b/loading/trace_test/results/3.result
index f909cae..bcebf63 100644
--- a/loading/trace_test/results/3.result
+++ b/loading/trace_test/results/3.result
@@ -1,6 +1,6 @@
 parser (no stack) 3a.js
 parser (no stack) 3c.js
-script () 3a.jpg
-script () 3b.jpg
-script () 3c.jpg
+script (3a.js:10/3a.js:14/3.html:23) 3a.jpg
 script (3a.js:20) 3b.js
+script (3b.js:9) 3b.jpg
+script (3c.js:7/3.html:24) 3c.jpg
diff --git a/loading/trace_test/tests/3.html b/loading/trace_test/tests/3.html
index 03bbce6..f6850eb 100644
--- a/loading/trace_test/tests/3.html
+++ b/loading/trace_test/tests/3.html
@@ -10,9 +10,6 @@
 
   Note that as 3b.js adds a tag to the body, it is executed only after
   the body has been parsed. No, I don't know how that works either.
-
-  At any rate, only 3c.js has a meaningful stack trace in the
-  initiator. The images have script initiators with empty stacks.
 -->
 <html>
 <head>
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index 2b413a8..800b8d6 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -148,14 +148,24 @@ class InitiatorSequence(object):
       return
     for rq in trace.request_track.GetEvents():
       if rq.initiator['type'] in ('parser', 'script'):
-        stack = 'no stack'
-        if 'stack' in rq.initiator:
-          stack = '/'.join(
+        stack_string = ''
+        stack = rq.initiator.get('stack')
+        # Iteratively walk the stack and its parents.
+        while stack:
+          current_string = '/'.join(
               ['%s:%s' % (self._ShortUrl(frame['url']), frame['lineNumber'])
-               for frame in rq.initiator['stack']['callFrames']])
+               for frame in stack['callFrames']])
+          if len(current_string) and len(stack_string):
+            stack_string += '/'
+          stack_string += current_string
+          stack = stack.get('parent')
+
+        if stack_string == '':
+          stack_string = 'no stack'
+
         self._seq.append('%s (%s) %s' % (
             rq.initiator['type'],
-            stack,
+            stack_string,
             self._ShortUrl(rq.url)))
     self._seq.sort()
 

commit f0f2e8c39bcb1064205d3d621282be7b4b34fbb5
Author: mattcary <mattcary@chromium.org>
Date:   Fri Feb 19 07:49:12 2016 -0800

    Loading model test tweak.
    
    Extract SimpleLens to test_utils and change how it's plugged into ResourceGraph.
    
    This will make it easier to create ResourceGraphs in other tests.
    
    Review URL: https://codereview.chromium.org/1708223005
    
    Cr-Original-Commit-Position: refs/heads/master@{#376442}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 82369343882999c0829645d6559d4f3180c4f29e

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 5deec03..f9905a0 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -31,6 +31,10 @@ class ResourceGraph(object):
 
   See model parameters in Set().
   """
+  # The lens to build request dependencies. Exposed here for subclasses in
+  # unittesting.
+  REQUEST_LENS = request_dependencies_lens.RequestDependencyLens
+
   EDGE_KIND_KEY = 'edge_kind'
   EDGE_KINDS = request_track.Request.INITIATORS + (
       'script_inferred', 'after-load', 'before-load', 'timing')
@@ -477,8 +481,7 @@ class ResourceGraph(object):
       self._nodes.append(node)
       self._node_info.append(node_info)
 
-    dependencies = request_dependencies_lens.RequestDependencyLens(
-        trace).GetRequestDependencies()
+    dependencies = self.REQUEST_LENS(trace).GetRequestDependencies()
     for dep in dependencies:
       (parent_rq, child_rq, reason) = dep
       parent = self._node_info[index_by_request[parent_rq]]
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 44dcbae..ee5e84d 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -13,36 +13,7 @@ import request_dependencies_lens
 import test_utils
 
 
-class SimpleLens(object):
-  def __init__(self, trace):
-    self._trace = trace
-
-  def GetRequestDependencies(self):
-    url_to_rq = {}
-    deps = []
-    for rq in self._trace.request_track.GetEvents():
-      assert rq.url not in url_to_rq
-      url_to_rq[rq.url] = rq
-    for rq in self._trace.request_track.GetEvents():
-      initiating_url = rq.initiator['url']
-      if initiating_url in url_to_rq:
-        deps.append((url_to_rq[initiating_url], rq, rq.initiator['type']))
-    return deps
-
-
 class LoadingModelTestCase(unittest.TestCase):
-
-  def setUp(self):
-    self.old_lens = request_dependencies_lens.RequestDependencyLens
-    request_dependencies_lens.RequestDependencyLens = SimpleLens
-
-  def tearDown(self):
-    request_dependencies_lens.RequestDependencyLens = self.old_lens
-
-  def MakeGraph(self, requests):
-    return loading_model.ResourceGraph(
-        test_utils.LoadingTraceFromEvents(requests))
-
   def SortedIndicies(self, graph):
     return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
 
@@ -50,7 +21,7 @@ class LoadingModelTestCase(unittest.TestCase):
     return [c.Index() for c in node.SortedSuccessors()]
 
   def test_DictConstruction(self):
-    graph = loading_model.ResourceGraph(
+    graph = test_utils.TestResourceGraph(
         {'request_track': {
             'events': [
                 test_utils.MakeRequest(0, 'null', 100, 100.5, 101).ToJsonDict(),
@@ -77,7 +48,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 test_utils.MakeRequest(4, 3, 127, 127.5, 128),
                 test_utils.MakeRequest(5, 'null', 100, 103, 105),
                 test_utils.MakeRequest(6, 5, 105, 107, 110)]
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
@@ -98,7 +69,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 test_utils.MakeRequest(4, 3, 127, 128, 129),
                 test_utils.MakeRequest(5, 'null', 100, 105, 106),
                 test_utils.MakeRequest(6, 5, 105, 110, 111)]
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     path_list = []
     self.assertEqual(29, graph.Cost(path_list))
     self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
@@ -119,7 +90,7 @@ class LoadingModelTestCase(unittest.TestCase):
                                magic_content_type=True),
         test_utils.MakeRequest(4, 2, 122, 126, 126),
         test_utils.MakeRequest(5, 2, 122, 126, 126)]
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -131,7 +102,7 @@ class LoadingModelTestCase(unittest.TestCase):
     # Change node 1 so it is a parent of 3, which becomes the parent of 2.
     requests[1] = test_utils.MakeRequest(
         1, 0, 110, 111, 111, magic_content_type=True)
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -144,12 +115,12 @@ class LoadingModelTestCase(unittest.TestCase):
     requests[1] = test_utils.MakeRequest(
         1, 0, 110, 111, 111, magic_content_type=True)
     requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     # Check it doesn't change until we change the content type of 6.
     self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
     requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
                                          magic_content_type=True)
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -169,7 +140,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 test_utils.MakeRequest(5, 2, 122, 126, 126)]
     for r in requests:
       r.response_headers['Content-Type'] = 'image/gif'
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 654e94b..aea73a7 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -5,6 +5,7 @@
 """Common utilities used in unit tests, within this directory."""
 
 import devtools_monitor
+import loading_model
 import loading_trace
 import page_track
 import request_track
@@ -80,3 +81,33 @@ def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
     tracing_track = None
   return loading_trace.LoadingTrace(
       None, None, page_event_track, request, tracing_track)
+
+
+class SimpleLens(object):
+  """A simple replacement for RequestDependencyLens.
+
+  Uses only the initiator url of a request for determining a dependency.
+  """
+  def __init__(self, trace):
+    self._trace = trace
+
+  def GetRequestDependencies(self):
+    url_to_rq = {}
+    deps = []
+    for rq in self._trace.request_track.GetEvents():
+      assert rq.url not in url_to_rq
+      url_to_rq[rq.url] = rq
+    for rq in self._trace.request_track.GetEvents():
+      initiating_url = rq.initiator['url']
+      if initiating_url in url_to_rq:
+        deps.append((url_to_rq[initiating_url], rq, rq.initiator['type']))
+    return deps
+
+
+class TestResourceGraph(loading_model.ResourceGraph):
+  """Replace the default request lens in a ResourceGraph with our SimpleLens."""
+  REQUEST_LENS = SimpleLens
+
+  @classmethod
+  def FromRequestList(cls, requests, page_events=None, trace_events=None):
+    return cls(LoadingTraceFromEvents(requests, page_events, trace_events))

commit f69df4b4519552ac5bb60ba2af99627b4394d4ec
Author: mattcary <mattcary@chromium.org>
Date:   Fri Feb 19 03:41:15 2016 -0800

    Improve trace integration test documentation.
    
    Review URL: https://codereview.chromium.org/1706323003
    
    Cr-Original-Commit-Position: refs/heads/master@{#376432}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: efbeed8cb1c0ec676ae81a3a678d0ffd82d70a09

diff --git a/loading/trace_test/README.md b/loading/trace_test/README.md
new file mode 100644
index 0000000..c16e472
--- /dev/null
+++ b/loading/trace_test/README.md
@@ -0,0 +1,8 @@
+Trace Integration Tests
+
+This directory defines integration tests which verify traces in various corners
+of the HTML/JS/CSS world.
+
+The unittests in this directory are run as part of
+tools/android/loading/run_tests. The integration tests are only run
+manually. See webserver_test.py for details.
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index f6724c0..2b413a8 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -10,11 +10,23 @@ operation it can be run with no arguments (or perhaps --no_sandbox depending on
 how you have chrome set up). When debugging or adding tests, setting
 --failed_trace_dir could be useful.
 
-Spawns a local http server to serve web pages. The trace generated by each
-file in tests/*.html will be compared with the corresponding results/*.result.
+The integration test spawns a local http server to serve web pages. The trace
+generated by each file in tests/*.html will be compared with the corresponding
+results/*.result. Each test should have a detailed comment explaining its
+organization and what the important part of the test result is.
 
 By default this will use a release version of chrome built in this same
 code tree (out/Release/chrome), see --local_binary to override.
+
+See InitiatorSequence for what the integration tests verify. The idea is to
+capture a sketch of the initiator and call stack relationship. The output is
+human-readable. To create a new test, first run test_server.py locally with
+--source_dir pointing to tests/, and verify that the test page works as expected
+by pointing a browser to localhost:XXX/your_new_test.html (with XXX the port
+reported in the console output of test_server.py). Then run this
+webserver_test.py with --failed_trace_dir set. Verify that the actual output is
+what you expect it to be, then copy it to results/. If your test is 7.html, you
+should copy to results/7.result.
 """
 
 import argparse
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index 35fcb4d..2daa271 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -43,7 +43,6 @@ class TracingTrackTestCase(unittest.TestCase):
       sock.connect((host, int(port)))
       sock.sendall('GET test.html HTTP/1.1\n\n')
       data = sock.recv(4096)
-      print '%%% ' + data
       self.assertTrue('HTTP/1.0 200 OK' in data)
 
       sock.close()

commit 04a67846fd49a0177b73b2d3c309d5018f276dde
Author: droger <droger@chromium.org>
Date:   Thu Feb 18 09:15:29 2016 -0800

    tools/android/loading Ignore timings for requests coming from cache
    
    The requests coming from Blink cache have stale timings: these
    are the timings corresponding to the request that put the content
    in the cache, rather than of the request that got them from the cache.
    
    Ignoring these timings allows to fallback to the timestamp, which is
    more accurate (although not perfect), and fixes issues where the
    inconsistent timings was breaking initiators.
    
    Review URL: https://codereview.chromium.org/1708233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376187}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0dec7bcceeec4f9b2283fea5040879724b9b04a5

diff --git a/loading/request_track.py b/loading/request_track.py
index 1b3cde2..685ab0d 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -491,12 +491,14 @@ class RequestTrack(devtools_monitor.Track):
                       # network stack.
                       ('requestHeaders', 'request_headers'),
                       ('headers', 'response_headers')))
-    # data URLs don't have a timing dict.
     timing_dict = {}
-    if r.protocol != 'data':
-      timing_dict = response['timing']
-    else:
+    # data URLs don't have a timing dict, and timings for cached requests are
+    # stale.
+    # TODO(droger): the timestamp is inacurate, get the real timings instead.
+    if r.protocol == 'data' or r.served_from_cache:
       timing_dict = {'requestTime': r.timestamp}
+    else:
+      timing_dict = response['timing']
     r.timing = TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
     self._request_id_to_response_received[request_id] = params

commit 9ffe77c6341caeb1acb2322a117b868336daca6c
Author: lizeb <lizeb@chromium.org>
Date:   Thu Feb 18 09:09:37 2016 -0800

    tools/android/loading: Improve nested events handling in ActivityLens.
    
    Review URL: https://codereview.chromium.org/1703863002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376184}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2e161dcde1b6585b12374c90ef458adcfb314dd2

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index 257fdc3..c99d47a 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -17,6 +17,9 @@ import request_track
 
 class ActivityLens(object):
   """Reconstructs the activity of the main renderer thread between requests."""
+  _SCRIPT_EVENT_NAMES = ('EvaluateScript', 'FunctionCall')
+  _PARSING_EVENT_NAMES = ('ParseHTML', 'ParseAuthorStyleSheet')
+
   def __init__(self, trace):
     """Initializes an instance of ActivityLens.
 
@@ -103,8 +106,8 @@ class ActivityLens(object):
     script_to_duration = collections.defaultdict(float)
     script_events = [e for e in events
                      if ('devtools.timeline' in e.tracing_event['cat']
-                         and e.tracing_event['name'] in (
-                             'EvaluateScript', 'FunctionCall'))]
+                         and (e.tracing_event['name']
+                              in cls._SCRIPT_EVENT_NAMES))]
     for event in script_events:
       clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
       script_url = event.args['data'].get('scriptName', None)
@@ -112,6 +115,16 @@ class ActivityLens(object):
     return dict(script_to_duration)
 
   @classmethod
+  def _FullyIncludedEvents(cls, events, event):
+    """Return a list of events wholly included in the |event| span."""
+    (start, end) = (event.start_msec, event.end_msec)
+    result = []
+    for event in events:
+      if start <= event.start_msec < end and start <= event.end_msec < end:
+        result.append(event)
+    return result
+
+  @classmethod
   def _Parsing(cls, events, start_msec, end_msec):
     """Returns the HTML/CSS parsing time within an interval.
 
@@ -127,16 +140,25 @@ class ActivityLens(object):
     url_to_duration = collections.defaultdict(float)
     parsing_events = [e for e in events
                       if ('devtools.timeline' in e.tracing_event['cat']
-                          and e.tracing_event['name'] in (
-                              'ParseHTML', 'ParseAuthorStyleSheet'))]
+                          and (e.tracing_event['name']
+                               in cls._PARSING_EVENT_NAMES))]
     for event in parsing_events:
+      # Parsing events can contain nested script execution events, avoid
+      # double-counting by discounting these.
+      nested_events = cls._FullyIncludedEvents(events, event)
+      events_tree = _EventsTree(event, nested_events)
+      js_events = events_tree.DominatingEventsWithNames(cls._SCRIPT_EVENT_NAMES)
+      duration_to_subtract = sum(
+          cls._ClampedDuration(e, start_msec, end_msec) for e in js_events)
       tracing_event = event.tracing_event
       clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
       if tracing_event['name'] == 'ParseAuthorStyleSheet':
         url = tracing_event['args']['data']['styleSheetUrl']
       else:
         url = tracing_event['args']['beginData']['url']
-      url_to_duration[url] += clamped_duration
+      parsing_duration = clamped_duration - duration_to_subtract
+      assert parsing_duration >= 0
+      url_to_duration[url] += parsing_duration
     return dict(url_to_duration)
 
   def GenerateEdgeActivity(self, dep):
@@ -170,28 +192,69 @@ class ActivityLens(object):
            RequestDependencyLens.GetRequestDependencies().
 
     Returns:
-      {'script': float, 'parsing': float, 'other': float, 'unknown': float}
+      {'script': float, 'parsing': float, 'other_url': float,
+       'unknown_url': float, 'unrelated_work': float}
       where the values are durations in ms:
+      - idle: The renderer main thread was idle.
       - script: The initiating file was executing.
       - parsing: The initiating file was being parsed.
-      - other: Other scripts and/or parsing activities.
-      - unknown: Activity which is not associated with a URL.
+      - other_url: Other scripts and/or parsing activities.
+      - unknown_url: Activity which is not associated with a URL.
+      - unrelated_work: Activity unrelated to scripts or parsing.
     """
     activity = self.GenerateEdgeActivity(dep)
-    related = {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 0}
+    breakdown = {'unrelated_work': activity['busy'],
+                 'idle': activity['edge_cost'] - activity['busy'],
+                 'script': 0, 'parsing': 0,
+                 'other_url': 0, 'unknown_url': 0}
     for kind in ('script', 'parsing'):
       for (script_name, duration_ms) in activity[kind].items():
         if not script_name:
-          related['unknown_url'] += duration_ms
+          breakdown['unknown_url'] += duration_ms
         elif script_name == dep[0].url:
-          related[kind] += duration_ms
+          breakdown[kind] += duration_ms
         else:
-          # A lot of "ParseHTML" tasks are mostly about executing
-          # scripts. Don't double-count.
-          # TODO(lizeb): Better handle TraceEvents nesting.
-          if kind == 'script':
-            related['other_url'] += duration_ms
-    return related
+          breakdown['other_url'] += duration_ms
+    breakdown['unrelated_work'] -= sum(
+        breakdown[x] for x in ('script', 'parsing', 'other_url', 'unknown_url'))
+    return breakdown
+
+
+class _EventsTree(object):
+  """Builds the hierarchy of events from a list of fully nested events."""
+  def __init__(self, root_event, events):
+    """Creates the tree.
+
+    Args:
+      root_event: (Event) Event held by the tree root.
+      events: ([Event]) List of events that are fully included in |root_event|.
+    """
+    self.event = root_event
+    self.start_msec = root_event.start_msec
+    self.end_msec = root_event.end_msec
+    self.children = []
+    events.sort(key=operator.attrgetter('start_msec'))
+    if not events:
+      return
+    current_child = (events[0], [])
+    for event in events[1:]:
+      if event.end_msec < current_child[0].end_msec:
+        current_child[1].append(event)
+      else:
+        self.children.append(_EventsTree(current_child[0], current_child[1]))
+        current_child = (event, [])
+    self.children.append(_EventsTree(current_child[0], current_child[1]))
+
+  def DominatingEventsWithNames(self, names):
+    """Returns a list of the top-most events in the tree with a matching name.
+    """
+    if self.event.name in names:
+      return [self.event]
+    else:
+      result = []
+      for child in self.children:
+        result += child.DominatingEventsWithNames(names)
+      return result
 
 
 if __name__ == '__main__':
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index f96bbd4..5abc172 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -2,9 +2,11 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import collections
+import copy
 import unittest
 
-from activity_lens import ActivityLens
+from activity_lens import (ActivityLens, _EventsTree)
 import test_utils
 import tracing
 
@@ -202,23 +204,33 @@ class ActivityLensTestCast(unittest.TestCase):
          u'ph': u'X',
          u'pid': 1,
          u'tid': 1,
+         u'ts': 0},
+        {u'cat': u'toplevel',
+         u'dur': 100 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 1,
          u'ts': 0}]
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'parser')
     self.assertEquals(
-        {'script': 0, 'parsing': 12, 'other_url': 0, 'unknown_url': 0},
+        {'unrelated_work': 18, 'idle': 0, 'script': 0, 'parsing': 12,
+         'other_url': 0, 'unknown_url': 0},
         activity.BreakdownEdgeActivityByInitiator(dep))
     dep = (requests[0], requests[1], 'other')
-    # Truncating the event from the parent xrequest end.
+    # Truncating the event from the parent request end.
     self.assertEquals(
-        {'script': 0, 'parsing': 7, 'other_url': 0, 'unknown_url': 0},
+        {'unrelated_work': 13, 'idle': 0, 'script': 0, 'parsing': 7,
+         'other_url': 0, 'unknown_url': 0},
         activity.BreakdownEdgeActivityByInitiator(dep))
     # Unknown URL
     raw_events[0]['args']['beginData']['url'] = None
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'parser')
     self.assertEquals(
-        {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 12},
+        {'unrelated_work': 18, 'idle': 0, 'script': 0, 'parsing': 0,
+         'other_url': 0, 'unknown_url': 12},
         activity.BreakdownEdgeActivityByInitiator(dep))
     # Script
     raw_events[1]['ts'] = 40 * 1000
@@ -226,13 +238,15 @@ class ActivityLensTestCast(unittest.TestCase):
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'script')
     self.assertEquals(
-        {'script': 6, 'parsing': 0, 'other_url': 0, 'unknown_url': 7},
+        {'unrelated_work': 7, 'idle': 0, 'script': 6, 'parsing': 0,
+         'other_url': 0, 'unknown_url': 7},
         activity.BreakdownEdgeActivityByInitiator(dep))
     # Other URL
     raw_events[1]['args']['data']['scriptName'] = 'http://other.com/url'
     activity = self._ActivityLens(requests, raw_events)
     self.assertEquals(
-        {'script': 0., 'parsing': 0., 'other_url': 6., 'unknown_url': 7.},
+        {'unrelated_work': 7, 'idle': 0, 'script': 0., 'parsing': 0.,
+         'other_url': 6., 'unknown_url': 7.},
         activity.BreakdownEdgeActivityByInitiator(dep))
 
   def _ActivityLens(self, requests, raw_events):
@@ -241,5 +255,37 @@ class ActivityLensTestCast(unittest.TestCase):
     return ActivityLens(loading_trace)
 
 
+class EventsTreeTestCase(unittest.TestCase):
+  FakeEvent = collections.namedtuple(
+      'FakeEvent', ('name', 'start_msec', 'end_msec'))
+  _ROOT_EVENT = FakeEvent('-1', 0, 20)
+  _EVENTS = [
+      FakeEvent('0', 2, 4), FakeEvent('1', 1, 5),
+      FakeEvent('2', 6, 9),
+      FakeEvent('3', 13, 14), FakeEvent('4', 14, 17), FakeEvent('5', 12, 18)]
+
+  def setUp(self):
+    self.tree = _EventsTree(self._ROOT_EVENT, copy.deepcopy(self._EVENTS))
+
+  def testEventsTreeConstruction(self):
+    self.assertEquals(self._ROOT_EVENT, self.tree.event)
+    self.assertEquals(3, len(self.tree.children))
+    self.assertEquals(self._EVENTS[1], self.tree.children[0].event)
+    self.assertEquals(self._EVENTS[0], self.tree.children[0].children[0].event)
+    self.assertEquals(self._EVENTS[2], self.tree.children[1].event)
+    self.assertEquals([], self.tree.children[1].children)
+    self.assertEquals(self._EVENTS[5], self.tree.children[2].event)
+    self.assertEquals(2, len(self.tree.children[2].children))
+
+  def testDominatingEventsWithNames(self):
+    self.assertListEqual(
+        [self._ROOT_EVENT], self.tree.DominatingEventsWithNames(('-1')))
+    self.assertListEqual(
+        [self._ROOT_EVENT], self.tree.DominatingEventsWithNames(('-1', '0')))
+    self.assertListEqual(
+        [self._EVENTS[1], self._EVENTS[5]],
+        self.tree.DominatingEventsWithNames(('1', '5')))
+
+
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/model_graph.py b/loading/model_graph.py
index e7fcc93..eaded4a 100644
--- a/loading/model_graph.py
+++ b/loading/model_graph.py
@@ -56,8 +56,8 @@ class GraphVisualization(object):
   }
 
   _ACTIVITY_TYPE_LABEL = (
-      ('script', 'S'), ('parsing', 'P'), ('other_url', 'O'),
-      ('unknown_url', 'U'))
+      ('idle', 'I'), ('unrelated_work', 'W'), ('script', 'S'),
+      ('parsing', 'P'), ('other_url', 'O'), ('unknown_url', 'U'))
 
   def __init__(self, graph):
     """Initialize.
diff --git a/loading/tracing.py b/loading/tracing.py
index 7071011..cf79908 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -312,6 +312,10 @@ class Event(object):
     return self._tracing_event.get('id')
 
   @property
+  def name(self):
+    return self._tracing_event['name']
+
+  @property
   def tracing_event(self):
     return self._tracing_event
 
@@ -413,6 +417,7 @@ class _IntervalTree(object):
     return _IntervalTree(start, end, filtered_events)
 
   def OverlappingEvents(self, start, end):
+    """Returns a set of events overlapping with [start, end)."""
     if min(end, self.end) - max(start, self.start) <= 0:
       return set()
     elif self._IsLeaf():

commit 13c672c69aac1480b3c191d856434c3ef70046f4
Author: droger <droger@chromium.org>
Date:   Thu Feb 18 05:29:57 2016 -0800

    tools/android/loading Fix import issues in webserver unittests
    
    Review URL: https://codereview.chromium.org/1708143002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376158}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b7823f1dab8b7416f1818dfc80c1d322059a5137

diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index 2716492..35fcb4d 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -8,7 +8,7 @@ import sys
 import unittest
 
 _SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
+    os.path.dirname(__file__), '..', '..', '..', '..'))
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'android', 'loading'))
 
 import options
@@ -48,3 +48,7 @@ class TracingTrackTestCase(unittest.TestCase):
 
       sock.close()
       self.assertTrue(server.Stop())
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 66d7f3457728b2553dfe28fe1d701130b67b8efc
Author: droger <droger@chromium.org>
Date:   Thu Feb 18 04:54:03 2016 -0800

    tools/android/loading Add 'redirect' and 'ping' content types.
    
    Review URL: https://codereview.chromium.org/1707173003
    
    Cr-Original-Commit-Position: refs/heads/master@{#376152}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: af41852675231437476a78d79d2a564a44f8b507

diff --git a/loading/model_graph.py b/loading/model_graph.py
index 591bd7f..e7fcc93 100644
--- a/loading/model_graph.py
+++ b/loading/model_graph.py
@@ -37,6 +37,8 @@ class GraphVisualization(object):
       'gif':             'grey',
       'image':           'orange',
       'jpeg':            'orange',
+      'ping':            'cyan',  # Empty response
+      'redirect':        'forestgreen',
       'png':             'orange',
       'plain':           'brown3',
       'octet-stream':    'brown3',
diff --git a/loading/request_track.py b/loading/request_track.py
index 2c94703..1b3cde2 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -179,19 +179,36 @@ class Request(object):
       result.timing = TimingFromDict({'requestTime': result.timestamp})
     return result
 
+  def GetHTTPResponseHeader(self, header_name):
+    """Gets the value of a HTTP response header.
+
+    Does a case-insensitive search for the header name in the HTTP response
+    headers, in order to support servers that use a wrong capitalization.
+    """
+    lower_case_name = header_name.lower()
+    result = None
+    for name, value in self.response_headers.iteritems():
+      if name.lower() == lower_case_name:
+        result = value
+        break
+    return result
+
   def GetContentType(self):
     """Returns the content type, or None."""
+    # Check for redirects. Use the "Location" header, because the HTTP status is
+    # not reliable.
+    if self.GetHTTPResponseHeader('Location') is not None:
+      return 'redirect'
+
+    # Check if the response is empty.
+    if (self.GetHTTPResponseHeader('Content-Length') == '0' or
+        self.status == 204):
+      return 'ping'
+
     if self.mime_type:
       return self.mime_type
 
-    # Case-insensitive search because servers sometimes use a wrong
-    # capitalization.
-    content_type = None
-    for header, value in self.response_headers.iteritems():
-      if header.lower() == 'content-type':
-        content_type = value
-        break
-
+    content_type = self.GetHTTPResponseHeader('Content-Type')
     if not content_type or ';' not in content_type:
       return content_type
     else:
@@ -207,14 +224,7 @@ class Request(object):
     if not self.response_headers:
       return -1
 
-    # Case-insensitive search because servers sometimes use a wrong
-    # capitalization.
-    cache_control_str = None
-    for header, value in self.response_headers.iteritems():
-      if header.lower() == 'cache-control':
-        cache_control_str = value
-        break
-
+    cache_control_str = self.GetHTTPResponseHeader('Cache-Control')
     if cache_control_str is not None:
       directives = [s.strip() for s in cache_control_str.split(',')]
       for directive in directives:
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 7a85f07..6e54208 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -47,10 +47,31 @@ class RequestTestCase(unittest.TestCase):
     # Parameters are filtered out.
     r.response_headers = {'Content-Type': 'application/javascript;bla'}
     self.assertEquals('application/javascript', r.GetContentType())
-    # MIME type takes precedence over headers.
+    # MIME type takes precedence over 'Content-Type' header.
     r.mime_type = 'image/webp'
     self.assertEquals('image/webp', r.GetContentType())
     r.mime_type = None
+    # Test for 'ping' type.
+    r.status = 204
+    self.assertEquals('ping', r.GetContentType())
+    r.status = None
+    r.response_headers = {'Content-Type': 'application/javascript',
+                          'content-length': '0'}
+    self.assertEquals('ping', r.GetContentType())
+    # Test for 'redirect' type.
+    r.response_headers = {'Content-Type': 'application/javascript',
+                          'location': 'http://foo',
+                          'content-length': '0'}
+    self.assertEquals('redirect', r.GetContentType())
+
+  def testGetHTTPResponseHeader(self):
+    r = Request()
+    r.response_headers = {}
+    self.assertEquals(None, r.GetHTTPResponseHeader('Foo'))
+    r.response_headers = {'Foo': 'Bar', 'Baz': 'Foo'}
+    self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
+    r.response_headers = {'foo': 'Bar', 'Baz': 'Foo'}
+    self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
 
 
 class RequestTrackTestCase(unittest.TestCase):

commit 83218c736ace6b097ec68ce90181d84c4218f0a8
Author: mattcary <mattcary@chromium.org>
Date:   Wed Feb 17 08:30:11 2016 -0800

    Local webserver test harness plus 3 tests.
    
    This adds a trace_test directory to tools/android/loading that
    performs an integration test for our tracing, and also serves as a
    respository for initiator corner cases that we are investigating.
    
    Review URL: https://codereview.chromium.org/1696353002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375901}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 07593ac7760c73d9597994c08eeac37f4b01df75

diff --git a/loading/trace_test/__init__.py b/loading/trace_test/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/loading/trace_test/results/1.result b/loading/trace_test/results/1.result
new file mode 100644
index 0000000..0b445dc
--- /dev/null
+++ b/loading/trace_test/results/1.result
@@ -0,0 +1,7 @@
+parser (no stack) 1.css
+parser (no stack) 1a.js
+parser (no stack) 1a.png
+parser (no stack) 1b.png
+script (1a.js:9) 1b.js
+script (1b.js:54) 1.ttf
+script (1b.js:54) application/font-wof...Zk73/mAw==
diff --git a/loading/trace_test/results/2.result b/loading/trace_test/results/2.result
new file mode 100644
index 0000000..6fe8a06
--- /dev/null
+++ b/loading/trace_test/results/2.result
@@ -0,0 +1,5 @@
+parser (no stack) 1.css
+parser (no stack) 1a.png
+parser (no stack) 1b.js
+parser (no stack) 1b.png
+parser (no stack) application/font-wof...Zk73/mAw==
diff --git a/loading/trace_test/results/3.result b/loading/trace_test/results/3.result
new file mode 100644
index 0000000..f909cae
--- /dev/null
+++ b/loading/trace_test/results/3.result
@@ -0,0 +1,6 @@
+parser (no stack) 3a.js
+parser (no stack) 3c.js
+script () 3a.jpg
+script () 3b.jpg
+script () 3c.jpg
+script (3a.js:20) 3b.js
diff --git a/loading/trace_test/test_server.py b/loading/trace_test/test_server.py
new file mode 100755
index 0000000..5898c66
--- /dev/null
+++ b/loading/trace_test/test_server.py
@@ -0,0 +1,86 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""A simple http server for running local integration tests.
+
+This chooses a port dynamically and so can communicate that back to its spawner
+via a named pipe at --fifo. Sources are served from the tree named at
+--source_dir.
+"""
+
+
+import argparse
+import cgi
+import os.path
+import logging
+import re
+import time
+import wsgiref.simple_server
+
+
+class ServerApp(object):
+  """WSGI App.
+
+  Dispatches by matching, in order, against GetPaths.
+  """
+  def __init__(self, source_dir):
+    self._source_dir = source_dir
+
+  def __call__(self, environ, start_response):
+    """WSGI dispatch.
+
+    Args:
+      environ: environment list.
+      start_response: WSGI response start.
+
+    Returns:
+      Iterable server result.
+    """
+    path = environ.get('PATH_INFO', '')
+    while path.startswith('/'):
+      path = path[1:]
+    filename = os.path.join(self._source_dir, path)
+    if not os.path.exists(filename):
+      logging.info('%s not found', filename)
+      start_response('404 Not Found', [('Content-Type', 'text/html')])
+      return ["""<!DOCTYPE html>
+<html>
+<head>
+<title>Not Found</title>
+<body>%s not found</body>
+</html>""" % path]
+
+    logging.info('responding with %s', filename)
+    suffix = path[path.rfind('.') + 1:]
+    start_response('200 OK', [('Content-Type',
+                               {'css': 'text/css',
+                                'html': 'text/html',
+                                'jpg': 'image/jpeg',
+                                'js': 'text/javascript',
+                                'png': 'image/png',
+                                'ttf': 'font/ttf',
+                              }[suffix])])
+    return file(filename).read()
+
+
+if __name__ == '__main__':
+  logging.basicConfig(level=logging.INFO)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--fifo', default=None,
+                      help='Named pipe used to communicate port')
+  parser.add_argument('--source_dir', required=True,
+                      help='Directory holding sources to serve.')
+  args = parser.parse_args()
+  server_app = ServerApp(args.source_dir)
+  server = wsgiref.simple_server.make_server(
+      'localhost', 0, server_app)
+  ip, port = server.server_address
+  logging.info('Listening on port %s at %s', port, args.source_dir)
+  if args.fifo:
+    fifo = file(args.fifo, 'w')
+    fifo.write('%s\n' % port)
+    fifo.flush()
+    fifo.close()
+  server.serve_forever()
diff --git a/loading/trace_test/tests/1.css b/loading/trace_test/tests/1.css
new file mode 100644
index 0000000..5ae3598
--- /dev/null
+++ b/loading/trace_test/tests/1.css
@@ -0,0 +1,9 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+.outside {
+    font-family: inline;
+    color: red;
+}
diff --git a/loading/trace_test/tests/1.html b/loading/trace_test/tests/1.html
new file mode 100644
index 0000000..5dd15ce
--- /dev/null
+++ b/loading/trace_test/tests/1.html
@@ -0,0 +1,43 @@
+<!DOCTYPE html>
+<!--
+  Test Javascript Redirection in <head>
+
+  In <head> we have a CSS, a javascript file and a <style> tag. The javascript
+  file inserts another javascript file into head, which itself inserts a <style>
+  tag containing an inline font. The static <style> tag below has a font
+  resource. We expect the static font resource to have an initiator with a stack
+  trace incorrectly attached from the javascript.
+
+  TODO(mattcary): It also appears that if resources are found in the cache we
+  get different intiators: namely both the fonts have a parser initiator with no
+  stack. This is not exactly the problem, as occasionally the initiator sequence
+  changes, but can become consistent again by switching binaries with each run
+  (eg, out/Debug vs out/Release).
+-->
+<html>
+<head>
+<title>Test Javascript Redirection</title>
+<link rel='stylesheet' type='text/css' href='1.css'>
+<script type='text/javascript' src='1a.js'></script>
+<style>
+/* Custom font */
+@font-face {
+ font-family: 'test1';
+ font-style: normal;
+ font-weight: normal;
+ src: local('test1'), local(test1), url(1.ttf) format('truetype');
+}
+</style>
+<style>
+div {
+ background: url('1a.png')
+}
+</style>
+</head>
+<body>
+<img src='1b.png' alt=''>
+
+<div class="outside">ABCpqrst</div>
+<div class="inside">ABCpqrst</div>
+</body>
+</html>
diff --git a/loading/trace_test/tests/1.ttf b/loading/trace_test/tests/1.ttf
new file mode 100644
index 0000000..d268785
Binary files /dev/null and b/loading/trace_test/tests/1.ttf differ
diff --git a/loading/trace_test/tests/1a.js b/loading/trace_test/tests/1a.js
new file mode 100644
index 0000000..496cebb
--- /dev/null
+++ b/loading/trace_test/tests/1a.js
@@ -0,0 +1,9 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+var scr = document.createElement('script');
+scr.setAttribute('type', 'text/javascript');
+scr.setAttribute('src', '1b.js');
+document.getElementsByTagName('head')[0].appendChild(scr)
diff --git a/loading/trace_test/tests/1a.png b/loading/trace_test/tests/1a.png
new file mode 100644
index 0000000..88a0325
Binary files /dev/null and b/loading/trace_test/tests/1a.png differ
diff --git a/loading/trace_test/tests/1b.js b/loading/trace_test/tests/1b.js
new file mode 100644
index 0000000..eaf905d
--- /dev/null
+++ b/loading/trace_test/tests/1b.js
@@ -0,0 +1,60 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+/* Droid Sans from Google Fonts */
+var font = '@font-face { font-family: "inline"; ' +
+    'src: url(data:application/font-woff2;charset=utf-8;base64,' +
+    'd09GMgABAAAAAAboABEAAAAADcgAAAaGAAEAAAAAAAAAAAAAAAAAAAAAAA' +
+    'AAAAAAP0ZGVE0cGigGVgCCeggUCYRlEQgKiDSIWgE2AiQDWAsuAAQgBYJ+B' +
+    '4FVDHg/d2ViZgYbkQxRlC1Sm+yLArvh2MHKbfiAVIG+Knex+u6x+Pyd0L+n' +
+    '4Cl0n74VfYIZH6AMqEOdEag0hxQqkzJcpeRdedy7DCB9T9LF3Y3l8976Xbg' +
+    'X6AArK4qytKYdx2UW4LK8xGbPr2v+AmhM4aV1UgMv5btaum+17iX0YpGGCG' +
+    'EYLIOf7Zf340t4NJtpeX7PFhBmixQP5C/r1GtZokUUskL2f9fU3r93GZDv8' +
+    '+jM5uzlH7wmKVHaEV07AFCtGtkaPQtEalMT1s5gePQ3sRnV4Ie/BQjAB0te' +
+    '/QV450a0AsBn99o2dz6vCnQQAg6CMAHBq5hchnij85b8j4/nx/4LIH3J2e5' +
+    'XnHWa4BC4kDXZW4H4ypUcLmTqeMADwE+YsRuLDoNQTwOuCFHme+wHNKnjeQ' +
+    '4VQlZxh0I4HB6bOp5lQIUVVdi92f3s9+zLil/yP//x853/zhXWky0SLJ0S5' +
+    '4zrezfa/qbk/3t+wEvL5BhOBEmi7632G4otEyCtC2O/ot+wANdlQyrVGts8' +
+    'YN/SC/C0smwfFwt9QSr1wUnXoLawNbial7VsAvWrAVkfgrAdYtjs6G/3rQ1' +
+    'prtX/7j8bsoFYqqg3bKtO6FyHi5IwOe5DkoPCi688Potvk0Fgih5ZDqp6NR' +
+    '2tSGoKVcR8qEL7C7Ab4UkZ+PwOJggFnUA/cz93Uzq5PGiMDbqKNoiLBbWdd' +
+    'SUHk81sPbrQ01ECBl4Qg1w6qURt3Dq3TkqL8+xIw81VqTxILmtzfUV2mSuX' +
+    '4jxxDKTSs2EtB1oqUXphrTK/5i3bmCC9uSugDMMdBIzsS5gxw7YwvS18KJN' +
+    '2DQUNmFV3mLEd7EpyXcjnRpsqxjkfzhOAwd3NY1rOA3dxgOWS2VOgLH2hnf' +
+    'P/lR3auchORtav1cGLzmsDOUK9VN/Y6HWdO4EFRDgyvioOmZTnCeDGoKywg' +
+    'MUlNKiHoEBT0njIyMNMZAtIl0LryFDQIRkIr/M9BUGyDBuANvmGAaAEfAh8' +
+    'Dxn1wNn1oazEwf00PlI8b3EQVsszOvJSeki/GZNCuSSCHSolHeYacwCKIkV' +
+    'gk0lGdQlFrwAlijFrUPfCPiHBEieVgkVuOoyOOaMxTXcR3AqCGkGfJQCoYX' +
+    'DR0JjAYqMqiuIQszkxdjNRcCh0k26crIa2hwb7S6x64eeF5UQEQuWvZN80m' +
+    'wrN8Xqyl8cyNI2QiZ/ARSYML05ZL/9fbIz/Q15LOjnMbVPpwZQNCuOmwM3L' +
+    'UiDSG5Te4UTpIZyv1JidE620EGKWp6qyYKVa2kGqomYifgQbFl05rNhXdk2' +
+    '39FozuhTZgW7ZxrT0CHrQTGiwxf6RRbMBj8ykW+lgFqPbD7MqhUhzUFOzSI' +
+    'y0Bgv5lRBu4PGKZ4kYGSXtw4jSajk1kHG6FI6ayMYtqVtyIPfmKDtmhsA5s' +
+    'IsBVWRHjmKyii7cJGTPWkAzzVY8Mn5iHJvJtlTehFLHzNU61VhdMNiyC1a7' +
+    '/o0MazQ1udRV1/RSwbgdhHPmTmlfgHUljaZl+YIF21T7wXFURxbqSgaPMXu' +
+    'AKkHFhRQaoCcoQsY5NxVP+7KyQxe8OGLMrp1iuoqu33iNFHQxsQnbG9dkX+' +
+    'mmSC6pbrljMi3Tu7p0zSqlUK3aoeOw827lGNdLWkAuD+wzpiunoecYa+ppN' +
+    'g0uIIfopXHHsrt7Fi0+0zg9123bWyYiwx5W2Asewfq7ckv+qphwrLb4fr4/' +
+    'D/zVWZssC/ATIP8Nc5KAn2R/ECQDG/9xOKzN+ZfVAJzXgmMS8CHxEqHmDhJ' +
+    '3mc9OTpEvQY4D3BOWKkgnsBXYnXT/WbePNtZ/v0kHCURbm/UROYYyz+EiXm' +
+    'G3IQoQks87lP8mIdwuTXrcHm0MuX1CVrsD8px2v0Mbl93vMsIT7veoksL72' +
+    't1Dv3Xp4iOukLFEdgL+7JSKja5Z3qEopSoEbFbnVwz0UEa8/ChDiY5IyMFC' +
+    'IR+TUyC/aWEHS0WxdgAFMY/fmcdC4oqzkDFiW8Qzyrmchn9OxEYbGteVGVs' +
+    'U8eYdv4uJjapb93SE21+g2IOMb1Pj79pAHHFxmcJUpoknvgSSk7wUpCglKU' +
+    'tFqlKTujSkGZxrr7E0c1f7yiDB1UndihFm1SyQURKTMTKbzCFzyTwynyzQV' +
+    'jTEa7U5uvS0VS+ePQ15hk3KbWcPLs8esLd/QzHV/ujFrK/UOR3oVeZfxDPA' +
+    'nXCNktFqJcM1KVF3ohJQDWSpTdTvBwboLiPX7iqwaaZUPuIAt0Zk73/mAw==) ' +
+    'format("woff2"); font-weight: normal; font-style: normal;} ' +
+    '.inside { font-family: test1; color: green;}';
+
+var sty = document.createElement('style');
+sty.appendChild(document.createTextNode(font));
+document.getElementsByTagName('head')[0].appendChild(sty);
+
+var dummyToCheckStackIsnotJustEndOfFile = 0;
+function makingTheStackTraceReallyInteresting(x) {
+  dummyToCheckStackIsnotJustEndOfFile = x + 3;
+}
+makingTheStackTraceReallyInteresting(5);
diff --git a/loading/trace_test/tests/1b.png b/loading/trace_test/tests/1b.png
new file mode 100644
index 0000000..dca89e0
Binary files /dev/null and b/loading/trace_test/tests/1b.png differ
diff --git a/loading/trace_test/tests/2.html b/loading/trace_test/tests/2.html
new file mode 100644
index 0000000..1d79ff3
--- /dev/null
+++ b/loading/trace_test/tests/2.html
@@ -0,0 +1,36 @@
+<!DOCTYPE html>
+<!--
+  Test Less Javascript Redirection in <head>
+
+  Like 1.html, in <head> we have a CSS, a javascript file and a <style> tag. In
+  this case, the javacript file directly inserts a <style> tag into <head>. This
+  causes the subsequently loaded font to not be associated with a stack trace,
+  but also causes the dynamically loaded static font from 1b.js to also not have
+  a stack trace.
+-->
+<html>
+<head>
+<title>As 1.html, but one less redirection</title>
+<link rel='stylesheet' type='text/css' href='1.css'>
+<script type='text/javascript' src='1b.js'></script>
+<style>
+@font-face {
+ font-family: 'indie';
+ font-style: normal;
+ font-weight: normal;
+ src: local('Indie Flower'), local('IndieFlower'), url(1.ttf) format('truetype');
+}
+</style>
+<style>
+div {
+ background: url('1a.png')
+}
+</style>
+</head>
+<body>
+<img src='1b.png' alt=''>
+
+<div class="outside">Outside</div>
+<div class="inside">Inside</div>
+</body>
+</html>
diff --git a/loading/trace_test/tests/3.html b/loading/trace_test/tests/3.html
new file mode 100644
index 0000000..03bbce6
--- /dev/null
+++ b/loading/trace_test/tests/3.html
@@ -0,0 +1,27 @@
+<!DOCTYPE html>
+<!--
+  Javascript indirect image loading.
+
+  3a.js defines fn1(), which adds an <img> tag to the body. 3a.js also
+  inserts a script tag with 3b.js into head (between the scripts for
+  3a and 3c). 3b.js itself creates an <img> tag which directly adds it
+  to the body. Finally, 3c.js defines fn3(), which
+  modifies <img id='img3'>.
+
+  Note that as 3b.js adds a tag to the body, it is executed only after
+  the body has been parsed. No, I don't know how that works either.
+
+  At any rate, only 3c.js has a meaningful stack trace in the
+  initiator. The images have script initiators with empty stacks.
+-->
+<html>
+<head>
+<script type='text/javascript' src='3a.js'></script>
+<script type='text/javascript' src='3c.js'></script>
+<img src='' alt='' id='img3'>
+<script type='text/javascript'>
+ fn1();
+ fn3();
+</script>
+</body>
+</html>
diff --git a/loading/trace_test/tests/3a.jpg b/loading/trace_test/tests/3a.jpg
new file mode 100644
index 0000000..25f3a43
Binary files /dev/null and b/loading/trace_test/tests/3a.jpg differ
diff --git a/loading/trace_test/tests/3a.js b/loading/trace_test/tests/3a.js
new file mode 100644
index 0000000..31f6dca
--- /dev/null
+++ b/loading/trace_test/tests/3a.js
@@ -0,0 +1,21 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+function addImg(img_link) {
+  var img = document.createElement('img');
+  img.setAttribute('src', img_link);
+  img.setAttribute('alt', '');
+  document.body.appendChild(img);
+}
+
+function fn1() {
+  addImg('3a.jpg');
+}
+
+var scr = document.createElement('script');
+scr.setAttribute('src', '3b.js');
+scr.setAttribute('type', 'text/javascript');
+document.getElementsByTagName('head')[0].insertBefore(
+    scr, document.getElementsByTagName('script')[0].nextSibling);
diff --git a/loading/trace_test/tests/3b.jpg b/loading/trace_test/tests/3b.jpg
new file mode 100644
index 0000000..de44b66
Binary files /dev/null and b/loading/trace_test/tests/3b.jpg differ
diff --git a/loading/trace_test/tests/3b.js b/loading/trace_test/tests/3b.js
new file mode 100644
index 0000000..9ccc020
--- /dev/null
+++ b/loading/trace_test/tests/3b.js
@@ -0,0 +1,9 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+var img = document.createElement('img');
+img.setAttribute('src', '3b.jpg');
+img.setAttribute('alt', '');
+document.body.appendChild(img);
diff --git a/loading/trace_test/tests/3c.jpg b/loading/trace_test/tests/3c.jpg
new file mode 100644
index 0000000..688b70b
Binary files /dev/null and b/loading/trace_test/tests/3c.jpg differ
diff --git a/loading/trace_test/tests/3c.js b/loading/trace_test/tests/3c.js
new file mode 100644
index 0000000..b34da79
--- /dev/null
+++ b/loading/trace_test/tests/3c.js
@@ -0,0 +1,8 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+function fn3() {
+  document.getElementById('img3').setAttribute('src', '3c.jpg');
+}
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
new file mode 100755
index 0000000..f6724c0
--- /dev/null
+++ b/loading/trace_test/webserver_test.py
@@ -0,0 +1,258 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""An integration test for tracing.
+
+This is not run as part of unittests and is executed directly. In normal
+operation it can be run with no arguments (or perhaps --no_sandbox depending on
+how you have chrome set up). When debugging or adding tests, setting
+--failed_trace_dir could be useful.
+
+Spawns a local http server to serve web pages. The trace generated by each
+file in tests/*.html will be compared with the corresponding results/*.result.
+
+By default this will use a release version of chrome built in this same
+code tree (out/Release/chrome), see --local_binary to override.
+"""
+
+import argparse
+import contextlib
+import json
+import os
+import shutil
+import subprocess
+import sys
+import tempfile
+import urlparse
+
+sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
+
+from device_setup import DeviceConnection
+import loading_trace
+import options
+import trace_recorder
+
+OPTIONS = options.OPTIONS
+WEBSERVER = os.path.join(os.path.dirname(__file__), 'test_server.py')
+TESTDIR = os.path.join(os.path.dirname(__file__), 'tests')
+RESULTDIR = os.path.join(os.path.dirname(__file__), 'results')
+
+
+@contextlib.contextmanager
+def TemporaryDirectory():
+  """Returns a freshly-created directory that gets automatically deleted after
+  usage.
+  """
+  name = tempfile.mkdtemp()
+  try:
+    yield name
+  finally:
+    shutil.rmtree(name)
+
+
+class WebServer(object):
+  """Wrap the webserver."""
+  def __init__(self, source_dir, communication_dir):
+    """Initialize the server but does not start it.
+
+    Args:
+      source_dir: the directory where source data (html, js, etc) will be found.
+      communication_dir: a directory to use for IPC (eg, discovering the
+        port, which is dynamically allocated). This should probably be a
+        temporary directory.
+    """
+    self._source_dir = source_dir
+    self._communication_dir = communication_dir
+    self._fifo = None
+    self._server_process = None
+    self._port = None
+
+  @classmethod
+  @contextlib.contextmanager
+  def Context(cls, *args, **kwargs):
+    """Creates a webserver as a context manager.
+
+    Args:
+      As in __init__.
+
+    Returns:
+      A context manager for an instance of a WebServer.
+    """
+    try:
+      server = cls(*args, **kwargs)
+      server.Start()
+      yield server
+    finally:
+      server.Stop()
+
+  def Start(self):
+    """Start the server by spawning a process."""
+    fifo_name = os.path.join(self._communication_dir, 'from_server')
+    os.mkfifo(fifo_name)
+    server_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+    self._server_process = subprocess.Popen(
+        [WEBSERVER,
+         '--source_dir=%s' % self._source_dir,
+         '--fifo=%s' % fifo_name],
+        shell=False, stdout=server_out, stderr=server_out)
+    fifo = file(fifo_name)
+    # TODO(mattcary): timeout?
+    self._port = int(fifo.readline())
+    fifo.close()
+
+  def Stop(self):
+    """Stops the server, waiting for it to complete.
+
+    Returns:
+      True if the server stopped correctly.
+    """
+    if self._server_process is None:
+      return False
+    self._server_process.kill()
+    # TODO(mattcary): timeout & error?
+    self._server_process.wait()
+    return True
+
+  def Address(self):
+    """Returns a host:port string suitable for an http request."""
+    assert self._port is not None, \
+        "No port exists until the server is started."
+    return 'localhost:%s' % self._port
+
+
+class InitiatorSequence(object):
+  """The interesting parts of the initiator dependancies that are tested."""
+  def __init__(self, trace):
+    """Create.
+
+    Args:
+      trace: a LoadingTrace.
+    """
+    self._seq = []
+    # ReadFromFile will initialize without a trace.
+    if trace is None:
+      return
+    for rq in trace.request_track.GetEvents():
+      if rq.initiator['type'] in ('parser', 'script'):
+        stack = 'no stack'
+        if 'stack' in rq.initiator:
+          stack = '/'.join(
+              ['%s:%s' % (self._ShortUrl(frame['url']), frame['lineNumber'])
+               for frame in rq.initiator['stack']['callFrames']])
+        self._seq.append('%s (%s) %s' % (
+            rq.initiator['type'],
+            stack,
+            self._ShortUrl(rq.url)))
+    self._seq.sort()
+
+  @classmethod
+  def ReadFromFile(cls, input_file):
+    """Read a file from DumpToFile.
+
+    Args:
+      input_file: a file-like object.
+
+    Returns:
+      An InitiatorSequence instance.
+    """
+    seq = cls(None)
+    seq._seq = sorted([l.strip() for l in input_file.readlines() if l])
+    return seq
+
+  def DumpToFile(self, output):
+    """Write to a file.
+
+    Args:
+      output: a writeable file-like object.
+    """
+    output.write('\n'.join(self._seq) + '\n')
+
+  def __eq__(self, other):
+    if other is None:
+      return False
+    assert type(other) is InitiatorSequence
+    if len(self._seq) != len(other._seq):
+      return False
+    for a, b in zip(self._seq, other._seq):
+      if a != b:
+        return False
+    return True
+
+  def _ShortUrl(self, url):
+    short = urlparse.urlparse(url).path
+    while short.startswith('/'):
+      short = short[1:]
+    if len(short) > 40:
+      short = '...'.join((short[:20], short[-10:]))
+    return short
+
+
+def RunTest(webserver, connection, test_page, expected):
+  """Run an webserver test.
+
+  The expected result can be None, in which case --failed_trace_dir can be set
+  to output the observed trace.
+
+  Args:
+    webserver [WebServer]: the webserver to use for the test. It must be
+      started.
+    connection [DevToolsConnection]: the connection to trace against.
+    test_page: the name of the page to load.
+    expected [InitiatorSequence]: expected initiator sequence.
+
+  Returns:
+    True if the test passed and false otherwise. Status is printed to stdout.
+  """
+  url = 'http://%s/%s' % (webserver.Address(), test_page)
+  sys.stdout.write('Testing %s...' % url)
+  observed_seq = InitiatorSequence(trace_recorder.MonitorUrl(
+      connection, url, clear_cache=True))
+  if observed_seq == expected:
+    sys.stdout.write(' ok\n')
+    return True
+  else:
+    sys.stdout.write(' FAILED!\n')
+    if OPTIONS.failed_trace_dir:
+      outname = os.path.join(OPTIONS.failed_trace_dir,
+                             test_page + '.observed_result')
+      with file(outname, 'w') as output:
+        observed_seq.DumpToFile(output)
+      sys.stdout.write('Wrote observed result to %s\n' % outname)
+  return False
+
+
+def RunAllTests():
+  """Run all tests in TESTDIR.
+
+  All tests must have a corresponding result in RESULTDIR unless
+  --failed_trace_dir is set.
+  """
+  with TemporaryDirectory() as temp_dir, \
+       WebServer.Context(TESTDIR, temp_dir) as webserver, \
+       DeviceConnection(None) as connection:
+    failure = False
+    for test in sorted(os.listdir(TESTDIR)):
+      if test.endswith('.html'):
+        result = os.path.join(RESULTDIR, test[:test.rfind('.')] + '.result')
+        assert OPTIONS.failed_trace_dir or os.path.exists(result), \
+            'No result found for test'
+        expected = None
+        if os.path.exists(result):
+          with file(result) as result_file:
+            expected = InitiatorSequence.ReadFromFile(result_file)
+        if not RunTest(webserver, connection, test, expected):
+          failure = True
+  if failure:
+    print 'FAILED!'
+  else:
+    print 'all tests passed'
+
+
+if __name__ == '__main__':
+  OPTIONS.ParseArgs(sys.argv[1:],
+                    description='Run webserver integration test',
+                    extra=[('--failed_trace_dir', ''),
+                           ('--noisy', False)])
+  RunAllTests()
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
new file mode 100644
index 0000000..2716492
--- /dev/null
+++ b/loading/trace_test/webserver_unittest.py
@@ -0,0 +1,50 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import socket
+import sys
+import unittest
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'android', 'loading'))
+
+import options
+from trace_test import webserver_test
+
+
+OPTIONS = options.OPTIONS
+
+
+class TracingTrackTestCase(unittest.TestCase):
+  def setUp(self):
+    OPTIONS.ParseArgs('', extra=[('--noisy', False)])
+
+  def testWebserver(self):
+    with webserver_test.TemporaryDirectory() as temp_dir:
+      test_html = file(os.path.join(temp_dir, 'test.html'), 'w')
+      test_html.write('<!DOCTYPE html><html><head><title>Test</title></head>'
+                      '<body><h1>Test Page</h1></body></html>')
+      test_html.close()
+
+      server = webserver_test.WebServer(temp_dir, temp_dir)
+      server.Start()
+      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+      host, port = server.Address().split(':')
+      sock.connect((host, int(port)))
+      sock.sendall('GET null HTTP/1.1\n\n')
+      data = sock.recv(4096)
+      self.assertTrue(data.startswith('HTTP/1.0 404 Not Found'))
+      sock.close()
+
+      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+      sock.connect((host, int(port)))
+      sock.sendall('GET test.html HTTP/1.1\n\n')
+      data = sock.recv(4096)
+      print '%%% ' + data
+      self.assertTrue('HTTP/1.0 200 OK' in data)
+
+      sock.close()
+      self.assertTrue(server.Stop())

commit 68296502d4944bfc8f8d2a775678792fe4aa9646
Author: blundell <blundell@chromium.org>
Date:   Wed Feb 17 06:44:37 2016 -0800

    tools/android/loading: Fix local profile dir default value
    
    The default value was '' but the code was checking for a default value
    of None.
    
    Review URL: https://codereview.chromium.org/1708553002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375880}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 910318e07e5d98ac95f8e250bf7facd878bc7629

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
index 8d7948c..a1f6849 100644
--- a/loading/chrome_setup.py
+++ b/loading/chrome_setup.py
@@ -55,8 +55,8 @@ def DevToolsConnectionForLocalBinary(flags):
   """
   binary_filename = OPTIONS.local_binary
   profile_dir = OPTIONS.local_profile_dir
-  temp_profile_dir = profile_dir is None
-  if temp_profile_dir:
+  using_temp_profile_dir = profile_dir is None
+  if using_temp_profile_dir:
     profile_dir = tempfile.mkdtemp()
   flags.append('--user-data-dir=%s' % profile_dir)
   chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
@@ -68,8 +68,8 @@ def DevToolsConnectionForLocalBinary(flags):
         OPTIONS.devtools_hostname, OPTIONS.devtools_port)
   finally:
     process.kill()
-    if temp_profile_dir:
-      shutil.rmtree(temp_profile_dir)
+    if using_temp_profile_dir:
+      shutil.rmtree(profile_dir)
 
 
 def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
diff --git a/loading/options.py b/loading/options.py
index 04b5bb3..a11b56c 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -30,7 +30,7 @@ class Options(object):
              'chrome binary for local runs'),
             ('local_noisy', False,
              'Enable local chrome console output'),
-            ('local_profile_dir', '',
+            ('local_profile_dir', None,
              'profile directory to use for local runs'),
             ('no_sandbox', False,
              'pass --no-sandbox to browser (local run only; see also '

commit 10b4355a469585544cb2986c504c7d8b6abdfdb2
Author: droger <droger@chromium.org>
Date:   Wed Feb 17 05:36:01 2016 -0800

    tools/android/loading Enable async call stacks
    
    This CL enables the "Debugger" domains and sets the async
    call stack depth to 4 for the request track.
    The asynchronous stacks are merged with the basic stacks, which
    provides more information in the initiators and dramatically reduces
    the number of orphan requests.
    
    Review URL: https://codereview.chromium.org/1692983002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375870}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b382e4f33c93cf1e95c6e351f365f72ef8b27f54

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 5279e4f..68c785f 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -92,6 +92,7 @@ class DevToolsConnection(object):
     self._ws = self._Connect(hostname, port)
     self._event_listeners = {}
     self._domain_listeners = {}
+    self._scoped_states = {}
     self._domains_to_enable = set()
     self._tearing_down_tracing = False
     self._set_up = False
@@ -130,6 +131,30 @@ class DevToolsConnection(object):
       if key in self._domain_listeners:
         del(self._domain_listeners[key])
 
+  def SetScopedState(self, method, params, default_params, enable_domain):
+    """Changes state at the beginning the monitoring and resets it at the end.
+
+    |method| is called with |params| at the beginning of the monitoring. After
+    the monitoring completes, the state is reset by calling |method| with
+    |default_params|.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Parameters to set when the monitoring starts.
+      default_params: (dict) Parameters to reset the state at the end.
+      enable_domain: (bool) True if enabling the domain is required.
+    """
+    if enable_domain:
+      if '.' in method:
+        domain = method[:method.index('.')]
+        assert domain, 'No valid domain'
+        self._domains_to_enable.add(domain)
+    scoped_state_value = (params, default_params)
+    if self._scoped_states.has_key(method):
+      assert self._scoped_states[method] == scoped_state_value
+    else:
+      self._scoped_states[method] = scoped_state_value
+
   def SyncRequest(self, method, params=None):
     """Issues a synchronous request to the DevTools server.
 
@@ -186,6 +211,9 @@ class DevToolsConnection(object):
         self.SyncRequestNoResponse('%s.enable' % domain)
         # Tracing setup must be done by the tracing track to control filtering
         # and output.
+    for scoped_state in self._scoped_states:
+      self.SyncRequestNoResponse(scoped_state,
+                                 self._scoped_states[scoped_state][0])
     self._tearing_down_tracing = False
     self._set_up = True
 
@@ -218,6 +246,9 @@ class DevToolsConnection(object):
       self.SyncRequestNoResponse(self.TRACING_END_METHOD)
       self._tearing_down_tracing = True
       self._Dispatch(kind='Tracing', timeout=self.TRACING_TIMEOUT)
+    for scoped_state in self._scoped_states:
+      self.SyncRequestNoResponse(scoped_state,
+                                 self._scoped_states[scoped_state][1])
     for domain in self._domains_to_enable:
       if domain != self.TRACING_DOMAIN:
         self.SyncRequest('%s.disable' % domain)
@@ -225,6 +256,7 @@ class DevToolsConnection(object):
     self._domains_to_enable.clear()
     self._domain_listeners.clear()
     self._event_listeners.clear()
+    self._scoped_states.clear()
 
   def _OnDataReceived(self, msg):
     if 'method' not in msg:
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index b255656..4a72a4f 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -8,6 +8,7 @@ When executed as a script, loads a trace and outputs the dependencies.
 """
 
 import collections
+import copy
 import logging
 import operator
 
@@ -18,6 +19,7 @@ import request_track
 class RequestDependencyLens(object):
   """Analyses and infers request dependencies."""
   DEPENDENCIES = ('redirect', 'parser', 'script', 'inferred', 'other')
+  CALLFRAMES_KEY = 'callFrames'
   def __init__(self, trace):
     """Initializes an instance of RequestDependencyLens.
 
@@ -89,6 +91,30 @@ class RequestDependencyLens(object):
     initiating_request = self._FindBestMatchingInitiator(request, candidates)
     return (initiating_request, request, 'parser')
 
+  def _FlattenScriptStack(self, stack):
+    """Recursively collapses the stack of asynchronous callstacks.
+
+    A stack has a list of call frames and optionnally a "parent" stack.
+    This function recursively folds the parent stacks into the root stack by
+    concatening all the call frames.
+
+    Args:
+      stack: (dict) the stack that must be flattened
+
+    Returns:
+      A stack with no parent, which is a dictionary with a single "callFrames"
+      key, and no "parent" key.
+    """
+    PARENT_KEY = 'parent'
+    if not PARENT_KEY in stack:
+      return stack
+    stack[self.CALLFRAMES_KEY] += stack[PARENT_KEY][self.CALLFRAMES_KEY]
+    if not PARENT_KEY in stack[PARENT_KEY]:
+      stack.pop(PARENT_KEY)
+    else:
+      stack[PARENT_KEY] = stack[PARENT_KEY][PARENT_KEY]
+    return self._FlattenScriptStack(stack)
+
   def _GetInitiatingRequestScript(self, request):
     STACK_KEY = 'stack'
     if not STACK_KEY in request.initiator:
@@ -96,7 +122,10 @@ class RequestDependencyLens(object):
       return None
     initiating_request = None
     timestamp = request.timing.request_time
-    call_frames = request.initiator[STACK_KEY]['callFrames']
+    # Deep copy the initiator's stack to avoid mutating the input request.
+    stack = self._FlattenScriptStack(
+        copy.deepcopy(request.initiator[STACK_KEY]))
+    call_frames = stack[self.CALLFRAMES_KEY]
     for frame in call_frames:
       url = frame['url']
       candidates = self._FindMatchingRequests(url, timestamp)
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 4035abf..1429a0e 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -83,6 +83,24 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         deps[0],
         self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
 
+  def testAsyncScriptDependency(self):
+    JS_REQUEST_WITH_ASYNC_STACK = Request.FromJsonDict(
+        {'url': 'http://bla.com/cat.js', 'request_id': '1234.14',
+         'initiator': {
+             'type': 'script',
+             'stack': {'callFrames': [],
+                       'parent': {'callFrames': [
+                                      {'url': 'http://bla.com/nyancat.js'}]}}},
+         'timestamp': 10, 'timing': TimingFromDict({})})
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0], self._JS_REQUEST.request_id,
+        JS_REQUEST_WITH_ASYNC_STACK.request_id, 'script')
+
   def testParserDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._REQUEST, self._JS_REQUEST])
diff --git a/loading/request_track.py b/loading/request_track.py
index 89e3a5d..2c94703 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -273,6 +273,10 @@ class RequestTrack(devtools_monitor.Track):
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
+      # Enable asynchronous callstacks to get full javascript callstacks in
+      # initiators
+      self._connection.SetScopedState('Debugger.setAsyncCallStackDepth',
+                                      {'maxDepth': 4}, {'maxDepth': 0}, True)
     # responseReceived message are sometimes duplicated. Records the message to
     # detect this.
     self._request_id_to_response_received = {}

commit d56ead08a3df9f92db5cdade6b8ca9d5a1acf19e
Author: wnwen <wnwen@chromium.org>
Date:   Tue Feb 16 13:29:50 2016 -0800

    Also adds debug build flag.
    
    BUG=583690
    
    Review URL: https://codereview.chromium.org/1597273005
    
    Cr-Original-Commit-Position: refs/heads/master@{#375666}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b0d6314305ddd99c0808a0a41e90b7feb89a79ea

diff --git a/eclipse/.classpath b/eclipse/.classpath
index b740bfe..db88a17 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -148,6 +148,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/web_input_event_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_text_input_type"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/window_open_disposition_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/templates/base_build_config_gen"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/base_native_libraries_gen"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/chrome_version_java"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/dom_distiller_core_font_family_java"/>

commit dd141c80296d24f4a165807abd0ac3cc59d5d4f7
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 10:04:54 2016 -0800

    tools/android/loading: TIDs are not globally unique on OS X.
    
    BUG=587097
    
    Review URL: https://codereview.chromium.org/1698183004
    
    Cr-Original-Commit-Position: refs/heads/master@{#375606}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b272be9da423be9a2df94eb35fa98f760dafdcfc

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index 92ffd14..257fdc3 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -25,9 +25,9 @@ class ActivityLens(object):
     """
     self._trace = trace
     events = trace.tracing_track.GetEvents()
-    self._renderer_main_tid = self._GetRendererMainThreadId(events)
+    self._renderer_main_pid_tid = self._GetRendererMainThreadId(events)
     self._tracing = self._trace.tracing_track.TracingTrackForThread(
-        self._renderer_main_tid)
+        self._renderer_main_pid_tid)
 
   @classmethod
   def _GetRendererMainThreadId(cls, events):
@@ -41,28 +41,28 @@ class ActivityLens(object):
       events: [tracing.Event] List of trace events.
 
     Returns:
-      The thread ID (int) of the busiest renderer main thread.
-
+      (PID (int), TID (int)) of the busiest renderer main thread.
     """
-    events_count_per_tid = collections.defaultdict(int)
+    events_count_per_pid_tid = collections.defaultdict(int)
     main_renderer_thread_ids = set()
     for event in events:
       tracing_event = event.tracing_event
+      pid = event.tracing_event['pid']
       tid = event.tracing_event['tid']
-      events_count_per_tid[tid] += 1
+      events_count_per_pid_tid[(pid, tid)] += 1
       if (tracing_event['cat'] == '__metadata'
           and tracing_event['name'] == 'thread_name'
           and event.args['name'] == 'CrRendererMain'):
-        main_renderer_thread_ids.add(tid)
-    tid_events_counts = sorted(events_count_per_tid.items(),
-                               key=operator.itemgetter(1), reverse=True)
-    if (len(tid_events_counts) > 1
-        and tid_events_counts[0][1] < 2 * tid_events_counts[1][1]):
+        main_renderer_thread_ids.add((pid, tid))
+    pid_tid_events_counts = sorted(events_count_per_pid_tid.items(),
+                                   key=operator.itemgetter(1), reverse=True)
+    if (len(pid_tid_events_counts) > 1
+        and pid_tid_events_counts[0][1] < 2 * pid_tid_events_counts[1][1]):
       logging.warning(
           'Several active renderers (%d and %d with %d and %d events).'
-          % (tid_events_counts[0][0], tid_events_counts[1][0],
-             tid_events_counts[0][1], tid_events_counts[1][1]))
-    return tid_events_counts[0][0]
+          % (pid_tid_events_counts[0][0][0], pid_tid_events_counts[1][0][0],
+             pid_tid_events_counts[0][1], pid_tid_events_counts[1][1]))
+    return pid_tid_events_counts[0][0]
 
   def _OverlappingMainRendererThreadEvents(self, start_msec, end_msec):
     return self._tracing.OverlappingEvents(start_msec, end_msec)
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index 3b4062f..f96bbd4 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -28,21 +28,21 @@ class ActivityLensTestCast(unittest.TestCase):
          u'cat': u'__metadata',
          u'name': u'thread_name',
          u'ph': u'M',
-         u'pid': 123,
+         u'pid': 1,
          u'tid': 123,
          u'ts': 0},
         {u'args': {u'name': u'CrRendererMain'},
          u'cat': u'__metadata',
          u'name': u'thread_name',
          u'ph': u'M',
-         u'pid': 1234,
+         u'pid': 1,
          u'tid': first_renderer_tid,
          u'ts': 0},
         {u'args': {u'name': u'CrRendererMain'},
          u'cat': u'__metadata',
          u'name': u'thread_name',
          u'ph': u'M',
-         u'pid': 12345,
+         u'pid': 1,
          u'tid': second_renderer_tid,
          u'ts': 0}]
     raw_events += [
@@ -50,7 +50,7 @@ class ActivityLensTestCast(unittest.TestCase):
          u'cat': u'devtools.timeline,v8',
          u'name': u'FunctionCall',
          u'ph': u'X',
-         u'pid': 32723,
+         u'pid': 1,
          u'tdur': 0,
          u'tid': first_renderer_tid,
          u'ts': 251427174674,
@@ -60,13 +60,25 @@ class ActivityLensTestCast(unittest.TestCase):
          u'cat': u'devtools.timeline,v8',
          u'name': u'FunctionCall',
          u'ph': u'X',
-         u'pid': 1234,
+         u'pid': 1,
          u'tdur': 0,
          u'tid': second_renderer_tid,
          u'ts': 251427174674,
          u'tts': 5107725}] * 150
+    # There are more events from first_renderer_tid when (incorrectly) ignoring
+    # the PID.
+    raw_events += [
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 12,
+         u'tdur': 0,
+         u'tid': first_renderer_tid,
+         u'ts': 251427174674,
+         u'tts': 5107725}] * 100
     events = self._EventsFromRawEvents(raw_events)
-    self.assertEquals(second_renderer_tid,
+    self.assertEquals((1, second_renderer_tid),
                       ActivityLens._GetRendererMainThreadId(events))
 
   def testThreadBusiness(self):
diff --git a/loading/tracing.py b/loading/tracing.py
index 35907d3..7071011 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -86,16 +86,19 @@ class TracingTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
-  def TracingTrackForThread(self, tid):
+  def TracingTrackForThread(self, pid_tid):
     """Returns a new TracingTrack with only the events from a given thread.
 
     Args:
-      tid: (int) Thread ID.
+      pid_tid: ((int, int) PID and TID.
 
     Returns:
       A new instance of TracingTrack.
     """
-    events = [e for e in self._events if e.tracing_event['tid'] == tid]
+    (pid, tid) = pid_tid
+    events = [e for e in self._events
+              if (e.tracing_event['pid'] == pid
+                  and e.tracing_event['tid'] == tid)]
     tracing_track = TracingTrack(None)
     tracing_track._events = events
     return tracing_track
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index a2c9bce..ade5f8d 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -28,12 +28,16 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 15, 'ph': 'D', 'id': 1}]
 
   _EVENTS = [
-      {'ts': 5, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'B'}},
-      {'ts': 3, 'ph': 'X', 'dur': 4, 'tid': 1, 'args': {'name': 'A'}},
-      {'ts': 10, 'ph': 'X', 'dur': 1, 'tid': 2, 'args': {'name': 'C'}},
-      {'ts': 10, 'ph': 'X', 'dur': 2, 'tid': 2, 'args': {'name': 'D'}},
-      {'ts': 13, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'F'}},
-      {'ts': 12, 'ph': 'X', 'dur': 3, 'tid': 1, 'args': {'name': 'E'}}]
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'pid': 2, 'tid': 1, 'args': {'name': 'B'}},
+      {'ts': 3, 'ph': 'X', 'dur': 4, 'pid': 2, 'tid': 1, 'args': {'name': 'A'}},
+      {'ts': 10, 'ph': 'X', 'dur': 1, 'pid': 2, 'tid': 2,
+       'args': {'name': 'C'}},
+      {'ts': 10, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 2,
+       'args': {'name': 'D'}},
+      {'ts': 13, 'ph': 'X', 'dur': 1, 'pid': 2, 'tid': 1,
+       'args': {'name': 'F'}},
+      {'ts': 12, 'ph': 'X', 'dur': 3, 'pid': 2, 'tid': 1,
+       'args': {'name': 'E'}}]
 
   def setUp(self):
     self.tree_threshold = _IntervalTree._TRESHOLD
@@ -257,10 +261,10 @@ class TracingTrackTestCase(unittest.TestCase):
     self.track.Handle(
         'Tracing.dataCollected', {'params': {'value': [
             self.EventToMicroseconds(e) for e in self._EVENTS]}})
-    tracing_track = self.track.TracingTrackForThread(1)
+    tracing_track = self.track.TracingTrackForThread((2, 1))
     self.assertTrue(tracing_track is not self.track)
     self.assertEquals(4, len(tracing_track.GetEvents()))
-    tracing_track = self.track.TracingTrackForThread(42)
+    tracing_track = self.track.TracingTrackForThread((2, 42))
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
 

commit 9ae51de2957d5f8f618a7c3183d0db17d848cacb
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 08:46:17 2016 -0800

    tools/android/loading: Add support for device and network emulation.
    
    Review URL: https://codereview.chromium.org/1698293002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375597}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 84efb4de22a98e02ecead98c8bdfe95e9d3f1c7a

diff --git a/loading/analyze.py b/loading/analyze.py
index 1c31de0..17ab770 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -25,6 +25,7 @@ import devil_chromium
 from pylib import constants
 
 import activity_lens
+import chrome_setup
 import content_classification_lens
 import device_setup
 import frame_load_lens
@@ -99,7 +100,7 @@ def _GetPrefetchHtml(graph, name=None):
 
 
 def _LogRequests(url, clear_cache_override=None):
-  """Log requests for a web page.
+  """Logs requests for a web page.
 
   Args:
     url: url to log as string.
@@ -111,8 +112,14 @@ def _LogRequests(url, clear_cache_override=None):
   device = device_setup.GetFirstDevice() if not OPTIONS.local else None
   clear_cache = (clear_cache_override if clear_cache_override is not None
                  else OPTIONS.clear_cache)
+
   with device_setup.DeviceConnection(device) as connection:
+    additional_metadata = {}
+    if OPTIONS.local:
+      additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
+          connection, OPTIONS.emulate_device, OPTIONS.emulate_network)
     trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
+    trace.metadata.update(additional_metadata)
     return trace.ToJsonDict()
 
 
diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
new file mode 100644
index 0000000..8d7948c
--- /dev/null
+++ b/loading/chrome_setup.py
@@ -0,0 +1,169 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Handles Chrome's configuration."""
+
+import contextlib
+import json
+import shutil
+import subprocess
+import tempfile
+import time
+
+import devtools_monitor
+from options import OPTIONS
+
+
+# Copied from
+# WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
+_NETWORK_CONDITIONS = {
+    'Offline': {
+        'download': 0 * 1024 / 8, 'upload': 0 * 1024 / 8, 'latency': 0},
+    'GPRS': {
+        'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
+    'Regular 2G': {
+        'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
+    'Good 2G': {
+        'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
+    'Regular 3G': {
+        'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
+    'Good 3G': {
+        'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
+        'latency': 40},
+    'Regular 4G': {
+        'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
+        'latency': 20},
+    'DSL': {
+        'download': 2 * 1024 * 1024 / 8, 'upload': 1 * 1024 * 1024 / 8,
+        'latency': 5},
+    'WiFi': {
+        'download': 30 * 1024 * 1024 / 8, 'upload': 15 * 1024 * 1024 / 8,
+        'latency': 2}
+}
+
+
+@contextlib.contextmanager
+def DevToolsConnectionForLocalBinary(flags):
+  """Returns a DevToolsConnection context manager for a local binary.
+
+  Args:
+    flags: ([str]) List of flags to pass to the browser.
+
+  Returns:
+    A DevToolsConnection context manager.
+  """
+  binary_filename = OPTIONS.local_binary
+  profile_dir = OPTIONS.local_profile_dir
+  temp_profile_dir = profile_dir is None
+  if temp_profile_dir:
+    profile_dir = tempfile.mkdtemp()
+  flags.append('--user-data-dir=%s' % profile_dir)
+  chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+  process = subprocess.Popen(
+      [binary_filename] + flags, shell=False, stderr=chrome_out)
+  try:
+    time.sleep(10)
+    yield devtools_monitor.DevToolsConnection(
+        OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+  finally:
+    process.kill()
+    if temp_profile_dir:
+      shutil.rmtree(temp_profile_dir)
+
+
+def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
+                                    emulated_network_name):
+  """Sets up the device and network emulation and returns the trace metadata.
+
+  Args:
+    connection: (DevToolsConnection)
+    emulated_device_name: (str) Key in the dict returned by
+                          _LoadEmulatedDevices().
+    emulated_network_name: (str) Key in _NETWORK_CONDITIONS.
+
+  Returns:
+    A metadata dict {'deviceEmulation': params, 'networkEmulation': params}.
+  """
+  result = {'deviceEmulation': {}, 'networkEmulation': {}}
+  if emulated_device_name:
+    devices = _LoadEmulatedDevices(OPTIONS.devices_file)
+    emulated_device = devices[emulated_device_name]
+    emulation_params = _SetUpDeviceEmulationAndReturnMetadata(
+        connection, emulated_device)
+    result['deviceEmulation'] = emulation_params
+  if emulated_network_name:
+    params = _NETWORK_CONDITIONS[emulated_network_name]
+    _SetUpNetworkEmulation(
+        connection, params['latency'], params['download'], params['upload'])
+    result['networkEmulation'] = params
+  return result
+
+
+def _LoadEmulatedDevices(filename):
+  """Loads a list of emulated devices from the DevTools JSON registry.
+
+  Args:
+    filename: (str) Path to the JSON file.
+
+  Returns:
+    {'device_name': device}
+  """
+  json_dict = json.load(open(filename, 'r'))
+  devices = {}
+  for device in json_dict['extensions']:
+    device = device['device']
+    devices[device['title']] = device
+  return devices
+
+
+def _GetDeviceEmulationMetadata(device):
+  """Returns the metadata associated with a given device."""
+  return {'width': device['screen']['vertical']['width'],
+          'height': device['screen']['vertical']['height'],
+          'deviceScaleFactor': device['screen']['device-pixel-ratio'],
+          'mobile': 'mobile' in device['capabilities'],
+          'userAgent': device['user-agent']}
+
+
+def _SetUpDeviceEmulationAndReturnMetadata(connection, device):
+  """Configures an instance of Chrome for device emulation.
+
+  Args:
+    connection: (DevToolsConnection)
+    device: (dict) As returned by LoadEmulatedDevices().
+
+  Returns:
+    A dict containing the device emulation metadata.
+  """
+  print device
+  res = connection.SyncRequest('Emulation.canEmulate')
+  assert res['result'], 'Cannot set device emulation.'
+  data = _GetDeviceEmulationMetadata(device)
+  connection.SyncRequestNoResponse(
+      'Emulation.setDeviceMetricsOverride',
+      {'width': data['width'],
+       'height': data['height'],
+       'deviceScaleFactor': data['deviceScaleFactor'],
+       'mobile': data['mobile'],
+       'fitWindow': True})
+  connection.SyncRequestNoResponse('Network.setUserAgentOverride',
+                                   {'userAgent': data['userAgent']})
+  return data
+
+
+def _SetUpNetworkEmulation(connection, latency, download, upload):
+  """Configures an instance of Chrome for network emulation.
+
+  Args:
+    connection: (DevToolsConnection)
+    latency: (float) Latency in ms.
+    download: (float) Download speed (Bytes / s).
+    upload: (float) Upload speed (Bytes / s).
+  """
+  res = connection.SyncRequest('Network.canEmulateNetworkConditions')
+  assert res['result'], 'Cannot set network emulation.'
+  connection.SyncRequestNoResponse(
+      'Network.emulateNetworkConditions',
+      {'offline': False, 'latency': latency, 'downloadThroughput': download,
+       'uploadThroughput': upload})
diff --git a/loading/device_setup.py b/loading/device_setup.py
index b758eec..0fe95fd 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -33,6 +33,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
 import adb_install_cert
 import certutils
 
+import chrome_setup
 import devtools_monitor
 import options
 
@@ -40,18 +41,6 @@ import options
 OPTIONS = options.OPTIONS
 
 
-@contextlib.contextmanager
-def TemporaryDirectory():
-  """Returns a freshly-created directory that gets automatically deleted after
-  usage.
-  """
-  name = tempfile.mkdtemp()
-  try:
-    yield name
-  finally:
-    shutil.rmtree(name)
-
-
 class DeviceSetupException(Exception):
   def __init__(self, msg):
     super(DeviceSetupException, self).__init__(msg)
@@ -185,6 +174,31 @@ def WprHost(device, wpr_archive_path, record=False,
 
 
 @contextlib.contextmanager
+def _DevToolsConnectionOnDevice(device, flags):
+  """Returns a DevToolsConnection context manager for a given device.
+
+  Args:
+    device: Device to connect to.
+    flags: ([str]) List of flags.
+
+  Returns:
+    A DevToolsConnection context manager.
+  """
+  package_info = OPTIONS.ChromePackage()
+  command_line_path = '/data/local/chrome-command-line'
+  _SetUpDevice(device, package_info)
+  with FlagReplacer(device, command_line_path, flags):
+    start_intent = intent.Intent(
+        package=package_info.package, activity=package_info.activity,
+        data='about:blank')
+    device.StartActivity(start_intent, blocking=True)
+    time.sleep(2)
+    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
+                     'localabstract:chrome_devtools_remote'):
+      yield devtools_monitor.DevToolsConnection(
+          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+
+
 def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
 
@@ -198,8 +212,6 @@ def DeviceConnection(device, additional_flags=None):
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
   """
-  package_info = OPTIONS.ChromePackage()
-  command_line_path = '/data/local/chrome-command-line'
   new_flags = ['--disable-fre',
                '--enable-test-events',
                '--remote-debugging-port=%d' % OPTIONS.devtools_port]
@@ -207,41 +219,8 @@ def DeviceConnection(device, additional_flags=None):
     new_flags.append('--no-sandbox')
   if additional_flags != None:
     new_flags.extend(additional_flags)
+
   if device:
-    _SetUpDevice(device, package_info)
-  with FlagReplacer(device, command_line_path, new_flags):
-    host_process = None
-    if device:
-      start_intent = intent.Intent(
-          package=package_info.package, activity=package_info.activity,
-          data='about:blank')
-      device.StartActivity(start_intent, blocking=True)
-    else:
-      # Run on the host. We don't care about startup time so will skip the about
-      # page.
-      assert os.path.exists(OPTIONS.local_binary)
-
-      local_profile_dir = OPTIONS.local_profile_dir
-      if not local_profile_dir:
-        local_profile_dir = TemporaryDirectory()
-
-      new_flags.append('--user-data-dir=%s' % local_profile_dir)
-      chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-      host_process = subprocess.Popen(
-          [OPTIONS.local_binary] + new_flags,
-          shell=False, stdout=chrome_out, stderr=chrome_out)
-    if device:
-      time.sleep(2)
-    else:
-      # TODO(mattcary): This seems to be related to chrome startup. There should
-      # be a way to ping chrome --- maybe keep trying to connect to the devtools
-      # port?
-      time.sleep(10)
-    try:
-      with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
-                       'localabstract:chrome_devtools_remote'):
-        yield devtools_monitor.DevToolsConnection(
-            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-    finally:
-      if host_process:
-        host_process.kill()
+    return _DevToolsConnectionOnDevice(device, new_flags)
+  else:
+    return chrome_setup.DevToolsConnectionForLocalBinary(new_flags)
diff --git a/loading/options.py b/loading/options.py
index 1d9ff72..04b5bb3 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -36,6 +36,13 @@ class Options(object):
              'pass --no-sandbox to browser (local run only; see also '
              'https://chromium.googlesource.com/chromium/src/+/master/'
              'docs/linux_suid_sandbox_development.md)'),
+            ('devices_file', _SRC_DIR + '/third_party/WebKit/Source/devtools'
+             '/front_end/emulated_devices/module.json', 'File containing a'
+             ' list of emulated devices characteristics.'),
+            ('emulate_device', '', 'Name of the device to emulate. Must be '
+             'present in --devices_file, or empty for no emulation.'),
+            ('emulate_network', '', 'Type of network emulation. Empty for no'
+             ' emulation.')
           ]
 
 

commit c5e28a46c31de9e82ae2e36e8c10b01c310be612
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 08:44:29 2016 -0800

    sandwich: Implements reload cache operation to compare with push.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1701973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375594}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bbcc51966eccb4e6dc7c4ecf2ad8e04add8ac1bc

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index a99a5a3..ef7fca8 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -86,15 +86,13 @@ def _ReadUrlsFromJobDescription(job_name):
   raise Exception('Job description does not define a list named "urls"')
 
 
-def _SaveChromeTrace(events, directory, subdirectory):
+def _SaveChromeTrace(events, target_directory):
   """Saves the trace events, ignores IO errors.
 
   Args:
     events: a dict as returned by TracingTrack.ToJsonDict()
-    directory: directory name contining all traces
-    subdirectory: directory name to create this particular trace in
+    target_directory: Directory path where trace is created.
   """
-  target_directory = os.path.join(directory, subdirectory)
   filename = os.path.join(target_directory, 'trace.json')
   try:
     os.makedirs(target_directory)
@@ -288,7 +286,7 @@ def main():
   parser.add_argument('--repeat', default=1, type=int,
                       help='How many times to run the job')
   parser.add_argument('--cache-op',
-                      choices=['clear', 'save', 'push'],
+                      choices=['clear', 'save', 'push', 'reload'],
                       default='clear',
                       help='Configures cache operation to do before launching '
                           +'Chrome. (Default is clear).')
@@ -329,26 +327,37 @@ def main():
 
   with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
       args.disable_wpr_script_injection) as additional_flags:
+    def _RunNavigation(url, clear_cache, trace_id):
+      with device_setup.DeviceConnection(
+          device=device,
+          additional_flags=additional_flags) as connection:
+        if clear_cache:
+          connection.ClearCache()
+        page_track.PageTrack(connection)
+        tracing_track = tracing.TracingTrack(connection,
+            categories=pull_sandwich_metrics.CATEGORIES)
+        connection.SetUpMonitoring()
+        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+        connection.StartMonitoring()
+        if trace_id != None:
+          trace_target_directory = os.path.join(args.output, str(trace_id))
+          _SaveChromeTrace(tracing_track.ToJsonDict(), trace_target_directory)
+
     for _ in xrange(args.repeat):
       for url in job_urls:
-        if args.cache_op == 'push':
+        clear_cache = False
+        if args.cache_op == 'clear':
+          clear_cache = True
+        elif args.cache_op == 'push':
           device.KillAll(_CHROME_PACKAGE, quiet=True)
           _PushBrowserCache(device, local_cache_directory_path)
-        with device_setup.DeviceConnection(
-            device=device,
-            additional_flags=additional_flags) as connection:
-          if (not run_infos['urls'] and args.cache_op == 'save' or
-              args.cache_op == 'clear'):
-            connection.ClearCache()
-          page_track.PageTrack(connection)
-          tracing_track = tracing.TracingTrack(connection,
-              categories=pull_sandwich_metrics.CATEGORIES)
-          connection.SetUpMonitoring()
-          connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-          connection.StartMonitoring()
-          _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
-              str(len(run_infos['urls'])))
-          run_infos['urls'].append(url)
+        elif args.cache_op == 'reload':
+          _RunNavigation(url, clear_cache=True, trace_id=None)
+        elif args.cache_op == 'save':
+          clear_cache = not run_infos['urls']
+        _RunNavigation(url, clear_cache=clear_cache,
+                       trace_id=len(run_infos['urls']))
+        run_infos['urls'].append(url)
 
   if local_cache_directory_path:
     shutil.rmtree(local_cache_directory_path)

commit 622645e5b637a970247e89b7b33aba4196685181
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 08:19:37 2016 -0800

    tools/android/loading: Add blundell and droger to OWNERS.
    
    Review URL: https://codereview.chromium.org/1698393002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375590}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a0630c858350f2edc8b6dbf3c9d83681bfd3475d

diff --git a/loading/OWNERS b/loading/OWNERS
index 0b168ed..52bebbb 100644
--- a/loading/OWNERS
+++ b/loading/OWNERS
@@ -1,4 +1,6 @@
+blundell@chromium.org
+droger@chromium.org
 lizeb@chromium.org
-pasko@chromium.org
 # Not a committer yet, but OWNER nonetheless:
 # mattcary@chromium.org
+pasko@chromium.org

commit 6413fcce09661de6e9b4dadec177bd95970735e3
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 07:53:03 2016 -0800

    sandwich: Aggregates metrics per URLs
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1694253002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375583}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: eb00ce505fec519a6ca3206e4e3d69e60837abd4

diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
index 1558243..df24f1c 100755
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -20,6 +20,7 @@ CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
 
 _CSV_FIELD_NAMES = [
     'id',
+    'url',
     'total_load',
     'onload',
     'browser_malloc_avg',
@@ -145,6 +146,10 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
     List of dictionaries with all _CSV_FIELD_NAMES's field set.
   """
   assert os.path.isdir(output_directory_path)
+  run_infos = None
+  with open(os.path.join(output_directory_path, 'run_infos.json')) as f:
+    run_infos = json.load(f)
+  assert run_infos
   metrics = []
   for node_name in os.listdir(output_directory_path):
     if not os.path.isdir(os.path.join(output_directory_path, node_name)):
@@ -161,6 +166,7 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
       trace = json.load(trace_file)
       trace_metrics = _PullMetricsFromTrace(trace)
       trace_metrics['id'] = page_id
+      trace_metrics['url'] = run_infos['urls'][page_id]
       metrics.append(trace_metrics)
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
index ad9fbe1..3c98508 100644
--- a/loading/pull_sandwich_metrics_unittest.py
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -146,6 +146,8 @@ class PageTrackTest(unittest.TestCase):
 
   def testCommandLine(self):
     tmp_dir = tempfile.mkdtemp()
+    with open(os.path.join(tmp_dir, 'run_infos.json'), 'w') as out_file:
+      json.dump({'urls': ['a.com', 'b.com', 'c.org']}, out_file)
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
       with open(os.path.join(tmp_dir, dirname, 'trace.json'), 'w') as out_file:
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index f72ba82..a99a5a3 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -312,6 +312,11 @@ def main():
   else:
     _CleanPreviousTraces(args.output)
 
+  run_infos = {
+    'cache-op': args.cache_op,
+    'job': args.job,
+    'urls': []
+  }
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
   local_cache_archive_path = os.path.join(args.output, 'cache.zip')
@@ -324,7 +329,6 @@ def main():
 
   with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
       args.disable_wpr_script_injection) as additional_flags:
-    pages_loaded = 0
     for _ in xrange(args.repeat):
       for url in job_urls:
         if args.cache_op == 'push':
@@ -333,7 +337,7 @@ def main():
         with device_setup.DeviceConnection(
             device=device,
             additional_flags=additional_flags) as connection:
-          if (pages_loaded == 0 and args.cache_op == 'save' or
+          if (not run_infos['urls'] and args.cache_op == 'save' or
               args.cache_op == 'clear'):
             connection.ClearCache()
           page_track.PageTrack(connection)
@@ -342,9 +346,9 @@ def main():
           connection.SetUpMonitoring()
           connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
           connection.StartMonitoring()
-          pages_loaded += 1
           _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
-              str(pages_loaded))
+              str(len(run_infos['urls'])))
+          run_infos['urls'].append(url)
 
   if local_cache_directory_path:
     shutil.rmtree(local_cache_directory_path)
@@ -360,6 +364,9 @@ def main():
     _ZipDirectoryContent(cache_directory_path, local_cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
+  with open(os.path.join(args.output, 'run_infos.json'), 'w') as file_output:
+    json.dump(run_infos, file_output, indent=2)
+
 
 if __name__ == '__main__':
   sys.exit(main())

commit 376bc675924d93f92310101039462fd3a2447709
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 16 07:46:38 2016 -0800

    Remove some noisy logging and optionally squash local chrome device output.
    
    This will make my upcoming webserver integration tests nicer.
    
    Review URL: https://codereview.chromium.org/1701723003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375580}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0cbea5abf37d765ec043f66edc51daed33ac3f75

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 73b9d85..b758eec 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -226,8 +226,10 @@ def DeviceConnection(device, additional_flags=None):
         local_profile_dir = TemporaryDirectory()
 
       new_flags.append('--user-data-dir=%s' % local_profile_dir)
-      host_process = subprocess.Popen([OPTIONS.local_binary] + new_flags,
-                                      shell=False)
+      chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+      host_process = subprocess.Popen(
+          [OPTIONS.local_binary] + new_flags,
+          shell=False, stdout=chrome_out, stderr=chrome_out)
     if device:
       time.sleep(2)
     else:
diff --git a/loading/options.py b/loading/options.py
index f477cd2..1d9ff72 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -28,6 +28,8 @@ class Options(object):
              'port for devtools websocket connection'),
             ('local_binary', 'out/Release/chrome',
              'chrome binary for local runs'),
+            ('local_noisy', False,
+             'Enable local chrome console output'),
             ('local_profile_dir', '',
              'profile directory to use for local runs'),
             ('no_sandbox', False,
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 61015d2..34293f8 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -41,7 +41,6 @@ def MonitorUrl(connection, url, clear_cache=False):
   Returns:
     loading_trace.LoadingTrace.
   """
-  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
   page = page_track.PageTrack(connection)
   request = request_track.RequestTrack(connection)
   trace = tracing.TracingTrack(connection)

commit c09a73626e42da061de215a88651108ca5fe52ef
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 07:00:47 2016 -0800

    tools/android/loading: Speed up trace events processing.
    
    The previous way to index trace events allowed for O(log n) query of
    in-flight events at a given time, and O(n) constrction time, but at the
    cost of replicating events up to p times (p being the maximum number of
    concurrent events). It turns out that p is fairly high.
    
    This replaces this with a tree, which is worse, but in practice faster.
    On a typical loading trace, creating the PNG graph goes from 3m30s to ~20s.
    
    Review URL: https://codereview.chromium.org/1694223002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375575}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1570bad784b3dd9fa1daf0bf125635a2b77f08cb

diff --git a/loading/tracing.py b/loading/tracing.py
index 37ee7ce..35907d3 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -7,6 +7,7 @@
 import bisect
 import itertools
 import logging
+import operator
 
 import devtools_monitor
 
@@ -40,11 +41,10 @@ class TracingTrack(devtools_monitor.Track):
 
     if connection:
       connection.SyncRequestNoResponse('Tracing.start', params)
-    self._events = []
 
-    self._event_msec_index = None
-    self._event_lists = None
+    self._events = []
     self._base_msec = None
+    self._interval_tree = None
 
   def Handle(self, method, event):
     for e in event['params']['value']:
@@ -52,10 +52,9 @@ class TracingTrack(devtools_monitor.Track):
       self._events.append(event)
       if self._base_msec is None or event.start_msec < self._base_msec:
         self._base_msec = event.start_msec
-    # Just invalidate our indices rather than trying to be fancy and
-    # incrementally update.
-    self._event_msec_index = None
-    self._event_lists = None
+    # Invalidate our index rather than trying to be fancy and incrementally
+    # update.
+    self._interval_tree = None
 
   def GetFirstEventMillis(self):
     """Find the canonical start time for this track.
@@ -80,18 +79,9 @@ class TracingTrack(devtools_monitor.Track):
       sample and counter) events are never included. Event end times are
       exclusive, so that an event ending at the usec parameter will not be
       returned.
-      TODO(mattcary): currently live objects are included. If this is too big we
-      may break that out into a separate index.
     """
     self._IndexEvents()
-    idx = bisect.bisect_right(self._event_msec_index, msec) - 1
-    if idx < 0:
-      return []
-    events = self._event_lists[idx]
-    assert events.start_msec <= msec
-    if not events or events.end_msec < msec:
-      return []
-    return events.event_list
+    return self._interval_tree.EventsAt(msec)
 
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
@@ -125,31 +115,30 @@ class TracingTrack(devtools_monitor.Track):
         tracing_track._base_msec = e.start_msec
     return tracing_track
 
-  def OverlappingEvents(self, start_msec, end_msec):
-    """Gets the list of events overlapping with an interval.
+  def _IndexEvents(self, strict=False):
+    if self._interval_tree:
+      return
+    complete_events = []
+    spanning_events = self._SpanningEvents()
+    for event in self._events:
+      if not event.IsIndexable():
+        continue
+      if event.IsComplete():
+        complete_events.append(event)
+        continue
+      matched_event = spanning_events.Match(event, strict)
+      if matched_event is not None:
+        complete_events.append(matched_event)
+    self._interval_tree = _IntervalTree.FromEvents(complete_events)
 
-    Args:
-      start_msec: the start of the range to query, in milliseconds, inclusive.
-      end_msec: the end of the range to query, in milliseconds, inclusive.
+    if strict and spanning_events.HasPending():
+      raise devtools_monitor.DevToolsConnectionException(
+          'Pending spanning events: %s' %
+          '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
 
-    Returns:
-      List of events overlapping with the range. Events are overlapping only if
-      the overlap is strictly larger than 0.
-    """
+  def OverlappingEvents(self, start_msec, end_msec):
     self._IndexEvents()
-    low_idx = bisect.bisect_left(self._event_msec_index, start_msec) - 1
-    high_idx = bisect.bisect_right(self._event_msec_index, end_msec)
-    matched_events = set()
-    for i in xrange(max(0, low_idx), high_idx):
-      if self._event_lists[i]:
-        for e in self._event_lists[i].event_list:
-          if e.end_msec is None:
-            continue
-          overlap_duration = max(
-              0, min(end_msec, e.end_msec) - max(start_msec, e.start_msec))
-          if overlap_duration > 0:
-            matched_events.add(e)
-    return list(matched_events)
+    return self._interval_tree.OverlappingEvents(start_msec, end_msec)
 
   def EventsEndingBetween(self, start_msec, end_msec):
     """Gets the list of events ending within an interval.
@@ -165,55 +154,9 @@ class TracingTrack(devtools_monitor.Track):
     return [e for e in overlapping_events
             if start_msec <= e.end_msec <= end_msec]
 
-  def _IndexEvents(self, strict=False):
-    """Computes index for in-flight events.
-
-    Creates a list of timestamps where events start or end, and tracks the
-    current set of in-flight events at the instant after each timestamp. To do
-    this we have to synthesize ending events for complete events, as well as
-    join and track the nesting of async, flow and other spanning events.
-
-    Events such as instant and counter events that aren't indexable are skipped.
-    """
-    if self._event_msec_index is not None:
-      return  # Already indexed.
-
-    if not self._events:
-      raise devtools_monitor.DevToolsConnectionException('No events to index')
-
-    self._event_msec_index = []
-    self._event_lists = []
-    synthetic_events = []
-    for e in self._events:
-      synthetic_events.extend(e.Synthesize())
-    synthetic_events.sort(key=lambda e: e.start_msec)
-    current_events = set()
-    next_idx = 0
-    spanning_events = self._SpanningEvents()
-    while next_idx < len(synthetic_events):
-      current_msec = synthetic_events[next_idx].start_msec
-      while next_idx < len(synthetic_events):
-        event = synthetic_events[next_idx]
-        assert event.IsIndexable()
-        if event.start_msec > current_msec:
-          break
-        matched_event = spanning_events.Match(event, strict)
-        if matched_event is not None:
-          event = matched_event
-        if not event.synthetic and (
-            event.end_msec is None or event.end_msec >= current_msec):
-          current_events.add(event)
-        next_idx += 1
-      current_events -= set([
-          e for e in current_events
-          if e.end_msec is not None and e.end_msec <= current_msec])
-      self._event_msec_index.append(current_msec)
-      self._event_lists.append(self._EventList(current_events))
-
-    if strict and spanning_events.HasPending():
-      raise devtools_monitor.DevToolsConnectionException(
-          'Pending spanning events: %s' %
-          '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
+  def _GetEvents(self):
+    self._IndexEvents()
+    return self._interval_tree.GetEvents()
 
   class _SpanningEvents(object):
     def __init__(self):
@@ -315,31 +258,6 @@ class TracingTrack(devtools_monitor.Track):
       start.SetClose(event)
       return start
 
-  class _EventList(object):
-    def __init__(self, events):
-      self._events = [e for e in events]
-      if self._events:
-        self._start_msec = min(e.start_msec for e in self._events)
-        # Event end times may be changed after this list is created so the end
-        # can't be cached.
-      else:
-        self._start_msec = self._end_msec = None
-
-    @property
-    def event_list(self):
-      return self._events
-
-    @property
-    def start_msec(self):
-      return self._start_msec
-
-    @property
-    def end_msec(self):
-      return max(e.end_msec for e in self._events)
-
-    def __nonzero__(self):
-      return bool(self._events)
-
 
 class Event(object):
   """Wraps a tracing event."""
@@ -411,6 +329,9 @@ class Event(object):
         'M'             # Metadata
         ]
 
+  def IsComplete(self):
+    return self.type == 'X'
+
   def Synthesize(self):
     """Expand into synthetic events.
 
@@ -421,7 +342,7 @@ class Event(object):
     """
     if not self.IsIndexable():
       return []
-    if self.type == 'X':
+    if self.IsComplete():
       # Tracing event timestamps are microseconds!
       return [self, Event({'ts': self.end_msec * 1000}, synthetic=True)]
     return [self]
@@ -455,3 +376,81 @@ class Event(object):
   @classmethod
   def FromJsonDict(cls, json_dict):
     return Event(json_dict)
+
+
+class _IntervalTree(object):
+  """Simple interval tree. This is not an optimal one, as the split is done with
+  an equal number of events on each side, according to start time.
+  """
+  _TRESHOLD = 100
+  def __init__(self, start, end, events):
+    """Builds an interval tree.
+
+    Args:
+      start: start timestamp of this node, in ms.
+      end: end timestamp covered by this node, in ms.
+      events: Iterable of objects having start_msec and end_msec fields. Has to
+              be sorted by start_msec.
+    """
+    self.start = start
+    self.end = end
+    self._events = events
+    self._left = self._right = None
+    if len(self._events) > self._TRESHOLD:
+      self._Divide()
+
+  @classmethod
+  def FromEvents(cls, events):
+    """Returns an IntervalTree instance from a list of events."""
+    filtered_events = [e for e in events
+                       if e.start_msec is not None and e.end_msec is not None]
+    filtered_events.sort(key=operator.attrgetter('start_msec'))
+    start = min(event.start_msec for event in filtered_events)
+    end = max(event.end_msec for event in filtered_events)
+    return _IntervalTree(start, end, filtered_events)
+
+  def OverlappingEvents(self, start, end):
+    if min(end, self.end) - max(start, self.start) <= 0:
+      return set()
+    elif self._IsLeaf():
+      result = set()
+      for event in self._events:
+        if self._Overlaps(event, start, end):
+          result.add(event)
+      return result
+    else:
+      return (self._left.OverlappingEvents(start, end)
+              | self._right.OverlappingEvents(start, end))
+
+  def EventsAt(self, timestamp):
+    result = set()
+    if self._IsLeaf():
+      for event in self._events:
+        if event.start_msec <= timestamp < event.end_msec:
+          result.add(event)
+    else:
+      if self._left.start <= timestamp < self._left.end:
+        result |= self._left.EventsAt(timestamp)
+      if self._right.start <= timestamp < self._right.end:
+        result |= self._right.EventsAt(timestamp)
+    return result
+
+  def GetEvents(self):
+    return self._events
+
+  def _Divide(self):
+    middle = len(self._events) / 2
+    left_events = self._events[:middle]
+    right_events = self._events[middle:]
+    left_end = max(e.end_msec for e in left_events)
+    right_start = min(e.start_msec for e in right_events)
+    self._left = _IntervalTree(self.start, left_end, left_events)
+    self._right = _IntervalTree(right_start, self.end, right_events)
+
+  def _IsLeaf(self):
+    return self._left is None
+
+  @classmethod
+  def _Overlaps(cls, event, start, end):
+    return (min(end, event.end_msec) - max(start, event.start_msec) > 0
+            or start <= event.start_msec < end)  # For instant events.
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 9c1ac3a..a2c9bce 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -2,13 +2,15 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import collections
 import copy
 import logging
+import operator
 import unittest
 
 import devtools_monitor
 
-from tracing import (Event, TracingTrack)
+from tracing import (Event, TracingTrack, _IntervalTree)
 
 
 class TracingTrackTestCase(unittest.TestCase):
@@ -34,8 +36,13 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 12, 'ph': 'X', 'dur': 3, 'tid': 1, 'args': {'name': 'E'}}]
 
   def setUp(self):
+    self.tree_threshold = _IntervalTree._TRESHOLD
+    _IntervalTree._TRESHOLD = 2  # Expose more edge cases in the tree.
     self.track = TracingTrack(None)
 
+  def tearDown(self):
+    _IntervalTree._TRESHOLD = self.tree_threshold
+
   def EventToMicroseconds(self, event):
     result = copy.deepcopy(event)
     if 'ts' in result:
@@ -257,5 +264,67 @@ class TracingTrackTestCase(unittest.TestCase):
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
 
+class IntervalTreeTestCase(unittest.TestCase):
+  class FakeEvent(object):
+    def __init__(self, start_msec, end_msec):
+      self.start_msec = start_msec
+      self.end_msec = end_msec
+
+    def __eq__(self, o):
+      return self.start_msec == o.start_msec and self.end_msec == o.end_msec
+
+  _COUNT = 1000
+
+  def testCreateTree(self):
+    events = [self.FakeEvent(100 * i, 100 * (i + 1))
+              for i in range(self._COUNT)]
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(0, tree.start)
+    self.assertEquals(100 * self._COUNT, tree.end)
+    self.assertFalse(tree._IsLeaf())
+
+  def testEventsAt(self):
+    events = ([self.FakeEvent(100 * i, 100 * (i + 1))
+               for i in range(self._COUNT)]
+              + [self.FakeEvent(100 * i + 50, 100 * i + 150)
+                 for i in range(self._COUNT)])
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(0, tree.start)
+    self.assertEquals(100 * self._COUNT + 50, tree.end)
+    self.assertFalse(tree._IsLeaf())
+    for i in range(self._COUNT):
+      self.assertEquals(2, len(tree.EventsAt(100 * i + 75)))
+    # Add instant events, check that they are excluded.
+    events += [self.FakeEvent(100 * i + 75, 100 * i + 75)
+               for i in range(self._COUNT)]
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(3 * self._COUNT, len(tree._events))
+    for i in range(self._COUNT):
+      self.assertEquals(2, len(tree.EventsAt(100 * i + 75)))
+
+  def testOverlappingEvents(self):
+    events = ([self.FakeEvent(100 * i, 100 * (i + 1))
+               for i in range(self._COUNT)]
+              + [self.FakeEvent(100 * i + 50, 100 * i + 150)
+                 for i in range(self._COUNT)])
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(0, tree.start)
+    self.assertEquals(100 * self._COUNT + 50, tree.end)
+    self.assertFalse(tree._IsLeaf())
+    # 400 -> 500, 450 -> 550, 500 -> 600
+    self.assertEquals(3, len(tree.OverlappingEvents(450, 550)))
+    overlapping = sorted(
+        tree.OverlappingEvents(450, 550), key=operator.attrgetter('start_msec'))
+    self.assertEquals(self.FakeEvent(400, 500), overlapping[0])
+    self.assertEquals(self.FakeEvent(450, 550), overlapping[1])
+    self.assertEquals(self.FakeEvent(500, 600), overlapping[2])
+    self.assertEquals(8, len(tree.OverlappingEvents(450, 800)))
+    # Add instant events, check that they are included.
+    events += [self.FakeEvent(500, 500) for i in range(10)]
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(3 + 10, len(tree.OverlappingEvents(450, 550)))
+    self.assertEquals(8 + 10, len(tree.OverlappingEvents(450, 800)))
+
+
 if __name__ == '__main__':
   unittest.main()

commit 1ef48539c8d1598197c5ac3d7310ad60b5ea06c7
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 06:39:07 2016 -0800

    sandwich: Adds command line flag to disable WPR injections
    
    By default, the Web Page Replay server automatically inject a
    javascript (deterministic.js) file that overrides Math.random()
    and Date() to implementations that return a deterministic value.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1698883002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375569}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9d7185f35638545a782d62619d5d73a9ac44552c

diff --git a/loading/device_setup.py b/loading/device_setup.py
index a056ec2..73b9d85 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -115,7 +115,8 @@ def _SetUpDevice(device, package_info):
 
 
 @contextlib.contextmanager
-def WprHost(device, wpr_archive_path, record=False):
+def WprHost(device, wpr_archive_path, record=False,
+            disable_script_injection=False):
   """Launches web page replay host.
 
   Args:
@@ -140,6 +141,11 @@ def WprHost(device, wpr_archive_path, record=False):
   else:
     assert os.path.exists(wpr_archive_path)
 
+  if disable_script_injection:
+    # Remove default WPR injected scripts like deterministic.js which
+    # overrides Math.random.
+    wpr_server_args.extend(['--inject_scripts', ''])
+
   # Deploy certification authority to the device.
   temp_certificate_dir = tempfile.mkdtemp()
   wpr_ca_cert_path = os.path.join(temp_certificate_dir, 'testca.pem')
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 9274819..f72ba82 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -296,6 +296,11 @@ def main():
                       help='Web page replay archive to load job\'s urls from.')
   parser.add_argument('--wpr-record', default=False, action='store_true',
                       help='Record web page replay archive.')
+  parser.add_argument('--disable-wpr-script-injection', default=False,
+                      action='store_true',
+                      help='Disable WPR default script injection such as ' +
+                          'overriding javascript\'s Math.random() and Date() ' +
+                          'with deterministic implementations.')
   args = parser.parse_args()
 
   if not os.path.isdir(args.output):
@@ -317,9 +322,8 @@ def main():
     local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
     _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
 
-  with device_setup.WprHost(device,
-                            args.wpr_archive,
-                            args.wpr_record) as additional_flags:
+  with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
+      args.disable_wpr_script_injection) as additional_flags:
     pages_loaded = 0
     for _ in xrange(args.repeat):
       for url in job_urls:

commit 4528545fb389017a73627909782f53e88ce0828e
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 05:40:23 2016 -0800

    sandwich: Pushes locally saved HTTP cache to the device.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1692873003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375559}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c240a2a6b423ed0afcc4c10ad9a03ddf544ede51

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 242cfbf..9274819 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -12,10 +12,12 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
+from datetime import datetime
 import json
 import logging
 import os
 import shutil
+import subprocess
 import sys
 import tempfile
 import time
@@ -59,6 +61,9 @@ _CHROME_PACKAGE = (
 # operations, such as opening the launcher activity.
 _TIME_TO_DEVICE_IDLE_SECONDS = 2
 
+# Cache directory's path on the device.
+_REMOTE_CACHE_DIRECTORY = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
+
 
 def _ReadUrlsFromJobDescription(job_name):
   """Retrieves the list of URLs associated with the job name."""
@@ -104,6 +109,17 @@ def _UpdateTimestampFromAdbStat(filename, stat):
   os.utime(filename, (stat.st_time, stat.st_time))
 
 
+def _AdbShell(adb, cmd):
+  adb.Shell(subprocess.list2cmdline(cmd))
+
+
+def _AdbUtime(adb, filename, timestamp):
+  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
+  """
+  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
+  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
+
+
 def _PullBrowserCache(device):
   """Pulls the browser cache from the device and saves it locally.
 
@@ -114,14 +130,13 @@ def _PullBrowserCache(device):
     Temporary directory containing all the browser cache.
   """
   save_target = tempfile.mkdtemp(suffix='.cache')
-  cache_directory = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
-  for filename, stat in device.adb.Ls(cache_directory):
+  for filename, stat in device.adb.Ls(_REMOTE_CACHE_DIRECTORY):
     if filename == '..':
       continue
     if filename == '.':
       cache_directory_stat = stat
       continue
-    original_file = os.path.join(cache_directory, filename)
+    original_file = os.path.join(_REMOTE_CACHE_DIRECTORY, filename)
     saved_file = os.path.join(save_target, filename)
     device.adb.Pull(original_file, saved_file)
     _UpdateTimestampFromAdbStat(saved_file, stat)
@@ -145,6 +160,35 @@ def _PullBrowserCache(device):
   return save_target
 
 
+def _PushBrowserCache(device, local_cache_path):
+  """Pushes the browser cache saved locally to the device.
+
+  Args:
+    device: Android device.
+    local_cache_path: The directory's path containing the cache locally.
+  """
+  # Clear previous cache.
+  _AdbShell(device.adb, ['rm', '-rf', _REMOTE_CACHE_DIRECTORY])
+  _AdbShell(device.adb, ['mkdir', _REMOTE_CACHE_DIRECTORY])
+
+  # Push cache content.
+  device.adb.Push(local_cache_path, _REMOTE_CACHE_DIRECTORY)
+
+  # Walk through the local cache to update mtime on the device.
+  def MirrorMtime(local_path):
+    cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
+    remote_path = os.path.join(_REMOTE_CACHE_DIRECTORY, cache_relative_path)
+    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
+
+  for local_directory_path, dirnames, filenames in os.walk(
+        local_cache_path, topdown=False):
+    for filename in filenames:
+      MirrorMtime(os.path.join(local_directory_path, filename))
+    for dirname in dirnames:
+      MirrorMtime(os.path.join(local_directory_path, dirname))
+  MirrorMtime(local_cache_path)
+
+
 def _ZipDirectoryContent(root_directory_path, archive_dest_path):
   """Zip a directory's content recursively with all the directories'
   timestamps preserved.
@@ -155,6 +199,10 @@ def _ZipDirectoryContent(root_directory_path, archive_dest_path):
   """
   with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
     timestamps = {}
+    root_directory_stats = os.stat(root_directory_path)
+    timestamps['.'] = {
+        'atime': root_directory_stats.st_atime,
+        'mtime': root_directory_stats.st_mtime}
     for directory_path, dirnames, filenames in os.walk(root_directory_path):
       for dirname in dirnames:
         subdirectory_path = os.path.join(directory_path, dirname)
@@ -210,6 +258,24 @@ def _UnzipDirectoryContent(archive_path, directory_dest_path):
       os.utime(output_path, (stats['atime'], stats['mtime']))
 
 
+def _CleanPreviousTraces(output_directories_path):
+  """Cleans previous traces from the output directory.
+
+  Args:
+    output_directories_path: The output directory path where to clean the
+        previous traces.
+  """
+  for dirname in os.listdir(output_directories_path):
+    directory_path = os.path.join(output_directories_path, dirname)
+    if not os.path.isdir(directory_path):
+      continue
+    try:
+      int(dirname)
+    except ValueError:
+      continue
+    shutil.rmtree(directory_path)
+
+
 def main():
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -221,35 +287,50 @@ def main():
                       help='Name of output directory to create.')
   parser.add_argument('--repeat', default=1, type=int,
                       help='How many times to run the job')
-  parser.add_argument('--save-cache', default=False,
-                      action='store_true',
-                      help='Clear HTTP cache before start,' +
-                      'save cache before exit.')
+  parser.add_argument('--cache-op',
+                      choices=['clear', 'save', 'push'],
+                      default='clear',
+                      help='Configures cache operation to do before launching '
+                          +'Chrome. (Default is clear).')
   parser.add_argument('--wpr-archive', default=None, type=str,
                       help='Web page replay archive to load job\'s urls from.')
   parser.add_argument('--wpr-record', default=False, action='store_true',
                       help='Record web page replay archive.')
   args = parser.parse_args()
 
-  try:
-    os.makedirs(args.output)
-  except OSError:
-    logging.error('Cannot create directory for results: %s' % args.output)
-    raise
+  if not os.path.isdir(args.output):
+    try:
+      os.makedirs(args.output)
+    except OSError:
+      logging.error('Cannot create directory for results: %s' % args.output)
+      raise
+  else:
+    _CleanPreviousTraces(args.output)
 
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
+  local_cache_archive_path = os.path.join(args.output, 'cache.zip')
+  local_cache_directory_path = None
+
+  if args.cache_op == 'push':
+    assert os.path.isfile(local_cache_archive_path)
+    local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+    _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
 
   with device_setup.WprHost(device,
                             args.wpr_archive,
                             args.wpr_record) as additional_flags:
     pages_loaded = 0
-    for iteration in xrange(args.repeat):
+    for _ in xrange(args.repeat):
       for url in job_urls:
+        if args.cache_op == 'push':
+          device.KillAll(_CHROME_PACKAGE, quiet=True)
+          _PushBrowserCache(device, local_cache_directory_path)
         with device_setup.DeviceConnection(
             device=device,
             additional_flags=additional_flags) as connection:
-          if iteration == 0 and pages_loaded == 0 and args.save_cache:
+          if (pages_loaded == 0 and args.cache_op == 'save' or
+              args.cache_op == 'clear'):
             connection.ClearCache()
           page_track.PageTrack(connection)
           tracing_track = tracing.TracingTrack(connection,
@@ -261,7 +342,10 @@ def main():
           _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
               str(pages_loaded))
 
-  if args.save_cache:
+  if local_cache_directory_path:
+    shutil.rmtree(local_cache_directory_path)
+
+  if args.cache_op == 'save':
     # Move Chrome to background to allow it to flush the index.
     device.adb.Shell('am start com.google.android.launcher')
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
@@ -269,8 +353,7 @@ def main():
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
 
     cache_directory_path = _PullBrowserCache(device)
-    _ZipDirectoryContent(cache_directory_path,
-                         os.path.join(args.output, 'cache.zip'))
+    _ZipDirectoryContent(cache_directory_path, local_cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
 

commit 02f7586737a55729a5a341726f82a209c997633c
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 16 01:42:47 2016 -0800

    Tweak device_setup.py so that chrome is killed more reliably.
    
    Review URL: https://codereview.chromium.org/1698193003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375542}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 440f20a7611ff629cf55174439c00cf0d4c976f7

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 728bf75..a056ec2 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -177,6 +177,7 @@ def WprHost(device, wpr_archive_path, record=False):
     device_cert_util.remove_cert()
     shutil.rmtree(temp_certificate_dir)
 
+
 @contextlib.contextmanager
 def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
@@ -210,7 +211,8 @@ def DeviceConnection(device, additional_flags=None):
           data='about:blank')
       device.StartActivity(start_intent, blocking=True)
     else:
-      # Run on the host.
+      # Run on the host. We don't care about startup time so will skip the about
+      # page.
       assert os.path.exists(OPTIONS.local_binary)
 
       local_profile_dir = OPTIONS.local_profile_dir
@@ -223,13 +225,15 @@ def DeviceConnection(device, additional_flags=None):
     if device:
       time.sleep(2)
     else:
-      # TODO(blundell): Figure out why a lower sleep time causes an assertion
-      # in request_track.py to fire.
+      # TODO(mattcary): This seems to be related to chrome startup. There should
+      # be a way to ping chrome --- maybe keep trying to connect to the devtools
+      # port?
       time.sleep(10)
-    # If no device, we don't care about chrome startup so skip the about page.
-    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
-                     'localabstract:chrome_devtools_remote'):
-      yield devtools_monitor.DevToolsConnection(
-          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-    if host_process:
-      host_process.kill()
+    try:
+      with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
+                       'localabstract:chrome_devtools_remote'):
+        yield devtools_monitor.DevToolsConnection(
+            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+    finally:
+      if host_process:
+        host_process.kill()

commit d48e82cf18707ea2d0a29fbd887cd1d2698b0d3d
Author: lizeb <lizeb@chromium.org>
Date:   Mon Feb 15 09:07:58 2016 -0800

    tools/android/loading: Add the renderer activity to the PNG output.
    
    Adds edge annotations showing the amount of time spent executing the
    initiating script, parsing the initiating document, and doing "other"
    work.
    
    Review URL: https://codereview.chromium.org/1681103002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375469}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bce5e88fe35eff0cd975f28ecbbfd2ae3b66ba60

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index 2973ac4..92ffd14 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -12,6 +12,8 @@ import collections
 import logging
 import operator
 
+import request_track
+
 
 class ActivityLens(object):
   """Reconstructs the activity of the main renderer thread between requests."""
@@ -24,6 +26,8 @@ class ActivityLens(object):
     self._trace = trace
     events = trace.tracing_track.GetEvents()
     self._renderer_main_tid = self._GetRendererMainThreadId(events)
+    self._tracing = self._trace.tracing_track.TracingTrackForThread(
+        self._renderer_main_tid)
 
   @classmethod
   def _GetRendererMainThreadId(cls, events):
@@ -60,9 +64,8 @@ class ActivityLens(object):
              tid_events_counts[0][1], tid_events_counts[1][1]))
     return tid_events_counts[0][0]
 
-  def _OverlappingEventsForTid(self, tid, start_msec, end_msec):
-    events = self._trace.tracing_track.OverlappingEvents(start_msec, end_msec)
-    return [e for e in events if e.tracing_event['tid'] == tid]
+  def _OverlappingMainRendererThreadEvents(self, start_msec, end_msec):
+    return self._tracing.OverlappingEvents(start_msec, end_msec)
 
   @classmethod
   def _ClampedDuration(cls, event, start_msec, end_msec):
@@ -136,7 +139,7 @@ class ActivityLens(object):
       url_to_duration[url] += clamped_duration
     return dict(url_to_duration)
 
-  def ExplainEdgeCost(self, dep):
+  def GenerateEdgeActivity(self, dep):
     """For a dependency between two requests, returns the renderer activity
     breakdown.
 
@@ -148,19 +151,48 @@ class ActivityLens(object):
       {'edge_cost': (float) ms, 'busy': (float) ms,
        'parsing': {'url' -> time_ms}, 'script' -> {'url' -> time_ms}}
     """
-    (first, second, _) = dep
-    # TODO(lizeb): Refactor the edge cost computations.
-    start_msec = first.start_msec
-    end_msec = second.start_msec
+    (first, second, reason) = dep
+    (start_msec, end_msec) = request_track.IntervalBetween(
+        first, second, reason)
     assert end_msec - start_msec >= 0.
-    tid = self._renderer_main_tid
-    events = self._OverlappingEventsForTid(tid, start_msec, end_msec)
+    events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
     result = {'edge_cost': end_msec - start_msec,
               'busy': self._ThreadBusiness(events, start_msec, end_msec),
               'parsing': self._Parsing(events, start_msec, end_msec),
               'script': self._ScriptsExecuting(events, start_msec, end_msec)}
     return result
 
+  def BreakdownEdgeActivityByInitiator(self, dep):
+    """For a dependency between two requests, categorizes the renderer activity.
+
+    Args:
+      dep: (Request, Request, str) As returned from
+           RequestDependencyLens.GetRequestDependencies().
+
+    Returns:
+      {'script': float, 'parsing': float, 'other': float, 'unknown': float}
+      where the values are durations in ms:
+      - script: The initiating file was executing.
+      - parsing: The initiating file was being parsed.
+      - other: Other scripts and/or parsing activities.
+      - unknown: Activity which is not associated with a URL.
+    """
+    activity = self.GenerateEdgeActivity(dep)
+    related = {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 0}
+    for kind in ('script', 'parsing'):
+      for (script_name, duration_ms) in activity[kind].items():
+        if not script_name:
+          related['unknown_url'] += duration_ms
+        elif script_name == dep[0].url:
+          related[kind] += duration_ms
+        else:
+          # A lot of "ParseHTML" tasks are mostly about executing
+          # scripts. Don't double-count.
+          # TODO(lizeb): Better handle TraceEvents nesting.
+          if kind == 'script':
+            related['other_url'] += duration_ms
+    return related
+
 
 if __name__ == '__main__':
   import sys
@@ -176,4 +208,4 @@ if __name__ == '__main__':
       loading_trace)
   deps = dependencies_lens.GetRequestDependencies()
   for requests_dep in deps:
-    print activity_lens.ExplainEdgeCost(requests_dep)
+    print activity_lens.GenerateEdgeActivity(requests_dep)
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index a84c864..3b4062f 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -5,6 +5,7 @@
 import unittest
 
 from activity_lens import ActivityLens
+import test_utils
 import tracing
 
 
@@ -170,6 +171,63 @@ class ActivityLensTestCast(unittest.TestCase):
     self.assertTrue(html_url in ActivityLens._Parsing(events, 0, 1000))
     self.assertEquals(42, ActivityLens._Parsing(events, 0, 1000)[html_url])
 
+  def testBreakdownEdgeActivityByInitiator(self):
+    requests = [test_utils.MakeRequest(0, 1, 10, 20, 30),
+                test_utils.MakeRequest(0, 1, 50, 60, 70)]
+    raw_events = [
+        {u'args': {u'beginData': {u'url': requests[0].url}},
+         u'cat': u'devtools.timeline',
+         u'dur': 12 * 1000,
+         u'name': u'ParseHTML',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 1,
+         u'ts': 25 * 1000},
+        {u'args': {u'data': {'scriptName': requests[0].url}},
+         u'cat': u'devtools.timeline,v8',
+         u'dur': 0,
+         u'name': u'EvaluateScript',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 1,
+         u'ts': 0}]
+    activity = self._ActivityLens(requests, raw_events)
+    dep = (requests[0], requests[1], 'parser')
+    self.assertEquals(
+        {'script': 0, 'parsing': 12, 'other_url': 0, 'unknown_url': 0},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    dep = (requests[0], requests[1], 'other')
+    # Truncating the event from the parent xrequest end.
+    self.assertEquals(
+        {'script': 0, 'parsing': 7, 'other_url': 0, 'unknown_url': 0},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    # Unknown URL
+    raw_events[0]['args']['beginData']['url'] = None
+    activity = self._ActivityLens(requests, raw_events)
+    dep = (requests[0], requests[1], 'parser')
+    self.assertEquals(
+        {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 12},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    # Script
+    raw_events[1]['ts'] = 40 * 1000
+    raw_events[1]['dur'] = 6 * 1000
+    activity = self._ActivityLens(requests, raw_events)
+    dep = (requests[0], requests[1], 'script')
+    self.assertEquals(
+        {'script': 6, 'parsing': 0, 'other_url': 0, 'unknown_url': 7},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    # Other URL
+    raw_events[1]['args']['data']['scriptName'] = 'http://other.com/url'
+    activity = self._ActivityLens(requests, raw_events)
+    self.assertEquals(
+        {'script': 0., 'parsing': 0., 'other_url': 6., 'unknown_url': 7.},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+
+  def _ActivityLens(self, requests, raw_events):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        requests, None, raw_events)
+    return ActivityLens(loading_trace)
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/analyze.py b/loading/analyze.py
index fdf18da..1c31de0 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -24,6 +24,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 from pylib import constants
 
+import activity_lens
 import content_classification_lens
 import device_setup
 import frame_load_lens
@@ -158,7 +159,9 @@ def _ProcessRequests(filename):
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
             trace, OPTIONS.ad_rules, OPTIONS.tracking_rules))
     frame_lens = frame_load_lens.FrameLoadLens(trace)
-    graph = loading_model.ResourceGraph(trace, content_lens, frame_lens)
+    activity = activity_lens.ActivityLens(trace)
+    graph = loading_model.ResourceGraph(
+        trace, content_lens, frame_lens, activity)
     if OPTIONS.noads:
       graph.Set(node_filter=graph.FilterAds)
     return graph
diff --git a/loading/loading_model.py b/loading/loading_model.py
index d2889e8..5deec03 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -20,17 +20,22 @@ import os
 import urlparse
 import sys
 
+import activity_lens
 import dag
 import loading_trace
 import request_dependencies_lens
+import request_track
 
 class ResourceGraph(object):
-  """A model of loading by a DAG (tree?) of resource dependancies.
+  """A model of loading by a DAG of resource dependencies.
 
-  Set parameters:
-    cache_all: if true, assume zero loading time for all resources.
+  See model parameters in Set().
   """
-  def __init__(self, trace, content_lens=None, frame_lens=None):
+  EDGE_KIND_KEY = 'edge_kind'
+  EDGE_KINDS = request_track.Request.INITIATORS + (
+      'script_inferred', 'after-load', 'before-load', 'timing')
+  def __init__(self, trace, content_lens=None, frame_lens=None,
+               activity=None):
     """Create from a LoadingTrace (or json of a trace).
 
     Args:
@@ -38,12 +43,15 @@ class ResourceGraph(object):
       content_lens: (ContentClassificationLens) Lens used to annotate the
                     nodes, or None.
       frame_lens: (FrameLoadLens) Lens used to augment graph with load nodes.
+      activity:   (ActivityLens) Lens used to augment the edges with the
+                   activity.
     """
     if type(trace) == dict:
       trace = loading_trace.LoadingTrace.FromJsonDict(trace)
     self._trace = trace
     self._content_lens = content_lens
     self._frame_lens = frame_lens
+    self._activity_lens = activity
     self._BuildDag(trace)
     # Sort before splitting children so that we can correctly dectect if a
     # reparented child is actually a dependency for a child of its new parent.
@@ -58,7 +66,7 @@ class ResourceGraph(object):
 
   @classmethod
   def CheckImageLoadConsistency(cls, g1, g2):
-    """Check that images have the same dependancies between ResourceGraphs.
+    """Check that images have the same dependencies between ResourceGraphs.
 
     Image resources are identified by their short names.
 
@@ -101,10 +109,10 @@ class ResourceGraph(object):
     """Set model parameters.
 
     TODO(mattcary): add parameters for caching certain types of resources (just
-    scripts, just cachable, etc).
+    scripts, just cacheable, etc).
 
     Args:
-      cache_all: boolean that if true ignores emperical resource load times for
+      cache_all: boolean that if true ignores empirical resource load times for
         all resources.
       node_filter: a Node->boolean used to restrict the graph for most
         operations.
@@ -167,7 +175,6 @@ class ResourceGraph(object):
       if self._node_filter(n.Node()) and n.Url() in other_map:
         yield(n, other_map[n.Url()])
 
-
   def Cost(self, path_list=None):
     """Compute cost of current model.
 
@@ -279,9 +286,9 @@ class ResourceGraph(object):
     """Convenience function for redirecting to NodeInfo."""
     return self.NodeInfo(parent).EdgeCost(self.NodeInfo(child))
 
-  def EdgeAnnotation(self, parent, child):
+  def EdgeAnnotations(self, parent, child):
     """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(parent).EdgeAnnotation(self.NodeInfo(child))
+    return self.NodeInfo(parent).EdgeAnnotations(self.NodeInfo(child))
 
   ##
   ## Internal items
@@ -372,7 +379,7 @@ class ResourceGraph(object):
     def EndTime(self):
       return self.StartTime() + self._node_cost
 
-    def EdgeAnnotation(self, s):
+    def EdgeAnnotations(self, s):
       assert s.Node() in self.Node().Successors()
       return self._edge_annotations.get(s, [])
 
@@ -411,9 +418,9 @@ class ResourceGraph(object):
       assert child.Node() in self._node.Successors()
       self._edge_costs[child] = cost
 
-    def AddEdgeAnnotation(self, s, annotation):
+    def AddEdgeAnnotations(self, s, annotations):
       assert s.Node() in self._node.Successors()
-      self._edge_annotations.setdefault(s, []).append(annotation)
+      self._edge_annotations.setdefault(s, {}).update(annotations)
 
     def ReparentTo(self, old_parent, new_parent):
       """Move costs and annotatations from old_parent to new_parent.
@@ -429,13 +436,12 @@ class ResourceGraph(object):
       """
       assert old_parent.Node() in self.Node().Predecessors()
       assert new_parent.Node() not in self.Node().Predecessors()
-      edge_annotations = old_parent._edge_annotations.pop(self, [])
+      edge_annotations = old_parent._edge_annotations.pop(self, {})
       edge_cost =  old_parent._edge_costs.pop(self)
       old_parent.Node().RemoveSuccessor(self.Node())
       new_parent.Node().AddSuccessor(self.Node())
       new_parent.SetEdgeCost(self, edge_cost)
-      for a in edge_annotations:
-        new_parent.AddEdgeAnnotation(self, a)
+      new_parent.AddEdgeAnnotations(self, edge_annotations)
 
     def __eq__(self, o):
       """Note this works whether o is a Node or a NodeInfo."""
@@ -473,10 +479,11 @@ class ResourceGraph(object):
 
     dependencies = request_dependencies_lens.RequestDependencyLens(
         trace).GetRequestDependencies()
-    for parent_rq, child_rq, reason in dependencies:
+    for dep in dependencies:
+      (parent_rq, child_rq, reason) = dep
       parent = self._node_info[index_by_request[parent_rq]]
       child = self._node_info[index_by_request[child_rq]]
-      edge_cost = child.StartTime() - parent.EndTime()
+      edge_cost = request_track.TimeBetween(parent_rq, child_rq, reason)
       if edge_cost < 0:
         edge_cost = 0
         if child.StartTime() < parent.StartTime():
@@ -486,7 +493,10 @@ class ResourceGraph(object):
           # fair amount in practice.
       parent.Node().AddSuccessor(child.Node())
       parent.SetEdgeCost(child, edge_cost)
-      parent.AddEdgeAnnotation(child, reason)
+      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: reason})
+      if self._activity_lens:
+        activity = self._activity_lens.BreakdownEdgeActivityByInitiator(dep)
+        parent.AddEdgeAnnotations(child, {'activity': activity})
 
     self._AugmentFrameLoads(index_by_request)
 
@@ -507,12 +517,12 @@ class ResourceGraph(object):
       parent = self._node_info[load_index_to_node[load_idx]]
       child = self._node_info[index_by_request[rq]]
       parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotation(child, 'after-load')
+      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'after-load'})
     for rq, load_idx in frame_deps[1]:
       child = self._node_info[load_index_to_node[load_idx]]
       parent = self._node_info[index_by_request[rq]]
       parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotation(child, 'before-load')
+      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'before-load'})
 
   def _SplitChildrenByTime(self, parent):
     """Split children of a node by request times.
@@ -535,7 +545,7 @@ class ResourceGraph(object):
     This is refined by only considering assets which we believe actually create
     a dependency. We only split if the original parent is a script, and the new
     parent a data file. We confirm these relationships heuristically by loading
-    pages multiple times and ensuring that dependacies do not change; see
+    pages multiple times and ensuring that dependencies do not change; see
     CheckImageLoadConsistency() for details.
 
     We incorporate this heuristic by skipping over any non-script/json resources
@@ -583,7 +593,8 @@ class ResourceGraph(object):
                   # eligible.
       if children_by_end_time[end_mark].EndTime() <= current.StartTime():
         current.ReparentTo(parent, children_by_end_time[end_mark])
-        children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
+        children_by_end_time[end_mark].AddEdgeAnnotations(
+            current, {self.EDGE_KIND_KEY: 'timing'})
 
   def _ExtractImages(self):
     """Return interesting image resources.
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 7afe8ef..44dcbae 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -24,8 +24,9 @@ class SimpleLens(object):
       assert rq.url not in url_to_rq
       url_to_rq[rq.url] = rq
     for rq in self._trace.request_track.GetEvents():
-      if rq.initiator in url_to_rq:
-        deps.append(( url_to_rq[rq.initiator], rq, ''))
+      initiating_url = rq.initiator['url']
+      if initiating_url in url_to_rq:
+        deps.append((url_to_rq[initiating_url], rq, rq.initiator['type']))
     return deps
 
 
@@ -34,31 +35,10 @@ class LoadingModelTestCase(unittest.TestCase):
   def setUp(self):
     self.old_lens = request_dependencies_lens.RequestDependencyLens
     request_dependencies_lens.RequestDependencyLens = SimpleLens
-    self._next_request_id = 0
 
   def tearDown(self):
     request_dependencies_lens.RequestDependencyLens = self.old_lens
 
-  def MakeParserRequest(self, url, source_url, start_time, end_time,
-                        magic_content_type=False):
-    timing = request_track.TimingAsList(request_track.TimingFromDict({
-        # connectEnd should be ignored.
-        'connectEnd': (end_time - start_time) / 2,
-        'receiveHeadersEnd': end_time - start_time,
-        'requestTime': start_time / 1000.0}))
-    rq = request_track.Request.FromJsonDict({
-        'timestamp': start_time / 1000.0,
-        'request_id': self._next_request_id,
-        'url': 'http://' + str(url),
-        'initiator': 'http://' + str(source_url),
-        'response_headers': {'Content-Type':
-                             'null' if not magic_content_type
-                             else 'magic-debug-content' },
-        'timing': timing
-        })
-    self._next_request_id += 1
-    return rq
-
   def MakeGraph(self, requests):
     return loading_model.ResourceGraph(
         test_utils.LoadingTraceFromEvents(requests))
@@ -72,10 +52,11 @@ class LoadingModelTestCase(unittest.TestCase):
   def test_DictConstruction(self):
     graph = loading_model.ResourceGraph(
         {'request_track': {
-            'events': [self.MakeParserRequest(0, 'null', 100, 101).ToJsonDict(),
-                       self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
-                       self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
-                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()],
+            'events': [
+                test_utils.MakeRequest(0, 'null', 100, 100.5, 101).ToJsonDict(),
+                test_utils.MakeRequest(1, 0, 102, 102.5, 103).ToJsonDict(),
+                test_utils.MakeRequest(2, 0, 102, 102.5, 103).ToJsonDict(),
+                test_utils.MakeRequest(3, 2, 104, 114.5, 105).ToJsonDict()],
             'metadata': {
                 request_track.RequestTrack._DUPLICATES_KEY: 0,
                 request_track.RequestTrack._INCONSISTENT_INITIATORS_KEY: 0}},
@@ -89,13 +70,13 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
 
   def test_Costing(self):
-    requests = [self.MakeParserRequest(0, 'null', 100, 110),
-                self.MakeParserRequest(1, 0, 115, 120),
-                self.MakeParserRequest(2, 0, 112, 120),
-                self.MakeParserRequest(3, 1, 122, 126),
-                self.MakeParserRequest(4, 3, 127, 128),
-                self.MakeParserRequest(5, 'null', 100, 105),
-                self.MakeParserRequest(6, 5, 105, 110)]
+    requests = [test_utils.MakeRequest(0, 'null', 100, 105, 110),
+                test_utils.MakeRequest(1, 0, 115, 117, 120),
+                test_utils.MakeRequest(2, 0, 112, 116, 120),
+                test_utils.MakeRequest(3, 1, 122, 124, 126),
+                test_utils.MakeRequest(4, 3, 127, 127.5, 128),
+                test_utils.MakeRequest(5, 'null', 100, 103, 105),
+                test_utils.MakeRequest(6, 5, 105, 107, 110)]
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
@@ -110,16 +91,16 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(8, graph.Cost())
 
   def test_MaxPath(self):
-    requests = [self.MakeParserRequest(0, 'null', 100, 110),
-                self.MakeParserRequest(1, 0, 115, 120),
-                self.MakeParserRequest(2, 0, 112, 120),
-                self.MakeParserRequest(3, 1, 122, 126),
-                self.MakeParserRequest(4, 3, 127, 128),
-                self.MakeParserRequest(5, 'null', 100, 105),
-                self.MakeParserRequest(6, 5, 105, 110)]
+    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 111),
+                test_utils.MakeRequest(1, 0, 115, 120, 121),
+                test_utils.MakeRequest(2, 0, 112, 120, 121),
+                test_utils.MakeRequest(3, 1, 122, 126, 127),
+                test_utils.MakeRequest(4, 3, 127, 128, 129),
+                test_utils.MakeRequest(5, 'null', 100, 105, 106),
+                test_utils.MakeRequest(6, 5, 105, 110, 111)]
     graph = self.MakeGraph(requests)
     path_list = []
-    self.assertEqual(28, graph.Cost(path_list))
+    self.assertEqual(29, graph.Cost(path_list))
     self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
 
     # More interesting would be a test when a node has multiple predecessors,
@@ -127,16 +108,17 @@ class LoadingModelTestCase(unittest.TestCase):
 
   def test_TimingSplit(self):
     # Timing adds node 1 as a parent to 2 but not 3.
-    requests = [self.MakeParserRequest(0, 'null', 100, 110,
-                                       magic_content_type=True),
-                self.MakeParserRequest(1, 0, 115, 120,
-                                       magic_content_type=True),
-                self.MakeParserRequest(2, 0, 121, 122,
-                                       magic_content_type=True),
-                self.MakeParserRequest(3, 0, 112, 119,
-                                       magic_content_type=True),
-                self.MakeParserRequest(4, 2, 122, 126),
-                self.MakeParserRequest(5, 2, 122, 126)]
+    requests = [
+        test_utils.MakeRequest(0, 'null', 100, 110, 110,
+                               magic_content_type=True),
+        test_utils.MakeRequest(1, 0, 115, 120, 120,
+                               magic_content_type=True),
+        test_utils.MakeRequest(2, 0, 121, 122, 122,
+                               magic_content_type=True),
+        test_utils.MakeRequest(3, 0, 112, 119, 119,
+                               magic_content_type=True),
+        test_utils.MakeRequest(4, 2, 122, 126, 126),
+        test_utils.MakeRequest(5, 2, 122, 126, 126)]
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
@@ -147,8 +129,8 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
     # Change node 1 so it is a parent of 3, which becomes the parent of 2.
-    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
-                                         magic_content_type=True)
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
@@ -159,13 +141,13 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
     # Add an initiator dependence to 1 that will become the parent of 3.
-    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
-                                         magic_content_type=True)
-    requests.append(self.MakeParserRequest(6, 1, 111, 112))
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
+    requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
     graph = self.MakeGraph(requests)
     # Check it doesn't change until we change the content type of 6.
     self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
-    requests[6] = self.MakeParserRequest(6, 1, 111, 112,
+    requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
                                          magic_content_type=True)
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
@@ -179,12 +161,12 @@ class LoadingModelTestCase(unittest.TestCase):
 
   def test_TimingSplitImage(self):
     # If we're all image types, then we shouldn't split by timing.
-    requests = [self.MakeParserRequest(0, 'null', 100, 110),
-                self.MakeParserRequest(1, 0, 115, 120),
-                self.MakeParserRequest(2, 0, 121, 122),
-                self.MakeParserRequest(3, 0, 112, 119),
-                self.MakeParserRequest(4, 2, 122, 126),
-                self.MakeParserRequest(5, 2, 122, 126)]
+    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
+                test_utils.MakeRequest(1, 0, 115, 120, 120),
+                test_utils.MakeRequest(2, 0, 121, 122, 122),
+                test_utils.MakeRequest(3, 0, 112, 119, 119),
+                test_utils.MakeRequest(4, 2, 122, 126, 126),
+                test_utils.MakeRequest(5, 2, 122, 126, 126)]
     for r in requests:
       r.response_headers['Content-Type'] = 'image/gif'
     graph = self.MakeGraph(requests)
diff --git a/loading/model_graph.py b/loading/model_graph.py
index 5c0bf09..591bd7f 100644
--- a/loading/model_graph.py
+++ b/loading/model_graph.py
@@ -7,6 +7,8 @@
 import dag
 import itertools
 
+import loading_model
+
 
 class GraphVisualization(object):
   """Manipulate visual representations of a resource graph.
@@ -23,6 +25,7 @@ class GraphVisualization(object):
       'font':            'grey70',
       'image':           'orange',    # This probably catches gifs?
       'video':           'hotpink1',
+      'audio':           'hotpink2',
       }
 
   _CONTENT_TYPE_TO_COLOR = {
@@ -41,6 +44,19 @@ class GraphVisualization(object):
       'synthetic':       'yellow',
       }
 
+  _EDGE_KIND_TO_COLOR = {
+    'redirect': 'black',
+    'parser': 'red',
+    'script': 'blue',
+    'script_inferred': 'purple',
+    'after-load': 'forestgreen',
+    'before-load': 'forestgreen',
+  }
+
+  _ACTIVITY_TYPE_LABEL = (
+      ('script', 'S'), ('parsing', 'P'), ('other_url', 'O'),
+      ('unknown_url', 'U'))
+
   def __init__(self, graph):
     """Initialize.
 
@@ -98,22 +114,27 @@ class GraphVisualization(object):
         if s not in visited_nodes:
           continue
         style = 'color = orange'
-        annotations = self._graph.EdgeAnnotation(n, s)
-        if 'redirect' in annotations:
-          style = 'color = black'
-        elif 'parser' in annotations:
-          style = 'color = red'
-        elif 'stack' in annotations:
-          style = 'color = blue'
-        elif 'script_inferred' in annotations:
-          style = 'color = purple'
-        if 'after-load' in annotations or 'before-load' in annotations:
-          style = 'color = forestgreen'
-        if 'timing' in annotations:
+        label = '%.02f' % self._graph.EdgeCost(n, s)
+        annotations = self._graph.EdgeAnnotations(n, s)
+        edge_kind = annotations.get(
+            loading_model.ResourceGraph.EDGE_KIND_KEY, None)
+        assert ((edge_kind is None)
+                or (edge_kind in loading_model.ResourceGraph.EDGE_KINDS))
+        style = 'color = %s' % self._EDGE_KIND_TO_COLOR[edge_kind]
+        if edge_kind == 'timing':
           style += '; style=dashed'
         if self._graph.EdgeCost(n, s) > self._LONG_EDGE_THRESHOLD_MS:
           style += '; penwidth=5; weight=2'
-        arrow = '[%s; label="%s"]' % (style, self._graph.EdgeCost(n, s))
+
+        label = '%.02f' % self._graph.EdgeCost(n, s)
+        if 'activity' in annotations:
+          activity = annotations['activity']
+          separator = ' - '
+          for activity_type, activity_label in self._ACTIVITY_TYPE_LABEL:
+            label += '%s%s:%.02f ' % (
+                separator, activity_label, activity[activity_type])
+            separator = ' '
+        arrow = '[%s; label="%s"]' % (style, label)
         output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
     output.write('}\n')
 
diff --git a/loading/model_graph_unittest.py b/loading/model_graph_unittest.py
index 510e945..831b1e1 100644
--- a/loading/model_graph_unittest.py
+++ b/loading/model_graph_unittest.py
@@ -27,3 +27,7 @@ class ModelGraphTestCase(unittest.TestCase):
       graph = loading_model.ResourceGraph(trace=trace, frame_lens=frame_lens)
       visualization = model_graph.GraphVisualization(graph)
       visualization.OutputDot(tmp)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index a19c5e5..89e3a5d 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -29,6 +29,40 @@ _TIMING_NAMES_MAPPING = {
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
 
+def IntervalBetween(first, second, reason):
+  """Returns the start and end of the inteval between two requests, in ms.
+
+  This is defined as:
+  - [first.headers, second.start] if reason is 'parser'. This is to account
+    for incremental parsing.
+  - [first.end, second.start] if reason is 'script', 'redirect' or 'other'.
+
+  Args:
+    first: (Request) First request.
+    second: (Request) Second request.
+    reason: (str) Link between the two requests, in Request.INITIATORS.
+
+  Returns:
+    (start_msec (float), end_msec (float)),
+  """
+  assert reason in Request.INITIATORS
+  second_ms = second.timing.request_time * 1000
+  if reason == 'parser':
+    first_offset_ms = first.timing.receive_headers_end
+  else:
+    first_offset_ms = max(
+        [0] + [t for f, t in first.timing._asdict().iteritems()
+               if f != 'request_time'])
+  return (first.timing.request_time * 1000 + first_offset_ms, second_ms)
+
+
+def TimeBetween(first, second, reason):
+  """(end_msec - start_msec), with the values as returned by IntervalBetween().
+  """
+  (first_ms, second_ms) = IntervalBetween(first, second, reason)
+  return second_ms - first_ms
+
+
 def TimingAsList(timing):
   """Transform Timing to a list, eg as is used in JSON output.
 
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index cad4a11..7a85f07 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -6,7 +6,32 @@ import copy
 import json
 import unittest
 
-from request_track import (Request, RequestTrack, TimingFromDict)
+from request_track import (TimeBetween, Request, RequestTrack, TimingFromDict)
+
+
+class TimeBetweenTestCase(unittest.TestCase):
+  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'request_id': '1234.1',
+                                   'frame_id': '123.1',
+                                   'initiator': {'type': 'other'},
+                                   'timestamp': 2,
+                                   'timing': TimingFromDict({})})
+  def setUp(self):
+    super(TimeBetweenTestCase, self).setUp()
+    self.first = copy.deepcopy(self._REQUEST)
+    self.first.timing = TimingFromDict({'requestTime': 123456,
+                                        'receiveHeadersEnd': 100,
+                                        'loadingFinished': 500})
+    self.second = copy.deepcopy(self._REQUEST)
+    self.second.timing = TimingFromDict({'requestTime': 123456 + 1,
+                                        'receiveHeadersEnd': 200,
+                                        'loadingFinished': 600})
+
+  def testTimeBetweenParser(self):
+    self.assertEquals(900, TimeBetween(self.first, self.second, 'parser'))
+
+  def testTimeBetweenScript(self):
+    self.assertEquals(500, TimeBetween(self.first, self.second, 'script'))
 
 
 class RequestTestCase(unittest.TestCase):
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 2e5bb74..654e94b 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -7,6 +7,7 @@
 import devtools_monitor
 import loading_trace
 import page_track
+import request_track
 import tracing
 
 
@@ -40,9 +41,36 @@ class FakePageTrack(devtools_monitor.Track):
     return event['frame_id']
 
 
+def MakeRequest(
+    url, source_url, start_time, headers_time, end_time,
+    magic_content_type=False, initiator_type='other'):
+  assert initiator_type in ('other', 'parser')
+  timing = request_track.TimingAsList(request_track.TimingFromDict({
+      # connectEnd should be ignored.
+      'connectEnd': (end_time - start_time) / 2,
+      'receiveHeadersEnd': headers_time - start_time,
+      'loadingFinished': end_time - start_time,
+      'requestTime': start_time / 1000.0}))
+  rq = request_track.Request.FromJsonDict({
+      'timestamp': start_time / 1000.0,
+      'request_id': str(MakeRequest._next_request_id),
+      'url': 'http://' + str(url),
+      'initiator': {'type': initiator_type, 'url': 'http://' + str(source_url)},
+      'response_headers': {'Content-Type':
+                           'null' if not magic_content_type
+                           else 'magic-debug-content' },
+      'timing': timing
+  })
+  MakeRequest._next_request_id += 1
+  return rq
+
+
+MakeRequest._next_request_id = 0
+
+
 def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
-  request_track = FakeRequestTrack(requests)
+  request = FakeRequestTrack(requests)
   page_event_track = FakePageTrack(page_events if page_events else [])
   if trace_events:
     tracing_track = tracing.TracingTrack(None)
@@ -51,4 +79,4 @@ def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   else:
     tracing_track = None
   return loading_trace.LoadingTrace(
-      None, None, page_event_track, request_track, tracing_track)
+      None, None, page_event_track, request, tracing_track)
diff --git a/loading/tracing.py b/loading/tracing.py
index c69ffaf..37ee7ce 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -6,6 +6,7 @@
 
 import bisect
 import itertools
+import logging
 
 import devtools_monitor
 
@@ -95,6 +96,20 @@ class TracingTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
+  def TracingTrackForThread(self, tid):
+    """Returns a new TracingTrack with only the events from a given thread.
+
+    Args:
+      tid: (int) Thread ID.
+
+    Returns:
+      A new instance of TracingTrack.
+    """
+    events = [e for e in self._events if e.tracing_event['tid'] == tid]
+    tracing_track = TracingTrack(None)
+    tracing_track._events = events
+    return tracing_track
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     assert 'events' in json_dict
@@ -182,7 +197,7 @@ class TracingTrack(devtools_monitor.Track):
         assert event.IsIndexable()
         if event.start_msec > current_msec:
           break
-        matched_event = spanning_events.Match(event)
+        matched_event = spanning_events.Match(event, strict)
         if matched_event is not None:
           event = matched_event
         if not event.synthetic and (
@@ -220,9 +235,9 @@ class TracingTrack(devtools_monitor.Track):
           None: self._Ignore,
           }
 
-    def Match(self, event):
+    def Match(self, event, strict=False):
       return self._MATCH_HANDLER.get(
-          event.type, self._Unsupported)(event)
+          event.type, self._Unsupported)(event, strict)
 
     def HasPending(self):
       return (self._duration_stack or
@@ -236,21 +251,21 @@ class TracingTrack(devtools_monitor.Track):
           itertools.chain.from_iterable((
               (e for e in s) for s in self._async_stacks.itervalues())))
 
-    def _AsyncKey(self, event):
+    def _AsyncKey(self, event, _):
       return (event.tracing_event['cat'], event.id)
 
-    def _Ignore(self, _event):
+    def _Ignore(self, _event, _):
       return None
 
-    def _Unsupported(self, event):
+    def _Unsupported(self, event, _):
       raise devtools_monitor.DevToolsConnectionException(
           'Unsupported spanning event type: %s' % event)
 
-    def _DurationBegin(self, event):
+    def _DurationBegin(self, event, _):
       self._duration_stack.append(event)
       return None
 
-    def _DurationEnd(self, event):
+    def _DurationEnd(self, event, _):
       if not self._duration_stack:
         raise devtools_monitor.DevToolsConnectionException(
             'Unmatched duration end: %s' % event)
@@ -258,16 +273,20 @@ class TracingTrack(devtools_monitor.Track):
       start.SetClose(event)
       return start
 
-    def _AsyncStart(self, event):
-      key = self._AsyncKey(event)
+    def _AsyncStart(self, event, strict):
+      key = self._AsyncKey(event, strict)
       self._async_stacks.setdefault(key, []).append(event)
       return None
 
-    def _AsyncEnd(self, event):
-      key = self._AsyncKey(event)
+    def _AsyncEnd(self, event, strict):
+      key = self._AsyncKey(event, strict)
       if key not in self._async_stacks:
-        raise devtools_monitor.DevToolsConnectionException(
-            'Unmatched async end %s: %s' % (key, event))
+        message = 'Unmatched async end %s: %s' % (key, event)
+        if strict:
+          raise devtools_monitor.DevToolsConnectionException(message)
+        else:
+          logging.warning(message)
+        return None
       stack = self._async_stacks[key]
       start = stack.pop()
       if not stack:
@@ -275,7 +294,7 @@ class TracingTrack(devtools_monitor.Track):
       start.SetClose(event)
       return start
 
-    def _ObjectCreated(self, event):
+    def _ObjectCreated(self, event, _):
       # The tracing event format has object deletion timestamps being exclusive,
       # that is the timestamp for a deletion my equal that of the next create at
       # the same address. This asserts that does not happen in practice as it is
@@ -287,7 +306,7 @@ class TracingTrack(devtools_monitor.Track):
       self._objects[event.id] = event
       return None
 
-    def _ObjectDestroyed(self, event):
+    def _ObjectDestroyed(self, event, _):
       if event.id not in self._objects:
         raise devtools_monitor.DevToolsConnectionException(
             'Missing object creation for %s' % event)
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 0f30dcc..9c1ac3a 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -26,12 +26,12 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 15, 'ph': 'D', 'id': 1}]
 
   _EVENTS = [
-      {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
-      {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
-      {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
-      {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
-      {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
-      {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'B'}},
+      {'ts': 3, 'ph': 'X', 'dur': 4, 'tid': 1, 'args': {'name': 'A'}},
+      {'ts': 10, 'ph': 'X', 'dur': 1, 'tid': 2, 'args': {'name': 'C'}},
+      {'ts': 10, 'ph': 'X', 'dur': 2, 'tid': 2, 'args': {'name': 'D'}},
+      {'ts': 13, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'F'}},
+      {'ts': 12, 'ph': 'X', 'dur': 3, 'tid': 1, 'args': {'name': 'E'}}]
 
   def setUp(self):
     self.track = TracingTrack(None)
@@ -246,6 +246,16 @@ class TracingTrackTestCase(unittest.TestCase):
                      set([e.args['name']
                           for e in self.track.OverlappingEvents(6, 10.1)]))
 
+  def testTracingTrackForThread(self):
+    self.track.Handle(
+        'Tracing.dataCollected', {'params': {'value': [
+            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    tracing_track = self.track.TracingTrackForThread(1)
+    self.assertTrue(tracing_track is not self.track)
+    self.assertEquals(4, len(tracing_track.GetEvents()))
+    tracing_track = self.track.TracingTrackForThread(42)
+    self.assertEquals(0, len(tracing_track.GetEvents()))
+
 
 if __name__ == '__main__':
   unittest.main()

commit 4e7fb1f2450c1e736d8bd90a8d0fb58fecf62a63
Author: droger <droger@chromium.org>
Date:   Mon Feb 15 08:55:39 2016 -0800

    tools/android/loading Improve content type detection
    
    This CL improves content type detection:
    - uses the |mime_type| field when available, because some requests don't
      have response headers (such as data: URLs).
    - ignore the capitalization for the "Content-Type" header, because some
      servers use the wrong capitatization.
    
    Unittests are updated accordingly.
    
    Review URL: https://codereview.chromium.org/1698153002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375467}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8b9df8cca22c81f04cc95430522d0fc013660453

diff --git a/loading/request_track.py b/loading/request_track.py
index 52e22fb..a19c5e5 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -94,6 +94,7 @@ class Request(object):
     self.url = None
     self.protocol = None
     self.method = None
+    self.mime_type = None
     self.request_headers = None
     self.response_headers = None
     self.initial_priority = None
@@ -146,7 +147,17 @@ class Request(object):
 
   def GetContentType(self):
     """Returns the content type, or None."""
-    content_type = self.response_headers.get('Content-Type', None)
+    if self.mime_type:
+      return self.mime_type
+
+    # Case-insensitive search because servers sometimes use a wrong
+    # capitalization.
+    content_type = None
+    for header, value in self.response_headers.iteritems():
+      if header.lower() == 'content-type':
+        content_type = value
+        break
+
     if not content_type or ';' not in content_type:
       return content_type
     else:
@@ -161,7 +172,15 @@ class Request(object):
     cache_control = {}
     if not self.response_headers:
       return -1
-    cache_control_str = self.response_headers.get('Cache-Control', None)
+
+    # Case-insensitive search because servers sometimes use a wrong
+    # capitalization.
+    cache_control_str = None
+    for header, value in self.response_headers.iteritems():
+      if header.lower() == 'cache-control':
+        cache_control_str = value
+        break
+
     if cache_control_str is not None:
       directives = [s.strip() for s in cache_control_str.split(',')]
       for directive in directives:
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index e742f60..cad4a11 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -16,8 +16,16 @@ class RequestTestCase(unittest.TestCase):
     self.assertEquals(None, r.GetContentType())
     r.response_headers = {'Content-Type': 'application/javascript'}
     self.assertEquals('application/javascript', r.GetContentType())
+    # Case-insensitive match.
+    r.response_headers = {'content-type': 'application/javascript'}
+    self.assertEquals('application/javascript', r.GetContentType())
+    # Parameters are filtered out.
     r.response_headers = {'Content-Type': 'application/javascript;bla'}
     self.assertEquals('application/javascript', r.GetContentType())
+    # MIME type takes precedence over headers.
+    r.mime_type = 'image/webp'
+    self.assertEquals('image/webp', r.GetContentType())
+    r.mime_type = None
 
 
 class RequestTrackTestCase(unittest.TestCase):
@@ -340,6 +348,10 @@ class RequestTrackTestCase(unittest.TestCase):
     rq.response_headers[
         'Cache-Control'] = 'private,s-maxage=0'
     self.assertEqual(-1, rq.MaxAge())
+    # Case-insensitive match.
+    rq.response_headers['cache-control'] = 'max-age=600'
+    self.assertEqual(600, rq.MaxAge())
+
 
   @classmethod
   def _ValidSequence(cls, request_track):

commit 74d33ab0c7d5f0699e5eb279f3d0c6eb8f83e3d2
Author: droger <droger@chromium.org>
Date:   Mon Feb 15 08:47:10 2016 -0800

    tools/android/loading Fix broken initiator format
    
    The initiator format has been changed by CL:
    https://codereview.chromium.org/1666563005/
    
    This CL integrates the new format in //tools/android/loading.
    
    Review URL: https://codereview.chromium.org/1695323002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375465}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5e9d81f956cab1871be97e7f4d1b7fbe9d9fbf7f

diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 8128957..b255656 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -90,12 +90,14 @@ class RequestDependencyLens(object):
     return (initiating_request, request, 'parser')
 
   def _GetInitiatingRequestScript(self, request):
-    if not 'stackTrace' in request.initiator:
+    STACK_KEY = 'stack'
+    if not STACK_KEY in request.initiator:
       logging.warning('Script initiator but no stack trace.')
       return None
     initiating_request = None
     timestamp = request.timing.request_time
-    for frame in request.initiator['stackTrace']:
+    call_frames = request.initiator[STACK_KEY]['callFrames']
+    for frame in call_frames:
       url = frame['url']
       candidates = self._FindMatchingRequests(url, timestamp)
       if candidates:
@@ -104,7 +106,7 @@ class RequestDependencyLens(object):
         if initiating_request:
           break
     else:
-      for frame in request.initiator['stackTrace']:
+      for frame in call_frames:
         if not frame.get('url', None) and frame.get(
             'functionName', None) == 'window.onload':
           logging.warning('Unmatched request for onload handler.')
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 0ab7403..4035abf 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -55,8 +55,9 @@ class RequestDependencyLensTestCase(unittest.TestCase):
       {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
        'frame_id': '123.1',
        'initiator': {'type': 'script',
-                     'stackTrace': [{'url': 'unknown'},
-                                    {'url': 'http://bla.com/nyancat.js'}]},
+                     'stack': {'callFrames': [
+                         {'url': 'unknown'},
+                         {'url': 'http://bla.com/nyancat.js'}]}},
        'timestamp': 10, 'timing': TimingFromDict({})})
   _PAGE_EVENTS = [{'method': 'Page.frameAttached',
                    'frame_id': '123.13', 'parent_frame_id': '123.1'}]

commit 81b01f8d6fcdb0417b2f1333583228784874cf1c
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 15 05:39:11 2016 -0800

    sandwich: Save browser cache as zip archive preserving all timestamps
    
    Note that the timestamps are saved in a json in the archive to be
    able to save directories's timestamps as well.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1690233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375453}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2dc964d1727a9a45c94f411d959b59bcb45a33ec

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 68ca389..242cfbf 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -12,10 +12,14 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
+import json
 import logging
 import os
+import shutil
 import sys
+import tempfile
 import time
+import zipfile
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -100,22 +104,16 @@ def _UpdateTimestampFromAdbStat(filename, stat):
   os.utime(filename, (stat.st_time, stat.st_time))
 
 
-def _SaveBrowserCache(device, output_directory):
+def _PullBrowserCache(device):
   """Pulls the browser cache from the device and saves it locally.
 
   Cache is saved with the same file structure as on the device. Timestamps are
   important to preserve because indexing and eviction depends on them.
 
-  Args:
-    output_directory: name of the directory for saving cache.
+  Returns:
+    Temporary directory containing all the browser cache.
   """
-  save_target = os.path.join(output_directory, _CACHE_DIRECTORY_NAME)
-  try:
-    os.makedirs(save_target)
-  except IOError:
-    logging.warning('Could not create directory: %s' % save_target)
-    raise
-
+  save_target = tempfile.mkdtemp(suffix='.cache')
   cache_directory = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
   for filename, stat in device.adb.Ls(cache_directory):
     if filename == '..':
@@ -144,6 +142,72 @@ def _SaveBrowserCache(device, output_directory):
   # after all files in it have been written. The timestamp is compared with
   # the contents of the index file when freshness is determined.
   _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
+  return save_target
+
+
+def _ZipDirectoryContent(root_directory_path, archive_dest_path):
+  """Zip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    root_directory_path: The directory's path to archive.
+    archive_dest_path: Archive destination's path.
+  """
+  with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
+    timestamps = {}
+    for directory_path, dirnames, filenames in os.walk(root_directory_path):
+      for dirname in dirnames:
+        subdirectory_path = os.path.join(directory_path, dirname)
+        subdirectory_relative_path = os.path.relpath(subdirectory_path,
+                                                     root_directory_path)
+        subdirectory_stats = os.stat(subdirectory_path)
+        timestamps[subdirectory_relative_path] = {
+            'atime': subdirectory_stats.st_atime,
+            'mtime': subdirectory_stats.st_mtime}
+      for filename in filenames:
+        file_path = os.path.join(directory_path, filename)
+        file_archive_name = os.path.join('content',
+            os.path.relpath(file_path, root_directory_path))
+        file_stats = os.stat(file_path)
+        timestamps[file_archive_name[8:]] = {
+            'atime': file_stats.st_atime,
+            'mtime': file_stats.st_mtime}
+        zip_output.write(file_path, arcname=file_archive_name)
+    zip_output.writestr('timestamps.json',
+                        json.dumps(timestamps, indent=2))
+
+
+def _UnzipDirectoryContent(archive_path, directory_dest_path):
+  """Unzip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    archive_path: Archive's path to unzip.
+    directory_dest_path: Directory destination path.
+  """
+  if not os.path.exists(directory_dest_path):
+    os.makedirs(directory_dest_path)
+
+  with zipfile.ZipFile(archive_path) as zip_input:
+    timestamps = None
+    for file_archive_name in zip_input.namelist():
+      if file_archive_name == 'timestamps.json':
+        timestamps = json.loads(zip_input.read(file_archive_name))
+      elif file_archive_name.startswith('content/'):
+        file_relative_path = file_archive_name[8:]
+        file_output_path = os.path.join(directory_dest_path, file_relative_path)
+        file_parent_directory_path = os.path.dirname(file_output_path)
+        if not os.path.exists(file_parent_directory_path):
+          os.makedirs(file_parent_directory_path)
+        with open(file_output_path, 'w') as f:
+          f.write(zip_input.read(file_archive_name))
+
+    assert timestamps
+    for relative_path, stats in timestamps.iteritems():
+      output_path = os.path.join(directory_dest_path, relative_path)
+      if not os.path.exists(output_path):
+        os.makedirs(output_path)
+      os.utime(output_path, (stats['atime'], stats['mtime']))
 
 
 def main():
@@ -203,7 +267,11 @@ def main():
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
     device.KillAll(_CHROME_PACKAGE, quiet=True)
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    _SaveBrowserCache(device, args.output)
+
+    cache_directory_path = _PullBrowserCache(device)
+    _ZipDirectoryContent(cache_directory_path,
+                         os.path.join(args.output, 'cache.zip'))
+    shutil.rmtree(cache_directory_path)
 
 
 if __name__ == '__main__':

commit 5f522e189ad94b4e652fca62a306b64be9ec21c2
Author: gabadie <gabadie@chromium.org>
Date:   Fri Feb 12 06:48:16 2016 -0800

    sandwich: Builds a script to pull the metrics from the traces
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1690813003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375181}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7c2b4ef2202045b91929d8d9d80ff75b48aa8676

diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
new file mode 100755
index 0000000..1558243
--- /dev/null
+++ b/loading/pull_sandwich_metrics.py
@@ -0,0 +1,190 @@
+#! /usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Pull a sandwich run's output directory's metrics from traces into a CSV.
+
+python pull_sandwich_metrics.py -h
+"""
+
+import argparse
+import csv
+import json
+import logging
+import os
+import sys
+
+
+CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
+
+_CSV_FIELD_NAMES = [
+    'id',
+    'total_load',
+    'onload',
+    'browser_malloc_avg',
+    'browser_malloc_max']
+
+_TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
+
+
+def _GetBrowserPID(trace):
+  """Get the browser PID from a trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    The browser's PID as an integer.
+  """
+  for event in trace['traceEvents']:
+    if event['cat'] != '__metadata' or event['name'] != 'process_name':
+      continue
+    if event['args']['name'] == 'Browser':
+      return event['pid']
+  raise ValueError('couldn\'t find browser\'s PID')
+
+
+def _GetBrowserDumpEvents(trace):
+  """Get the browser memory dump events from a trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    List of memory dump events.
+  """
+  browser_pid = _GetBrowserPID(trace)
+  browser_dumps_events = []
+  for event in trace['traceEvents']:
+    if event['cat'] != 'disabled-by-default-memory-infra':
+      continue
+    if event['ph'] != 'v' or event['name'] != 'periodic_interval':
+      continue
+    # Ignore dump events for processes other than the browser process
+    if event['pid'] != browser_pid:
+      continue
+    browser_dumps_events.append(event)
+  if len(browser_dumps_events) == 0:
+    raise ValueError('No browser dump events found.')
+  return browser_dumps_events
+
+
+def _GetWebPageTrackedEvents(trace):
+  """Get the web page's tracked events from a trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    Dictionary all tracked events.
+  """
+  main_frame = None
+  tracked_events = {}
+  for event in trace['traceEvents']:
+    if event['cat'] != 'blink.user_timing':
+      continue
+    event_name = event['name']
+    # Ignore events until about:blank's unloadEventEnd that give the main
+    # frame id.
+    if not main_frame:
+      if event_name == 'unloadEventEnd':
+        main_frame = event['args']['frame']
+        logging.info('found about:blank\'s event \'unloadEventEnd\'')
+      continue
+    # Ignore sub-frames events. requestStart don't have the frame set but it
+    # is fine since tracking the first one after about:blank's unloadEventEnd.
+    if 'frame' in event['args'] and event['args']['frame'] != main_frame:
+      continue
+    if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
+      logging.info('found url\'s event \'%s\'' % event_name)
+      tracked_events[event_name] = event
+  assert len(tracked_events) == len(_TRACKED_EVENT_NAMES)
+  return tracked_events
+
+
+def _PullMetricsFromTrace(trace):
+  """Pulls all the metrics from a given trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    Dictionary with all _CSV_FIELD_NAMES's field set (except the 'id').
+  """
+  browser_dump_events = _GetBrowserDumpEvents(trace)
+  web_page_tracked_events = _GetWebPageTrackedEvents(trace)
+
+  browser_malloc_sum = 0
+  browser_malloc_max = 0
+  for dump_event in browser_dump_events:
+    attr = dump_event['args']['dumps']['allocators']['malloc']['attrs']['size']
+    assert attr['units'] == 'bytes'
+    size = int(attr['value'], 16)
+    browser_malloc_sum += size
+    browser_malloc_max = max(browser_malloc_max, size)
+
+  return {
+    'total_load': (web_page_tracked_events['loadEventEnd']['ts'] -
+                   web_page_tracked_events['requestStart']['ts']),
+    'onload': (web_page_tracked_events['loadEventEnd']['ts'] -
+               web_page_tracked_events['loadEventStart']['ts']),
+    'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
+    'browser_malloc_max': browser_malloc_max
+  }
+
+
+def _PullMetricsFromOutputDirectory(output_directory_path):
+  """Pulls all the metrics from all the traces of a sandwich run directory.
+
+  Args:
+    output_directory_path: The sandwich run's output directory to pull the
+        metrics from.
+
+  Returns:
+    List of dictionaries with all _CSV_FIELD_NAMES's field set.
+  """
+  assert os.path.isdir(output_directory_path)
+  metrics = []
+  for node_name in os.listdir(output_directory_path):
+    if not os.path.isdir(os.path.join(output_directory_path, node_name)):
+      continue
+    try:
+      page_id = int(node_name)
+    except ValueError:
+      continue
+    trace_path = os.path.join(output_directory_path, node_name, 'trace.json')
+    if not os.path.isfile(trace_path):
+      continue
+    logging.info('processing \'%s\'' % trace_path)
+    with open(trace_path) as trace_file:
+      trace = json.load(trace_file)
+      trace_metrics = _PullMetricsFromTrace(trace)
+      trace_metrics['id'] = page_id
+      metrics.append(trace_metrics)
+  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
+                            'run directory.').format(output_directory_path)
+  return metrics
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument('output', type=str,
+                      help='Output directory of run_sandwich.py command.')
+  args = parser.parse_args()
+
+  trace_metrics_list = _PullMetricsFromOutputDirectory(args.output)
+  trace_metrics_list.sort(key=lambda e: e['id'])
+  cs_file_path = os.path.join(args.output, 'trace_analysis.csv')
+  with open(cs_file_path, 'w') as csv_file:
+    writer = csv.DictWriter(csv_file, fieldnames=_CSV_FIELD_NAMES)
+    writer.writeheader()
+    for trace_metrics in trace_metrics_list:
+      writer.writerow(trace_metrics)
+  return 0
+
+
+if __name__ == '__main__':
+  sys.exit(main())
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
new file mode 100644
index 0000000..ad9fbe1
--- /dev/null
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -0,0 +1,162 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import json
+import os
+import shutil
+import subprocess
+import tempfile
+import unittest
+
+import pull_sandwich_metrics as puller
+
+_BLINK_CAT = 'blink.user_timing'
+_MEM_CAT = 'disabled-by-default-memory-infra'
+_START='requestStart'
+_LOADS='loadEventStart'
+_LOADE='loadEventEnd'
+_UNLOAD='unloadEventEnd'
+
+_MINIMALIST_TRACE = {'traceEvents': [
+    {'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10, 'args': {'frame': '0'}},
+    {'cat': _BLINK_CAT, 'name': _START,  'ts': 20, 'args': {},           },
+    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
+        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+            'units': 'bytes', 'value': '1af2', }}}}}}},
+    {'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35, 'args': {'frame': '0'}},
+    {'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40, 'args': {'frame': '0'}},
+    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
+        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+            'units': 'bytes', 'value': 'd704', }}}}}}},
+    {'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'args': {
+        'name': 'Browser'}}]}
+
+
+class PageTrackTest(unittest.TestCase):
+  def testGetBrowserPID(self):
+    def RunHelper(expected, trace):
+      self.assertEquals(expected, puller._GetBrowserPID(trace))
+
+    RunHelper(123, {'traceEvents': [
+        {'pid': 354, 'cat': 'whatever0'},
+        {'pid': 354, 'cat': 'whatever1'},
+        {'pid': 354, 'cat': '__metadata', 'name': 'thread_name'},
+        {'pid': 354, 'cat': '__metadata', 'name': 'process_name', 'args': {
+            'name': 'Renderer'}},
+        {'pid': 123, 'cat': '__metadata', 'name': 'process_name', 'args': {
+            'name': 'Browser'}},
+        {'pid': 354, 'cat': 'whatever0'}]})
+
+    with self.assertRaises(ValueError):
+      RunHelper(123, {'traceEvents': [
+          {'pid': 354, 'cat': 'whatever0'},
+          {'pid': 354, 'cat': 'whatever1'}]})
+
+  def testGetBrowserDumpEvents(self):
+    NAME = 'periodic_interval'
+
+    def RunHelper(trace_events, browser_pid):
+      trace_events = copy.copy(trace_events)
+      trace_events.append({
+          'pid': browser_pid,
+          'cat': '__metadata',
+          'name': 'process_name',
+          'args': {'name': 'Browser'}})
+      return puller._GetBrowserDumpEvents({'traceEvents': trace_events})
+
+    TRACE_EVENTS = [
+        {'pid': 354, 'ts':  1, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  2, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  3, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  4, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  5, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  6, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  7, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  8, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  9, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts': 10, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts': 11, 'cat': 'whatever0'},
+        {'pid': 672, 'ts': 12, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
+
+    self.assertTrue(_MEM_CAT in puller.CATEGORIES)
+
+    bump_events = RunHelper(TRACE_EVENTS, 123)
+    self.assertEquals(2, len(bump_events))
+    self.assertEquals(5, bump_events[0]['ts'])
+    self.assertEquals(10, bump_events[1]['ts'])
+
+    bump_events = RunHelper(TRACE_EVENTS, 354)
+    self.assertEquals(1, len(bump_events))
+    self.assertEquals(1, bump_events[0]['ts'])
+
+    bump_events = RunHelper(TRACE_EVENTS, 672)
+    self.assertEquals(3, len(bump_events))
+    self.assertEquals(3, bump_events[0]['ts'])
+    self.assertEquals(7, bump_events[1]['ts'])
+    self.assertEquals(12, bump_events[2]['ts'])
+
+    with self.assertRaises(ValueError):
+      RunHelper(TRACE_EVENTS, 895)
+
+  def testGetWebPageTrackedEvents(self):
+    self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
+
+    trace_events = puller._GetWebPageTrackedEvents({'traceEvents': [
+        {'ts':  0, 'args': {},             'cat': 'whatever', 'name': _START},
+        {'ts':  1, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
+        {'ts':  2, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
+        {'ts':  3, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts':  4, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts':  5, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts':  6, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _UNLOAD},
+        {'ts':  7, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts':  8, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts':  9, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts': 10, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _UNLOAD},
+        {'ts': 11, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _START},
+        {'ts': 12, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
+        {'ts': 13, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
+        {'ts': 14, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts': 15, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts': 16, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts': 17, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts': 18, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts': 19, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts': 20, 'args': {},             'cat': 'whatever', 'name': _START},
+        {'ts': 21, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
+        {'ts': 22, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
+        {'ts': 23, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts': 24, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts': 25, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE}]})
+
+    self.assertEquals(3, len(trace_events))
+    self.assertEquals(14, trace_events['requestStart']['ts'])
+    self.assertEquals(17, trace_events['loadEventStart']['ts'])
+    self.assertEquals(19, trace_events['loadEventEnd']['ts'])
+
+  def testPullMetricsFromTrace(self):
+    metrics = puller._PullMetricsFromTrace(_MINIMALIST_TRACE)
+    self.assertEquals(4, len(metrics))
+    self.assertEquals(20, metrics['total_load'])
+    self.assertEquals(5, metrics['onload'])
+    self.assertEquals(30971, metrics['browser_malloc_avg'])
+    self.assertEquals(55044, metrics['browser_malloc_max'])
+
+  def testCommandLine(self):
+    tmp_dir = tempfile.mkdtemp()
+    for dirname in ['1', '2', 'whatever']:
+      os.mkdir(os.path.join(tmp_dir, dirname))
+      with open(os.path.join(tmp_dir, dirname, 'trace.json'), 'w') as out_file:
+        json.dump(_MINIMALIST_TRACE, out_file)
+
+    process = subprocess.Popen(['python', puller.__file__, tmp_dir])
+    process.wait()
+    shutil.rmtree(tmp_dir)
+
+    self.assertEquals(0, process.returncode)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 8286c3c..68ca389 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -31,6 +31,7 @@ import device_setup
 import devtools_monitor
 import json
 import page_track
+import pull_sandwich_metrics
 import tracing
 
 
@@ -89,7 +90,7 @@ def _SaveChromeTrace(events, directory, subdirectory):
   try:
     os.makedirs(target_directory)
     with open(filename, 'w') as f:
-      json.dump({'traceEvents': events['events'], 'metadata': {}}, f)
+      json.dump({'traceEvents': events['events'], 'metadata': {}}, f, indent=2)
   except IOError:
     logging.warning('Could not save a trace: %s' % filename)
     # Swallow the exception.
@@ -188,7 +189,7 @@ def main():
             connection.ClearCache()
           page_track.PageTrack(connection)
           tracing_track = tracing.TracingTrack(connection,
-              categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
+              categories=pull_sandwich_metrics.CATEGORIES)
           connection.SetUpMonitoring()
           connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
           connection.StartMonitoring()

commit c216bebf1224291b9e3331afa37c464e9430c11d
Author: mattcary <mattcary@chromium.org>
Date:   Fri Feb 12 05:59:08 2016 -0800

    Refactor options in analyze.py to global structure
    
    Plumbing all sorts of options through arguments was getting unmaintainable,
    especially as many of them should be common across all commands, or affect
    low-level operation that's inappropriate to expose to high-level functions.
    
    Review URL: https://codereview.chromium.org/1694473002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375173}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 23bd22480abd2209f258d1dd8d3a8572ee17954a

diff --git a/loading/analyze.py b/loading/analyze.py
index 5695e7d..fdf18da 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -30,6 +30,7 @@ import frame_load_lens
 import loading_model
 import loading_trace
 import model_graph
+import options
 import trace_recorder
 
 
@@ -38,8 +39,7 @@ import trace_recorder
 # output. For now we just do logging.warning.
 
 
-# TODO(mattcary): probably we want this piped in through a flag.
-CHROME = constants.PACKAGE_INFO['chrome']
+OPTIONS = options.OPTIONS
 
 
 def _LoadPage(device, url):
@@ -50,7 +50,9 @@ def _LoadPage(device, url):
     url: url as a string to load.
   """
   load_intent = intent.Intent(
-      package=CHROME.package, activity=CHROME.activity, data=url)
+      package=OPTIONS.ChromePackage().package,
+      activity=OPTIONS.ChromePackage().activity,
+      data=url)
   logging.warning('Loading ' + url)
   device.StartActivity(load_intent, blocking=True)
 
@@ -95,39 +97,33 @@ def _GetPrefetchHtml(graph, name=None):
   return '\n'.join(output)
 
 
-def _LogRequests(url, clear_cache=True, local=False,
-                 host_exe="out/Release/chrome", host_profile_dir=None):
+def _LogRequests(url, clear_cache_override=None):
   """Log requests for a web page.
 
   Args:
     url: url to log as string.
-    clear_cache: optional flag to clear the cache.
-    local: log from local (desktop) chrome session.
-    host_exe: Binary to execute when running locally.
-    host_profile_dir: Profile dir to use when running locally (if None, a
-      fresh profile dir will be used).
+    clear_cache_override: if not None, set clear_cache different from OPTIONS.
 
   Returns:
     JSON dict of logged information (ie, a dict that describes JSON).
   """
-  device = device_setup.GetFirstDevice() if not local else None
-  with device_setup.DeviceConnection(device, host_exe=host_exe,
-      host_profile_dir=host_profile_dir) as connection:
+  device = device_setup.GetFirstDevice() if not OPTIONS.local else None
+  clear_cache = (clear_cache_override if clear_cache_override is not None
+                 else OPTIONS.clear_cache)
+  with device_setup.DeviceConnection(device) as connection:
     trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
     return trace.ToJsonDict()
 
 
-def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
-               host_exe, host_profile_dir):
+def _FullFetch(url, json_output, prefetch):
   """Do a full fetch with optional prefetching."""
-  if not url.startswith('http'):
+  if not url.startswith('http') and not url.startswith('file'):
     url = 'http://' + url
   logging.warning('Cold fetch')
-  cold_data = _LogRequests(url, local=local,
-                           host_exe=host_exe, host_profile_dir=host_profile_dir)
+  cold_data = _LogRequests(url)
   assert cold_data, 'Cold fetch failed to produce data. Check your phone.'
   if prefetch:
-    assert not local
+    assert not OPTIONS.local
     logging.warning('Generating prefetch')
     prefetch_html = _GetPrefetchHtml(
         loading_model.ResourceGraph(cold_data), name=url)
@@ -140,9 +136,9 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
     device.adb.Push(tmp.name, target)
     logging.warning('Pushed prefetch %s to device at %s' % (tmp.name, target))
     _LoadPage(device, 'file://' + target)
-    time.sleep(prefetch_delay_seconds)
+    time.sleep(OPTIONS.prefetch_delay_seconds)
     logging.warning('Warm fetch')
-    warm_data = _LogRequests(url, clear_cache=False)
+    warm_data = _LogRequests(url, clear_cache_override=False)
     with open(json_output, 'w') as f:
       _WriteJson(f, warm_data)
     logging.warning('Wrote ' + json_output)
@@ -155,17 +151,17 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
     logging.warning('Wrote ' + json_output)
 
 
-# TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
-# with here.
-def _ProcessRequests(filename, ad_rules_filename='',
-                     tracking_rules_filename=''):
+def _ProcessRequests(filename):
   with open(filename) as f:
     trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
     content_lens = (
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
-            trace, ad_rules_filename, tracking_rules_filename))
+            trace, OPTIONS.ad_rules, OPTIONS.tracking_rules))
     frame_lens = frame_load_lens.FrameLoadLens(trace)
-    return loading_model.ResourceGraph(trace, content_lens, frame_lens)
+    graph = loading_model.ResourceGraph(trace, content_lens, frame_lens)
+    if OPTIONS.noads:
+      graph.Set(node_filter=graph.FilterAds)
+    return graph
 
 
 def InvalidCommand(cmd):
@@ -173,63 +169,34 @@ def InvalidCommand(cmd):
            (cmd, ' '.join(COMMAND_MAP.keys())))
 
 
-def DoCost(arg_str):
-  parser = argparse.ArgumentParser(description='Tabulates cost')
-  parser.add_argument('request_json')
-  parser.add_argument('--parameter', nargs='*', default=[])
-  parser.add_argument('--path', action='store_true')
-  parser.add_argument('--noads', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  for p in args.parameter:
-    graph.Set(**{p: True})
-  path_list = []
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
-  print 'Graph cost: ' + str(graph.Cost(path_list))
-  if args.path:
-    for p in path_list:
-      print '  ' + p.ShortName()
-
-
 def DoPng(arg_str):
-  parser = argparse.ArgumentParser(
-      description='Generates a PNG from a trace')
-  parser.add_argument('request_json')
-  parser.add_argument('png_output', nargs='?')
-  parser.add_argument('--eog', action='store_true')
-  parser.add_argument('--noads', action='store_true')
-  parser.add_argument('--ad_rules', default='')
-  parser.add_argument('--tracking_rules', default='')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(
-      args.request_json, args.ad_rules, args.tracking_rules)
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
+  OPTIONS.ParseArgs(arg_str, description='Generates a PNG from a trace',
+                    extra=['request_json', ('--png_output', ''),
+                           ('--eog', False)])
+  graph = _ProcessRequests(OPTIONS.request_json)
   visualization = model_graph.GraphVisualization(graph)
   tmp = tempfile.NamedTemporaryFile()
   visualization.OutputDot(tmp)
   tmp.flush()
-  png_output = args.png_output
+  png_output = OPTIONS.png_output
   if not png_output:
-    if args.request_json.endswith('.json'):
-      png_output = args.request_json[:args.request_json.rfind('.json')] + '.png'
+    if OPTIONS.request_json.endswith('.json'):
+      png_output = OPTIONS.request_json[
+          :OPTIONS.request_json.rfind('.json')] + '.png'
     else:
-      png_output = args.request_json + '.png'
+      png_output = OPTIONS.request_json + '.png'
   subprocess.check_call(['dot', '-Tpng', tmp.name, '-o', png_output])
   logging.warning('Wrote ' + png_output)
-  if args.eog:
+  if OPTIONS.eog:
     subprocess.Popen(['eog', png_output])
   tmp.close()
 
 
 def DoCompare(arg_str):
-  parser = argparse.ArgumentParser(description='Compares two traces')
-  parser.add_argument('g1_json')
-  parser.add_argument('g2_json')
-  args = parser.parse_args(arg_str)
-  g1 = _ProcessRequests(args.g1_json)
-  g2 = _ProcessRequests(args.g2_json)
+  OPTIONS.ParseArgs(arg_str, description='Compares two traces',
+                    extra=['g1_json', 'g2_json'])
+  g1 = _ProcessRequests(OPTIONS.g1_json)
+  g2 = _ProcessRequests(OPTIONS.g2_json)
   discrepancies = loading_model.ResourceGraph.CheckImageLoadConsistency(g1, g2)
   if discrepancies:
     print '%d discrepancies' % len(discrepancies)
@@ -239,104 +206,103 @@ def DoCompare(arg_str):
 
 
 def DoPrefetchSetup(arg_str):
-  parser = argparse.ArgumentParser(description='Sets up prefetch')
-  parser.add_argument('request_json')
-  parser.add_argument('target_html')
-  parser.add_argument('--upload', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  with open(args.target_html, 'w') as html:
+  OPTIONS.ParseArgs(arg_str, description='Sets up prefetch',
+                    extra=['request_json', 'target_html', ('--upload', False)])
+  graph = _ProcessRequests(OPTIONS.request_json)
+  with open(OPTIONS.target_html, 'w') as html:
     html.write(_GetPrefetchHtml(
-        graph, name=os.path.basename(args.request_json)))
-  if args.upload:
+        graph, name=os.path.basename(OPTIONS.request_json)))
+  if OPTIONS.upload:
     device = device_setup.GetFirstDevice()
     destination = os.path.join('/sdcard/Download',
-                               os.path.basename(args.target_html))
-    device.adb.Push(args.target_html, destination)
+                               os.path.basename(OPTIONS.target_html))
+    device.adb.Push(OPTIONS.target_html, destination)
 
     logging.warning(
-        'Pushed %s to device at %s' % (args.target_html, destination))
+        'Pushed %s to device at %s' % (OPTIONS.target_html, destination))
 
 
 def DoLogRequests(arg_str):
-  parser = argparse.ArgumentParser(description='Logs requests of a load')
-  parser.add_argument('--url', required=True)
-  parser.add_argument('--output', required=True)
-  parser.add_argument('--prefetch', action='store_true')
-  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
-  parser.add_argument('--local', action='store_true')
-  parser.add_argument('--host_exe', default='out/Release/chrome')
-  parser.add_argument('--host_profile_dir', default=None)
-  args = parser.parse_args(arg_str)
-  _FullFetch(url=args.url,
-             json_output=args.output,
-             prefetch=args.prefetch,
-             prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=args.local,
-             host_exe=args.host_exe,
-             host_profile_dir=args.host_profile_dir)
+  OPTIONS.ParseArgs(arg_str, description='Logs requests of a load',
+                    extra=['--url', '--output', ('--prefetch', False)])
+  _FullFetch(url=OPTIONS.url,
+             json_output=OPTIONS.output,
+             prefetch=OPTIONS.prefetch)
 
 
 def DoFetch(arg_str):
-  parser = argparse.ArgumentParser(description='Fetches SITE into DIR with '
-                                   'standard naming that can be processed by '
-                                   './cost_to_csv.py.  Both warm and cold '
-                                   'fetches are done.  SITE can be a full url '
-                                   'but the filename may be strange so better '
-                                   'to just use a site (ie, domain).')
-  # Arguments are flags as it's easy to get the wrong order of site vs dir.
-  parser.add_argument('--site', required=True)
-  parser.add_argument('--dir', required=True)
-  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
-  args = parser.parse_args(arg_str)
-  if not os.path.exists(args.dir):
-    os.makedirs(args.dir)
-  _FullFetch(url=args.site,
-             json_output=os.path.join(args.dir, args.site + '.json'),
-             prefetch=True,
-             prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=False,
-             host_exe=None,
-             host_profile_dir=None)
+  OPTIONS.ParseArgs(arg_str,
+                    description=('Fetches SITE into DIR with '
+                                 'standard naming that can be processed by '
+                                 './cost_to_csv.py.  Both warm and cold '
+                                 'fetches are done.  SITE can be a full url '
+                                 'but the filename may be strange so better '
+                                 'to just use a site (ie, domain).'),
+                    extra=['--site', '--dir'])
+  if not os.path.exists(OPTIONS.dir):
+    os.makedirs(OPTIONS.dir)
+  _FullFetch(url=OPTIONS.site,
+             json_output=os.path.join(OPTIONS.dir, OPTIONS.site + '.json'),
+             prefetch=True)
 
 
 def DoLongPole(arg_str):
-  parser = argparse.ArgumentParser(description='Calculates long pole')
-  parser.add_argument('request_json')
-  parser.add_argument('--noads', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
+  OPTIONS.ParseArgs(arg_str, description='Calculates long pole',
+                    extra='request_json')
+  graph = _ProcessRequests(OPTIONS.request_json)
   path_list = []
   cost = graph.Cost(path_list=path_list)
   print '%s (%s)' % (path_list[-1], cost)
 
 
 def DoNodeCost(arg_str):
-  parser = argparse.ArgumentParser(description='Calculates node cost')
-  parser.add_argument('request_json')
-  parser.add_argument('--noads', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
+  OPTIONS.ParseArgs(arg_str,
+                    description='Calculates node cost',
+                    extra='request_json')
+  graph = _ProcessRequests(OPTIONS.request_json)
   print sum((n.NodeCost() for n in graph.Nodes()))
 
 
+def DoCost(arg_str):
+  OPTIONS.ParseArgs(arg_str,
+                    description='Calculates total cost',
+                    extra=['request_json', ('--path', False)])
+  graph = _ProcessRequests(OPTIONS.request_json)
+  path_list = []
+  print 'Graph cost: %s' % graph.Cost(path_list)
+  if OPTIONS.path:
+    for p in path_list:
+      print '  ' + p.ShortName()
+
+
 COMMAND_MAP = {
-    'cost': DoCost,
     'png': DoPng,
     'compare': DoCompare,
     'prefetch_setup': DoPrefetchSetup,
     'log_requests': DoLogRequests,
     'longpole': DoLongPole,
     'nodecost': DoNodeCost,
+    'cost': DoCost,
     'fetch': DoFetch,
 }
 
 def main():
   logging.basicConfig(level=logging.WARNING)
+  OPTIONS.AddGlobalArgument(
+      'local', False,
+      'run against local desktop chrome rather than device '
+      '(see also --local_binary and local_profile_dir)')
+  OPTIONS.AddGlobalArgument(
+      'noads', False, 'ignore ad resources in modeling')
+  OPTIONS.AddGlobalArgument(
+      'ad_rules', '', 'AdBlocker+ ad rules file.')
+  OPTIONS.AddGlobalArgument(
+      'tracking_rules', '', 'AdBlocker+ tracking rules file.')
+  OPTIONS.AddGlobalArgument(
+      'prefetch_delay_seconds', 5,
+      'delay after requesting load of prefetch page '
+      '(only when running full fetch)')
+
   parser = argparse.ArgumentParser(description='Analyzes loading')
   parser.add_argument('command', help=' '.join(COMMAND_MAP.keys()))
   parser.add_argument('rest', nargs=argparse.REMAINDER)
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 1ad0388..728bf75 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -34,10 +34,10 @@ import adb_install_cert
 import certutils
 
 import devtools_monitor
+import options
 
-DEVTOOLS_PORT = 9222
-DEVTOOLS_HOSTNAME = 'localhost'
-DEFAULT_CHROME_PACKAGE = 'chrome'
+
+OPTIONS = options.OPTIONS
 
 
 @contextlib.contextmanager
@@ -178,13 +178,7 @@ def WprHost(device, wpr_archive_path, record=False):
     shutil.rmtree(temp_certificate_dir)
 
 @contextlib.contextmanager
-def DeviceConnection(device,
-                     package=DEFAULT_CHROME_PACKAGE,
-                     hostname=DEVTOOLS_HOSTNAME,
-                     port=DEVTOOLS_PORT,
-                     host_exe='out/Release/chrome',
-                     host_profile_dir=None,
-                     additional_flags=None):
+def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
 
   Sets up and restores any device and tracing appropriately
@@ -192,21 +186,18 @@ def DeviceConnection(device,
   Args:
     device: Android device, or None for a local run (in which case chrome needs
       to have been started with --remote-debugging-port=XXX).
-    package: The key for chrome package info.
-    port: The port on which to enable remote debugging.
-    host_exe: The binary to execute when running on the host.
-    host_profile_dir: The profile dir to use when running on the host (if None,
-      a fresh profile dir will be used).
     additional_flags: Additional chromium arguments.
 
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
   """
-  package_info = constants.PACKAGE_INFO[package]
+  package_info = OPTIONS.ChromePackage()
   command_line_path = '/data/local/chrome-command-line'
   new_flags = ['--disable-fre',
                '--enable-test-events',
-               '--remote-debugging-port=%d' % port]
+               '--remote-debugging-port=%d' % OPTIONS.devtools_port]
+  if OPTIONS.no_sandbox:
+    new_flags.append('--no-sandbox')
   if additional_flags != None:
     new_flags.extend(additional_flags)
   if device:
@@ -220,14 +211,14 @@ def DeviceConnection(device,
       device.StartActivity(start_intent, blocking=True)
     else:
       # Run on the host.
-      assert os.path.exists(host_exe)
+      assert os.path.exists(OPTIONS.local_binary)
 
-      user_data_dir = host_profile_dir
-      if not user_data_dir:
-        user_data_dir = TemporaryDirectory()
+      local_profile_dir = OPTIONS.local_profile_dir
+      if not local_profile_dir:
+        local_profile_dir = TemporaryDirectory()
 
-      new_flags += ['--user-data-dir=%s' % user_data_dir]
-      host_process = subprocess.Popen([host_exe] + new_flags,
+      new_flags.append('--user-data-dir=%s' % local_profile_dir)
+      host_process = subprocess.Popen([OPTIONS.local_binary] + new_flags,
                                       shell=False)
     if device:
       time.sleep(2)
@@ -236,27 +227,9 @@ def DeviceConnection(device,
       # in request_track.py to fire.
       time.sleep(10)
     # If no device, we don't care about chrome startup so skip the about page.
-    with ForwardPort(device, 'tcp:%d' % port,
+    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
                      'localabstract:chrome_devtools_remote'):
-      yield devtools_monitor.DevToolsConnection(hostname, port)
+      yield devtools_monitor.DevToolsConnection(
+          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
     if host_process:
       host_process.kill()
-
-
-def SetUpAndExecute(device, package, fn):
-  """Start logging process.
-
-  Wrapper for DeviceConnection for those functionally inclined.
-
-  Args:
-    device: Android device, or None for a local run.
-    package: the key for chrome package info.
-    fn: the function to execute that launches chrome and performs the
-        appropriate instrumentation. The function will receive a
-        DevToolsConnection as its sole parameter.
-
-  Returns:
-    As fn() returns.
-  """
-  with DeviceConnection(device, package) as connection:
-    return fn(connection)
diff --git a/loading/loading_model.py b/loading/loading_model.py
index b20d3eb..d2889e8 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -220,7 +220,7 @@ class ResourceGraph(object):
     Returns:
       A list of NodeInfo objects that describe the resources fetched.
     """
-    return self._node_info
+    return [n for n in self._node_info if n.Request() is not None]
 
   def DebugString(self):
     """Graph structure for debugging.
@@ -596,8 +596,9 @@ class ResourceGraph(object):
     """
     image_to_info = {}
     for n in self._node_info:
-      if (n.ContentType().startswith('image') and
-          not self._IsAdUrl(n.Url())):
+      if (n.ContentType() is not None and
+          n.ContentType().startswith('image') and
+          self.FilterAds(n)):
         key = str((n.Url(), n.ShortName(), n.StartTime()))
         assert key not in image_to_info, n.Url()
         image_to_info[key] = n
diff --git a/loading/options.py b/loading/options.py
new file mode 100644
index 0000000..f477cd2
--- /dev/null
+++ b/loading/options.py
@@ -0,0 +1,122 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import os.path
+import sys
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
+
+class Options(object):
+  """Global options repository.
+
+  ParseArgs must be called before use. See _ARGS for common members, these will
+  be available as instance attributes (eg, OPTIONS.clear_cache).
+  """
+  # Tuples of (argument name, default value, help string).
+  _ARGS = [ ('clear_cache', True,
+             'clear browser cache before loading'),
+            ('chrome_package_name', 'chrome',
+             'build/android/pylib/constants package description'),
+            ('devtools_hostname', 'localhost',
+             'hostname for devtools websocket connection'),
+            ('devtools_port', 9222,
+             'port for devtools websocket connection'),
+            ('local_binary', 'out/Release/chrome',
+             'chrome binary for local runs'),
+            ('local_profile_dir', '',
+             'profile directory to use for local runs'),
+            ('no_sandbox', False,
+             'pass --no-sandbox to browser (local run only; see also '
+             'https://chromium.googlesource.com/chromium/src/+/master/'
+             'docs/linux_suid_sandbox_development.md)'),
+          ]
+
+
+  def __init__(self):
+    self._arg_set = set()
+    self._parsed_args = None
+
+  def AddGlobalArgument(self, arg_name, default, help_str):
+    """Add a global argument.
+
+    Args:
+      arg_name: the name of the argument. This will be used as an optional --
+        argument.
+      default: the default value for the argument. The type of this default will
+        be used as the type of the argument.
+      help_str: the argument help string.
+    """
+    self._ARGS.append((arg_name, default, help_str))
+
+  def ParseArgs(self, arg_str, description=None, extra=None):
+    """Parse command line arguments.
+
+    Args:
+      arg_str: command line argument string.
+      description: description to use in argument parser.
+      extra: additional required arguments to add. These will be exposed as
+        instance attributes. This is either a list of extra arguments, or a
+        single string or tuple. If a tuple, the first item is the argument and
+        the second a default, otherwise the argument is required. Arguments are
+        used as in argparse, ie those beginning with -- are named, and those
+        without a dash are positional. Don't use a single dash.
+    """
+    parser = argparse.ArgumentParser(description=description)
+    for arg, default, help_str in self._ARGS:
+      # All global options are named.
+      arg = '--' + arg
+      self._AddArg(parser, arg, default, help_str=help_str)
+    if extra is not None:
+      if type(extra) is not list:
+        extra = [extra]
+      for arg in extra:
+        if type(arg) is tuple:
+          argname, default = arg
+          self._AddArg(parser, argname, default)
+        else:
+          self._AddArg(parser, arg, None, required=True)
+    self._parsed_args = parser.parse_args(arg_str)
+
+  def _AddArg(self, parser, arg, default, required=False, help_str=None):
+    assert not arg.startswith('-') or arg.startswith('--'), \
+        "Single dash arguments aren't supported: %s" % arg
+    arg_name = arg
+    if arg.startswith('--'):
+      arg_name = arg[2:]
+    assert arg_name not in self._arg_set, \
+      '%s extra arg is a duplicate' % arg_name
+    self._arg_set.add(arg_name)
+
+    kwargs = {}
+    if required and arg.startswith('--'):
+      kwargs['required'] = required
+    if help_str is not None:
+      kwargs['help'] = help_str
+    if default is not None:
+      if type(default) is bool:
+        # If the default of a switch is true, setting the flag stores false.
+        if default:
+          kwargs['action'] = 'store_false'
+        else:
+          kwargs['action'] = 'store_true'
+      else:
+        kwargs['default'] = default
+        kwargs['type'] = type(default)
+
+    parser.add_argument(arg, **kwargs)
+
+  def __getattr__(self, name):
+    if name in self._arg_set:
+      assert self._parsed_args, 'Option requested before ParseArgs called'
+      return getattr(self._parsed_args, name)
+    raise AttributeError(name)
+
+  def ChromePackage(self):
+    return constants.PACKAGE_INFO[self.chrome_package_name]
+
+OPTIONS = Options()

commit b289e1f886249c475776329f38eb28ab6d97d5a5
Author: agrieve <agrieve@chromium.org>
Date:   Thu Feb 11 12:15:48 2016 -0800

    Create wrapper scripts that set --output-directory
    
    For:
    - build/android/adb_gdb*
    - build/android/asan_symbolize.py
    - build/android/tombstones.py
    - third_party/android_platform/development/scripts/stack
    - tools/perf/run_benchmark
    
    BUG=573345
    
    Review URL: https://codereview.chromium.org/1663103004
    
    Cr-Original-Commit-Position: refs/heads/master@{#374964}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b8d204bc1b7018d3f32f0dda59d4b41655e67349

diff --git a/BUILD.gn b/BUILD.gn
index 4e074bd..bcadd0f 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -8,6 +8,8 @@
 # GYP: //tools/android/android_tools.gyp:android_tools
 group("android_tools") {
   deps = [
+    "//build/android:wrapper_scripts",
+    "//third_party/android_platform:stack_wrapper",
     "//third_party/catapult/telemetry:bitmaptools($host_toolchain)",
     "//tools/android/adb_reboot",
     "//tools/android/file_poller",
@@ -15,6 +17,7 @@ group("android_tools") {
     "//tools/android/md5sum",
     "//tools/android/memtrack_helper:memtrack_helper",
     "//tools/android/purge_ashmem",
+    "//tools/perf:run_benchmark_wrapper",
   ]
 }
 

commit f83b86fa6a2b96b258ac2e329b99013e025ccee4
Author: zhenw <zhenw@chromium.org>
Date:   Thu Feb 11 11:47:47 2016 -0800

    Use flag_changer in catapult devil.android
    
    This CL updates the dependency on pylib.flag_changer to
    Catapult's devil.android.flag_changer in Chromium repo.
    
    - The previous CL added devil.android.flag_changer in
    catapult: https://crrev.com/1675773002/
    - The next CL will remove pylib.flag_changer.
    
    BUG=583785
    
    Review URL: https://codereview.chromium.org/1677313002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374936}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5e903500791f38618b69b8258fc6a92124553f97

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 5782b58..1ad0388 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -16,12 +16,12 @@ _SRC_DIR = os.path.abspath(os.path.join(
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
 from devil.android import device_utils
+from devil.android import flag_changer
 from devil.android import forwarder
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
-from pylib import flag_changer
 
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
 from chrome_telemetry_build import chromium_config
diff --git a/mempressure.py b/mempressure.py
index 7d67d9d..ffa7c12 100755
--- a/mempressure.py
+++ b/mempressure.py
@@ -15,12 +15,12 @@ _SRC_PATH = os.path.abspath(os.path.join(
 sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil')
 from devil.android import device_errors
 from devil.android import device_utils
+from devil.android import flag_changer
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
 import devil_chromium
 from pylib import constants
-from pylib import flag_changer
 
 # Browser Constants
 DEFAULT_BROWSER = 'chrome'

commit 2f02f2c1b4300267809164cdf8e03332014e354f
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 11 07:55:35 2016 -0800

    sandwich: Adds web page replay support for HTTPS
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1685133002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374902}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: af2c7d276387b9a0930c22dbc0ba095aeae2c993

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 4986957..5782b58 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -29,6 +29,10 @@ from chrome_telemetry_build import chromium_config
 sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.util import webpagereplay
 
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
+import adb_install_cert
+import certutils
+
 import devtools_monitor
 
 DEVTOOLS_PORT = 9222
@@ -135,6 +139,20 @@ def WprHost(device, wpr_archive_path, record=False):
       os.remove(wpr_archive_path)
   else:
     assert os.path.exists(wpr_archive_path)
+
+  # Deploy certification authority to the device.
+  temp_certificate_dir = tempfile.mkdtemp()
+  wpr_ca_cert_path = os.path.join(temp_certificate_dir, 'testca.pem')
+  certutils.write_dummy_ca_cert(*certutils.generate_dummy_ca_cert(),
+                                cert_path=wpr_ca_cert_path)
+
+  device_cert_util = adb_install_cert.AndroidCertInstaller(
+      device.adb.GetDeviceSerial(), None, wpr_ca_cert_path)
+  device_cert_util.install_cert(overwrite_cert=True)
+  wpr_server_args.extend(['--should_generate_certs',
+                          '--https_root_ca_cert_path=' + wpr_ca_cert_path])
+
+  # Set up WPR server and device forwarder.
   wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
       '127.0.0.1', 0, 0, None, wpr_server_args)
   ports = wpr_server.StartServer()[:-1]
@@ -155,6 +173,10 @@ def WprHost(device, wpr_archive_path, record=False):
     forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
     wpr_server.StopServer()
 
+    # Remove certification authority from the device.
+    device_cert_util.remove_cert()
+    shutil.rmtree(temp_certificate_dir)
+
 @contextlib.contextmanager
 def DeviceConnection(device,
                      package=DEFAULT_CHROME_PACKAGE,

commit 6f4253a4bea52030b61909e9a35f2d4810b3133a
Author: gabadie <gabadie@chromium.org>
Date:   Wed Feb 10 07:38:04 2016 -0800

    sandwich: Adds web page replay support for HTTP
    
    Adds to option --wpr-{archive,record} to respectively run with or
    generate the benchmark's Web Page Replay archive. Https urls
    support are postponed to another CL.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1684653003
    
    Cr-Original-Commit-Position: refs/heads/master@{#374662}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 90f06edabf6cb2880141ad3dfecd775a6559db2d

diff --git a/loading/device_setup.py b/loading/device_setup.py
index c384707..4986957 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -16,12 +16,19 @@ _SRC_DIR = os.path.abspath(os.path.join(
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
 from devil.android import device_utils
+from devil.android import forwarder
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 from pylib import flag_changer
 
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
+from chrome_telemetry_build import chromium_config
+
+sys.path.append(chromium_config.GetTelemetryDir())
+from telemetry.internal.util import webpagereplay
+
 import devtools_monitor
 
 DEVTOOLS_PORT = 9222
@@ -104,12 +111,58 @@ def _SetUpDevice(device, package_info):
 
 
 @contextlib.contextmanager
+def WprHost(device, wpr_archive_path, record=False):
+  """Launches web page replay host.
+
+  Args:
+    device: Android device.
+    wpr_archive_path: host sided WPR archive's path.
+    record: Enables or disables WPR archive recording.
+
+  Returns:
+    Additional flags list that may be used for chromium to load web page through
+    the running web page replay host.
+  """
+  assert device
+  if wpr_archive_path == None:
+    yield []
+    return
+
+  wpr_server_args = ['--use_closest_match']
+  if record:
+    wpr_server_args.append('--record')
+    if os.path.exists(wpr_archive_path):
+      os.remove(wpr_archive_path)
+  else:
+    assert os.path.exists(wpr_archive_path)
+  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
+      '127.0.0.1', 0, 0, None, wpr_server_args)
+  ports = wpr_server.StartServer()[:-1]
+  host_http_port = ports[0]
+  host_https_port = ports[1]
+
+  forwarder.Forwarder.Map([(0, host_http_port), (0, host_https_port)], device)
+  device_http_port = forwarder.Forwarder.DevicePortForHostPort(host_http_port)
+  device_https_port = forwarder.Forwarder.DevicePortForHostPort(host_https_port)
+
+  try:
+    yield [
+      '--host-resolver-rules="MAP * 127.0.0.1,EXCLUDE localhost"',
+      '--testing-fixed-http-port={}'.format(device_http_port),
+      '--testing-fixed-https-port={}'.format(device_https_port)]
+  finally:
+    forwarder.Forwarder.UnmapDevicePort(device_http_port, device)
+    forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
+    wpr_server.StopServer()
+
+@contextlib.contextmanager
 def DeviceConnection(device,
                      package=DEFAULT_CHROME_PACKAGE,
                      hostname=DEVTOOLS_HOSTNAME,
                      port=DEVTOOLS_PORT,
                      host_exe='out/Release/chrome',
-                     host_profile_dir=None):
+                     host_profile_dir=None,
+                     additional_flags=None):
   """Context for starting recording on a device.
 
   Sets up and restores any device and tracing appropriately
@@ -122,6 +175,7 @@ def DeviceConnection(device,
     host_exe: The binary to execute when running on the host.
     host_profile_dir: The profile dir to use when running on the host (if None,
       a fresh profile dir will be used).
+    additional_flags: Additional chromium arguments.
 
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
@@ -131,6 +185,8 @@ def DeviceConnection(device,
   new_flags = ['--disable-fre',
                '--enable-test-events',
                '--remote-debugging-port=%d' % port]
+  if additional_flags != None:
+    new_flags.extend(additional_flags)
   if device:
     _SetUpDevice(device, package_info)
   with FlagReplacer(device, command_line_path, new_flags):
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 7b688a7..8286c3c 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -160,6 +160,10 @@ def main():
                       action='store_true',
                       help='Clear HTTP cache before start,' +
                       'save cache before exit.')
+  parser.add_argument('--wpr-archive', default=None, type=str,
+                      help='Web page replay archive to load job\'s urls from.')
+  parser.add_argument('--wpr-record', default=False, action='store_true',
+                      help='Record web page replay archive.')
   args = parser.parse_args()
 
   try:
@@ -171,21 +175,26 @@ def main():
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
 
-  pages_loaded = 0
-  for iteration in xrange(args.repeat):
-    for url in job_urls:
-      with device_setup.DeviceConnection(device) as connection:
-        if iteration == 0 and pages_loaded == 0 and args.save_cache:
-          connection.ClearCache()
-        page_track.PageTrack(connection)
-        tracing_track = tracing.TracingTrack(connection,
-            categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
-        connection.SetUpMonitoring()
-        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-        connection.StartMonitoring()
-        pages_loaded += 1
-        _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
-            str(pages_loaded))
+  with device_setup.WprHost(device,
+                            args.wpr_archive,
+                            args.wpr_record) as additional_flags:
+    pages_loaded = 0
+    for iteration in xrange(args.repeat):
+      for url in job_urls:
+        with device_setup.DeviceConnection(
+            device=device,
+            additional_flags=additional_flags) as connection:
+          if iteration == 0 and pages_loaded == 0 and args.save_cache:
+            connection.ClearCache()
+          page_track.PageTrack(connection)
+          tracing_track = tracing.TracingTrack(connection,
+              categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
+          connection.SetUpMonitoring()
+          connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+          connection.StartMonitoring()
+          pages_loaded += 1
+          _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
+              str(pages_loaded))
 
   if args.save_cache:
     # Move Chrome to background to allow it to flush the index.

commit 027c959b8ab9e70bd7258c1918dd24460d54bbfa
Author: mostynb <mostynb@opera.com>
Date:   Mon Feb 8 15:27:20 2016 -0800

    update obsolete code.google.com documentation links
    
    This is a documentation-only change.
    
    Disabling presubmit checks, due to "noparent" settings for the following files:
    components/policy/resources/policy_templates.json
    content/common/font_config_ipc_linux.h
    
    BUG=567488
    NOPRESUBMIT=true
    TBR=atwilson,dcheng
    
    Review URL: https://codereview.chromium.org/1592403002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374213}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: df175a8c3d1c1060f96d123a5896fdfbe4b3782d

diff --git a/eclipse/.classpath b/eclipse/.classpath
index fb1fd36..b740bfe 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -2,7 +2,7 @@
 <!-- {% block header_message %}
 
 Eclipse classpath. See instructions at
-https://code.google.com/p/chromium/wiki/AndroidEclipseDev for setting up Eclipse
+https://www.chromium.org/developers/android-eclipse-dev for setting up Eclipse
 for Chrome Android development.
 
 Obsolete entries can be found using:
diff --git a/findbugs_plugin/findbugs.xml b/findbugs_plugin/findbugs.xml
index a1c4b5e..002e841 100644
--- a/findbugs_plugin/findbugs.xml
+++ b/findbugs_plugin/findbugs.xml
@@ -9,7 +9,7 @@
         xsi:noNamespaceSchemaLocation="findbugsplugin.xsd"
         pluginid="SynchronizedThisDetector"
         provider="chromium"
-        website="http://code.google.com/p/chromium/wiki/UseFindBugsForAndroid">
+        website="https://chromium.googlesource.com/chromium/src/+/master/docs/use_find_bugs_for_android.md">
         <Detector class="org.chromium.tools.findbugs.plugin.SynchronizedThisDetector" reports="CHROMIUM_SYNCHRONIZED_THIS" />
         <BugPattern type="CHROMIUM_SYNCHRONIZED_THIS" abbrev="CHROMIUM" category="CORRECTNESS"/>
 

commit e0c8b2afb00ba99c968a5d50f6c7b410d2b2df54
Author: newt <newt@chromium.org>
Date:   Mon Feb 8 15:24:51 2016 -0800

    Clean up dead code related to enhanced_bookmarks.
    
    This deletes the last bit of components/enhanced_bookmarks (by moving
    the enum into chrome/android/java, the only place where it's used),
    and cleans up dead code related to enhanced bookmarks.
    
    BUG=474719
    
    Review URL: https://codereview.chromium.org/1664503002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374210}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 407d1de019d5baadbd5eda34cc8f852da1ec41fc

diff --git a/eclipse/.classpath b/eclipse/.classpath
index ab36257..fb1fd36 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -115,7 +115,6 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/content_setting_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_settings_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/cronet_url_request_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/enhanced_bookmarks_java_enums_srcjar"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/gesture_event_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/infobar_action_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/invalidate_types_java"/>

commit 5916bb1e485c56d3ac5828a8c3d288c17d591744
Author: blundell <blundell@chromium.org>
Date:   Mon Feb 8 07:36:49 2016 -0800

    [loading] Extend ability to run on host
    
    This extends the functionality of running on the host:
    - Starts the executable before navigation
    - Kills it when done
    - Allows passing the executable
    - Allows running with a specified or fresh profile (the latter is currently
      blocked on a fix to crbug.com/xxx)
    
    Review URL: https://codereview.chromium.org/1669283002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374126}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5ff3c6f038f45615cd53aed19790d6b82710e8e2

diff --git a/loading/analyze.py b/loading/analyze.py
index 1b1a2a3..5695e7d 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -95,29 +95,36 @@ def _GetPrefetchHtml(graph, name=None):
   return '\n'.join(output)
 
 
-def _LogRequests(url, clear_cache=True, local=False):
+def _LogRequests(url, clear_cache=True, local=False,
+                 host_exe="out/Release/chrome", host_profile_dir=None):
   """Log requests for a web page.
 
   Args:
     url: url to log as string.
     clear_cache: optional flag to clear the cache.
     local: log from local (desktop) chrome session.
+    host_exe: Binary to execute when running locally.
+    host_profile_dir: Profile dir to use when running locally (if None, a
+      fresh profile dir will be used).
 
   Returns:
     JSON dict of logged information (ie, a dict that describes JSON).
   """
   device = device_setup.GetFirstDevice() if not local else None
-  with device_setup.DeviceConnection(device) as connection:
+  with device_setup.DeviceConnection(device, host_exe=host_exe,
+      host_profile_dir=host_profile_dir) as connection:
     trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
     return trace.ToJsonDict()
 
 
-def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
+def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
+               host_exe, host_profile_dir):
   """Do a full fetch with optional prefetching."""
   if not url.startswith('http'):
     url = 'http://' + url
   logging.warning('Cold fetch')
-  cold_data = _LogRequests(url, local=local)
+  cold_data = _LogRequests(url, local=local,
+                           host_exe=host_exe, host_profile_dir=host_profile_dir)
   assert cold_data, 'Cold fetch failed to produce data. Check your phone.'
   if prefetch:
     assert not local
@@ -258,12 +265,16 @@ def DoLogRequests(arg_str):
   parser.add_argument('--prefetch', action='store_true')
   parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
   parser.add_argument('--local', action='store_true')
+  parser.add_argument('--host_exe', default='out/Release/chrome')
+  parser.add_argument('--host_profile_dir', default=None)
   args = parser.parse_args(arg_str)
   _FullFetch(url=args.url,
              json_output=args.output,
              prefetch=args.prefetch,
              prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=args.local)
+             local=args.local,
+             host_exe=args.host_exe,
+             host_profile_dir=args.host_profile_dir)
 
 
 def DoFetch(arg_str):
@@ -284,7 +295,9 @@ def DoFetch(arg_str):
              json_output=os.path.join(args.dir, args.site + '.json'),
              prefetch=True,
              prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=False)
+             local=False,
+             host_exe=None,
+             host_profile_dir=None)
 
 
 def DoLongPole(arg_str):
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 0f24668..c384707 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -5,7 +5,10 @@
 import contextlib
 import logging
 import os
+import shutil
+import subprocess
 import sys
+import tempfile
 import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -26,6 +29,18 @@ DEVTOOLS_HOSTNAME = 'localhost'
 DEFAULT_CHROME_PACKAGE = 'chrome'
 
 
+@contextlib.contextmanager
+def TemporaryDirectory():
+  """Returns a freshly-created directory that gets automatically deleted after
+  usage.
+  """
+  name = tempfile.mkdtemp()
+  try:
+    yield name
+  finally:
+    shutil.rmtree(name)
+
+
 class DeviceSetupException(Exception):
   def __init__(self, msg):
     super(DeviceSetupException, self).__init__(msg)
@@ -92,7 +107,9 @@ def _SetUpDevice(device, package_info):
 def DeviceConnection(device,
                      package=DEFAULT_CHROME_PACKAGE,
                      hostname=DEVTOOLS_HOSTNAME,
-                     port=DEVTOOLS_PORT):
+                     port=DEVTOOLS_PORT,
+                     host_exe='out/Release/chrome',
+                     host_profile_dir=None):
   """Context for starting recording on a device.
 
   Sets up and restores any device and tracing appropriately
@@ -100,7 +117,11 @@ def DeviceConnection(device,
   Args:
     device: Android device, or None for a local run (in which case chrome needs
       to have been started with --remote-debugging-port=XXX).
-    package: the key for chrome package info.
+    package: The key for chrome package info.
+    port: The port on which to enable remote debugging.
+    host_exe: The binary to execute when running on the host.
+    host_profile_dir: The profile dir to use when running on the host (if None,
+      a fresh profile dir will be used).
 
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
@@ -113,16 +134,35 @@ def DeviceConnection(device,
   if device:
     _SetUpDevice(device, package_info)
   with FlagReplacer(device, command_line_path, new_flags):
+    host_process = None
     if device:
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
       device.StartActivity(start_intent, blocking=True)
+    else:
+      # Run on the host.
+      assert os.path.exists(host_exe)
+
+      user_data_dir = host_profile_dir
+      if not user_data_dir:
+        user_data_dir = TemporaryDirectory()
+
+      new_flags += ['--user-data-dir=%s' % user_data_dir]
+      host_process = subprocess.Popen([host_exe] + new_flags,
+                                      shell=False)
+    if device:
       time.sleep(2)
+    else:
+      # TODO(blundell): Figure out why a lower sleep time causes an assertion
+      # in request_track.py to fire.
+      time.sleep(10)
     # If no device, we don't care about chrome startup so skip the about page.
     with ForwardPort(device, 'tcp:%d' % port,
                      'localabstract:chrome_devtools_remote'):
       yield devtools_monitor.DevToolsConnection(hostname, port)
+    if host_process:
+      host_process.kill()
 
 
 def SetUpAndExecute(device, package, fn):

commit c5cb8f90750eb8ac1f516220be4292154b3eefdc
Author: mattcary <mattcary@chromium.org>
Date:   Thu Feb 4 12:43:19 2016 -0800

    Lens for user satisfied metrics.
    
    Currently uses first contentful paint, but could be easily generalized or
    changed if we saw a need.
    
    This is also an experiment with a different sort of lens, that provides a filter
    rather than dependancy information.
    
    Review URL: https://codereview.chromium.org/1658933003
    
    Cr-Original-Commit-Position: refs/heads/master@{#373611}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8ba13fc45df1ba8c588bbe8abe4053d5c0f3145c

diff --git a/loading/test_utils.py b/loading/test_utils.py
index cc24f36..2e5bb74 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -7,6 +7,7 @@
 import devtools_monitor
 import loading_trace
 import page_track
+import tracing
 
 
 class FakeRequestTrack(devtools_monitor.Track):
@@ -39,8 +40,15 @@ class FakePageTrack(devtools_monitor.Track):
     return event['frame_id']
 
 
-def LoadingTraceFromEvents(requests, page_events=None):
+def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
   request_track = FakeRequestTrack(requests)
-  page = FakePageTrack(page_events if page_events else [])
-  return loading_trace.LoadingTrace(None, None, page, request_track, None)
+  page_event_track = FakePageTrack(page_events if page_events else [])
+  if trace_events:
+    tracing_track = tracing.TracingTrack(None)
+    tracing_track.Handle('Tracing.dataCollected',
+                         {'params': {'value': [e for e in trace_events]}})
+  else:
+    tracing_track = None
+  return loading_trace.LoadingTrace(
+      None, None, page_event_track, request_track, tracing_track)
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
new file mode 100644
index 0000000..0b23342
--- /dev/null
+++ b/loading/user_satisfied_lens.py
@@ -0,0 +1,51 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Identifies a user satisfied event, and marks all the relationship of all
+model events accordingly.
+"""
+
+import logging
+
+
+class UserSatisfiedLens(object):
+  def __init__(self, trace, graph):
+    """Initialize the lens.
+
+    Args:
+      trace: (LoadingTrace) the trace to use in the analysis.
+      graph: (ResourceGraph) the graph to annotate, using the current filter set
+        on the graph.
+    """
+    satisfied_time = self._GetFirstContentfulPaintTime(trace.tracing_track)
+    self._satisfied_nodes = set(self._GetNodesAfter(
+        graph.Nodes(), satisfied_time))
+
+  def Filter(self, node):
+    """A ResourceGraph filter.
+
+    Meant to be used in some_graph.Set(node_filter=this_lens.Filter).
+
+    Args:
+      node: a ResourceGraph NodeInfo.
+
+    Returns:
+      True iff the node occurred before user satisfaction occurred.
+    """
+    return node not in self._satisfied_nodes
+
+  # TODO(mattcary): hoist to base class?
+  @classmethod
+  def _GetNodesAfter(cls, nodes, time):
+    return (n for n in nodes if n.StartTime() >= time)
+
+  def _GetFirstContentfulPaintTime(self, tracing_track):
+    first_paints = [e.start_msec for e in tracing_track.GetEvents()
+                    if (e.tracing_event['name'] == 'firstContentfulPaint' and
+                        'blink.user_timing' in e.tracing_event['cat'])]
+    if len(first_paints) != 1:
+      # TODO(mattcary): in some cases a trace has two contentful paints. Why?
+      logging.error('%d first paints with spread of %s', len(first_paints),
+                    max(first_paints) - min(first_paints))
+    return min(first_paints)
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
new file mode 100644
index 0000000..7b16f23
--- /dev/null
+++ b/loading/user_satisfied_lens_unittest.py
@@ -0,0 +1,56 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import loading_model
+import request_track
+import test_utils
+import user_satisfied_lens
+
+class UserSatisfiedLensTestCase(unittest.TestCase):
+  def setUp(self):
+    super(UserSatisfiedLensTestCase, self).setUp()
+    self._request_index = 1
+
+  def _RequestAt(self, timestamp_msec, duration=1):
+    timestamp_sec = float(timestamp_msec) / 1000
+    rq = request_track.Request.FromJsonDict({
+        'url': 'http://bla-%s-.com' % timestamp_msec,
+        'request_id': '1234.%s' % self._request_index,
+        'frame_id': '123.%s' % timestamp_msec,
+        'initiator': {'type': 'other'},
+        'timestamp': timestamp_sec,
+        'timing': request_track.TimingFromDict({
+            'requestTime': timestamp_sec,
+            'loadingFinished': duration})
+        })
+    self._request_index += 1
+    return rq
+
+  def testUserSatisfiedLens(self):
+    # We track all times in milliseconds, but raw trace events are in
+    # microseconds.
+    MILLI_TO_MICRO = 1000
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink.some_other_user_timing',
+                       'name': 'firstContentfulPaint'},
+                      {'ts': 9 * MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstDiscontentPaint'},
+                      {'ts': 12 * MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint'},
+                      {'ts': 22 * MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint'}])
+    graph = loading_model.ResourceGraph(loading_trace)
+    lens = user_satisfied_lens.UserSatisfiedLens(loading_trace, graph)
+    for n in graph.Nodes():
+      if n.Request().frame_id == '123.20':
+        self.assertFalse(lens.Filter(n))
+      else:
+        self.assertTrue(lens.Filter(n))

commit b4dbc20a942f4e7abb95fa8c2b9b79941a0354e4
Author: gogerald <gogerald@chromium.org>
Date:   Thu Feb 4 11:06:01 2016 -0800

    Implementation of newly designed sign in related histograms for Android.
    
    This CL is dedicated to implement newly designed sign in related histograms (Signin.SigninStartedAccessPoint, Signin.SigninCompletedAccessPoint, Signin.SigninReason) for Android. Please refer https://docs.google.com/a/google.com/document/d/1-gXYAMXXgsJhk6jxO55RuYJ00JBGermevJZ0sIlk6ko/edit?usp=sharing for details.
    
    BUG=532557
    
    Review URL: https://codereview.chromium.org/1578433002
    
    Cr-Original-Commit-Position: refs/heads/master@{#373577}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5a9e2b34d01056dfdc1a60ce418f5e9f884c9967

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 3f65bf4..ab36257 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -136,6 +136,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/result_codes_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/screen_orientation_values_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/selection_event_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/signin_metrics_enum_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/speech_recognition_error_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/system_ui_resource_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/tab_load_status_java"/>

commit 5e475ccf6b29ac33b33c03ca89a46aa1a33d8174
Author: lizeb <lizeb@chromium.org>
Date:   Thu Feb 4 08:17:48 2016 -0800

    tools/android/loading: Map the renderer main thread activity during intervals.
    
    Review URL: https://codereview.chromium.org/1670583002
    
    Cr-Original-Commit-Position: refs/heads/master@{#373545}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8a81a4ad08bffced9db75e261827d6dd94595aa8

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
new file mode 100644
index 0000000..2973ac4
--- /dev/null
+++ b/loading/activity_lens.py
@@ -0,0 +1,179 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gives a picture of the CPU activity between timestamps.
+
+When executed as a script, takes a loading trace, and prints the activity
+breakdown for the request dependencies.
+"""
+
+import collections
+import logging
+import operator
+
+
+class ActivityLens(object):
+  """Reconstructs the activity of the main renderer thread between requests."""
+  def __init__(self, trace):
+    """Initializes an instance of ActivityLens.
+
+    Args:
+      trace: (LoadingTrace) loading trace.
+    """
+    self._trace = trace
+    events = trace.tracing_track.GetEvents()
+    self._renderer_main_tid = self._GetRendererMainThreadId(events)
+
+  @classmethod
+  def _GetRendererMainThreadId(cls, events):
+    """Returns the most active main renderer thread.
+
+    Several renderers may be running concurrently, but we assume that only one
+    of them is busy during the time covered by the loading trace.. It can be
+    selected by looking at the number of trace events generated.
+
+    Args:
+      events: [tracing.Event] List of trace events.
+
+    Returns:
+      The thread ID (int) of the busiest renderer main thread.
+
+    """
+    events_count_per_tid = collections.defaultdict(int)
+    main_renderer_thread_ids = set()
+    for event in events:
+      tracing_event = event.tracing_event
+      tid = event.tracing_event['tid']
+      events_count_per_tid[tid] += 1
+      if (tracing_event['cat'] == '__metadata'
+          and tracing_event['name'] == 'thread_name'
+          and event.args['name'] == 'CrRendererMain'):
+        main_renderer_thread_ids.add(tid)
+    tid_events_counts = sorted(events_count_per_tid.items(),
+                               key=operator.itemgetter(1), reverse=True)
+    if (len(tid_events_counts) > 1
+        and tid_events_counts[0][1] < 2 * tid_events_counts[1][1]):
+      logging.warning(
+          'Several active renderers (%d and %d with %d and %d events).'
+          % (tid_events_counts[0][0], tid_events_counts[1][0],
+             tid_events_counts[0][1], tid_events_counts[1][1]))
+    return tid_events_counts[0][0]
+
+  def _OverlappingEventsForTid(self, tid, start_msec, end_msec):
+    events = self._trace.tracing_track.OverlappingEvents(start_msec, end_msec)
+    return [e for e in events if e.tracing_event['tid'] == tid]
+
+  @classmethod
+  def _ClampedDuration(cls, event, start_msec, end_msec):
+      return max(0, (min(end_msec, event.end_msec)
+                     - max(start_msec, event.start_msec)))
+
+  @classmethod
+  def _ThreadBusiness(cls, events, start_msec, end_msec):
+    """Amount of time a thread spent executing from the message loop."""
+    busy_duration = 0
+    message_loop_events = [
+        e for e in events
+        if (e.tracing_event['cat'] == 'toplevel'
+            and e.tracing_event['name'] == 'MessageLoop::RunTask')]
+    for event in message_loop_events:
+      clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
+      busy_duration += clamped_duration
+    interval_msec = end_msec - start_msec
+    assert busy_duration <= interval_msec
+    return busy_duration
+
+  @classmethod
+  def _ScriptsExecuting(cls, events, start_msec, end_msec):
+    """Returns the time during which scripts executed within an interval.
+
+    Args:
+      events: ([tracing.Event]) list of tracing events.
+      start_msec: (float) start time in ms, inclusive.
+      end_msec: (float) end time in ms, inclusive.
+
+    Returns:
+      A dict {URL (str) -> duration_msec (float)}. The dict may have a None key
+      for scripts that aren't associated with a URL.
+    """
+    script_to_duration = collections.defaultdict(float)
+    script_events = [e for e in events
+                     if ('devtools.timeline' in e.tracing_event['cat']
+                         and e.tracing_event['name'] in (
+                             'EvaluateScript', 'FunctionCall'))]
+    for event in script_events:
+      clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
+      script_url = event.args['data'].get('scriptName', None)
+      script_to_duration[script_url] += clamped_duration
+    return dict(script_to_duration)
+
+  @classmethod
+  def _Parsing(cls, events, start_msec, end_msec):
+    """Returns the HTML/CSS parsing time within an interval.
+
+    Args:
+      events: ([tracing.Event]) list of events.
+      start_msec: (float) start time in ms, inclusive.
+      end_msec: (float) end time in ms, inclusive.
+
+    Returns:
+      A dict {URL (str) -> duration_msec (float)}. The dict may have a None key
+      for tasks that aren't associated with a URL.
+    """
+    url_to_duration = collections.defaultdict(float)
+    parsing_events = [e for e in events
+                      if ('devtools.timeline' in e.tracing_event['cat']
+                          and e.tracing_event['name'] in (
+                              'ParseHTML', 'ParseAuthorStyleSheet'))]
+    for event in parsing_events:
+      tracing_event = event.tracing_event
+      clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
+      if tracing_event['name'] == 'ParseAuthorStyleSheet':
+        url = tracing_event['args']['data']['styleSheetUrl']
+      else:
+        url = tracing_event['args']['beginData']['url']
+      url_to_duration[url] += clamped_duration
+    return dict(url_to_duration)
+
+  def ExplainEdgeCost(self, dep):
+    """For a dependency between two requests, returns the renderer activity
+    breakdown.
+
+    Args:
+      dep: (Request, Request, str) As returned from
+           RequestDependencyLens.GetRequestDependencies().
+
+    Returns:
+      {'edge_cost': (float) ms, 'busy': (float) ms,
+       'parsing': {'url' -> time_ms}, 'script' -> {'url' -> time_ms}}
+    """
+    (first, second, _) = dep
+    # TODO(lizeb): Refactor the edge cost computations.
+    start_msec = first.start_msec
+    end_msec = second.start_msec
+    assert end_msec - start_msec >= 0.
+    tid = self._renderer_main_tid
+    events = self._OverlappingEventsForTid(tid, start_msec, end_msec)
+    result = {'edge_cost': end_msec - start_msec,
+              'busy': self._ThreadBusiness(events, start_msec, end_msec),
+              'parsing': self._Parsing(events, start_msec, end_msec),
+              'script': self._ScriptsExecuting(events, start_msec, end_msec)}
+    return result
+
+
+if __name__ == '__main__':
+  import sys
+  import json
+  import loading_trace
+  import request_dependencies_lens
+
+  filename = sys.argv[1]
+  json_dict = json.load(open(filename))
+  loading_trace = loading_trace.LoadingTrace.FromJsonDict(json_dict)
+  activity_lens = ActivityLens(loading_trace)
+  dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+      loading_trace)
+  deps = dependencies_lens.GetRequestDependencies()
+  for requests_dep in deps:
+    print activity_lens.ExplainEdgeCost(requests_dep)
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
new file mode 100644
index 0000000..a84c864
--- /dev/null
+++ b/loading/activity_lens_unittest.py
@@ -0,0 +1,175 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from activity_lens import ActivityLens
+import tracing
+
+
+class ActivityLensTestCast(unittest.TestCase):
+  @classmethod
+  def _EventsFromRawEvents(cls, raw_events):
+    tracing_track = tracing.TracingTrack(None)
+    tracing_track.Handle(
+        'Tracing.dataCollected', {'params': {'value': raw_events}})
+    return tracing_track.GetEvents()
+
+  def setUp(self):
+    self.tracing_track = tracing.TracingTrack(None)
+
+  def testGetRendererMainThread(self):
+    first_renderer_tid = 12345
+    second_renderer_tid = 123456
+    raw_events =  [
+        {u'args': {u'name': u'CrBrowserMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 123,
+         u'tid': 123,
+         u'ts': 0},
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 1234,
+         u'tid': first_renderer_tid,
+         u'ts': 0},
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 12345,
+         u'tid': second_renderer_tid,
+         u'ts': 0}]
+    raw_events += [
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 32723,
+         u'tdur': 0,
+         u'tid': first_renderer_tid,
+         u'ts': 251427174674,
+         u'tts': 5107725}] * 100
+    raw_events += [
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 1234,
+         u'tdur': 0,
+         u'tid': second_renderer_tid,
+         u'ts': 251427174674,
+         u'tts': 5107725}] * 150
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(second_renderer_tid,
+                      ActivityLens._GetRendererMainThreadId(events))
+
+  def testThreadBusiness(self):
+    raw_events =  [
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 200 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 56485},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 8 * 200,
+         u'name': u'MessageLoop::NestedSomething',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 0}]
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(200, ActivityLens._ThreadBusiness(events, 0, 1000))
+    # Clamping duration.
+    self.assertEquals(100, ActivityLens._ThreadBusiness(events, 0, 100))
+    self.assertEquals(50, ActivityLens._ThreadBusiness(events, 25, 75))
+
+  def testScriptExecuting(self):
+    url = u'http://example.com/script.js'
+    raw_events = [
+        {u'args': {u'data': {u'scriptName': url}},
+         u'cat': u'devtools.timeline,v8',
+         u'dur': 250 * 1000,
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tdur': 247,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 0},
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'dur': 350 * 1000,
+         u'name': u'EvaluateScript',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tdur': 247,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 0}]
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(2, len(ActivityLens._ScriptsExecuting(events, 0, 1000)))
+    self.assertTrue(None in ActivityLens._ScriptsExecuting(events, 0, 1000))
+    self.assertEquals(
+        350, ActivityLens._ScriptsExecuting(events, 0, 1000)[None])
+    self.assertTrue(url in ActivityLens._ScriptsExecuting(events, 0, 1000))
+    self.assertEquals(250, ActivityLens._ScriptsExecuting(events, 0, 1000)[url])
+    # Aggreagates events.
+    raw_events.append({u'args': {u'data': {}},
+                       u'cat': u'devtools.timeline,v8',
+                       u'dur': 50 * 1000,
+                       u'name': u'EvaluateScript',
+                       u'ph': u'X',
+                       u'pid': 123,
+                       u'tdur': 247,
+                       u'tid': 123,
+                       u'ts': 0,
+                       u'tts': 0})
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(
+        350 + 50, ActivityLens._ScriptsExecuting(events, 0, 1000)[None])
+
+  def testParsing(self):
+    css_url = u'http://example.com/style.css'
+    html_url = u'http://example.com/yeah.htnl'
+    raw_events = [
+        {u'args': {u'data': {u'styleSheetUrl': css_url}},
+         u'cat': u'blink,devtools.timeline',
+         u'dur': 400 * 1000,
+         u'name': u'ParseAuthorStyleSheet',
+         u'ph': u'X',
+         u'pid': 32723,
+         u'tdur': 49721,
+         u'tid': 32738,
+         u'ts': 0,
+         u'tts': 216148},
+        {u'args': {u'beginData': {u'url': html_url}},
+         u'cat': u'devtools.timeline',
+         u'dur': 42 * 1000,
+         u'name': u'ParseHTML',
+         u'ph': u'X',
+         u'pid': 32723,
+         u'tdur': 49721,
+         u'tid': 32738,
+         u'ts': 0,
+         u'tts': 5032310},]
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(2, len(ActivityLens._Parsing(events, 0, 1000)))
+    self.assertTrue(css_url in ActivityLens._Parsing(events, 0, 1000))
+    self.assertEquals(400, ActivityLens._Parsing(events, 0, 1000)[css_url])
+    self.assertTrue(html_url in ActivityLens._Parsing(events, 0, 1000))
+    self.assertEquals(42, ActivityLens._Parsing(events, 0, 1000)[html_url])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index f4cf0e6..52e22fb 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -28,6 +28,7 @@ _TIMING_NAMES_MAPPING = {
 
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
+
 def TimingAsList(timing):
   """Transform Timing to a list, eg as is used in JSON output.
 
@@ -40,6 +41,7 @@ def TimingAsList(timing):
   """
   return json.loads(json.dumps(timing))
 
+
 class Request(object):
   """Represents a single request.
 
diff --git a/loading/trace_to_chrome_trace.py b/loading/trace_to_chrome_trace.py
index 998614f..382d87d 100755
--- a/loading/trace_to_chrome_trace.py
+++ b/loading/trace_to_chrome_trace.py
@@ -5,8 +5,8 @@
 
 """Convert trace output for Chrome.
 
-Take the tracing track output from tracing_driver.py to a zip'd json that can be
-loading by chrome devtools tracing.
+Takes a loading trace from 'analyze.py log_requests' and outputs a zip'd json
+that can be loaded by chrome's about:tracing..
 """
 
 import argparse
@@ -19,5 +19,5 @@ if __name__ == '__main__':
   parser.add_argument('output')
   args = parser.parse_args()
   with gzip.GzipFile(args.output, 'w') as output_f, file(args.input) as input_f:
-    events = json.load(input_f)
+    events = json.load(input_f)['tracing_track']['events']
     json.dump({'traceEvents': events, 'metadata': {}}, output_f)
diff --git a/loading/tracing.py b/loading/tracing.py
index bc4d45e..c69ffaf 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -110,36 +110,47 @@ class TracingTrack(devtools_monitor.Track):
         tracing_track._base_msec = e.start_msec
     return tracing_track
 
-  def EventsEndingBetween(self, start_msec, end_msec):
-    """Gets the list of events whose end lies in a range.
+  def OverlappingEvents(self, start_msec, end_msec):
+    """Gets the list of events overlapping with an interval.
 
     Args:
       start_msec: the start of the range to query, in milliseconds, inclusive.
       end_msec: the end of the range to query, in milliseconds, inclusive.
 
     Returns:
-      List of events whose end time lies in the range. Note that although the
-      range is inclusive at both ends, an ending timestamp is considered to be
-      exclusive of the actual event. An event ending at 10 msec will be returned
-      for a range [10, 14] as well as [8, 10], though the event is considered to
-      end the instant before 10 msec. In practice this is only important when
-      considering how events overlap; an event ending at 10 msec does not
-      overlap with one starting at 10 msec and so may unambiguously share ids,
-      etc.
+      List of events overlapping with the range. Events are overlapping only if
+      the overlap is strictly larger than 0.
     """
     self._IndexEvents()
     low_idx = bisect.bisect_left(self._event_msec_index, start_msec) - 1
     high_idx = bisect.bisect_right(self._event_msec_index, end_msec)
-    matched_events = []
+    matched_events = set()
     for i in xrange(max(0, low_idx), high_idx):
       if self._event_lists[i]:
         for e in self._event_lists[i].event_list:
-          assert e.end_msec is not None
-          if e.end_msec >= start_msec and e.end_msec <= end_msec:
-            matched_events.append(e)
-    return matched_events
+          if e.end_msec is None:
+            continue
+          overlap_duration = max(
+              0, min(end_msec, e.end_msec) - max(start_msec, e.start_msec))
+          if overlap_duration > 0:
+            matched_events.add(e)
+    return list(matched_events)
+
+  def EventsEndingBetween(self, start_msec, end_msec):
+    """Gets the list of events ending within an interval.
 
-  def _IndexEvents(self):
+    Args:
+      start_msec: the start of the range to query, in milliseconds, inclusive.
+      end_msec: the end of the range to query, in milliseconds, inclusive.
+
+    Returns:
+      See OverlappingEvents() above.
+    """
+    overlapping_events = self.OverlappingEvents(start_msec, end_msec)
+    return [e for e in overlapping_events
+            if start_msec <= e.end_msec <= end_msec]
+
+  def _IndexEvents(self, strict=False):
     """Computes index for in-flight events.
 
     Creates a list of timestamps where events start or end, and tracks the
@@ -183,7 +194,8 @@ class TracingTrack(devtools_monitor.Track):
           if e.end_msec is not None and e.end_msec <= current_msec])
       self._event_msec_index.append(current_msec)
       self._event_lists.append(self._EventList(current_events))
-    if spanning_events.HasPending():
+
+    if strict and spanning_events.HasPending():
       raise devtools_monitor.DevToolsConnectionException(
           'Pending spanning events: %s' %
           '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
@@ -202,8 +214,9 @@ class TracingTrack(devtools_monitor.Track):
           'F': self._AsyncEnd,
           'N': self._ObjectCreated,
           'D': self._ObjectDestroyed,
-          'X': self._Ignore,
           'M': self._Ignore,
+          'X': self._Ignore,
+          'R': self._Ignore,
           None: self._Ignore,
           }
 
@@ -315,6 +328,7 @@ class Event(object):
                     'e': 'b',
                     'F': 'S',
                     'D': 'N'}
+  __slots__ = ('_tracing_event', 'start_msec', 'end_msec', '_synthetic')
   def __init__(self, tracing_event, synthetic=False):
     """Creates Event.
 
@@ -334,22 +348,14 @@ class Event(object):
 
     self._tracing_event = tracing_event
     # Note tracing event times are in microseconds.
-    self._start_msec = tracing_event['ts'] / 1000.0
-    self._end_msec = None
+    self.start_msec = tracing_event['ts'] / 1000.0
+    self.end_msec = None
     self._synthetic = synthetic
     if self.type == 'X':
       # Some events don't have a duration.
       duration = (tracing_event['dur']
                   if 'dur' in tracing_event else tracing_event['tdur'])
-      self._end_msec = self.start_msec + duration / 1000.0
-
-  @property
-  def start_msec(self):
-    return self._start_msec
-
-  @property
-  def end_msec(self):
-    return self._end_msec
+      self.end_msec = self.start_msec + duration / 1000.0
 
   @property
   def type(self):
@@ -383,6 +389,7 @@ class Event(object):
         'I', 'P', 'c', 'C',
         'n', 'T', 'p',  # TODO(mattcary): ?? instant types of async events.
         'O',            # TODO(mattcary): ?? object snapshot
+        'M'             # Metadata
         ]
 
   def Synthesize(self):
@@ -418,7 +425,7 @@ class Event(object):
         self.id != closing.id):
       raise devtools_monitor.DevToolsConnectionException(
         'Bad async closing: %s --> %s' % (self, closing))
-    self._end_msec = closing.start_msec
+    self.end_msec = closing.start_msec
     if 'args' in closing.tracing_event:
       self.tracing_event.setdefault(
           'args', {}).update(closing.tracing_event['args'])
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index d190f8a..0f30dcc 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import copy
 import logging
 import unittest
 
@@ -24,17 +25,27 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
       {'ts': 15, 'ph': 'D', 'id': 1}]
 
+  _EVENTS = [
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+      {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
+      {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
+      {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
+      {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
+      {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]
+
   def setUp(self):
     self.track = TracingTrack(None)
 
   def EventToMicroseconds(self, event):
-    if 'ts' in event:
-      event['ts'] *= 1000
-    if 'dur' in event:
-      event['dur'] *= 1000
-    return event
+    result = copy.deepcopy(event)
+    if 'ts' in result:
+      result['ts'] *= 1000
+    if 'dur' in result:
+      result['dur'] *= 1000
+    return result
 
   def CheckTrack(self, timestamp, names):
+    self.track._IndexEvents(strict=True)
     self.assertEqual(
         set((e.args['name'] for e in self.track.EventsAt(timestamp))),
         set(names))
@@ -206,15 +217,8 @@ class TracingTrackTestCase(unittest.TestCase):
 
   def testEventsEndingBetween(self):
     self.track.Handle(
-        'Tracing.dataCollected',
-        {'params':
-         {'value': [self.EventToMicroseconds(e) for e in
-          [{'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
-           {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
-           {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
-           {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
-           {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
-           {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]]}})
+        'Tracing.dataCollected', {'params': {'value': [
+            self.EventToMicroseconds(e) for e in self._EVENTS]}})
     self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
                           for e in self.track.EventsEndingBetween(0, 100)]))
@@ -225,24 +229,22 @@ class TracingTrackTestCase(unittest.TestCase):
     self.assertEqual(set('B'),
                      set([e.args['name']
                           for e in self.track.EventsEndingBetween(3, 6)]))
-    self.assertEqual(set('AB'),
-                     set([e.args['name']
-                          for e in self.track.EventsEndingBetween(3, 7)]))
-    self.assertEqual(set('AB'),
-                     set([e.args['name']
-                          for e in self.track.EventsEndingBetween(6, 7)]))
-    self.assertEqual(set('A'),
+
+  def testOverlappingEvents(self):
+    self.track.Handle(
+        'Tracing.dataCollected', {'params': {'value': [
+            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
-                          for e in self.track.EventsEndingBetween(7, 10)]))
-    self.assertEqual(set('AC'),
+                          for e in self.track.OverlappingEvents(0, 100)]))
+    self.assertFalse([e.args['name']
+                      for e in self.track.OverlappingEvents(0, 2)])
+    self.assertEqual(set('BA'),
                      set([e.args['name']
-                          for e in self.track.EventsEndingBetween(7, 11)]))
-    self.assertEqual(set('CD'),
+                          for e in self.track.OverlappingEvents(4, 5.1)]))
+    self.assertEqual(set('ACD'),
                      set([e.args['name']
-                          for e in self.track.EventsEndingBetween(8, 13)]))
-
-
-
+                          for e in self.track.OverlappingEvents(6, 10.1)]))
 
 
 if __name__ == '__main__':

commit 14949a5c7193269c7603f2063b261445b217adb8
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 4 06:15:31 2016 -0800

    tools/android/loading: Automatically --disable-fre at device setup.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1664693005
    
    Cr-Original-Commit-Position: refs/heads/master@{#373524}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 78c75fcb2f86c32aa7a1e96f9aa354bf43e141ce

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 8784024..0f24668 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -107,7 +107,8 @@ def DeviceConnection(device,
   """
   package_info = constants.PACKAGE_INFO[package]
   command_line_path = '/data/local/chrome-command-line'
-  new_flags = ['--enable-test-events',
+  new_flags = ['--disable-fre',
+               '--enable-test-events',
                '--remote-debugging-port=%d' % port]
   if device:
     _SetUpDevice(device, package_info)

commit cdd1f5dfca5944d58c533093cd236a2976c0a97f
Author: pkotwicz <pkotwicz@chromium.org>
Date:   Wed Feb 3 19:43:34 2016 -0800

    Add script which extracts defines and include dirs from clang
    
    This CL adds a script which generates an XML file which can be imported into an
    Eclipse CDT project. The combination of the XML file generated by
    generate_cdt_clang_settings.py and the XML file generated by
    "gn gen out/Release --ide=eclipse" is equivalent to the XML file generated by
    gyp/generator/eclipse.py
    
    generate_cdt_clang_settings.py writes an XML file with the include directories
    and the defines that all projects which use the clang compiler inherit.
    
    Using generate_cdt_clang_settings.py makes it no longer necessary to use GYP to
    generate Eclipse CDT settings.
    
    BUG=530676
    
    Review URL: https://codereview.chromium.org/1645383002
    
    Cr-Original-Commit-Position: refs/heads/master@{#373448}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cfa2836a7fb20d603c33b6c25a6183264e43394c

diff --git a/eclipse/generate_cdt_clang_settings.py b/eclipse/generate_cdt_clang_settings.py
new file mode 100755
index 0000000..903d0f1
--- /dev/null
+++ b/eclipse/generate_cdt_clang_settings.py
@@ -0,0 +1,151 @@
+#!/usr/bin/env python
+#
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Generates XML file which can be imported into an Eclipse CDT project.
+
+The XML file contains the include directories and defines that all applications
+which use the clang compiler inherit. Should be used in conjunction with the
+XML file generated by "gn gen out/Release --ide=eclipse"
+"""
+
+
+from xml.sax.saxutils import escape
+import os
+import subprocess
+import sys
+
+def GetClangIncludeDirectories(compiler_path):
+  """Gets the system include directories as determined by the clang compiler.
+
+  Returns:
+    The list of include directories.
+  """
+
+  includes_set = set()
+
+  command = [compiler_path, '-E', '-xc++', '-v', '-']
+  proc = subprocess.Popen(args=command, stdin=subprocess.PIPE,
+                          stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+  output = proc.communicate()[1]
+  # Extract the list of include dirs from the output, which has this format:
+  #   ...
+  #   #include "..." search starts here:
+  #   #include <...> search starts here:
+  #    /usr/include/c++/4.6
+  #    /usr/local/include
+  #   End of search list.
+  #   ...
+  in_includes_list = False
+  for line in output.splitlines():
+    if line.startswith('#include'):
+      in_includes_list = True
+      continue
+    if line.startswith('End of search list.'):
+      break
+    if in_includes_list:
+      includes_set.add(line.strip())
+
+  return sorted(includes_set)
+
+
+def GetClangDefines(compiler_path):
+  """Gets the system defines as determined by the clang compiler.
+
+  Returns:
+    The dict of defines.
+  """
+
+  all_defines = {}
+  command = [compiler_path, '-E', '-dM', '-']
+  proc = subprocess.Popen(args=command, stdin=subprocess.PIPE,
+                          stdout=subprocess.PIPE)
+
+  # Extract the list of defines from the output, which has this format:
+  # #define __SIZEOF_INT__ 4
+  # ...
+  # #define unix 1
+  output = proc.communicate()[0]
+  for line in output.splitlines():
+    if not line.strip():
+      continue
+    line_parts = line.split(' ', 2)
+    key = line_parts[1]
+    if len(line_parts) >= 3:
+      val = line_parts[2]
+    else:
+      val = '1'
+    all_defines[key] = val
+
+  return all_defines
+
+
+def WriteIncludePaths(out, eclipse_langs, include_dirs):
+  """Write the includes section of a CDT settings export file."""
+
+  out.write('  <section name="org.eclipse.cdt.internal.ui.wizards.' \
+            'settingswizards.IncludePaths">\n')
+  out.write('    <language name="holder for library settings"></language>\n')
+  for lang in eclipse_langs:
+    out.write('    <language name="%s">\n' % lang)
+    for include_dir in include_dirs:
+      out.write('      <includepath workspace_path="false">%s</includepath>\n' %
+                include_dir)
+    out.write('    </language>\n')
+  out.write('  </section>\n')
+
+
+def WriteMacros(out, eclipse_langs, defines):
+  """Write the macros section of a CDT settings export file."""
+
+  out.write('  <section name="org.eclipse.cdt.internal.ui.wizards.' \
+            'settingswizards.Macros">\n')
+  out.write('    <language name="holder for library settings"></language>\n')
+  for lang in eclipse_langs:
+    out.write('    <language name="%s">\n' % lang)
+    for key in sorted(defines.iterkeys()):
+      out.write('      <macro><name>%s</name><value>%s</value></macro>\n' %
+                (escape(key), escape(defines[key])))
+    out.write('    </language>\n')
+  out.write('  </section>\n')
+
+
+def main(argv):
+  if len(argv) != 2:
+    print("Usage: generate_cdt_clang_settings.py destination_file")
+    return
+
+  compiler_path = os.path.abspath(
+      'third_party/llvm-build/Release+Asserts/bin/clang')
+  if not os.path.exists(compiler_path):
+    print('Please run this script from the Chromium src/ directory.')
+    return
+
+  include_dirs = GetClangIncludeDirectories(compiler_path)
+  if not include_dirs:
+    print('ERROR: Could not extract include dirs from %s.' % compiler_path)
+    return
+
+  defines = GetClangDefines(compiler_path)
+  if not defines:
+    print('ERROR: Could not extract defines from %s.' % compiler_path)
+
+  destination_file = os.path.abspath(argv[1])
+  destination_dir = os.path.dirname(destination_file)
+  if not os.path.exists(destination_dir):
+    os.makedirs(destination_dir)
+
+  with open(destination_file, 'w') as out:
+    eclipse_langs = ['C++ Source File', 'C Source File', 'Assembly Source File',
+                     'GNU C++', 'GNU C', 'Assembly']
+
+    out.write('<?xml version="1.0" encoding="UTF-8"?>\n')
+    out.write('<cdtprojectproperties>\n')
+    WriteIncludePaths(out, eclipse_langs, include_dirs)
+    WriteMacros(out, eclipse_langs, defines)
+    out.write('</cdtprojectproperties>\n')
+
+if __name__ == '__main__':
+  sys.exit(main(sys.argv))

commit 8d3613339a3e6bc0f76871ab5ca3da61a507381d
Author: tedchoc <tedchoc@chromium.org>
Date:   Wed Feb 3 12:39:33 2016 -0800

    Share suggestion highlighting behavior in Android omnibox.
    
    TBR=sky@chromium.org
    BUG=568220
    
    Review URL: https://codereview.chromium.org/1630833003
    
    Cr-Original-Commit-Position: refs/heads/master@{#373326}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2d34190c3f29355e6bd2ffea788a0d82fe8d1e7c

diff --git a/eclipse/.classpath b/eclipse/.classpath
index eef7d41..3f65bf4 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -97,6 +97,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/accessibility_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/activity_type_ids_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/android_resource_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_java/"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_type_java/"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/ax_enumerations_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_application_state"/>

commit fb5e61faa19b62b335dc4ca35f6065deb07c1754
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 2 09:29:20 2016 -0800

    tools/android/loading: Properly handle 'third-party' AdBlock rules.
    
    Review URL: https://codereview.chromium.org/1658633003
    
    Cr-Original-Commit-Position: refs/heads/master@{#372972}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c8aa169f3214fc377dd3b1342b8167eeb0e77376

diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
index b12b9af..be05fcb 100644
--- a/loading/content_classification_lens.py
+++ b/loading/content_classification_lens.py
@@ -7,6 +7,7 @@
 import collections
 import logging
 import os
+import urlparse
 
 import loading_trace
 import request_track
@@ -31,6 +32,7 @@ class ContentClassificationLens(object):
     self._tracking_requests = set()
     self._ad_matcher = _RulesMatcher(ad_rules, True)
     self._tracking_matcher = _RulesMatcher(tracking_rules, True)
+    self._document_url = self._GetDocumentUrl()
     self._GroupRequestsByFrameId()
     self._LabelRequests()
 
@@ -73,11 +75,21 @@ class ContentClassificationLens(object):
   def _LabelRequests(self):
     for request in self._requests:
       request_id = request.request_id
-      if self._ad_matcher.Matches(request):
+      if self._ad_matcher.Matches(request, self._document_url):
         self._ad_requests.add(request_id)
-      if self._tracking_matcher.Matches(request):
+      if self._tracking_matcher.Matches(request, self._document_url):
         self._tracking_requests.add(request_id)
 
+  def _GetDocumentUrl(self):
+    main_frame_id = self._trace.page_track.GetMainFrameId()
+    # Take the last one as JS redirects can change the document URL.
+    document_url = None
+    for r in self._requests:
+      # 304: not modified.
+      if r.frame_id == main_frame_id and r.status in (200, 304):
+        document_url = r.document_url
+    return document_url
+
 
 class _RulesMatcher(object):
   """Matches requests with rules in Adblock+ format."""
@@ -106,20 +118,23 @@ class _RulesMatcher(object):
     else:
       self._matcher = None
 
-  def Matches(self, request):
+  def Matches(self, request, document_url):
     """Returns whether a request matches one of the rules."""
     if self._matcher is None:
       return False
     url = request.url
-    return self._matcher.should_block(url, self._GetOptions(request))
+    return self._matcher.should_block(
+        url, self._GetOptions(request, document_url))
 
   @classmethod
-  def _GetOptions(cls, request):
+  def _GetOptions(cls, request, document_url):
     options = {}
     resource_type = request.resource_type
     option = cls._RESOURCE_TYPE_TO_OPTIONS_KEY.get(resource_type)
     if option:
       options[option] = True
+    if cls._IsThirdParty(request.url, document_url):
+      options['third-party'] = True
     return options
 
   @classmethod
@@ -129,3 +144,30 @@ class _RulesMatcher(object):
     else:
       return [rule for rule in rules
               if not rule.startswith(cls._WHITELIST_PREFIX)]
+
+  @classmethod
+  def _IsThirdParty(cls, url, document_url):
+    # Common definition of "third-party" is "not from the same TLD+1".
+    # Unfortunately, knowing what is a TLD is not trivial. To do it without a
+    # database, we use the following simple (and incorrect) rules:
+    # - co.{in,uk,jp,hk} is a TLD
+    # - com.{au,hk} is a TLD
+    # Otherwise, this is the part after the last dot.
+    return cls._GetTldPlusOne(url) != cls._GetTldPlusOne(document_url)
+
+  @classmethod
+  def _GetTldPlusOne(cls, url):
+    hostname = urlparse.urlparse(url).hostname
+    if not hostname:
+      return hostname
+    parts = hostname.split('.')
+    if len(parts) <= 2:
+      return hostname
+    tld_parts_count = 1
+    may_be_tld = parts[-2:]
+    if may_be_tld[0] == 'co' and may_be_tld[1] in ('in', 'uk', 'jp'):
+      tld_parts_count = 2
+    elif may_be_tld[0] == 'com' and may_be_tld[1] in ('au', 'hk'):
+      tld_parts_count = 2
+    tld_plus_one = '.'.join(parts[-(tld_parts_count + 1):])
+    return tld_plus_one
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index b4f5ad1..6a64c86 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -12,19 +12,47 @@ import test_utils
 
 
 class ContentClassificationLensTestCase(unittest.TestCase):
+  _DOCUMENT_URL = 'http://bla.com'
+  _MAIN_FRAME_ID = '123.1'
   _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'document_url': _DOCUMENT_URL,
                                    'request_id': '1234.1',
-                                   'frame_id': '123.1',
+                                   'frame_id': _MAIN_FRAME_ID,
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
+                                   'status': 200,
                                    'timing': TimingFromDict({})})
-  _MAIN_FRAME_ID = '123.1'
   _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
                    'frame_id': _MAIN_FRAME_ID},
                   {'method': 'Page.frameAttached',
                    'frame_id': '123.13', 'parent_frame_id': _MAIN_FRAME_ID}]
   _RULES = ['bla.com']
 
+  def testGetDocumentUrl(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertEquals(self._DOCUMENT_URL, lens._GetDocumentUrl())
+    # Don't be fooled by redirects.
+    request = copy.deepcopy(self._REQUEST)
+    request.status = 302
+    request.document_url = 'http://www.bla.com'
+    trace = test_utils.LoadingTraceFromEvents(
+        [request, self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertEquals(self._DOCUMENT_URL, lens._GetDocumentUrl())
+
+  def testGetDocumentUrlSeveralChanges(self):
+    request = copy.deepcopy(self._REQUEST)
+    request.status = 200
+    request.document_url = 'http://www.blabla.com'
+    request2 = copy.deepcopy(request)
+    request2.document_url = 'http://www.blablabla.com'
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST, request, request2], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertEquals(request2.document_url, lens._GetDocumentUrl())
+
   def testNoRules(self):
     trace = test_utils.LoadingTraceFromEvents(
         [self._REQUEST], self._PAGE_EVENTS)
@@ -53,18 +81,18 @@ class ContentClassificationLensTestCase(unittest.TestCase):
     self.assertFalse(lens.IsAdFrame(self._MAIN_FRAME_ID, .5))
 
   def testAdFrame(self):
-    request = self._REQUEST
+    request = copy.deepcopy(self._REQUEST)
     request.frame_id = '123.123'
     trace = test_utils.LoadingTraceFromEvents(
         [request] * 10 + [self._REQUEST] * 5, self._PAGE_EVENTS)
     lens = ContentClassificationLens(trace, self._RULES, [])
     self.assertTrue(lens.IsAdFrame(request.frame_id, .5))
 
-
 class _MatcherTestCase(unittest.TestCase):
   _RULES_WITH_WHITELIST = ['/thisisanad.', '@@myadvertisingdomain.com/*',
                            '@@||www.mydomain.com/ads/$elemhide']
   _SCRIPT_RULE = 'domainwithscripts.com/*$script'
+  _THIRD_PARTY_RULE = 'domainwithscripts.com/*$third-party'
   _SCRIPT_REQUEST = Request.FromJsonDict(
       {'url': 'http://domainwithscripts.com/bla.js',
        'resource_type': 'Script',
@@ -84,8 +112,29 @@ class _MatcherTestCase(unittest.TestCase):
     matcher = _RulesMatcher([self._SCRIPT_RULE], False)
     request = copy.deepcopy(self._SCRIPT_REQUEST)
     request.resource_type = 'Stylesheet'
-    self.assertFalse(matcher.Matches(request))
-    self.assertTrue(matcher.Matches(self._SCRIPT_REQUEST))
+    self.assertFalse(matcher.Matches(
+        request, ContentClassificationLensTestCase._DOCUMENT_URL))
+    self.assertTrue(matcher.Matches(
+        self._SCRIPT_REQUEST, ContentClassificationLensTestCase._DOCUMENT_URL))
+
+  def testGetTldPlusOne(self):
+    self.assertEquals(
+        'easy.com',
+        _RulesMatcher._GetTldPlusOne('http://www.easy.com/hello/you'))
+    self.assertEquals(
+        'not-so-easy.co.uk',
+        _RulesMatcher._GetTldPlusOne('http://www.not-so-easy.co.uk/hello/you'))
+    self.assertEquals(
+        'hard.co.uk',
+        _RulesMatcher._GetTldPlusOne('http://hard.co.uk/'))
+
+  def testThirdPartyRule(self):
+    matcher = _RulesMatcher([self._THIRD_PARTY_RULE], False)
+    request = copy.deepcopy(self._SCRIPT_REQUEST)
+    document_url = 'http://www.domainwithscripts.com/good-morning'
+    self.assertFalse(matcher.Matches(request, document_url))
+    document_url = 'http://anotherdomain.com/good-morning'
+    self.assertTrue(matcher.Matches(request, document_url))
 
 
 if __name__ == '__main__':
diff --git a/loading/page_track.py b/loading/page_track.py
index bb289db..bc904ba 100644
--- a/loading/page_track.py
+++ b/loading/page_track.py
@@ -9,6 +9,7 @@ class PageTrack(devtools_monitor.Track):
   """Records the events from the page track."""
   _METHODS = ('Page.frameStartedLoading', 'Page.frameStoppedLoading',
               'Page.frameAttached')
+  FRAME_STARTED_LOADING = 'Page.frameStartedLoading'
   def __init__(self, connection):
     super(PageTrack, self).__init__(connection)
     self._connection = connection
@@ -26,7 +27,7 @@ class PageTrack(devtools_monitor.Track):
     frame_id = params['frameId']
     should_stop = False
     event = {'method': method, 'frame_id': frame_id}
-    if method == 'Page.frameStartedLoading':
+    if method == self.FRAME_STARTED_LOADING:
       if self._main_frame_id is None:
         self._main_frame_id = params['frameId']
       self._pending_frames.add(frame_id)
@@ -53,6 +54,14 @@ class PageTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [event for event in self._events]}
 
+  def GetMainFrameId(self):
+    """Returns the Id (str) of the main frame, or raises a ValueError."""
+    for event in self._events:
+      if event['method'] == self.FRAME_STARTED_LOADING:
+        return event['frame_id']
+    else:
+      raise ValueError('No frame loads in the track.')
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     assert 'events' in json_dict
diff --git a/loading/page_track_unittest.py b/loading/page_track_unittest.py
index 6757a01..3056d99 100644
--- a/loading/page_track_unittest.py
+++ b/loading/page_track_unittest.py
@@ -53,6 +53,13 @@ class PageTrackTest(unittest.TestCase):
     with self.assertRaises(AssertionError):
       page_track.Handle(msg['method'], msg)
 
+  def testGetMainFrameId(self):
+    devtools_connection = MockDevToolsConnection()
+    page_track = PageTrack(devtools_connection)
+    for msg in PageTrackTest._EVENTS:
+      page_track.Handle(msg['method'], msg)
+    self.assertEquals('1234.1', page_track.GetMainFrameId())
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 9051062..cc24f36 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -6,15 +6,7 @@
 
 import devtools_monitor
 import loading_trace
-
-
-class FakeTrack(devtools_monitor.Track):
-  def __init__(self, events):
-    super(FakeTrack, self).__init__(None)
-    self._events = events
-
-  def GetEvents(self):
-    return self._events
+import page_track
 
 
 class FakeRequestTrack(devtools_monitor.Track):
@@ -32,9 +24,23 @@ class FakeRequestTrack(devtools_monitor.Track):
     return event
 
 
+class FakePageTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakePageTrack, self).__init__(None)
+    self._events = events
+
+  def GetEvents(self):
+    return self._events
+
+  def GetMainFrameId(self):
+    event = self._events[0]
+    # Make sure our laziness is not an issue here.
+    assert event['method'] == page_track.PageTrack.FRAME_STARTED_LOADING
+    return event['frame_id']
+
+
 def LoadingTraceFromEvents(requests, page_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
   request_track = FakeRequestTrack(requests)
-  page_track = FakeTrack(page_events if page_events else [])
-  return loading_trace.LoadingTrace(
-      None, None, page_track, request_track, None)
+  page = FakePageTrack(page_events if page_events else [])
+  return loading_trace.LoadingTrace(None, None, page, request_track, None)

commit 4155f7eec5d0f18aa876409606c8f618c0a17e80
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 2 06:38:38 2016 -0800

    Loading dependency analysis
    
    This defines a Lens that ads synthetic load events from the trace, and
    correlates them with request events via timestamp. This hasn't been
    sucessful in identifying the cause of long edges.
    
    This CL also does some refactoring to the loading model which I think is
    ultimately useful and should be retained. The DOT drawing has been
    pulled out into its own class and unittested, for a certain very limited
    concept of unittesting.
    
    Review URL: https://codereview.chromium.org/1641203002
    
    Cr-Original-Commit-Position: refs/heads/master@{#372939}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 40a628d4b55ace395c35d733c629ca543651b342

diff --git a/loading/analyze.py b/loading/analyze.py
index 5f15fe2..1b1a2a3 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -26,8 +26,10 @@ from pylib import constants
 
 import content_classification_lens
 import device_setup
+import frame_load_lens
 import loading_model
 import loading_trace
+import model_graph
 import trace_recorder
 
 
@@ -155,7 +157,8 @@ def _ProcessRequests(filename, ad_rules_filename='',
     content_lens = (
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
             trace, ad_rules_filename, tracking_rules_filename))
-    return loading_model.ResourceGraph(trace, content_lens)
+    frame_lens = frame_load_lens.FrameLoadLens(trace)
+    return loading_model.ResourceGraph(trace, content_lens, frame_lens)
 
 
 def InvalidCommand(cmd):
@@ -188,7 +191,6 @@ def DoPng(arg_str):
   parser.add_argument('request_json')
   parser.add_argument('png_output', nargs='?')
   parser.add_argument('--eog', action='store_true')
-  parser.add_argument('--highlight')
   parser.add_argument('--noads', action='store_true')
   parser.add_argument('--ad_rules', default='')
   parser.add_argument('--tracking_rules', default='')
@@ -197,10 +199,9 @@ def DoPng(arg_str):
       args.request_json, args.ad_rules, args.tracking_rules)
   if args.noads:
     graph.Set(node_filter=graph.FilterAds)
+  visualization = model_graph.GraphVisualization(graph)
   tmp = tempfile.NamedTemporaryFile()
-  graph.MakeGraphviz(
-      tmp,
-      highlight=args.highlight.split(',') if args.highlight else None)
+  visualization.OutputDot(tmp)
   tmp.flush()
   png_output = args.png_output
   if not png_output:
diff --git a/loading/frame_load_lens.py b/loading/frame_load_lens.py
new file mode 100644
index 0000000..e0479ab
--- /dev/null
+++ b/loading/frame_load_lens.py
@@ -0,0 +1,109 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gathers and infers dependencies between requests.
+
+When executed as a script, loads a trace and outputs synthetic frame load nodes
+and the new introduced dependencies.
+"""
+
+import bisect
+import collections
+import logging
+import operator
+
+import loading_trace
+
+
+class FrameLoadLens(object):
+  """Analyses and creates request dependencies for inferred frame events."""
+  _FRAME_EVENT = 'RenderFrameImpl::didFinishLoad'
+  _REQUEST_TO_LOAD_GAP_MSEC = 100
+  _LOAD_TO_REQUEST_GAP_MSEC = 100
+  def __init__(self, trace):
+    """Instance initialization.
+
+    Args:
+      trace: (LoadingTrace) Loading trace.
+    """
+    self._frame_load_events = self._GetFrameLoadEvents(trace.tracing_track)
+    self._request_track = trace.request_track
+    self._tracing_track = trace.tracing_track
+    self._load_dependencies = []
+    self._request_dependencies = []
+    for i, load in enumerate(self._frame_load_events):
+      self._load_dependencies.extend(
+          [(i, r) for r in self._GetLoadDependencies(load)])
+      self._request_dependencies.extend(
+          [(r, i) for r in self._GetRequestDependencies(load)])
+
+  def GetFrameLoadInfo(self):
+    """Returns [(index, msec)]."""
+    return [collections.namedtuple('LoadInfo', ['index', 'msec'])._make(
+        (i, self._frame_load_events[i].start_msec))
+            for i in xrange(len(self._frame_load_events))]
+
+  def GetFrameResourceComplete(self, request_track):
+    """Returns [(frame id, msec)]."""
+    frame_to_end_msec = collections.defaultdict(int)
+    for r in request_track.GetEvents():
+      if r.end_msec > frame_to_end_msec[r.frame_id]:
+        frame_to_end_msec[r.frame_id] = r.end_msec
+    loads = []
+    for f in sorted(frame_to_end_msec.keys()):
+      loads.append((f, frame_to_end_msec[f]))
+    return loads
+
+  def GetFrameLoadDependencies(self):
+    """Returns a list of frame load dependencies.
+
+    Returns:
+      ([(frame load index, request), ...],
+       [(request, frame load index), ...]), where request are instances of
+      request_trace.Request, and frame load index is an integer. The first list
+      in the tuple gives the requests that are dependent on the given frame
+      load, and the second list gives the frame loads that are dependent on the
+      given request.
+    """
+    return (self._load_dependencies, self._request_dependencies)
+
+  def _GetFrameLoadEvents(self, tracing_track):
+    events = []
+    for e in tracing_track.GetEvents():
+      if e.tracing_event['name'] == self._FRAME_EVENT:
+        events.append(e)
+    return events
+
+  def _GetLoadDependencies(self, load):
+    for r in self._request_track.GetEventsStartingBetween(
+        load.start_msec, load.start_msec + self._LOAD_TO_REQUEST_GAP_MSEC):
+      yield r
+
+  def _GetRequestDependencies(self, load):
+    for r in self._request_track.GetEventsEndingBetween(
+        load.start_msec - self._REQUEST_TO_LOAD_GAP_MSEC, load.start_msec):
+      yield r
+
+
+if __name__ == '__main__':
+  import loading_trace
+  import json
+  import sys
+  lens = FrameLoadLens(loading_trace.LoadingTrace.FromJsonDict(
+      json.load(open(sys.argv[1]))))
+  load_times = lens.GetFrameLoadInfo()
+  for t in load_times:
+    print t
+  print (lens._request_track.GetFirstRequestMillis(),
+         lens._request_track.GetLastRequestMillis())
+  load_dep, request_dep = lens.GetFrameLoadDependencies()
+  rq_str = lambda r: '%s (%d-%d)' % (
+      r.request_id,
+      r.start_msec - lens._request_track.GetFirstRequestMillis(),
+      r.end_msec - lens._request_track.GetFirstRequestMillis())
+  load_str = lambda i: '%s (%d)' % (i, load_times[i][1])
+  for load_idx, request in load_dep:
+    print '%s -> %s' % (load_str(load_idx), rq_str(request))
+  for request, load_idx in request_dep:
+    print '%s -> %s' % (rq_str(request), load_str(load_idx))
diff --git a/loading/loading_model.py b/loading/loading_model.py
index e46b705..b20d3eb 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -30,19 +30,21 @@ class ResourceGraph(object):
   Set parameters:
     cache_all: if true, assume zero loading time for all resources.
   """
-  def __init__(self, trace, content_lens=None):
+  def __init__(self, trace, content_lens=None, frame_lens=None):
     """Create from a LoadingTrace (or json of a trace).
 
     Args:
       trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
       content_lens: (ContentClassificationLens) Lens used to annotate the
                     nodes, or None.
+      frame_lens: (FrameLoadLens) Lens used to augment graph with load nodes.
     """
     if type(trace) == dict:
       trace = loading_trace.LoadingTrace.FromJsonDict(trace)
+    self._trace = trace
     self._content_lens = content_lens
+    self._frame_lens = frame_lens
     self._BuildDag(trace)
-    self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
     # reparented child is actually a dependency for a child of its new parent.
     try:
@@ -112,15 +114,22 @@ class ResourceGraph(object):
     if node_filter is not None:
       self._node_filter = node_filter
 
-  def Nodes(self):
-    """Return iterable of all nodes via their _NodeInfos.
+  def Nodes(self, sort=False):
+    """Return iterable of all nodes via their NodeInfos.
+
+    Args:
+      sort: if true, return nodes in sorted order. This may prune additional
+        nodes from the unsorted list (eg, non-root, non-ad nodes reachable only
+        by ad nodes)
 
     Returns:
-      Iterable of node infos in arbitrary order.
+      Iterable of node infos.
+
     """
-    for n in self._node_info:
-      if self._node_filter(n.Node()):
-        yield n
+    if sort:
+      return (self._node_info[n.Index()]
+              for n in dag.TopologicalSort(self._nodes, self._node_filter))
+    return (n for n in self._node_info if self._node_filter(n.Node()))
 
   def EdgeCosts(self, node_filter=None):
     """Edge costs.
@@ -140,7 +149,7 @@ class ResourceGraph(object):
         continue
       for s in n.Node().Successors():
         if node_filter(s):
-          total += self._EdgeCost(n.Node(), s)
+          total += self.EdgeCost(n.Node(), s)
     return total
 
   def Intersect(self, other_nodes):
@@ -173,10 +182,10 @@ class ResourceGraph(object):
     for n in dag.TopologicalSort(self._nodes, self._node_filter):
       cost = 0
       if n.Predecessors():
-        cost = max([costs[p.Index()] + self._EdgeCost(p, n)
+        cost = max([costs[p.Index()] + self.EdgeCost(p, n)
                     for p in n.Predecessors()])
       if not self._cache_all:
-        cost += self._NodeCost(n)
+        cost += self.NodeCost(n)
       costs[n.Index()] = cost
     max_cost = max(costs)
     assert max_cost > 0  # Otherwise probably the filter went awry.
@@ -205,68 +214,11 @@ class ResourceGraph(object):
     node_info = self._node_info[node.Index()]
     return not (node_info.IsAd() or node_info.IsTracking())
 
-  def MakeGraphviz(self, output, highlight=None):
-    """Output a graphviz representation of our DAG.
-
-    Args:
-      output: a file-like output stream which recieves a graphviz dot.
-      highlight: a list of node items to emphasize. Any resource url which
-        contains any highlight text will be distinguished in the output.
-    """
-    output.write("""digraph dependencies {
-    rankdir = LR;
-    """)
-    orphans = set()
-    try:
-      sorted_nodes = dag.TopologicalSort(self._nodes,
-                                         node_filter=self._node_filter)
-    except AssertionError as exc:
-      sys.stderr.write('Bad topological sort: %s\n'
-                       'Writing children in order\n' % str(exc))
-      sorted_nodes = self._nodes
-    for n in sorted_nodes:
-      if not n.Successors() and not n.Predecessors():
-        orphans.add(n)
-    if orphans:
-      output.write("""subgraph cluster_orphans {
-  color=black;
-  label="Orphans";
-""")
-      for n in orphans:
-        output.write(self._GraphvizNode(n.Index(), highlight))
-      output.write('}\n')
-
-    output.write("""subgraph cluster_nodes {
-  color=invis;
-""")
-    for n in sorted_nodes:
-      if not n.Successors() and not n.Predecessors():
-        continue
-      output.write(self._GraphvizNode(n.Index(), highlight))
-
-    for n in sorted_nodes:
-      for s in n.Successors():
-        style = 'color = orange'
-        annotations = self._EdgeAnnotation(n, s)
-        if 'redirect' in annotations:
-          style = 'color = black'
-        elif 'parser' in annotations:
-          style = 'color = red'
-        elif 'stack' in annotations:
-          style = 'color = blue'
-        elif 'script_inferred' in annotations:
-          style = 'color = purple'
-        if 'timing' in annotations:
-          style += '; style=dashed'
-        arrow = '[%s; label="%s"]' % (style, self._EdgeCost(n, s))
-        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
-    output.write('}\n}\n')
-
   def ResourceInfo(self):
     """Get resource info.
 
     Returns:
-      A list of _NodeInfo objects that describe the resources fetched.
+      A list of NodeInfo objects that describe the resources fetched.
     """
     return self._node_info
 
@@ -295,32 +247,46 @@ class ResourceGraph(object):
     assert len(visited) == len(self._nodes)
     return '\n'.join(output)
 
+  def NodeInfo(self, node):
+    """Return the node info for a graph node.
+
+    Args:
+      node: (int, dag.Node or NodeInfo) a node representation. An int is taken
+      to be the node's index.
+
+    Returns:
+      The NodeInfo instance corresponding to the node.
+    """
+    if type(node) is self._NodeInfo:
+      return node
+    elif type(node) is int:
+      return self._node_info[node]
+    return self._node_info[node.Index()]
+
+  def ShortName(self, node):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(node).ShortName()
+
+  def Url(self, node):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(node).Url()
+
+  def NodeCost(self, node):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(node).NodeCost()
+
+  def EdgeCost(self, parent, child):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(parent).EdgeCost(self.NodeInfo(child))
+
+  def EdgeAnnotation(self, parent, child):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(parent).EdgeAnnotation(self.NodeInfo(child))
+
   ##
   ## Internal items
   ##
 
-  _CONTENT_KIND_TO_COLOR = {
-      'application':     'blue',      # Scripts.
-      'font':            'grey70',
-      'image':           'orange',    # This probably catches gifs?
-      'video':           'hotpink1',
-      }
-
-  _CONTENT_TYPE_TO_COLOR = {
-      'html':            'red',
-      'css':             'green',
-      'script':          'blue',
-      'javascript':      'blue',
-      'json':            'purple',
-      'gif':             'grey',
-      'image':           'orange',
-      'jpeg':            'orange',
-      'png':             'orange',
-      'plain':           'brown3',
-      'octet-stream':    'brown3',
-      'other':           'white',
-      }
-
   # This resource type may induce a timing dependency. See _SplitChildrenByTime
   # for details.
   # TODO(mattcary): are these right?
@@ -342,19 +308,28 @@ class ResourceGraph(object):
 
       Args:
         node: The node to augment.
-        request: The request associated with this node.
+        request: The request associated with this node, or an (index, msec)
+          tuple.
       """
-      self._request = request
+      self._node = node
       self._is_ad = False
       self._is_tracking = False
-      self._node = node
       self._edge_costs = {}
       self._edge_annotations = {}
-      # All fields in timing are millis relative to request_time, which is epoch
-      # seconds.
-      self._node_cost = max(
-          [0] + [t for f, t in request.timing._asdict().iteritems()
-                 if f != 'request_time'])
+
+      if type(request) == tuple:
+        self._request = None
+        self._node_cost = 0
+        self._shortname = 'LOAD %s' % request[0]
+        self._start_time = request[1]
+      else:
+        self._shortname = None
+        self._start_time = None
+        self._request = request
+        # All fields in timing are millis relative to request_time.
+        self._node_cost = max(
+            [0] + [t for f, t in request.timing._asdict().iteritems()
+                   if f != 'request_time'])
 
     def __str__(self):
       return self.ShortName()
@@ -387,25 +362,31 @@ class ResourceGraph(object):
       return self._node_cost
 
     def EdgeCost(self, s):
-      return self._edge_costs[s]
+      return self._edge_costs.get(s, 0)
 
     def StartTime(self):
+      if self._start_time:
+        return self._start_time
       return self._request.timing.request_time * 1000
 
     def EndTime(self):
-      return self._request.timing.request_time * 1000 + self._node_cost
+      return self.StartTime() + self._node_cost
 
     def EdgeAnnotation(self, s):
       assert s.Node() in self.Node().Successors()
       return self._edge_annotations.get(s, [])
 
     def ContentType(self):
+      if self._request is None:
+        return 'synthetic'
       return self._request.GetContentType()
 
     def ShortName(self):
       """Returns either the hostname of the resource, or the filename,
       or the end of the path. Tries to include the domain as much as possible.
       """
+      if self._shortname:
+        return self._shortname
       parsed = urlparse.urlparse(self._request.url)
       path = parsed.path
       hostname = parsed.hostname if parsed.hostname else '?.?.?'
@@ -441,9 +422,9 @@ class ResourceGraph(object):
       old_parent.RemoveSuccessor(), etc.
 
       Args:
-        old_parent: the _NodeInfo of a current parent of self. We assert this
+        old_parent: the NodeInfo of a current parent of self. We assert this
           is actually a parent.
-        new_parent: the _NodeInfo of the new parent. We assert it is not already
+        new_parent: the NodeInfo of the new parent. We assert it is not already
           a parent.
       """
       assert old_parent.Node() in self.Node().Predecessors()
@@ -457,37 +438,16 @@ class ResourceGraph(object):
         new_parent.AddEdgeAnnotation(self, a)
 
     def __eq__(self, o):
-      return self.Node().Index() == o.Node().Index()
+      """Note this works whether o is a Node or a NodeInfo."""
+      return self.Index() == o.Index()
 
     def __hash__(self):
       return hash(self.Node().Index())
 
-  def _ShortName(self, node):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[node.Index()].ShortName()
-
-  def _Url(self, node):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[node.Index()].Url()
-
-  def _NodeCost(self, node):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[node.Index()].NodeCost()
-
-  def _EdgeCost(self, parent, child):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[parent.Index()].EdgeCost(
-        self._node_info[child.Index()])
-
-  def _EdgeAnnotation(self, parent, child):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[parent.Index()].EdgeAnnotation(
-        self._node_info[child.Index()])
-
   def _BuildDag(self, trace):
     """Build DAG of resources.
 
-    Build a DAG from our requests and augment with _NodeInfo (see above) in a
+    Build a DAG from our requests and augment with NodeInfo (see above) in a
     parallel array indexed by Node.Index().
 
     Creates self._nodes and self._node_info.
@@ -528,6 +488,32 @@ class ResourceGraph(object):
       parent.SetEdgeCost(child, edge_cost)
       parent.AddEdgeAnnotation(child, reason)
 
+    self._AugmentFrameLoads(index_by_request)
+
+  def _AugmentFrameLoads(self, index_by_request):
+    if not self._frame_lens:
+      return
+    loads = self._frame_lens.GetFrameLoadInfo()
+    load_index_to_node = {}
+    for l in loads:
+      next_index = len(self._nodes)
+      node = dag.Node(next_index)
+      node_info = self._NodeInfo(node, (l.index, l.msec))
+      load_index_to_node[l.index] = next_index
+      self._nodes.append(node)
+      self._node_info.append(node_info)
+    frame_deps = self._frame_lens.GetFrameLoadDependencies()
+    for load_idx, rq in frame_deps[0]:
+      parent = self._node_info[load_index_to_node[load_idx]]
+      child = self._node_info[index_by_request[rq]]
+      parent.Node().AddSuccessor(child.Node())
+      parent.AddEdgeAnnotation(child, 'after-load')
+    for rq, load_idx in frame_deps[1]:
+      child = self._node_info[load_index_to_node[load_idx]]
+      parent = self._node_info[index_by_request[rq]]
+      parent.Node().AddSuccessor(child.Node())
+      parent.AddEdgeAnnotation(child, 'before-load')
+
   def _SplitChildrenByTime(self, parent):
     """Split children of a node by request times.
 
@@ -599,50 +585,6 @@ class ResourceGraph(object):
         current.ReparentTo(parent, children_by_end_time[end_mark])
         children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
 
-  def _ContentTypeToColor(self, content_type):
-    if not content_type:
-      type_str = 'other'
-    elif '/' in content_type:
-      kind, type_str = content_type.split('/', 1)
-      if kind in self._CONTENT_KIND_TO_COLOR:
-        return self._CONTENT_KIND_TO_COLOR[kind]
-    else:
-      type_str = content_type
-    return self._CONTENT_TYPE_TO_COLOR[type_str]
-
-  def _GraphvizNode(self, index, highlight):
-    """Returns a graphviz node description for a given node.
-
-    Args:
-      index: index of the node.
-      highlight: a list of node items to emphasize. Any resource url which
-        contains any highlight text will be distinguished in the output.
-
-    Returns:
-      A string describing the resource in graphviz format.
-      The resource is color-coded according to its content type, and its shape
-      is oval if its max-age is less than 300s (or if it's not cacheable).
-    """
-    node_info = self._node_info[index]
-    color = self._ContentTypeToColor(node_info.ContentType())
-    max_age = node_info.Request().MaxAge()
-    shape = 'polygon' if max_age > 300 else 'oval'
-    styles = ['filled']
-    if highlight:
-      for fragment in highlight:
-        if fragment in node_info.Url():
-          styles.append('dotted')
-          break
-    if node_info.IsAd() or node_info.IsTracking():
-      styles += ['bold', 'diagonals']
-    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
-            'fillcolor = %s; shape = %s];\n'
-            % (index, node_info.ShortName(),
-               node_info.StartTime() - self._global_start,
-               node_info.EndTime() - self._global_start,
-               node_info.EndTime() - node_info.StartTime(),
-               ','.join(styles), color, shape))
-
   def _ExtractImages(self):
     """Return interesting image resources.
 
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 88a62d9..7afe8ef 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -32,9 +32,13 @@ class SimpleLens(object):
 class LoadingModelTestCase(unittest.TestCase):
 
   def setUp(self):
+    self.old_lens = request_dependencies_lens.RequestDependencyLens
     request_dependencies_lens.RequestDependencyLens = SimpleLens
     self._next_request_id = 0
 
+  def tearDown(self):
+    request_dependencies_lens.RequestDependencyLens = self.old_lens
+
   def MakeParserRequest(self, url, source_url, start_time, end_time,
                         magic_content_type=False):
     timing = request_track.TimingAsList(request_track.TimingFromDict({
diff --git a/loading/model_graph.py b/loading/model_graph.py
new file mode 100644
index 0000000..5c0bf09
--- /dev/null
+++ b/loading/model_graph.py
@@ -0,0 +1,161 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Visualize a loading_model.ResourceGraph."""
+
+import dag
+import itertools
+
+
+class GraphVisualization(object):
+  """Manipulate visual representations of a resource graph.
+
+  The output will change as the ResourceGraph is changed, for example by setting
+  filters.
+
+  Currently only DOT output is supported.
+  """
+  _LONG_EDGE_THRESHOLD_MS = 2000  # Time in milliseconds.
+
+  _CONTENT_KIND_TO_COLOR = {
+      'application':     'blue',      # Scripts.
+      'font':            'grey70',
+      'image':           'orange',    # This probably catches gifs?
+      'video':           'hotpink1',
+      }
+
+  _CONTENT_TYPE_TO_COLOR = {
+      'html':            'red',
+      'css':             'green',
+      'script':          'blue',
+      'javascript':      'blue',
+      'json':            'purple',
+      'gif':             'grey',
+      'image':           'orange',
+      'jpeg':            'orange',
+      'png':             'orange',
+      'plain':           'brown3',
+      'octet-stream':    'brown3',
+      'other':           'white',
+      'synthetic':       'yellow',
+      }
+
+  def __init__(self, graph):
+    """Initialize.
+
+    Args:
+      graph: (loading_model.ResourceGraph) the graph to visualize.
+    """
+    self._graph = graph
+    self._global_start = None
+
+  def OutputDot(self, output):
+    """Output DOT (graphviz) representation.
+
+    Args:
+      output: a file-like output stream to receive the dot file.
+    """
+    sorted_nodes = [n for n in self._graph.Nodes(sort=True)]
+    self._global_start = min([n.StartTime() for n in sorted_nodes])
+    visited_nodes = set([n for n in sorted_nodes])
+
+    output.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+
+    orphans = set()
+    for n in sorted_nodes:
+      for s in itertools.chain(n.Node().Successors(),
+                               n.Node().Predecessors()):
+        if s in visited_nodes:
+          break
+      else:
+        orphans.add(n)
+    if orphans:
+      output.write("""subgraph cluster_orphans {
+                        color=black;
+                        label="Orphans";
+                   """)
+      for n in orphans:
+        # Ignore synthetic nodes for orphan display.
+        if not self._graph.NodeInfo(n).Request():
+          continue
+        output.write(self.DotNode(n))
+      output.write('}\n')
+
+    output.write("""subgraph cluster_nodes {
+                      color=invis;
+                 """)
+
+    for n in sorted_nodes:
+      if n in orphans:
+        continue
+      output.write(self.DotNode(n))
+
+    for n in visited_nodes:
+      for s in n.Node().Successors():
+        if s not in visited_nodes:
+          continue
+        style = 'color = orange'
+        annotations = self._graph.EdgeAnnotation(n, s)
+        if 'redirect' in annotations:
+          style = 'color = black'
+        elif 'parser' in annotations:
+          style = 'color = red'
+        elif 'stack' in annotations:
+          style = 'color = blue'
+        elif 'script_inferred' in annotations:
+          style = 'color = purple'
+        if 'after-load' in annotations or 'before-load' in annotations:
+          style = 'color = forestgreen'
+        if 'timing' in annotations:
+          style += '; style=dashed'
+        if self._graph.EdgeCost(n, s) > self._LONG_EDGE_THRESHOLD_MS:
+          style += '; penwidth=5; weight=2'
+        arrow = '[%s; label="%s"]' % (style, self._graph.EdgeCost(n, s))
+        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
+    output.write('}\n')
+
+    output.write('}\n')
+
+  def _ContentTypeToColor(self, content_type):
+    if not content_type:
+      type_str = 'other'
+    elif '/' in content_type:
+      kind, type_str = content_type.split('/', 1)
+      if kind in self._CONTENT_KIND_TO_COLOR:
+        return self._CONTENT_KIND_TO_COLOR[kind]
+    else:
+      type_str = content_type
+    return self._CONTENT_TYPE_TO_COLOR[type_str]
+
+  def DotNode(self, node):
+    """Returns a graphviz node description for a given node.
+
+    Args:
+      node: a dag.Node or ResourceGraph node info.
+
+    Returns:
+      A string describing the resource in graphviz format.
+      The resource is color-coded according to its content type, and its shape
+      is oval if its max-age is less than 300s (or if it's not cacheable).
+    """
+    if type(node) is dag.Node:
+      node = self._graph.NodeInfo(node)
+    color = self._ContentTypeToColor(node.ContentType())
+    if node.Request():
+      max_age = node.Request().MaxAge()
+      shape = 'polygon' if max_age > 300 else 'oval'
+    else:
+      shape = 'doubleoctagon'
+    styles = ['filled']
+    if node.IsAd() or node.IsTracking():
+      styles += ['bold', 'diagonals']
+    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
+            'fillcolor = %s; shape = %s];\n'
+            % (node.Index(), node.ShortName(),
+               node.StartTime() - self._global_start,
+               node.EndTime() - self._global_start,
+               node.EndTime() - node.StartTime(),
+               ','.join(styles), color, shape))
diff --git a/loading/model_graph_unittest.py b/loading/model_graph_unittest.py
new file mode 100644
index 0000000..510e945
--- /dev/null
+++ b/loading/model_graph_unittest.py
@@ -0,0 +1,29 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import gzip
+import json
+import os.path
+import tempfile
+import unittest
+
+import frame_load_lens
+import loading_model
+import loading_trace
+import model_graph
+
+TEST_DATA_DIR = os.path.join(os.path.dirname(__file__), 'testdata')
+
+class ModelGraphTestCase(unittest.TestCase):
+  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
+
+  def test_EndToEnd(self):
+    # Test that we don't crash. This also runs through frame_load_lens.
+    tmp = tempfile.NamedTemporaryFile()
+    with gzip.GzipFile(self._ROLLING_STONE) as f:
+      trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
+      frame_lens = frame_load_lens.FrameLoadLens(trace)
+      graph = loading_model.ResourceGraph(trace=trace, frame_lens=frame_lens)
+      visualization = model_graph.GraphVisualization(graph)
+      visualization.OutputDot(tmp)
diff --git a/loading/request_track.py b/loading/request_track.py
index 566fb3c..f4cf0e6 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -7,6 +7,7 @@
 When executed, parses a JSON dump of DevTools messages.
 """
 
+import bisect
 import collections
 import copy
 import json
@@ -72,6 +73,9 @@ class Request(object):
                  chunks received, with their offset in ms relative to
                  Timing.requestTime.
     failed: (bool) Whether the request failed.
+    start_msec: (float) Request start time, in milliseconds from chrome start.
+    end_msec: (float) Request end time, in milliseconds from chrome start.
+      start_msec.
   """
   REQUEST_PRIORITIES = ('VeryLow', 'Low', 'Medium', 'High', 'VeryHigh')
   RESOURCE_TYPES = ('Document', 'Stylesheet', 'Image', 'Media', 'Font',
@@ -104,6 +108,19 @@ class Request(object):
     self.data_chunks = []
     self.failed = False
 
+  @property
+  def start_msec(self):
+    return self.timing.request_time * 1000
+
+  @property
+  def end_msec(self):
+    if self.start_msec is None:
+      return None
+    return self.start_msec + max(
+        [0] + [t for f, t in self.timing._asdict().iteritems()
+               if f != 'request_time'])
+
+
   def _TimestampOffsetFromStartMs(self, timestamp):
     assert self.timing.request_time != -1
     request_time = self.timing.request_time
@@ -193,6 +210,11 @@ class RequestTrack(devtools_monitor.Track):
     self._requests_in_flight = {}  # requestId -> (request, status)
     self._completed_requests_by_id = {}
     self._redirects_count_by_id = collections.defaultdict(int)
+    self._indexed = False
+    self._request_start_timestamps = None
+    self._request_end_timestamps = None
+    self._requests_by_start = None
+    self._requests_by_end = None
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
@@ -204,6 +226,7 @@ class RequestTrack(devtools_monitor.Track):
 
   def Handle(self, method, msg):
     assert method in RequestTrack._METHOD_TO_HANDLER
+    self._indexed = False
     params = msg['params']
     request_id = params['requestId']
     RequestTrack._METHOD_TO_HANDLER[method](self, request_id, params)
@@ -214,6 +237,56 @@ class RequestTrack(devtools_monitor.Track):
                       % len(self._requests_in_flight))
     return self._requests
 
+  def GetFirstRequestMillis(self):
+    """Find the canonical start time for this track.
+
+    Returns:
+      The millisecond timestamp of the first request.
+    """
+    assert self._requests, "No requests to analyze."
+    self._IndexRequests()
+    return self._request_start_timestamps[0]
+
+  def GetLastRequestMillis(self):
+    """Find the canonical start time for this track.
+
+    Returns:
+      The millisecond timestamp of the first request.
+    """
+    assert self._requests, "No requests to analyze."
+    self._IndexRequests()
+    return self._request_end_timestamps[-1]
+
+  def GetEventsStartingBetween(self, start_ms, end_ms):
+    """Return events that started in a range.
+
+    Args:
+      start_ms: the start time to query, in milliseconds from the first request.
+      end_ms: the end time to query, in milliseconds from the first request.
+
+    Returns:
+      A list of requests whose start time is in [start_ms, end_ms].
+    """
+    self._IndexRequests()
+    low = bisect.bisect_left(self._request_start_timestamps, start_ms)
+    high = bisect.bisect_right(self._request_start_timestamps, end_ms)
+    return self._requests_by_start[low:high]
+
+  def GetEventsEndingBetween(self, start_ms, end_ms):
+    """Return events that ended in a range.
+
+    Args:
+      start_ms: the start time to query, in milliseconds from the first request.
+      end_ms: the end time to query, in milliseconds from the first request.
+
+    Returns:
+      A list of requests whose end time is in [start_ms, end_ms].
+    """
+    self._IndexRequests()
+    low = bisect.bisect_left(self._request_end_timestamps, start_ms)
+    high = bisect.bisect_right(self._request_end_timestamps, end_ms)
+    return self._requests_by_end[low:high]
+
   def ToJsonDict(self):
     if self._requests_in_flight:
       logging.warning('Requests in flight, will be ignored in the dump')
@@ -238,6 +311,24 @@ class RequestTrack(devtools_monitor.Track):
         cls._INCONSISTENT_INITIATORS_KEY, 0)
     return result
 
+  def _IndexRequests(self):
+    # TODO(mattcary): if we ever have requests without timing then we either
+    # need a default, or to make an index that only includes requests with
+    # timings.
+    if self._indexed:
+      return
+    valid_requests = [r for r in self._requests
+                      if r.start_msec is not None]
+    self._requests_by_start = sorted(valid_requests,
+                                     key=lambda r: r.start_msec)
+    self._request_start_timestamps = [r.start_msec
+                                      for r in self._requests_by_start]
+    self._requests_by_end = sorted(valid_requests,
+                                     key=lambda r: r.end_msec)
+    self._request_end_timestamps = [r.end_msec
+                                    for r in self._requests_by_end]
+    self._indexed = True
+
   def _RequestWillBeSent(self, request_id, params):
     # Several "requestWillBeSent" events can be dispatched in a row in the case
     # of redirects.
diff --git a/loading/testdata/rollingstone.trace.gz b/loading/testdata/rollingstone.trace.gz
new file mode 100644
index 0000000..f4a239e
Binary files /dev/null and b/loading/testdata/rollingstone.trace.gz differ
diff --git a/loading/tracing.py b/loading/tracing.py
index 4959ed7..bc4d45e 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -43,15 +43,27 @@ class TracingTrack(devtools_monitor.Track):
 
     self._event_msec_index = None
     self._event_lists = None
+    self._base_msec = None
 
   def Handle(self, method, event):
     for e in event['params']['value']:
-      self._events.append(Event(e))
+      event = Event(e)
+      self._events.append(event)
+      if self._base_msec is None or event.start_msec < self._base_msec:
+        self._base_msec = event.start_msec
     # Just invalidate our indices rather than trying to be fancy and
     # incrementally update.
     self._event_msec_index = None
     self._event_lists = None
 
+  def GetFirstEventMillis(self):
+    """Find the canonical start time for this track.
+
+    Returns:
+      The millisecond timestamp of the first request.
+    """
+    return self._base_msec
+
   def GetEvents(self):
     return self._events
 
@@ -89,6 +101,13 @@ class TracingTrack(devtools_monitor.Track):
     events = [Event(e) for e in json_dict['events']]
     tracing_track = TracingTrack(None)
     tracing_track._events = events
+    tracing_track._base_msec = events[0].start_msec if events else 0
+    for e in events[1:]:
+      if e.type == 'M':
+        continue  # No timestamp for metadata events.
+      assert e.start_msec > 0
+      if e.start_msec < tracing_track._base_msec:
+        tracing_track._base_msec = e.start_msec
     return tracing_track
 
   def EventsEndingBetween(self, start_msec, end_msec):
@@ -129,7 +148,6 @@ class TracingTrack(devtools_monitor.Track):
     join and track the nesting of async, flow and other spanning events.
 
     Events such as instant and counter events that aren't indexable are skipped.
-
     """
     if self._event_msec_index is not None:
       return  # Already indexed.
@@ -185,6 +203,7 @@ class TracingTrack(devtools_monitor.Track):
           'N': self._ObjectCreated,
           'D': self._ObjectDestroyed,
           'X': self._Ignore,
+          'M': self._Ignore,
           None: self._Ignore,
           }
 

commit 90e5144b8ae4e976cff67db733a6b201974c80bb
Author: pasko <pasko@chromium.org>
Date:   Tue Feb 2 05:13:38 2016 -0800

    sandwich: Allow saving cache directory
    
    Adds an option --save-cache to run_sandwich.py to pull the cache directory from
    the device after all page loads completed, preserfing modification times.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1651193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#372931}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3438bda5ae0b01193ea61e88cef435dac3ab0d69

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 647e18e..7b688a7 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -15,6 +15,7 @@ import argparse
 import logging
 import os
 import sys
+import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -23,6 +24,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
 from devil.android import device_utils
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
 import devil_chromium
 
 import device_setup
@@ -34,6 +36,24 @@ import tracing
 
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
+# Directory name under --output to save the cache from the device.
+_CACHE_DIRECTORY_NAME = 'cache'
+
+# Name of cache subdirectory on the device where the cache index is stored.
+_INDEX_DIRECTORY_NAME = 'index-dir'
+
+# Name of the file containing the cache index. This file is stored on the device
+# in the cache directory under _INDEX_DIRECTORY_NAME.
+_REAL_INDEX_FILE_NAME = 'the-real-index'
+
+# Name of the chrome package.
+_CHROME_PACKAGE = (
+    constants.PACKAGE_INFO[device_setup.DEFAULT_CHROME_PACKAGE].package)
+
+# An estimate of time to wait for the device to become idle after expensive
+# operations, such as opening the launcher activity.
+_TIME_TO_DEVICE_IDLE_SECONDS = 2
+
 
 def _ReadUrlsFromJobDescription(job_name):
   """Retrieves the list of URLs associated with the job name."""
@@ -65,16 +85,66 @@ def _SaveChromeTrace(events, directory, subdirectory):
     subdirectory: directory name to create this particular trace in
   """
   target_directory = os.path.join(directory, subdirectory)
-  file_name = os.path.join(target_directory, 'trace.json')
+  filename = os.path.join(target_directory, 'trace.json')
   try:
     os.makedirs(target_directory)
-    with open(file_name, 'w') as f:
+    with open(filename, 'w') as f:
       json.dump({'traceEvents': events['events'], 'metadata': {}}, f)
   except IOError:
-    logging.warning('Could not save a trace: %s' % file_name)
+    logging.warning('Could not save a trace: %s' % filename)
     # Swallow the exception.
 
 
+def _UpdateTimestampFromAdbStat(filename, stat):
+  os.utime(filename, (stat.st_time, stat.st_time))
+
+
+def _SaveBrowserCache(device, output_directory):
+  """Pulls the browser cache from the device and saves it locally.
+
+  Cache is saved with the same file structure as on the device. Timestamps are
+  important to preserve because indexing and eviction depends on them.
+
+  Args:
+    output_directory: name of the directory for saving cache.
+  """
+  save_target = os.path.join(output_directory, _CACHE_DIRECTORY_NAME)
+  try:
+    os.makedirs(save_target)
+  except IOError:
+    logging.warning('Could not create directory: %s' % save_target)
+    raise
+
+  cache_directory = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
+  for filename, stat in device.adb.Ls(cache_directory):
+    if filename == '..':
+      continue
+    if filename == '.':
+      cache_directory_stat = stat
+      continue
+    original_file = os.path.join(cache_directory, filename)
+    saved_file = os.path.join(save_target, filename)
+    device.adb.Pull(original_file, saved_file)
+    _UpdateTimestampFromAdbStat(saved_file, stat)
+    if filename == _INDEX_DIRECTORY_NAME:
+      # The directory containing the index was pulled recursively, update the
+      # timestamps for known files. They are ignored by cache backend, but may
+      # be useful for debugging.
+      index_dir_stat = stat
+      saved_index_dir = os.path.join(save_target, _INDEX_DIRECTORY_NAME)
+      saved_index_file = os.path.join(saved_index_dir, _REAL_INDEX_FILE_NAME)
+      for sub_file, sub_stat in device.adb.Ls(original_file):
+        if sub_file == _REAL_INDEX_FILE_NAME:
+          _UpdateTimestampFromAdbStat(saved_index_file, sub_stat)
+          break
+      _UpdateTimestampFromAdbStat(saved_index_dir, index_dir_stat)
+
+  # Store the cache directory modification time. It is important to update it
+  # after all files in it have been written. The timestamp is compared with
+  # the contents of the index file when freshness is determined.
+  _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
+
+
 def main():
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -86,6 +156,10 @@ def main():
                       help='Name of output directory to create.')
   parser.add_argument('--repeat', default=1, type=int,
                       help='How many times to run the job')
+  parser.add_argument('--save-cache', default=False,
+                      action='store_true',
+                      help='Clear HTTP cache before start,' +
+                      'save cache before exit.')
   args = parser.parse_args()
 
   try:
@@ -96,10 +170,13 @@ def main():
 
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
+
   pages_loaded = 0
-  for _ in xrange(args.repeat):
+  for iteration in xrange(args.repeat):
     for url in job_urls:
       with device_setup.DeviceConnection(device) as connection:
+        if iteration == 0 and pages_loaded == 0 and args.save_cache:
+          connection.ClearCache()
         page_track.PageTrack(connection)
         tracing_track = tracing.TracingTrack(connection,
             categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
@@ -110,6 +187,14 @@ def main():
         _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
             str(pages_loaded))
 
+  if args.save_cache:
+    # Move Chrome to background to allow it to flush the index.
+    device.adb.Shell('am start com.google.android.launcher')
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    device.KillAll(_CHROME_PACKAGE, quiet=True)
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    _SaveBrowserCache(device, args.output)
+
 
 if __name__ == '__main__':
   sys.exit(main())

commit 61ad3844d5ac9493e425007e3d00bfaed8f06091
Author: pasko <pasko@chromium.org>
Date:   Thu Jan 28 09:04:09 2016 -0800

    Script to load a list of URLs in a loop and save traces.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1645003003
    
    Cr-Original-Commit-Position: refs/heads/master@{#372111}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6f4d9bc54affbe4eb8a26adc5121481093e1e886

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
new file mode 100755
index 0000000..647e18e
--- /dev/null
+++ b/loading/run_sandwich.py
@@ -0,0 +1,115 @@
+#! /usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Instructs Chrome to load series of web pages and reports results.
+
+When running Chrome is sandwiched between preprocessed disk caches and
+WepPageReplay serving all connections.
+
+TODO(pasko): implement cache preparation and WPR.
+"""
+
+import argparse
+import logging
+import os
+import sys
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
+
+import device_setup
+import devtools_monitor
+import json
+import page_track
+import tracing
+
+
+_JOB_SEARCH_PATH = 'sandwich_jobs'
+
+
+def _ReadUrlsFromJobDescription(job_name):
+  """Retrieves the list of URLs associated with the job name."""
+  try:
+    # Extra sugar: attempt to load from a relative path.
+    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
+        job_name)
+    with open(json_file_name) as f:
+      json_data = json.load(f)
+  except IOError:
+    # Attempt to read by regular file name.
+    with open(job_name) as f:
+      json_data = json.load(f)
+
+  key = 'urls'
+  if json_data and key in json_data:
+    url_list = json_data[key]
+    if isinstance(url_list, list) and len(url_list) > 0:
+      return url_list
+  raise Exception('Job description does not define a list named "urls"')
+
+
+def _SaveChromeTrace(events, directory, subdirectory):
+  """Saves the trace events, ignores IO errors.
+
+  Args:
+    events: a dict as returned by TracingTrack.ToJsonDict()
+    directory: directory name contining all traces
+    subdirectory: directory name to create this particular trace in
+  """
+  target_directory = os.path.join(directory, subdirectory)
+  file_name = os.path.join(target_directory, 'trace.json')
+  try:
+    os.makedirs(target_directory)
+    with open(file_name, 'w') as f:
+      json.dump({'traceEvents': events['events'], 'metadata': {}}, f)
+  except IOError:
+    logging.warning('Could not save a trace: %s' % file_name)
+    # Swallow the exception.
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+  devil_chromium.Initialize()
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--job', required=True,
+                      help='JSON file with job description.')
+  parser.add_argument('--output', required=True,
+                      help='Name of output directory to create.')
+  parser.add_argument('--repeat', default=1, type=int,
+                      help='How many times to run the job')
+  args = parser.parse_args()
+
+  try:
+    os.makedirs(args.output)
+  except OSError:
+    logging.error('Cannot create directory for results: %s' % args.output)
+    raise
+
+  job_urls = _ReadUrlsFromJobDescription(args.job)
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  pages_loaded = 0
+  for _ in xrange(args.repeat):
+    for url in job_urls:
+      with device_setup.DeviceConnection(device) as connection:
+        page_track.PageTrack(connection)
+        tracing_track = tracing.TracingTrack(connection,
+            categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
+        connection.SetUpMonitoring()
+        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+        connection.StartMonitoring()
+        pages_loaded += 1
+        _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
+            str(pages_loaded))
+
+
+if __name__ == '__main__':
+  sys.exit(main())
diff --git a/loading/sandwich_jobs/wikipedia.json b/loading/sandwich_jobs/wikipedia.json
new file mode 100644
index 0000000..84e91d8
--- /dev/null
+++ b/loading/sandwich_jobs/wikipedia.json
@@ -0,0 +1,5 @@
+{
+  "urls": [
+    "https://en.m.wikipedia.org/wiki/Science"
+  ]
+}

commit 9b293438d94b7cfa0a74235edd76bf3122c4b0a6
Author: mattcary <mattcary@chromium.org>
Date:   Thu Jan 28 04:42:25 2016 -0800

    Practical fixes for ad filtering.
    
    Only import adblocker if we actually define any rules. If we define
    rules, and fail to import adblockerparser, give detailed installation
    instructions.
    
    Fix typo that prevented the filtering from working in the model, and have the filtering actually use the new adblocker matcher rather than the old heuristic.
    
    Review URL: https://codereview.chromium.org/1645953002
    
    Cr-Original-Commit-Position: refs/heads/master@{#372079}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8497c4725e9a01b5c05e8f10af5e523fa77d1580

diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
index 2e8f0bd..b12b9af 100644
--- a/loading/content_classification_lens.py
+++ b/loading/content_classification_lens.py
@@ -4,8 +4,8 @@
 
 """Labels requests according to the type of content they represent."""
 
-import adblockparser # Available on PyPI, through pip.
 import collections
+import logging
 import os
 
 import loading_trace
@@ -93,10 +93,23 @@ class _RulesMatcher(object):
       no_whitelist: (bool) Whether the whitelisting rules should be ignored.
     """
     self._rules = self._FilterRules(rules, no_whitelist)
-    self._matcher = adblockparser.AdblockRules(self._rules)
+    if self._rules:
+      try:
+        import adblockparser
+        self._matcher = adblockparser.AdblockRules(self._rules)
+      except ImportError:
+        logging.critical('Likely you need to install adblockparser. Try:\n'
+                         ' pip install --user adblockparser\n'
+                         'For 10-100x better performance, also try:\n'
+                         " pip install --user 're2 >= 0.2.21'")
+        raise
+    else:
+      self._matcher = None
 
   def Matches(self, request):
     """Returns whether a request matches one of the rules."""
+    if self._matcher is None:
+      return False
     url = request.url
     return self._matcher.should_block(url, self._GetOptions(request))
 
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index cca7d52..b4f5ad1 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -25,6 +25,13 @@ class ContentClassificationLensTestCase(unittest.TestCase):
                    'frame_id': '123.13', 'parent_frame_id': _MAIN_FRAME_ID}]
   _RULES = ['bla.com']
 
+  def testNoRules(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertFalse(lens.IsAdRequest(self._REQUEST))
+    self.assertFalse(lens.IsTrackingRequest(self._REQUEST))
+
   def testAdRequest(self):
     trace = test_utils.LoadingTraceFromEvents(
         [self._REQUEST], self._PAGE_EVENTS)
diff --git a/loading/loading_model.py b/loading/loading_model.py
index b6aeca6..e46b705 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -202,7 +202,8 @@ class ResourceGraph(object):
     Returns:
       True if the node is not ad-related.
     """
-    return not self._IsAdUrl(self._node_info[node.Index()].Url())
+    node_info = self._node_info[node.Index()]
+    return not (node_info.IsAd() or node_info.IsTracking())
 
   def MakeGraphviz(self, output, highlight=None):
     """Output a graphviz representation of our DAG.
@@ -504,8 +505,9 @@ class ResourceGraph(object):
       node = dag.Node(next_index)
       node_info = self._NodeInfo(node, request)
       if self._content_lens:
-        node.SetRequestContent(self._content_lens.IsAdRequest(request),
-                               self._content_lens.IsTrackingRequest(request))
+        node_info.SetRequestContent(
+            self._content_lens.IsAdRequest(request),
+            self._content_lens.IsTrackingRequest(request))
       self._nodes.append(node)
       self._node_info.append(node_info)
 
@@ -641,82 +643,6 @@ class ResourceGraph(object):
                node_info.EndTime() - node_info.StartTime(),
                ','.join(styles), color, shape))
 
-  @classmethod
-  def _IsAdUrl(cls, url):
-    """Return true if the url is an ad.
-
-    We group content that doesn't seem to be specific to the website along with
-    ads, eg staticxx.facebook.com, as well as analytics like googletagmanager (?
-    is this correct?).
-
-    Args:
-      url: The full string url to examine.
-
-    Returns:
-      True iff the url appears to be an ad.
-
-    """
-    # See below for how these patterns are defined.
-    AD_PATTERNS = ['2mdn.net',
-                   'admarvel.com',
-                   'adnxs.com',
-                   'adobedtm.com',
-                   'adsrvr.org',
-                   'adsafeprotected.com',
-                   'adsymptotic.com',
-                   'adtech.de',
-                   'adtechus.com',
-                   'advertising.com',
-                   'atwola.com',  # brand protection from cscglobal.com?
-                   'bounceexchange.com',
-                   'betrad.com',
-                   'casalemedia.com',
-                   'cloudfront.net//test.png',
-                   'cloudfront.net//atrk.js',
-                   'contextweb.com',
-                   'crwdcntrl.net',
-                   'doubleclick.net',
-                   'dynamicyield.com',
-                   'krxd.net',
-                   'facebook.com//ping',
-                   'fastclick.net',
-                   'google.com//-ads.js',
-                   'cse.google.com',  # Custom search engine.
-                   'googleadservices.com',
-                   'googlesyndication.com',
-                   'googletagmanager.com',
-                   'lightboxcdn.com',
-                   'mediaplex.com',
-                   'meltdsp.com',
-                   'mobile.nytimes.com//ads-success',
-                   'mookie1.com',
-                   'newrelic.com',
-                   'nr-data.net',   # Apparently part of newrelic.
-                   'optnmnstr.com',
-                   'pubmatic.com',
-                   'quantcast.com',
-                   'quantserve.com',
-                   'rubiconproject.com',
-                   'scorecardresearch.com',
-                   'sekindo.com',
-                   'serving-sys.com',
-                   'sharethrough.com',
-                   'staticxx.facebook.com',  # ?
-                   'syndication.twimg.com',
-                   'tapad.com',
-                   'yieldmo.com',
-                ]
-    parts = urlparse.urlparse(url)
-    for pattern in AD_PATTERNS:
-      if '//' in pattern:
-        domain, path = pattern.split('//')
-      else:
-        domain, path = (pattern, None)
-      if parts.netloc.endswith(domain):
-        if not path or path in parts.path:
-          return True
-    return False
-
   def _ExtractImages(self):
     """Return interesting image resources.
 
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index b866136..88a62d9 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -192,18 +192,6 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5])
 
-  def test_AdUrl(self):
-    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/test.png'))
-    self.assertFalse(loading_model.ResourceGraph._IsAdUrl(
-        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/tst.png'))
-
-    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://ums.adtechus.com/mapuser?providerid=1003;'
-        'userid=RUmecco4z3o===='))
-    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'))
-
 
 if __name__ == '__main__':
   unittest.main()

commit 952de4bd9ca163f3c2865041d5bdaf9b7e159e8e
Author: nednguyen <nednguyen@google.com>
Date:   Tue Jan 26 17:39:28 2016 -0800

    Update references to tools/telemetry in .gn, .gyp & .isolate files
    
    Now that tools/telemetry is moved to catapult/ repo
    (https://github.com/catapult-project/catapult/commit/40afc53353daf28b955397c8e27d6f1de7bb10b6), we will soon remove tools/telemetry/.
    
    This patch update the reference to telemetry/ in .gn, .gyp & .isolate files
    
    BUG=478864
    TEST=patch set 1 delete telemetry/telemetry.gyp, telemetry/BUILD.gn & telemetry/telemetry.isolate, CQ shows that all bots are green.
    
    Review URL: https://codereview.chromium.org/1638483002
    
    Cr-Original-Commit-Position: refs/heads/master@{#371681}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c42bf9152ace37475c1149ec57ffe2838671085b

diff --git a/BUILD.gn b/BUILD.gn
index 012dc35..4e074bd 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -8,13 +8,13 @@
 # GYP: //tools/android/android_tools.gyp:android_tools
 group("android_tools") {
   deps = [
+    "//third_party/catapult/telemetry:bitmaptools($host_toolchain)",
     "//tools/android/adb_reboot",
     "//tools/android/file_poller",
     "//tools/android/forwarder2",
     "//tools/android/md5sum",
     "//tools/android/memtrack_helper:memtrack_helper",
     "//tools/android/purge_ashmem",
-    "//tools/telemetry:bitmaptools($host_toolchain)",
   ]
 }
 
diff --git a/android_tools.gyp b/android_tools.gyp
index b963b3f..065c804 100644
--- a/android_tools.gyp
+++ b/android_tools.gyp
@@ -17,7 +17,7 @@
         'md5sum/md5sum.gyp:md5sum',
         'memtrack_helper/memtrack_helper.gyp:memtrack_helper',
         'purge_ashmem/purge_ashmem.gyp:purge_ashmem',
-        '../../tools/telemetry/telemetry.gyp:*#host',
+        '../../third_party/catapult/telemetry/telemetry.gyp:*#host',
       ],
     },
     {

commit b274fe6201a6a50df3c83378ce2915944995fd4a
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 26 08:24:51 2016 -0800

    tools/android/loading: Add support for multiple redirects.
    
    This properly identify redirect chains. Before this CL, each link in the
    redirect chain would seem to be initiated by the eventual request,
    instead of the previous redirect.
    
    Also logs the number of cases where the redirected request doesn't have
    the same initiator as the initial request.
    
    Review URL: https://codereview.chromium.org/1633813005
    
    Cr-Original-Commit-Position: refs/heads/master@{#371522}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 57c2ecb994e01eeb2fc46ecd9ebee8ae9cca8d13

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 90e77a8..b6aeca6 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -247,7 +247,9 @@ class ResourceGraph(object):
       for s in n.Successors():
         style = 'color = orange'
         annotations = self._EdgeAnnotation(n, s)
-        if 'parser' in annotations:
+        if 'redirect' in annotations:
+          style = 'color = black'
+        elif 'parser' in annotations:
           style = 'color = red'
         elif 'stack' in annotations:
           style = 'color = blue'
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 7033cef..b866136 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -72,7 +72,9 @@ class LoadingModelTestCase(unittest.TestCase):
                        self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
                        self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
                        self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()],
-            'metadata': { 'duplicates_count' : 0 }},
+            'metadata': {
+                request_track.RequestTrack._DUPLICATES_KEY: 0,
+                request_track.RequestTrack._INCONSISTENT_INITIATORS_KEY: 0}},
          'url': 'foo.com',
          'tracing_track': {'events': []},
          'page_track': {'events': []},
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 47ea3af..8128957 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -64,8 +64,7 @@ class RequestDependencyLens(object):
     """
     reason = request.initiator['type']
     assert reason in request_track.Request.INITIATORS
-    # Redirect suffixes are added in RequestTrack.
-    if request.request_id.endswith(request_track.RequestTrack.REDIRECT_SUFFIX):
+    if reason == 'redirect':
       return self._GetInitiatingRequestRedirect(request)
     elif reason == 'parser':
       return self._GetInitiatingRequestParser(request)
@@ -76,14 +75,11 @@ class RequestDependencyLens(object):
       return self._GetInitiatingRequestOther(request)
 
   def _GetInitiatingRequestRedirect(self, request):
-    request_id = request.request_id[:request.request_id.index(
-        request_track.RequestTrack.REDIRECT_SUFFIX)]
-    assert request_id in self._requests_by_id
-    dependent_request = self._requests_by_id[request_id]
-    assert request.timing.request_time < \
-        dependent_request.timing.request_time, '.\n'.join(
-            [str(request), str(dependent_request)])
-    return (request, dependent_request, 'redirect')
+    assert request_track.Request.INITIATING_REQUEST in request.initiator
+    initiating_request_id = request.initiator[
+        request_track.Request.INITIATING_REQUEST]
+    assert initiating_request_id in self._requests_by_id
+    return (self._requests_by_id[initiating_request_id], request, 'redirect')
 
   def _GetInitiatingRequestParser(self, request):
     url = request.initiator['url']
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 4fc5d90..0ab7403 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -13,9 +13,17 @@ import test_utils
 
 class RequestDependencyLensTestCase(unittest.TestCase):
   _REDIRECT_REQUEST = Request.FromJsonDict(
-      {'url': 'http://bla.com', 'request_id': '1234.1.redirect',
+      {'url': 'http://bla.com', 'request_id': '1234.redirect.1',
        'initiator': {'type': 'other'},
        'timestamp': 1, 'timing': TimingFromDict({})})
+  _REDIRECTED_REQUEST = Request.FromJsonDict({
+      'url': 'http://bla.com',
+      'request_id': '1234.1',
+      'frame_id': '123.1',
+      'initiator': {'type': 'redirect',
+                    'initiating_request': '1234.redirect.1'},
+      'timestamp': 2,
+      'timing': TimingFromDict({})})
   _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
                                    'request_id': '1234.1',
                                    'frame_id': '123.1',
@@ -55,7 +63,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testRedirectDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REQUEST])
+        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -86,7 +94,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testSeveralDependencies(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
+        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST, self._JS_REQUEST,
          self._JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
diff --git a/loading/request_track.py b/loading/request_track.py
index 60a54e5..566fb3c 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -46,7 +46,7 @@ class Request(object):
   third_party/WebKit/Source/devtools/protocol.json.
 
   Fields:
-    request_id: (str) unique request ID. Postfixed with REDIRECT_SUFFIX for
+    request_id: (str) unique request ID. Postfixed with _REDIRECT_SUFFIX for
                 redirects.
     frame_id: (str) unique frame identifier.
     loader_id: (str) unique frame identifier.
@@ -77,7 +77,9 @@ class Request(object):
   RESOURCE_TYPES = ('Document', 'Stylesheet', 'Image', 'Media', 'Font',
                     'Script', 'TextTrack', 'XHR', 'Fetch', 'EventSource',
                     'WebSocket', 'Manifest', 'Other')
-  INITIATORS = ('parser', 'script', 'other')
+  INITIATORS = ('parser', 'script', 'other', 'redirect')
+  INITIATING_REQUEST = 'initiating_request'
+  ORIGINAL_INITIATOR = 'original_initiator'
   def __init__(self):
     self.request_id = None
     self.frame_id = None
@@ -172,7 +174,7 @@ class Request(object):
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
-  REDIRECT_SUFFIX = '.redirect'
+  _REDIRECT_SUFFIX = '.redirect'
   # Request status
   _STATUS_SENT = 0
   _STATUS_RESPONSE = 1
@@ -183,12 +185,14 @@ class RequestTrack(devtools_monitor.Track):
   _EVENTS_KEY = 'events'
   _METADATA_KEY = 'metadata'
   _DUPLICATES_KEY = 'duplicates_count'
+  _INCONSISTENT_INITIATORS_KEY = 'inconsistent_initiators'
   def __init__(self, connection):
     super(RequestTrack, self).__init__(connection)
     self._connection = connection
     self._requests = []
     self._requests_in_flight = {}  # requestId -> (request, status)
     self._completed_requests_by_id = {}
+    self._redirects_count_by_id = collections.defaultdict(int)
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
@@ -196,6 +200,7 @@ class RequestTrack(devtools_monitor.Track):
     # detect this.
     self._request_id_to_response_received = {}
     self.duplicates_count = 0
+    self.inconsistent_initiators_count = 0
 
   def Handle(self, method, msg):
     assert method in RequestTrack._METHOD_TO_HANDLER
@@ -214,7 +219,10 @@ class RequestTrack(devtools_monitor.Track):
       logging.warning('Requests in flight, will be ignored in the dump')
     return {self._EVENTS_KEY: [
         request.ToJsonDict() for request in self._requests],
-            self._METADATA_KEY: {self._DUPLICATES_KEY: self.duplicates_count}}
+            self._METADATA_KEY: {
+                self._DUPLICATES_KEY: self.duplicates_count,
+                self._INCONSISTENT_INITIATORS_KEY:
+                self.inconsistent_initiators_count}}
 
   @classmethod
   def FromJsonDict(cls, json_dict):
@@ -224,14 +232,18 @@ class RequestTrack(devtools_monitor.Track):
     requests = [Request.FromJsonDict(request)
                 for request in json_dict[cls._EVENTS_KEY]]
     result._requests = requests
-    result.duplicates_count = json_dict[cls._METADATA_KEY][cls._DUPLICATES_KEY]
+    metadata = json_dict[cls._METADATA_KEY]
+    result.duplicates_count = metadata.get(cls._DUPLICATES_KEY, 0)
+    result.inconsistent_initiators_count = metadata.get(
+        cls._INCONSISTENT_INITIATORS_KEY, 0)
     return result
 
   def _RequestWillBeSent(self, request_id, params):
     # Several "requestWillBeSent" events can be dispatched in a row in the case
     # of redirects.
+    redirect_initiator = None
     if request_id in self._requests_in_flight:
-      self._HandleRedirect(request_id, params)
+      redirect_initiator = self._HandleRedirect(request_id, params)
     assert (request_id not in self._requests_in_flight
             and request_id not in self._completed_requests_by_id)
     r = Request()
@@ -247,6 +259,16 @@ class RequestTrack(devtools_monitor.Track):
                      ('headers', 'headers'),
                      ('initialPriority', 'initial_priority')))
     r.resource_type = params.get('type', 'Other')
+    if redirect_initiator:
+      original_initiator = r.initiator
+      r.initiator = redirect_initiator
+      r.initiator[Request.ORIGINAL_INITIATOR] = original_initiator
+      initiating_request = self._completed_requests_by_id[
+          redirect_initiator[Request.INITIATING_REQUEST]]
+      initiating_initiator = initiating_request.initiator.get(
+          Request.ORIGINAL_INITIATOR, initiating_request.initiator)
+      if initiating_initiator != original_initiator:
+        self.inconsistent_initiators_count += 1
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_SENT)
 
   def _HandleRedirect(self, request_id, params):
@@ -256,15 +278,23 @@ class RequestTrack(devtools_monitor.Track):
     # one. Finalize the first request.
     assert 'redirectResponse' in params
     redirect_response = params['redirectResponse']
+
     _CopyFromDictToObject(redirect_response, r,
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache')))
     r.timing = TimingFromDict(redirect_response['timing'])
-    r.request_id = request_id + self.REDIRECT_SUFFIX
+
+    redirect_index = self._redirects_count_by_id[request_id]
+    self._redirects_count_by_id[request_id] += 1
+    r.request_id = '%s%s.%d' % (request_id, self._REDIRECT_SUFFIX,
+                                 redirect_index + 1)
+    initiator = {
+        'type': 'redirect', Request.INITIATING_REQUEST: r.request_id}
     self._requests_in_flight[r.request_id] = (r, RequestTrack._STATUS_FINISHED)
     del self._requests_in_flight[request_id]
     self._FinalizeRequest(r.request_id)
+    return initiator
 
   def _RequestServedFromCache(self, request_id, _):
     assert request_id in self._requests_in_flight
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index fde7399..e742f60 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -195,7 +195,47 @@ class RequestTrackTestCase(unittest.TestCase):
     self.assertEquals(1, len(self.request_track.GetEvents()))
     redirect_request = self.request_track.GetEvents()[0]
     self.assertTrue(redirect_request.request_id.endswith(
-        RequestTrack.REDIRECT_SUFFIX))
+        RequestTrack._REDIRECT_SUFFIX + '.1'))
+    request = self.request_track._requests_in_flight.values()[0][0]
+    self.assertEquals('redirect', request.initiator['type'])
+    self.assertEquals(
+        redirect_request.request_id,
+        request.initiator[Request.INITIATING_REQUEST])
+    self.assertEquals(0, self.request_track.inconsistent_initiators_count)
+
+  def testMultipleRedirects(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REDIRECT)
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REDIRECT)
+    self.assertEquals(1, len(self.request_track._requests_in_flight))
+    self.assertEquals(2, len(self.request_track.GetEvents()))
+    first_redirect_request = self.request_track.GetEvents()[0]
+    self.assertTrue(first_redirect_request.request_id.endswith(
+        RequestTrack._REDIRECT_SUFFIX + '.1'))
+    second_redirect_request = self.request_track.GetEvents()[1]
+    self.assertTrue(second_redirect_request.request_id.endswith(
+        RequestTrack._REDIRECT_SUFFIX + '.2'))
+    self.assertEquals('redirect', second_redirect_request.initiator['type'])
+    self.assertEquals(
+        first_redirect_request.request_id,
+        second_redirect_request.initiator[Request.INITIATING_REQUEST])
+    request = self.request_track._requests_in_flight.values()[0][0]
+    self.assertEquals('redirect', request.initiator['type'])
+    self.assertEquals(
+        second_redirect_request.request_id,
+        request.initiator[Request.INITIATING_REQUEST])
+    self.assertEquals(0, self.request_track.inconsistent_initiators_count)
+
+  def testInconsistentInitiators(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    request = copy.deepcopy(RequestTrackTestCase._REDIRECT)
+    request['params']['initiator']['type'] = 'script'
+    self.request_track.Handle('Network.requestWillBeSent', request)
+    self.assertEquals(1, self.request_track.inconsistent_initiators_count)
 
   def testRejectDuplicates(self):
     msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
@@ -281,6 +321,7 @@ class RequestTrackTestCase(unittest.TestCase):
   def testCanDeserialize(self):
     self._ValidSequence(self.request_track)
     self.request_track.duplicates_count = 142
+    self.request_track.inconsistent_initiators_count = 123
     json_dict = self.request_track.ToJsonDict()
     request_track = RequestTrack.FromJsonDict(json_dict)
     self.assertEquals(self.request_track, request_track)

commit 3e8df3edfc407047f059165cd6b7285582d05cf4
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 26 05:59:21 2016 -0800

    tools/android/loading: ContentClassificationLens, ads and tracking requests.
    
    Adds a lens leveraging a rules file to tag whether a given request is
    related to Ads and/or tracking/analytics. This CL also displays this in
    the PNG output of the dependency graph.
    
    Review URL: https://codereview.chromium.org/1626393002
    
    Cr-Original-Commit-Position: refs/heads/master@{#371504}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4208a052792bde3b964aa443201ee6c66906f423

diff --git a/loading/analyze.py b/loading/analyze.py
index 27294d3..5f15fe2 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -24,6 +24,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 from pylib import constants
 
+import content_classification_lens
 import device_setup
 import loading_model
 import loading_trace
@@ -147,10 +148,14 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
 
 # TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
 # with here.
-def _ProcessRequests(filename):
+def _ProcessRequests(filename, ad_rules_filename='',
+                     tracking_rules_filename=''):
   with open(filename) as f:
-    return loading_model.ResourceGraph(
-        loading_trace.LoadingTrace.FromJsonDict(json.load(f)))
+    trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
+    content_lens = (
+        content_classification_lens.ContentClassificationLens.WithRulesFiles(
+            trace, ad_rules_filename, tracking_rules_filename))
+    return loading_model.ResourceGraph(trace, content_lens)
 
 
 def InvalidCommand(cmd):
@@ -185,8 +190,11 @@ def DoPng(arg_str):
   parser.add_argument('--eog', action='store_true')
   parser.add_argument('--highlight')
   parser.add_argument('--noads', action='store_true')
+  parser.add_argument('--ad_rules', default='')
+  parser.add_argument('--tracking_rules', default='')
   args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
+  graph = _ProcessRequests(
+      args.request_json, args.ad_rules, args.tracking_rules)
   if args.noads:
     graph.Set(node_filter=graph.FilterAds)
   tmp = tempfile.NamedTemporaryFile()
diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
new file mode 100644
index 0000000..2e8f0bd
--- /dev/null
+++ b/loading/content_classification_lens.py
@@ -0,0 +1,118 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Labels requests according to the type of content they represent."""
+
+import adblockparser # Available on PyPI, through pip.
+import collections
+import os
+
+import loading_trace
+import request_track
+
+
+class ContentClassificationLens(object):
+  """Associates requests and frames with the type of content they represent."""
+  def __init__(self, trace, ad_rules, tracking_rules):
+    """Initializes an instance of ContentClassificationLens.
+
+    Args:
+      trace: (LoadingTrace) loading trace.
+      ad_rules: ([str]) List of Adblock+ compatible rules used to classify ads.
+      tracking_rules: ([str]) List of Adblock+ compatible rules used to
+                      classify tracking and analytics.
+    """
+    self._trace = trace
+    self._requests = trace.request_track.GetEvents()
+    self._main_frame_id = trace.page_track.GetEvents()[0]['frame_id']
+    self._frame_to_requests = collections.defaultdict(list)
+    self._ad_requests = set()
+    self._tracking_requests = set()
+    self._ad_matcher = _RulesMatcher(ad_rules, True)
+    self._tracking_matcher = _RulesMatcher(tracking_rules, True)
+    self._GroupRequestsByFrameId()
+    self._LabelRequests()
+
+  def IsAdRequest(self, request):
+    """Returns True iff the request matches one of the ad_rules."""
+    return request.request_id in self._ad_requests
+
+  def IsTrackingRequest(self, request):
+    """Returns True iff the request matches one of the tracking_rules."""
+    return request.request_id in self._tracking_requests
+
+  def IsAdFrame(self, frame_id, ratio):
+    """A Frame is an Ad frame if more than |ratio| of its requests are
+    ad-related, and is not the main frame."""
+    if frame_id == self._main_frame_id:
+      return False
+    ad_requests_count = sum(r in self._ad_requests
+                            for r in self._frame_to_requests[frame_id])
+    frame_requests_count = len(self._frame_to_requests[frame_id])
+    return (float(ad_requests_count) / frame_requests_count) > ratio
+
+  @classmethod
+  def WithRulesFiles(cls, trace, ad_rules_filename, tracking_rules_filename):
+    """Returns an instance of ContentClassificationLens with the rules read
+    from files.
+    """
+    ad_rules = []
+    tracking_rules = []
+    if os.path.exists(ad_rules_filename):
+      ad_rules = open(ad_rules_filename, 'r').readlines()
+    if os.path.exists(tracking_rules_filename):
+      tracking_rules = open(tracking_rules_filename, 'r').readlines()
+    return ContentClassificationLens(trace, ad_rules, tracking_rules)
+
+  def _GroupRequestsByFrameId(self):
+    for request in self._requests:
+      frame_id = request.frame_id
+      self._frame_to_requests[frame_id].append(request.request_id)
+
+  def _LabelRequests(self):
+    for request in self._requests:
+      request_id = request.request_id
+      if self._ad_matcher.Matches(request):
+        self._ad_requests.add(request_id)
+      if self._tracking_matcher.Matches(request):
+        self._tracking_requests.add(request_id)
+
+
+class _RulesMatcher(object):
+  """Matches requests with rules in Adblock+ format."""
+  _WHITELIST_PREFIX = '@@'
+  _RESOURCE_TYPE_TO_OPTIONS_KEY = {
+      'Script': 'script', 'Stylesheet': 'stylesheet', 'Image': 'image',
+      'XHR': 'xmlhttprequest'}
+  def __init__(self, rules, no_whitelist):
+    """Initializes an instance of _RulesMatcher.
+
+    Args:
+      rules: ([str]) list of rules.
+      no_whitelist: (bool) Whether the whitelisting rules should be ignored.
+    """
+    self._rules = self._FilterRules(rules, no_whitelist)
+    self._matcher = adblockparser.AdblockRules(self._rules)
+
+  def Matches(self, request):
+    """Returns whether a request matches one of the rules."""
+    url = request.url
+    return self._matcher.should_block(url, self._GetOptions(request))
+
+  @classmethod
+  def _GetOptions(cls, request):
+    options = {}
+    resource_type = request.resource_type
+    option = cls._RESOURCE_TYPE_TO_OPTIONS_KEY.get(resource_type)
+    if option:
+      options[option] = True
+    return options
+
+  @classmethod
+  def _FilterRules(cls, rules, no_whitelist):
+    if not no_whitelist:
+      return rules
+    else:
+      return [rule for rule in rules
+              if not rule.startswith(cls._WHITELIST_PREFIX)]
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
new file mode 100644
index 0000000..cca7d52
--- /dev/null
+++ b/loading/content_classification_lens_unittest.py
@@ -0,0 +1,85 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import unittest
+
+from content_classification_lens import (ContentClassificationLens,
+                                         _RulesMatcher)
+from request_track import (Request, TimingFromDict)
+import test_utils
+
+
+class ContentClassificationLensTestCase(unittest.TestCase):
+  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'request_id': '1234.1',
+                                   'frame_id': '123.1',
+                                   'initiator': {'type': 'other'},
+                                   'timestamp': 2,
+                                   'timing': TimingFromDict({})})
+  _MAIN_FRAME_ID = '123.1'
+  _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
+                   'frame_id': _MAIN_FRAME_ID},
+                  {'method': 'Page.frameAttached',
+                   'frame_id': '123.13', 'parent_frame_id': _MAIN_FRAME_ID}]
+  _RULES = ['bla.com']
+
+  def testAdRequest(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertTrue(lens.IsAdRequest(self._REQUEST))
+    self.assertFalse(lens.IsTrackingRequest(self._REQUEST))
+
+  def testTrackingRequest(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], self._RULES)
+    self.assertFalse(lens.IsAdRequest(self._REQUEST))
+    self.assertTrue(lens.IsTrackingRequest(self._REQUEST))
+
+  def testMainFrameIsNotAdFrame(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST] * 10, self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertFalse(lens.IsAdFrame(self._MAIN_FRAME_ID, .5))
+
+  def testAdFrame(self):
+    request = self._REQUEST
+    request.frame_id = '123.123'
+    trace = test_utils.LoadingTraceFromEvents(
+        [request] * 10 + [self._REQUEST] * 5, self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertTrue(lens.IsAdFrame(request.frame_id, .5))
+
+
+class _MatcherTestCase(unittest.TestCase):
+  _RULES_WITH_WHITELIST = ['/thisisanad.', '@@myadvertisingdomain.com/*',
+                           '@@||www.mydomain.com/ads/$elemhide']
+  _SCRIPT_RULE = 'domainwithscripts.com/*$script'
+  _SCRIPT_REQUEST = Request.FromJsonDict(
+      {'url': 'http://domainwithscripts.com/bla.js',
+       'resource_type': 'Script',
+       'request_id': '1234.1',
+       'frame_id': '123.1',
+       'initiator': {'type': 'other'},
+       'timestamp': 2,
+       'timing': TimingFromDict({})})
+
+  def testRemovesWhitelistRules(self):
+    matcher = _RulesMatcher(self._RULES_WITH_WHITELIST, False)
+    self.assertEquals(3, len(matcher._rules))
+    matcher = _RulesMatcher(self._RULES_WITH_WHITELIST, True)
+    self.assertEquals(1, len(matcher._rules))
+
+  def testScriptRule(self):
+    matcher = _RulesMatcher([self._SCRIPT_RULE], False)
+    request = copy.deepcopy(self._SCRIPT_REQUEST)
+    request.resource_type = 'Stylesheet'
+    self.assertFalse(matcher.Matches(request))
+    self.assertTrue(matcher.Matches(self._SCRIPT_REQUEST))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/loading_model.py b/loading/loading_model.py
index 155da1d..90e77a8 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -30,14 +30,17 @@ class ResourceGraph(object):
   Set parameters:
     cache_all: if true, assume zero loading time for all resources.
   """
-  def __init__(self, trace):
+  def __init__(self, trace, content_lens=None):
     """Create from a LoadingTrace (or json of a trace).
 
     Args:
       trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
+      content_lens: (ContentClassificationLens) Lens used to annotate the
+                    nodes, or None.
     """
     if type(trace) == dict:
       trace = loading_trace.LoadingTrace.FromJsonDict(trace)
+    self._content_lens = content_lens
     self._BuildDag(trace)
     self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
@@ -339,6 +342,8 @@ class ResourceGraph(object):
         request: The request associated with this node.
       """
       self._request = request
+      self._is_ad = False
+      self._is_tracking = False
       self._node = node
       self._edge_costs = {}
       self._edge_annotations = {}
@@ -357,6 +362,21 @@ class ResourceGraph(object):
     def Index(self):
       return self._node.Index()
 
+    def SetRequestContent(self, is_ad, is_tracking):
+      """Sets the kind of content the request relates to.
+
+      Args:
+        is_ad: (bool) Whether the request is an Ad.
+        is_tracking: (bool) Whether the request is related to tracking.
+      """
+      (self._is_ad, self._is_tracking) = (is_ad, is_tracking)
+
+    def IsAd(self):
+      return self._is_ad
+
+    def IsTracking(self):
+      return self._is_tracking
+
     def Request(self):
       return self._request
 
@@ -481,6 +501,9 @@ class ResourceGraph(object):
       index_by_request[request] = next_index
       node = dag.Node(next_index)
       node_info = self._NodeInfo(node, request)
+      if self._content_lens:
+        node.SetRequestContent(self._content_lens.IsAdRequest(request),
+                               self._content_lens.IsTrackingRequest(request))
       self._nodes.append(node)
       self._node_info.append(node_info)
 
@@ -606,6 +629,8 @@ class ResourceGraph(object):
         if fragment in node_info.Url():
           styles.append('dotted')
           break
+    if node_info.IsAd() or node_info.IsTracking():
+      styles += ['bold', 'diagonals']
     return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
             'fillcolor = %s; shape = %s];\n'
             % (index, node_info.ShortName(),

commit efc254ebca7e8ed57f8d278b48e3adb2384bf49a
Author: lizeb <lizeb@chromium.org>
Date:   Mon Jan 25 08:09:15 2016 -0800

    tools/android/loading: Move testing boilerplate to a separate file.
    
    Review URL: https://codereview.chromium.org/1626353002
    
    Cr-Original-Commit-Position: refs/heads/master@{#371249}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 09f101135900007dec6edd54bc551d83aac140c8

diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index c65eb95..7033cef 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -8,9 +8,9 @@ import unittest
 
 import dag
 import loading_model
-import loading_trace
 import request_track
 import request_dependencies_lens
+import test_utils
 
 
 class SimpleLens(object):
@@ -29,14 +29,6 @@ class SimpleLens(object):
     return deps
 
 
-class MockRequestTrack(object):
-  def __init__(self, requests):
-    self._requests = requests
-
-  def GetEvents(self):
-    return self._requests
-
-
 class LoadingModelTestCase(unittest.TestCase):
 
   def setUp(self):
@@ -51,6 +43,7 @@ class LoadingModelTestCase(unittest.TestCase):
         'receiveHeadersEnd': end_time - start_time,
         'requestTime': start_time / 1000.0}))
     rq = request_track.Request.FromJsonDict({
+        'timestamp': start_time / 1000.0,
         'request_id': self._next_request_id,
         'url': 'http://' + str(url),
         'initiator': 'http://' + str(source_url),
@@ -63,8 +56,8 @@ class LoadingModelTestCase(unittest.TestCase):
     return rq
 
   def MakeGraph(self, requests):
-    return loading_model.ResourceGraph(loading_trace.LoadingTrace(
-        None, None, None, MockRequestTrack(requests), None))
+    return loading_model.ResourceGraph(
+        test_utils.LoadingTraceFromEvents(requests))
 
   def SortedIndicies(self, graph):
     return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 9549d04..4fc5d90 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -8,31 +8,7 @@ import devtools_monitor
 from loading_trace import LoadingTrace
 from request_dependencies_lens import RequestDependencyLens
 from request_track import (Request, TimingFromDict)
-from page_track import PageTrack
-
-
-class FakeTrack(devtools_monitor.Track):
-  def __init__(self, events):
-    super(FakeTrack, self).__init__(None)
-    self._events = events
-
-  def GetEvents(self):
-    return self._events
-
-
-class FakeRequestTrack(devtools_monitor.Track):
-  def __init__(self, events):
-    super(FakeRequestTrack, self).__init__(None)
-    self._events = [self._RewriteEvent(e) for e in events]
-
-  def GetEvents(self):
-    return self._events
-
-  def _RewriteEvent(self, event):
-    # This modifies the instance used across tests, so this method
-    # must be idempotent.
-    event.timing = event.timing._replace(request_time=event.timestamp)
-    return event
+import test_utils
 
 
 class RequestDependencyLensTestCase(unittest.TestCase):
@@ -74,14 +50,12 @@ class RequestDependencyLensTestCase(unittest.TestCase):
                      'stackTrace': [{'url': 'unknown'},
                                     {'url': 'http://bla.com/nyancat.js'}]},
        'timestamp': 10, 'timing': TimingFromDict({})})
-  _PAGE_TRACK = FakeTrack(
-      [{'method': 'Page.frameAttached',
-        'frame_id': '123.13', 'parent_frame_id': '123.1'}])
+  _PAGE_EVENTS = [{'method': 'Page.frameAttached',
+                   'frame_id': '123.13', 'parent_frame_id': '123.1'}]
 
   def testRedirectDependency(self):
-    request_track = FakeRequestTrack([self._REDIRECT_REQUEST, self._REQUEST])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._REDIRECT_REQUEST, self._REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -91,9 +65,8 @@ class RequestDependencyLensTestCase(unittest.TestCase):
     self.assertEquals(self._REQUEST.request_id, second.request_id)
 
   def testScriptDependency(self):
-    request_track = FakeRequestTrack([self._JS_REQUEST, self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._JS_REQUEST, self._JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -102,9 +75,8 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
 
   def testParserDependency(self):
-    request_track = FakeRequestTrack([self._REQUEST, self._JS_REQUEST])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST, self._JS_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -113,11 +85,9 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
 
   def testSeveralDependencies(self):
-    request_track = FakeRequestTrack(
+    loading_trace = test_utils.LoadingTraceFromEvents(
         [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
          self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(3, len(deps))
@@ -133,10 +103,8 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testDependencyDifferentFrame(self):
     """Checks that a more recent request from another frame is ignored."""
-    request_track = FakeRequestTrack(
+    loading_trace = test_utils.LoadingTraceFromEvents(
         [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -147,11 +115,9 @@ class RequestDependencyLensTestCase(unittest.TestCase):
   def testDependencySameParentFrame(self):
     """Checks that a more recent request from an unrelated frame is ignored
     if there is one from a related frame."""
-    request_track = FakeRequestTrack(
+    loading_trace = test_utils.LoadingTraceFromEvents(
         [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
-         self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, self._PAGE_TRACK,
-                                 request_track, None)
+         self._JS_REQUEST_2], self._PAGE_EVENTS)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
diff --git a/loading/test_utils.py b/loading/test_utils.py
new file mode 100644
index 0000000..9051062
--- /dev/null
+++ b/loading/test_utils.py
@@ -0,0 +1,40 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Common utilities used in unit tests, within this directory."""
+
+import devtools_monitor
+import loading_trace
+
+
+class FakeTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeTrack, self).__init__(None)
+    self._events = events
+
+  def GetEvents(self):
+    return self._events
+
+
+class FakeRequestTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeRequestTrack, self).__init__(None)
+    self._events = [self._RewriteEvent(e) for e in events]
+
+  def GetEvents(self):
+    return self._events
+
+  def _RewriteEvent(self, event):
+    # This modifies the instance used across tests, so this method
+    # must be idempotent.
+    event.timing = event.timing._replace(request_time=event.timestamp)
+    return event
+
+
+def LoadingTraceFromEvents(requests, page_events=None):
+  """Returns a LoadingTrace instance from a list of requests and page events."""
+  request_track = FakeRequestTrack(requests)
+  page_track = FakeTrack(page_events if page_events else [])
+  return loading_trace.LoadingTrace(
+      None, None, page_track, request_track, None)

commit 21651a72b641e1a9666f0b4099031ddfe2ac1bd2
Author: eakuefner <eakuefner@chromium.org>
Date:   Fri Jan 22 11:48:33 2016 -0800

    [Telemetry] Update all clients to use chromium_config.GetTelemetryDir()
    
    Telemetry is soon to move to Catapult. The effect will be that Telemetry will
    begin to be located at src/third_party/catapult/telemetry, rather than
    tools/telemetry.
    
    This CL updates all src-side Telemetry clients to use
    chromium_config.GetTelemetryDir(), which is a helper that Telemetry team will
    update when the move is complete.
    
    BUG=576374
    
    Review URL: https://codereview.chromium.org/1582793006
    
    Cr-Original-Commit-Position: refs/heads/master@{#371013}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a5d5d3d6b40905cf3dd3ead516b9c42bf9b94aff

diff --git a/loading/deprecated/log_requests.py b/loading/deprecated/log_requests.py
index ee5f091..12ebb66 100755
--- a/loading/deprecated/log_requests.py
+++ b/loading/deprecated/log_requests.py
@@ -24,7 +24,9 @@ from devil.android import device_utils
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'telemetry'))
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
+from chrome_telemetry_build import chromium_config
+sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index f1aab2e..5279e4f 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -12,7 +12,9 @@ import os
 import sys
 
 file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
+sys.path.append(os.path.join(file_dir, '..', '..', 'perf'))
+from chrome_telemetry_build import chromium_config
+sys.path.append(chromium_config.GetTelemetryDir())
 
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket

commit 6e767d1fb23e3daeea27a8c363a7dfbc9aa71113
Author: mattcary <mattcary@chromium.org>
Date:   Fri Jan 22 08:38:02 2016 -0800

    Fixes request_dependencies_lens to use timing.request_time in favor of timestamp. The request timestamp turns out to be based on when devtools processed the request, and not when the request actually came. This was causing us to incorrectly infer initiator when looking through stack frame.
    
    Also improves loading_model content-type handling.
    
    Review URL: https://codereview.chromium.org/1619373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370981}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0788ff3eb5eb2073705981eb6b0b0d1a6d59ea9d

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 91aedcd..155da1d 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -576,7 +576,7 @@ class ResourceGraph(object):
     if not content_type:
       type_str = 'other'
     elif '/' in content_type:
-      kind, type_str = content_type.split('/')
+      kind, type_str = content_type.split('/', 1)
       if kind in self._CONTENT_KIND_TO_COLOR:
         return self._CONTENT_KIND_TO_COLOR[kind]
     else:
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 81a029a..47ea3af 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -80,12 +80,14 @@ class RequestDependencyLens(object):
         request_track.RequestTrack.REDIRECT_SUFFIX)]
     assert request_id in self._requests_by_id
     dependent_request = self._requests_by_id[request_id]
-    assert request.timestamp < dependent_request.timestamp
+    assert request.timing.request_time < \
+        dependent_request.timing.request_time, '.\n'.join(
+            [str(request), str(dependent_request)])
     return (request, dependent_request, 'redirect')
 
   def _GetInitiatingRequestParser(self, request):
     url = request.initiator['url']
-    candidates = self._FindMatchingRequests(url, request.timestamp)
+    candidates = self._FindMatchingRequests(url, request.timing.request_time)
     if not candidates:
       return None
     initiating_request = self._FindBestMatchingInitiator(request, candidates)
@@ -96,16 +98,23 @@ class RequestDependencyLens(object):
       logging.warning('Script initiator but no stack trace.')
       return None
     initiating_request = None
-    timestamp = request.timestamp
+    timestamp = request.timing.request_time
     for frame in request.initiator['stackTrace']:
       url = frame['url']
       candidates = self._FindMatchingRequests(url, timestamp)
       if candidates:
         initiating_request = self._FindBestMatchingInitiator(
             request, candidates)
-        break
+        if initiating_request:
+          break
     else:
-      logging.warning('Unmatched request')
+      for frame in request.initiator['stackTrace']:
+        if not frame.get('url', None) and frame.get(
+            'functionName', None) == 'window.onload':
+          logging.warning('Unmatched request for onload handler.')
+          break
+      else:
+        logging.warning('Unmatched request.')
       return None
     return (initiating_request, request, 'script')
 
@@ -126,9 +135,9 @@ class RequestDependencyLens(object):
     """
     candidates = self._requests_by_url.get(url, [])
     candidates = [r for r in candidates if (
-        r.timestamp + max(
+        r.timing.request_time + max(
             0, r.timing.receive_headers_end / 1000) <= before_timestamp)]
-    candidates.sort(key=operator.attrgetter('timestamp'))
+    candidates.sort(key=lambda r: r.timing.request_time)
     return candidates
 
   def _FindBestMatchingInitiator(self, request, matches):
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index e5aed37..9549d04 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -20,6 +20,21 @@ class FakeTrack(devtools_monitor.Track):
     return self._events
 
 
+class FakeRequestTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeRequestTrack, self).__init__(None)
+    self._events = [self._RewriteEvent(e) for e in events]
+
+  def GetEvents(self):
+    return self._events
+
+  def _RewriteEvent(self, event):
+    # This modifies the instance used across tests, so this method
+    # must be idempotent.
+    event.timing = event.timing._replace(request_time=event.timestamp)
+    return event
+
+
 class RequestDependencyLensTestCase(unittest.TestCase):
   _REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com', 'request_id': '1234.1.redirect',
@@ -64,7 +79,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         'frame_id': '123.13', 'parent_frame_id': '123.1'}])
 
   def testRedirectDependency(self):
-    request_track = FakeTrack([self._REDIRECT_REQUEST, self._REQUEST])
+    request_track = FakeRequestTrack([self._REDIRECT_REQUEST, self._REQUEST])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
@@ -76,7 +91,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
     self.assertEquals(self._REQUEST.request_id, second.request_id)
 
   def testScriptDependency(self):
-    request_track = FakeTrack([self._JS_REQUEST, self._JS_REQUEST_2])
+    request_track = FakeRequestTrack([self._JS_REQUEST, self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
@@ -87,7 +102,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
 
   def testParserDependency(self):
-    request_track = FakeTrack([self._REQUEST, self._JS_REQUEST])
+    request_track = FakeRequestTrack([self._REQUEST, self._JS_REQUEST])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
@@ -98,7 +113,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
 
   def testSeveralDependencies(self):
-    request_track = FakeTrack(
+    request_track = FakeRequestTrack(
         [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
          self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
@@ -118,7 +133,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testDependencyDifferentFrame(self):
     """Checks that a more recent request from another frame is ignored."""
-    request_track = FakeTrack(
+    request_track = FakeRequestTrack(
         [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
@@ -132,7 +147,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
   def testDependencySameParentFrame(self):
     """Checks that a more recent request from an unrelated frame is ignored
     if there is one from a related frame."""
-    request_track = FakeTrack(
+    request_track = FakeRequestTrack(
         [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
          self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, self._PAGE_TRACK,
diff --git a/loading/request_track.py b/loading/request_track.py
index 5e73733..60a54e5 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -166,6 +166,9 @@ class Request(object):
   def __hash__(self):
     return hash(self.request_id)
 
+  def __str__(self):
+    return json.dumps(self.ToJsonDict(), sort_keys=True, indent=2)
+
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""

commit b56bbf98ac6058e2049a6a1efab1db729e528927
Author: blundell <blundell@chromium.org>
Date:   Fri Jan 22 07:08:05 2016 -0800

    [loading] Fix loading_model_unittest
    
    The test request_tracks dict didn't have the expected structure.
    
    Review URL: https://codereview.chromium.org/1614333003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370966}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b9d3191c645c5d349ff5cbe8267f9bd37b912997

diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index bb6495f..c65eb95 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -78,7 +78,8 @@ class LoadingModelTestCase(unittest.TestCase):
             'events': [self.MakeParserRequest(0, 'null', 100, 101).ToJsonDict(),
                        self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
                        self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
-                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()]},
+                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()],
+            'metadata': { 'duplicates_count' : 0 }},
          'url': 'foo.com',
          'tracing_track': {'events': []},
          'page_track': {'events': []},

commit 604f0bf5d379909a6af2ee76540aade24d884ed9
Author: blundell <blundell@chromium.org>
Date:   Fri Jan 22 06:36:55 2016 -0800

    [loading] Towards more useful help messages in analyze.py
    
    This CL changes the ArgParsers in analyze.py to be given 'description' instead
    of 'usage'. This way, they auto-generate usage messages, which are guaranteed
    to be consistent with the current arguments and options that they contain.
    
    Review URL: https://codereview.chromium.org/1620283002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370962}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2679052c78ff06908573c47b9f7021c9350407e0

diff --git a/loading/analyze.py b/loading/analyze.py
index cb2e837..27294d3 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -159,7 +159,7 @@ def InvalidCommand(cmd):
 
 
 def DoCost(arg_str):
-  parser = argparse.ArgumentParser(usage='cost [--parameter ...] REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Tabulates cost')
   parser.add_argument('request_json')
   parser.add_argument('--parameter', nargs='*', default=[])
   parser.add_argument('--path', action='store_true')
@@ -179,7 +179,7 @@ def DoCost(arg_str):
 
 def DoPng(arg_str):
   parser = argparse.ArgumentParser(
-      usage='png [--eog] [--highlight X[,...] REQUEST_JSON [PNG_OUTPUT]')
+      description='Generates a PNG from a trace')
   parser.add_argument('request_json')
   parser.add_argument('png_output', nargs='?')
   parser.add_argument('--eog', action='store_true')
@@ -208,7 +208,7 @@ def DoPng(arg_str):
 
 
 def DoCompare(arg_str):
-  parser = argparse.ArgumentParser(usage='compare REQUEST_JSON REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Compares two traces')
   parser.add_argument('g1_json')
   parser.add_argument('g2_json')
   args = parser.parse_args(arg_str)
@@ -223,8 +223,7 @@ def DoCompare(arg_str):
 
 
 def DoPrefetchSetup(arg_str):
-  parser = argparse.ArgumentParser(
-      usage='prefetch_setup [--upload] REQUEST_JSON TARGET_HTML')
+  parser = argparse.ArgumentParser(description='Sets up prefetch')
   parser.add_argument('request_json')
   parser.add_argument('target_html')
   parser.add_argument('--upload', action='store_true')
@@ -244,8 +243,7 @@ def DoPrefetchSetup(arg_str):
 
 
 def DoLogRequests(arg_str):
-  parser = argparse.ArgumentParser(
-      usage='log_requests [--prefetch] --site URL --output JSON_OUTPUT')
+  parser = argparse.ArgumentParser(description='Logs requests of a load')
   parser.add_argument('--url', required=True)
   parser.add_argument('--output', required=True)
   parser.add_argument('--prefetch', action='store_true')
@@ -260,13 +258,12 @@ def DoLogRequests(arg_str):
 
 
 def DoFetch(arg_str):
-  parser = argparse.ArgumentParser(usage='fetch --site SITE --dir DIR\n'
-                                   'Fetches SITE into DIR with standard naming '
-                                   'that can be processed by ./cost_to_csv.py. '
-                                   'Both warm and cold fetches are done. '
-                                   'SITE can be a full url but the filename '
-                                   'may be strange so better to just use a '
-                                   'site (ie, domain).')
+  parser = argparse.ArgumentParser(description='Fetches SITE into DIR with '
+                                   'standard naming that can be processed by '
+                                   './cost_to_csv.py.  Both warm and cold '
+                                   'fetches are done.  SITE can be a full url '
+                                   'but the filename may be strange so better '
+                                   'to just use a site (ie, domain).')
   # Arguments are flags as it's easy to get the wrong order of site vs dir.
   parser.add_argument('--site', required=True)
   parser.add_argument('--dir', required=True)
@@ -282,7 +279,7 @@ def DoFetch(arg_str):
 
 
 def DoLongPole(arg_str):
-  parser = argparse.ArgumentParser(usage='longpole [--noads] REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Calculates long pole')
   parser.add_argument('request_json')
   parser.add_argument('--noads', action='store_true')
   args = parser.parse_args(arg_str)
@@ -295,7 +292,7 @@ def DoLongPole(arg_str):
 
 
 def DoNodeCost(arg_str):
-  parser = argparse.ArgumentParser(usage='nodecost [--noads] REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Calculates node cost')
   parser.add_argument('request_json')
   parser.add_argument('--noads', action='store_true')
   args = parser.parse_args(arg_str)
@@ -318,8 +315,8 @@ COMMAND_MAP = {
 
 def main():
   logging.basicConfig(level=logging.WARNING)
-  parser = argparse.ArgumentParser(usage=' '.join(COMMAND_MAP.keys()))
-  parser.add_argument('command')
+  parser = argparse.ArgumentParser(description='Analyzes loading')
+  parser.add_argument('command', help=' '.join(COMMAND_MAP.keys()))
   parser.add_argument('rest', nargs=argparse.REMAINDER)
   args = parser.parse_args()
   devil_chromium.Initialize()

commit f125fab8afc77554cf12dd555aa82dbafe76457f
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jan 22 02:03:36 2016 -0800

    tools/android/loading: Cope with duplicated messages.
    
    TEST=record a trace on mobile for nytimes.com.
    
    Review URL: https://codereview.chromium.org/1619723003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370934}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 489a998950c75693eaa5bd96eacd7e034fab3fec

diff --git a/loading/request_track.py b/loading/request_track.py
index e83dc30..5e73733 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -176,6 +176,10 @@ class RequestTrack(devtools_monitor.Track):
   _STATUS_DATA = 2
   _STATUS_FINISHED = 3
   _STATUS_FAILED = 4
+  # Serialization KEYS
+  _EVENTS_KEY = 'events'
+  _METADATA_KEY = 'metadata'
+  _DUPLICATES_KEY = 'duplicates_count'
   def __init__(self, connection):
     super(RequestTrack, self).__init__(connection)
     self._connection = connection
@@ -185,6 +189,10 @@ class RequestTrack(devtools_monitor.Track):
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
+    # responseReceived message are sometimes duplicated. Records the message to
+    # detect this.
+    self._request_id_to_response_received = {}
+    self.duplicates_count = 0
 
   def Handle(self, method, msg):
     assert method in RequestTrack._METHOD_TO_HANDLER
@@ -201,15 +209,19 @@ class RequestTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     if self._requests_in_flight:
       logging.warning('Requests in flight, will be ignored in the dump')
-    return {'events': [request.ToJsonDict() for request in self._requests]}
+    return {self._EVENTS_KEY: [
+        request.ToJsonDict() for request in self._requests],
+            self._METADATA_KEY: {self._DUPLICATES_KEY: self.duplicates_count}}
 
   @classmethod
   def FromJsonDict(cls, json_dict):
-    assert 'events' in json_dict
+    assert cls._EVENTS_KEY in json_dict
+    assert cls._METADATA_KEY in json_dict
     result = RequestTrack(None)
     requests = [Request.FromJsonDict(request)
-                for request in json_dict['events']]
+                for request in json_dict[cls._EVENTS_KEY]]
     result._requests = requests
+    result.duplicates_count = json_dict[cls._METADATA_KEY][cls._DUPLICATES_KEY]
     return result
 
   def _RequestWillBeSent(self, request_id, params):
@@ -260,8 +272,16 @@ class RequestTrack(devtools_monitor.Track):
   def _ResponseReceived(self, request_id, params):
     assert request_id in self._requests_in_flight
     (r, status) = self._requests_in_flight[request_id]
+    if status == RequestTrack._STATUS_RESPONSE:
+      # Duplicated messages (apart from the timestamp) are OK.
+      old_params = self._request_id_to_response_received[request_id]
+      params_copy = copy.deepcopy(params)
+      params_copy['timestamp'] = None
+      old_params['timestamp'] = None
+      assert params_copy == old_params
+      self.duplicates_count += 1
+      return
     assert status == RequestTrack._STATUS_SENT
-    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
     assert r.frame_id == params['frameId']
     assert r.timestamp <= params['timestamp']
     if r.resource_type == 'Other':
@@ -286,6 +306,7 @@ class RequestTrack(devtools_monitor.Track):
       timing_dict = {'requestTime': r.timestamp}
     r.timing = TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
+    self._request_id_to_response_received[request_id] = params
 
   def _DataReceived(self, request_id, params):
     (r, status) = self._requests_in_flight[request_id]
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 97ea57f..fde7399 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import copy
 import json
 import unittest
 
@@ -257,6 +258,21 @@ class RequestTrackTestCase(unittest.TestCase):
         RequestTrackTestCase._DATA_RECEIVED_2['params']['encodedDataLength'],
         r.data_chunks[1][1])
 
+  def testDuplicatedResponseReceived(self):
+    msg1 = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    msg2 = copy.deepcopy(RequestTrackTestCase._RESPONSE_RECEIVED)
+    msg2_other_timestamp = copy.deepcopy(msg2)
+    msg2_other_timestamp['params']['timestamp'] += 12
+    msg2_different = copy.deepcopy(msg2)
+    msg2_different['params']['response']['encodedDataLength'] += 1
+    self.request_track.Handle('Network.requestWillBeSent', msg1)
+    self.request_track.Handle('Network.responseReceived', msg2)
+    # Should not raise an AssertionError.
+    self.request_track.Handle('Network.responseReceived', msg2)
+    self.assertEquals(1, self.request_track.duplicates_count)
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle('Network.responseReceived', msg2_different)
+
   def testCanSerialize(self):
     self._ValidSequence(self.request_track)
     json_dict = self.request_track.ToJsonDict()
@@ -264,6 +280,7 @@ class RequestTrackTestCase(unittest.TestCase):
 
   def testCanDeserialize(self):
     self._ValidSequence(self.request_track)
+    self.request_track.duplicates_count = 142
     json_dict = self.request_track.ToJsonDict()
     request_track = RequestTrack.FromJsonDict(json_dict)
     self.assertEquals(self.request_track, request_track)

commit f45d8aa2bec8ed78340507121704e863c496d06d
Author: mattcary <mattcary@chromium.org>
Date:   Thu Jan 21 08:55:20 2016 -0800

    Upgrade analyze.py and related scripts to new world order.
    
    Changes analyze.py to use loading_trace methods. Moved old files to
    deprecated/, I think we're all done with them but we may need them for
    reference.
    
    Updated loading_model graph drawing to account for new content-types;
    what I've done seems sub-optimal & suggestions are welcome.
    
    Fixed at least one minor bug (the request end units problem Benoit
    found).
    
    Review URL: https://codereview.chromium.org/1619713002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370715}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 76303eb216a17c2f93f19593f8ba8b81472c29e4

diff --git a/loading/analyze.py b/loading/analyze.py
index 3bb5890..cb2e837 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -24,31 +24,21 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 from pylib import constants
 
-import log_parser
-import log_requests
+import device_setup
 import loading_model
+import loading_trace
+import trace_recorder
 
 
-# TODO(mattcary): logging.info isn't that useful; we need something finer
-# grained. For now we just do logging.warning.
+# TODO(mattcary): logging.info isn't that useful, as the whole (tools) world
+# uses logging info; we need to introduce logging modules to get finer-grained
+# output. For now we just do logging.warning.
 
 
 # TODO(mattcary): probably we want this piped in through a flag.
 CHROME = constants.PACKAGE_INFO['chrome']
 
 
-def _SetupAndGetDevice():
-  """Gets an android device, set up the way we like it.
-
-  Returns:
-    An instance of DeviceUtils for the first device found.
-  """
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  device.EnableRoot()
-  device.KillAll(CHROME.package, quiet=True)
-  return device
-
-
 def _LoadPage(device, url):
   """Load a page on chrome on our device.
 
@@ -99,31 +89,24 @@ def _GetPrefetchHtml(graph, name=None):
 <body>%s</body>
 </html>
   """ % title)
-
   return '\n'.join(output)
 
 
 def _LogRequests(url, clear_cache=True, local=False):
   """Log requests for a web page.
 
-  TODO(mattcary): loading.log_requests probably needs to be refactored as we're
-  using private methods, also there's ugliness like _ResponseDataToJson return a
-  json.dumps that we immediately json.loads.
-
   Args:
     url: url to log as string.
     clear_cache: optional flag to clear the cache.
     local: log from local (desktop) chrome session.
 
   Returns:
-    JSON of logged information (ie, a dict that describes JSON).
+    JSON dict of logged information (ie, a dict that describes JSON).
   """
-  device = _SetupAndGetDevice() if not local else None
-  request_logger = log_requests.AndroidRequestsLogger(device)
-  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
-  response_data = request_logger.LogPageLoad(
-      url, clear_cache, 'chrome')
-  return json.loads(log_requests._ResponseDataToJson(response_data))
+  device = device_setup.GetFirstDevice() if not local else None
+  with device_setup.DeviceConnection(device) as connection:
+    trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
+    return trace.ToJsonDict()
 
 
 def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
@@ -136,13 +119,14 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
   if prefetch:
     assert not local
     logging.warning('Generating prefetch')
-    prefetch_html = _GetPrefetchHtml(_ProcessJson(cold_data), name=url)
+    prefetch_html = _GetPrefetchHtml(
+        loading_model.ResourceGraph(cold_data), name=url)
     tmp = tempfile.NamedTemporaryFile()
     tmp.write(prefetch_html)
     tmp.flush()
     # We hope that the tmpfile name is unique enough for the device.
     target = os.path.join('/sdcard/Download', os.path.basename(tmp.name))
-    device = _SetupAndGetDevice()
+    device = device_setup.GetFirstDevice()
     device.adb.Push(tmp.name, target)
     logging.warning('Pushed prefetch %s to device at %s' % (tmp.name, target))
     _LoadPage(device, 'file://' + target)
@@ -164,14 +148,9 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
 # TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
 # with here.
 def _ProcessRequests(filename):
-  requests = log_parser.FilterRequests(log_parser.ParseJsonFile(filename))
-  return loading_model.ResourceGraph(requests)
-
-
-def _ProcessJson(json_data):
-  assert json_data
-  return loading_model.ResourceGraph(log_parser.FilterRequests(
-      [log_parser.RequestData.FromDict(r) for r in json_data]))
+  with open(filename) as f:
+    return loading_model.ResourceGraph(
+        loading_trace.LoadingTrace.FromJsonDict(json.load(f)))
 
 
 def InvalidCommand(cmd):
@@ -255,7 +234,7 @@ def DoPrefetchSetup(arg_str):
     html.write(_GetPrefetchHtml(
         graph, name=os.path.basename(args.request_json)))
   if args.upload:
-    device = _SetupAndGetDevice()
+    device = device_setup.GetFirstDevice()
     destination = os.path.join('/sdcard/Download',
                                os.path.basename(args.target_html))
     device.adb.Push(args.target_html, destination)
@@ -302,20 +281,6 @@ def DoFetch(arg_str):
              local=False)
 
 
-def DoTracing(arg_str):
-  parser = argparse.ArgumentParser(
-      usage='tracing URL JSON_OUTPUT')
-  parser.add_argument('url')
-  parser.add_argument('json_output')
-  args = parser.parse_args(arg_str)
-  device = _SetupAndGetDevice()
-  request_logger = log_requests.AndroidRequestsLogger(device)
-  tracing = request_logger.LogTracing(args.url)
-  with open(args.json_output, 'w') as f:
-    _WriteJson(f, tracing)
-  logging.warning('Wrote ' + args.json_output)
-
-
 def DoLongPole(arg_str):
   parser = argparse.ArgumentParser(usage='longpole [--noads] REQUEST_JSON')
   parser.add_argument('request_json')
@@ -346,7 +311,6 @@ COMMAND_MAP = {
     'compare': DoCompare,
     'prefetch_setup': DoPrefetchSetup,
     'log_requests': DoLogRequests,
-    'tracing': DoTracing,
     'longpole': DoLongPole,
     'nodecost': DoNodeCost,
     'fetch': DoFetch,
diff --git a/loading/log_parser.py b/loading/deprecated/log_parser.py
similarity index 100%
rename from loading/log_parser.py
rename to loading/deprecated/log_parser.py
diff --git a/loading/log_requests.py b/loading/deprecated/log_requests.py
similarity index 100%
rename from loading/log_requests.py
rename to loading/deprecated/log_requests.py
diff --git a/loading/process_request_log.py b/loading/deprecated/process_request_log.py
similarity index 100%
rename from loading/process_request_log.py
rename to loading/deprecated/process_request_log.py
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 4d5d40b..8784024 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import contextlib
+import logging
 import os
 import sys
 import time
@@ -11,6 +12,7 @@ _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
@@ -23,6 +25,25 @@ DEVTOOLS_PORT = 9222
 DEVTOOLS_HOSTNAME = 'localhost'
 DEFAULT_CHROME_PACKAGE = 'chrome'
 
+
+class DeviceSetupException(Exception):
+  def __init__(self, msg):
+    super(DeviceSetupException, self).__init__(msg)
+    logging.error(msg)
+
+
+def GetFirstDevice():
+  """Returns the first connected device.
+
+  Raises:
+    DeviceSetupException if there is no such device.
+  """
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  if not devices:
+    raise DeviceSetupException('No devices found')
+  return devices[0]
+
+
 @contextlib.contextmanager
 def FlagReplacer(device, command_line_path, new_flags):
   """Replaces chrome flags in a context, restores them afterwards.
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 5e8470b..f1aab2e 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -168,6 +168,15 @@ class DevToolsConnection(object):
       raise DevToolsConnectionException(
           'Unexpected response for %s: %s' % (method, result))
 
+  def ClearCache(self):
+    """Clears buffer cache.
+
+    Will assert that the browser supports cache clearing.
+    """
+    res = self.SyncRequest('Network.canClearBrowserCache')
+    assert res['result'], 'Cache clearing is not supported by this browser.'
+    self.SyncRequest('Network.clearBrowserCache')
+
   def SetUpMonitoring(self):
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
diff --git a/loading/loading_model.py b/loading/loading_model.py
index 7ba75fc..91aedcd 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -21,6 +21,7 @@ import urlparse
 import sys
 
 import dag
+import loading_trace
 import request_dependencies_lens
 
 class ResourceGraph(object):
@@ -30,11 +31,13 @@ class ResourceGraph(object):
     cache_all: if true, assume zero loading time for all resources.
   """
   def __init__(self, trace):
-    """Create from a LoadingTrace.
+    """Create from a LoadingTrace (or json of a trace).
 
     Args:
-      trace: (LoadingTrace) Loading trace.
+      trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
     """
+    if type(trace) == dict:
+      trace = loading_trace.LoadingTrace.FromJsonDict(trace)
     self._BuildDag(trace)
     self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
@@ -290,9 +293,27 @@ class ResourceGraph(object):
   ## Internal items
   ##
 
-  _CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
-                            'json': 'purple', 'gif_image': 'grey',
-                            'image': 'orange', 'other': 'white'}
+  _CONTENT_KIND_TO_COLOR = {
+      'application':     'blue',      # Scripts.
+      'font':            'grey70',
+      'image':           'orange',    # This probably catches gifs?
+      'video':           'hotpink1',
+      }
+
+  _CONTENT_TYPE_TO_COLOR = {
+      'html':            'red',
+      'css':             'green',
+      'script':          'blue',
+      'javascript':      'blue',
+      'json':            'purple',
+      'gif':             'grey',
+      'image':           'orange',
+      'jpeg':            'orange',
+      'png':             'orange',
+      'plain':           'brown3',
+      'octet-stream':    'brown3',
+      'other':           'white',
+      }
 
   # This resource type may induce a timing dependency. See _SplitChildrenByTime
   # for details.
@@ -323,8 +344,9 @@ class ResourceGraph(object):
       self._edge_annotations = {}
       # All fields in timing are millis relative to request_time, which is epoch
       # seconds.
-      self._node_cost = max([t for f, t in request.timing._asdict().iteritems()
-                             if f != 'request_time'])
+      self._node_cost = max(
+          [0] + [t for f, t in request.timing._asdict().iteritems()
+                 if f != 'request_time'])
 
     def __str__(self):
       return self.ShortName()
@@ -363,19 +385,20 @@ class ResourceGraph(object):
       """
       parsed = urlparse.urlparse(self._request.url)
       path = parsed.path
+      hostname = parsed.hostname if parsed.hostname else '?.?.?'
       if path != '' and path != '/':
         last_path = parsed.path.split('/')[-1]
         if len(last_path) < 10:
           if len(path) < 10:
-            return parsed.hostname + '/' + path
+            return hostname + '/' + path
           else:
-            return parsed.hostname + '/..' + parsed.path[-10:]
+            return hostname + '/..' + parsed.path[-10:]
         elif len(last_path) > 10:
-          return parsed.hostname + '/..' + last_path[:5]
+          return hostname + '/..' + last_path[:5]
         else:
-          return parsed.hostname + '/..' + last_path
+          return hostname + '/..' + last_path
       else:
-        return parsed.hostname
+        return hostname
 
     def Url(self):
       return self._request.url
@@ -463,7 +486,7 @@ class ResourceGraph(object):
 
     dependencies = request_dependencies_lens.RequestDependencyLens(
         trace).GetRequestDependencies()
-    for child_rq, parent_rq, reason in dependencies:
+    for parent_rq, child_rq, reason in dependencies:
       parent = self._node_info[index_by_request[parent_rq]]
       child = self._node_info[index_by_request[child_rq]]
       edge_cost = child.StartTime() - parent.EndTime()
@@ -549,6 +572,17 @@ class ResourceGraph(object):
         current.ReparentTo(parent, children_by_end_time[end_mark])
         children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
 
+  def _ContentTypeToColor(self, content_type):
+    if not content_type:
+      type_str = 'other'
+    elif '/' in content_type:
+      kind, type_str = content_type.split('/')
+      if kind in self._CONTENT_KIND_TO_COLOR:
+        return self._CONTENT_KIND_TO_COLOR[kind]
+    else:
+      type_str = content_type
+    return self._CONTENT_TYPE_TO_COLOR[type_str]
+
   def _GraphvizNode(self, index, highlight):
     """Returns a graphviz node description for a given node.
 
@@ -563,7 +597,7 @@ class ResourceGraph(object):
       is oval if its max-age is less than 300s (or if it's not cacheable).
     """
     node_info = self._node_info[index]
-    color = self._CONTENT_TYPE_TO_COLOR[node_info.ContentType()]
+    color = self._ContentTypeToColor(node_info.ContentType())
     max_age = node_info.Request().MaxAge()
     shape = 'polygon' if max_age > 300 else 'oval'
     styles = ['filled']
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index d76f5ac..bb6495f 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -25,7 +25,7 @@ class SimpleLens(object):
       url_to_rq[rq.url] = rq
     for rq in self._trace.request_track.GetEvents():
       if rq.initiator in url_to_rq:
-        deps.append((rq, url_to_rq[rq.initiator], ''))
+        deps.append(( url_to_rq[rq.initiator], rq, ''))
     return deps
 
 
@@ -45,6 +45,11 @@ class LoadingModelTestCase(unittest.TestCase):
 
   def MakeParserRequest(self, url, source_url, start_time, end_time,
                         magic_content_type=False):
+    timing = request_track.TimingAsList(request_track.TimingFromDict({
+        # connectEnd should be ignored.
+        'connectEnd': (end_time - start_time) / 2,
+        'receiveHeadersEnd': end_time - start_time,
+        'requestTime': start_time / 1000.0}))
     rq = request_track.Request.FromJsonDict({
         'request_id': self._next_request_id,
         'url': 'http://' + str(url),
@@ -52,11 +57,7 @@ class LoadingModelTestCase(unittest.TestCase):
         'response_headers': {'Content-Type':
                              'null' if not magic_content_type
                              else 'magic-debug-content' },
-        'timing': request_track.TimingFromDict({
-            # connectEnd should be ignored.
-            'connectEnd': (end_time - start_time) / 2,
-            'receiveHeadersEnd': end_time - start_time,
-            'requestTime': start_time / 1000.0})
+        'timing': timing
         })
     self._next_request_id += 1
     return rq
@@ -71,6 +72,22 @@ class LoadingModelTestCase(unittest.TestCase):
   def SuccessorIndicies(self, node):
     return [c.Index() for c in node.SortedSuccessors()]
 
+  def test_DictConstruction(self):
+    graph = loading_model.ResourceGraph(
+        {'request_track': {
+            'events': [self.MakeParserRequest(0, 'null', 100, 101).ToJsonDict(),
+                       self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
+                       self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
+                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()]},
+         'url': 'foo.com',
+         'tracing_track': {'events': []},
+         'page_track': {'events': []},
+         'metadata': {}})
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
+
   def test_Costing(self):
     requests = [self.MakeParserRequest(0, 'null', 100, 110),
                 self.MakeParserRequest(1, 0, 115, 120),
diff --git a/loading/processing.py b/loading/processing.py
index 1f48939..7c00951 100644
--- a/loading/processing.py
+++ b/loading/processing.py
@@ -2,12 +2,13 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import json
 import os
 import os.path
 import sys
 
-import log_parser
 import loading_model
+import loading_trace
 
 
 def SitesFromDir(directory):
@@ -52,8 +53,9 @@ def WarmGraph(datadir, site):
   Returns:
     A loading model object.
   """
-  return loading_model.ResourceGraph(log_parser.FilterRequests(
-      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json'))))
+  with file(os.path.join(datadir, site + '.json')) as f:
+    return loading_model.ResourceGraph(loading_trace.LoadingTrace.FromJsonDict(
+        json.load(f)))
 
 
 def ColdGraph(datadir, site):
@@ -68,5 +70,6 @@ def ColdGraph(datadir, site):
   Returns:
     A loading model object.
   """
-  return loading_model.ResourceGraph(log_parser.FilterRequests(
-      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json.cold'))))
+  with file(os.path.join(datadir, site + '.json.cold')) as f:
+    return loading_model.ResourceGraph(loading_trace.LoadingTrace.FromJsonDict(
+        json.load(f)))
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 9aacd81..81a029a 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -126,7 +126,8 @@ class RequestDependencyLens(object):
     """
     candidates = self._requests_by_url.get(url, [])
     candidates = [r for r in candidates if (
-        r.timestamp + max(0, r.timing.receive_headers_end) <= before_timestamp)]
+        r.timestamp + max(
+            0, r.timing.receive_headers_end / 1000) <= before_timestamp)]
     candidates.sort(key=operator.attrgetter('timestamp'))
     return candidates
 
@@ -157,7 +158,8 @@ class RequestDependencyLens(object):
       parent_frame_id = self._frame_to_parent[request.frame_id]
       same_parent_matches = [
           r for r in matches
-          if self._frame_to_parent[r.frame_id] == parent_frame_id]
+          if r.frame_id in self._frame_to_parent and
+          self._frame_to_parent[r.frame_id] == parent_frame_id]
       if not same_parent_matches:
         logging.warning('All matches are from non-sibling frames.')
         return matches[-1]
diff --git a/loading/request_track.py b/loading/request_track.py
index e7338ab..e83dc30 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -9,7 +9,9 @@ When executed, parses a JSON dump of DevTools messages.
 
 import collections
 import copy
+import json
 import logging
+import re
 
 import devtools_monitor
 
@@ -25,6 +27,17 @@ _TIMING_NAMES_MAPPING = {
 
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
+def TimingAsList(timing):
+  """Transform Timing to a list, eg as is used in JSON output.
+
+  Args:
+    timing: a Timing.
+
+  Returns:
+    A list identical to what the eventual JSON output will be (eg,
+    Request.ToJsonDict).
+  """
+  return json.loads(json.dumps(timing))
 
 class Request(object):
   """Represents a single request.
@@ -102,8 +115,12 @@ class Request(object):
     result = Request()
     for (k, v) in data_dict.items():
       setattr(result, k, v)
+    if not result.response_headers:
+      result.response_headers = {}
     if result.timing:
       result.timing = Timing(*result.timing)
+    else:
+      result.timing = TimingFromDict({'requestTime': result.timestamp})
     return result
 
   def GetContentType(self):
@@ -137,7 +154,10 @@ class Request(object):
         or len(cache_control) == 0):
       return -1
     if 'max-age' in cache_control:
-      return int(cache_control['max-age'])
+      age_match = re.match(r'\s*(\d+)+', cache_control['max-age'])
+      if not age_match:
+        return -1
+      return int(age_match.group(1))
     return -1
 
   def __eq__(self, o):
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index fdf6cc6..61015d2 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -30,20 +30,34 @@ import request_track
 import tracing
 
 
+def MonitorUrl(connection, url, clear_cache=False):
+  """Monitor a URL via a trace recorder.
+
+  Args:
+    connection: A device_monitor.DevToolsConnection instance.
+    url: url to navigate to as string.
+    clear_cache: boolean indicating if cache should be cleared before loading.
+
+  Returns:
+    loading_trace.LoadingTrace.
+  """
+  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
+  page = page_track.PageTrack(connection)
+  request = request_track.RequestTrack(connection)
+  trace = tracing.TracingTrack(connection)
+  connection.SetUpMonitoring()
+  if clear_cache:
+    connection.ClearCache()
+  connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+  connection.StartMonitoring()
+  metadata = {'date': datetime.datetime.utcnow().isoformat(),
+              'seconds_since_epoch': time.time()}
+  return loading_trace.LoadingTrace(url, metadata, page, request, trace)
+
 def RecordAndDumpTrace(device, url, output_filename):
   with file(output_filename, 'w') as output,\
         device_setup.DeviceConnection(device) as connection:
-    page = page_track.PageTrack(connection)
-    request = request_track.RequestTrack(connection)
-    tracing_track = tracing.TracingTrack(connection)
-    connection.SetUpMonitoring()
-    connection.SendAndIgnoreResponse('Network.clearBrowserCache', {})
-    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-    connection.StartMonitoring()
-    metadata = {'date': datetime.datetime.utcnow().isoformat(),
-                'seconds_since_epoch': time.time()}
-    trace = loading_trace.LoadingTrace(url, metadata, page, request,
-                                       tracing_track)
+    trace = MonitorUrl(connection, url)
     json.dump(trace.ToJsonDict(), output)
 
 

commit 49ef39909ac5a58a6df65fcb1d956ac762c08964
Author: mattcary <mattcary@chromium.org>
Date:   Thu Jan 21 04:18:18 2016 -0800

    Upgrade loading_model to use the new request_track and loading_trace.
    
    Review URL: https://codereview.chromium.org/1610273002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370673}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6a14e6fe7c6bc7a0b355c2a3812179fa5cde4534

diff --git a/loading/loading_model.py b/loading/loading_model.py
index fed48e4..7ba75fc 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -6,11 +6,11 @@
 
 (Redirect the following to the general model module once we have one)
 A model is an object with the following methods.
-  CostMs(): return the cost of the cost in milliseconds.
-  Set(): set model-specifical parameters.
+  CostMs(): return the cost of the model in milliseconds.
+  Set(): set model-specific parameters.
 
 ResourceGraph
-  This creates a DAG of resource dependancies from loading.log_requests to model
+  This creates a DAG of resource dependencies from loading.log_requests to model
   loading time. The model may be parameterized by changing the loading time of
   a particular or all resources.
 """
@@ -21,7 +21,7 @@ import urlparse
 import sys
 
 import dag
-import log_parser
+import request_dependencies_lens
 
 class ResourceGraph(object):
   """A model of loading by a DAG (tree?) of resource dependancies.
@@ -29,14 +29,13 @@ class ResourceGraph(object):
   Set parameters:
     cache_all: if true, assume zero loading time for all resources.
   """
-
-  def __init__(self, requests):
-    """Create from a parsed request set.
+  def __init__(self, trace):
+    """Create from a LoadingTrace.
 
     Args:
-      requests: [RequestData, ...] filtered RequestData from loading.log_parser.
+      trace: (LoadingTrace) Loading trace.
     """
-    self._BuildDag(requests)
+    self._BuildDag(trace)
     self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
     # reparented child is actually a dependency for a child of its new parent.
@@ -182,7 +181,7 @@ class ResourceGraph(object):
       while n.Predecessors():
         n = reduce(lambda costliest, next:
                    next if (self._node_filter(next) and
-                            cost[next.Index()] > cost[costliest.Index()])
+                            costs[next.Index()] > costs[costliest.Index()])
                         else costliest,
                    n.Predecessors())
         path_list.insert(0, self._node_info[n.Index()])
@@ -322,10 +321,10 @@ class ResourceGraph(object):
       self._node = node
       self._edge_costs = {}
       self._edge_annotations = {}
-      # All fields in timing are millis relative to requestTime, which is epoch
+      # All fields in timing are millis relative to request_time, which is epoch
       # seconds.
       self._node_cost = max([t for f, t in request.timing._asdict().iteritems()
-                             if f != 'requestTime'])
+                             if f != 'request_time'])
 
     def __str__(self):
       return self.ShortName()
@@ -346,20 +345,37 @@ class ResourceGraph(object):
       return self._edge_costs[s]
 
     def StartTime(self):
-      return self._request.timing.requestTime * 1000
+      return self._request.timing.request_time * 1000
 
     def EndTime(self):
-      return self._request.timing.requestTime * 1000 + self._node_cost
+      return self._request.timing.request_time * 1000 + self._node_cost
 
     def EdgeAnnotation(self, s):
       assert s.Node() in self.Node().Successors()
       return self._edge_annotations.get(s, [])
 
     def ContentType(self):
-      return log_parser.Resource.FromRequest(self._request).GetContentType()
+      return self._request.GetContentType()
 
     def ShortName(self):
-      return log_parser.Resource.FromRequest(self._request).GetShortName()
+      """Returns either the hostname of the resource, or the filename,
+      or the end of the path. Tries to include the domain as much as possible.
+      """
+      parsed = urlparse.urlparse(self._request.url)
+      path = parsed.path
+      if path != '' and path != '/':
+        last_path = parsed.path.split('/')[-1]
+        if len(last_path) < 10:
+          if len(path) < 10:
+            return parsed.hostname + '/' + path
+          else:
+            return parsed.hostname + '/..' + parsed.path[-10:]
+        elif len(last_path) > 10:
+          return parsed.hostname + '/..' + last_path[:5]
+        else:
+          return parsed.hostname + '/..' + last_path
+      else:
+        return parsed.hostname
 
     def Url(self):
       return self._request.url
@@ -422,7 +438,7 @@ class ResourceGraph(object):
     return self._node_info[parent.Index()].EdgeAnnotation(
         self._node_info[child.Index()])
 
-  def _BuildDag(self, requests):
+  def _BuildDag(self, trace):
     """Build DAG of resources.
 
     Build a DAG from our requests and augment with _NodeInfo (see above) in a
@@ -431,112 +447,36 @@ class ResourceGraph(object):
     Creates self._nodes and self._node_info.
 
     Args:
-      requests: [Request, ...] Requests from loading.log_parser.
+      trace: A LoadingTrace.
     """
     self._nodes = []
     self._node_info = []
-    indicies_by_url = {}
-    requests_by_completion = log_parser.SortedByCompletion(requests)
-    for request in requests:
+    index_by_request = {}
+    for request in trace.request_track.GetEvents():
       next_index = len(self._nodes)
-      indicies_by_url.setdefault(request.url, []).append(next_index)
+      assert request not in index_by_request
+      index_by_request[request] = next_index
       node = dag.Node(next_index)
       node_info = self._NodeInfo(node, request)
       self._nodes.append(node)
       self._node_info.append(node_info)
-    for url, indicies in indicies_by_url.iteritems():
-      if len(indicies) > 1:
-        logging.warning('Multiple loads (%d) for url: %s' %
-                        (len(indicies), url))
-    for i in xrange(len(requests)):
-      request = requests[i]
-      current_node_info = self._node_info[i]
-      resource = log_parser.Resource.FromRequest(current_node_info.Request())
-      initiator = request.initiator
-      initiator_type = initiator['type']
-      predecessor_url = None
-      predecessor_type = None
-      # Classify & infer the predecessor. If a candidate url we identify as the
-      # predecessor is not in index_by_url, then we haven't seen it in our
-      # requests and we will try to find a better predecessor.
-      if initiator_type == 'parser':
-        url = initiator['url']
-        if url in indicies_by_url:
-          predecessor_url = url
-          predecessor_type = 'parser'
-      elif initiator_type == 'script' and 'stackTrace' in initiator:
-        for frame in initiator['stackTrace']:
-          url = frame['url']
-          if url in indicies_by_url:
-            predecessor_url = url
-            predecessor_type = 'stack'
-            break
-      elif initiator_type == 'script':
-        # When the initiator is a script without a stackTrace, infer that it
-        # comes from the most recent script from the same hostname.  TLD+1 might
-        # be better, but finding what is a TLD requires a database.
-        request_hostname = urlparse.urlparse(request.url).hostname
-        sorted_script_requests_from_hostname = [
-            r for r in requests_by_completion
-            if (resource.GetContentType() in ('script', 'html', 'json')
-                and urlparse.urlparse(r.url).hostname == request_hostname)]
-        most_recent = None
-        # Linear search is bad, but this shouldn't matter here.
-        for r in sorted_script_requests_from_hostname:
-          if r.timestamp < request.timing.requestTime:
-            most_recent = r
-          else:
-            break
-        if most_recent is not None:
-          url = most_recent.url
-          if url in indicies_by_url:
-            predecessor_url = url
-            predecessor_type = 'script_inferred'
-      # TODO(mattcary): we skip initiator type other, is that correct?
-      if predecessor_url is not None:
-        predecessor = self._FindBestPredecessor(
-            current_node_info, indicies_by_url[predecessor_url])
-        edge_cost = current_node_info.StartTime() - predecessor.EndTime()
-        if edge_cost < 0:
-          edge_cost = 0
-        if current_node_info.StartTime() < predecessor.StartTime():
-          logging.error('Inverted dependency: %s->%s',
-                        predecessor.ShortName(), current_node_info.ShortName())
-          # Note that current.StartTime() < predecessor.EndTime() appears to
-          # happen a fair amount in practice.
-        predecessor.Node().AddSuccessor(current_node_info.Node())
-        predecessor.SetEdgeCost(current_node_info, edge_cost)
-        predecessor.AddEdgeAnnotation(current_node_info, predecessor_type)
-
-  def _FindBestPredecessor(self, node_info, candidate_indicies):
-    """Find best predecessor for node_info
-
-    If there is only one candidate, we use it regardless of timings. We will
-    later warn about inverted dependencies. If there are more than one, we use
-    the latest whose end time is before node_info's start time. If there is no
-    such candidate, we throw up our hands and return an arbitrary one.
-
-    Args:
-      node_info: _NodeInfo of interest.
-      candidate_indicies: indicies of candidate predecessors.
-
-    Returns:
-      _NodeInfo of best predecessor.
-    """
-    assert candidate_indicies
-    if len(candidate_indicies) == 1:
-      return self._node_info[candidate_indicies[0]]
-    candidate = self._node_info[candidate_indicies[0]]
-    for i in xrange(1, len(candidate_indicies)):
-      next_candidate = self._node_info[candidate_indicies[i]]
-      if (next_candidate.EndTime() < node_info.StartTime() and
-          next_candidate.StartTime() > candidate.StartTime()):
-        candidate = next_candidate
-    if candidate.EndTime() > node_info.StartTime():
-      logging.warning('Multiple candidates but all inverted for ' +
-                      node_info.ShortName())
-    return candidate
 
+    dependencies = request_dependencies_lens.RequestDependencyLens(
+        trace).GetRequestDependencies()
+    for child_rq, parent_rq, reason in dependencies:
+      parent = self._node_info[index_by_request[parent_rq]]
+      child = self._node_info[index_by_request[child_rq]]
+      edge_cost = child.StartTime() - parent.EndTime()
+      if edge_cost < 0:
+        edge_cost = 0
+        if child.StartTime() < parent.StartTime():
+          logging.error('Inverted dependency: %s->%s',
+                        parent.ShortName(), child.ShortName())
+          # Note that child.StartTime() < parent.EndTime() appears to happen a
+          # fair amount in practice.
+      parent.Node().AddSuccessor(child.Node())
+      parent.SetEdgeCost(child, edge_cost)
+      parent.AddEdgeAnnotation(child, reason)
 
   def _SplitChildrenByTime(self, parent):
     """Split children of a node by request times.
@@ -624,7 +564,7 @@ class ResourceGraph(object):
     """
     node_info = self._node_info[index]
     color = self._CONTENT_TYPE_TO_COLOR[node_info.ContentType()]
-    max_age = log_parser.MaxAge(node_info.Request())
+    max_age = node_info.Request().MaxAge()
     shape = 'polygon' if max_age > 300 else 'oval'
     styles = ['filled']
     if highlight:
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 970f00a..d76f5ac 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -8,22 +8,62 @@ import unittest
 
 import dag
 import loading_model
-import log_parser
+import loading_trace
+import request_track
+import request_dependencies_lens
+
+
+class SimpleLens(object):
+  def __init__(self, trace):
+    self._trace = trace
+
+  def GetRequestDependencies(self):
+    url_to_rq = {}
+    deps = []
+    for rq in self._trace.request_track.GetEvents():
+      assert rq.url not in url_to_rq
+      url_to_rq[rq.url] = rq
+    for rq in self._trace.request_track.GetEvents():
+      if rq.initiator in url_to_rq:
+        deps.append((rq, url_to_rq[rq.initiator], ''))
+    return deps
+
+
+class MockRequestTrack(object):
+  def __init__(self, requests):
+    self._requests = requests
+
+  def GetEvents(self):
+    return self._requests
+
 
 class LoadingModelTestCase(unittest.TestCase):
 
+  def setUp(self):
+    request_dependencies_lens.RequestDependencyLens = SimpleLens
+    self._next_request_id = 0
+
   def MakeParserRequest(self, url, source_url, start_time, end_time,
                         magic_content_type=False):
-    timing_data = {f: -1 for f in log_parser.Timing._fields}
-    # We should ignore connectEnd.
-    timing_data['connectEnd'] = (end_time - start_time) / 2
-    timing_data['receiveHeadersEnd'] = end_time - start_time
-    timing_data['requestTime'] = start_time / 1000.0
-    return log_parser.RequestData(
-        None, {'Content-Type': 'null' if not magic_content_type
-                                      else 'magic-debug-content' },
-        None, start_time, timing_data, 'http://' + str(url), False,
-        {'type': 'parser', 'url': 'http://' + str(source_url)})
+    rq = request_track.Request.FromJsonDict({
+        'request_id': self._next_request_id,
+        'url': 'http://' + str(url),
+        'initiator': 'http://' + str(source_url),
+        'response_headers': {'Content-Type':
+                             'null' if not magic_content_type
+                             else 'magic-debug-content' },
+        'timing': request_track.TimingFromDict({
+            # connectEnd should be ignored.
+            'connectEnd': (end_time - start_time) / 2,
+            'receiveHeadersEnd': end_time - start_time,
+            'requestTime': start_time / 1000.0})
+        })
+    self._next_request_id += 1
+    return rq
+
+  def MakeGraph(self, requests):
+    return loading_model.ResourceGraph(loading_trace.LoadingTrace(
+        None, None, None, MockRequestTrack(requests), None))
 
   def SortedIndicies(self, graph):
     return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
@@ -39,7 +79,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 self.MakeParserRequest(4, 3, 127, 128),
                 self.MakeParserRequest(5, 'null', 100, 105),
                 self.MakeParserRequest(6, 5, 105, 110)]
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
@@ -60,7 +100,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 self.MakeParserRequest(4, 3, 127, 128),
                 self.MakeParserRequest(5, 'null', 100, 105),
                 self.MakeParserRequest(6, 5, 105, 110)]
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     path_list = []
     self.assertEqual(28, graph.Cost(path_list))
     self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
@@ -76,10 +116,11 @@ class LoadingModelTestCase(unittest.TestCase):
                                        magic_content_type=True),
                 self.MakeParserRequest(2, 0, 121, 122,
                                        magic_content_type=True),
-                self.MakeParserRequest(3, 0, 112, 119),
+                self.MakeParserRequest(3, 0, 112, 119,
+                                       magic_content_type=True),
                 self.MakeParserRequest(4, 2, 122, 126),
                 self.MakeParserRequest(5, 2, 122, 126)]
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -88,10 +129,10 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
-    # Change node 1 so it is a parent of 3, which become parent of 2.
+    # Change node 1 so it is a parent of 3, which becomes the parent of 2.
     requests[1] = self.MakeParserRequest(1, 0, 110, 111,
                                          magic_content_type=True)
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -101,14 +142,15 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
     # Add an initiator dependence to 1 that will become the parent of 3.
-    requests[1] = self.MakeParserRequest(1, 0, 110, 111)
-    requests.append(self.MakeParserRequest(6, 1, 111, 112))
-    graph = loading_model.ResourceGraph(requests)
-    # Check it doesn't change until we change the content type of 1.
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3, 6])
     requests[1] = self.MakeParserRequest(1, 0, 110, 111,
                                          magic_content_type=True)
-    graph = loading_model.ResourceGraph(requests)
+    requests.append(self.MakeParserRequest(6, 1, 111, 112))
+    graph = self.MakeGraph(requests)
+    # Check it doesn't change until we change the content type of 6.
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
+    requests[6] = self.MakeParserRequest(6, 1, 111, 112,
+                                         magic_content_type=True)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -127,8 +169,8 @@ class LoadingModelTestCase(unittest.TestCase):
                 self.MakeParserRequest(4, 2, 122, 126),
                 self.MakeParserRequest(5, 2, 122, 126)]
     for r in requests:
-      r.headers['Content-Type'] = 'image/gif'
-    graph = loading_model.ResourceGraph(requests)
+      r.response_headers['Content-Type'] = 'image/gif'
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
diff --git a/loading/request_track.py b/loading/request_track.py
index f3b5c9c..e7338ab 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -117,10 +117,35 @@ class Request(object):
   def IsDataRequest(self):
     return self.protocol == 'data'
 
-  # For testing.
+  def MaxAge(self):
+    """Returns the max-age of a resource, or -1."""
+    # TODO(lizeb): Handle the "Expires" header as well.
+    cache_control = {}
+    if not self.response_headers:
+      return -1
+    cache_control_str = self.response_headers.get('Cache-Control', None)
+    if cache_control_str is not None:
+      directives = [s.strip() for s in cache_control_str.split(',')]
+      for directive in directives:
+        parts = [s.strip() for s in directive.split('=')]
+        if len(parts) == 1:
+          cache_control[parts[0]] = True
+        else:
+          cache_control[parts[0]] = parts[1]
+    if (u'no-store' in cache_control
+        or u'no-cache' in cache_control
+        or len(cache_control) == 0):
+      return -1
+    if 'max-age' in cache_control:
+      return int(cache_control['max-age'])
+    return -1
+
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
 
+  def __hash__(self):
+    return hash(self.request_id)
+
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index ac08b30..97ea57f 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -268,6 +268,21 @@ class RequestTrackTestCase(unittest.TestCase):
     request_track = RequestTrack.FromJsonDict(json_dict)
     self.assertEquals(self.request_track, request_track)
 
+  def testMaxAge(self):
+    rq = Request()
+    self.assertEqual(-1, rq.MaxAge())
+    rq.response_headers = {}
+    self.assertEqual(-1, rq.MaxAge())
+    rq.response_headers[
+        'Cache-Control'] = 'private,s-maxage=0,max-age=0,must-revalidate'
+    self.assertEqual(0, rq.MaxAge())
+    rq.response_headers[
+        'Cache-Control'] = 'private,s-maxage=0,no-store,max-age=100'
+    self.assertEqual(-1, rq.MaxAge())
+    rq.response_headers[
+        'Cache-Control'] = 'private,s-maxage=0'
+    self.assertEqual(-1, rq.MaxAge())
+
   @classmethod
   def _ValidSequence(cls, request_track):
     request_track.Handle(

commit 9069d98e5b3b61f0a42be2aa1125d556e32393d0
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jan 20 08:45:54 2016 -0800

    tools/android/loading: Fix RequestTrack deserialization.
    
    Review URL: https://codereview.chromium.org/1604133004
    
    Cr-Original-Commit-Position: refs/heads/master@{#370414}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d10b4905ef6365b1488b18fecefcc70b67fc190a

diff --git a/loading/request_track.py b/loading/request_track.py
index 77b3d4a..f3b5c9c 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -102,6 +102,8 @@ class Request(object):
     result = Request()
     for (k, v) in data_dict.items():
       setattr(result, k, v)
+    if result.timing:
+      result.timing = Timing(*result.timing)
     return result
 
   def GetContentType(self):

commit 5843c7c34b244d188033371336b6307adb283bb9
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jan 20 07:22:55 2016 -0800

    tools/android/loading: Add RequestDependencyLens.
    
    Represents and infers the dependencies between two requests, as surfaced
    by the various tracks.
    
    Review URL: https://codereview.chromium.org/1603883002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370397}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e376bba4c0d6ce5765b28d003dd2bbc74c1b7a84

diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
new file mode 100644
index 0000000..9aacd81
--- /dev/null
+++ b/loading/request_dependencies_lens.py
@@ -0,0 +1,186 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gathers and infers dependencies between requests.
+
+When executed as a script, loads a trace and outputs the dependencies.
+"""
+
+import collections
+import logging
+import operator
+
+import loading_trace
+import request_track
+
+
+class RequestDependencyLens(object):
+  """Analyses and infers request dependencies."""
+  DEPENDENCIES = ('redirect', 'parser', 'script', 'inferred', 'other')
+  def __init__(self, trace):
+    """Initializes an instance of RequestDependencyLens.
+
+    Args:
+      trace: (LoadingTrace) Loading trace.
+    """
+    self.loading_trace = trace
+    self._requests = self.loading_trace.request_track.GetEvents()
+    self._requests_by_id = {r.request_id: r for r in self._requests}
+    self._requests_by_url = collections.defaultdict(list)
+    for request in self._requests:
+      self._requests_by_url[request.url].append(request)
+    self._frame_to_parent = {}
+    for event in self.loading_trace.page_track.GetEvents():
+      if event['method'] == 'Page.frameAttached':
+        self._frame_to_parent[event['frame_id']] = event['parent_frame_id']
+
+  def GetRequestDependencies(self):
+    """Returns a list of request dependencies.
+
+    Returns:
+      [(first, second, reason), ...] where first and second are instances of
+      request_track.Request, and reason is in DEPENDENCIES. The second request
+      depends on the first one, with the listed reason.
+    """
+    deps = []
+    for request in self._requests:
+      dependency = self._GetDependency(request)
+      if dependency:
+        deps.append(dependency)
+    return deps
+
+  def _GetDependency(self, request):
+    """Returns (first, second, reason), or None.
+
+    |second| depends on |first|.
+
+    Args:
+      request: (Request) the request we wish to get the initiator of.
+
+    Returns:
+      None if no dependency is found from this request, or
+      (initiator (Request), blocked_request (Request), reason (str)).
+    """
+    reason = request.initiator['type']
+    assert reason in request_track.Request.INITIATORS
+    # Redirect suffixes are added in RequestTrack.
+    if request.request_id.endswith(request_track.RequestTrack.REDIRECT_SUFFIX):
+      return self._GetInitiatingRequestRedirect(request)
+    elif reason == 'parser':
+      return self._GetInitiatingRequestParser(request)
+    elif reason == 'script':
+      return self._GetInitiatingRequestScript(request)
+    else:
+      assert reason == 'other'
+      return self._GetInitiatingRequestOther(request)
+
+  def _GetInitiatingRequestRedirect(self, request):
+    request_id = request.request_id[:request.request_id.index(
+        request_track.RequestTrack.REDIRECT_SUFFIX)]
+    assert request_id in self._requests_by_id
+    dependent_request = self._requests_by_id[request_id]
+    assert request.timestamp < dependent_request.timestamp
+    return (request, dependent_request, 'redirect')
+
+  def _GetInitiatingRequestParser(self, request):
+    url = request.initiator['url']
+    candidates = self._FindMatchingRequests(url, request.timestamp)
+    if not candidates:
+      return None
+    initiating_request = self._FindBestMatchingInitiator(request, candidates)
+    return (initiating_request, request, 'parser')
+
+  def _GetInitiatingRequestScript(self, request):
+    if not 'stackTrace' in request.initiator:
+      logging.warning('Script initiator but no stack trace.')
+      return None
+    initiating_request = None
+    timestamp = request.timestamp
+    for frame in request.initiator['stackTrace']:
+      url = frame['url']
+      candidates = self._FindMatchingRequests(url, timestamp)
+      if candidates:
+        initiating_request = self._FindBestMatchingInitiator(
+            request, candidates)
+        break
+    else:
+      logging.warning('Unmatched request')
+      return None
+    return (initiating_request, request, 'script')
+
+  def _GetInitiatingRequestOther(self, _):
+    # TODO(lizeb): Infer "other" initiator types.
+    return None
+
+  def _FindMatchingRequests(self, url, before_timestamp):
+    """Returns a list of requests matching a URL, before a timestamp.
+
+    Args:
+      url: (str) URL to match in requests.
+      before_timestamp: (int) Only keep requests submitted before a given
+                        timestamp.
+
+    Returns:
+      A list of candidates, ordered by timestamp.
+    """
+    candidates = self._requests_by_url.get(url, [])
+    candidates = [r for r in candidates if (
+        r.timestamp + max(0, r.timing.receive_headers_end) <= before_timestamp)]
+    candidates.sort(key=operator.attrgetter('timestamp'))
+    return candidates
+
+  def _FindBestMatchingInitiator(self, request, matches):
+    """Returns the best matching request within a list of matches.
+
+    Iteratively removes candidates until one is left:
+    - With the same parent frame.
+    - From the same frame.
+
+    If this is not successful, takes the most recent request.
+
+    Args:
+      request: (Request) Request.
+      matches: [Request] As returned by _FindMatchingRequests(), that is
+               sorted by timestamp.
+
+    Returns:
+      The best matching initiating request, or None.
+    """
+    if not matches:
+      return None
+    if len(matches) == 1:
+      return matches[0]
+    # Several matches, try to reduce this number to 1. Otherwise, return the
+    # most recent one.
+    if request.frame_id in self._frame_to_parent: # Main frame has no parent.
+      parent_frame_id = self._frame_to_parent[request.frame_id]
+      same_parent_matches = [
+          r for r in matches
+          if self._frame_to_parent[r.frame_id] == parent_frame_id]
+      if not same_parent_matches:
+        logging.warning('All matches are from non-sibling frames.')
+        return matches[-1]
+      if len(same_parent_matches) == 1:
+        return same_parent_matches[0]
+    same_frame_matches = [r for r in matches if r.frame_id == request.frame_id]
+    if not same_frame_matches:
+      logging.warning('All matches are from non-sibling frames.')
+      return matches[-1]
+    if len(same_frame_matches) == 1:
+      return same_frame_matches[0]
+    else:
+      logging.warning('Several matches')
+      return same_frame_matches[-1]
+
+
+if __name__ == '__main__':
+  import json
+  import sys
+  trace_filename = sys.argv[1]
+  json_dict = json.load(open(trace_filename, 'r'))
+  lens = RequestDependencyLens(
+      loading_trace.LoadingTrace.FromJsonDict(json_dict))
+  depedencies = lens.GetRequestDependencies()
+  for (first, second, dep_reason) in depedencies:
+    print '%s -> %s\t(%s)' % (first.request_id, second.request_id, dep_reason)
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
new file mode 100644
index 0000000..e5aed37
--- /dev/null
+++ b/loading/request_dependencies_lens_unittest.py
@@ -0,0 +1,157 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import devtools_monitor
+from loading_trace import LoadingTrace
+from request_dependencies_lens import RequestDependencyLens
+from request_track import (Request, TimingFromDict)
+from page_track import PageTrack
+
+
+class FakeTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeTrack, self).__init__(None)
+    self._events = events
+
+  def GetEvents(self):
+    return self._events
+
+
+class RequestDependencyLensTestCase(unittest.TestCase):
+  _REDIRECT_REQUEST = Request.FromJsonDict(
+      {'url': 'http://bla.com', 'request_id': '1234.1.redirect',
+       'initiator': {'type': 'other'},
+       'timestamp': 1, 'timing': TimingFromDict({})})
+  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'request_id': '1234.1',
+                                   'frame_id': '123.1',
+                                   'initiator': {'type': 'other'},
+                                   'timestamp': 2,
+                                   'timing': TimingFromDict({})})
+  _JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
+                                      'request_id': '1234.12',
+                                      'frame_id': '123.1',
+                                      'initiator': {'type': 'parser',
+                                                    'url': 'http://bla.com'},
+                                      'timestamp': 3,
+                                      'timing': TimingFromDict({})})
+  _JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
+      {'url': 'http://bla.com/nyancat.js',
+       'request_id': '1234.42',
+       'frame_id': '123.13',
+       'initiator': {'type': 'parser',
+                     'url': 'http://bla.com'},
+       'timestamp': 4, 'timing': TimingFromDict({})})
+  _JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
+      {'url': 'http://bla.com/nyancat.js',
+       'request_id': '1234.42',
+       'frame_id': '123.99',
+       'initiator': {'type': 'parser',
+                     'url': 'http://bla.com'},
+       'timestamp': 5, 'timing': TimingFromDict({})})
+  _JS_REQUEST_2 = Request.FromJsonDict(
+      {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
+       'frame_id': '123.1',
+       'initiator': {'type': 'script',
+                     'stackTrace': [{'url': 'unknown'},
+                                    {'url': 'http://bla.com/nyancat.js'}]},
+       'timestamp': 10, 'timing': TimingFromDict({})})
+  _PAGE_TRACK = FakeTrack(
+      [{'method': 'Page.frameAttached',
+        'frame_id': '123.13', 'parent_frame_id': '123.1'}])
+
+  def testRedirectDependency(self):
+    request_track = FakeTrack([self._REDIRECT_REQUEST, self._REQUEST])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    (first, second, reason) = deps[0]
+    self.assertEquals('redirect', reason)
+    self.assertEquals(self._REDIRECT_REQUEST.request_id, first.request_id)
+    self.assertEquals(self._REQUEST.request_id, second.request_id)
+
+  def testScriptDependency(self):
+    request_track = FakeTrack([self._JS_REQUEST, self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+
+  def testParserDependency(self):
+    request_track = FakeTrack([self._REQUEST, self._JS_REQUEST])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+
+  def testSeveralDependencies(self):
+    request_track = FakeTrack(
+        [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
+         self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(3, len(deps))
+    self._AssertDependencyIs(
+        deps[0], self._REDIRECT_REQUEST.request_id, self._REQUEST.request_id,
+        'redirect')
+    self._AssertDependencyIs(
+        deps[1],
+        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+    self._AssertDependencyIs(
+        deps[2],
+        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+
+  def testDependencyDifferentFrame(self):
+    """Checks that a more recent request from another frame is ignored."""
+    request_track = FakeTrack(
+        [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+
+  def testDependencySameParentFrame(self):
+    """Checks that a more recent request from an unrelated frame is ignored
+    if there is one from a related frame."""
+    request_track = FakeTrack(
+        [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
+         self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, self._PAGE_TRACK,
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._JS_REQUEST_OTHER_FRAME.request_id,
+        self._JS_REQUEST_2.request_id, 'script')
+
+  def _AssertDependencyIs(
+      self, dep, first_request_id, second_request_id, reason):
+    (first, second, dependency_reason) = dep
+    self.assertEquals(reason, dependency_reason)
+    self.assertEquals(first_request_id, first.request_id)
+    self.assertEquals(second_request_id, second.request_id)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index 7130965..77b3d4a 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -33,7 +33,7 @@ class Request(object):
   third_party/WebKit/Source/devtools/protocol.json.
 
   Fields:
-    request_id: (str) unique request ID. Postfixed with ".redirect" for
+    request_id: (str) unique request ID. Postfixed with REDIRECT_SUFFIX for
                 redirects.
     frame_id: (str) unique frame identifier.
     loader_id: (str) unique frame identifier.
@@ -122,6 +122,7 @@ class Request(object):
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
+  REDIRECT_SUFFIX = '.redirect'
   # Request status
   _STATUS_SENT = 0
   _STATUS_RESPONSE = 1
@@ -197,8 +198,8 @@ class RequestTrack(devtools_monitor.Track):
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache')))
-    r.timing = _TimingFromDict(redirect_response['timing'])
-    r.request_id = request_id + '.redirect'
+    r.timing = TimingFromDict(redirect_response['timing'])
+    r.request_id = request_id + self.REDIRECT_SUFFIX
     self._requests_in_flight[r.request_id] = (r, RequestTrack._STATUS_FINISHED)
     del self._requests_in_flight[request_id]
     self._FinalizeRequest(r.request_id)
@@ -236,7 +237,7 @@ class RequestTrack(devtools_monitor.Track):
       timing_dict = response['timing']
     else:
       timing_dict = {'requestTime': r.timestamp}
-    r.timing = _TimingFromDict(timing_dict)
+    r.timing = TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
 
   def _DataReceived(self, request_id, params):
@@ -286,7 +287,8 @@ RequestTrack._METHOD_TO_HANDLER = {
     'Network.loadingFailed': RequestTrack._LoadingFailed}
 
 
-def _TimingFromDict(timing_dict):
+def TimingFromDict(timing_dict):
+  """Returns an instance of Timing from an () dict."""
   complete_timing_dict = {field: -1 for field in Timing._fields}
   timing_dict_mapped = {
       _TIMING_NAMES_MAPPING[k]: v for (k, v) in timing_dict.items()}
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 1e9403b..ac08b30 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -5,7 +5,7 @@
 import json
 import unittest
 
-from request_track import (Request, RequestTrack, _TimingFromDict)
+from request_track import (Request, RequestTrack, TimingFromDict)
 
 
 class RequestTestCase(unittest.TestCase):
@@ -192,6 +192,9 @@ class RequestTrackTestCase(unittest.TestCase):
                               RequestTrackTestCase._REDIRECT)
     self.assertEquals(1, len(self.request_track._requests_in_flight))
     self.assertEquals(1, len(self.request_track.GetEvents()))
+    redirect_request = self.request_track.GetEvents()[0]
+    self.assertTrue(redirect_request.request_id.endswith(
+        RequestTrack.REDIRECT_SUFFIX))
 
   def testRejectDuplicates(self):
     msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
@@ -231,7 +234,7 @@ class RequestTrackTestCase(unittest.TestCase):
     self.assertEquals(False, r.served_from_cache)
     self.assertEquals(False, r.from_disk_cache)
     self.assertEquals(False, r.from_service_worker)
-    timing = _TimingFromDict(response['timing'])
+    timing = TimingFromDict(response['timing'])
     loading_finished = RequestTrackTestCase._LOADING_FINISHED['params']
     loading_finished_offset = r._TimestampOffsetFromStartMs(
         loading_finished['timestamp'])

commit 56dd5cc56899f8931a6336ed926ced98d93416f5
Author: mattcary <mattcary@chromium.org>
Date:   Wed Jan 20 06:48:38 2016 -0800

    Lookup events which end in the given time range. This is a straightforward application of the index already computed.
    
    Review URL: https://codereview.chromium.org/1602963002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370389}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9619968005e1f8e2fbefbaa16499aee4ec3bffff

diff --git a/loading/tracing.py b/loading/tracing.py
index 47a0ae3..4959ed7 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -64,7 +64,9 @@ class TracingTrack(devtools_monitor.Track):
 
     Returns:
       List of events active at that timestamp. Instantaneous (ie, instant,
-      sample and counter) events are never included.
+      sample and counter) events are never included. Event end times are
+      exclusive, so that an event ending at the usec parameter will not be
+      returned.
       TODO(mattcary): currently live objects are included. If this is too big we
       may break that out into a separate index.
     """
@@ -78,7 +80,6 @@ class TracingTrack(devtools_monitor.Track):
       return []
     return events.event_list
 
-
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
@@ -90,6 +91,35 @@ class TracingTrack(devtools_monitor.Track):
     tracing_track._events = events
     return tracing_track
 
+  def EventsEndingBetween(self, start_msec, end_msec):
+    """Gets the list of events whose end lies in a range.
+
+    Args:
+      start_msec: the start of the range to query, in milliseconds, inclusive.
+      end_msec: the end of the range to query, in milliseconds, inclusive.
+
+    Returns:
+      List of events whose end time lies in the range. Note that although the
+      range is inclusive at both ends, an ending timestamp is considered to be
+      exclusive of the actual event. An event ending at 10 msec will be returned
+      for a range [10, 14] as well as [8, 10], though the event is considered to
+      end the instant before 10 msec. In practice this is only important when
+      considering how events overlap; an event ending at 10 msec does not
+      overlap with one starting at 10 msec and so may unambiguously share ids,
+      etc.
+    """
+    self._IndexEvents()
+    low_idx = bisect.bisect_left(self._event_msec_index, start_msec) - 1
+    high_idx = bisect.bisect_right(self._event_msec_index, end_msec)
+    matched_events = []
+    for i in xrange(max(0, low_idx), high_idx):
+      if self._event_lists[i]:
+        for e in self._event_lists[i].event_list:
+          assert e.end_msec is not None
+          if e.end_msec >= start_msec and e.end_msec <= end_msec:
+            matched_events.append(e)
+    return matched_events
+
   def _IndexEvents(self):
     """Computes index for in-flight events.
 
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 77116fe..d190f8a 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -204,6 +204,46 @@ class TracingTrackTestCase(unittest.TestCase):
     for (e1, e2) in zip(self.track._events, deserialized_track._events):
       self.assertEquals(e1.tracing_event, e2.tracing_event)
 
+  def testEventsEndingBetween(self):
+    self.track.Handle(
+        'Tracing.dataCollected',
+        {'params':
+         {'value': [self.EventToMicroseconds(e) for e in
+          [{'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+           {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
+           {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
+           {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
+           {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
+           {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]]}})
+    self.assertEqual(set('ABCDEF'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(0, 100)]))
+    self.assertFalse([e.args['name']
+                      for e in self.track.EventsEndingBetween(3, 5)])
+    self.assertTrue('B' in set([e.args['name']
+                          for e in self.track.EventsEndingBetween(3, 6)]))
+    self.assertEqual(set('B'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(3, 6)]))
+    self.assertEqual(set('AB'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(3, 7)]))
+    self.assertEqual(set('AB'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(6, 7)]))
+    self.assertEqual(set('A'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(7, 10)]))
+    self.assertEqual(set('AC'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(7, 11)]))
+    self.assertEqual(set('CD'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(8, 13)]))
+
+
+
+
 
 if __name__ == '__main__':
   unittest.main()

commit ecbb79355d6ff0346fec9067416ca8a95cf27aad
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 19 08:13:21 2016 -0800

    tools/android/loading: Archive tracks in LoadingTrace.
    
    Review URL: https://codereview.chromium.org/1606903002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370121}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6cf3e7e1f3cb4bd595f3ddabcc126ead2e10ddf7

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
new file mode 100644
index 0000000..720fa1d
--- /dev/null
+++ b/loading/loading_trace.py
@@ -0,0 +1,55 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Represents the trace of a page load."""
+
+import page_track
+import request_track
+import tracing
+
+class LoadingTrace(object):
+  """Represents the trace of a page load."""
+  _URL_KEY = 'url'
+  _METADATA_KEY = 'metadata'
+  _PAGE_KEY = 'page_track'
+  _REQUEST_KEY = 'request_track'
+  _TRACING_KEY = 'tracing_track'
+
+  def __init__(self, url, metadata, page, request, tracing_track):
+    """Initializes a loading trace instance.
+
+    Args:
+      url: (str) URL that has been loaded
+      metadata: (dict) Metadata associated with the load.
+      page: (PageTrack) instance of PageTrack.
+      request: (RequestTrack) instance of RequestTrack.
+      tracing_track: (TracingTrack) instance of TracingTrack.
+    """
+    self.url = url
+    self.metadata = metadata
+    self.page_track = page
+    self.request_track = request
+    self.tracing_track = tracing_track
+
+  def ToJsonDict(self):
+    """Returns a dictionary representing this instance."""
+    result = {self._URL_KEY: self.url, self._METADATA_KEY: self.metadata,
+              self._PAGE_KEY: self.page_track.ToJsonDict(),
+              self._REQUEST_KEY: self.request_track.ToJsonDict(),
+              self._TRACING_KEY: self.tracing_track.ToJsonDict()}
+    return result
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns an instance from a dictionary returned by ToJsonDict()."""
+    keys = (cls._URL_KEY, cls._METADATA_KEY, cls._PAGE_KEY, cls._REQUEST_KEY,
+            cls._TRACING_KEY)
+    assert all(key in json_dict for key in keys)
+    page = page_track.PageTrack.FromJsonDict(json_dict[cls._PAGE_KEY])
+    request = request_track.RequestTrack.FromJsonDict(
+        json_dict[cls._REQUEST_KEY])
+    tracing_track = tracing.TracingTrack.FromJsonDict(
+        json_dict[cls._TRACING_KEY])
+    return LoadingTrace(json_dict[cls._URL_KEY], json_dict[cls._METADATA_KEY],
+                        page, request, tracing_track)
diff --git a/loading/request_track.py b/loading/request_track.py
index 08e1588..7130965 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -230,7 +230,12 @@ class RequestTrack(devtools_monitor.Track):
                       # network stack.
                       ('requestHeaders', 'request_headers'),
                       ('headers', 'response_headers')))
-    timing_dict = response['timing'] if r.protocol != 'data' else {}
+    # data URLs don't have a timing dict.
+    timing_dict = {}
+    if r.protocol != 'data':
+      timing_dict = response['timing']
+    else:
+      timing_dict = {'requestTime': r.timestamp}
     r.timing = _TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
 
@@ -291,7 +296,8 @@ def _TimingFromDict(timing_dict):
 
 def _CopyFromDictToObject(d, o, key_attrs):
   for (key, attr) in key_attrs:
-    setattr(o, attr, d[key])
+    if key in d:
+      setattr(o, attr, d[key])
 
 
 if __name__ == '__main__':
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 0b96225..fdf6cc6 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -5,8 +5,13 @@
 
 """Loading trace recorder."""
 
+import argparse
+import datetime
+import json
+import logging
 import os
 import sys
+import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -19,32 +24,43 @@ import devil_chromium
 
 import device_setup
 import devtools_monitor
+import loading_trace
 import page_track
+import request_track
+import tracing
 
-class AndroidTraceRecorder(object):
-  """Records a loading trace."""
-  def __init__(self, url):
-    self.url = url
-    self.devtools_connection = None
-    self.page_track = None
 
-  def Go(self, connection):
-    self.devtools_connection = connection
-    self.page_track = page_track.PageTrack(self.devtools_connection)
-    self.devtools_connection.SetUpMonitoring()
-    self.devtools_connection.SendAndIgnoreResponse(
-        'Page.navigate', {'url': self.url})
-    self.devtools_connection.StartMonitoring()
-    print self.page_track.GetEvents()
+def RecordAndDumpTrace(device, url, output_filename):
+  with file(output_filename, 'w') as output,\
+        device_setup.DeviceConnection(device) as connection:
+    page = page_track.PageTrack(connection)
+    request = request_track.RequestTrack(connection)
+    tracing_track = tracing.TracingTrack(connection)
+    connection.SetUpMonitoring()
+    connection.SendAndIgnoreResponse('Network.clearBrowserCache', {})
+    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+    connection.StartMonitoring()
+    metadata = {'date': datetime.datetime.utcnow().isoformat(),
+                'seconds_since_epoch': time.time()}
+    trace = loading_trace.LoadingTrace(url, metadata, page, request,
+                                       tracing_track)
+    json.dump(trace.ToJsonDict(), output)
 
 
-def DoIt(url):
+def main():
+  logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
-  devices = device_utils.DeviceUtils.HealthyDevices()
-  device = devices[0]
-  trace_recorder = AndroidTraceRecorder(url)
-  device_setup.SetUpAndExecute(device, 'chrome', trace_recorder.Go)
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--url', required=True)
+  parser.add_argument('--output', required=True)
+  args = parser.parse_args()
+  url = args.url
+  if not url.startswith('http'):
+    url = 'http://' + url
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  RecordAndDumpTrace(device, url, args.output)
 
 
 if __name__ == '__main__':
-  DoIt(sys.argv[1])
+  main()
diff --git a/loading/tracing.py b/loading/tracing.py
index 318baf0..47a0ae3 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -28,7 +28,8 @@ class TracingTrack(devtools_monitor.Track):
         a stream is slower than the default reporting as dataCollected events.
     """
     super(TracingTrack, self).__init__(connection)
-    connection.RegisterListener('Tracing.dataCollected', self)
+    if connection:
+      connection.RegisterListener('Tracing.dataCollected', self)
     params = {}
     if categories:
       params['categories'] = (categories if type(categories) is str
@@ -36,7 +37,8 @@ class TracingTrack(devtools_monitor.Track):
     if fetch_stream:
       params['transferMode'] = 'ReturnAsStream'
 
-    connection.SyncRequestNoResponse('Tracing.start', params)
+    if connection:
+      connection.SyncRequestNoResponse('Tracing.start', params)
     self._events = []
 
     self._event_msec_index = None
@@ -76,6 +78,18 @@ class TracingTrack(devtools_monitor.Track):
       return []
     return events.event_list
 
+
+  def ToJsonDict(self):
+    return {'events': [e.ToJsonDict() for e in self._events]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    events = [Event(e) for e in json_dict['events']]
+    tracing_track = TracingTrack(None)
+    tracing_track._events = events
+    return tracing_track
+
   def _IndexEvents(self):
     """Computes index for in-flight events.
 
@@ -275,7 +289,10 @@ class Event(object):
     self._end_msec = None
     self._synthetic = synthetic
     if self.type == 'X':
-      self._end_msec = self.start_msec + tracing_event['dur'] / 1000.0
+      # Some events don't have a duration.
+      duration = (tracing_event['dur']
+                  if 'dur' in tracing_event else tracing_event['tdur'])
+      self._end_msec = self.start_msec + duration / 1000.0
 
   @property
   def start_msec(self):
@@ -356,3 +373,10 @@ class Event(object):
     if 'args' in closing.tracing_event:
       self.tracing_event.setdefault(
           'args', {}).update(closing.tracing_event['args'])
+
+  def ToJsonDict(self):
+    return self._tracing_event
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    return Event(json_dict)
diff --git a/loading/tracing_driver.py b/loading/tracing_driver.py
deleted file mode 100755
index c62e870..0000000
--- a/loading/tracing_driver.py
+++ /dev/null
@@ -1,49 +0,0 @@
-#! /usr/bin/python
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Drive TracingConnection"""
-
-import argparse
-import json
-import logging
-import os.path
-import sys
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-import device_setup
-import page_track
-import tracing
-
-
-def main():
-  logging.basicConfig(level=logging.INFO)
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--url', required=True)
-  parser.add_argument('--output', required=True)
-  args = parser.parse_args()
-  url = args.url
-  if not url.startswith('http'):
-    url = 'http://' + url
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  with file(args.output, 'w') as output, \
-       file(args.output + '.page', 'w') as page_output, \
-       device_setup.DeviceConnection(device) as connection:
-    track = tracing.TracingTrack(connection, fetch_stream=False)
-    page = page_track.PageTrack(connection)
-    connection.SetUpMonitoring()
-    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-    connection.StartMonitoring()
-    json.dump(page.GetEvents(), page_output, sort_keys=True, indent=2)
-    json.dump(track.GetEvents(), output, sort_keys=True, indent=2)
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 9a4bbb9..77116fe 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -7,20 +7,25 @@ import unittest
 
 import devtools_monitor
 
-from tracing import TracingTrack
-
-
-class StubConnection(object):
-  def RegisterListener(self, name, obj):
-    pass
-
-  def SyncRequestNoResponse(self, method, params):
-    pass
+from tracing import (Event, TracingTrack)
 
 
 class TracingTrackTestCase(unittest.TestCase):
+  _MIXED_EVENTS = [
+      {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+      {'ts': 7, 'ph': 'D', 'id': 1},
+      {'ts': 10, 'ph': 'B', 'args': {'name': 'D'}},
+      {'ts': 10, 'ph': 'b', 'cat': 'X', 'id': 1, 'args': {'name': 'C'}},
+      {'ts': 11, 'ph': 'e', 'cat': 'X', 'id': 1},
+      {'ts': 12, 'ph': 'E'},
+      {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
+      {'ts': 13, 'ph': 'b', 'cat': 'X', 'id': 2, 'args': {'name': 'F'}},
+      {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
+      {'ts': 15, 'ph': 'D', 'id': 1}]
+
   def setUp(self):
-    self.track = TracingTrack(StubConnection())
+    self.track = TracingTrack(None)
 
   def EventToMicroseconds(self, event):
     if 'ts' in event:
@@ -176,18 +181,28 @@ class TracingTrackTestCase(unittest.TestCase):
 
   def testMixed(self):
     # A and E are objects, B complete, D a duration, and C and F async.
-    self.CheckIntervals([
-        {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
-        {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
-        {'ts': 7, 'ph': 'D', 'id': 1},
-        {'ts': 10, 'ph': 'B', 'args': {'name': 'D'}},
-        {'ts': 10, 'ph': 'b', 'cat': 'X', 'id': 1, 'args': {'name': 'C'}},
-        {'ts': 11, 'ph': 'e', 'cat': 'X', 'id': 1},
-        {'ts': 12, 'ph': 'E'},
-        {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
-        {'ts': 13, 'ph': 'b', 'cat': 'X', 'id': 2, 'args': {'name': 'F'}},
-        {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
-        {'ts': 15, 'ph': 'D', 'id': 1}])
+    self.CheckIntervals(self._MIXED_EVENTS)
+
+  def testEventSerialization(self):
+    for e in self._MIXED_EVENTS:
+      event = Event(e)
+      json_dict = event.ToJsonDict()
+      deserialized_event = Event.FromJsonDict(json_dict)
+      self.assertEquals(
+          event.tracing_event, deserialized_event.tracing_event)
+
+  def testTracingTrackSerialization(self):
+    events = self._MIXED_EVENTS
+    self.track.Handle('Tracing.dataCollected',
+                      {'params': {'value': [self.EventToMicroseconds(e)
+                                            for e in events]}})
+    json_dict = self.track.ToJsonDict()
+    self.assertTrue('events' in json_dict)
+    deserialized_track = TracingTrack.FromJsonDict(json_dict)
+    self.assertEquals(
+        len(self.track._events), len(deserialized_track._events))
+    for (e1, e2) in zip(self.track._events, deserialized_track._events):
+      self.assertEquals(e1.tracing_event, e2.tracing_event)
 
 
 if __name__ == '__main__':

commit c4666ec5fee4a5408902341bf5b28f9b6e5fe4c1
Author: mattcary <mattcary@chromium.org>
Date:   Tue Jan 19 03:21:05 2016 -0800

    Processing for tracing: add EventsAt method which gives the in-flight events
    present at a timestamp.
    
    Review URL: https://codereview.chromium.org/1607503003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370095}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 47ad0a66f5966f783dee80b0677b5b68c29010b4

diff --git a/loading/tracing.py b/loading/tracing.py
index 83ad38c..318baf0 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -4,9 +4,17 @@
 
 """Monitor tracing events on chrome via chrome remote debugging."""
 
+import bisect
+import itertools
+
 import devtools_monitor
 
+
 class TracingTrack(devtools_monitor.Track):
+  """Grabs and processes trace event messages.
+
+  See https://goo.gl/Qabkqk for details on the protocol.
+  """
   def __init__(self, connection, categories=None, fetch_stream=False):
     """Initialize this TracingTrack.
 
@@ -31,8 +39,320 @@ class TracingTrack(devtools_monitor.Track):
     connection.SyncRequestNoResponse('Tracing.start', params)
     self._events = []
 
+    self._event_msec_index = None
+    self._event_lists = None
+
   def Handle(self, method, event):
-    self._events.append(event)
+    for e in event['params']['value']:
+      self._events.append(Event(e))
+    # Just invalidate our indices rather than trying to be fancy and
+    # incrementally update.
+    self._event_msec_index = None
+    self._event_lists = None
 
   def GetEvents(self):
     return self._events
+
+  def EventsAt(self, msec):
+    """Gets events active at a timestamp.
+
+    Args:
+      msec: tracing milliseconds to query. Tracing milliseconds appears to be
+        since chrome startup (ie, arbitrary epoch).
+
+    Returns:
+      List of events active at that timestamp. Instantaneous (ie, instant,
+      sample and counter) events are never included.
+      TODO(mattcary): currently live objects are included. If this is too big we
+      may break that out into a separate index.
+    """
+    self._IndexEvents()
+    idx = bisect.bisect_right(self._event_msec_index, msec) - 1
+    if idx < 0:
+      return []
+    events = self._event_lists[idx]
+    assert events.start_msec <= msec
+    if not events or events.end_msec < msec:
+      return []
+    return events.event_list
+
+  def _IndexEvents(self):
+    """Computes index for in-flight events.
+
+    Creates a list of timestamps where events start or end, and tracks the
+    current set of in-flight events at the instant after each timestamp. To do
+    this we have to synthesize ending events for complete events, as well as
+    join and track the nesting of async, flow and other spanning events.
+
+    Events such as instant and counter events that aren't indexable are skipped.
+
+    """
+    if self._event_msec_index is not None:
+      return  # Already indexed.
+
+    if not self._events:
+      raise devtools_monitor.DevToolsConnectionException('No events to index')
+
+    self._event_msec_index = []
+    self._event_lists = []
+    synthetic_events = []
+    for e in self._events:
+      synthetic_events.extend(e.Synthesize())
+    synthetic_events.sort(key=lambda e: e.start_msec)
+    current_events = set()
+    next_idx = 0
+    spanning_events = self._SpanningEvents()
+    while next_idx < len(synthetic_events):
+      current_msec = synthetic_events[next_idx].start_msec
+      while next_idx < len(synthetic_events):
+        event = synthetic_events[next_idx]
+        assert event.IsIndexable()
+        if event.start_msec > current_msec:
+          break
+        matched_event = spanning_events.Match(event)
+        if matched_event is not None:
+          event = matched_event
+        if not event.synthetic and (
+            event.end_msec is None or event.end_msec >= current_msec):
+          current_events.add(event)
+        next_idx += 1
+      current_events -= set([
+          e for e in current_events
+          if e.end_msec is not None and e.end_msec <= current_msec])
+      self._event_msec_index.append(current_msec)
+      self._event_lists.append(self._EventList(current_events))
+    if spanning_events.HasPending():
+      raise devtools_monitor.DevToolsConnectionException(
+          'Pending spanning events: %s' %
+          '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
+
+  class _SpanningEvents(object):
+    def __init__(self):
+      self._duration_stack = []
+      self._async_stacks = {}
+      self._objects = {}
+      self._MATCH_HANDLER = {
+          'B': self._DurationBegin,
+          'E': self._DurationEnd,
+          'b': self._AsyncStart,
+          'e': self._AsyncEnd,
+          'S': self._AsyncStart,
+          'F': self._AsyncEnd,
+          'N': self._ObjectCreated,
+          'D': self._ObjectDestroyed,
+          'X': self._Ignore,
+          None: self._Ignore,
+          }
+
+    def Match(self, event):
+      return self._MATCH_HANDLER.get(
+          event.type, self._Unsupported)(event)
+
+    def HasPending(self):
+      return (self._duration_stack or
+              self._async_stacks or
+              self._objects)
+
+    def PendingEvents(self):
+      return itertools.chain(
+          (e for e in self._duration_stack),
+          (o for o in self._objects),
+          itertools.chain.from_iterable((
+              (e for e in s) for s in self._async_stacks.itervalues())))
+
+    def _AsyncKey(self, event):
+      return (event.tracing_event['cat'], event.id)
+
+    def _Ignore(self, _event):
+      return None
+
+    def _Unsupported(self, event):
+      raise devtools_monitor.DevToolsConnectionException(
+          'Unsupported spanning event type: %s' % event)
+
+    def _DurationBegin(self, event):
+      self._duration_stack.append(event)
+      return None
+
+    def _DurationEnd(self, event):
+      if not self._duration_stack:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Unmatched duration end: %s' % event)
+      start = self._duration_stack.pop()
+      start.SetClose(event)
+      return start
+
+    def _AsyncStart(self, event):
+      key = self._AsyncKey(event)
+      self._async_stacks.setdefault(key, []).append(event)
+      return None
+
+    def _AsyncEnd(self, event):
+      key = self._AsyncKey(event)
+      if key not in self._async_stacks:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Unmatched async end %s: %s' % (key, event))
+      stack = self._async_stacks[key]
+      start = stack.pop()
+      if not stack:
+        del self._async_stacks[key]
+      start.SetClose(event)
+      return start
+
+    def _ObjectCreated(self, event):
+      # The tracing event format has object deletion timestamps being exclusive,
+      # that is the timestamp for a deletion my equal that of the next create at
+      # the same address. This asserts that does not happen in practice as it is
+      # inconvenient to handle that correctly here.
+      if event.id in self._objects:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Multiple objects at same address: %s, %s' %
+            (event, self._objects[event.id]))
+      self._objects[event.id] = event
+      return None
+
+    def _ObjectDestroyed(self, event):
+      if event.id not in self._objects:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Missing object creation for %s' % event)
+      start = self._objects[event.id]
+      del self._objects[event.id]
+      start.SetClose(event)
+      return start
+
+  class _EventList(object):
+    def __init__(self, events):
+      self._events = [e for e in events]
+      if self._events:
+        self._start_msec = min(e.start_msec for e in self._events)
+        # Event end times may be changed after this list is created so the end
+        # can't be cached.
+      else:
+        self._start_msec = self._end_msec = None
+
+    @property
+    def event_list(self):
+      return self._events
+
+    @property
+    def start_msec(self):
+      return self._start_msec
+
+    @property
+    def end_msec(self):
+      return max(e.end_msec for e in self._events)
+
+    def __nonzero__(self):
+      return bool(self._events)
+
+
+class Event(object):
+  """Wraps a tracing event."""
+  CLOSING_EVENTS = {'E': 'B',
+                    'e': 'b',
+                    'F': 'S',
+                    'D': 'N'}
+  def __init__(self, tracing_event, synthetic=False):
+    """Creates Event.
+
+    Intended to be created only by TracingTrack.
+
+    Args:
+      tracing_event: JSON tracing event, as defined in https://goo.gl/Qabkqk.
+      synthetic: True if the event is synthetic. This is only used for indexing
+        internal to TracingTrack.
+    """
+    if not synthetic and tracing_event['ph'] in ['s', 't', 'f']:
+      raise devtools_monitor.DevToolsConnectionException(
+          'Unsupported event: %s' % tracing_event)
+    if not synthetic and tracing_event['ph'] in ['p']:
+      raise devtools_monitor.DevToolsConnectionException(
+          'Deprecated event: %s' % tracing_event)
+
+    self._tracing_event = tracing_event
+    # Note tracing event times are in microseconds.
+    self._start_msec = tracing_event['ts'] / 1000.0
+    self._end_msec = None
+    self._synthetic = synthetic
+    if self.type == 'X':
+      self._end_msec = self.start_msec + tracing_event['dur'] / 1000.0
+
+  @property
+  def start_msec(self):
+    return self._start_msec
+
+  @property
+  def end_msec(self):
+    return self._end_msec
+
+  @property
+  def type(self):
+    if self._synthetic:
+      return None
+    return self._tracing_event['ph']
+
+  @property
+  def args(self):
+    return self._tracing_event.get('args', {})
+
+  @property
+  def id(self):
+    return self._tracing_event.get('id')
+
+  @property
+  def tracing_event(self):
+    return self._tracing_event
+
+  @property
+  def synthetic(self):
+    return self._synthetic
+
+  def __str__(self):
+    return ''.join([str(self._tracing_event),
+                    '[%s,%s]' % (self.start_msec, self.end_msec)])
+
+  def IsIndexable(self):
+    """True iff the event can be indexed by time."""
+    return self._synthetic or self.type not in [
+        'I', 'P', 'c', 'C',
+        'n', 'T', 'p',  # TODO(mattcary): ?? instant types of async events.
+        'O',            # TODO(mattcary): ?? object snapshot
+        ]
+
+  def Synthesize(self):
+    """Expand into synthetic events.
+
+    Returns:
+      A list of events, possibly some synthetic, whose start times are all
+      interesting for purposes of indexing. If the event is not indexable the
+      set may be empty.
+    """
+    if not self.IsIndexable():
+      return []
+    if self.type == 'X':
+      # Tracing event timestamps are microseconds!
+      return [self, Event({'ts': self.end_msec * 1000}, synthetic=True)]
+    return [self]
+
+  def SetClose(self, closing):
+    """Close a spanning event.
+
+    Args:
+      closing: The closing event.
+
+    Raises:
+      devtools_monitor.DevToolsConnectionException if closing can't property
+      close this event.
+    """
+    if self.type != self.CLOSING_EVENTS.get(closing.type):
+      raise devtools_monitor.DevToolsConnectionException(
+        'Bad closing: %s --> %s' % (self, closing))
+    if self.type in ['b', 'S'] and (
+        self.tracing_event['cat'] != closing.tracing_event['cat'] or
+        self.id != closing.id):
+      raise devtools_monitor.DevToolsConnectionException(
+        'Bad async closing: %s --> %s' % (self, closing))
+    self._end_msec = closing.start_msec
+    if 'args' in closing.tracing_event:
+      self.tracing_event.setdefault(
+          'args', {}).update(closing.tracing_event['args'])
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
new file mode 100644
index 0000000..9a4bbb9
--- /dev/null
+++ b/loading/tracing_unittest.py
@@ -0,0 +1,194 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+import unittest
+
+import devtools_monitor
+
+from tracing import TracingTrack
+
+
+class StubConnection(object):
+  def RegisterListener(self, name, obj):
+    pass
+
+  def SyncRequestNoResponse(self, method, params):
+    pass
+
+
+class TracingTrackTestCase(unittest.TestCase):
+  def setUp(self):
+    self.track = TracingTrack(StubConnection())
+
+  def EventToMicroseconds(self, event):
+    if 'ts' in event:
+      event['ts'] *= 1000
+    if 'dur' in event:
+      event['dur'] *= 1000
+    return event
+
+  def CheckTrack(self, timestamp, names):
+    self.assertEqual(
+        set((e.args['name'] for e in self.track.EventsAt(timestamp))),
+        set(names))
+
+  def CheckIntervals(self, events):
+    """All tests should produce the following sequence of intervals, each
+    identified by a 'name' in the event args.
+
+    Timestamp
+    3    |      A
+    4    |
+    5    | |    B
+    6    |
+    7
+    ..
+    10   | |    C, D
+    11     |
+    12   |      E
+    13   | |    F
+    14   |
+    """
+    self.track.Handle('Tracing.dataCollected',
+                      {'params': {'value': [self.EventToMicroseconds(e)
+                                            for e in events]}})
+    self.CheckTrack(0, '')
+    self.CheckTrack(2, '')
+    self.CheckTrack(3, 'A')
+    self.CheckTrack(4, 'A')
+    self.CheckTrack(5, 'AB')
+    self.CheckTrack(6, 'A')
+    self.CheckTrack(7, '')
+    self.CheckTrack(9, '')
+    self.CheckTrack(10, 'CD')
+    self.CheckTrack(11, 'D')
+    self.CheckTrack(12, 'E')
+    self.CheckTrack(13, 'EF')
+    self.CheckTrack(14, 'E')
+    self.CheckTrack(15, '')
+    self.CheckTrack(100, '')
+
+  def testComplete(self):
+    # These are deliberately out of order.
+    self.CheckIntervals([
+        {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+        {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
+        {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
+        {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
+        {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
+        {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}])
+
+  def testDuration(self):
+    self.CheckIntervals([
+        {'ts': 3, 'ph': 'B', 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'B', 'args': {'name': 'B'}},
+        {'ts': 6, 'ph': 'E'},
+        {'ts': 7, 'ph': 'E'},
+        # Since async intervals aren't named and must be nested, we fudge the
+        # beginning of D by a tenth to ensure it's consistently detected as the
+        # outermost event.
+        {'ts': 9.9, 'ph': 'B', 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'B', 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'E'},
+        # End of D. As end times are exclusive this should not conflict with the
+        # start of E.
+        {'ts': 12, 'ph': 'E'},
+        {'ts': 12, 'ph': 'B', 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'B', 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'E'},
+        {'ts': 15, 'ph': 'E'}])
+
+  def testBadDurationExtraBegin(self):
+    self.assertRaises(devtools_monitor.DevToolsConnectionException,
+                      self.CheckIntervals,
+                      [{'ts': 3, 'ph': 'B'},
+                       {'ts': 4, 'ph': 'B'},
+                       {'ts': 5, 'ph': 'E'}])
+
+  def testBadDurationExtraEnd(self):
+    self.assertRaises(devtools_monitor.DevToolsConnectionException,
+                      self.CheckIntervals,
+                      [{'ts': 3, 'ph': 'B'},
+                       {'ts': 4, 'ph': 'E'},
+                       {'ts': 5, 'ph': 'E'}])
+
+  def testAsync(self):
+    self.CheckIntervals([
+        # A, B and F have the same category/id (so that A & B nest); C-E do not.
+        {'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'B'}},
+        # Not indexable.
+        {'ts': 4, 'ph': 'n', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 1},
+        {'ts': 7, 'ph': 'e', 'cat': 'A', 'id': 1},
+        {'ts': 10, 'ph': 'b', 'cat': 'B', 'id': 2, 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'b', 'cat': 'B', 'id': 3, 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'e', 'cat': 'B', 'id': 3},
+        {'ts': 12, 'ph': 'e', 'cat': 'B', 'id': 2},
+        {'ts': 12, 'ph': 'b', 'cat': 'A', 'id': 2, 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'e', 'cat': 'A', 'id': 1},
+        {'ts': 15, 'ph': 'e', 'cat': 'A', 'id': 2}])
+
+  def testBadAsyncIdMismatch(self):
+    self.assertRaises(
+        devtools_monitor.DevToolsConnectionException,
+        self.CheckIntervals,
+        [{'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+         {'ts': 5, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'B'}},
+         {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 2},
+         {'ts': 7, 'ph': 'e', 'cat': 'A', 'id': 1}])
+
+  def testBadAsyncExtraBegin(self):
+    self.assertRaises(
+        devtools_monitor.DevToolsConnectionException,
+        self.CheckIntervals,
+        [{'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+         {'ts': 5, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'B'}},
+         {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 1}])
+
+  def testBadAsyncExtraEnd(self):
+    self.assertRaises(
+        devtools_monitor.DevToolsConnectionException,
+        self.CheckIntervals,
+        [{'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+         {'ts': 5, 'ph': 'e', 'cat': 'A', 'id': 1},
+         {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 1}])
+
+  def testObject(self):
+    # A and E share ids, which is okay as their scopes are disjoint.
+    self.CheckIntervals([
+        {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'N', 'id': 2, 'args': {'name': 'B'}},
+        {'ts': 6, 'ph': 'D', 'id': 2},
+        {'ts': 6, 'ph': 'O', 'id': 2},  #  Ignored.
+        {'ts': 7, 'ph': 'D', 'id': 1},
+        {'ts': 10, 'ph': 'N', 'id': 3, 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'N', 'id': 4, 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'D', 'id': 4},
+        {'ts': 12, 'ph': 'D', 'id': 3},
+        {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'N', 'id': 5, 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'D', 'id': 5},
+        {'ts': 15, 'ph': 'D', 'id': 1}])
+
+  def testMixed(self):
+    # A and E are objects, B complete, D a duration, and C and F async.
+    self.CheckIntervals([
+        {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+        {'ts': 7, 'ph': 'D', 'id': 1},
+        {'ts': 10, 'ph': 'B', 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'b', 'cat': 'X', 'id': 1, 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'e', 'cat': 'X', 'id': 1},
+        {'ts': 12, 'ph': 'E'},
+        {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'b', 'cat': 'X', 'id': 2, 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
+        {'ts': 15, 'ph': 'D', 'id': 1}])
+
+
+if __name__ == '__main__':
+  unittest.main()

commit a3f2f68229bbc4266c650b348741b57a35e54e0f
Author: lizeb <lizeb@chromium.org>
Date:   Mon Jan 18 09:41:36 2016 -0800

    tools/android/loading: Separate and improve PageTrack.
    
    Also add GetContentType() and IsDataRequest() to Request.
    
    Review URL: https://codereview.chromium.org/1606523003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370035}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 36937af36f5423184a35e07f89daef5a3e974609

diff --git a/loading/page_track.py b/loading/page_track.py
new file mode 100644
index 0000000..bb289db
--- /dev/null
+++ b/loading/page_track.py
@@ -0,0 +1,62 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import devtools_monitor
+
+
+class PageTrack(devtools_monitor.Track):
+  """Records the events from the page track."""
+  _METHODS = ('Page.frameStartedLoading', 'Page.frameStoppedLoading',
+              'Page.frameAttached')
+  def __init__(self, connection):
+    super(PageTrack, self).__init__(connection)
+    self._connection = connection
+    self._events = []
+    self._pending_frames = set()
+    self._known_frames = set()
+    self._main_frame_id = None
+    if self._connection:
+      for method in PageTrack._METHODS:
+        self._connection.RegisterListener(method, self)
+
+  def Handle(self, method, msg):
+    assert method in PageTrack._METHODS
+    params = msg['params']
+    frame_id = params['frameId']
+    should_stop = False
+    event = {'method': method, 'frame_id': frame_id}
+    if method == 'Page.frameStartedLoading':
+      if self._main_frame_id is None:
+        self._main_frame_id = params['frameId']
+      self._pending_frames.add(frame_id)
+      self._known_frames.add(frame_id)
+    elif method == 'Page.frameStoppedLoading':
+      assert frame_id in self._pending_frames
+      self._pending_frames.remove(frame_id)
+      if frame_id == self._main_frame_id:
+        should_stop = True
+    elif method == 'Page.frameAttached':
+      self._known_frames.add(frame_id)
+      parent_frame = params['parentFrameId']
+      assert parent_frame in self._known_frames
+      event['parent_frame_id'] = parent_frame
+    self._events.append(event)
+    if should_stop and self._connection:
+      self._connection.StopMonitoring()
+
+  def GetEvents(self):
+    #TODO(lizeb): Add more checks here (child frame stops loading before parent,
+    #for instance).
+    return self._events
+
+  def ToJsonDict(self):
+    return {'events': [event for event in self._events]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    result = PageTrack(None)
+    events = [event for event in json_dict['events']]
+    result._events = events
+    return result
diff --git a/loading/page_track_unittest.py b/loading/page_track_unittest.py
new file mode 100644
index 0000000..6757a01
--- /dev/null
+++ b/loading/page_track_unittest.py
@@ -0,0 +1,58 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import devtools_monitor
+from page_track import PageTrack
+
+class MockDevToolsConnection(object):
+  def __init__(self):
+    self.stop_has_been_called = False
+
+  def RegisterListener(self, name, listener):
+    pass
+
+  def StopMonitoring(self):
+    self.stop_has_been_called = True
+
+
+class PageTrackTest(unittest.TestCase):
+  _EVENTS = [{'method': 'Page.frameStartedLoading',
+              'params': {'frameId': '1234.1'}},
+             {'method': 'Page.frameAttached',
+              'params': {'frameId': '1234.12', 'parentFrameId': '1234.1'}},
+             {'method': 'Page.frameStartedLoading',
+              'params': {'frameId': '1234.12'}},
+             {'method': 'Page.frameStoppedLoading',
+              'params': {'frameId': '1234.12'}},
+             {'method': 'Page.frameStoppedLoading',
+              'params': {'frameId': '1234.1'}}]
+  def testAsksMonitoringToStop(self):
+    devtools_connection = MockDevToolsConnection()
+    page_track = PageTrack(devtools_connection)
+    for msg in PageTrackTest._EVENTS[:-1]:
+      page_track.Handle(msg['method'], msg)
+      self.assertFalse(devtools_connection.stop_has_been_called)
+    msg = PageTrackTest._EVENTS[-1]
+    page_track.Handle(msg['method'], msg)
+    self.assertTrue(devtools_connection.stop_has_been_called)
+
+  def testUnknownParent(self):
+    page_track = PageTrack(None)
+    msg = {'method': 'Page.frameAttached',
+           'params': {'frameId': '1234.12', 'parentFrameId': '1234.1'}}
+    with self.assertRaises(AssertionError):
+      page_track.Handle(msg['method'], msg)
+
+  def testStopsLoadingUnknownFrame(self):
+    page_track = PageTrack(None)
+    msg = {'method': 'Page.frameStoppedLoading',
+           'params': {'frameId': '1234.12'}}
+    with self.assertRaises(AssertionError):
+      page_track.Handle(msg['method'], msg)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index de3fd11..08e1588 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -104,6 +104,17 @@ class Request(object):
       setattr(result, k, v)
     return result
 
+  def GetContentType(self):
+    """Returns the content type, or None."""
+    content_type = self.response_headers.get('Content-Type', None)
+    if not content_type or ';' not in content_type:
+      return content_type
+    else:
+      return content_type[:content_type.index(';')]
+
+  def IsDataRequest(self):
+    return self.protocol == 'data'
+
   # For testing.
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 459e2de..1e9403b 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -8,6 +8,17 @@ import unittest
 from request_track import (Request, RequestTrack, _TimingFromDict)
 
 
+class RequestTestCase(unittest.TestCase):
+  def testContentType(self):
+    r = Request()
+    r.response_headers = {}
+    self.assertEquals(None, r.GetContentType())
+    r.response_headers = {'Content-Type': 'application/javascript'}
+    self.assertEquals('application/javascript', r.GetContentType())
+    r.response_headers = {'Content-Type': 'application/javascript;bla'}
+    self.assertEquals('application/javascript', r.GetContentType())
+
+
 class RequestTrackTestCase(unittest.TestCase):
   _REQUEST_WILL_BE_SENT = {
       'method': 'Network.requestWillBeSent',
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index cb9aa7a..0b96225 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -19,46 +19,7 @@ import devil_chromium
 
 import device_setup
 import devtools_monitor
-
-
-class PageTrack(devtools_monitor.Track):
-  """Records the events from the page track."""
-  def __init__(self, connection):
-    super(PageTrack, self).__init__(connection)
-    self._connection = connection
-    self._events = []
-    self._main_frame_id = None
-    if self._connection:
-      self._connection.RegisterListener('Page.frameStartedLoading', self)
-      self._connection.RegisterListener('Page.frameStoppedLoading', self)
-
-  def Handle(self, method, msg):
-    params = msg['params']
-    frame_id = params['frameId']
-    should_stop = False
-    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
-      self._main_frame_id = params['frameId']
-    elif (method == 'Page.frameStoppedLoading'
-          and params['frameId'] == self._main_frame_id):
-      should_stop = True
-    self._events.append((method, frame_id))
-    if should_stop:
-      self._connection.StopMonitoring()
-
-  def GetEvents(self):
-    return self._events
-
-  def ToJsonDict(self):
-    return {'events': [event for event in self._events]}
-
-  @classmethod
-  def FromJsonDict(cls, json_dict):
-    assert 'events' in json_dict
-    result = PageTrack(None)
-    events = [event for event in json_dict['events']]
-    result._events = events
-    return result
-
+import page_track
 
 class AndroidTraceRecorder(object):
   """Records a loading trace."""
@@ -69,8 +30,7 @@ class AndroidTraceRecorder(object):
 
   def Go(self, connection):
     self.devtools_connection = connection
-    self.page_track = PageTrack(self.devtools_connection)
-
+    self.page_track = page_track.PageTrack(self.devtools_connection)
     self.devtools_connection.SetUpMonitoring()
     self.devtools_connection.SendAndIgnoreResponse(
         'Page.navigate', {'url': self.url})
diff --git a/loading/tracing_driver.py b/loading/tracing_driver.py
index 0996d34..c62e870 100755
--- a/loading/tracing_driver.py
+++ b/loading/tracing_driver.py
@@ -19,7 +19,7 @@ from devil.android import device_utils
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import device_setup
-import trace_recorder
+import page_track
 import tracing
 
 
@@ -37,7 +37,7 @@ def main():
        file(args.output + '.page', 'w') as page_output, \
        device_setup.DeviceConnection(device) as connection:
     track = tracing.TracingTrack(connection, fetch_stream=False)
-    page = trace_recorder.PageTrack(connection)
+    page = page_track.PageTrack(connection)
     connection.SetUpMonitoring()
     connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
     connection.StartMonitoring()

commit cd86311f4fd2e04e16555090b0102711810f1135
Author: mattcary <mattcary@chromium.org>
Date:   Mon Jan 18 07:42:51 2016 -0800

    Tracing tack for devtools monitor.
    
    This just dumps tracing events, and doesn't do any processing of those events yet.
    
    I experimented with fetching trace data as a stream. It appears to not be
    beneficial but I have left the option in. I also refactored some of the device
    setup work to make it easier to switch between local and device debugging,
    changing devtools ports, etc. Note that I also changed the FlagChanger context
    to make it more explicit what's going on w/rt adding or replacing flags.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1589843002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370028}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fe374e019637b02c4e2540f2b43e1cf87122ccde

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 24becb7..4d5d40b 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -17,17 +17,21 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 from pylib import flag_changer
 
+import devtools_monitor
+
 DEVTOOLS_PORT = 9222
 DEVTOOLS_HOSTNAME = 'localhost'
+DEFAULT_CHROME_PACKAGE = 'chrome'
 
 @contextlib.contextmanager
-def FlagChanger(device, command_line_path, new_flags):
-  """Changes the flags in a context, restores them afterwards.
+def FlagReplacer(device, command_line_path, new_flags):
+  """Replaces chrome flags in a context, restores them afterwards.
 
   Args:
-    device: Device to target, from DeviceUtils.
+    device: Device to target, from DeviceUtils. Can be None, in which case this
+      context manager is a no-op.
     command_line_path: Full path to the command-line file.
-    new_flags: Flags to add.
+    new_flags: Flags to replace.
   """
   # If we're logging requests from a local desktop chrome instance there is no
   # device.
@@ -35,7 +39,7 @@ def FlagChanger(device, command_line_path, new_flags):
     yield
     return
   changer = flag_changer.FlagChanger(device, command_line_path)
-  changer.AddFlags(new_flags)
+  changer.ReplaceFlags(new_flags)
   try:
     yield
   finally:
@@ -63,28 +67,30 @@ def _SetUpDevice(device, package_info):
   device.KillAll(package_info.package, quiet=True)
 
 
-def SetUpAndExecute(device, package, fn):
-  """Start logging process.
+@contextlib.contextmanager
+def DeviceConnection(device,
+                     package=DEFAULT_CHROME_PACKAGE,
+                     hostname=DEVTOOLS_HOSTNAME,
+                     port=DEVTOOLS_PORT):
+  """Context for starting recording on a device.
 
-  Sets up any device and tracing appropriately and then executes the core
-  logging function.
+  Sets up and restores any device and tracing appropriately
 
   Args:
-    device: Android device, or None for a local run.
+    device: Android device, or None for a local run (in which case chrome needs
+      to have been started with --remote-debugging-port=XXX).
     package: the key for chrome package info.
-    fn: the function to execute that launches chrome and performs the
-        appropriate instrumentation, see _Log*Internal().
 
   Returns:
-    As fn() returns.
+    A context manager type which evaluates to a DevToolsConnection.
   """
   package_info = constants.PACKAGE_INFO[package]
   command_line_path = '/data/local/chrome-command-line'
   new_flags = ['--enable-test-events',
-               '--remote-debugging-port=%d' % DEVTOOLS_PORT]
+               '--remote-debugging-port=%d' % port]
   if device:
     _SetUpDevice(device, package_info)
-  with FlagChanger(device, command_line_path, new_flags):
+  with FlagReplacer(device, command_line_path, new_flags):
     if device:
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
@@ -92,6 +98,25 @@ def SetUpAndExecute(device, package, fn):
       device.StartActivity(start_intent, blocking=True)
       time.sleep(2)
     # If no device, we don't care about chrome startup so skip the about page.
-    with ForwardPort(device, 'tcp:%d' % DEVTOOLS_PORT,
+    with ForwardPort(device, 'tcp:%d' % port,
                      'localabstract:chrome_devtools_remote'):
-      return fn()
+      yield devtools_monitor.DevToolsConnection(hostname, port)
+
+
+def SetUpAndExecute(device, package, fn):
+  """Start logging process.
+
+  Wrapper for DeviceConnection for those functionally inclined.
+
+  Args:
+    device: Android device, or None for a local run.
+    package: the key for chrome package info.
+    fn: the function to execute that launches chrome and performs the
+        appropriate instrumentation. The function will receive a
+        DevToolsConnection as its sole parameter.
+
+  Returns:
+    As fn() returns.
+  """
+  with DeviceConnection(device, package) as connection:
+    return fn(connection)
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index fc652d2..5e8470b 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -24,9 +24,62 @@ class DevToolsConnectionException(Exception):
     logging.warning("DevToolsConnectionException: " + message)
 
 
+# Taken from telemetry.internal.backends.chrome_inspector.tracing_backend.
+# TODO(mattcary): combine this with the above and export?
+class _StreamReader(object):
+  def __init__(self, inspector, stream_handle):
+    self._inspector_websocket = inspector
+    self._handle = stream_handle
+    self._callback = None
+    self._data = None
+
+  def Read(self, callback):
+    # Do not allow the instance of this class to be reused, as
+    # we only read data sequentially at the moment, so a stream
+    # can only be read once.
+    assert not self._callback
+    self._data = []
+    self._callback = callback
+    self._ReadChunkFromStream()
+    # Queue one extra read ahead to avoid latency.
+    self._ReadChunkFromStream()
+
+  def _ReadChunkFromStream(self):
+    # Limit max block size to avoid fragmenting memory in sock.recv(),
+    # (see https://github.com/liris/websocket-client/issues/163 for details)
+    req = {'method': 'IO.read', 'params': {
+        'handle': self._handle, 'size': 32768}}
+    self._inspector_websocket.AsyncRequest(req, self._GotChunkFromStream)
+
+  def _GotChunkFromStream(self, response):
+    # Quietly discard responses from reads queued ahead after EOF.
+    if self._data is None:
+      return
+    if 'error' in response:
+      raise DevToolsConnectionException(
+          'Reading trace failed: %s' % response['error']['message'])
+    result = response['result']
+    self._data.append(result['data'])
+    if not result.get('eof', False):
+      self._ReadChunkFromStream()
+      return
+    req = {'method': 'IO.close', 'params': {'handle': self._handle}}
+    self._inspector_websocket.SendAndIgnoreResponse(req)
+    trace_string = ''.join(self._data)
+    self._data = None
+    self._callback(trace_string)
+
+
 class DevToolsConnection(object):
   """Handles the communication with a DevTools server.
   """
+  TRACING_DOMAIN = 'Tracing'
+  TRACING_END_METHOD = 'Tracing.end'
+  TRACING_DATA_METHOD = 'Tracing.dataCollected'
+  TRACING_DONE_EVENT = 'Tracing.tracingComplete'
+  TRACING_STREAM_EVENT = 'Tracing.tracingComplete'  # Same as TRACING_DONE.
+  TRACING_TIMEOUT = 300
+
   def __init__(self, hostname, port):
     """Initializes the connection with a DevTools server.
 
@@ -35,8 +88,11 @@ class DevToolsConnection(object):
       port: port number.
     """
     self._ws = self._Connect(hostname, port)
-    self._listeners = {}
+    self._event_listeners = {}
+    self._domain_listeners = {}
     self._domains_to_enable = set()
+    self._tearing_down_tracing = False
+    self._set_up = False
     self._please_stop = False
 
   def RegisterListener(self, name, listener):
@@ -45,12 +101,16 @@ class DevToolsConnection(object):
     Also takes care of enabling the relevant domain before starting monitoring.
 
     Args:
-      name: (str) Event the listener wants to listen to, e.g.
-            Network.requestWillBeSent.
+      name: (str) Domain or event the listener wants to listen to, e.g.
+            "Network.requestWillBeSent" or "Tracing".
       listener: (Listener) listener instance.
     """
-    domain = name[:name.index('.')]
-    self._listeners[name] = listener
+    if '.' in name:
+      domain = name[:name.index('.')]
+      self._event_listeners[name] = listener
+    else:
+      domain = name
+      self._domain_listeners[domain] = listener
     self._domains_to_enable.add(domain)
 
   def UnregisterListener(self, listener):
@@ -59,10 +119,14 @@ class DevToolsConnection(object):
     Args:
       listener: (Listener) listener to unregister.
     """
-    keys = [k for (k, v) in self._listeners if v is listener]
+    keys = ([k for k, l in self._event_listeners if l is listener] +
+            [k for k, l in self._domain_listeners if l is listener])
     assert keys, "Removing non-existent listener"
     for key in keys:
-      del(self._listeners[key])
+      if key in self._event_listeners:
+        del(self._event_listeners[key])
+      if key in self._domain_listeners:
+        del(self._domain_listeners[key])
 
   def SyncRequest(self, method, params=None):
     """Issues a synchronous request to the DevTools server.
@@ -91,41 +155,103 @@ class DevToolsConnection(object):
       request['params'] = params
     self._ws.SendAndIgnoreResponse(request)
 
+  def SyncRequestNoResponse(self, method, params=None):
+    """As SyncRequest, but asserts that no meaningful response was received.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Optional parameters to the request.
+    """
+    result = self.SyncRequest(method, params)
+    if 'error' in result or ('result' in result and
+                             result['result']):
+      raise DevToolsConnectionException(
+          'Unexpected response for %s: %s' % (method, result))
+
   def SetUpMonitoring(self):
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
-      self.SyncRequest('%s.enable' % domain)
+      if domain != self.TRACING_DOMAIN:
+        self.SyncRequestNoResponse('%s.enable' % domain)
+        # Tracing setup must be done by the tracing track to control filtering
+        # and output.
+    self._tearing_down_tracing = False
+    self._set_up = True
 
   def StartMonitoring(self):
     """Starts monitoring.
 
     DevToolsConnection.SetUpMonitoring() has to be called first.
     """
-    while not self._please_stop:
-      try:
-        self._ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException:
-        break
-    if not self._please_stop:
-      logging.warning('Monitoring stopped on a timeout.')
+    assert self._set_up, 'DevToolsConnection.SetUpMonitoring not called.'
+    self._Dispatch()
     self._TearDownMonitoring()
 
   def StopMonitoring(self):
     """Stops the monitoring."""
     self._please_stop = True
 
+  def _Dispatch(self, kind='Monitoring', timeout=10):
+    self._please_stop = False
+    while not self._please_stop:
+      try:
+        self._ws.DispatchNotifications(timeout=timeout)
+      except websocket.WebSocketTimeoutException:
+        break
+    if not self._please_stop:
+      logging.warning('%s stopped on a timeout.' % kind)
+
   def _TearDownMonitoring(self):
+    if self.TRACING_DOMAIN in self._domains_to_enable:
+      logging.info('Fetching tracing')
+      self.SyncRequestNoResponse(self.TRACING_END_METHOD)
+      self._tearing_down_tracing = True
+      self._Dispatch(kind='Tracing', timeout=self.TRACING_TIMEOUT)
     for domain in self._domains_to_enable:
-      self.SyncRequest('%s.disable' % domain)
+      if domain != self.TRACING_DOMAIN:
+        self.SyncRequest('%s.disable' % domain)
       self._ws.UnregisterDomain(domain)
     self._domains_to_enable.clear()
-    self._listeners.clear()
+    self._domain_listeners.clear()
+    self._event_listeners.clear()
 
   def _OnDataReceived(self, msg):
-    method = msg.get('method', None)
-    if method not in self._listeners:
+    if 'method' not in msg:
+      raise DevToolsConnectionException('Malformed message: %s' % msg)
+    method = msg['method']
+    domain = method[:method.index('.')]
+
+    if self._tearing_down_tracing and method == self.TRACING_STREAM_EVENT:
+      stream_handle = msg.get('params', {}).get('stream')
+      if not stream_handle:
+        self._tearing_down_tracing = False
+        self.StopMonitoring()
+        # Fall through to regular dispatching.
+      else:
+        _StreamReader(self._ws, stream_handle).Read(self._TracingStreamDone)
+        # Skip regular dispatching.
+        return
+
+    if (method not in self._event_listeners and
+        domain not in self._domain_listeners):
       return
-    self._listeners[method].Handle(method, msg)
+    if method in self._event_listeners:
+      self._event_listeners[method].Handle(method, msg)
+    if domain in self._domain_listeners:
+      self._domain_listeners[domain].Handle(method, msg)
+    if self._tearing_down_tracing and method == self.TRACING_DONE_EVENT:
+      self._tearing_down_tracing = False
+      self.StopMonitoring()
+
+  def _TracingStreamDone(self, data):
+    tracing_events = json.loads(data)
+    for evt in tracing_events:
+      self._OnDataReceived({'method': self.TRACING_DATA_METHOD,
+                            'params': {'value': [evt]}})
+      if self._please_stop:
+        break
+    self._tearing_down_tracing = False
+    self.StopMonitoring()
 
   @classmethod
   def _GetWebSocketUrl(cls, hostname, port):
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 6e638c4..cb9aa7a 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -24,7 +24,7 @@ import devtools_monitor
 class PageTrack(devtools_monitor.Track):
   """Records the events from the page track."""
   def __init__(self, connection):
-    super(PageTrack, self).__init__()
+    super(PageTrack, self).__init__(connection)
     self._connection = connection
     self._events = []
     self._main_frame_id = None
@@ -67,9 +67,8 @@ class AndroidTraceRecorder(object):
     self.devtools_connection = None
     self.page_track = None
 
-  def Go(self):
-    self.devtools_connection = devtools_monitor.DevToolsConnection(
-        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
+  def Go(self, connection):
+    self.devtools_connection = connection
     self.page_track = PageTrack(self.devtools_connection)
 
     self.devtools_connection.SetUpMonitoring()
diff --git a/loading/trace_to_chrome_trace.py b/loading/trace_to_chrome_trace.py
new file mode 100755
index 0000000..998614f
--- /dev/null
+++ b/loading/trace_to_chrome_trace.py
@@ -0,0 +1,23 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Convert trace output for Chrome.
+
+Take the tracing track output from tracing_driver.py to a zip'd json that can be
+loading by chrome devtools tracing.
+"""
+
+import argparse
+import gzip
+import json
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument('input')
+  parser.add_argument('output')
+  args = parser.parse_args()
+  with gzip.GzipFile(args.output, 'w') as output_f, file(args.input) as input_f:
+    events = json.load(input_f)
+    json.dump({'traceEvents': events, 'metadata': {}}, output_f)
diff --git a/loading/tracing.py b/loading/tracing.py
new file mode 100644
index 0000000..83ad38c
--- /dev/null
+++ b/loading/tracing.py
@@ -0,0 +1,38 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Monitor tracing events on chrome via chrome remote debugging."""
+
+import devtools_monitor
+
+class TracingTrack(devtools_monitor.Track):
+  def __init__(self, connection, categories=None, fetch_stream=False):
+    """Initialize this TracingTrack.
+
+    Args:
+      connection: a DevToolsConnection.
+      categories: None, or a string, or list of strings, of tracing categories
+        to filter.
+
+      fetch_stream: if true, use a websocket stream to fetch tracing data rather
+        than dataCollected events. It appears based on very limited testing that
+        a stream is slower than the default reporting as dataCollected events.
+    """
+    super(TracingTrack, self).__init__(connection)
+    connection.RegisterListener('Tracing.dataCollected', self)
+    params = {}
+    if categories:
+      params['categories'] = (categories if type(categories) is str
+                              else ','.join(categories))
+    if fetch_stream:
+      params['transferMode'] = 'ReturnAsStream'
+
+    connection.SyncRequestNoResponse('Tracing.start', params)
+    self._events = []
+
+  def Handle(self, method, event):
+    self._events.append(event)
+
+  def GetEvents(self):
+    return self._events
diff --git a/loading/tracing_driver.py b/loading/tracing_driver.py
new file mode 100755
index 0000000..0996d34
--- /dev/null
+++ b/loading/tracing_driver.py
@@ -0,0 +1,49 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Drive TracingConnection"""
+
+import argparse
+import json
+import logging
+import os.path
+import sys
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import device_setup
+import trace_recorder
+import tracing
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--url', required=True)
+  parser.add_argument('--output', required=True)
+  args = parser.parse_args()
+  url = args.url
+  if not url.startswith('http'):
+    url = 'http://' + url
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  with file(args.output, 'w') as output, \
+       file(args.output + '.page', 'w') as page_output, \
+       device_setup.DeviceConnection(device) as connection:
+    track = tracing.TracingTrack(connection, fetch_stream=False)
+    page = trace_recorder.PageTrack(connection)
+    connection.SetUpMonitoring()
+    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+    connection.StartMonitoring()
+    json.dump(page.GetEvents(), page_output, sort_keys=True, indent=2)
+    json.dump(track.GetEvents(), output, sort_keys=True, indent=2)
+
+
+if __name__ == '__main__':
+  main()

commit 2ca96ddd2444077f79ef67b5c89c6bb50fa0bbe0
Author: lizeb <lizeb@chromium.org>
Date:   Mon Jan 18 05:36:37 2016 -0800

    tools/android/loading: Add support for Serializing tracks to JSON.
    
    Review URL: https://codereview.chromium.org/1596293004
    
    Cr-Original-Commit-Position: refs/heads/master@{#370019}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d5a762d45595f0725056e5bee6b77c52098a97ee

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index d4847cd..fc652d2 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -173,3 +173,25 @@ class Track(Listener):
   def GetEvents(self):
     """Returns a list of collected events, finalizing the state if necessary."""
     pass
+
+  def ToJsonDict(self):
+    """Serializes to a dictionary, to be dumped as JSON.
+
+    Returns:
+      A dict that can be dumped by the json module, and loaded by
+      FromJsonDict().
+    """
+    pass
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns a Track instance constructed from data dumped by
+       Track.ToJsonDict().
+
+    Args:
+      json_data: (dict) Parsed from a JSON file using the json module.
+
+    Returns:
+      a Track instance.
+    """
+    pass
diff --git a/loading/request_track.py b/loading/request_track.py
index 995c574..de3fd11 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -94,16 +94,20 @@ class Request(object):
     request_time = self.timing.request_time
     return (timestamp - request_time) * 1000
 
-  def ToDict(self):
+  def ToJsonDict(self):
     return copy.deepcopy(self.__dict__)
 
   @classmethod
-  def FromDict(cls, data_dict):
+  def FromJsonDict(cls, data_dict):
     result = Request()
     for (k, v) in data_dict.items():
       setattr(result, k, v)
     return result
 
+  # For testing.
+  def __eq__(self, o):
+    return self.__dict__ == o.__dict__
+
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
@@ -135,6 +139,20 @@ class RequestTrack(devtools_monitor.Track):
                       % len(self._requests_in_flight))
     return self._requests
 
+  def ToJsonDict(self):
+    if self._requests_in_flight:
+      logging.warning('Requests in flight, will be ignored in the dump')
+    return {'events': [request.ToJsonDict() for request in self._requests]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    result = RequestTrack(None)
+    requests = [Request.FromJsonDict(request)
+                for request in json_dict['events']]
+    result._requests = requests
+    return result
+
   def _RequestWillBeSent(self, request_id, params):
     # Several "requestWillBeSent" events can be dispatched in a row in the case
     # of redirects.
@@ -239,6 +257,9 @@ class RequestTrack(devtools_monitor.Track):
     self._completed_requests_by_id[request_id] = request
     self._requests.append(request)
 
+  def __eq__(self, o):
+    return self._requests == o._requests
+
 
 RequestTrack._METHOD_TO_HANDLER = {
     'Network.requestWillBeSent': RequestTrack._RequestWillBeSent,
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 5a25b81..459e2de 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import json
 import unittest
 
 from request_track import (Request, RequestTrack, _TimingFromDict)
@@ -242,6 +243,17 @@ class RequestTrackTestCase(unittest.TestCase):
         RequestTrackTestCase._DATA_RECEIVED_2['params']['encodedDataLength'],
         r.data_chunks[1][1])
 
+  def testCanSerialize(self):
+    self._ValidSequence(self.request_track)
+    json_dict = self.request_track.ToJsonDict()
+    _ = json.dumps(json_dict)  # Should not raise an exception.
+
+  def testCanDeserialize(self):
+    self._ValidSequence(self.request_track)
+    json_dict = self.request_track.ToJsonDict()
+    request_track = RequestTrack.FromJsonDict(json_dict)
+    self.assertEquals(self.request_track, request_track)
+
   @classmethod
   def _ValidSequence(cls, request_track):
     request_track.Handle(
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index a3f289b..6e638c4 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -28,8 +28,9 @@ class PageTrack(devtools_monitor.Track):
     self._connection = connection
     self._events = []
     self._main_frame_id = None
-    self._connection.RegisterListener('Page.frameStartedLoading', self)
-    self._connection.RegisterListener('Page.frameStoppedLoading', self)
+    if self._connection:
+      self._connection.RegisterListener('Page.frameStartedLoading', self)
+      self._connection.RegisterListener('Page.frameStoppedLoading', self)
 
   def Handle(self, method, msg):
     params = msg['params']
@@ -47,6 +48,17 @@ class PageTrack(devtools_monitor.Track):
   def GetEvents(self):
     return self._events
 
+  def ToJsonDict(self):
+    return {'events': [event for event in self._events]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    result = PageTrack(None)
+    events = [event for event in json_dict['events']]
+    result._events = events
+    return result
+
 
 class AndroidTraceRecorder(object):
   """Records a loading trace."""

commit 1f62c12cc98cd98d0178be1e3a56d5897cfae7a5
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jan 15 02:36:58 2016 -0800

    tools/android/loading: Implement RequestTrack.
    
    Review URL: https://codereview.chromium.org/1582023002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369718}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 278b5aa86d9acb9ef99fc6cca755658b051dcf82

diff --git a/loading/request_track.py b/loading/request_track.py
new file mode 100644
index 0000000..995c574
--- /dev/null
+++ b/loading/request_track.py
@@ -0,0 +1,272 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""The request data track.
+
+When executed, parses a JSON dump of DevTools messages.
+"""
+
+import collections
+import copy
+import logging
+
+import devtools_monitor
+
+
+_TIMING_NAMES_MAPPING = {
+    'connectEnd': 'connect_end', 'connectStart': 'connect_start',
+    'dnsEnd': 'dns_end', 'dnsStart': 'dns_start', 'proxyEnd': 'proxy_end',
+    'proxyStart': 'proxy_start', 'receiveHeadersEnd': 'receive_headers_end',
+    'requestTime': 'request_time', 'sendEnd': 'send_end',
+    'sendStart': 'send_start', 'sslEnd': 'ssl_end', 'sslStart': 'ssl_start',
+    'workerReady': 'worker_ready', 'workerStart': 'worker_start',
+    'loadingFinished': 'loading_finished'}
+
+Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
+
+
+class Request(object):
+  """Represents a single request.
+
+  Generally speaking, fields here closely mirror those documented in
+  third_party/WebKit/Source/devtools/protocol.json.
+
+  Fields:
+    request_id: (str) unique request ID. Postfixed with ".redirect" for
+                redirects.
+    frame_id: (str) unique frame identifier.
+    loader_id: (str) unique frame identifier.
+    document_url: (str) URL of the document this request is loaded for.
+    url: (str) Request URL.
+    protocol: (str) protocol used for the request.
+    method: (str) HTTP method, such as POST or GET.
+    request_headers: (dict) {'header': 'value'} Request headers.
+    response_headers: (dict) {'header': 'value'} Response headers.
+    initial_priority: (str) Initial request priority, in REQUEST_PRIORITIES.
+    timestamp: (float) Request timestamp, in s.
+    wall_time: (float) Request timestamp, UTC timestamp in s.
+    initiator: (dict) Request initiator, in INITIATORS.
+    resource_type: (str) Resource type, in RESOURCE_TYPES
+    served_from_cache: (bool) Whether the request was served from cache.
+    from_disk_cache: (bool) Whether the request was served from the disk cache.
+    from_service_worker: (bool) Whether the request was served by a Service
+                         Worker.
+    timing: (Timing) Request timing, extended with loading_finished.
+    status: (int) Response status code.
+    encoded_data_length: (int) Total encoded data length.
+    data_chunks: (list) [(offset, encoded_data_length), ...] List of data
+                 chunks received, with their offset in ms relative to
+                 Timing.requestTime.
+    failed: (bool) Whether the request failed.
+  """
+  REQUEST_PRIORITIES = ('VeryLow', 'Low', 'Medium', 'High', 'VeryHigh')
+  RESOURCE_TYPES = ('Document', 'Stylesheet', 'Image', 'Media', 'Font',
+                    'Script', 'TextTrack', 'XHR', 'Fetch', 'EventSource',
+                    'WebSocket', 'Manifest', 'Other')
+  INITIATORS = ('parser', 'script', 'other')
+  def __init__(self):
+    self.request_id = None
+    self.frame_id = None
+    self.loader_id = None
+    self.document_url = None
+    self.url = None
+    self.protocol = None
+    self.method = None
+    self.request_headers = None
+    self.response_headers = None
+    self.initial_priority = None
+    self.timestamp = -1
+    self.wall_time = -1
+    self.initiator = None
+    self.resource_type = None
+    self.served_from_cache = False
+    self.from_disk_cache = False
+    self.from_service_worker = False
+    self.timing = None
+    self.status = None
+    self.encoded_data_length = 0
+    self.data_chunks = []
+    self.failed = False
+
+  def _TimestampOffsetFromStartMs(self, timestamp):
+    assert self.timing.request_time != -1
+    request_time = self.timing.request_time
+    return (timestamp - request_time) * 1000
+
+  def ToDict(self):
+    return copy.deepcopy(self.__dict__)
+
+  @classmethod
+  def FromDict(cls, data_dict):
+    result = Request()
+    for (k, v) in data_dict.items():
+      setattr(result, k, v)
+    return result
+
+
+class RequestTrack(devtools_monitor.Track):
+  """Aggregates request data."""
+  # Request status
+  _STATUS_SENT = 0
+  _STATUS_RESPONSE = 1
+  _STATUS_DATA = 2
+  _STATUS_FINISHED = 3
+  _STATUS_FAILED = 4
+  def __init__(self, connection):
+    super(RequestTrack, self).__init__(connection)
+    self._connection = connection
+    self._requests = []
+    self._requests_in_flight = {}  # requestId -> (request, status)
+    self._completed_requests_by_id = {}
+    if connection:  # Optional for testing.
+      for method in RequestTrack._METHOD_TO_HANDLER:
+        self._connection.RegisterListener(method, self)
+
+  def Handle(self, method, msg):
+    assert method in RequestTrack._METHOD_TO_HANDLER
+    params = msg['params']
+    request_id = params['requestId']
+    RequestTrack._METHOD_TO_HANDLER[method](self, request_id, params)
+
+  def GetEvents(self):
+    if self._requests_in_flight:
+      logging.warning('Number of requests still in flight: %d.'
+                      % len(self._requests_in_flight))
+    return self._requests
+
+  def _RequestWillBeSent(self, request_id, params):
+    # Several "requestWillBeSent" events can be dispatched in a row in the case
+    # of redirects.
+    if request_id in self._requests_in_flight:
+      self._HandleRedirect(request_id, params)
+    assert (request_id not in self._requests_in_flight
+            and request_id not in self._completed_requests_by_id)
+    r = Request()
+    r.request_id = request_id
+    _CopyFromDictToObject(
+        params, r, (('frameId', 'frame_id'), ('loaderId', 'loader_id'),
+                    ('documentURL', 'document_url'),
+                    ('timestamp', 'timestamp'), ('wallTime', 'wall_time'),
+                    ('initiator', 'initiator')))
+    request = params['request']
+    _CopyFromDictToObject(
+        request, r, (('url', 'url'), ('method', 'method'),
+                     ('headers', 'headers'),
+                     ('initialPriority', 'initial_priority')))
+    r.resource_type = params.get('type', 'Other')
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_SENT)
+
+  def _HandleRedirect(self, request_id, params):
+    (r, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_SENT
+    # The second request contains timing information pertaining to the first
+    # one. Finalize the first request.
+    assert 'redirectResponse' in params
+    redirect_response = params['redirectResponse']
+    _CopyFromDictToObject(redirect_response, r,
+                          (('headers', 'response_headers'),
+                           ('encodedDataLength', 'encoded_data_length'),
+                           ('fromDiskCache', 'from_disk_cache')))
+    r.timing = _TimingFromDict(redirect_response['timing'])
+    r.request_id = request_id + '.redirect'
+    self._requests_in_flight[r.request_id] = (r, RequestTrack._STATUS_FINISHED)
+    del self._requests_in_flight[request_id]
+    self._FinalizeRequest(r.request_id)
+
+  def _RequestServedFromCache(self, request_id, _):
+    assert request_id in self._requests_in_flight
+    (request, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_SENT
+    request.served_from_cache = True
+
+  def _ResponseReceived(self, request_id, params):
+    assert request_id in self._requests_in_flight
+    (r, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_SENT
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
+    assert r.frame_id == params['frameId']
+    assert r.timestamp <= params['timestamp']
+    if r.resource_type == 'Other':
+      r.resource_type = params.get('type', 'Other')
+    else:
+      assert r.resource_type == params.get('type', 'Other')
+    response = params['response']
+    _CopyFromDictToObject(
+        response, r, (('status', 'status'), ('mimeType', 'mime_type'),
+                      ('fromDiskCache', 'from_disk_cache'),
+                      ('fromServiceWorker', 'from_service_worker'),
+                      ('protocol', 'protocol'),
+                      # Actual request headers are not known before reaching the
+                      # network stack.
+                      ('requestHeaders', 'request_headers'),
+                      ('headers', 'response_headers')))
+    timing_dict = response['timing'] if r.protocol != 'data' else {}
+    r.timing = _TimingFromDict(timing_dict)
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
+
+  def _DataReceived(self, request_id, params):
+    (r, status) = self._requests_in_flight[request_id]
+    assert (status == RequestTrack._STATUS_RESPONSE
+            or status == RequestTrack._STATUS_DATA)
+    offset = r._TimestampOffsetFromStartMs(params['timestamp'])
+    r.data_chunks.append((offset, params['encodedDataLength']))
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_DATA)
+
+  def _LoadingFinished(self, request_id, params):
+    assert request_id in self._requests_in_flight
+    (r, status) = self._requests_in_flight[request_id]
+    assert (status == RequestTrack._STATUS_RESPONSE
+            or status == RequestTrack._STATUS_DATA)
+    r.encoded_data_length = params['encodedDataLength']
+    r.timing = r.timing._replace(
+        loading_finished=r._TimestampOffsetFromStartMs(params['timestamp']))
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
+    self._FinalizeRequest(request_id)
+
+  def _LoadingFailed(self, request_id, _):
+    assert request_id in self._requests_in_flight
+    (r, _) = self._requests_in_flight[request_id]
+    r.failed = True
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
+    self._FinalizeRequest(request_id)
+
+  def _FinalizeRequest(self, request_id):
+    assert request_id in self._requests_in_flight
+    (request, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_FINISHED
+    del self._requests_in_flight[request_id]
+    self._completed_requests_by_id[request_id] = request
+    self._requests.append(request)
+
+
+RequestTrack._METHOD_TO_HANDLER = {
+    'Network.requestWillBeSent': RequestTrack._RequestWillBeSent,
+    'Network.requestServedFromCache': RequestTrack._RequestServedFromCache,
+    'Network.responseReceived': RequestTrack._ResponseReceived,
+    'Network.dataReceived': RequestTrack._DataReceived,
+    'Network.loadingFinished': RequestTrack._LoadingFinished,
+    'Network.loadingFailed': RequestTrack._LoadingFailed}
+
+
+def _TimingFromDict(timing_dict):
+  complete_timing_dict = {field: -1 for field in Timing._fields}
+  timing_dict_mapped = {
+      _TIMING_NAMES_MAPPING[k]: v for (k, v) in timing_dict.items()}
+  complete_timing_dict.update(timing_dict_mapped)
+  return Timing(**complete_timing_dict)
+
+
+def _CopyFromDictToObject(d, o, key_attrs):
+  for (key, attr) in key_attrs:
+    setattr(o, attr, d[key])
+
+
+if __name__ == '__main__':
+  import json
+  import sys
+  events = json.load(open(sys.argv[1], 'r'))
+  request_track = RequestTrack(None)
+  for event in events:
+    event_method = event['method']
+    request_track.Handle(event_method, event)
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
new file mode 100644
index 0000000..5a25b81
--- /dev/null
+++ b/loading/request_track_unittest.py
@@ -0,0 +1,255 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from request_track import (Request, RequestTrack, _TimingFromDict)
+
+
+class RequestTrackTestCase(unittest.TestCase):
+  _REQUEST_WILL_BE_SENT = {
+      'method': 'Network.requestWillBeSent',
+      'params': {
+          'documentURL': 'http://example.com/',
+          'frameId': '32493.1',
+          'initiator': {
+              'type': 'other'
+              },
+          'loaderId': '32493.3',
+          'request': {
+              'headers': {
+                  'Accept': 'text/html',
+                  'Upgrade-Insecure-Requests': '1',
+                  'User-Agent': 'Mozilla/5.0'
+                  },
+              'initialPriority': 'VeryHigh',
+              'method': 'GET',
+              'mixedContentType': 'none',
+              'url': 'http://example.com/'
+              },
+          'requestId': '32493.1',
+          'timestamp': 5571441.535053,
+          'type': 'Document',
+          'wallTime': 1452691674.08878}}
+  _REDIRECT = {
+      'method': 'Network.requestWillBeSent',
+      'params': {
+          'documentURL': 'http://www.example.com/',
+          'frameId': '32493.1',
+          'initiator': {
+              'type': 'other'
+              },
+          'loaderId': '32493.3',
+          'redirectResponse': {
+              'connectionId': 18,
+              'connectionReused': False,
+              'encodedDataLength': 198,
+              'fromDiskCache': False,
+              'fromServiceWorker': False,
+              'headers': {},
+              'headersText': 'HTTP/1.1 301 Moved Permanently\r\n',
+              'mimeType': 'text/html',
+              'protocol': 'http/1.1',
+              'remoteIPAddress': '216.146.46.10',
+              'remotePort': 80,
+              'requestHeaders': {
+                  'Accept': 'text/html',
+                  'User-Agent': 'Mozilla/5.0'
+                  },
+              'securityState': 'neutral',
+              'status': 301,
+              'statusText': 'Moved Permanently',
+              'timing': {
+                  'connectEnd': 137.435999698937,
+                  'connectStart': 51.1459996923804,
+                  'dnsEnd': 51.1459996923804,
+                  'dnsStart': 0,
+                  'proxyEnd': -1,
+                  'proxyStart': -1,
+                  'receiveHeadersEnd': 228.187000378966,
+                  'requestTime': 5571441.55002,
+                  'sendEnd': 138.841999694705,
+                  'sendStart': 138.031999580562,
+                  'sslEnd': -1,
+                  'sslStart': -1,
+                  'workerReady': -1,
+                  'workerStart': -1
+                  },
+              'url': 'http://example.com/'
+              },
+          'request': {
+              'headers': {
+                  'Accept': 'text/html',
+                  'User-Agent': 'Mozilla/5.0'
+                  },
+              'initialPriority': 'VeryLow',
+              'method': 'GET',
+              'mixedContentType': 'none',
+              'url': 'http://www.example.com/'
+              },
+          'requestId': '32493.1',
+          'timestamp': 5571441.795948,
+          'type': 'Document',
+          'wallTime': 1452691674.34968}}
+  _RESPONSE_RECEIVED = {
+      'method': 'Network.responseReceived',
+      'params': {
+          'frameId': '32493.1',
+          'loaderId': '32493.3',
+          'requestId': '32493.1',
+          'response': {
+              'connectionId': 26,
+              'connectionReused': False,
+              'encodedDataLength': -1,
+              'fromDiskCache': False,
+              'fromServiceWorker': False,
+              'headers': {
+                  'Age': '67',
+                  'Cache-Control': 'max-age=0,must-revalidate',
+                  },
+              'headersText': 'HTTP/1.1 200 OK\r\n',
+              'mimeType': 'text/html',
+              'protocol': 'http/1.1',
+              'requestHeaders': {
+                  'Accept': 'text/html',
+                    'Host': 'www.example.com',
+                    'User-Agent': 'Mozilla/5.0'
+                },
+                'status': 200,
+                'timing': {
+                    'connectEnd': 37.9800004884601,
+                    'connectStart': 26.8250005319715,
+                    'dnsEnd': 26.8250005319715,
+                    'dnsStart': 0,
+                    'proxyEnd': -1,
+                    'proxyStart': -1,
+                    'receiveHeadersEnd': 54.9750002101064,
+                    'requestTime': 5571441.798671,
+                    'sendEnd': 38.3980004116893,
+                    'sendStart': 38.1810003891587,
+                    'sslEnd': -1,
+                    'sslStart': -1,
+                    'workerReady': -1,
+                    'workerStart': -1
+                },
+                'url': 'http://www.example.com/'
+            },
+            'timestamp': 5571441.865639,
+            'type': 'Document'}}
+  _DATA_RECEIVED_1 = {
+      "method": "Network.dataReceived",
+      "params": {
+          "dataLength": 1803,
+          "encodedDataLength": 1326,
+          "requestId": "32493.1",
+          "timestamp": 5571441.867347}}
+  _DATA_RECEIVED_2 = {
+      "method": "Network.dataReceived",
+      "params": {
+          "dataLength": 32768,
+          "encodedDataLength": 32768,
+          "requestId": "32493.1",
+          "timestamp": 5571441.893121}}
+  _LOADING_FINISHED = {'method': 'Network.loadingFinished',
+                       'params': {
+                           'encodedDataLength': 101829,
+                           'requestId': '32493.1',
+                           'timestamp': 5571441.891189}}
+
+  def setUp(self):
+    self.request_track = RequestTrack(None)
+
+  def testParseRequestWillBeSent(self):
+    msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    request_id = msg['params']['requestId']
+    self.request_track.Handle('Network.requestWillBeSent', msg)
+    self.assertTrue(request_id in self.request_track._requests_in_flight)
+    (_, status) = self.request_track._requests_in_flight[request_id]
+    self.assertEquals(RequestTrack._STATUS_SENT, status)
+
+  def testRejectsUnknownMethod(self):
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle(
+          'unknown', RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+
+  def testHandleRedirect(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REDIRECT)
+    self.assertEquals(1, len(self.request_track._requests_in_flight))
+    self.assertEquals(1, len(self.request_track.GetEvents()))
+
+  def testRejectDuplicates(self):
+    msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    self.request_track.Handle('Network.requestWillBeSent', msg)
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle('Network.requestWillBeSent', msg)
+
+  def testInvalidSequence(self):
+    msg1 = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    msg2 = RequestTrackTestCase._LOADING_FINISHED
+    self.request_track.Handle('Network.requestWillBeSent', msg1)
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle('Network.loadingFinished', msg2)
+
+  def testValidSequence(self):
+    self._ValidSequence(self.request_track)
+    self.assertEquals(1, len(self.request_track.GetEvents()))
+    self.assertEquals(0, len(self.request_track._requests_in_flight))
+    r = self.request_track.GetEvents()[0]
+    self.assertEquals('32493.1', r.request_id)
+    self.assertEquals('32493.1', r.frame_id)
+    self.assertEquals('32493.3', r.loader_id)
+    self.assertEquals('http://example.com/', r.document_url)
+    self.assertEquals('http://example.com/', r.url)
+    self.assertEquals('http/1.1', r.protocol)
+    self.assertEquals('GET', r.method)
+    response = RequestTrackTestCase._RESPONSE_RECEIVED['params']['response']
+    self.assertEquals(response['requestHeaders'], r.request_headers)
+    self.assertEquals(response['headers'], r.response_headers)
+    self.assertEquals('VeryHigh', r.initial_priority)
+    request_will_be_sent = (
+        RequestTrackTestCase._REQUEST_WILL_BE_SENT['params'])
+    self.assertEquals(request_will_be_sent['timestamp'], r.timestamp)
+    self.assertEquals(request_will_be_sent['wallTime'], r.wall_time)
+    self.assertEquals(request_will_be_sent['initiator'], r.initiator)
+    self.assertEquals(request_will_be_sent['type'], r.resource_type)
+    self.assertEquals(False, r.served_from_cache)
+    self.assertEquals(False, r.from_disk_cache)
+    self.assertEquals(False, r.from_service_worker)
+    timing = _TimingFromDict(response['timing'])
+    loading_finished = RequestTrackTestCase._LOADING_FINISHED['params']
+    loading_finished_offset = r._TimestampOffsetFromStartMs(
+        loading_finished['timestamp'])
+    timing = timing._replace(loading_finished=loading_finished_offset)
+    self.assertEquals(timing, r.timing)
+    self.assertEquals(200, r.status)
+    self.assertEquals(
+        loading_finished['encodedDataLength'], r.encoded_data_length)
+    self.assertEquals(False, r.failed)
+
+  def testDataReceived(self):
+    self._ValidSequence(self.request_track)
+    self.assertEquals(1, len(self.request_track.GetEvents()))
+    r = self.request_track.GetEvents()[0]
+    self.assertEquals(2, len(r.data_chunks))
+    self.assertEquals(
+        RequestTrackTestCase._DATA_RECEIVED_1['params']['encodedDataLength'],
+        r.data_chunks[0][1])
+    self.assertEquals(
+        RequestTrackTestCase._DATA_RECEIVED_2['params']['encodedDataLength'],
+        r.data_chunks[1][1])
+
+  @classmethod
+  def _ValidSequence(cls, request_track):
+    request_track.Handle(
+        'Network.requestWillBeSent', cls._REQUEST_WILL_BE_SENT)
+    request_track.Handle('Network.responseReceived', cls._RESPONSE_RECEIVED)
+    request_track.Handle('Network.dataReceived', cls._DATA_RECEIVED_1)
+    request_track.Handle('Network.dataReceived', cls._DATA_RECEIVED_2)
+    request_track.Handle('Network.loadingFinished', cls._LOADING_FINISHED)
+
+if __name__ == '__main__':
+  unittest.main()

commit 5e74beb3ab48ee8701496d1febc33f8e27818ab1
Author: jbudorick <jbudorick@chromium.org>
Date:   Thu Jan 14 10:14:34 2016 -0800

    [Android] Port tools/android from pylib to catapult+devil.
    
    BUG=476719
    
    Review URL: https://codereview.chromium.org/1584963002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369467}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 636bdd225528b28b4605bc704e366a3d9ef08d22

diff --git a/appstats.py b/appstats.py
index ba53eac..8bb93a5 100755
--- a/appstats.py
+++ b/appstats.py
@@ -14,13 +14,15 @@ import time
 
 from operator import sub
 
-sys.path.append(os.path.join(os.path.dirname(__file__),
-                             os.pardir,
-                             os.pardir,
-                             'build',
-                             'android'))
-from pylib.device import device_errors
-from pylib.device import device_utils
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..'))
+
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil'))
+from devil.android import device_errors
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 
 class Utils(object):
   """A helper class to hold various utility methods."""
@@ -944,6 +946,8 @@ def main(argv):
   if not args.show_net and not args.show_mem:
     args.show_mem = True
 
+  devil_chromium.Initialize()
+
   curses.setupterm()
 
   printer = OutputBeautifier(not args.dull_output, not args.no_overwrite)
diff --git a/customtabs_benchmark/scripts/customtabs_benchmark.py b/customtabs_benchmark/scripts/customtabs_benchmark.py
index 9bf4343..56f48bc 100755
--- a/customtabs_benchmark/scripts/customtabs_benchmark.py
+++ b/customtabs_benchmark/scripts/customtabs_benchmark.py
@@ -13,13 +13,17 @@ import re
 import sys
 import time
 
-sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
-                             os.pardir, os.pardir, 'build', 'android'))
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..'))
 
-from pylib.device import device_errors
-from pylib.device import device_utils
-from pylib.device import intent
-from pylib.perf import cache_control
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil'))
+from devil.android import device_errors
+from devil.android import device_utils
+from devil.android.perf import cache_control
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 
 
 def RunOnce(device, url, warmup, no_prerendering, delay_to_may_launch_url,
@@ -150,6 +154,7 @@ def _CreateOptionParser():
 def main():
   parser = _CreateOptionParser()
   options, _ = parser.parse_args()
+  devil_chromium.Initialize()
   devices = device_utils.DeviceUtils.HealthyDevices()
   device = devices[0]
   if len(devices) != 1 and options.device is None:
diff --git a/customtabs_benchmark/scripts/run_benchmark.py b/customtabs_benchmark/scripts/run_benchmark.py
index 5df547b..8a3b88d 100755
--- a/customtabs_benchmark/scripts/run_benchmark.py
+++ b/customtabs_benchmark/scripts/run_benchmark.py
@@ -15,12 +15,16 @@ import random
 import sys
 import threading
 
-sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
-                             os.pardir, os.pardir, 'build', 'android'))
+import customtabs_benchmark
 
-from pylib.device import device_utils
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..', '..'))
 
-import customtabs_benchmark
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 
 
 _KEYS = ['url', 'warmup', 'no_prerendering', 'delay_to_may_launch_url',
@@ -132,6 +136,7 @@ def main():
   if options.config is None:
     logging.error('A configuration file must be provided.')
     sys.exit(0)
+  devil_chromium.Initialize()
   configs = _ParseConfiguration(options.config)
   _Run(options.output_file_prefix, configs)
 
diff --git a/mempressure.py b/mempressure.py
index f316790..7d67d9d 100755
--- a/mempressure.py
+++ b/mempressure.py
@@ -9,17 +9,18 @@ import optparse
 import os
 import sys
 
-BUILD_ANDROID_DIR = os.path.join(os.path.dirname(__file__),
-                                 os.pardir,
-                                 os.pardir,
-                                 'build',
-                                 'android')
-sys.path.append(BUILD_ANDROID_DIR)
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..'))
+
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil')
+from devil.android import device_errors
+from devil.android import device_utils
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 from pylib import constants
 from pylib import flag_changer
-from pylib.device import device_errors
-from pylib.device import device_utils
-from pylib.device import intent
 
 # Browser Constants
 DEFAULT_BROWSER = 'chrome'
@@ -82,6 +83,8 @@ def main(argv):
   if not options.browser in constants.PACKAGE_INFO.keys():
     option_parser.error('Unknown browser option ' + options.browser)
 
+  devil_chromium.Initialize()
+
   package_info = constants.PACKAGE_INFO[options.browser]
 
   package = package_info.package

commit 74844a254d5dfefeddff54bc02c91cd54bff3fd5
Author: ruuda <ruuda@google.com>
Date:   Thu Jan 14 04:06:11 2016 -0800

    [Android] Fix sign of integer literal in test
    
    BUG=577575
    TBR=digit@chromium.org
    
    Review URL: https://codereview.chromium.org/1586033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369394}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 373cde59b9b483db9e897871b7a4d332dce76920

diff --git a/heap_profiler/heap_profiler_unittest.cc b/heap_profiler/heap_profiler_unittest.cc
index 7b9aedd..893f214 100644
--- a/heap_profiler/heap_profiler_unittest.cc
+++ b/heap_profiler/heap_profiler_unittest.cc
@@ -434,8 +434,8 @@ TEST_F(HeapProfilerTest, Test64Bit) {
       (void*)0x7ffffffffffff000L, 4096, st2.frames, st2.depth, 0);
   heap_profiler_alloc(
       (void*)0xfffffffffffff000L, 4096, st3.frames, st3.depth, 0);
-  EXPECT_EQ(3, stats_.num_allocs);
-  EXPECT_EQ(3, stats_.num_stack_traces);
+  EXPECT_EQ(3u, stats_.num_allocs);
+  EXPECT_EQ(3u, stats_.num_stack_traces);
   EXPECT_EQ(4096u + 4096 + 4096, stats_.total_alloc_bytes);
 
   heap_profiler_free((void*)0x1000, 4096, NULL);

commit 5c7ace1a3db24742a3e9bf808692264d9d39ca02
Author: jbudorick <jbudorick@chromium.org>
Date:   Wed Jan 13 09:54:18 2016 -0800

    [Android] pylib -> devil in tools/android/loading/.
    
    BUG=476719
    
    Review URL: https://codereview.chromium.org/1582573004
    
    Cr-Original-Commit-Position: refs/heads/master@{#369211}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c778474f46615814646078e12715116d866484c4

diff --git a/loading/analyze.py b/loading/analyze.py
index 1a49155..3bb5890 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -13,12 +13,16 @@ import sys
 import tempfile
 import time
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
 from pylib import constants
-from pylib.device import device_utils
-from pylib.device import intent
 
 import log_parser
 import log_requests
@@ -37,7 +41,7 @@ def _SetupAndGetDevice():
   """Gets an android device, set up the way we like it.
 
   Returns:
-    An AdbWrapper for the first device found.
+    An instance of DeviceUtils for the first device found.
   """
   device = device_utils.DeviceUtils.HealthyDevices()[0]
   device.EnableRoot()
@@ -354,6 +358,7 @@ def main():
   parser.add_argument('command')
   parser.add_argument('rest', nargs=argparse.REMAINDER)
   args = parser.parse_args()
+  devil_chromium.Initialize()
   COMMAND_MAP.get(args.command,
                   lambda _: InvalidCommand(args.command))(args.rest)
 
diff --git a/loading/device_setup.py b/loading/device_setup.py
index d98014c..24becb7 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -7,13 +7,15 @@ import os
 import sys
 import time
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 from pylib import flag_changer
-from pylib.device import device_utils
-from pylib.device import intent
 
 DEVTOOLS_PORT = 9222
 DEVTOOLS_HOSTNAME = 'localhost'
diff --git a/loading/log_requests.py b/loading/log_requests.py
index ecbb0e8..ee5f091 100755
--- a/loading/log_requests.py
+++ b/loading/log_requests.py
@@ -15,16 +15,22 @@ import optparse
 import os
 import sys
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
-sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
-sys.path.append(os.path.join(file_dir, '..', '..', 'chrome_proxy'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
-from pylib.device import device_utils
-from common import inspector_network
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
+
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'telemetry'))
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'chrome_proxy'))
+from common import inspector_network
+
 import device_setup
 
 
@@ -210,11 +216,15 @@ def main():
   logging.basicConfig(level=logging.WARNING)
   parser = _CreateOptionParser()
   options, _ = parser.parse_args()
+
+  devil_chromium.Initialize()
+
   if options.local:
     device = None
   else:
     devices = device_utils.DeviceUtils.HealthyDevices()
     device = devices[0]
+
   request_logger = AndroidRequestsLogger(device)
   response_data = request_logger.LogPageLoad(
       options.url, options.clear_cache, options.package)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index f5294ce..a3f289b 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -8,10 +8,14 @@
 import os
 import sys
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
-from pylib.device import device_utils
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
 
 import device_setup
 import devtools_monitor
@@ -64,6 +68,7 @@ class AndroidTraceRecorder(object):
 
 
 def DoIt(url):
+  devil_chromium.Initialize()
   devices = device_utils.DeviceUtils.HealthyDevices()
   device = devices[0]
   trace_recorder = AndroidTraceRecorder(url)

commit 05ca4e0b1209b4d6d5be17179348c4666ab2eab0
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jan 13 03:09:25 2016 -0800

    tools/android/loading: add PRESUBMIT.py.
    
    Fix pylint errors along the way.
    
    Review URL: https://codereview.chromium.org/1581913002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369142}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b56eb89d5185ac49a7721cb5d4fcd99abeb766b3

diff --git a/loading/PRESUBMIT.py b/loading/PRESUBMIT.py
new file mode 100644
index 0000000..b9bcc19
--- /dev/null
+++ b/loading/PRESUBMIT.py
@@ -0,0 +1,34 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Top-level presubmit script for loading.
+
+See http://dev.chromium.org/developers/how-tos/depottools/presubmit-scripts
+for more details on the presubmit API built into depot_tools.
+"""
+
+
+def CommonChecks(input_api, output_api):
+  output = []
+  blacklist = []
+  output.extend(input_api.canned_checks.RunPylint(
+      input_api, output_api, black_list=blacklist))
+  output.extend(input_api.canned_checks.RunUnitTests(
+      input_api,
+      output_api,
+      [input_api.os_path.join(input_api.PresubmitLocalPath(), 'run_tests')]))
+
+  if input_api.is_committing:
+    output.extend(input_api.canned_checks.PanProjectChecks(input_api,
+                                                           output_api,
+                                                           owners_check=False))
+  return output
+
+
+def CheckChangeOnUpload(input_api, output_api):
+  return CommonChecks(input_api, output_api)
+
+
+def CheckChangeOnCommit(input_api, output_api):
+  return CommonChecks(input_api, output_api)
diff --git a/loading/dag.py b/loading/dag.py
index d9f1a47..2eacd3a 100644
--- a/loading/dag.py
+++ b/loading/dag.py
@@ -96,10 +96,7 @@ def TopologicalSort(nodes, node_filter=None):
   sorted_nodes = []
   sources = []
   remaining_in_edges = {}
-  valid_node_count = 0
   for n in nodes:
-    if node_filter(n):
-      valid_node_count += 1
     if n.Predecessors():
       remaining_in_edges[n] = len(n.Predecessors())
     elif node_filter(n):
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index f811ff1..d4847cd 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -59,7 +59,7 @@ class DevToolsConnection(object):
     Args:
       listener: (Listener) listener to unregister.
     """
-    keys = [k for (k, v) in self._listeners if k == name]
+    keys = [k for (k, v) in self._listeners if v is listener]
     assert keys, "Removing non-existent listener"
     for key in keys:
       del(self._listeners[key])
@@ -104,7 +104,7 @@ class DevToolsConnection(object):
     while not self._please_stop:
       try:
         self._ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException as e:
+      except websocket.WebSocketTimeoutException:
         break
     if not self._please_stop:
       logging.warning('Monitoring stopped on a timeout.')
diff --git a/loading/loading_model.py b/loading/loading_model.py
index 2e76384..fed48e4 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -131,7 +131,8 @@ class ResourceGraph(object):
     node_filter = self._node_filter if node_filter is None else node_filter
     total = 0
     for n in self._node_info:
-      if not node_filter(n.Node()): continue
+      if not node_filter(n.Node()):
+        continue
       for s in n.Node().Successors():
         if node_filter(s):
           total += self._EdgeCost(n.Node(), s)
@@ -391,7 +392,7 @@ class ResourceGraph(object):
       new_parent.Node().AddSuccessor(self.Node())
       new_parent.SetEdgeCost(self, edge_cost)
       for a in edge_annotations:
-        new_parent.AddEdgeAnnotation(self, edge_annotations)
+        new_parent.AddEdgeAnnotation(self, a)
 
     def __eq__(self, o):
       return self.Node().Index() == o.Node().Index()
@@ -481,12 +482,13 @@ class ResourceGraph(object):
                 and urlparse.urlparse(r.url).hostname == request_hostname)]
         most_recent = None
         # Linear search is bad, but this shouldn't matter here.
-        for request in sorted_script_requests_from_hostname:
-          if request.timestamp < r.timing.requestTime:
-            most_recent = request
+        for r in sorted_script_requests_from_hostname:
+          if r.timestamp < request.timing.requestTime:
+            most_recent = r
           else:
             break
         if most_recent is not None:
+          url = most_recent.url
           if url in indicies_by_url:
             predecessor_url = url
             predecessor_type = 'script_inferred'
@@ -495,7 +497,8 @@ class ResourceGraph(object):
         predecessor = self._FindBestPredecessor(
             current_node_info, indicies_by_url[predecessor_url])
         edge_cost = current_node_info.StartTime() - predecessor.EndTime()
-        if edge_cost < 0: edge_cost = 0
+        if edge_cost < 0:
+          edge_cost = 0
         if current_node_info.StartTime() < predecessor.StartTime():
           logging.error('Inverted dependency: %s->%s',
                         predecessor.ShortName(), current_node_info.ShortName())
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index bdc3915..970f00a 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -144,7 +144,8 @@ class LoadingModelTestCase(unittest.TestCase):
         'http://afae61024b33032ef.profile.sfo20.cloudfront.net/tst.png'))
 
     self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://ums.adtechus.com/mapuser?providerid=1003;userid=RUmecco4z3o===='))
+        'http://ums.adtechus.com/mapuser?providerid=1003;'
+        'userid=RUmecco4z3o===='))
     self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
         'http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'))
 
diff --git a/loading/processing.py b/loading/processing.py
index a6ddde1..1f48939 100644
--- a/loading/processing.py
+++ b/loading/processing.py
@@ -10,25 +10,26 @@ import log_parser
 import loading_model
 
 
-def SitesFromDir(dir):
-  """Extract sites from a data dir.
+def SitesFromDir(directory):
+  """Extract sites from a data directory.
 
   Based on ./analyze.py fetch file name conventions. We assume each site
   corresponds to two files, <site>.json and <site>.json.cold, and that no other
   kind of file appears in the data directory.
 
   Args:
-    dir: the directory to process.
+    directory: the directory to process.
 
   Returns:
     A list of sites as strings.
 
   """
-  files = set(os.listdir(dir))
+  files = set(os.listdir(directory))
   assert files
   sites = []
   for f in files:
-    if f.endswith('.png'): continue
+    if f.endswith('.png'):
+      continue
     assert f.endswith('.json') or f.endswith('.json.cold'), f
     if f.endswith('.json'):
       assert f + '.cold' in files
diff --git a/loading/run_tests b/loading/run_tests
new file mode 100755
index 0000000..7f182a7
--- /dev/null
+++ b/loading/run_tests
@@ -0,0 +1,26 @@
+#!/usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+import os
+import sys
+import unittest
+
+
+if __name__ == '__main__':
+  logging.basicConfig(
+      level=logging.DEBUG if '-v' in sys.argv else logging.WARNING,
+      format='%(levelname)5s %(filename)15s(%(lineno)3d): %(message)s')
+
+  suite = unittest.TestSuite()
+  loader = unittest.TestLoader()
+  pattern = '*%s*_unittest.py' % ('' if len(sys.argv) < 2 else sys.argv[1])
+  root_dir = os.path.dirname(os.path.realpath(__file__))
+  suite.addTests(loader.discover(start_dir=root_dir, pattern=pattern))
+  res = unittest.TextTestRunner(verbosity=2).run(suite)
+  if res.wasSuccessful():
+    sys.exit(0)
+  else:
+    sys.exit(1)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index b6ff35b..f5294ce 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -20,6 +20,7 @@ import devtools_monitor
 class PageTrack(devtools_monitor.Track):
   """Records the events from the page track."""
   def __init__(self, connection):
+    super(PageTrack, self).__init__()
     self._connection = connection
     self._events = []
     self._main_frame_id = None
@@ -47,6 +48,8 @@ class AndroidTraceRecorder(object):
   """Records a loading trace."""
   def __init__(self, url):
     self.url = url
+    self.devtools_connection = None
+    self.page_track = None
 
   def Go(self):
     self.devtools_connection = devtools_monitor.DevToolsConnection(

commit 438cb7d8fb09c012d784d538df5948824d787fe1
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 12 10:42:06 2016 -0800

    tools/android/loading: Implement devtools_monitor.
    
     tools/android/loading: Implement devtools_monitor.
    
    Also,
    - Move device setup out of log_requests.py
    - Add trace_recorder.py as a skeleton example of the API use.
    
    Review URL: https://codereview.chromium.org/1580793002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368922}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3e6374730d7e1a650622fe98d6a65ebb79d3612b

diff --git a/loading/OWNERS b/loading/OWNERS
index 3301555..0b168ed 100644
--- a/loading/OWNERS
+++ b/loading/OWNERS
@@ -1,2 +1,4 @@
 lizeb@chromium.org
 pasko@chromium.org
+# Not a committer yet, but OWNER nonetheless:
+# mattcary@chromium.org
diff --git a/loading/device_setup.py b/loading/device_setup.py
new file mode 100644
index 0000000..d98014c
--- /dev/null
+++ b/loading/device_setup.py
@@ -0,0 +1,95 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import contextlib
+import os
+import sys
+import time
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+
+from pylib import constants
+from pylib import flag_changer
+from pylib.device import device_utils
+from pylib.device import intent
+
+DEVTOOLS_PORT = 9222
+DEVTOOLS_HOSTNAME = 'localhost'
+
+@contextlib.contextmanager
+def FlagChanger(device, command_line_path, new_flags):
+  """Changes the flags in a context, restores them afterwards.
+
+  Args:
+    device: Device to target, from DeviceUtils.
+    command_line_path: Full path to the command-line file.
+    new_flags: Flags to add.
+  """
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
+  changer = flag_changer.FlagChanger(device, command_line_path)
+  changer.AddFlags(new_flags)
+  try:
+    yield
+  finally:
+    changer.Restore()
+
+
+@contextlib.contextmanager
+def ForwardPort(device, local, remote):
+  """Forwards a local port to a remote one on a device in a context."""
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
+  device.adb.Forward(local, remote)
+  try:
+    yield
+  finally:
+    device.adb.ForwardRemove(local)
+
+
+def _SetUpDevice(device, package_info):
+  """Enables root and closes Chrome on a device."""
+  device.EnableRoot()
+  device.KillAll(package_info.package, quiet=True)
+
+
+def SetUpAndExecute(device, package, fn):
+  """Start logging process.
+
+  Sets up any device and tracing appropriately and then executes the core
+  logging function.
+
+  Args:
+    device: Android device, or None for a local run.
+    package: the key for chrome package info.
+    fn: the function to execute that launches chrome and performs the
+        appropriate instrumentation, see _Log*Internal().
+
+  Returns:
+    As fn() returns.
+  """
+  package_info = constants.PACKAGE_INFO[package]
+  command_line_path = '/data/local/chrome-command-line'
+  new_flags = ['--enable-test-events',
+               '--remote-debugging-port=%d' % DEVTOOLS_PORT]
+  if device:
+    _SetUpDevice(device, package_info)
+  with FlagChanger(device, command_line_path, new_flags):
+    if device:
+      start_intent = intent.Intent(
+          package=package_info.package, activity=package_info.activity,
+          data='about:blank')
+      device.StartActivity(start_intent, blocking=True)
+      time.sleep(2)
+    # If no device, we don't care about chrome startup so skip the about page.
+    with ForwardPort(device, 'tcp:%d' % DEVTOOLS_PORT,
+                     'localabstract:chrome_devtools_remote'):
+      return fn()
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 7104e3a..f811ff1 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -5,6 +5,25 @@
 """Library handling DevTools websocket interaction.
 """
 
+import httplib
+import json
+import logging
+import os
+import sys
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
+
+from telemetry.internal.backends.chrome_inspector import inspector_websocket
+from telemetry.internal.backends.chrome_inspector import websocket
+
+
+class DevToolsConnectionException(Exception):
+  def __init__(self, message):
+    super(DevToolsConnectionException, self).__init__(message)
+    logging.warning("DevToolsConnectionException: " + message)
+
+
 class DevToolsConnection(object):
   """Handles the communication with a DevTools server.
   """
@@ -15,7 +34,10 @@ class DevToolsConnection(object):
       hostname: server hostname.
       port: port number.
     """
-    pass
+    self._ws = self._Connect(hostname, port)
+    self._listeners = {}
+    self._domains_to_enable = set()
+    self._please_stop = False
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -27,7 +49,9 @@ class DevToolsConnection(object):
             Network.requestWillBeSent.
       listener: (Listener) listener instance.
     """
-    pass
+    domain = name[:name.index('.')]
+    self._listeners[name] = listener
+    self._domains_to_enable.add(domain)
 
   def UnregisterListener(self, listener):
     """Unregisters a listener.
@@ -35,7 +59,10 @@ class DevToolsConnection(object):
     Args:
       listener: (Listener) listener to unregister.
     """
-    pass
+    keys = [k for (k, v) in self._listeners if k == name]
+    assert keys, "Removing non-existent listener"
+    for key in keys:
+      del(self._listeners[key])
 
   def SyncRequest(self, method, params=None):
     """Issues a synchronous request to the DevTools server.
@@ -47,7 +74,10 @@ class DevToolsConnection(object):
     Returns:
       The answer.
     """
-    pass
+    request = {'method': method}
+    if params:
+      request['params'] = params
+    return self._ws.SyncRequest(request)
 
   def SendAndIgnoreResponse(self, method, params=None):
     """Issues a request to the DevTools server, do not wait for the response.
@@ -56,15 +86,66 @@ class DevToolsConnection(object):
       method: (str) Method.
       params: (dict) Optional parameters to the request.
     """
-    pass
+    request = {'method': method}
+    if params:
+      request['params'] = params
+    self._ws.SendAndIgnoreResponse(request)
+
+  def SetUpMonitoring(self):
+    for domain in self._domains_to_enable:
+      self._ws.RegisterDomain(domain, self._OnDataReceived)
+      self.SyncRequest('%s.enable' % domain)
 
   def StartMonitoring(self):
-    """Starts monitoring, enabling the relevant domains first."""
-    pass
+    """Starts monitoring.
 
-  def Stop(self):
+    DevToolsConnection.SetUpMonitoring() has to be called first.
+    """
+    while not self._please_stop:
+      try:
+        self._ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException as e:
+        break
+    if not self._please_stop:
+      logging.warning('Monitoring stopped on a timeout.')
+    self._TearDownMonitoring()
+
+  def StopMonitoring(self):
     """Stops the monitoring."""
-    pass
+    self._please_stop = True
+
+  def _TearDownMonitoring(self):
+    for domain in self._domains_to_enable:
+      self.SyncRequest('%s.disable' % domain)
+      self._ws.UnregisterDomain(domain)
+    self._domains_to_enable.clear()
+    self._listeners.clear()
+
+  def _OnDataReceived(self, msg):
+    method = msg.get('method', None)
+    if method not in self._listeners:
+      return
+    self._listeners[method].Handle(method, msg)
+
+  @classmethod
+  def _GetWebSocketUrl(cls, hostname, port):
+    r = httplib.HTTPConnection(hostname, port)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      raise DevToolsConnectionException(
+          'Cannot connect to DevTools, reponse code %d' % response.status)
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    return websocket_url
+
+  @classmethod
+  def _Connect(cls, hostname, port):
+    websocket_url = cls._GetWebSocketUrl(hostname, port)
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    return ws
 
 
 class Listener(object):
@@ -77,7 +158,7 @@ class Listener(object):
     """
     pass
 
-  def Handle(self, event_name, event):
+  def Handle(self, method, msg):
     """Handles an event this instance listens for.
 
     Args:
diff --git a/loading/log_requests.py b/loading/log_requests.py
index 071870c..ecbb0e8 100755
--- a/loading/log_requests.py
+++ b/loading/log_requests.py
@@ -14,66 +14,18 @@ import logging
 import optparse
 import os
 import sys
-import time
 
 file_dir = os.path.dirname(__file__)
 sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
 sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
 sys.path.append(os.path.join(file_dir, '..', '..', 'chrome_proxy'))
 
-from pylib import constants
-from pylib import flag_changer
 from pylib.device import device_utils
-from pylib.device import intent
 from common import inspector_network
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
-
-_PORT = 9222 # DevTools port number.
-
-
-@contextlib.contextmanager
-def FlagChanger(device, command_line_path, new_flags):
-  """Changes the flags in a context, restores them afterwards.
-
-  Args:
-    device: Device to target, from DeviceUtils.
-    command_line_path: Full path to the command-line file.
-    new_flags: Flags to add.
-  """
-  # If we're logging requests from a local desktop chrome instance there is no
-  # device.
-  if not device:
-    yield
-    return
-  changer = flag_changer.FlagChanger(device, command_line_path)
-  changer.AddFlags(new_flags)
-  try:
-    yield
-  finally:
-    changer.Restore()
-
-
-@contextlib.contextmanager
-def ForwardPort(device, local, remote):
-  """Forwards a local port to a remote one on a device in a context."""
-  # If we're logging requests from a local desktop chrome instance there is no
-  # device.
-  if not device:
-    yield
-    return
-  device.adb.Forward(local, remote)
-  try:
-    yield
-  finally:
-    device.adb.ForwardRemove(local)
-
-
-def _SetUpDevice(device, package_info):
-  """Enables root and closes Chrome on a device."""
-  device.EnableRoot()
-  device.KillAll(package_info.package, quiet=True)
+import device_setup
 
 
 class AndroidRequestsLogger(object):
@@ -111,7 +63,7 @@ class AndroidRequestsLogger(object):
   def _LogPageLoadInternal(self, url, clear_cache):
     """Returns the collection of requests made to load a given URL.
 
-    Assumes that DevTools is available on http://localhost:_PORT.
+    Assumes that DevTools is available on http://localhost:DEVTOOLS_PORT.
 
     Args:
       url: URL to load.
@@ -122,7 +74,8 @@ class AndroidRequestsLogger(object):
     """
     self._main_frame_id = None
     self._please_stop = False
-    r = httplib.HTTPConnection('localhost', _PORT)
+    r = httplib.HTTPConnection(
+        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
     r.request('GET', '/json')
     response = r.getresponse()
     if response.status != 200:
@@ -155,7 +108,7 @@ class AndroidRequestsLogger(object):
   def _LogTracingInternal(self, url):
     self._main_frame_id = None
     self._please_stop = False
-    r = httplib.HTTPConnection('localhost', _PORT)
+    r = httplib.HTTPConnection('localhost', device_setup.DEVTOOLS_PORT)
     r.request('GET', '/json')
     response = r.getresponse()
     if response.status != 200:
@@ -182,36 +135,6 @@ class AndroidRequestsLogger(object):
     return {'events': self._tracing_data,
             'end': ws.SyncRequest({'method': 'Tracing.end'})}
 
-  def _DoSomeLogging(self, package, fn):
-    """Start logging process.
-
-    Sets up any device and tracing appropriately and then executes the core
-    logging function.
-
-    Args:
-      package: the key for chrome package info.
-      fn: the function to execute that launches chrome and performs the
-      appropriate instrumentation, see _Log*Internal().
-
-    Returns:
-      As fn() returns.
-    """
-    package_info = constants.PACKAGE_INFO[package]
-    command_line_path = '/data/local/chrome-command-line'
-    new_flags = ['--enable-test-events', '--remote-debugging-port=%d' % _PORT]
-    if self.device:
-      _SetUpDevice(self.device, package_info)
-    with FlagChanger(self.device, command_line_path, new_flags):
-      if self.device:
-        start_intent = intent.Intent(
-            package=package_info.package, activity=package_info.activity,
-            data='about:blank')
-        self.device.StartActivity(start_intent, blocking=True)
-        time.sleep(2)
-      # If no device, we don't care about chrome startup so skip the about page.
-      with ForwardPort(self.device, 'tcp:%d' % _PORT,
-                       'localabstract:chrome_devtools_remote'):
-        return fn()
 
   def LogPageLoad(self, url, clear_cache, package):
     """Returns the collection of requests made to load a given URL on a device.
@@ -223,8 +146,9 @@ class AndroidRequestsLogger(object):
     Returns:
       See _LogPageLoadInternal().
     """
-    return self._DoSomeLogging(
-        package, lambda: self._LogPageLoadInternal(url, clear_cache))
+    return device_setup.SetUpAndExecute(
+        self.device, package,
+        lambda: self._LogPageLoadInternal(url, clear_cache))
 
   def LogTracing(self, url):
     """Log tracing events from a load of the given URL.
@@ -233,7 +157,8 @@ class AndroidRequestsLogger(object):
     simultaneously with network requests, but as that wasn't working the tracing
     logging was broken out separately. It still doesn't work...
     """
-    return self._DoSomeLogging('chrome', lambda: self._LogTracingInternal(url))
+    return device_setup.SetUpAndExecute(
+        self.device, 'chrome', lambda: self._LogTracingInternal(url))
 
 
 def _ResponseDataToJson(data):
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
new file mode 100755
index 0000000..b6ff35b
--- /dev/null
+++ b/loading/trace_recorder.py
@@ -0,0 +1,71 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loading trace recorder."""
+
+import os
+import sys
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+
+from pylib.device import device_utils
+
+import device_setup
+import devtools_monitor
+
+
+class PageTrack(devtools_monitor.Track):
+  """Records the events from the page track."""
+  def __init__(self, connection):
+    self._connection = connection
+    self._events = []
+    self._main_frame_id = None
+    self._connection.RegisterListener('Page.frameStartedLoading', self)
+    self._connection.RegisterListener('Page.frameStoppedLoading', self)
+
+  def Handle(self, method, msg):
+    params = msg['params']
+    frame_id = params['frameId']
+    should_stop = False
+    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
+      self._main_frame_id = params['frameId']
+    elif (method == 'Page.frameStoppedLoading'
+          and params['frameId'] == self._main_frame_id):
+      should_stop = True
+    self._events.append((method, frame_id))
+    if should_stop:
+      self._connection.StopMonitoring()
+
+  def GetEvents(self):
+    return self._events
+
+
+class AndroidTraceRecorder(object):
+  """Records a loading trace."""
+  def __init__(self, url):
+    self.url = url
+
+  def Go(self):
+    self.devtools_connection = devtools_monitor.DevToolsConnection(
+        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
+    self.page_track = PageTrack(self.devtools_connection)
+
+    self.devtools_connection.SetUpMonitoring()
+    self.devtools_connection.SendAndIgnoreResponse(
+        'Page.navigate', {'url': self.url})
+    self.devtools_connection.StartMonitoring()
+    print self.page_track.GetEvents()
+
+
+def DoIt(url):
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  device = devices[0]
+  trace_recorder = AndroidTraceRecorder(url)
+  device_setup.SetUpAndExecute(device, 'chrome', trace_recorder.Go)
+
+
+if __name__ == '__main__':
+  DoIt(sys.argv[1])

commit 30c9ec080d6c95e986a21ef66b6dc089c02caf04
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 12 05:39:13 2016 -0800

    tools/android/loading: Skeleton of the DevTools monitor.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1574253002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368864}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 450085ec099e0c8eb2ba6f2f369f759607658c84

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
new file mode 100644
index 0000000..7104e3a
--- /dev/null
+++ b/loading/devtools_monitor.py
@@ -0,0 +1,94 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Library handling DevTools websocket interaction.
+"""
+
+class DevToolsConnection(object):
+  """Handles the communication with a DevTools server.
+  """
+  def __init__(self, hostname, port):
+    """Initializes the connection with a DevTools server.
+
+    Args:
+      hostname: server hostname.
+      port: port number.
+    """
+    pass
+
+  def RegisterListener(self, name, listener):
+    """Registers a listener for an event.
+
+    Also takes care of enabling the relevant domain before starting monitoring.
+
+    Args:
+      name: (str) Event the listener wants to listen to, e.g.
+            Network.requestWillBeSent.
+      listener: (Listener) listener instance.
+    """
+    pass
+
+  def UnregisterListener(self, listener):
+    """Unregisters a listener.
+
+    Args:
+      listener: (Listener) listener to unregister.
+    """
+    pass
+
+  def SyncRequest(self, method, params=None):
+    """Issues a synchronous request to the DevTools server.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Optional parameters to the request.
+
+    Returns:
+      The answer.
+    """
+    pass
+
+  def SendAndIgnoreResponse(self, method, params=None):
+    """Issues a request to the DevTools server, do not wait for the response.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Optional parameters to the request.
+    """
+    pass
+
+  def StartMonitoring(self):
+    """Starts monitoring, enabling the relevant domains first."""
+    pass
+
+  def Stop(self):
+    """Stops the monitoring."""
+    pass
+
+
+class Listener(object):
+  """Listens to events forwarded by a DevToolsConnection instance."""
+  def __init__(self, connection):
+    """Initializes a Listener instance.
+
+    Args:
+      connection: (DevToolsConnection).
+    """
+    pass
+
+  def Handle(self, event_name, event):
+    """Handles an event this instance listens for.
+
+    Args:
+      event_name: (str) Event name, as registered.
+      event: (dict) complete event.
+    """
+    pass
+
+
+class Track(Listener):
+  """Collects data from a DevTools server."""
+  def GetEvents(self):
+    """Returns a list of collected events, finalizing the state if necessary."""
+    pass

commit 01f5dc9ebfc7d83490cc723e57c66a16a9a7a8bb
Author: mattcary <mattcary@google.com>
Date:   Mon Jan 11 08:04:34 2016 -0800

    Initial loading model and analysis tools.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1562373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368597}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 04a313090df5eb6a419becc1588449c74aa1a7ae

diff --git a/loading/analyze.py b/loading/analyze.py
new file mode 100755
index 0000000..1a49155
--- /dev/null
+++ b/loading/analyze.py
@@ -0,0 +1,362 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import cgi
+import json
+import logging
+import os
+import subprocess
+import sys
+import tempfile
+import time
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+
+from pylib import constants
+from pylib.device import device_utils
+from pylib.device import intent
+
+import log_parser
+import log_requests
+import loading_model
+
+
+# TODO(mattcary): logging.info isn't that useful; we need something finer
+# grained. For now we just do logging.warning.
+
+
+# TODO(mattcary): probably we want this piped in through a flag.
+CHROME = constants.PACKAGE_INFO['chrome']
+
+
+def _SetupAndGetDevice():
+  """Gets an android device, set up the way we like it.
+
+  Returns:
+    An AdbWrapper for the first device found.
+  """
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  device.EnableRoot()
+  device.KillAll(CHROME.package, quiet=True)
+  return device
+
+
+def _LoadPage(device, url):
+  """Load a page on chrome on our device.
+
+  Args:
+    device: an AdbWrapper for the device on which to load the page.
+    url: url as a string to load.
+  """
+  load_intent = intent.Intent(
+      package=CHROME.package, activity=CHROME.activity, data=url)
+  logging.warning('Loading ' + url)
+  device.StartActivity(load_intent, blocking=True)
+
+
+def _WriteJson(output, json_data):
+  """Write JSON data in a nice way.
+
+  Args:
+    output: a file object
+    json_data: JSON data as a dict.
+  """
+  json.dump(json_data, output, sort_keys=True, indent=2)
+
+
+def _GetPrefetchHtml(graph, name=None):
+  """Generate prefetch page for the resources in resource graph.
+
+  Args:
+    graph: a ResourceGraph.
+    name: optional string used in the generated page.
+
+  Returns:
+    HTML as a string containing all the link rel=prefetch directives necessary
+    for prefetching the given ResourceGraph.
+  """
+  if name:
+    title = 'Prefetch for ' + cgi.escape(name)
+  else:
+    title = 'Generated prefetch page'
+  output = []
+  output.append("""<!DOCTYPE html>
+<html>
+<head>
+<title>%s</title>
+""" % title)
+  for info in graph.ResourceInfo():
+    output.append('<link rel="prefetch" href="%s">\n' % info.Url())
+  output.append("""</head>
+<body>%s</body>
+</html>
+  """ % title)
+
+  return '\n'.join(output)
+
+
+def _LogRequests(url, clear_cache=True, local=False):
+  """Log requests for a web page.
+
+  TODO(mattcary): loading.log_requests probably needs to be refactored as we're
+  using private methods, also there's ugliness like _ResponseDataToJson return a
+  json.dumps that we immediately json.loads.
+
+  Args:
+    url: url to log as string.
+    clear_cache: optional flag to clear the cache.
+    local: log from local (desktop) chrome session.
+
+  Returns:
+    JSON of logged information (ie, a dict that describes JSON).
+  """
+  device = _SetupAndGetDevice() if not local else None
+  request_logger = log_requests.AndroidRequestsLogger(device)
+  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
+  response_data = request_logger.LogPageLoad(
+      url, clear_cache, 'chrome')
+  return json.loads(log_requests._ResponseDataToJson(response_data))
+
+
+def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
+  """Do a full fetch with optional prefetching."""
+  if not url.startswith('http'):
+    url = 'http://' + url
+  logging.warning('Cold fetch')
+  cold_data = _LogRequests(url, local=local)
+  assert cold_data, 'Cold fetch failed to produce data. Check your phone.'
+  if prefetch:
+    assert not local
+    logging.warning('Generating prefetch')
+    prefetch_html = _GetPrefetchHtml(_ProcessJson(cold_data), name=url)
+    tmp = tempfile.NamedTemporaryFile()
+    tmp.write(prefetch_html)
+    tmp.flush()
+    # We hope that the tmpfile name is unique enough for the device.
+    target = os.path.join('/sdcard/Download', os.path.basename(tmp.name))
+    device = _SetupAndGetDevice()
+    device.adb.Push(tmp.name, target)
+    logging.warning('Pushed prefetch %s to device at %s' % (tmp.name, target))
+    _LoadPage(device, 'file://' + target)
+    time.sleep(prefetch_delay_seconds)
+    logging.warning('Warm fetch')
+    warm_data = _LogRequests(url, clear_cache=False)
+    with open(json_output, 'w') as f:
+      _WriteJson(f, warm_data)
+    logging.warning('Wrote ' + json_output)
+    with open(json_output + '.cold', 'w') as f:
+      _WriteJson(f, cold_data)
+    logging.warning('Wrote ' + json_output + '.cold')
+  else:
+    with open(json_output, 'w') as f:
+      _WriteJson(f, cold_data)
+    logging.warning('Wrote ' + json_output)
+
+
+# TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
+# with here.
+def _ProcessRequests(filename):
+  requests = log_parser.FilterRequests(log_parser.ParseJsonFile(filename))
+  return loading_model.ResourceGraph(requests)
+
+
+def _ProcessJson(json_data):
+  assert json_data
+  return loading_model.ResourceGraph(log_parser.FilterRequests(
+      [log_parser.RequestData.FromDict(r) for r in json_data]))
+
+
+def InvalidCommand(cmd):
+  sys.exit('Invalid command "%s"\nChoices are: %s' %
+           (cmd, ' '.join(COMMAND_MAP.keys())))
+
+
+def DoCost(arg_str):
+  parser = argparse.ArgumentParser(usage='cost [--parameter ...] REQUEST_JSON')
+  parser.add_argument('request_json')
+  parser.add_argument('--parameter', nargs='*', default=[])
+  parser.add_argument('--path', action='store_true')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  for p in args.parameter:
+    graph.Set(**{p: True})
+  path_list = []
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  print 'Graph cost: ' + str(graph.Cost(path_list))
+  if args.path:
+    for p in path_list:
+      print '  ' + p.ShortName()
+
+
+def DoPng(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='png [--eog] [--highlight X[,...] REQUEST_JSON [PNG_OUTPUT]')
+  parser.add_argument('request_json')
+  parser.add_argument('png_output', nargs='?')
+  parser.add_argument('--eog', action='store_true')
+  parser.add_argument('--highlight')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  tmp = tempfile.NamedTemporaryFile()
+  graph.MakeGraphviz(
+      tmp,
+      highlight=args.highlight.split(',') if args.highlight else None)
+  tmp.flush()
+  png_output = args.png_output
+  if not png_output:
+    if args.request_json.endswith('.json'):
+      png_output = args.request_json[:args.request_json.rfind('.json')] + '.png'
+    else:
+      png_output = args.request_json + '.png'
+  subprocess.check_call(['dot', '-Tpng', tmp.name, '-o', png_output])
+  logging.warning('Wrote ' + png_output)
+  if args.eog:
+    subprocess.Popen(['eog', png_output])
+  tmp.close()
+
+
+def DoCompare(arg_str):
+  parser = argparse.ArgumentParser(usage='compare REQUEST_JSON REQUEST_JSON')
+  parser.add_argument('g1_json')
+  parser.add_argument('g2_json')
+  args = parser.parse_args(arg_str)
+  g1 = _ProcessRequests(args.g1_json)
+  g2 = _ProcessRequests(args.g2_json)
+  discrepancies = loading_model.ResourceGraph.CheckImageLoadConsistency(g1, g2)
+  if discrepancies:
+    print '%d discrepancies' % len(discrepancies)
+    print '\n'.join([str(r) for r in discrepancies])
+  else:
+    print 'Consistent!'
+
+
+def DoPrefetchSetup(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='prefetch_setup [--upload] REQUEST_JSON TARGET_HTML')
+  parser.add_argument('request_json')
+  parser.add_argument('target_html')
+  parser.add_argument('--upload', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  with open(args.target_html, 'w') as html:
+    html.write(_GetPrefetchHtml(
+        graph, name=os.path.basename(args.request_json)))
+  if args.upload:
+    device = _SetupAndGetDevice()
+    destination = os.path.join('/sdcard/Download',
+                               os.path.basename(args.target_html))
+    device.adb.Push(args.target_html, destination)
+
+    logging.warning(
+        'Pushed %s to device at %s' % (args.target_html, destination))
+
+
+def DoLogRequests(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='log_requests [--prefetch] --site URL --output JSON_OUTPUT')
+  parser.add_argument('--url', required=True)
+  parser.add_argument('--output', required=True)
+  parser.add_argument('--prefetch', action='store_true')
+  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
+  parser.add_argument('--local', action='store_true')
+  args = parser.parse_args(arg_str)
+  _FullFetch(url=args.url,
+             json_output=args.output,
+             prefetch=args.prefetch,
+             prefetch_delay_seconds=args.prefetch_delay_seconds,
+             local=args.local)
+
+
+def DoFetch(arg_str):
+  parser = argparse.ArgumentParser(usage='fetch --site SITE --dir DIR\n'
+                                   'Fetches SITE into DIR with standard naming '
+                                   'that can be processed by ./cost_to_csv.py. '
+                                   'Both warm and cold fetches are done. '
+                                   'SITE can be a full url but the filename '
+                                   'may be strange so better to just use a '
+                                   'site (ie, domain).')
+  # Arguments are flags as it's easy to get the wrong order of site vs dir.
+  parser.add_argument('--site', required=True)
+  parser.add_argument('--dir', required=True)
+  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
+  args = parser.parse_args(arg_str)
+  if not os.path.exists(args.dir):
+    os.makedirs(args.dir)
+  _FullFetch(url=args.site,
+             json_output=os.path.join(args.dir, args.site + '.json'),
+             prefetch=True,
+             prefetch_delay_seconds=args.prefetch_delay_seconds,
+             local=False)
+
+
+def DoTracing(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='tracing URL JSON_OUTPUT')
+  parser.add_argument('url')
+  parser.add_argument('json_output')
+  args = parser.parse_args(arg_str)
+  device = _SetupAndGetDevice()
+  request_logger = log_requests.AndroidRequestsLogger(device)
+  tracing = request_logger.LogTracing(args.url)
+  with open(args.json_output, 'w') as f:
+    _WriteJson(f, tracing)
+  logging.warning('Wrote ' + args.json_output)
+
+
+def DoLongPole(arg_str):
+  parser = argparse.ArgumentParser(usage='longpole [--noads] REQUEST_JSON')
+  parser.add_argument('request_json')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  path_list = []
+  cost = graph.Cost(path_list=path_list)
+  print '%s (%s)' % (path_list[-1], cost)
+
+
+def DoNodeCost(arg_str):
+  parser = argparse.ArgumentParser(usage='nodecost [--noads] REQUEST_JSON')
+  parser.add_argument('request_json')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  print sum((n.NodeCost() for n in graph.Nodes()))
+
+
+COMMAND_MAP = {
+    'cost': DoCost,
+    'png': DoPng,
+    'compare': DoCompare,
+    'prefetch_setup': DoPrefetchSetup,
+    'log_requests': DoLogRequests,
+    'tracing': DoTracing,
+    'longpole': DoLongPole,
+    'nodecost': DoNodeCost,
+    'fetch': DoFetch,
+}
+
+def main():
+  logging.basicConfig(level=logging.WARNING)
+  parser = argparse.ArgumentParser(usage=' '.join(COMMAND_MAP.keys()))
+  parser.add_argument('command')
+  parser.add_argument('rest', nargs=argparse.REMAINDER)
+  args = parser.parse_args()
+  COMMAND_MAP.get(args.command,
+                  lambda _: InvalidCommand(args.command))(args.rest)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/cost_to_csv.py b/loading/cost_to_csv.py
new file mode 100755
index 0000000..d271d06
--- /dev/null
+++ b/loading/cost_to_csv.py
@@ -0,0 +1,43 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import logging
+import os
+import sys
+
+from processing import (SitesFromDir, WarmGraph, ColdGraph)
+
+
+def main():
+  logging.basicConfig(level=logging.ERROR)
+  parser = argparse.ArgumentParser(
+      description=('Convert a directory created by ./analyze.py fetch '
+                   'to a CSV.'))
+  parser.add_argument('--datadir', required=True)
+  parser.add_argument('--csv', required=True)
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args()
+  sites = SitesFromDir(args.datadir)
+  with open(args.csv, 'w') as output:
+    output.write('site,kind,cost\n')
+    for site in sites:
+      print site
+      warm = WarmGraph(args.datadir, site)
+      if args.noads:
+        warm.Set(node_filter=warm.FilterAds)
+      cold = ColdGraph(args.datadir, site)
+      if args.noads:
+        cold.Set(node_filter=cold.FilterAds)
+      output.write('%s,%s,%s\n' % (site, 'warm', warm.Cost()))
+      warm.Set(cache_all=True)
+      output.write('%s,%s,%s\n' % (site, 'warm-cache', warm.Cost()))
+      output.write('%s,%s,%s\n' % (site, 'cold', cold.Cost()))
+      cold.Set(cache_all=True)
+      output.write('%s,%s,%s\n' % (site, 'cold-cache', cold.Cost()))
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/dag.py b/loading/dag.py
new file mode 100644
index 0000000..d9f1a47
--- /dev/null
+++ b/loading/dag.py
@@ -0,0 +1,119 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Support for directed acyclic graphs.
+
+Used in the ResourceGraph model for chrome loading.
+"""
+
+class Node(object):
+  """A node in a DAG.
+
+  We do not enforce at a node level that a graph is a DAG. Methods like
+  TopologicalSort will assume a DAG and may fail if that's not the case.
+
+  Nodes are identified with an index that must be unique for a particular graph
+  (it is used for hashing and equality). A graph is represented as a list of
+  nodes, for example in the TopologicalSort class method. By convention a node's
+  index is its position in this list, making it easy to store auxillary
+  information.
+  """
+  def __init__(self, index):
+    """Create a new node.
+
+    Args:
+      index: index of the node. We assume these indicies uniquely identify a
+        node (and so use it for hashing and equality).
+    """
+    self._predecessors = set()
+    self._successors = set()
+    self._index = index
+
+  def Predecessors(self):
+    return self._predecessors
+
+  def Successors(self):
+    return self._successors
+
+  def AddSuccessor(self, s):
+    """Add a successor.
+
+    Updates appropriate links. Any existing parents of s are unchanged; to move
+    a node you must do a combination of RemoveSuccessor and AddSuccessor.
+
+    Args:
+      s: the node to add as a successor.
+    """
+    self._successors.add(s)
+    s._predecessors.add(self)
+
+  def RemoveSuccessor(self, s):
+    """Removes a successor.
+
+    Updates appropriate links.
+
+    Args:
+      s: the node to remove as a successor. Will raise a set exception if s is
+         not an existing successor.
+    """
+    self._successors.remove(s)
+    s._predecessors.remove(self)
+
+  def SortedSuccessors(self):
+    children = [c for c in self.Successors()]
+    children.sort(key=lambda c: c.Index())
+    return children
+
+  def Index(self):
+    return self._index
+
+  def __eq__(self, o):
+    return self.Index() == o.Index()
+
+  def __hash__(self):
+    return hash(self.Index())
+
+
+def TopologicalSort(nodes, node_filter=None):
+  """Topological sort.
+
+  We use a BFS-like walk which ensures that sibling are always grouped
+  together in the output. This is more convenient for some later analyses.
+
+  Args:
+    nodes: [Node, ...] Nodes to sort.
+    node_filter: a filter Node->boolean to restrict the graph. A node passes the
+      filter on a return value of True. Only the subgraph reachable from a root
+      passing the filter is considered.
+
+  Returns:
+    A list of Nodes in topological order. Note that node indicies are
+    unchanged; the original list nodes is not modified.
+  """
+  if node_filter is None:
+    node_filter = lambda _: True
+  sorted_nodes = []
+  sources = []
+  remaining_in_edges = {}
+  valid_node_count = 0
+  for n in nodes:
+    if node_filter(n):
+      valid_node_count += 1
+    if n.Predecessors():
+      remaining_in_edges[n] = len(n.Predecessors())
+    elif node_filter(n):
+      sources.append(n)
+  while sources:
+    n = sources.pop(0)
+    assert node_filter(n)
+    sorted_nodes.append(n)
+    # We sort by index to get consistent sorts across runs/machines.
+    for c in n.SortedSuccessors():
+      assert remaining_in_edges[c] > 0
+      if not node_filter(c):
+        continue
+      remaining_in_edges[c] -= 1
+      if not remaining_in_edges[c]:
+        sources.append(c)
+  return sorted_nodes
diff --git a/loading/dag_unittest.py b/loading/dag_unittest.py
new file mode 100644
index 0000000..6701c9e
--- /dev/null
+++ b/loading/dag_unittest.py
@@ -0,0 +1,92 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import sys
+import unittest
+
+import dag
+
+class DagTestCase(unittest.TestCase):
+
+  def MakeDag(self, links):
+    """Make a graph from a description of links.
+
+    Args:
+      links: A list of (index, (successor index...)) tuples. Index must equal
+        the location of the tuple in the list and are provided to make it easier
+        to read.
+
+    Returns:
+      A list of Nodes.
+    """
+    nodes = []
+    for i in xrange(len(links)):
+      assert i == links[i][0]
+      nodes.append(dag.Node(i))
+    for l in links:
+      for s in l[1]:
+        nodes[l[0]].AddSuccessor(nodes[s])
+    return nodes
+
+  def SortedIndicies(self, graph, node_filter=None):
+    return [n.Index() for n in dag.TopologicalSort(graph, node_filter)]
+
+  def SuccessorIndicies(self, node):
+    return [c.Index() for c in node.SortedSuccessors()]
+
+  def test_SimpleSorting(self):
+    graph = self.MakeDag([(0, (1,2)),
+                          (1, (3,)),
+                          (2, ()),
+                          (3, (4,)),
+                          (4, ()),
+                          (5, (6,)),
+                          (6, ())])
+    self.assertEqual(self.SuccessorIndicies(graph[0]), [1, 2])
+    self.assertEqual(self.SuccessorIndicies(graph[1]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph[2]), [])
+    self.assertEqual(self.SuccessorIndicies(graph[3]), [4])
+    self.assertEqual(self.SuccessorIndicies(graph[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph[5]), [6])
+    self.assertEqual(self.SuccessorIndicies(graph[6]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 5, 1, 2, 6, 3, 4])
+
+  def test_SortSiblingsAreGrouped(self):
+    graph = self.MakeDag([(0, (1, 2, 3)),
+                          (1, (4,)),
+                          (2, (5, 6)),
+                          (3, (7, 8)),
+                          (4, ()),
+                          (5, ()),
+                          (6, ()),
+                          (7, ()),
+                          (8, ())])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5, 6, 7, 8])
+
+  def test_FilteredSorting(self):
+    # 0 is a filtered-out root, which means the subgraph containing 1, 2, 3 and
+    # 4 should be ignored. 5 is an unfiltered root, and the subgraph containing
+    # 6, 7, 8 and 10 should be sorted. 9 and 11 are filtered out, and should
+    # exclude the unfiltred node 12.
+    graph = self.MakeDag([(0, (1,)),
+                          (1, (2, 3)),
+                          (2, ()),
+                          (3, (4,)),
+                          (4, ()),
+                          (5, (6, 7)),
+                          (6, (11,)),
+                          (7, (8,)),
+                          (8, (9, 10)),
+                          (9, ()),
+                          (10, ()),
+                          (11, (12,)),
+                          (12, ())])
+    self.assertEqual(self.SortedIndicies(
+        graph, lambda n: n.Index() not in (0, 3, 9, 11)),
+                     [5, 6, 7, 8, 10])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/loading_model.py b/loading/loading_model.py
new file mode 100644
index 0000000..2e76384
--- /dev/null
+++ b/loading/loading_model.py
@@ -0,0 +1,732 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Models for loading in chrome.
+
+(Redirect the following to the general model module once we have one)
+A model is an object with the following methods.
+  CostMs(): return the cost of the cost in milliseconds.
+  Set(): set model-specifical parameters.
+
+ResourceGraph
+  This creates a DAG of resource dependancies from loading.log_requests to model
+  loading time. The model may be parameterized by changing the loading time of
+  a particular or all resources.
+"""
+
+import logging
+import os
+import urlparse
+import sys
+
+import dag
+import log_parser
+
+class ResourceGraph(object):
+  """A model of loading by a DAG (tree?) of resource dependancies.
+
+  Set parameters:
+    cache_all: if true, assume zero loading time for all resources.
+  """
+
+  def __init__(self, requests):
+    """Create from a parsed request set.
+
+    Args:
+      requests: [RequestData, ...] filtered RequestData from loading.log_parser.
+    """
+    self._BuildDag(requests)
+    self._global_start = min([n.StartTime() for n in self._node_info])
+    # Sort before splitting children so that we can correctly dectect if a
+    # reparented child is actually a dependency for a child of its new parent.
+    try:
+      for n in dag.TopologicalSort(self._nodes):
+        self._SplitChildrenByTime(self._node_info[n.Index()])
+    except AssertionError as exc:
+      sys.stderr.write('Bad topological sort: %s\n'
+                       'Skipping child split\n' % str(exc))
+    self._cache_all = False
+    self._node_filter = lambda _: True
+
+  @classmethod
+  def CheckImageLoadConsistency(cls, g1, g2):
+    """Check that images have the same dependancies between ResourceGraphs.
+
+    Image resources are identified by their short names.
+
+    Args:
+      g1: a ResourceGraph instance
+      g2: a ResourceGraph instance
+
+    Returns:
+      A list of discrepancy tuples. If this list is empty, g1 and g2 are
+      consistent with respect to image load dependencies. Otherwise, each tuple
+      is of the form:
+        ( g1 resource short name or str(list of short names),
+          g2 resource short name or str(list of short names),
+          human-readable discrepancy reason )
+      Either or both of the g1 and g2 image resource short names may be None if
+      it's not applicable for the discrepancy reason.
+    """
+    discrepancies = []
+    g1_image_to_info = g1._ExtractImages()
+    g2_image_to_info = g2._ExtractImages()
+    for image in set(g1_image_to_info.keys()) - set(g2_image_to_info.keys()):
+      discrepancies.append((image, None, 'Missing in g2'))
+    for image in set(g2_image_to_info.keys()) - set(g1_image_to_info.keys()):
+      discrepancies.append((None, image, 'Missing in g1'))
+
+    for image in set(g1_image_to_info.keys()) & set(g2_image_to_info.keys()):
+      def PredecessorInfo(g, n):
+        info = [g._ShortName(p) for p in n.Node().Predecessors()]
+        info.sort()
+        return str(info)
+      g1_pred = PredecessorInfo(g1, g1_image_to_info[image])
+      g2_pred = PredecessorInfo(g2, g2_image_to_info[image])
+      if g1_pred != g2_pred:
+        discrepancies.append((g1_pred, g2_pred,
+                              'Predecessor mismatch for ' + image))
+
+    return discrepancies
+
+  def Set(self, cache_all=None, node_filter=None):
+    """Set model parameters.
+
+    TODO(mattcary): add parameters for caching certain types of resources (just
+    scripts, just cachable, etc).
+
+    Args:
+      cache_all: boolean that if true ignores emperical resource load times for
+        all resources.
+      node_filter: a Node->boolean used to restrict the graph for most
+        operations.
+    """
+    if self._cache_all is not None:
+      self._cache_all = cache_all
+    if node_filter is not None:
+      self._node_filter = node_filter
+
+  def Nodes(self):
+    """Return iterable of all nodes via their _NodeInfos.
+
+    Returns:
+      Iterable of node infos in arbitrary order.
+    """
+    for n in self._node_info:
+      if self._node_filter(n.Node()):
+        yield n
+
+  def EdgeCosts(self, node_filter=None):
+    """Edge costs.
+
+    Args:
+      node_filter: if not none, a Node->boolean filter to use instead of the
+      current one from Set.
+
+    Returns:
+      The total edge costs of our graph.
+
+    """
+    node_filter = self._node_filter if node_filter is None else node_filter
+    total = 0
+    for n in self._node_info:
+      if not node_filter(n.Node()): continue
+      for s in n.Node().Successors():
+        if node_filter(s):
+          total += self._EdgeCost(n.Node(), s)
+    return total
+
+  def Intersect(self, other_nodes):
+    """Return iterable of nodes that intersect with another graph.
+
+    Args:
+      other_nodes: iterable of the nodes of another graph, eg from Nodes().
+
+    Returns:
+      an iterable of (mine, other) pairs for all nodes for which the URL is
+      identical.
+    """
+    other_map = {n.Url(): n for n in other_nodes}
+    for n in self._node_info:
+      if self._node_filter(n.Node()) and n.Url() in other_map:
+        yield(n, other_map[n.Url()])
+
+
+  def Cost(self, path_list=None):
+    """Compute cost of current model.
+
+    Args:
+      path_list: if not None, gets a list of NodeInfo in the longest path.
+
+    Returns:
+      Cost of the longest path.
+
+    """
+    costs = [0] * len(self._nodes)
+    for n in dag.TopologicalSort(self._nodes, self._node_filter):
+      cost = 0
+      if n.Predecessors():
+        cost = max([costs[p.Index()] + self._EdgeCost(p, n)
+                    for p in n.Predecessors()])
+      if not self._cache_all:
+        cost += self._NodeCost(n)
+      costs[n.Index()] = cost
+    max_cost = max(costs)
+    assert max_cost > 0  # Otherwise probably the filter went awry.
+    if path_list is not None:
+      del path_list[:-1]
+      n = (i for i in self._nodes if costs[i.Index()] == max_cost).next()
+      path_list.append(self._node_info[n.Index()])
+      while n.Predecessors():
+        n = reduce(lambda costliest, next:
+                   next if (self._node_filter(next) and
+                            cost[next.Index()] > cost[costliest.Index()])
+                        else costliest,
+                   n.Predecessors())
+        path_list.insert(0, self._node_info[n.Index()])
+    return max_cost
+
+  def FilterAds(self, node):
+    """A filter for use in eg, Cost, to remove advertising nodes.
+
+    Args:
+      node: A dag.Node.
+
+    Returns:
+      True if the node is not ad-related.
+    """
+    return not self._IsAdUrl(self._node_info[node.Index()].Url())
+
+  def MakeGraphviz(self, output, highlight=None):
+    """Output a graphviz representation of our DAG.
+
+    Args:
+      output: a file-like output stream which recieves a graphviz dot.
+      highlight: a list of node items to emphasize. Any resource url which
+        contains any highlight text will be distinguished in the output.
+    """
+    output.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+    orphans = set()
+    try:
+      sorted_nodes = dag.TopologicalSort(self._nodes,
+                                         node_filter=self._node_filter)
+    except AssertionError as exc:
+      sys.stderr.write('Bad topological sort: %s\n'
+                       'Writing children in order\n' % str(exc))
+      sorted_nodes = self._nodes
+    for n in sorted_nodes:
+      if not n.Successors() and not n.Predecessors():
+        orphans.add(n)
+    if orphans:
+      output.write("""subgraph cluster_orphans {
+  color=black;
+  label="Orphans";
+""")
+      for n in orphans:
+        output.write(self._GraphvizNode(n.Index(), highlight))
+      output.write('}\n')
+
+    output.write("""subgraph cluster_nodes {
+  color=invis;
+""")
+    for n in sorted_nodes:
+      if not n.Successors() and not n.Predecessors():
+        continue
+      output.write(self._GraphvizNode(n.Index(), highlight))
+
+    for n in sorted_nodes:
+      for s in n.Successors():
+        style = 'color = orange'
+        annotations = self._EdgeAnnotation(n, s)
+        if 'parser' in annotations:
+          style = 'color = red'
+        elif 'stack' in annotations:
+          style = 'color = blue'
+        elif 'script_inferred' in annotations:
+          style = 'color = purple'
+        if 'timing' in annotations:
+          style += '; style=dashed'
+        arrow = '[%s; label="%s"]' % (style, self._EdgeCost(n, s))
+        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
+    output.write('}\n}\n')
+
+  def ResourceInfo(self):
+    """Get resource info.
+
+    Returns:
+      A list of _NodeInfo objects that describe the resources fetched.
+    """
+    return self._node_info
+
+  def DebugString(self):
+    """Graph structure for debugging.
+
+    TODO(mattcary): this fails for graphs with more than one component or where
+    self._nodes[0] is not a root.
+
+    Returns:
+      A human-readable string of the graph.
+    """
+    output = []
+    queue = [self._nodes[0]]
+    visited = set()
+    while queue:
+      n = queue.pop(0)
+      assert n not in visited
+      visited.add(n)
+      children = n.SortedSuccessors()
+      output.append('%d -> [%s]' %
+                    (n.Index(), ' '.join([str(c.Index()) for c in children])))
+      for c in children:
+        assert n in c.Predecessors()  # Integrity checking
+        queue.append(c)
+    assert len(visited) == len(self._nodes)
+    return '\n'.join(output)
+
+  ##
+  ## Internal items
+  ##
+
+  _CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
+                            'json': 'purple', 'gif_image': 'grey',
+                            'image': 'orange', 'other': 'white'}
+
+  # This resource type may induce a timing dependency. See _SplitChildrenByTime
+  # for details.
+  # TODO(mattcary): are these right?
+  _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
+  _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
+
+  class _NodeInfo(object):
+    """Our internal class that adds cost and other information to nodes.
+
+    Costs are stored on the node as well as edges. Edge information is only
+    stored on successor edges and not predecessor, that is, you get them from
+    the parent and not the child.
+
+    We also store the request on the node, and expose request-derived
+    information like content type.
+    """
+    def __init__(self, node, request):
+      """Create a new node info.
+
+      Args:
+        node: The node to augment.
+        request: The request associated with this node.
+      """
+      self._request = request
+      self._node = node
+      self._edge_costs = {}
+      self._edge_annotations = {}
+      # All fields in timing are millis relative to requestTime, which is epoch
+      # seconds.
+      self._node_cost = max([t for f, t in request.timing._asdict().iteritems()
+                             if f != 'requestTime'])
+
+    def __str__(self):
+      return self.ShortName()
+
+    def Node(self):
+      return self._node
+
+    def Index(self):
+      return self._node.Index()
+
+    def Request(self):
+      return self._request
+
+    def NodeCost(self):
+      return self._node_cost
+
+    def EdgeCost(self, s):
+      return self._edge_costs[s]
+
+    def StartTime(self):
+      return self._request.timing.requestTime * 1000
+
+    def EndTime(self):
+      return self._request.timing.requestTime * 1000 + self._node_cost
+
+    def EdgeAnnotation(self, s):
+      assert s.Node() in self.Node().Successors()
+      return self._edge_annotations.get(s, [])
+
+    def ContentType(self):
+      return log_parser.Resource.FromRequest(self._request).GetContentType()
+
+    def ShortName(self):
+      return log_parser.Resource.FromRequest(self._request).GetShortName()
+
+    def Url(self):
+      return self._request.url
+
+    def SetEdgeCost(self, child, cost):
+      assert child.Node() in self._node.Successors()
+      self._edge_costs[child] = cost
+
+    def AddEdgeAnnotation(self, s, annotation):
+      assert s.Node() in self._node.Successors()
+      self._edge_annotations.setdefault(s, []).append(annotation)
+
+    def ReparentTo(self, old_parent, new_parent):
+      """Move costs and annotatations from old_parent to new_parent.
+
+      Also updates the underlying node connections, ie, do not call
+      old_parent.RemoveSuccessor(), etc.
+
+      Args:
+        old_parent: the _NodeInfo of a current parent of self. We assert this
+          is actually a parent.
+        new_parent: the _NodeInfo of the new parent. We assert it is not already
+          a parent.
+      """
+      assert old_parent.Node() in self.Node().Predecessors()
+      assert new_parent.Node() not in self.Node().Predecessors()
+      edge_annotations = old_parent._edge_annotations.pop(self, [])
+      edge_cost =  old_parent._edge_costs.pop(self)
+      old_parent.Node().RemoveSuccessor(self.Node())
+      new_parent.Node().AddSuccessor(self.Node())
+      new_parent.SetEdgeCost(self, edge_cost)
+      for a in edge_annotations:
+        new_parent.AddEdgeAnnotation(self, edge_annotations)
+
+    def __eq__(self, o):
+      return self.Node().Index() == o.Node().Index()
+
+    def __hash__(self):
+      return hash(self.Node().Index())
+
+  def _ShortName(self, node):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[node.Index()].ShortName()
+
+  def _Url(self, node):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[node.Index()].Url()
+
+  def _NodeCost(self, node):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[node.Index()].NodeCost()
+
+  def _EdgeCost(self, parent, child):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[parent.Index()].EdgeCost(
+        self._node_info[child.Index()])
+
+  def _EdgeAnnotation(self, parent, child):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[parent.Index()].EdgeAnnotation(
+        self._node_info[child.Index()])
+
+  def _BuildDag(self, requests):
+    """Build DAG of resources.
+
+    Build a DAG from our requests and augment with _NodeInfo (see above) in a
+    parallel array indexed by Node.Index().
+
+    Creates self._nodes and self._node_info.
+
+    Args:
+      requests: [Request, ...] Requests from loading.log_parser.
+    """
+    self._nodes = []
+    self._node_info = []
+    indicies_by_url = {}
+    requests_by_completion = log_parser.SortedByCompletion(requests)
+    for request in requests:
+      next_index = len(self._nodes)
+      indicies_by_url.setdefault(request.url, []).append(next_index)
+      node = dag.Node(next_index)
+      node_info = self._NodeInfo(node, request)
+      self._nodes.append(node)
+      self._node_info.append(node_info)
+    for url, indicies in indicies_by_url.iteritems():
+      if len(indicies) > 1:
+        logging.warning('Multiple loads (%d) for url: %s' %
+                        (len(indicies), url))
+    for i in xrange(len(requests)):
+      request = requests[i]
+      current_node_info = self._node_info[i]
+      resource = log_parser.Resource.FromRequest(current_node_info.Request())
+      initiator = request.initiator
+      initiator_type = initiator['type']
+      predecessor_url = None
+      predecessor_type = None
+      # Classify & infer the predecessor. If a candidate url we identify as the
+      # predecessor is not in index_by_url, then we haven't seen it in our
+      # requests and we will try to find a better predecessor.
+      if initiator_type == 'parser':
+        url = initiator['url']
+        if url in indicies_by_url:
+          predecessor_url = url
+          predecessor_type = 'parser'
+      elif initiator_type == 'script' and 'stackTrace' in initiator:
+        for frame in initiator['stackTrace']:
+          url = frame['url']
+          if url in indicies_by_url:
+            predecessor_url = url
+            predecessor_type = 'stack'
+            break
+      elif initiator_type == 'script':
+        # When the initiator is a script without a stackTrace, infer that it
+        # comes from the most recent script from the same hostname.  TLD+1 might
+        # be better, but finding what is a TLD requires a database.
+        request_hostname = urlparse.urlparse(request.url).hostname
+        sorted_script_requests_from_hostname = [
+            r for r in requests_by_completion
+            if (resource.GetContentType() in ('script', 'html', 'json')
+                and urlparse.urlparse(r.url).hostname == request_hostname)]
+        most_recent = None
+        # Linear search is bad, but this shouldn't matter here.
+        for request in sorted_script_requests_from_hostname:
+          if request.timestamp < r.timing.requestTime:
+            most_recent = request
+          else:
+            break
+        if most_recent is not None:
+          if url in indicies_by_url:
+            predecessor_url = url
+            predecessor_type = 'script_inferred'
+      # TODO(mattcary): we skip initiator type other, is that correct?
+      if predecessor_url is not None:
+        predecessor = self._FindBestPredecessor(
+            current_node_info, indicies_by_url[predecessor_url])
+        edge_cost = current_node_info.StartTime() - predecessor.EndTime()
+        if edge_cost < 0: edge_cost = 0
+        if current_node_info.StartTime() < predecessor.StartTime():
+          logging.error('Inverted dependency: %s->%s',
+                        predecessor.ShortName(), current_node_info.ShortName())
+          # Note that current.StartTime() < predecessor.EndTime() appears to
+          # happen a fair amount in practice.
+        predecessor.Node().AddSuccessor(current_node_info.Node())
+        predecessor.SetEdgeCost(current_node_info, edge_cost)
+        predecessor.AddEdgeAnnotation(current_node_info, predecessor_type)
+
+  def _FindBestPredecessor(self, node_info, candidate_indicies):
+    """Find best predecessor for node_info
+
+    If there is only one candidate, we use it regardless of timings. We will
+    later warn about inverted dependencies. If there are more than one, we use
+    the latest whose end time is before node_info's start time. If there is no
+    such candidate, we throw up our hands and return an arbitrary one.
+
+    Args:
+      node_info: _NodeInfo of interest.
+      candidate_indicies: indicies of candidate predecessors.
+
+    Returns:
+      _NodeInfo of best predecessor.
+    """
+    assert candidate_indicies
+    if len(candidate_indicies) == 1:
+      return self._node_info[candidate_indicies[0]]
+    candidate = self._node_info[candidate_indicies[0]]
+    for i in xrange(1, len(candidate_indicies)):
+      next_candidate = self._node_info[candidate_indicies[i]]
+      if (next_candidate.EndTime() < node_info.StartTime() and
+          next_candidate.StartTime() > candidate.StartTime()):
+        candidate = next_candidate
+    if candidate.EndTime() > node_info.StartTime():
+      logging.warning('Multiple candidates but all inverted for ' +
+                      node_info.ShortName())
+    return candidate
+
+
+  def _SplitChildrenByTime(self, parent):
+    """Split children of a node by request times.
+
+    The initiator of a request may not be the true dependency of a request. For
+    example, a script may appear to load several resources independently, but in
+    fact one of them may be a JSON data file, and the remaining resources assets
+    described in the JSON. The assets should be dependent upon the JSON data
+    file, and not the original script.
+
+    This function approximates that by rearranging the children of a node
+    according to their request times. The predecessor of each child is made to
+    be the node with the greatest finishing time, that is before the start time
+    of the child.
+
+    We do this by sorting the nodes twice, once by start time and once by end
+    time. We mark the earliest end time, and then we walk the start time list,
+    advancing the end time mark when it is less than our current start time.
+
+    This is refined by only considering assets which we believe actually create
+    a dependency. We only split if the original parent is a script, and the new
+    parent a data file. We confirm these relationships heuristically by loading
+    pages multiple times and ensuring that dependacies do not change; see
+    CheckImageLoadConsistency() for details.
+
+    We incorporate this heuristic by skipping over any non-script/json resources
+    when moving the end mark.
+
+    TODO(mattcary): More heuristics, like incorporating cachability somehow, and
+    not just picking arbitrarily if there are two nodes with the same end time
+    (does that ever really happen?)
+
+    Args:
+      parent: the NodeInfo whose children we are going to rearrange.
+
+    """
+    if parent.ContentType() not in self._CAN_BE_TIMING_PARENT:
+      return  # No dependency changes.
+    children_by_start_time = [self._node_info[s.Index()]
+                              for s in parent.Node().Successors()]
+    children_by_start_time.sort(key=lambda c: c.StartTime())
+    children_by_end_time = [self._node_info[s.Index()]
+                            for s in parent.Node().Successors()]
+    children_by_end_time.sort(key=lambda c: c.EndTime())
+    end_mark = 0
+    for current in children_by_start_time:
+      if current.StartTime() < parent.EndTime() - 1e-5:
+        logging.warning('Child loaded before parent finished: %s -> %s',
+                        parent.ShortName(), current.ShortName())
+      go_to_next_child = False
+      while end_mark < len(children_by_end_time):
+        if children_by_end_time[end_mark] == current:
+          go_to_next_child = True
+          break
+        elif (children_by_end_time[end_mark].ContentType() not in
+            self._CAN_MAKE_TIMING_DEPENDENCE):
+          end_mark += 1
+        elif (end_mark < len(children_by_end_time) - 1 and
+              children_by_end_time[end_mark + 1].EndTime() <
+                  current.StartTime()):
+          end_mark += 1
+        else:
+          break
+      if end_mark >= len(children_by_end_time):
+        break  # It's not possible to rearrange any more children.
+      if go_to_next_child:
+        continue  # We can't rearrange this child, but the next child may be
+                  # eligible.
+      if children_by_end_time[end_mark].EndTime() <= current.StartTime():
+        current.ReparentTo(parent, children_by_end_time[end_mark])
+        children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
+
+  def _GraphvizNode(self, index, highlight):
+    """Returns a graphviz node description for a given node.
+
+    Args:
+      index: index of the node.
+      highlight: a list of node items to emphasize. Any resource url which
+        contains any highlight text will be distinguished in the output.
+
+    Returns:
+      A string describing the resource in graphviz format.
+      The resource is color-coded according to its content type, and its shape
+      is oval if its max-age is less than 300s (or if it's not cacheable).
+    """
+    node_info = self._node_info[index]
+    color = self._CONTENT_TYPE_TO_COLOR[node_info.ContentType()]
+    max_age = log_parser.MaxAge(node_info.Request())
+    shape = 'polygon' if max_age > 300 else 'oval'
+    styles = ['filled']
+    if highlight:
+      for fragment in highlight:
+        if fragment in node_info.Url():
+          styles.append('dotted')
+          break
+    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
+            'fillcolor = %s; shape = %s];\n'
+            % (index, node_info.ShortName(),
+               node_info.StartTime() - self._global_start,
+               node_info.EndTime() - self._global_start,
+               node_info.EndTime() - node_info.StartTime(),
+               ','.join(styles), color, shape))
+
+  @classmethod
+  def _IsAdUrl(cls, url):
+    """Return true if the url is an ad.
+
+    We group content that doesn't seem to be specific to the website along with
+    ads, eg staticxx.facebook.com, as well as analytics like googletagmanager (?
+    is this correct?).
+
+    Args:
+      url: The full string url to examine.
+
+    Returns:
+      True iff the url appears to be an ad.
+
+    """
+    # See below for how these patterns are defined.
+    AD_PATTERNS = ['2mdn.net',
+                   'admarvel.com',
+                   'adnxs.com',
+                   'adobedtm.com',
+                   'adsrvr.org',
+                   'adsafeprotected.com',
+                   'adsymptotic.com',
+                   'adtech.de',
+                   'adtechus.com',
+                   'advertising.com',
+                   'atwola.com',  # brand protection from cscglobal.com?
+                   'bounceexchange.com',
+                   'betrad.com',
+                   'casalemedia.com',
+                   'cloudfront.net//test.png',
+                   'cloudfront.net//atrk.js',
+                   'contextweb.com',
+                   'crwdcntrl.net',
+                   'doubleclick.net',
+                   'dynamicyield.com',
+                   'krxd.net',
+                   'facebook.com//ping',
+                   'fastclick.net',
+                   'google.com//-ads.js',
+                   'cse.google.com',  # Custom search engine.
+                   'googleadservices.com',
+                   'googlesyndication.com',
+                   'googletagmanager.com',
+                   'lightboxcdn.com',
+                   'mediaplex.com',
+                   'meltdsp.com',
+                   'mobile.nytimes.com//ads-success',
+                   'mookie1.com',
+                   'newrelic.com',
+                   'nr-data.net',   # Apparently part of newrelic.
+                   'optnmnstr.com',
+                   'pubmatic.com',
+                   'quantcast.com',
+                   'quantserve.com',
+                   'rubiconproject.com',
+                   'scorecardresearch.com',
+                   'sekindo.com',
+                   'serving-sys.com',
+                   'sharethrough.com',
+                   'staticxx.facebook.com',  # ?
+                   'syndication.twimg.com',
+                   'tapad.com',
+                   'yieldmo.com',
+                ]
+    parts = urlparse.urlparse(url)
+    for pattern in AD_PATTERNS:
+      if '//' in pattern:
+        domain, path = pattern.split('//')
+      else:
+        domain, path = (pattern, None)
+      if parts.netloc.endswith(domain):
+        if not path or path in parts.path:
+          return True
+    return False
+
+  def _ExtractImages(self):
+    """Return interesting image resources.
+
+    Uninteresting image resources are things like ads that we don't expect to be
+    constant across fetches.
+
+    Returns:
+      Dict of image url + short name to NodeInfo.
+    """
+    image_to_info = {}
+    for n in self._node_info:
+      if (n.ContentType().startswith('image') and
+          not self._IsAdUrl(n.Url())):
+        key = str((n.Url(), n.ShortName(), n.StartTime()))
+        assert key not in image_to_info, n.Url()
+        image_to_info[key] = n
+    return image_to_info
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
new file mode 100644
index 0000000..bdc3915
--- /dev/null
+++ b/loading/loading_model_unittest.py
@@ -0,0 +1,153 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import sys
+import unittest
+
+import dag
+import loading_model
+import log_parser
+
+class LoadingModelTestCase(unittest.TestCase):
+
+  def MakeParserRequest(self, url, source_url, start_time, end_time,
+                        magic_content_type=False):
+    timing_data = {f: -1 for f in log_parser.Timing._fields}
+    # We should ignore connectEnd.
+    timing_data['connectEnd'] = (end_time - start_time) / 2
+    timing_data['receiveHeadersEnd'] = end_time - start_time
+    timing_data['requestTime'] = start_time / 1000.0
+    return log_parser.RequestData(
+        None, {'Content-Type': 'null' if not magic_content_type
+                                      else 'magic-debug-content' },
+        None, start_time, timing_data, 'http://' + str(url), False,
+        {'type': 'parser', 'url': 'http://' + str(source_url)})
+
+  def SortedIndicies(self, graph):
+    return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
+
+  def SuccessorIndicies(self, node):
+    return [c.Index() for c in node.SortedSuccessors()]
+
+  def test_Costing(self):
+    requests = [self.MakeParserRequest(0, 'null', 100, 110),
+                self.MakeParserRequest(1, 0, 115, 120),
+                self.MakeParserRequest(2, 0, 112, 120),
+                self.MakeParserRequest(3, 1, 122, 126),
+                self.MakeParserRequest(4, 3, 127, 128),
+                self.MakeParserRequest(5, 'null', 100, 105),
+                self.MakeParserRequest(6, 5, 105, 110)]
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [4])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [6])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 5, 1, 2, 6, 3, 4])
+    self.assertEqual(28, graph.Cost())
+    graph.Set(cache_all=True)
+    self.assertEqual(8, graph.Cost())
+
+  def test_MaxPath(self):
+    requests = [self.MakeParserRequest(0, 'null', 100, 110),
+                self.MakeParserRequest(1, 0, 115, 120),
+                self.MakeParserRequest(2, 0, 112, 120),
+                self.MakeParserRequest(3, 1, 122, 126),
+                self.MakeParserRequest(4, 3, 127, 128),
+                self.MakeParserRequest(5, 'null', 100, 105),
+                self.MakeParserRequest(6, 5, 105, 110)]
+    graph = loading_model.ResourceGraph(requests)
+    path_list = []
+    self.assertEqual(28, graph.Cost(path_list))
+    self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
+
+    # More interesting would be a test when a node has multiple predecessors,
+    # but it's not possible for us to construct such a graph from requests yet.
+
+  def test_TimingSplit(self):
+    # Timing adds node 1 as a parent to 2 but not 3.
+    requests = [self.MakeParserRequest(0, 'null', 100, 110,
+                                       magic_content_type=True),
+                self.MakeParserRequest(1, 0, 115, 120,
+                                       magic_content_type=True),
+                self.MakeParserRequest(2, 0, 121, 122,
+                                       magic_content_type=True),
+                self.MakeParserRequest(3, 0, 112, 119),
+                self.MakeParserRequest(4, 2, 122, 126),
+                self.MakeParserRequest(5, 2, 122, 126)]
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
+
+    # Change node 1 so it is a parent of 3, which become parent of 2.
+    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
+                                         magic_content_type=True)
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
+
+    # Add an initiator dependence to 1 that will become the parent of 3.
+    requests[1] = self.MakeParserRequest(1, 0, 110, 111)
+    requests.append(self.MakeParserRequest(6, 1, 111, 112))
+    graph = loading_model.ResourceGraph(requests)
+    # Check it doesn't change until we change the content type of 1.
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3, 6])
+    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
+                                         magic_content_type=True)
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [3])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 6, 3, 2, 4, 5])
+
+  def test_TimingSplitImage(self):
+    # If we're all image types, then we shouldn't split by timing.
+    requests = [self.MakeParserRequest(0, 'null', 100, 110),
+                self.MakeParserRequest(1, 0, 115, 120),
+                self.MakeParserRequest(2, 0, 121, 122),
+                self.MakeParserRequest(3, 0, 112, 119),
+                self.MakeParserRequest(4, 2, 122, 126),
+                self.MakeParserRequest(5, 2, 122, 126)]
+    for r in requests:
+      r.headers['Content-Type'] = 'image/gif'
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5])
+
+  def test_AdUrl(self):
+    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
+        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/test.png'))
+    self.assertFalse(loading_model.ResourceGraph._IsAdUrl(
+        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/tst.png'))
+
+    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
+        'http://ums.adtechus.com/mapuser?providerid=1003;userid=RUmecco4z3o===='))
+    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
+        'http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/log_parser.py b/loading/log_parser.py
index e77fd4d..a58bd55 100644
--- a/loading/log_parser.py
+++ b/loading/log_parser.py
@@ -9,13 +9,11 @@ import json
 import operator
 import urlparse
 
-
 Timing = collections.namedtuple(
     'Timing',
     ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
      'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
-     'serviceWorkerFetchEnd', 'serviceWorkerFetchReady',
-     'serviceWorkerFetchStart', 'sslEnd', 'sslStart'])
+     'sslEnd', 'sslStart', 'workerReady', 'workerStart', 'loadingFinished'])
 
 
 class Resource(object):
@@ -33,7 +31,7 @@ class Resource(object):
 
   def GetShortName(self):
     """Returns either the hostname of the resource, or the filename,
-    or the end of the path.
+    or the end of the path. Tries to include the domain as much as possible.
     """
     parsed = urlparse.urlparse(self.url)
     path = parsed.path
@@ -41,17 +39,22 @@ class Resource(object):
       last_path = parsed.path.split('/')[-1]
       if len(last_path) < 10:
         if len(path) < 10:
-          return path
+          return parsed.hostname + '/' + path
         else:
-          return parsed.path[-10:]
+          return parsed.hostname + '/..' + parsed.path[-10:]
+      elif len(last_path) > 10:
+        return parsed.hostname + '/..' + last_path[:5]
       else:
-        return last_path
+        return parsed.hostname + '/..' + last_path
     else:
       return parsed.hostname
 
   def GetContentType(self):
     mime = self.content_type
-    if mime == 'text/html':
+    if 'magic-debug-content' in mime:
+      # A silly hack to make the unittesting easier.
+      return 'magic-debug-content'
+    elif mime == 'text/html':
       return 'html'
     elif mime == 'text/css':
       return 'css'
diff --git a/loading/log_requests.py b/loading/log_requests.py
index 16a10d5..071870c 100755
--- a/loading/log_requests.py
+++ b/loading/log_requests.py
@@ -42,6 +42,11 @@ def FlagChanger(device, command_line_path, new_flags):
     command_line_path: Full path to the command-line file.
     new_flags: Flags to add.
   """
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
   changer = flag_changer.FlagChanger(device, command_line_path)
   changer.AddFlags(new_flags)
   try:
@@ -53,6 +58,11 @@ def FlagChanger(device, command_line_path, new_flags):
 @contextlib.contextmanager
 def ForwardPort(device, local, remote):
   """Forwards a local port to a remote one on a device in a context."""
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
   device.adb.Forward(local, remote)
   try:
     yield
@@ -70,9 +80,11 @@ class AndroidRequestsLogger(object):
   """Logs all the requests made to load a page on a device."""
 
   def __init__(self, device):
+    """If device is None, we connect to a local chrome session."""
     self.device = device
     self._please_stop = False
     self._main_frame_id = None
+    self._tracing_data = []
 
   def _PageDataReceived(self, msg):
     """Called when a Page event is received.
@@ -93,6 +105,9 @@ class AndroidRequestsLogger(object):
           and params['frameId'] == self._main_frame_id):
       self._please_stop = True
 
+  def _TracingDataReceived(self, msg):
+    self._tracing_data.append(msg)
+
   def _LogPageLoadInternal(self, url, clear_cache):
     """Returns the collection of requests made to load a given URL.
 
@@ -132,32 +147,93 @@ class AndroidRequestsLogger(object):
       except websocket.WebSocketTimeoutException as e:
         logging.warning('Exception: ' + str(e))
         break
+    if not self._please_stop:
+      logging.warning('Finished with timeout instead of page load')
     inspector.StopMonitoringNetwork()
     return inspector.GetResponseData()
 
-  def LogPageLoad(self, url, clear_cache):
-    """Returns the collection of requests made to load a given URL on a device.
+  def _LogTracingInternal(self, url):
+    self._main_frame_id = None
+    self._please_stop = False
+    r = httplib.HTTPConnection('localhost', _PORT)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      logging.error('Cannot connect to the remote target.')
+      return None
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    ws.RegisterDomain('Tracing', self._TracingDataReceived)
+    logging.warning('Tracing.start: ' +
+                    str(ws.SyncRequest({'method': 'Tracing.start',
+                                        'options': 'zork'})))
+    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
+                              'params': {'url': url}})
+    while not self._please_stop:
+      try:
+        ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException:
+        break
+    if not self._please_stop:
+      logging.warning('Finished with timeout instead of page load')
+    return {'events': self._tracing_data,
+            'end': ws.SyncRequest({'method': 'Tracing.end'})}
+
+  def _DoSomeLogging(self, package, fn):
+    """Start logging process.
+
+    Sets up any device and tracing appropriately and then executes the core
+    logging function.
 
     Args:
-      url: (str) URL to load on the device.
-      clear_cache: (bool) Whether to clear the HTTP cache.
+      package: the key for chrome package info.
+      fn: the function to execute that launches chrome and performs the
+      appropriate instrumentation, see _Log*Internal().
 
     Returns:
-      See _LogPageLoadInternal().
+      As fn() returns.
     """
-    package_info = constants.PACKAGE_INFO['chrome']
+    package_info = constants.PACKAGE_INFO[package]
     command_line_path = '/data/local/chrome-command-line'
     new_flags = ['--enable-test-events', '--remote-debugging-port=%d' % _PORT]
-    _SetUpDevice(self.device, package_info)
+    if self.device:
+      _SetUpDevice(self.device, package_info)
     with FlagChanger(self.device, command_line_path, new_flags):
-      start_intent = intent.Intent(
-          package=package_info.package, activity=package_info.activity,
-          data='about:blank')
-      self.device.StartActivity(start_intent, blocking=True)
-      time.sleep(2)
+      if self.device:
+        start_intent = intent.Intent(
+            package=package_info.package, activity=package_info.activity,
+            data='about:blank')
+        self.device.StartActivity(start_intent, blocking=True)
+        time.sleep(2)
+      # If no device, we don't care about chrome startup so skip the about page.
       with ForwardPort(self.device, 'tcp:%d' % _PORT,
                        'localabstract:chrome_devtools_remote'):
-        return self._LogPageLoadInternal(url, clear_cache)
+        return fn()
+
+  def LogPageLoad(self, url, clear_cache, package):
+    """Returns the collection of requests made to load a given URL on a device.
+
+    Args:
+      url: (str) URL to load on the device.
+      clear_cache: (bool) Whether to clear the HTTP cache.
+
+    Returns:
+      See _LogPageLoadInternal().
+    """
+    return self._DoSomeLogging(
+        package, lambda: self._LogPageLoadInternal(url, clear_cache))
+
+  def LogTracing(self, url):
+    """Log tracing events from a load of the given URL.
+
+    TODO(mattcary): This doesn't work. It would be best to log tracing
+    simultaneously with network requests, but as that wasn't working the tracing
+    logging was broken out separately. It still doesn't work...
+    """
+    return self._DoSomeLogging('chrome', lambda: self._LogTracingInternal(url))
 
 
 def _ResponseDataToJson(data):
@@ -197,16 +273,26 @@ def _CreateOptionParser():
   parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
                                               'before loading the URL.'),
                     default=True, action='store_false', dest='clear_cache')
+  parser.add_option('--package', help='Package info for chrome build. '
+                                      'See build/android/pylib/constants.',
+                    default='chrome')
+  parser.add_option('--local', action='store_true', default=False,
+                    help='Connect to local chrome session rather than android.')
   return parser
 
 
 def main():
+  logging.basicConfig(level=logging.WARNING)
   parser = _CreateOptionParser()
   options, _ = parser.parse_args()
-  devices = device_utils.DeviceUtils.HealthyDevices()
-  device = devices[0]
+  if options.local:
+    device = None
+  else:
+    devices = device_utils.DeviceUtils.HealthyDevices()
+    device = devices[0]
   request_logger = AndroidRequestsLogger(device)
-  response_data = request_logger.LogPageLoad(options.url, options.clear_cache)
+  response_data = request_logger.LogPageLoad(
+      options.url, options.clear_cache, options.package)
   json_data = _ResponseDataToJson(response_data)
   with open(options.output, 'w') as f:
     f.write(json_data)
diff --git a/loading/node_cost_csv.py b/loading/node_cost_csv.py
new file mode 100755
index 0000000..79d3b06
--- /dev/null
+++ b/loading/node_cost_csv.py
@@ -0,0 +1,61 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import logging
+import os
+import sys
+
+from processing import (SitesFromDir, WarmGraph, ColdGraph)
+
+
+def main():
+  logging.basicConfig(level=logging.ERROR)
+  parser = argparse.ArgumentParser(
+      description=('Convert a directory created by ./analyze.py fetch '
+                   'to a node cost CSV which compares cold and warm total '
+                   'node costs.'))
+  parser.add_argument('--datadir', required=True)
+  parser.add_argument('--csv', required=True)
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args()
+  sites = SitesFromDir(args.datadir)
+  with open(args.csv, 'w') as output:
+    output.write('site,cold.total,warm.total,cold.common,warm.common,'
+                 'cold.node.count,common.cold.node.count,'
+                 'cold.all.edges,warm.all.edges,'
+                 'cold.common.edges,warm.common.edges,'
+                 'cold.edge.fraction,common.cold.edge.fraction\n')
+    for site in sites:
+      print site
+      warm = WarmGraph(args.datadir, site)
+      if args.noads:
+        warm.Set(node_filter=warm.FilterAds)
+      cold = ColdGraph(args.datadir, site)
+      if args.noads:
+        cold.Set(node_filter=cold.FilterAds)
+      common = [p for p in cold.Intersect(warm.Nodes())]
+      common_cold = set([c.Node() for c, w in common])
+      common_warm = set([w.Node() for c, w in common])
+      output.write(','.join([str(s) for s in [
+          site,
+          sum((n.NodeCost() for n in cold.Nodes())),
+          sum((n.NodeCost() for n in warm.Nodes())),
+          sum((c.NodeCost() for c, w in common)),
+          sum((w.NodeCost() for c, w in common)),
+          sum((1 for n in cold.Nodes())),
+          len(common),
+          cold.EdgeCosts(), warm.EdgeCosts(),
+          cold.EdgeCosts(lambda n: n in common_cold),
+          warm.EdgeCosts(lambda n: n in common_warm),
+          (cold.EdgeCosts() /
+           sum((n.NodeCost() for n in cold.Nodes()))),
+          (cold.EdgeCosts(lambda n: n in common_cold) /
+           sum((c.NodeCost() for c, w in common)))
+          ]]) + '\n')
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/processing.py b/loading/processing.py
new file mode 100644
index 0000000..a6ddde1
--- /dev/null
+++ b/loading/processing.py
@@ -0,0 +1,71 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import os.path
+import sys
+
+import log_parser
+import loading_model
+
+
+def SitesFromDir(dir):
+  """Extract sites from a data dir.
+
+  Based on ./analyze.py fetch file name conventions. We assume each site
+  corresponds to two files, <site>.json and <site>.json.cold, and that no other
+  kind of file appears in the data directory.
+
+  Args:
+    dir: the directory to process.
+
+  Returns:
+    A list of sites as strings.
+
+  """
+  files = set(os.listdir(dir))
+  assert files
+  sites = []
+  for f in files:
+    if f.endswith('.png'): continue
+    assert f.endswith('.json') or f.endswith('.json.cold'), f
+    if f.endswith('.json'):
+      assert f + '.cold' in files
+      sites.append(f[:f.rfind('.json')])
+    elif f.endswith('.cold'):
+      assert f[:f.rfind('.cold')] in files
+  sites.sort()
+  return sites
+
+
+def WarmGraph(datadir, site):
+  """Return a loading model graph for the warm pull of site.
+
+  Based on ./analyze.py fetch file name conventions.
+
+  Args:
+    datadir: the directory containing site JSON data.
+    site: a site string.
+
+  Returns:
+    A loading model object.
+  """
+  return loading_model.ResourceGraph(log_parser.FilterRequests(
+      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json'))))
+
+
+def ColdGraph(datadir, site):
+  """Return a loading model graph for the cold pull of site.
+
+  Based on ./analyze.py fetch file name conventions.
+
+  Args:
+    datadir: the directory containing site JSON data.
+    site: a site string.
+
+  Returns:
+    A loading model object.
+  """
+  return loading_model.ResourceGraph(log_parser.FilterRequests(
+      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json.cold'))))
diff --git a/loading/util.r b/loading/util.r
new file mode 100644
index 0000000..a65c572
--- /dev/null
+++ b/loading/util.r
@@ -0,0 +1,47 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Useful R routines for analyzing output from several cost_to_csv.py
+# output and producing interesting graphs.
+
+combine.runs <- function(times, prefix, suffix)
+  do.call("rbind", lapply(times, function (t)
+    with(read.csv(paste0(prefix, t, suffix)),
+         data.frame(site, kind, cost, time=t))))
+
+get.ordered.names <- function(runs) {
+  means <- with(runs, tapply(cost, list(site, kind), mean))
+  return(names(means[,"cold"])[order(means[,"cold"])])
+}
+
+plot.warm.cold <- function(runs, main="") {
+  ordered.names <- get.ordered.names(runs)
+  n <- length(ordered.names)
+  par(mar=c(8,4,4,4), bg="white")
+  plot(NULL, xlim=c(1,25), ylim=range(runs$cost), xaxt="n",
+       ylab="ms", xlab="", main=main)
+  axis(1, 1:n, labels=ordered.names, las=2)
+  getdata <- function(k, t) sapply(
+      ordered.names, function (s) with(runs, cost[site==s & kind==k & time==t]))
+  for (t in unique(runs$time)) {
+    points(1:n, getdata("cold", t), pch=1)
+    points(1:n, getdata("warm", t), pch=3)
+  }
+  legend("topleft", pch=c(1, 3), legend=c("cold", "warm"))
+}
+
+plot.relative.sds <- function(runs, main="") {
+  sds <- with(runs, tapply(cost, list(site, kind), sd))
+  means <- with(runs, tapply(cost, list(site, kind), mean))
+  ordered.names <- get.ordered.names(runs)
+  n <- length(ordered.names)
+  par(mar=c(8,4,4,4), bg="white")
+  plot(NULL, xlim=c(1,25), ylim=c(0,.8),
+       xaxt="n", ylab="Relative SD", xlab="", main=main)
+  axis(1, 1:n, labels=ordered.names, las=2)
+  getdata <- function(k) sapply(ordered.names, function(s) (sds/means)[s, k])
+  points(1:n, getdata("cold"), pch=1)
+  points(1:n, getdata("warm"), pch=3)
+  legend("topleft", pch=c(1, 3), legend=c("cold", "warm"))
+}

commit fa2b2135fd3d1946be06e49c195b4fa9c6b90fc9
Author: wangxianzhu <wangxianzhu@chromium.org>
Date:   Fri Jan 8 15:53:24 2016 -0800

    Fix crash in device_forwarder
    
    There are many crashes in device_forwarder on bots, e.g.
    https://build.chromium.org/p/tryserver.chromium.android/builders/linux_android_rel_ng/builds/5713/steps/stack_tool_for_tombstones/logs/stdio/text
    
    The original method was incorrect because
    ServiceDelegate::DeleteControllerOnInternalThread()
    may execute after ServiceDelegate::~ServiceDelegate().
    
    Review URL: https://codereview.chromium.org/1571643003
    
    Cr-Original-Commit-Position: refs/heads/master@{#368466}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 91b8d5ad02d99b59cb17e100eb645b2fefb4eb7f

diff --git a/forwarder2/device_forwarder_main.cc b/forwarder2/device_forwarder_main.cc
index fdf5fe5..09790f9 100644
--- a/forwarder2/device_forwarder_main.cc
+++ b/forwarder2/device_forwarder_main.cc
@@ -52,19 +52,9 @@ class ServerDelegate : public Daemon::ServerDelegate {
     if (!controller_thread_.get())
       return;
     // The DeviceController instance, if any, is constructed on the controller
-    // thread. Make sure that it gets deleted on that same thread. Note that
-    // DeleteSoon() is not used here since it would imply reading |controller_|
-    // from the main thread while it's set on the internal thread.
-    controller_thread_->task_runner()->PostTask(
-        FROM_HERE,
-        base::Bind(&ServerDelegate::DeleteControllerOnInternalThread,
-                   base::Unretained(this)));
-  }
-
-  void DeleteControllerOnInternalThread() {
-    DCHECK(
-        controller_thread_->task_runner()->RunsTasksOnCurrentThread());
-    controller_.reset();
+    // thread. Make sure that it gets deleted on that same thread.
+    controller_thread_->task_runner()->DeleteSoon(
+        FROM_HERE, controller_.release());
   }
 
   // Daemon::ServerDelegate:

commit 837d21cc509b7b52049f515b523a7450be851df0
Author: thakis <thakis@chromium.org>
Date:   Fri Jan 8 11:37:43 2016 -0800

    Enable warning on reserved user-defined literals everywhere but CrOS.
    
    This was disabled because a dbus header was missing a few spaces.
    The version of the dbus header in the sysroot has them though,
    and we use the sysroot everywhere except in Chrome OS-on-Linux
    builds these days.
    
    The spaces were added here, almost 3 years ago by now:
    http://cgit.freedesktop.org/dbus/dbus/commit/?h=dbus-1.4&id=51b88b4c7919487290c0862b013cd8e7cd2de34b
    
    No behavior change.
    
    BUG=263960, 573778
    
    Review URL: https://codereview.chromium.org/1570193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368408}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6fc6aa0987377a5d5633103de9c153fcccbf2ed8

diff --git a/memdump/memdump.cc b/memdump/memdump.cc
index 0564f85..7f135ee 100644
--- a/memdump/memdump.cc
+++ b/memdump/memdump.cc
@@ -422,9 +422,9 @@ void DumpProcessesMemoryMapsInExtendedFormat(
       AppendAppSharedField(memory_map.app_shared_pages, &app_shared_buf);
       base::SStringPrintf(
           &buf,
-          "%"PRIx64"-%"PRIx64" %s %"PRIx64" private_unevictable=%d private=%d "
-          "shared_app=%s shared_other_unevictable=%d shared_other=%d "
-          "\"%s\" [%s]\n",
+          "%" PRIx64 "-%" PRIx64 " %s %" PRIx64 " private_unevictable=%d "
+          "private=%d shared_app=%s shared_other_unevictable=%d "
+          "shared_other=%d \"%s\" [%s]\n",
           memory_map.start_address,
           memory_map.end_address,
           memory_map.flags.c_str(),

commit 60002985a7fa9e3cd8e483abd4b052be5740227c
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jan 8 07:52:58 2016 -0800

    customtabs: Add a test script for benchmarking.
    
    BUG=520967
    
    Review URL: https://codereview.chromium.org/1292363002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368337}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 76bb5b8d785094765773c926e54046effda39a9e

diff --git a/customtabs_benchmark/scripts/__init__.py b/customtabs_benchmark/scripts/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/customtabs_benchmark/scripts/customtabs_benchmark.py b/customtabs_benchmark/scripts/customtabs_benchmark.py
new file mode 100755
index 0000000..9bf4343
--- /dev/null
+++ b/customtabs_benchmark/scripts/customtabs_benchmark.py
@@ -0,0 +1,170 @@
+#!/usr/bin/python
+#
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loops Custom Tabs tests and outputs the results into a CSV file."""
+
+import logging
+import optparse
+import os
+import re
+import sys
+import time
+
+sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
+                             os.pardir, os.pardir, 'build', 'android'))
+
+from pylib.device import device_errors
+from pylib.device import device_utils
+from pylib.device import intent
+from pylib.perf import cache_control
+
+
+def RunOnce(device, url, warmup, no_prerendering, delay_to_may_launch_url,
+            delay_to_launch_url, cold):
+  """Runs a test on a device once.
+
+  Args:
+    device: (DeviceUtils) device to run the tests on.
+    warmup: (bool) Whether to call warmup.
+    no_prerendering: (bool) Whether to disable prerendering.
+    delay_to_may_launch_url: (int) Delay to mayLaunchUrl() in ms.
+    delay_to_launch_url: (int) Delay to launchUrl() in ms.
+    cold: (bool) Whether the page cache should be dropped.
+
+  Returns:
+    The output line (str), like this (one line only):
+    <warmup>,<no_prerendering>,<delay_to_may_launch_url>,<intent_sent_ms>,
+      <page_load_started_ms>,<page_load_finished_ms>
+    or None on error.
+  """
+  launch_intent = intent.Intent(
+      action='android.intent.action.MAIN',
+      package='org.chromium.customtabsclient.test',
+      activity='org.chromium.customtabs.test.MainActivity',
+      extras={'url': url, 'warmup': warmup, 'no_prerendering': no_prerendering,
+              'delay_to_may_launch_url': delay_to_may_launch_url,
+              'delay_to_launch_url': delay_to_launch_url})
+  result_line_re = re.compile(r'W/CUSTOMTABSBENCH.*: (.*)')
+  logcat_monitor = device.GetLogcatMonitor(clear=True)
+  logcat_monitor.Start()
+  device.ForceStop('com.google.android.apps.chrome')
+  device.ForceStop('org.chromium.customtabsclient.test')
+  if cold:
+    if not device.HasRoot():
+      device.EnableRoot()
+    cache_control.CacheControl(device).DropRamCaches()
+  device.StartActivity(launch_intent, blocking=True)
+  match = None
+  try:
+    match = logcat_monitor.WaitFor(result_line_re, timeout=10)
+  except device_errors.CommandTimeoutError as e:
+    logging.warning('Timeout waiting for the result line')
+  return match.group(1) if match is not None else None
+
+
+def LoopOnDevice(device, url, warmup, no_prerendering, delay_to_may_launch_url,
+                 delay_to_launch_url, cold, output_filename, once=False):
+  """Loops the tests on a device.
+
+  Args:
+    device: (DeviceUtils) device to run the tests on.
+    url: (str) URL to navigate to.
+    warmup: (bool) Whether to call warmup.
+    no_prerendering: (bool) Whether to disable prerendering.
+    delay_to_may_launch_url: (int) Delay to mayLaunchUrl() in ms.
+    delay_to_launch_url: (int) Delay to launchUrl() in ms.
+    cold: (bool) Whether the page cache should be dropped.
+    output_filename: (str) Output filename. '-' for stdout.
+    once: (bool) Run only once.
+  """
+  while True:
+    out = sys.stdout if output_filename == '-' else open(output_filename, 'a')
+    try:
+      result = RunOnce(device, url, warmup, no_prerendering,
+                       delay_to_may_launch_url, delay_to_launch_url, cold)
+      if result is not None:
+        out.write(result + '\n')
+        out.flush()
+      if once:
+        return
+      time.sleep(10)
+    finally:
+      if output_filename != '-':
+        out.close()
+
+
+def ProcessOutput(filename):
+  """Reads an output file, and returns a processed numpy array.
+
+  Args:
+    filename: (str) file to process.
+
+  Returns:
+    A numpy structured array.
+  """
+  import numpy as np
+  data = np.genfromtxt(filename, delimiter=',')
+  result = np.array(np.zeros(len(data)),
+                    dtype=[('warmup', bool), ('no_prerendering', bool),
+                           ('delay_to_may_launch_url', np.int32),
+                           ('delay_to_launch_url', np.int32),
+                           ('commit', np.int32), ('plt', np.int32)])
+  result['warmup'] = data[:, 0]
+  result['no_prerendering'] = data[:, 1]
+  result['delay_to_may_launch_url'] = data[:, 2]
+  result['delay_to_launch_url'] = data[:, 3]
+  result['commit'] = data[:, 5] - data[:, 4]
+  result['plt'] = data[:, 6] - data[:, 4]
+  return result
+
+
+def _CreateOptionParser():
+  parser = optparse.OptionParser(description='Loops Custom Tabs tests on a '
+                                 'device, and outputs the navigation timings '
+                                 'in a CSV file.')
+  parser.add_option('--device', help='Device ID')
+  parser.add_option('--url', help='URL to navigate to.',
+                    default='https://www.android.com')
+  parser.add_option('--warmup', help='Call warmup.', default=False,
+                    action='store_true')
+  parser.add_option('--no_prerendering', help='Disable prerendering.',
+                    default=False, action='store_true')
+  parser.add_option('--delay_to_may_launch_url',
+                    help='Delay before calling mayLaunchUrl() in ms.',
+                    type='int')
+  parser.add_option('--delay_to_launch_url',
+                    help='Delay before calling launchUrl() in ms.',
+                    type='int')
+  parser.add_option('--cold', help='Purge the page cache before each run.',
+                    default=False, action='store_true')
+  parser.add_option('--output_file', help='Output file (append). "-" for '
+                    'stdout')
+  parser.add_option('--once', help='Run only one iteration.',
+                    action='store_true', default=False)
+  return parser
+
+
+def main():
+  parser = _CreateOptionParser()
+  options, _ = parser.parse_args()
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  device = devices[0]
+  if len(devices) != 1 and options.device is None:
+    logging.error('Several devices attached, must specify one with --device.')
+    sys.exit(0)
+  if options.device is not None:
+    matching_devices = [d for d in devices if str(d) == options.device]
+    if len(matching_devices) == 0:
+      logging.error('Device not found.')
+      sys.exit(0)
+    device = matching_devices[0]
+  LoopOnDevice(device, options.url, options.warmup, options.no_prerendering,
+               options.delay_to_may_launch_url, options.delay_to_launch_url,
+               options.cold, options.output_file, options.once)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/customtabs_benchmark/scripts/run_benchmark.py b/customtabs_benchmark/scripts/run_benchmark.py
new file mode 100755
index 0000000..5df547b
--- /dev/null
+++ b/customtabs_benchmark/scripts/run_benchmark.py
@@ -0,0 +1,140 @@
+#!/usr/bin/env python
+#
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loops Custom Tabs tests and outputs the results into a CSV file."""
+
+import copy
+import json
+import logging
+import optparse
+import os
+import random
+import sys
+import threading
+
+sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
+                             os.pardir, os.pardir, 'build', 'android'))
+
+from pylib.device import device_utils
+
+import customtabs_benchmark
+
+
+_KEYS = ['url', 'warmup', 'no_prerendering', 'delay_to_may_launch_url',
+         'delay_to_launch_url', 'cold']
+
+
+def _ParseConfiguration(filename):
+  """Reads a JSON file and returns a list of configurations.
+
+  Each valid value in the JSON file can be either a scalar or a list of
+  values. This function expands the scalar values to be lists. All list must
+  have the same length.
+
+  Sample configuration:
+  {
+    "url": "https://www.android.com",
+    "warmup": [false, true],
+    "no_prerendering": false,
+    "delay_to_may_launch_url": [-1, 1000],
+    "delay_to_launch_url": [-1, 1000],
+    "cold": true
+  }
+
+  Args:
+    filename: (str) Point to a file containins a JSON dictionnary of config
+              values.
+
+  Returns:
+    A list of configurations, where each value is specified.
+  """
+  config = json.load(open(filename, 'r'))
+  has_all_values = all(k in config for k in _KEYS)
+  assert has_all_values
+  config['url'] = str(config['url']) # Intents don't like unicode.
+  has_list = any(isinstance(config[k], list) for k in _KEYS)
+  if not has_list:
+    return [config]
+  list_keys = [k for k in _KEYS if isinstance(config[k], list)]
+  list_length = len(config[list_keys[0]])
+  assert all(len(config[k]) == list_length for k in list_keys)
+  result = []
+  for i in range(list_length):
+    result.append(copy.deepcopy(config))
+    for k in list_keys:
+      result[-1][k] = result[-1][k][i]
+  return result
+
+
+def _CreateOptionParser():
+  parser = optparse.OptionParser(description='Loops tests on all attached '
+                                 'devices, with randomly selected '
+                                 'configurations, and outputs the results in '
+                                 'CSV files.')
+  parser.add_option('--config', help='JSON configuration file. Required.')
+  parser.add_option('--output_file_prefix', help='Output file prefix. Actual '
+                    'output file is prefix_<device ID>.csv', default='result')
+  return parser
+
+
+def _RunOnDevice(device, output_filename, configs, should_stop):
+  """Loops the tests described by configs on a device.
+
+  Args:
+    device: (DeviceUtils) device to run the tests on.
+    output_filename: (str) Output file name.
+    configs: (list of dict) List of configurations.
+    should_stop: (Event) When set, this function should return.
+  """
+  with open(output_filename, 'a') as f:
+    while not should_stop.is_set():
+      config = configs[random.randint(0, len(configs) - 1)]
+      result = customtabs_benchmark.RunOnce(
+          device, config['url'], config['warmup'], config['no_prerendering'],
+          config['delay_to_may_launch_url'], config['delay_to_launch_url'],
+          config['cold'])
+      if result is not None:
+        f.write(result + '\n')
+        f.flush()
+      should_stop.wait(10.)
+
+
+def _Run(output_file_prefix, configs):
+  """Loops the tests described by the configs on connected devices.
+
+  Args:
+    output_file_prefix: (str) Prefix for the output file name.
+    configs: (list of dict) List of configurations.
+  """
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  should_stop = threading.Event()
+  threads = []
+  for device in devices:
+    output_filename = '%s_%s.csv' % (output_file_prefix, str(device))
+    thread = threading.Thread(
+        target=_RunOnDevice,
+        args=(device, output_filename, configs, should_stop))
+    thread.start()
+    threads.append(thread)
+  for thread in threads:
+    try:
+      thread.join()
+    except KeyboardInterrupt as e:
+      should_stop.set()
+
+
+def main():
+  parser = _CreateOptionParser()
+  options, _ = parser.parse_args()
+  if options.config is None:
+    logging.error('A configuration file must be provided.')
+    sys.exit(0)
+  configs = _ParseConfiguration(options.config)
+  _Run(options.output_file_prefix, configs)
+
+
+if __name__ == '__main__':
+  main()

commit daf76dc482f89d42dcbacddb0198a0c5be71f894
Author: pasko <pasko@chromium.org>
Date:   Fri Jan 8 07:16:56 2016 -0800

    Add OWNERS for tools/android/customtabs_benchmark
    
    BUG=none
    
    Review URL: https://codereview.chromium.org/1573463002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368331}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4caac96fa8857a46100feeac45c74cef264c0cb4

diff --git a/customtabs_benchmark/OWNERS b/customtabs_benchmark/OWNERS
new file mode 100644
index 0000000..c319c21
--- /dev/null
+++ b/customtabs_benchmark/OWNERS
@@ -0,0 +1,3 @@
+lizeb@chromium.org
+pasko@chromium.org
+yusufo@chromium.org

commit 05879a9bdf44908492f2984c29aa6b10f635719b
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 5 11:00:14 2016 -0800

    tools/android/loading: Add service worker timings.
    
    Names have changed, making the current version crash.
    
    Review URL: https://codereview.chromium.org/1555343002
    
    Cr-Original-Commit-Position: refs/heads/master@{#367597}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9a43baf66057981abc88abaed50523ce6f4b5e00

diff --git a/loading/log_parser.py b/loading/log_parser.py
index bb2f953..e77fd4d 100644
--- a/loading/log_parser.py
+++ b/loading/log_parser.py
@@ -14,7 +14,8 @@ Timing = collections.namedtuple(
     'Timing',
     ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
      'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
-     'sslEnd', 'sslStart', 'workerReady', 'workerStart'])
+     'serviceWorkerFetchEnd', 'serviceWorkerFetchReady',
+     'serviceWorkerFetchStart', 'sslEnd', 'sslStart'])
 
 
 class Resource(object):

commit 42172ce9ef5ffc04ac7d8a5ab13227e5d3bb729d
Author: zqzhang <zqzhang@chromium.org>
Date:   Tue Jan 5 09:58:56 2016 -0800

    A simple app for simulating audio focus actions for testing MediaSession
    
    We introduce a simple app, called "AudioFocusGrabber", for testing audio
    focus handling in apps, especially MediaSession in Chrome. It can
    simulate a short sound when receiving an SMS or email, requesting the
    current playback to pause or lower its volume for a short while. It can
    also simulate the situation that another media player app gains the
    audio focus permanently, where the app under test need to pause the
    playback until the user explicitly resumes it.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1513743002
    
    Cr-Original-Commit-Position: refs/heads/master@{#367571}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0dfd8454bbdc16a1e3a12d8c8889de9ca1508407

diff --git a/BUILD.gn b/BUILD.gn
index 102072f..012dc35 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -68,3 +68,11 @@ group("customtabs_benchmark") {
     "//tools/android/customtabs_benchmark:customtabs_benchmark_apk",
   ]
 }
+
+# GYP: //tools/android/android_tools.gyp:audio_focus_grabber
+group("audio_focus_grabber") {
+  testonly = true
+  deps = [
+    "//tools/android/audio_focus_grabber:audio_focus_grabber_apk",
+  ]
+}
diff --git a/android_tools.gyp b/android_tools.gyp
index 152636a..b963b3f 100644
--- a/android_tools.gyp
+++ b/android_tools.gyp
@@ -77,5 +77,13 @@
         'customtabs_benchmark/customtabs_benchmark.gyp:customtabs_benchmark_apk',
       ],
     },
+    {
+      # GN: //tools/android:audio_focus_grabber
+      'target_name': 'audio_focus_grabber',
+      'type': 'none',
+      'dependencies': [
+        'audio_focus_grabber/audio_focus_grabber.gyp:audio_focus_grabber_apk',
+      ],
+    },
   ],
 }
diff --git a/audio_focus_grabber/BUILD.gn b/audio_focus_grabber/BUILD.gn
new file mode 100644
index 0000000..94ea751
--- /dev/null
+++ b/audio_focus_grabber/BUILD.gn
@@ -0,0 +1,29 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import("//build/config/android/rules.gni")
+
+# GYP: //tools/android/audio_focus_grabber/audio_focus_grabber.gyp:audio_focus_grabber_apk
+android_apk("audio_focus_grabber_apk") {
+  testonly = true
+  android_manifest = "java/AndroidManifest.xml"
+  apk_name = "AudioFocusGrabber"
+
+  deps = [
+    ":audio_focus_grabber_apk_resources",
+    "//base:base_java",
+    "//third_party/android_tools:android_support_v13_java",
+  ]
+
+  java_files = [
+    "java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java",
+    "java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java",
+  ]
+}
+
+android_resources("audio_focus_grabber_apk_resources") {
+  testonly = true
+  resource_dirs = [ "java/res" ]
+  android_manifest = "java/AndroidManifest.xml"
+}
diff --git a/audio_focus_grabber/OWNERS b/audio_focus_grabber/OWNERS
new file mode 100644
index 0000000..5ad525b
--- /dev/null
+++ b/audio_focus_grabber/OWNERS
@@ -0,0 +1 @@
+zqzhang@chromium.org
diff --git a/audio_focus_grabber/README.md b/audio_focus_grabber/README.md
new file mode 100644
index 0000000..9640898
--- /dev/null
+++ b/audio_focus_grabber/README.md
@@ -0,0 +1,50 @@
+## AudioFocusGrabber: a Tool for Testing Audio Focus Handling in Apps
+
+A simple app used to test audio focus handling in apps, especially
+MediaSession in Chrome. You can perform audio gain/abandon actions, in
+order to simulate a short ping from an SMS or email, or permanent
+audio focus gain from other media player apps.
+
+### Setup
+
+#### 1: Build and install the AudioFocusGrabber app
+
+	ninja -C out/Debug audio_focus_grabber_apk
+	adb install -r out/Debug/apks/AudioFocusGrabber.apk
+
+#### 2: Simulate audio focus actions
+
+You can simulate audio focus actions using the UI, the notification
+bar or through `adb` shell. There are three kinds of audio focus
+actions, corresponding to:
+
+* `AudioManager.AUDIOFOCUS_GAIN`
+* `AudioManager.AUDIOFOCUS_TRANSIENT`
+* `AudioManager.AUDIOFOCUS_TRANSIENT_MAY_DUCK`
+
+##### 2.1: Controlling from the UI
+
+From the UI, there are three buttons for the three actions. Just click
+it, and AudioFocusGrabber will perform the audio focus action, and play a ping
+sound.
+
+However in this way, the app must be in background.
+
+##### 2.2: Controlling from the notification
+
+You can also start a notification from the UI, and then you can make
+controls from the notification.
+
+In this way, the app must be in background or losed window focus.
+
+##### 2.3 Controlling from the `adb` shell
+
+From the `adb` shell, which you can do it even if the AudioFocusGrabber is not
+in foreground. You may use the following three commands:
+
+	adb shell am startservice -a AUDIO_FOCUS_GRABBER_GAIN
+	adb shell am startservice -a AUDIO_FOCUS_GRABBER_TRANSIENT_PAUSE
+	adb shell am startservice -a AUDIO_FOCUS_GRABBER_TRANSIENT_DUCK
+
+In this way, the app may be in the foreground, in the background or
+losed window focus.
diff --git a/audio_focus_grabber/audio_focus_grabber.gyp b/audio_focus_grabber/audio_focus_grabber.gyp
new file mode 100644
index 0000000..bad0432
--- /dev/null
+++ b/audio_focus_grabber/audio_focus_grabber.gyp
@@ -0,0 +1,22 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+{
+  'targets': [
+    {
+      # GN: //tools/android/audio_focus_grabber:audio_focus_grabber_apk
+      'target_name': 'audio_focus_grabber_apk',
+      'type': 'none',
+      'variables': {
+        'apk_name': 'AudioFocusGrabber',
+        'android_manifest_path': 'java/AndroidManifest.xml',
+        'java_in_dir': 'java',
+        'resource_dir': 'java/res',
+      },
+      'dependencies': [
+        '../../../base/base.gyp:base_java',
+        '../../../third_party/android_tools/android_tools.gyp:android_support_v13_javalib'
+      ],
+      'includes': [ '../../../build/java_apk.gypi' ],
+  }]
+}
diff --git a/audio_focus_grabber/java/AndroidManifest.xml b/audio_focus_grabber/java/AndroidManifest.xml
new file mode 100644
index 0000000..e83b915
--- /dev/null
+++ b/audio_focus_grabber/java/AndroidManifest.xml
@@ -0,0 +1,37 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2016 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<manifest xmlns:android="http://schemas.android.com/apk/res/android"
+    package="org.chromium.tools.audio_focus_grabber" >
+
+    <uses-sdk android:minSdkVersion="16" android:targetSdkVersion="23" />
+
+    <application
+        android:label="@string/app_name" >
+
+        <activity
+            android:name="org.chromium.tools.audio_focus_grabber.AudioFocusGrabberActivity"
+            android:label="@string/app_name" >
+            <intent-filter>
+                <action android:name="android.intent.action.MAIN" />
+                <category android:name="android.intent.category.LAUNCHER" />
+            </intent-filter>
+        </activity>
+
+        <service
+            android:name="org.chromium.tools.audio_focus_grabber.AudioFocusGrabberListenerService"
+            android:exported="true" >
+            <intent-filter>
+                <action android:name="AUDIO_FOCUS_GRABBER_GAIN" />
+                <action android:name="AUDIO_FOCUS_GRABBER_TRANSIENT_PAUSE" />
+                <action android:name="AUDIO_FOCUS_GRABBER_TRANSIENT_DUCK" />
+            </intent-filter>
+        </service>
+
+    </application>
+
+</manifest>
diff --git a/audio_focus_grabber/java/res/drawable-hdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-hdpi/notification_icon.png
new file mode 100644
index 0000000..ca6ce5f
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-hdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-mdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-mdpi/notification_icon.png
new file mode 100644
index 0000000..82d17aa
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-mdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-xhdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-xhdpi/notification_icon.png
new file mode 100644
index 0000000..833485e
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-xhdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-xxhdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-xxhdpi/notification_icon.png
new file mode 100644
index 0000000..9eaf533
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-xxhdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-xxxhdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-xxxhdpi/notification_icon.png
new file mode 100644
index 0000000..5c0f943
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-xxxhdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/layout/audio_focus_grabber_activity.xml b/audio_focus_grabber/java/res/layout/audio_focus_grabber_activity.xml
new file mode 100644
index 0000000..101b2e6
--- /dev/null
+++ b/audio_focus_grabber/java/res/layout/audio_focus_grabber_activity.xml
@@ -0,0 +1,80 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2015 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+    android:orientation="vertical"
+    android:layout_width="match_parent"
+    android:layout_height="match_parent"
+    android:layout_margin="20dp" >
+
+    <TextView
+        android:id="@+id/message"
+        android:layout_gravity="center_horizontal|top"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_weight="1"
+        android:text="" />
+
+    <LinearLayout
+        android:orientation="horizontal"
+        android:layout_gravity="center_horizontal|bottom"
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:layout_weight="0"
+        style="?android:attr/buttonBarStyle" >
+
+        <Button
+            android:id="@+id/button_gain"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_gain_name"
+            android:onClick="onButtonClicked"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/button_transient_pause"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_pause_name"
+            android:onClick="onButtonClicked"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/button_transient_duck"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_duck_name"
+            android:onClick="onButtonClicked"
+            style="?android:attr/buttonBarButtonStyle" />
+
+    </LinearLayout>
+
+    <LinearLayout
+        android:orientation="horizontal"
+        android:layout_gravity="center_horizontal|bottom"
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:layout_weight="0" >
+
+        <TextView
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/text_notification_prompt" />
+        <Button
+            android:id="@+id/button_show_notification"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_show_notification_name"
+            android:onClick="onButtonClicked" />
+        <Button
+            android:id="@+id/button_hide_notification"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_hide_notification_name"
+            android:onClick="onButtonClicked" />
+
+    </LinearLayout>
+
+</LinearLayout>
diff --git a/audio_focus_grabber/java/res/layout/audio_focus_grabber_notification_bar.xml b/audio_focus_grabber/java/res/layout/audio_focus_grabber_notification_bar.xml
new file mode 100644
index 0000000..ec7ffd8
--- /dev/null
+++ b/audio_focus_grabber/java/res/layout/audio_focus_grabber_notification_bar.xml
@@ -0,0 +1,54 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2015 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+    android:layout_width="match_parent"
+    android:layout_height="wrap_content"
+    android:gravity="center_vertical"
+    android:orientation="vertical"
+    style="?android:attr/buttonBarStyle">
+
+    <TextView
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:text="@string/app_name" />
+
+    <LinearLayout
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:gravity="center_vertical"
+        android:orientation="horizontal"
+        style="?android:attr/buttonBarStyle">
+
+        <Button
+            android:id="@+id/notification_button_gain"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_gain_name"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/notification_button_transient_pause"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_pause_name"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/notification_button_transient_duck"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_duck_name"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/notification_button_hide"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_hide_notification_name"
+            style="?android:attr/buttonBarButtonStyle" />
+
+    </LinearLayout>
+
+</LinearLayout>
diff --git a/audio_focus_grabber/java/res/raw/ping.mp3 b/audio_focus_grabber/java/res/raw/ping.mp3
new file mode 100644
index 0000000..75b83ba
Binary files /dev/null and b/audio_focus_grabber/java/res/raw/ping.mp3 differ
diff --git a/audio_focus_grabber/java/res/values/strings.xml b/audio_focus_grabber/java/res/values/strings.xml
new file mode 100644
index 0000000..222879c
--- /dev/null
+++ b/audio_focus_grabber/java/res/values/strings.xml
@@ -0,0 +1,16 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2016 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<resources>
+    <string name="app_name">AudioFocusGrabber</string>
+    <string name="button_gain_name">Gain</string>
+    <string name="button_transient_pause_name">Pause</string>
+    <string name="button_transient_duck_name">Duck</string>
+    <string name="button_hide_notification_name">Hide</string>
+    <string name="button_show_notification_name">Show</string>
+    <string name="text_notification_prompt">Notification Controller:</string>
+</resources>
diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
new file mode 100644
index 0000000..7030b8f
--- /dev/null
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
@@ -0,0 +1,45 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.audio_focus_grabber;
+
+import android.app.Activity;
+import android.content.Intent;
+import android.os.Bundle;
+import android.view.View;
+
+/**
+ * The main activity of AudioFocusGrabber. It starts the background service,
+ * and responds to UI button controls.
+ */
+public class AudioFocusGrabberActivity extends Activity {
+    @Override
+    protected void onCreate(Bundle savedInstanceState) {
+        super.onCreate(savedInstanceState);
+        setContentView(R.layout.audio_focus_grabber_activity);
+    }
+
+    public void onButtonClicked(View view) {
+        Intent intent = new Intent(this, AudioFocusGrabberListenerService.class);
+        switch (view.getId()) {
+            case R.id.button_gain:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_GAIN);
+                break;
+            case R.id.button_transient_pause:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_TRANSIENT_PAUSE);
+                break;
+            case R.id.button_transient_duck:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_TRANSIENT_DUCK);
+                break;
+            case R.id.button_show_notification:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_SHOW_NOTIFICATION);
+                break;
+            case R.id.button_hide_notification:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_HIDE_NOTIFICATION);
+                break;
+        }
+        startService(intent);
+    }
+
+}
diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
new file mode 100644
index 0000000..9981fd8
--- /dev/null
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
@@ -0,0 +1,180 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.audio_focus_grabber;
+
+import android.app.PendingIntent;
+import android.app.Service;
+import android.content.Context;
+import android.content.Intent;
+import android.media.AudioManager;
+import android.media.MediaPlayer;
+import android.os.IBinder;
+import android.support.v4.app.NotificationCompat;
+import android.support.v4.app.NotificationManagerCompat;
+import android.widget.RemoteViews;
+
+import org.chromium.base.Log;
+
+/**
+ * The listener service, which listens to intents and perform audio focus actions.
+ */
+public class AudioFocusGrabberListenerService extends Service {
+    private static final String TAG = "AudioFocusGrabber";
+
+    public static final String ACTION_GAIN = "AUDIO_FOCUS_GRABBER_GAIN";
+    public static final String ACTION_TRANSIENT_PAUSE = "AUDIO_FOCUS_GRABBER_TRANSIENT_PAUSE";
+    public static final String ACTION_TRANSIENT_DUCK = "AUDIO_FOCUS_GRABBER_TRANSIENT_DUCK";
+    public static final String ACTION_SHOW_NOTIFICATION = "AUDIO_FOCUS_GRABBER_SHOW_NOTIFICATION";
+    public static final String ACTION_HIDE_NOTIFICATION = "AUDIO_FOCUS_GRABBER_HIDE_NOTIFICATION";
+
+    private static final int NOTIFICATION_ID = 1;
+
+    AudioManager mAudioManager = null;
+    MediaPlayer mMediaPlayer = null;
+    boolean mIsDucking = false;
+
+    @Override
+    public void onCreate() {
+        super.onCreate();
+        mAudioManager = (AudioManager) getApplicationContext()
+                .getSystemService(Context.AUDIO_SERVICE);
+    }
+
+    @Override
+    public void onDestroy() {
+        hideNotification();
+    }
+
+    @Override
+    public IBinder onBind(Intent intent) {
+        return null;
+    }
+
+    @Override
+    public int onStartCommand(Intent intent, int flags, int startId) {
+        if (intent != null) {
+            Log.i(TAG, "received intent: " + intent.getAction());
+        } else {
+            Log.i(TAG, "received null intent");
+            return START_NOT_STICKY;
+        }
+        processIntent(intent);
+        return START_NOT_STICKY;
+    }
+
+    void processIntent(Intent intent) {
+        if (mMediaPlayer != null) {
+            Log.i(TAG, "There's already a MediaPlayer playing,"
+                    + " stopping the existing player and abandon focus");
+            releaseAndAbandonAudioFocus();
+        }
+        String action = intent.getAction();
+        if (ACTION_SHOW_NOTIFICATION.equals(action)) {
+            showNotification();
+        } else if (ACTION_HIDE_NOTIFICATION.equals(action)) {
+            hideNotification();
+        } else if (ACTION_GAIN.equals(action)) {
+            gainFocusAndPlay(AudioManager.AUDIOFOCUS_GAIN);
+        } else if (ACTION_TRANSIENT_PAUSE.equals(action)) {
+            gainFocusAndPlay(AudioManager.AUDIOFOCUS_GAIN_TRANSIENT);
+        } else if (ACTION_TRANSIENT_DUCK.equals(action)) {
+            gainFocusAndPlay(AudioManager.AUDIOFOCUS_GAIN_TRANSIENT_MAY_DUCK);
+        } else {
+            assert false;
+        }
+    }
+
+
+    void gainFocusAndPlay(int focusType) {
+        int result = mAudioManager.requestAudioFocus(
+                mOnAudioFocusChangeListener,
+                AudioManager.STREAM_MUSIC,
+                focusType);
+        if (result == AudioManager.AUDIOFOCUS_REQUEST_GRANTED) {
+            playSound();
+        } else {
+            Log.i(TAG, "cannot request audio focus");
+        }
+    }
+
+    void playSound() {
+        mMediaPlayer = MediaPlayer.create(getApplicationContext(), R.raw.ping);
+        mMediaPlayer.setOnCompletionListener(mOnCompletionListener);
+        mMediaPlayer.start();
+    }
+
+    void releaseAndAbandonAudioFocus() {
+        mMediaPlayer.release();
+        mMediaPlayer = null;
+        mAudioManager.abandonAudioFocus(mOnAudioFocusChangeListener);
+    }
+
+    MediaPlayer.OnCompletionListener mOnCompletionListener =
+            new MediaPlayer.OnCompletionListener() {
+                @Override
+                public void onCompletion(MediaPlayer mp) {
+                    releaseAndAbandonAudioFocus();
+                }
+            };
+
+    AudioManager.OnAudioFocusChangeListener mOnAudioFocusChangeListener =
+            new AudioManager.OnAudioFocusChangeListener() {
+                @Override
+                public void onAudioFocusChange(int focusChange) {
+                    switch (focusChange) {
+                        case AudioManager.AUDIOFOCUS_GAIN:
+                            if (mIsDucking) {
+                                mMediaPlayer.setVolume(1.0f, 1.0f);
+                                mIsDucking = false;
+                            } else {
+                                mMediaPlayer.start();
+                            }
+                            break;
+                        case AudioManager.AUDIOFOCUS_LOSS:
+                            mMediaPlayer.stop();
+                            mMediaPlayer.release();
+                            mMediaPlayer = null;
+                            break;
+                        case AudioManager.AUDIOFOCUS_LOSS_TRANSIENT:
+                            mMediaPlayer.pause();
+                            break;
+                        case AudioManager.AUDIOFOCUS_LOSS_TRANSIENT_CAN_DUCK:
+                            mMediaPlayer.setVolume(0.1f, 0.1f);
+                            mIsDucking = true;
+                            break;
+                    }
+                }
+            };
+
+    private void showNotification() {
+        RemoteViews view = new RemoteViews(this.getPackageName(),
+                                           R.layout.audio_focus_grabber_notification_bar);
+        view.setOnClickPendingIntent(R.id.notification_button_gain,
+                createPendingIntent(ACTION_GAIN));
+        view.setOnClickPendingIntent(R.id.notification_button_transient_pause,
+                createPendingIntent(ACTION_TRANSIENT_PAUSE));
+        view.setOnClickPendingIntent(R.id.notification_button_transient_duck,
+                createPendingIntent(ACTION_TRANSIENT_DUCK));
+        view.setOnClickPendingIntent(R.id.notification_button_hide,
+                createPendingIntent(ACTION_HIDE_NOTIFICATION));
+
+        NotificationManagerCompat manager = NotificationManagerCompat.from(this);
+        NotificationCompat.Builder builder = new NotificationCompat.Builder(this)
+                .setContent(view)
+                .setSmallIcon(R.drawable.notification_icon);
+        manager.notify(NOTIFICATION_ID, builder.build());
+    }
+
+    private void hideNotification() {
+        NotificationManagerCompat manager = NotificationManagerCompat.from(this);
+        manager.cancel(NOTIFICATION_ID);
+    }
+
+    private PendingIntent createPendingIntent(String action) {
+        Intent i = new Intent(this, AudioFocusGrabberListenerService.class);
+        i.setAction(action);
+        return PendingIntent.getService(this, 0, i, PendingIntent.FLAG_CANCEL_CURRENT);
+    }
+}

commit 07560f59ae10d4dc40c0fefeb96df4569c569cb8
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 5 05:46:52 2016 -0800

    Tools to log all requests made to load a web page on Android.
    
    The requests are output to a JSON file, and are logged by connecting to
    DevTools. Also another tool to output a graphviz representation of the
    resources dependency graph.
    
    TBR=yfriedman
    
    Review URL: https://codereview.chromium.org/1400463003
    
    Cr-Original-Commit-Position: refs/heads/master@{#367516}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 20aaba1ed5c0b33f84ef5057e39874158379552a

diff --git a/loading/OWNERS b/loading/OWNERS
new file mode 100644
index 0000000..3301555
--- /dev/null
+++ b/loading/OWNERS
@@ -0,0 +1,2 @@
+lizeb@chromium.org
+pasko@chromium.org
diff --git a/loading/log_parser.py b/loading/log_parser.py
new file mode 100644
index 0000000..bb2f953
--- /dev/null
+++ b/loading/log_parser.py
@@ -0,0 +1,214 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Parses a JSON request log created by log_requests.py."""
+
+import collections
+import json
+import operator
+import urlparse
+
+
+Timing = collections.namedtuple(
+    'Timing',
+    ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
+     'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
+     'sslEnd', 'sslStart', 'workerReady', 'workerStart'])
+
+
+class Resource(object):
+  """Describes a resource."""
+
+  def __init__(self, url, content_type):
+    """Creates an instance of Resource.
+
+    Args:
+      url: URL of the resource
+      content_type: Content-Type of the resources.
+    """
+    self.url = url
+    self.content_type = content_type
+
+  def GetShortName(self):
+    """Returns either the hostname of the resource, or the filename,
+    or the end of the path.
+    """
+    parsed = urlparse.urlparse(self.url)
+    path = parsed.path
+    if path != '' and path != '/':
+      last_path = parsed.path.split('/')[-1]
+      if len(last_path) < 10:
+        if len(path) < 10:
+          return path
+        else:
+          return parsed.path[-10:]
+      else:
+        return last_path
+    else:
+      return parsed.hostname
+
+  def GetContentType(self):
+    mime = self.content_type
+    if mime == 'text/html':
+      return 'html'
+    elif mime == 'text/css':
+      return 'css'
+    elif mime in ('application/x-javascript', 'text/javascript',
+                  'application/javascript'):
+      return 'script'
+    elif mime == 'application/json':
+      return 'json'
+    elif mime == 'image/gif':
+      return 'gif_image'
+    elif mime.startswith('image/'):
+      return 'image'
+    else:
+      return 'other'
+
+  @classmethod
+  def FromRequest(cls, request):
+    """Creates a Resource from an instance of RequestData."""
+    return Resource(request.url, request.GetContentType())
+
+  def __Fields(self):
+    return (self.url, self.content_type)
+
+  def __eq__(self, o):
+    return  self.__Fields() == o.__Fields()
+
+  def __hash__(self):
+    return hash(self.__Fields())
+
+
+class RequestData(object):
+  """Represents a request, as dumped by log_requests.py."""
+
+  def __init__(self, status, headers, request_headers, timestamp, timing, url,
+               served_from_cache, initiator):
+    self.status = status
+    self.headers = headers
+    self.request_headers = request_headers
+    self.timestamp = timestamp
+    self.timing = Timing(**timing) if timing else None
+    self.url = url
+    self.served_from_cache = served_from_cache
+    self.initiator = initiator
+
+  def IsDataUrl(self):
+    return self.url.startswith('data:')
+
+  def GetContentType(self):
+    content_type = self.headers['Content-Type']
+    if ';' in content_type:
+      return content_type[:content_type.index(';')]
+    else:
+      return content_type
+
+  @classmethod
+  def FromDict(cls, r):
+    """Creates a RequestData object from a dict."""
+    return RequestData(r['status'], r['headers'], r['request_headers'],
+                       r['timestamp'], r['timing'], r['url'],
+                       r['served_from_cache'], r['initiator'])
+
+
+def ParseJsonFile(filename):
+  """Converts a JSON file to a sequence of RequestData."""
+  with open(filename) as f:
+    json_data = json.load(f)
+    return [RequestData.FromDict(r) for r in json_data]
+
+
+def FilterRequests(requests):
+  """Filters a list of requests.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    A list of requests that are not data URL, have a Content-Type, and are
+    not served from the cache.
+  """
+  return [r for r in requests if not r.IsDataUrl()
+          and 'Content-Type' in r.headers and not r.served_from_cache]
+
+
+def ResourceToRequestMap(requests):
+  """Returns a Resource -> Request map.
+
+  A resource can be requested several times in a single page load. Keeps the
+  first request in this case.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    [Resource, ...]
+  """
+  # reversed(requests) because we want the first one to win.
+  return dict([(Resource.FromRequest(r), r) for r in reversed(requests)])
+
+
+def GetResources(requests):
+  """Returns an ordered list of resources from a list of requests.
+
+  The same resource can be requested several time for a single page load. This
+  keeps only the first request.
+
+  Args:
+    requests: [RequestData]
+
+  Returns:
+    [Resource]
+  """
+  resources = []
+  known_resources = set()
+  for r in requests:
+    resource = Resource.FromRequest(r)
+    if r in known_resources:
+      continue
+    known_resources.add(resource)
+    resources.append(resource)
+  return resources
+
+
+def ParseCacheControl(headers):
+  """Parses the "Cache-Control" header and returns a dict representing it.
+
+  Args:
+    headers: (dict) Response headers.
+
+  Returns:
+    {Directive: Value, ...}
+  """
+  # TODO(lizeb): Handle the "Expires" header as well.
+  result = {}
+  cache_control = headers.get('Cache-Control', None)
+  if cache_control is None:
+    return result
+  directives = [s.strip() for s in cache_control.split(',')]
+  for directive in directives:
+    parts = [s.strip() for s in directive.split('=')]
+    if len(parts) == 1:
+      result[parts[0]] = True
+    else:
+      result[parts[0]] = parts[1]
+  return result
+
+
+def MaxAge(request):
+  """Returns the max-age of a resource, or -1."""
+  cache_control = ParseCacheControl(request.headers)
+  if (u'no-store' in cache_control
+      or u'no-cache' in cache_control
+      or len(cache_control) == 0):
+    return -1
+  if 'max-age' in cache_control:
+    return int(cache_control['max-age'])
+  return -1
+
+
+def SortedByCompletion(requests):
+  """Returns the requests, sorted by completion time."""
+  return sorted(requests, key=operator.attrgetter('timestamp'))
diff --git a/loading/log_requests.py b/loading/log_requests.py
new file mode 100755
index 0000000..16a10d5
--- /dev/null
+++ b/loading/log_requests.py
@@ -0,0 +1,216 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loads a URL on an Android device, logging all the requests made to do it
+to a JSON file using DevTools.
+"""
+
+import contextlib
+import httplib
+import json
+import logging
+import optparse
+import os
+import sys
+import time
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
+sys.path.append(os.path.join(file_dir, '..', '..', 'chrome_proxy'))
+
+from pylib import constants
+from pylib import flag_changer
+from pylib.device import device_utils
+from pylib.device import intent
+from common import inspector_network
+from telemetry.internal.backends.chrome_inspector import inspector_websocket
+from telemetry.internal.backends.chrome_inspector import websocket
+
+
+_PORT = 9222 # DevTools port number.
+
+
+@contextlib.contextmanager
+def FlagChanger(device, command_line_path, new_flags):
+  """Changes the flags in a context, restores them afterwards.
+
+  Args:
+    device: Device to target, from DeviceUtils.
+    command_line_path: Full path to the command-line file.
+    new_flags: Flags to add.
+  """
+  changer = flag_changer.FlagChanger(device, command_line_path)
+  changer.AddFlags(new_flags)
+  try:
+    yield
+  finally:
+    changer.Restore()
+
+
+@contextlib.contextmanager
+def ForwardPort(device, local, remote):
+  """Forwards a local port to a remote one on a device in a context."""
+  device.adb.Forward(local, remote)
+  try:
+    yield
+  finally:
+    device.adb.ForwardRemove(local)
+
+
+def _SetUpDevice(device, package_info):
+  """Enables root and closes Chrome on a device."""
+  device.EnableRoot()
+  device.KillAll(package_info.package, quiet=True)
+
+
+class AndroidRequestsLogger(object):
+  """Logs all the requests made to load a page on a device."""
+
+  def __init__(self, device):
+    self.device = device
+    self._please_stop = False
+    self._main_frame_id = None
+
+  def _PageDataReceived(self, msg):
+    """Called when a Page event is received.
+
+    Records the main frame, and stops the recording once it has finished
+    loading.
+
+    Args:
+      msg: (dict) Message sent by DevTools.
+    """
+    if 'params' not in msg:
+      return
+    params = msg['params']
+    method = msg.get('method', None)
+    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
+      self._main_frame_id = params['frameId']
+    elif (method == 'Page.frameStoppedLoading'
+          and params['frameId'] == self._main_frame_id):
+      self._please_stop = True
+
+  def _LogPageLoadInternal(self, url, clear_cache):
+    """Returns the collection of requests made to load a given URL.
+
+    Assumes that DevTools is available on http://localhost:_PORT.
+
+    Args:
+      url: URL to load.
+      clear_cache: Whether to clear the HTTP cache.
+
+    Returns:
+      [inspector_network.InspectorNetworkResponseData, ...]
+    """
+    self._main_frame_id = None
+    self._please_stop = False
+    r = httplib.HTTPConnection('localhost', _PORT)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      logging.error('Cannot connect to the remote target.')
+      return None
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    inspector = inspector_network.InspectorNetwork(ws)
+    if clear_cache:
+      inspector.ClearCache()
+    ws.SyncRequest({'method': 'Page.enable'})
+    ws.RegisterDomain('Page', self._PageDataReceived)
+    inspector.StartMonitoringNetwork()
+    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
+                              'params': {'url': url}})
+    while not self._please_stop:
+      try:
+        ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException as e:
+        logging.warning('Exception: ' + str(e))
+        break
+    inspector.StopMonitoringNetwork()
+    return inspector.GetResponseData()
+
+  def LogPageLoad(self, url, clear_cache):
+    """Returns the collection of requests made to load a given URL on a device.
+
+    Args:
+      url: (str) URL to load on the device.
+      clear_cache: (bool) Whether to clear the HTTP cache.
+
+    Returns:
+      See _LogPageLoadInternal().
+    """
+    package_info = constants.PACKAGE_INFO['chrome']
+    command_line_path = '/data/local/chrome-command-line'
+    new_flags = ['--enable-test-events', '--remote-debugging-port=%d' % _PORT]
+    _SetUpDevice(self.device, package_info)
+    with FlagChanger(self.device, command_line_path, new_flags):
+      start_intent = intent.Intent(
+          package=package_info.package, activity=package_info.activity,
+          data='about:blank')
+      self.device.StartActivity(start_intent, blocking=True)
+      time.sleep(2)
+      with ForwardPort(self.device, 'tcp:%d' % _PORT,
+                       'localabstract:chrome_devtools_remote'):
+        return self._LogPageLoadInternal(url, clear_cache)
+
+
+def _ResponseDataToJson(data):
+  """Converts a list of inspector_network.InspectorNetworkResponseData to JSON.
+
+  Args:
+    data: as returned by _LogPageLoad()
+
+  Returns:
+    A JSON file with the following format:
+    [request1, request2, ...], and a request is:
+    {'status': str, 'headers': dict, 'request_headers': dict,
+     'timestamp': double, 'timing': dict, 'url': str,
+      'served_from_cache': bool, 'initiator': str})
+  """
+  result = []
+  for r in data:
+    result.append({'status': r.status,
+                   'headers': r.headers,
+                   'request_headers': r.request_headers,
+                   'timestamp': r.timestamp,
+                   'timing': r.timing,
+                   'url': r.url,
+                   'served_from_cache': r.served_from_cache,
+                   'initiator': r.initiator})
+  return json.dumps(result)
+
+
+def _CreateOptionParser():
+  """Returns the option parser for this tool."""
+  parser = optparse.OptionParser(description='Starts a browser on an Android '
+                                 'device, gathers the requests made to load a '
+                                 'page and dumps it to a JSON file.')
+  parser.add_option('--url', help='URL to load.',
+                    default='https://www.google.com', metavar='URL')
+  parser.add_option('--output', help='Output file.', default='result.json')
+  parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
+                                              'before loading the URL.'),
+                    default=True, action='store_false', dest='clear_cache')
+  return parser
+
+
+def main():
+  parser = _CreateOptionParser()
+  options, _ = parser.parse_args()
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  device = devices[0]
+  request_logger = AndroidRequestsLogger(device)
+  response_data = request_logger.LogPageLoad(options.url, options.clear_cache)
+  json_data = _ResponseDataToJson(response_data)
+  with open(options.output, 'w') as f:
+    f.write(json_data)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/process_request_log.py b/loading/process_request_log.py
new file mode 100755
index 0000000..bbec0a8
--- /dev/null
+++ b/loading/process_request_log.py
@@ -0,0 +1,189 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Creates a Graphviz file visualizing the resource dependencies from a JSON
+file dumped by log_requests.py.
+"""
+
+import collections
+import sys
+import urlparse
+
+import log_parser
+from log_parser import Resource
+
+
+def _BuildResourceDependencyGraph(requests):
+  """Builds the graph of resource dependencies.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    A tuple ([Resource], [(resource1, resource2, reason), ...])
+  """
+  resources = log_parser.GetResources(requests)
+  resources_from_url = {resource.url: resource for resource in resources}
+  requests_by_completion = log_parser.SortedByCompletion(requests)
+  deps = []
+  for r in requests:
+    resource = Resource.FromRequest(r)
+    initiator = r.initiator
+    initiator_type = initiator['type']
+    dep = None
+    if initiator_type == 'parser':
+      url = initiator['url']
+      blocking_resource = resources_from_url.get(url, None)
+      if blocking_resource is None:
+        continue
+      dep = (blocking_resource, resource, 'parser')
+    elif initiator_type == 'script' and 'stackTrace' in initiator:
+      for frame in initiator['stackTrace']:
+        url = frame['url']
+        blocking_resource = resources_from_url.get(url, None)
+        if blocking_resource is None:
+          continue
+        dep = (blocking_resource, resource, 'stack')
+        break
+    else:
+      # When the initiator is a script without a stackTrace, infer that it comes
+      # from the most recent script from the same hostname.
+      # TLD+1 might be better, but finding what is a TLD requires a database.
+      request_hostname = urlparse.urlparse(r.url).hostname
+      sorted_script_requests_from_hostname = [
+          r for r in requests_by_completion
+          if (resource.GetContentType() in ('script', 'html', 'json')
+              and urlparse.urlparse(r.url).hostname == request_hostname)]
+      most_recent = None
+      # Linear search is bad, but this shouldn't matter here.
+      for request in sorted_script_requests_from_hostname:
+        if request.timestamp < r.timing.requestTime:
+          most_recent = request
+        else:
+          break
+      if most_recent is not None:
+        blocking = resources_from_url.get(most_recent.url, None)
+        if blocking is not None:
+          dep = (blocking, resource, 'script_inferred')
+    if dep is not None:
+      deps.append(dep)
+  return (resources, deps)
+
+
+def PrefetchableResources(requests):
+  """Returns a list of resources that are discoverable without JS.
+
+  Args:
+    requests: List of requests.
+
+  Returns:
+    List of discoverable resources, with their initial request.
+  """
+  resource_to_request = log_parser.ResourceToRequestMap(requests)
+  (_, all_deps) = _BuildResourceDependencyGraph(requests)
+  # Only keep "parser" arcs
+  deps = [(first, second) for (first, second, reason) in all_deps
+          if reason == 'parser']
+  deps_per_resource = collections.defaultdict(list)
+  for (first, second) in deps:
+    deps_per_resource[first].append(second)
+  result = []
+  visited = set()
+  to_visit = [deps[0][0]]
+  while len(to_visit) != 0:
+    r = to_visit.pop()
+    visited.add(r)
+    to_visit += deps_per_resource[r]
+    result.append(resource_to_request[r])
+  return result
+
+
+_CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
+                          'json': 'purple', 'gif_image': 'grey',
+                          'image': 'orange', 'other': 'white'}
+
+
+def _ResourceGraphvizNode(resource, request, resource_to_index):
+  """Returns the node description for a given resource.
+
+  Args:
+    resource: Resource.
+    request: RequestData associated with the resource.
+    resource_to_index: {Resource: int}.
+
+  Returns:
+    A string describing the resource in graphviz format.
+    The resource is color-coded according to its content type, and its shape is
+    oval if its max-age is less than 300s (or if it's not cacheable).
+  """
+  color = _CONTENT_TYPE_TO_COLOR[resource.GetContentType()]
+  max_age = log_parser.MaxAge(request)
+  shape = 'polygon' if max_age > 300 else 'oval'
+  return ('%d [label = "%s"; style = "filled"; fillcolor = %s; shape = %s];\n'
+          % (resource_to_index[resource], resource.GetShortName(), color,
+             shape))
+
+
+def _GraphvizFileFromDeps(resources, requests, deps, output_filename):
+  """Writes a graphviz file from a set of resource dependencies.
+
+  Args:
+    resources: [Resource, ...]
+    requests: list of requests
+    deps: [(resource1, resource2, reason), ...]
+    output_filename: file to write the graph to.
+  """
+  with open(output_filename, 'w') as f:
+    f.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+    resource_to_request = log_parser.ResourceToRequestMap(requests)
+    resource_to_index = {r: i for (i, r) in enumerate(resources)}
+    resources_with_edges = set()
+    for (first, second, reason) in deps:
+      resources_with_edges.add(first)
+      resources_with_edges.add(second)
+    if len(resources_with_edges) != len(resources):
+      f.write("""subgraph cluster_orphans {
+  color=black;
+  label="Orphans";
+""")
+      for resource in resources:
+        if resource not in resources_with_edges:
+          request = resource_to_request[resource]
+          f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
+      f.write('}\n')
+
+    f.write("""subgraph cluster_nodes {
+  color=invis;
+""")
+    for resource in resources:
+      request = resource_to_request[resource]
+      print resource.url
+      if resource in resources_with_edges:
+        f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
+    for (first, second, reason) in deps:
+      arrow = ''
+      if reason == 'parser':
+        arrow = '[color = red]'
+      elif reason == 'stack':
+        arrow = '[color = blue]'
+      elif reason == 'script_inferred':
+        arrow = '[color = blue; style=dotted]'
+      f.write('%d -> %d %s;\n' % (
+          resource_to_index[first], resource_to_index[second], arrow))
+    f.write('}\n}\n')
+
+
+def main():
+  filename = sys.argv[1]
+  requests = log_parser.ParseJsonFile(filename)
+  requests = log_parser.FilterRequests(requests)
+  (resources, deps) = _BuildResourceDependencyGraph(requests)
+  _GraphvizFileFromDeps(resources, requests, deps, filename + '.dot')
+
+
+if __name__ == '__main__':
+  main()

commit 1f10edc48758361ad944c8aab3d845e4f67ea300
Author: thakis <thakis@chromium.org>
Date:   Mon Jan 4 12:07:23 2016 -0800

    Enable -Wformat in linux, android, chromeos, cast builds.
    
    No behavior change.
    
    BUG=573780
    
    Review URL: https://codereview.chromium.org/1551313002
    
    Cr-Original-Commit-Position: refs/heads/master@{#367353}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 932e1e5939963b5b67019f428e8f209246069a44

diff --git a/file_poller/file_poller.cc b/file_poller/file_poller.cc
index f7b6d58..448e229 100644
--- a/file_poller/file_poller.cc
+++ b/file_poller/file_poller.cc
@@ -88,7 +88,7 @@ void acquire_sample(int fd, const Context& context) {
   struct timeval tv;
   gettimeofday(&tv, NULL);
   char buffer[1024];
-  int n = snprintf(buffer, sizeof(buffer), "%d.%06d ", tv.tv_sec, tv.tv_usec);
+  int n = snprintf(buffer, sizeof(buffer), "%ld.%06ld ", tv.tv_sec, tv.tv_usec);
   safe_write(fd, buffer, n);
 
   for (int i = 0; i < context.nb_files; ++i)
diff --git a/memconsumer/memconsumer_hook.cc b/memconsumer/memconsumer_hook.cc
index 9ae0bc1..78a98d9 100644
--- a/memconsumer/memconsumer_hook.cc
+++ b/memconsumer/memconsumer_hook.cc
@@ -42,7 +42,7 @@ JNIEXPORT void JNICALL
   if (!g_memory) {
     __android_log_print(ANDROID_LOG_WARN,
                         "MemConsumer",
-                        "Unable to allocate %ld bytes",
+                        "Unable to allocate %lld bytes",
                         memory);
   }
   // If memory allocation failed, try to allocate as much as possible.
