commit eab227add8d7aad2d72c67862fdcabdf9636f38b
Author: lizeb <lizeb@chromium.org>
Date:   Fri May 13 09:57:22 2016 -0700

    clovis: Silence noisy NetworkActivityLens.
    
    Review-Url: https://codereview.chromium.org/1972413002
    Cr-Original-Commit-Position: refs/heads/master@{#393556}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f4b254570593e5e1adeeb95facd1b83285710707

diff --git a/loading/network_activity_lens.py b/loading/network_activity_lens.py
index b30cac8..7e087e4 100644
--- a/loading/network_activity_lens.py
+++ b/loading/network_activity_lens.py
@@ -237,6 +237,4 @@ class NetworkEvent(object):
     """Returns the download rate of this event in Bytes / s."""
     downloaded_bytes = self.DownloadedBytes()
     value = 1000 * downloaded_bytes / float(self.end_msec - self.start_msec)
-    if value > 1e6:
-      print self._kind, downloaded_bytes, self.end_msec - self.start_msec
     return value

commit f465864c69b754f9cfd1b738bab5c0eeb09c85d2
Author: droger <droger@chromium.org>
Date:   Fri May 13 08:53:29 2016 -0700

    tools/android/loading Move LoadingTraceDatabase under cloud/
    
    The file is moved so that it can be used from the AppEngine frontend.
    
    Review-Url: https://codereview.chromium.org/1969373002
    Cr-Original-Commit-Position: refs/heads/master@{#393532}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9312f6d6c8c18b177ca14fb82246ca8e6175b492

diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
index 47365b5..1836bbf 100644
--- a/loading/cloud/backend/report_task_handler.py
+++ b/loading/cloud/backend/report_task_handler.py
@@ -7,10 +7,10 @@ import uuid
 
 from googleapiclient import errors
 
+from common.loading_trace_database import LoadingTraceDatabase
 import common.google_error_helper as google_error_helper
 from failure_database import FailureDatabase
 from loading_trace import LoadingTrace
-from loading_trace_database import LoadingTraceDatabase
 from report import LoadingReport
 
 
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index 33591d3..a549c29 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -8,10 +8,10 @@ import re
 import sys
 
 from common.clovis_task import ClovisTask
+from common.loading_trace_database import LoadingTraceDatabase
 import controller
 from failure_database import FailureDatabase
 import loading_trace
-from loading_trace_database import LoadingTraceDatabase
 import options
 
 
diff --git a/loading/cloud/common/loading_trace_database.py b/loading/cloud/common/loading_trace_database.py
new file mode 100644
index 0000000..f6e946a
--- /dev/null
+++ b/loading/cloud/common/loading_trace_database.py
@@ -0,0 +1,57 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Represents a database of on-disk traces."""
+
+import json
+
+
+class LoadingTraceDatabase(object):
+  def __init__(self, traces_dict):
+    """traces_dict is a dictionary mapping filenames of traces to metadata
+       about those traces."""
+    self._traces_dict = traces_dict
+
+  def SetTrace(self, filename, trace_dict):
+    """Sets a mapping from |filename| to |trace_dict| into the database.
+    If there is an existing mapping for filename, it is replaced.
+    """
+    self._traces_dict[filename] = trace_dict
+
+  def GetTraceFilesForURL(self, url):
+    """Given a URL, returns the set of filenames of traces that were generated
+       for this URL."""
+    trace_files = [f for f in self._traces_dict.keys()
+        if self._traces_dict[f]["url"] == url]
+    return trace_files
+
+  def ToJsonDict(self):
+    """Returns a dict representing this instance."""
+    return self._traces_dict
+
+  def ToJsonString(self):
+    """Returns a string representing this instance."""
+    return json.dumps(self._traces_dict, indent=2)
+
+  def ToJsonFile(self, json_path):
+    """Saves a json file representing this instance."""
+    json_dict = self.ToJsonDict()
+    with open(json_path, 'w') as output_file:
+       json.dump(json_dict, output_file, indent=2)
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns an instance from a dict returned by ToJsonDict()."""
+    return LoadingTraceDatabase(json_dict)
+
+  @classmethod
+  def FromJsonString(cls, json_string):
+    """Returns an instance from a string returned by ToJsonString()."""
+    return LoadingTraceDatabase(json.loads(json_string))
+
+  @classmethod
+  def FromJsonFile(cls, json_path):
+    """Returns an instance from a json file saved by ToJsonFile()."""
+    with open(json_path) as input_file:
+      return cls.FromJsonDict(json.load(input_file))
diff --git a/loading/cloud/common/loading_trace_database_unittest.py b/loading/cloud/common/loading_trace_database_unittest.py
new file mode 100644
index 0000000..72ffcba
--- /dev/null
+++ b/loading/cloud/common/loading_trace_database_unittest.py
@@ -0,0 +1,45 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from cloud.common.loading_trace_database import LoadingTraceDatabase
+
+
+class LoadingTraceDatabaseUnittest(unittest.TestCase):
+  _JSON_DATABASE = {
+    "traces/trace1.json" : { "url" : "http://bar.html", },
+    "traces/trace2.json" : { "url" : "http://bar.html", },
+    "traces/trace3.json" : { "url" : "http://qux.html", },
+  }
+
+  def setUp(self):
+    self.database = LoadingTraceDatabase.FromJsonDict(self._JSON_DATABASE)
+
+  def testGetTraceFilesForURL(self):
+    # Test a URL with no matching traces.
+    self.assertEqual(
+        self.database.GetTraceFilesForURL("http://foo.html"),
+        [])
+
+    # Test a URL with matching traces.
+    self.assertEqual(
+        set(self.database.GetTraceFilesForURL("http://bar.html")),
+        set(["traces/trace1.json", "traces/trace2.json"]))
+
+  def testSerialization(self):
+    self.assertEqual(
+        self._JSON_DATABASE, self.database.ToJsonDict())
+
+  def testSetTrace(self):
+    dummy_url = "http://dummy.com"
+    new_trace_file = "traces/new_trace.json"
+    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url), [])
+    self.database.SetTrace(new_trace_file, {"url" : dummy_url})
+    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url),
+                     [new_trace_file])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
deleted file mode 100644
index f6e946a..0000000
--- a/loading/loading_trace_database.py
+++ /dev/null
@@ -1,57 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Represents a database of on-disk traces."""
-
-import json
-
-
-class LoadingTraceDatabase(object):
-  def __init__(self, traces_dict):
-    """traces_dict is a dictionary mapping filenames of traces to metadata
-       about those traces."""
-    self._traces_dict = traces_dict
-
-  def SetTrace(self, filename, trace_dict):
-    """Sets a mapping from |filename| to |trace_dict| into the database.
-    If there is an existing mapping for filename, it is replaced.
-    """
-    self._traces_dict[filename] = trace_dict
-
-  def GetTraceFilesForURL(self, url):
-    """Given a URL, returns the set of filenames of traces that were generated
-       for this URL."""
-    trace_files = [f for f in self._traces_dict.keys()
-        if self._traces_dict[f]["url"] == url]
-    return trace_files
-
-  def ToJsonDict(self):
-    """Returns a dict representing this instance."""
-    return self._traces_dict
-
-  def ToJsonString(self):
-    """Returns a string representing this instance."""
-    return json.dumps(self._traces_dict, indent=2)
-
-  def ToJsonFile(self, json_path):
-    """Saves a json file representing this instance."""
-    json_dict = self.ToJsonDict()
-    with open(json_path, 'w') as output_file:
-       json.dump(json_dict, output_file, indent=2)
-
-  @classmethod
-  def FromJsonDict(cls, json_dict):
-    """Returns an instance from a dict returned by ToJsonDict()."""
-    return LoadingTraceDatabase(json_dict)
-
-  @classmethod
-  def FromJsonString(cls, json_string):
-    """Returns an instance from a string returned by ToJsonString()."""
-    return LoadingTraceDatabase(json.loads(json_string))
-
-  @classmethod
-  def FromJsonFile(cls, json_path):
-    """Returns an instance from a json file saved by ToJsonFile()."""
-    with open(json_path) as input_file:
-      return cls.FromJsonDict(json.load(input_file))
diff --git a/loading/loading_trace_database_unittest.py b/loading/loading_trace_database_unittest.py
deleted file mode 100644
index e64fb8c..0000000
--- a/loading/loading_trace_database_unittest.py
+++ /dev/null
@@ -1,45 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import unittest
-
-from loading_trace_database import LoadingTraceDatabase
-
-
-class LoadingTraceDatabaseUnittest(unittest.TestCase):
-  _JSON_DATABASE = {
-    "traces/trace1.json" : { "url" : "http://bar.html", },
-    "traces/trace2.json" : { "url" : "http://bar.html", },
-    "traces/trace3.json" : { "url" : "http://qux.html", },
-  }
-
-  def setUp(self):
-    self.database = LoadingTraceDatabase.FromJsonDict(self._JSON_DATABASE)
-
-  def testGetTraceFilesForURL(self):
-    # Test a URL with no matching traces.
-    self.assertEqual(
-        self.database.GetTraceFilesForURL("http://foo.html"),
-        [])
-
-    # Test a URL with matching traces.
-    self.assertEqual(
-        set(self.database.GetTraceFilesForURL("http://bar.html")),
-        set(["traces/trace1.json", "traces/trace2.json"]))
-
-  def testSerialization(self):
-    self.assertEqual(
-        self._JSON_DATABASE, self.database.ToJsonDict())
-
-  def testSetTrace(self):
-    dummy_url = "http://dummy.com"
-    new_trace_file = "traces/new_trace.json"
-    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url), [])
-    self.database.SetTrace(new_trace_file, {"url" : dummy_url})
-    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url),
-                     [new_trace_file])
-
-
-if __name__ == '__main__':
-  unittest.main()

commit d5d55763bff71980ad52337f86b1480d5358fbf4
Author: droger <droger@chromium.org>
Date:   Fri May 13 07:09:34 2016 -0700

    tools/android/loading Remove dead code in LoadingTraceDatabase
    
    Review-Url: https://codereview.chromium.org/1969393002
    Cr-Original-Commit-Position: refs/heads/master@{#393518}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d0cedd7896dc80c8619c4e2828cef41ee0244080

diff --git a/loading/google_storage_util.py b/loading/google_storage_util.py
deleted file mode 100644
index 82a59a5..0000000
--- a/loading/google_storage_util.py
+++ /dev/null
@@ -1,19 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Contains utility functions for interacting with Google Storage."""
-
-import subprocess
-
-def ReadFromGoogleStorage(path):
-  """Given a Google Storage path, returns the contents of the file at that path
-     as a string. Will fail if the user does not have authorization to access
-     the path or if the path does not exist. To gain authorization, follow the
-     instructions for installing gsutil and setting up credentials to access
-     protected data that are on this page:
-     https://cloud.google.com/storage/docs/gsutil_install"""
-  # TODO(blundell): Change this to use the gcloud Python module once
-  # https://github.com/GoogleCloudPlatform/gcloud-python/issues/14360 is fixed.
-  contents = subprocess.check_output(["gsutil", "cat", path])
-  return contents
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index 91705aa..f6e946a 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -5,7 +5,6 @@
 """Represents a database of on-disk traces."""
 
 import json
-from google_storage_util import ReadFromGoogleStorage
 
 
 class LoadingTraceDatabase(object):
@@ -56,10 +55,3 @@ class LoadingTraceDatabase(object):
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:
       return cls.FromJsonDict(json.load(input_file))
-
-  @classmethod
-  def FromJsonFileInGoogleStorage(cls, json_google_storage_path):
-    """Returns an instance from a json file in Google Storage whose contents
-       were generated by ToJsonFile()."""
-    json_string = ReadFromGoogleStorage(json_google_storage_path)
-    return cls.FromJsonDict(json.loads(json_string))

commit c20f81f0f10fdfc3473b8cbb8468e1031c619fb9
Author: droger <droger@chromium.org>
Date:   Fri May 13 06:24:53 2016 -0700

    tools/android/loading Lazy initialization of TraceTaskHandler
    
    Now that the backend has another task handler, it is possible that
    TraceTaskHandler is not used.
    This CL makes the initialization of TraceTaskHandler lazy, to avoid
    unnecessary calls to cloud storage.
    
    Review-Url: https://codereview.chromium.org/1966953003
    Cr-Original-Commit-Position: refs/heads/master@{#393513}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: df043a065371182d04f2d600ba258264c751c2f3

diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index a6e83a7..33591d3 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -30,23 +30,31 @@ class TraceTaskHandler(object):
     self._logger = logger
     self._google_storage_accessor = google_storage_accessor
     self._base_path = base_path
+    self._is_initialized = False
+    self._trace_database = None
     if instance_name:
       trace_database_filename = 'trace_database_%s.json' % instance_name
     else:
       trace_database_filename = 'trace_database.json'
     self._trace_database_path = os.path.join(base_path, trace_database_filename)
 
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs(['--local_build_dir', binaries_path])
+
+  def _Initialize(self):
+    """Initializes the trace task handler. Can be called multiple times."""
+    if self._is_initialized:
+      return
+    self._is_initialized = True
+
     # Recover any existing traces in case the worker died.
     self._DownloadTraceDatabase()
     if self._trace_database.ToJsonDict():
-      # Script is restarting after a crash, or there are already files from a
-      # previous run in the directory.
+      # There are already files from a previous run in the directory, likely
+      # because the script is restarting after a crash.
       self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
                                         'trace_database')
 
-    # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs(['--local_build_dir', binaries_path])
-
   def _DownloadTraceDatabase(self):
     """Downloads the trace database from CloudStorage."""
     self._logger.info('Downloading trace database')
@@ -58,6 +66,7 @@ class TraceTaskHandler(object):
   def _UploadTraceDatabase(self):
     """Uploads the trace database to CloudStorage."""
     self._logger.info('Uploading trace database')
+    assert self._is_initialized
     self._google_storage_accessor.UploadString(
         self._trace_database.ToJsonString(),
         self._trace_database_path)
@@ -137,6 +146,7 @@ class TraceTaskHandler(object):
                              the log (with a .log extension added) are uploaded.
       trace_metadata (dict): Metadata associated with the trace generation.
     """
+    assert self._is_initialized
     if trace_metadata['succeeded']:
       traces_dir = os.path.join(self._base_path, 'traces')
       remote_trace_location = os.path.join(traces_dir, remote_filename)
@@ -174,6 +184,8 @@ class TraceTaskHandler(object):
                                         'trace_task_handler_run')
       return
 
+    self._Initialize()
+
     # Extract the task parameters.
     params = clovis_task.ActionParams()
     urls = params['urls']

commit 5abd28ff3adda80e4324dbc94e98a2d46eb22641
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 13 06:23:56 2016 -0700

    sandwich: Reset the chrome state between repeated runs
    
    Before Sandwich was not reseting the chrome state between repeated
    runs that could lead to JavaScript entropy because of the previous
    cookies.
    
    This CL fixes this issue by replacing the options --clear_device_data
    with the programmatic ChromeControllerBase.ResetBrowserState().
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1976113002
    Cr-Original-Commit-Position: refs/heads/master@{#393512}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a5d46d30ea9597918275a45f03bfe5f68068f95f

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index ead016e..24d3f75 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -121,7 +121,7 @@ def PushBrowserCache(device, local_cache_path):
 
   # Clear previous cache.
   _AdbShell(device.adb, ['rm', '-rf', remote_cache_directory])
-  _AdbShell(device.adb, ['mkdir', remote_cache_directory])
+  _AdbShell(device.adb, ['mkdir', '-p', remote_cache_directory])
 
   # Push cache content.
   device.adb.Push(local_cache_path, remote_cache_directory)
diff --git a/loading/controller.py b/loading/controller.py
index 5ccf688..511685f 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -199,6 +199,10 @@ class ChromeControllerBase(object):
     assert network_name in emulation.NETWORK_CONDITIONS or network_name is None
     self._network_name = network_name
 
+  def ResetBrowserState(self):
+    """Resets the chrome's browser state."""
+    raise NotImplementedError
+
   def PushBrowserCache(self, cache_path):
     """Pushes the HTTP chrome cache to the profile directory.
 
@@ -287,8 +291,10 @@ class RemoteChromeController(ChromeControllerBase):
   def __init__(self, device):
     """Initialize the controller.
 
+    Caution: The browser state might need to be manually reseted.
+
     Args:
-      device: an andriod device.
+      device: an android device.
     """
     assert device is not None, 'Should you be using LocalController instead?'
     super(RemoteChromeController, self).__init__()
@@ -312,9 +318,6 @@ class RemoteChromeController(ChromeControllerBase):
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
     self._device.ForceStop(package_info.package)
-    if OPTIONS.clear_device_data:
-      logging.info('Clear Chrome data')
-      self._device.adb.Shell('pm clear ' + package_info.package)
     chrome_args = self._GetChromeArguments()
     logging.info('Launching %s with flags: %s' % (package_info.package,
         subprocess.list2cmdline(chrome_args)))
@@ -355,13 +358,24 @@ class RemoteChromeController(ChromeControllerBase):
       finally:
         self._device.ForceStop(package_info.package)
 
+  def ResetBrowserState(self):
+    """Override for chrome state reseting."""
+    logging.info('Reset chrome\'s profile')
+    package_info = OPTIONS.ChromePackage()
+    # We assume all the browser is in the Default user profile directory.
+    cmd = ['rm', '-rf', '/data/data/{}/app_chrome/Default'.format(
+               package_info.package)]
+    self._device.adb.Shell(subprocess.list2cmdline(cmd))
+
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
+    logging.info('Push cache from %s' % cache_path)
     chrome_cache.PushBrowserCache(self._device, cache_path)
 
   def PullBrowserCache(self):
     """Override for chrome cache pulling."""
     assert self._slow_death, 'Must do SetSlowDeath() before opening chrome.'
+    logging.info('Pull cache from device')
     return chrome_cache.PullBrowserCache(self._device)
 
   @contextlib.contextmanager
@@ -385,6 +399,10 @@ class LocalChromeController(ChromeControllerBase):
   """Controller for a local (desktop) chrome instance."""
 
   def __init__(self):
+    """Initialize the controller.
+
+    Caution: The browser state might need to be manually reseted.
+    """
     super(LocalChromeController, self).__init__()
     if OPTIONS.no_sandbox:
       self.AddChromeArgument('--no-sandbox')
@@ -485,6 +503,18 @@ class LocalChromeController(ChromeControllerBase):
       if self._headless:
         xvfb_process.kill()
 
+  def ResetBrowserState(self):
+    """Override for chrome state reseting."""
+    assert os.path.isdir(self._profile_dir)
+    logging.info('Reset chrome\'s profile')
+    # Don't do a rmtree(self._profile_dir) because it might be a temp directory.
+    for filename in os.listdir(self._profile_dir):
+      path = os.path.join(self._profile_dir, filename)
+      if os.path.isdir(path):
+        shutil.rmtree(path)
+      else:
+        os.remove(path)
+
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
     self._EnsureProfileDirectory()
@@ -524,7 +554,6 @@ class LocalChromeController(ChromeControllerBase):
       # Launch chrome so that it populates the profile directory.
       with self.Open():
         pass
-      print os.listdir(self._profile_dir + '/Default')
     assert os.path.isdir(self._profile_dir)
     assert os.path.isdir(os.path.dirname(self._GetCacheDirectoryPath()))
 
diff --git a/loading/options.py b/loading/options.py
index b7f03a2..af21ae3 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -18,9 +18,7 @@ class Options(object):
   be available as instance attributes (eg, OPTIONS.clear_cache).
   """
   # Tuples of (argument name, default value, help string).
-  _ARGS = [ ('clear_device_data', False,
-             'Clear Chrome data from device before loading'),
-            ('chrome_package_name', 'chrome',
+  _ARGS = [ ('chrome_package_name', 'chrome',
              'build/android/pylib/constants package description'),
             ('devtools_hostname', 'localhost',
              'hostname for devtools websocket connection'),
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 665824b..03c6b04 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -224,6 +224,7 @@ class SandwichRunner(object):
       trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, run_id):
+    self._chrome_ctl.ResetBrowserState()
     clear_cache = False
     if self.cache_operation == 'clear':
       clear_cache = True
diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
index 3dc6c81..ec85dca 100755
--- a/loading/unmaintained/gce_validation_collect.sh
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -37,7 +37,6 @@ EOF
  for ((run=0;run<$repeat_count;++run)); do
    echo '****'  $run
    tools/android/loading/analyze.py log_requests \
-      --clear_device_data \
       --devtools_port 9222 \
       --url $site \
       --output $outdir/${output_subdir}/${run}

commit ec2e204d8d44d271fc2d338a63c3f091b7e2f24a
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 13 05:35:05 2016 -0700

    tools/android/loading: Implement DevToolsConnectionTargetCrashed
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1972133002
    Cr-Original-Commit-Position: refs/heads/master@{#393507}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6d741930ddf7556885fa3abdfe8da4dd903b02e5

diff --git a/loading/controller.py b/loading/controller.py
index e9f5995..5ccf688 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -78,7 +78,8 @@ class ChromeControllerError(Exception):
     Some of these errors might be known intermittent errors that can usually be
     retried by the caller after re-doing any specific setup again.
   """
-  _INTERMITTENT_WHITE_LIST = {websocket.WebSocketTimeoutException}
+  _INTERMITTENT_WHITE_LIST = {websocket.WebSocketTimeoutException,
+                              devtools_monitor.DevToolsConnectionTargetCrashed}
 
   def __init__(self, log):
     """Constructor
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 4952d10..c2d5971 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -33,6 +33,9 @@ class DevToolsConnectionException(Exception):
     super(DevToolsConnectionException, self).__init__(message)
     logging.warning("DevToolsConnectionException: " + message)
 
+class DevToolsConnectionTargetCrashed(DevToolsConnectionException):
+  pass
+
 
 # Taken from telemetry.internal.backends.chrome_inspector.tracing_backend.
 # TODO(mattcary): combine this with the above and export?
@@ -111,6 +114,7 @@ class DevToolsConnection(object):
     self._target_descriptor = None
 
     self._Connect()
+    self.RegisterListener('Inspector.targetCrashed', self)
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -297,6 +301,11 @@ class DevToolsConnection(object):
     if not self._please_stop:
       logging.warning('%s stopped on a timeout.' % kind)
 
+  def Handle(self, method, event):
+    del event # unused
+    if method == 'Inspector.targetCrashed':
+      raise DevToolsConnectionTargetCrashed('Renderer crashed.')
+
   def _TearDownMonitoring(self):
     if self.TRACING_DOMAIN in self._domains_to_enable:
       logging.info('Fetching tracing')

commit 141c8659df4e0c159f18940e52c86c9c7bb79f00
Author: droger <droger@chromium.org>
Date:   Fri May 13 04:43:50 2016 -0700

    tools/android/loading Add a task handler for generating trace reports
    
    The report task:
    - searches files with prefix "trace_database" and loads them.
    - for each URL, lookup in the database and find the traces
    - load the trace, generate a report from it
    - upload to BigQuery using the streaming API. This creates
      the table lazily, but requires a table template (not
      created automatically).
    
    The change of the URL in TraceReportHandler is to ensure
    that the URL in the loading trace database is the same as
    the one given by the user.
    Otherwise, the user will not be able to find some URLs in
    the database when generating reports.
    
    Review-Url: https://codereview.chromium.org/1947123002
    Cr-Original-Commit-Position: refs/heads/master@{#393500}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d47c255f3700621eeb9b8b5ba3897cb106b8364c

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index 44191e0..5a8be02 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -6,14 +6,16 @@ import os
 
 from common.clovis_task import ClovisTask
 from failure_database import FailureDatabase
+from report_task_handler import ReportTaskHandler
 from trace_task_handler import TraceTaskHandler
 
 
 class ClovisTaskHandler(object):
   """Handles all the supported clovis tasks."""
 
-  def __init__(self, base_path, failure_database, google_storage_accessor,
-               binaries_path, logger, instance_name=None):
+  def __init__(self, project_name, base_path, failure_database,
+               google_storage_accessor, bigquery_service, binaries_path, logger,
+               instance_name=None):
     """Creates a ClovisTaskHandler.
 
     Args:
@@ -22,10 +24,14 @@ class ClovisTaskHandler(object):
       instance_name(str, optional): Name of the ComputeEngine instance.
     """
     self._failure_database = failure_database
+    trace_path = os.path.join(base_path, 'trace')
     self._handlers = {
         'trace': TraceTaskHandler(
-            os.path.join(base_path, 'trace'), failure_database,
-            google_storage_accessor, binaries_path, logger, instance_name)}
+            trace_path, failure_database, google_storage_accessor,
+            binaries_path, logger, instance_name),
+        'report': ReportTaskHandler(
+            project_name, failure_database, google_storage_accessor,
+            bigquery_service, logger)}
 
   def Run(self, clovis_task):
     """Runs a clovis_task.
diff --git a/loading/cloud/backend/report_task_handler.py b/loading/cloud/backend/report_task_handler.py
new file mode 100644
index 0000000..47365b5
--- /dev/null
+++ b/loading/cloud/backend/report_task_handler.py
@@ -0,0 +1,140 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import uuid
+
+from googleapiclient import errors
+
+import common.google_error_helper as google_error_helper
+from failure_database import FailureDatabase
+from loading_trace import LoadingTrace
+from loading_trace_database import LoadingTraceDatabase
+from report import LoadingReport
+
+
+def LoadRemoteTrace(storage_accessor, remote_trace_path, logger):
+  """Loads and returns the LoadingTrace located at the remote trace path.
+
+  Args:
+    storage_accessor: (GoogleStorageAccessor) Used to download the trace from
+                                              CloudStorage.
+    remote_trace_path: (str) Path to the trace file.
+  """
+
+  # Cut the gs://<bucket_name> prefix from trace paths if needed.
+  prefix = 'gs://%s/' % storage_accessor.BucketName()
+  prefix_length = len(prefix)
+  if remote_trace_path.startswith(prefix):
+    remote_trace_path = remote_trace_path[prefix_length:]
+
+  trace_string = storage_accessor.DownloadAsString(
+      remote_trace_path)
+  if not trace_string:
+    logger.error('Failed to download: ' + remote_trace_path)
+    return None
+
+  trace_dict = json.loads(trace_string)
+  if not trace_dict:
+    logger.error('Failed to parse: ' + remote_trace_path)
+    return None
+
+  trace = LoadingTrace.FromJsonDict(trace_dict)
+  if not trace:
+    logger.error('Invalid format for: ' + remote_trace_path)
+    return None
+
+  return trace
+
+
+class ReportTaskHandler(object):
+  """Handles 'report' tasks.
+
+  This handler loads the traces given in the task parameters, generates a report
+  from them, and add them to a BigQuery table.
+  The BigQuery table is implicitly created from a template (using the stream
+  mode), and identified by the task tag.
+  """
+
+  def __init__(self, project_name, failure_database, google_storage_accessor,
+               bigquery_service, logger):
+    self._project_name = project_name
+    self._failure_database = failure_database
+    self._google_storage_accessor = google_storage_accessor
+    self._bigquery_service = bigquery_service
+    self._logger = logger
+
+  def _StreamRowsToBigQuery(self, rows, table_id):
+    """Uploads a list of rows to the BigQuery table associated with the given
+    table_id.
+
+    Args:
+      rows: (list of dict) Each dictionary is a row to add to the table.
+      table_id: (str) Identifier of the BigQuery table to update.
+    """
+    # Assumes that the dataset and the table template already exist.
+    dataset = 'clovis_dataset'
+    template = 'report'
+    rows_data = [{'json': row, 'insertId': str(uuid.uuid4())} for row in rows]
+    body = {'rows': rows_data, 'templateSuffix':'_'+table_id}
+    self._logger.info('BigQuery API request:\n' + str(body))
+
+    try:
+      response = self._bigquery_service.tabledata().insertAll(
+          projectId=self._project_name, datasetId=dataset, tableId=template,
+          body=body).execute()
+      self._logger.info('BigQuery API response:\n' + str(response))
+    except errors.HttpError as http_error:
+      # Handles HTTP error response codes (such as 404), typically indicating a
+      # problem in parameters other than 'body'.
+      error_content = google_error_helper.GetErrorContent(http_error)
+      error_reason = google_error_helper.GetErrorReason(error_content)
+      self._logger.error('BigQuery API error (reason: "%s"):\n%s' % (
+            error_reason, http_error))
+      self._failure_database.AddFailure('big_query_error', error_reason)
+      if error_content:
+        self._logger.error('Error details:\n%s' % error_content)
+      return
+
+    # Handles other errors, typically when the body is ill-formatted.
+    insert_errors = response.get('insertErrors')
+    if insert_errors:
+      self._logger.error('BigQuery API error:\n' + str(insert_errors))
+      for insert_error in insert_errors:
+        self._failure_database.AddFailure('big_query_insert_error',
+                                          str(insert_error.get('errors')))
+
+  def Run(self, clovis_task):
+    """Runs a 'report' clovis_task.
+
+    Args:
+      clovis_task: (ClovisTask) The task to run.
+    """
+    if clovis_task.Action() != 'report':
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      self._failure_database.AddFailure(FailureDatabase.CRITICAL_ERROR,
+                                        'report_task_handler_run')
+      return
+
+    rows = []
+    for path in clovis_task.ActionParams()['traces']:
+      self._logger.info('Generating report for: ' + path)
+      trace = LoadRemoteTrace(self._google_storage_accessor, path, self._logger)
+      if not trace:
+        self._logger.error('Failed loading trace at: ' + path)
+        self._failure_database.AddFailure('missing_trace_for_report', path)
+        continue
+      report = LoadingReport(trace).GenerateReport()
+      if not report:
+        self._logger.error('Failed generating report for: ' + path)
+        self._failure_database.AddFailure('report_generation_failed', path)
+        continue
+      rows.append(report)
+
+    if rows:
+      tag = clovis_task.BackendParams()['tag']
+      # BigQuery table names can contain only alpha numeric characters and
+      # underscores.
+      table_id = ''.join(c for c in tag if c.isalnum() or c == '_')
+      self._StreamRowsToBigQuery(rows, table_id)
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index bcb185b..a6e83a7 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -83,14 +83,13 @@ class TraceTaskHandler(object):
     except OSError:
       pass  # Nothing to remove.
 
-    if not url.startswith('http') and not url.startswith('file'):
-      url = 'http://' + url
-
     old_stdout = sys.stdout
     old_stderr = sys.stderr
 
     trace_metadata = { 'succeeded' : False, 'url' : url }
     trace = None
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
     with open(log_filename, 'w') as sys.stdout:
       try:
         sys.stderr = sys.stdout
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index eb6cbdb..e0ebebf 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -73,10 +73,12 @@ class Worker(object):
       self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
                                         'failure_database')
 
+    bigquery_service = discovery.build('bigquery', 'v2',
+                                      credentials=self._credentials)
     self._clovis_task_handler = ClovisTaskHandler(
-        self._base_path_in_bucket, self._failure_database,
-        self._google_storage_accessor, config['binaries_path'], self._logger,
-        self._instance_name)
+        self._project_name, self._base_path_in_bucket, self._failure_database,
+        self._google_storage_accessor, bigquery_service,
+        config['binaries_path'], self._logger, self._instance_name)
 
     self._UploadFailureDatabase()
 
diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
index 20a5182..cf470e7 100644
--- a/loading/cloud/common/clovis_task.py
+++ b/loading/cloud/common/clovis_task.py
@@ -23,7 +23,7 @@ class ClovisTask(object):
           'tag' key, a unique tag will be generated.
     """
     self._action = action
-    self._action_params = action_params
+    self._action_params = action_params or {}
     self._backend_params = backend_params or {}
     # If no tag is specified, generate a unique tag.
     if not self._backend_params.get('tag'):
@@ -39,12 +39,14 @@ class ClovisTask(object):
     try:
       data = json.loads(json_dict)
       action = data['action']
-      action_params = data['action_params']
       # Vaidate the format.
       if action == 'trace':
+        action_params = data['action_params']
         urls = action_params['urls']
         if (type(urls) is not list) or (len(urls) == 0):
           return None
+      elif action == 'report':
+        action_params = data.get('action_params')
       else:
         # When more actions are supported, check that they are valid here.
         return None

commit 76833cf5b6ade8e13437185218da28cab5db7903
Author: mattcary <mattcary@chromium.org>
Date:   Fri May 13 04:17:33 2016 -0700

    Clovis: Enable tracking of % of downloaded bytes for first X paint events.
    
    Adds these stats to report.py, and includes some minor test refactoring to make
    everything easier.
    
    BUG=
    
    Review-Url: https://codereview.chromium.org/1971123002
    Cr-Original-Commit-Position: refs/heads/master@{#393498}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1b1572b4493d59e4af008b1351137297f6fb42c5

diff --git a/loading/network_activity_lens.py b/loading/network_activity_lens.py
index 2040093..b30cac8 100644
--- a/loading/network_activity_lens.py
+++ b/loading/network_activity_lens.py
@@ -36,6 +36,7 @@ class NetworkActivityLens(object):
     self._downloaded_bytes_timeline = []
     self._upload_rate_timeline = []
     self._download_rate_timeline = []
+    self._total_downloaded_bytes = 0
     requests = trace.request_track.GetEvents()
     self._network_events = list(itertools.chain.from_iterable(
         NetworkEvent.EventsFromRequest(request) for request in requests))
@@ -58,6 +59,36 @@ class NetworkActivityLens(object):
   def download_rate_timeline(self):
     return (self._start_end_times, self._download_rate_timeline)
 
+  @property
+  def total_download_bytes(self):
+    return self._total_downloaded_bytes
+
+  def DownloadedBytesAt(self, time_msec):
+    """Return the the downloaded bytes at a given timestamp.
+
+    Args:
+      time_msec: a timestamp, in the same scale as the timelines.
+
+    Returns:
+      The total bytes downloaded up until the time period ending at time_msec.
+    """
+    # We just do a linear cumulative sum. Currently this is only called a couple
+    # of times, so making an indexed cumulative sum does not seem to be worth
+    # the bother.
+    total_bytes = 0
+    previous_msec = self.downloaded_bytes_timeline[0][0]
+    for msec, nbytes in zip(*self.downloaded_bytes_timeline):
+      if msec < time_msec:
+        total_bytes += nbytes
+        previous_msec = msec
+      else:
+        if time_msec > previous_msec:
+          fraction_of_chunk = ((time_msec - previous_msec)
+                               / (msec - previous_msec))
+          total_bytes += float(nbytes) * fraction_of_chunk
+        break
+    return total_bytes
+
   def _IndexEvents(self):
     start_end_times_set = set()
     for event in self._network_events:
@@ -87,6 +118,7 @@ class NetworkActivityLens(object):
       downloaded_bytes = sum(
           e.DownloadedBytes() for e in self._active_events_list[index]
           if timestamp == e.end_msec)
+      self._total_downloaded_bytes += downloaded_bytes
       self._uploaded_bytes_timeline.append(uploaded_bytes)
       self._downloaded_bytes_timeline.append(downloaded_bytes)
       self._upload_rate_timeline.append(upload_rate)
diff --git a/loading/network_activity_lens_unittest.py b/loading/network_activity_lens_unittest.py
index e857679..7b2031e 100644
--- a/loading/network_activity_lens_unittest.py
+++ b/loading/network_activity_lens_unittest.py
@@ -67,6 +67,7 @@ class NetworkActivityLensTestCase(unittest.TestCase):
     download_rate = lens.download_rate_timeline
     self.assertEquals(4 / 10e-3, download_rate[1][5])
     self.assertEquals(0, download_rate[1][6])
+    self.assertAlmostEquals(4, lens.total_download_bytes)
 
   def testLongRequest(self):
     timing_dict = {
@@ -102,6 +103,32 @@ class NetworkActivityLensTestCase(unittest.TestCase):
         self.assertEquals(0, downloaded_bytes[index])
     self.assertEquals(1000, downloaded_bytes[-1])
 
+  def testDownloadedBytesAt(self):
+    timing_dict = {
+        'requestTime': 1.2,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    lens = self._NetworkActivityLens([request])
+    # See testTransferredBytes for key events times. We test around events at
+    # the start, middle and end of the data transfer as well as for the
+    # interpolation.
+    self.assertEquals(0, lens.DownloadedBytesAt(1219))
+    self.assertEquals(0, lens.DownloadedBytesAt(1220))
+    self.assertEquals(0, lens.DownloadedBytesAt(1225))
+    self.assertEquals(0, lens.DownloadedBytesAt(1280))
+    self.assertEquals(1.6, lens.DownloadedBytesAt(1281))
+    self.assertEquals(8, lens.DownloadedBytesAt(1285))
+    self.assertEquals(14.4, lens.DownloadedBytesAt(1289))
+    self.assertEquals(16, lens.DownloadedBytesAt(1290))
+    self.assertEquals(16, lens.DownloadedBytesAt(1291))
+    self.assertEquals(16, lens.DownloadedBytesAt(1295))
+    self.assertEquals(16, lens.DownloadedBytesAt(1300))
+    self.assertEquals(16, lens.DownloadedBytesAt(1400))
+
   def _NetworkActivityLens(self, requests):
     trace = test_utils.LoadingTraceFromEvents(requests)
     return NetworkActivityLens(trace)
diff --git a/loading/report.py b/loading/report.py
index a38ed5d..eb19779 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -8,6 +8,7 @@ When executed as a script, takes a trace filename and print the report.
 """
 
 import loading_trace
+from network_activity_lens import NetworkActivityLens
 from user_satisfied_lens import (
     FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
 
@@ -33,6 +34,18 @@ class LoadingReport(object):
     self._max_msec = max(
         r.end_msec or -1 for r in self.trace.request_track.GetEvents())
 
+    network_lens = NetworkActivityLens(self.trace)
+    if network_lens.total_download_bytes > 0:
+      self._contentful_byte_frac = network_lens.DownloadedBytesAt(
+          self._contentful_paint_msec -
+          self._base_msec) / float(network_lens.total_download_bytes)
+      self._significant_byte_frac = network_lens.DownloadedBytesAt(
+          self._significant_paint_msec -
+          self._base_msec) / float(network_lens.total_download_bytes)
+    else:
+      self._contentful_byte_frac = float('Nan')
+      self._significant_byte_frac = float('Nan')
+
   def GenerateReport(self):
     """Returns a report as a dict."""
     return {
@@ -40,7 +53,9 @@ class LoadingReport(object):
         'first_text_ms': self._text_msec - self._base_msec,
         'contentful_paint_ms': self._contentful_paint_msec - self._base_msec,
         'significant_paint_ms': self._significant_paint_msec - self._base_msec,
-        'plt_ms': self._max_msec - self._base_msec}
+        'plt_ms': self._max_msec - self._base_msec,
+        'contentful_byte_frac': self._contentful_byte_frac,
+        'significant_byte_frac': self._significant_byte_frac,}
 
   @classmethod
   def FromTraceFilename(cls, filename):
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 4d4fb3f..58c08c8 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -20,11 +20,15 @@ class LoadingReportTestCase(unittest.TestCase):
 
   @classmethod
   def _MakeTrace(cls):
-    trace_creator = user_satisfied_lens_unittest.TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(cls._FIRST_REQUEST_TIME),
                 trace_creator.RequestAt(
                     cls._FIRST_REQUEST_TIME + cls._REQUEST_OFFSET,
                     cls._DURATION)]
+    requests[0].timing.receive_headers_end = 0
+    requests[1].timing.receive_headers_end = 0
+    requests[0].encoded_data_length = 128
+    requests[1].encoded_data_length = 1024
     trace = trace_creator.CreateTrace(
         requests,
         [{'ts': cls._CONTENTFUL_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
@@ -57,6 +61,8 @@ class LoadingReportTestCase(unittest.TestCase):
                      loading_report['contentful_paint_ms'])
     self.assertEqual(self._REQUEST_OFFSET + self._DURATION,
                      loading_report['plt_ms'])
+    self.assertAlmostEqual(0.3, loading_report['contentful_byte_frac'])
+    self.assertAlmostEqual(0.14, loading_report['significant_byte_frac'], 2)
 
 
 if __name__ == '__main__':
diff --git a/loading/request_track.py b/loading/request_track.py
index 8f7829b..34e9ef8 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -51,6 +51,9 @@ class Timing(object):
     return all(getattr(self, attr) == getattr(o, attr)
                for attr in self.__slots__)
 
+  def __str__(self):
+    return str(self.ToJsonDict())
+
   def LargestOffset(self):
     """Returns the largest offset in the available timings."""
     return max(0, max(
@@ -60,6 +63,7 @@ class Timing(object):
   def ToJsonDict(self):
     return {attr: getattr(self, attr)
             for attr in self.__slots__ if getattr(self, attr) != -1}
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     return cls(**json_dict)
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 10385b5..5f0d743 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -225,3 +225,27 @@ class MockUserSatisfiedLens(user_satisfied_lens._FirstEventLens):
   def _CalculateTimes(self, _):
     self._satisfied_msec = float('inf')
     self._event_msec = float('inf')
+
+
+class TraceCreator(object):
+  def __init__(self):
+    self._request_index = 1
+
+  def RequestAt(self, timestamp_msec, duration=1):
+    timestamp_sec = float(timestamp_msec) / 1000
+    rq = request_track.Request.FromJsonDict({
+        'url': 'http://bla-%s-.com' % timestamp_msec,
+        'request_id': '0.%s' % self._request_index,
+        'frame_id': '123.%s' % timestamp_msec,
+        'initiator': {'type': 'other'},
+        'timestamp': timestamp_sec,
+        'timing': {'request_time': timestamp_sec,
+                   'loading_finished': duration}
+        })
+    self._request_index += 1
+    return rq
+
+  def CreateTrace(self, requests, events, main_frame_id):
+    trace = LoadingTraceFromEvents(requests, trace_events=events)
+    trace.tracing_track.SetMainFrameID(main_frame_id)
+    return trace
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 4d0bda7..225548d 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -46,7 +46,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testFirstContentfulPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -71,7 +71,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
 
   def testCantGetNoSatisfaction(self):
     MAINFRAME = 1
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -88,7 +88,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testFirstTextPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -115,7 +115,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
 
   def testFirstSignificantPaintLens(self):
     MAINFRAME = 1
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(15), trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(
@@ -154,7 +154,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testRequestFingerprintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    trace_creator = TraceCreator()
+    trace_creator = test_utils.TraceCreator()
     requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
                 trace_creator.RequestAt(20)]
     loading_trace = trace_creator.CreateTrace(

commit 7572784ca61ddae6c8b288994ab389d94e14fea1
Author: droger <droger@chromium.org>
Date:   Fri May 13 02:22:02 2016 -0700

    tools/android/loading Split error helpers to their own file
    
    This will allow this code to be shared more easily in a follow up CL.
    
    Review-Url: https://codereview.chromium.org/1961353002
    Cr-Original-Commit-Position: refs/heads/master@{#393489}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 992640148a4d737e9b7cfd8338b1054adba1205e

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index 00ba434..44191e0 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -4,7 +4,7 @@
 
 import os
 
-from cloud.common.clovis_task import ClovisTask
+from common.clovis_task import ClovisTask
 from failure_database import FailureDatabase
 from trace_task_handler import TraceTaskHandler
 
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index 25aff4f..bcb185b 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -7,7 +7,7 @@ import os
 import re
 import sys
 
-from cloud.common.clovis_task import ClovisTask
+from common.clovis_task import ClovisTask
 import controller
 from failure_database import FailureDatabase
 import loading_trace
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index e5777c8..eb6cbdb 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -14,11 +14,15 @@ from oauth2client.client import GoogleCredentials
 # NOTE: The parent directory needs to be first in sys.path to avoid conflicts
 # with catapult modules that have colliding names, as catapult inserts itself
 # into the path as the second element. This is an ugly and fragile hack.
-sys.path.insert(0,
-    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir,
-                 os.pardir))
-from cloud.common.clovis_task import ClovisTask
-from cloud.common.google_instance_helper import GoogleInstanceHelper
+_CLOUD_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)),
+                          os.pardir)
+sys.path.insert(0, os.path.join(_CLOUD_DIR, os.pardir))
+# Add _CLOUD_DIR to the path to access common code through the same path as the
+# frontend.
+sys.path.append(_CLOUD_DIR)
+
+from common.clovis_task import ClovisTask
+from common.google_instance_helper import GoogleInstanceHelper
 from clovis_task_handler import ClovisTaskHandler
 from failure_database import FailureDatabase
 from google_storage_accessor import GoogleStorageAccessor
diff --git a/loading/cloud/common/google_error_helper.py b/loading/cloud/common/google_error_helper.py
new file mode 100644
index 0000000..eac968a
--- /dev/null
+++ b/loading/cloud/common/google_error_helper.py
@@ -0,0 +1,33 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Helper functions to manage errors returned by Google Compute APIs."""
+
+import json
+
+
+# Error reason returned by GetErrorReason() when a resource is not found.
+REASON_NOT_FOUND = 'notFound'
+
+
+def GetErrorContent(error):
+  """Returns the contents of an error returned by Google Compute APIs as a
+  dictionary or None.
+  """
+  if not error.resp.get('content-type', '').startswith('application/json'):
+    return None
+  return json.loads(error.content)
+
+
+def GetErrorReason(error_content):
+  """Returns the error reason as a string."""
+  if not error_content:
+    return None
+  if (not error_content.get('error') or
+      not error_content['error'].get('errors')):
+    return None
+  error_list = error_content['error']['errors']
+  if not error_list:
+    return None
+  return error_list[0].get('reason')
diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 746512d..02c4121 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -7,6 +7,8 @@ import time
 
 from googleapiclient import (discovery, errors)
 
+import common.google_error_helper as google_error_helper
+
 
 class GoogleInstanceHelper(object):
   """Helper class for the Google Compute API, allowing to manage groups of
@@ -32,8 +34,8 @@ class GoogleInstanceHelper(object):
       self._logger.info('Compute API response:\n' + str(response))
       return (True, response)
     except errors.HttpError as err:
-      error_content = self._GetErrorContent(err)
-      error_reason = self._GetErrorReason(error_content)
+      error_content = google_error_helper.GetErrorContent(err)
+      error_reason = google_error_helper.GetErrorReason(error_content)
       if error_reason == 'resourceNotReady' and retry_count > 0:
         # Retry after a delay
         delay_seconds = 1
@@ -56,26 +58,6 @@ class GoogleInstanceHelper(object):
     """Returns the name of the instance group associated with tag."""
     return 'group-' + tag
 
-  def _GetErrorContent(self, error):
-    """Returns the contents of an error returned by the Compute API as a
-    dictionary or None.
-    """
-    if not error.resp.get('content-type', '').startswith('application/json'):
-      return None
-    return json.loads(error.content)
-
-  def _GetErrorReason(self, error_content):
-    """Returns the error reason as a string."""
-    if not error_content:
-      return None
-    if (not error_content.get('error') or
-        not error_content['error'].get('errors')):
-      return None
-    error_list = error_content['error']['errors']
-    if not error_list:
-      return None
-    return error_list[0].get('reason')
-
   def CreateTemplate(self, tag, bucket):
     """Creates an instance template for instances identified by tag and using
     bucket for deployment. Returns True if successful.
@@ -128,7 +110,8 @@ class GoogleInstanceHelper(object):
     (success, result) = self._ExecuteApiRequest(request)
     if success:
       return True
-    if self._GetErrorReason(result) == 'notFound':
+    if google_error_helper.GetErrorReason(result) == \
+        google_error_helper.REASON_NOT_FOUND:
       # The template does not exist, nothing to do.
       self._logger.warning('Template not found: ' + template_name)
       return True
@@ -176,7 +159,8 @@ class GoogleInstanceHelper(object):
     (success, result) = self._ExecuteApiRequest(request)
     if success:
       return True
-    if self._GetErrorReason(result) == 'notFound':
+    if google_error_helper.GetErrorReason(result) == \
+        google_error_helper.REASON_NOT_FOUND:
       # The group does not exist, nothing to do.
       self._logger.warning('Instance group not found: ' + group_name)
       return True

commit ba1cc0c57318dd1d5487ff3dbf8e5584cdb3f6a6
Author: gabadie <gabadie@chromium.org>
Date:   Fri May 13 01:51:40 2016 -0700

    sandwich: Add the cache archive patching task.
    
    Set-cookie response header is by design pruned from the request's
    reponse headers that are cached to keep the cookie modification
    coherent from JavaScript and Server sided modifications.
    
    But redirected requests are getting an implicit Vary: cookie. As
    a result, some redirected requests were getting invalidated.
    
    Moreover, this change is believe to reduce noise in the requests
    interminism brought by JavaScript.
    
    This CL take this oportunity to also remove stream index 2 from
    cached entries since NoState-Prefetch will only fetch resources,
    but not parse them.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1964643003
    Cr-Original-Commit-Position: refs/heads/master@{#393481}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 75174ba408c51748e01d6462c1f74b02c00573b0

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index df8f955..ead016e 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -29,7 +29,7 @@ OPTIONS = options.OPTIONS
 
 
 # Cache back-end types supported by cachetool.
-BACKEND_TYPES = ['simple']
+BACKEND_TYPES = {'simple', 'blockfile'}
 
 # Regex used to parse HTTP headers line by line.
 HEADER_PARSING_REGEX = re.compile(r'^(?P<header>\S+):(?P<value>.*)$')
@@ -251,6 +251,11 @@ class CacheBackend(object):
     # Make sure cache_directory_path is a valid cache.
     self._CachetoolCmd('validate')
 
+  def GetSize(self):
+    """Gets total size of cache entries in bytes."""
+    size = self._CachetoolCmd('get_size')
+    return int(size.strip())
+
   def ListKeys(self):
     """Lists cache's keys.
 
@@ -272,7 +277,16 @@ class CacheBackend(object):
     Returns:
       String holding stream binary content.
     """
-    return self._CachetoolCmd('get_stream', key, str(index))
+    return self._CachetoolCmd('get_stream', [key, str(index)])
+
+  def DeleteStreamForKey(self, key, index):
+    """Delete a key's stream.
+
+    Args:
+      key: The key to access the stream.
+      index: The stream index
+    """
+    self._CachetoolCmd('delete_stream', [key, str(index)])
 
   def DeleteKey(self, key):
     """Deletes a key from the cache.
@@ -280,14 +294,15 @@ class CacheBackend(object):
     Args:
       key: The key delete.
     """
-    self._CachetoolCmd('delete_key', key)
+    self._CachetoolCmd('delete_key', [key])
 
-  def _CachetoolCmd(self, operation, *args):
+  def _CachetoolCmd(self, operation, args=None, stdin=''):
     """Runs the cache editor tool and return the stdout.
 
     Args:
       operation: Cachetool operation.
-      *args: Additional operation argument to append to the command line.
+      args: Additional operation argument to append to the command line.
+      stdin: String to pipe to the Cachetool's stdin.
 
     Returns:
       Cachetool's stdout string.
@@ -297,12 +312,22 @@ class CacheBackend(object):
         self._cache_directory_path,
         self._cache_backend_type,
         operation]
-    editor_tool_cmd.extend(args)
-    process = subprocess.Popen(editor_tool_cmd, stdout=subprocess.PIPE)
-    stdout_data, _ = process.communicate()
+    editor_tool_cmd.extend(args or [])
+    process = subprocess.Popen(
+        editor_tool_cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE)
+    stdout_data, _ = process.communicate(input=stdin)
     assert process.returncode == 0
     return stdout_data
 
+  def UpdateRawResponseHeaders(self, key, raw_headers):
+    """Updates a key's raw response headers.
+
+    Args:
+      key: The key to modify.
+      raw_headers: Raw response headers to set.
+    """
+    self._CachetoolCmd('update_raw_headers', [key], stdin=raw_headers)
+
   def GetDecodedContentForKey(self, key):
     """Gets a key's decoded content.
 
diff --git a/loading/request_track.py b/loading/request_track.py
index 5da2abe..8f7829b 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -163,6 +163,7 @@ class Request(object):
                          Worker.
     timing: (Timing) Request timing, extended with loading_finished.
     status: (int) Response status code.
+    status_text: (str) Response status text received in the status line.
     encoded_data_length: (int) Total encoded data length.
     data_chunks: (list) [(offset, encoded_data_length), ...] List of data
                  chunks received, with their offset in ms relative to
@@ -200,6 +201,7 @@ class Request(object):
     self.from_service_worker = False
     self.timing = None
     self.status = None
+    self.status_text = None
     self.encoded_data_length = 0
     self.data_chunks = []
     self.failed = False
@@ -346,6 +348,17 @@ class Request(object):
     # All fields in timing are millis relative to request_time.
     return self.timing.LargestOffset()
 
+  def GetRawResponseHeaders(self):
+    """Gets the request's raw response headers compatible with
+    net::HttpResponseHeaders's constructor.
+    """
+    assert not self.IsDataRequest()
+    headers = '{} {} {}\x00'.format(
+        self.protocol.upper(), self.status, self.status_text)
+    for key in sorted(self.response_headers.keys()):
+      headers += '{}: {}\x00'.format(key, self.response_headers[key])
+    return headers
+
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
 
@@ -668,7 +681,8 @@ class RequestTrack(devtools_monitor.Track):
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache'),
-                           ('protocol', 'protocol'), ('status', 'status')))
+                           ('protocol', 'protocol'), ('status', 'status'),
+                           ('statusText', 'status_text')))
     r.timing = Timing.FromDevToolsDict(redirect_response['timing'])
 
     redirect_index = self._redirects_count_by_id[request_id]
@@ -712,7 +726,7 @@ class RequestTrack(devtools_monitor.Track):
         response, r, (('status', 'status'), ('mimeType', 'mime_type'),
                       ('fromDiskCache', 'from_disk_cache'),
                       ('fromServiceWorker', 'from_service_worker'),
-                      ('protocol', 'protocol'),
+                      ('protocol', 'protocol'), ('statusText', 'status_text'),
                       # Actual request headers are not known before reaching the
                       # network stack.
                       ('requestHeaders', 'request_headers'),
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 19f7b4a..bac6218 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -74,6 +74,15 @@ class RequestTestCase(unittest.TestCase):
     r.response_headers = {'foo': 'Bar', 'Baz': 'Foo'}
     self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
 
+  def testGetRawResponseHeaders(self):
+    r = Request()
+    r.protocol = 'http/1.1'
+    r.status = 200
+    r.status_text = 'Hello world'
+    r.response_headers = {'Foo': 'Bar', 'Baz': 'Foo'}
+    self.assertEquals('HTTP/1.1 200 Hello world\x00Baz: Foo\x00Foo: Bar\x00',
+                      r.GetRawResponseHeaders())
+
 
 class CachingPolicyTestCase(unittest.TestCase):
   _REQUEST = {
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index bbd2e72..8005350 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -101,6 +101,63 @@ def _FilterOutDataAndIncompleteRequests(requests):
     yield request
 
 
+def PatchCacheArchive(cache_archive_path, loading_trace_path,
+                      cache_archive_dest_path):
+  """Patch the cache archive.
+
+  Note: This method update the raw response headers of cache entries' to store
+    the ones such as Set-Cookie that were pruned by the
+    net::HttpCacheTransaction, and remove the stream index 2 holding resource's
+    compile meta data.
+
+  Args:
+    cache_archive_path: Input archive's path to patch.
+    loading_trace_path: Path of the loading trace that have recorded the cache
+        archive <cache_archive_path>.
+    cache_archive_dest_path: Archive destination's path.
+  """
+  trace = LoadingTrace.FromJsonFile(loading_trace_path)
+  with common_util.TemporaryDirectory(prefix='sandwich_tmp') as tmp_path:
+    cache_path = os.path.join(tmp_path, 'cache')
+    chrome_cache.UnzipDirectoryContent(cache_archive_path, cache_path)
+    cache_backend = chrome_cache.CacheBackend(cache_path, 'simple')
+    cache_entries = set(cache_backend.ListKeys())
+    logging.info('Original cache size: %d bytes' % cache_backend.GetSize())
+    for request in _FilterOutDataAndIncompleteRequests(
+        trace.request_track.GetEvents()):
+      # On requests having an upload data stream such as POST requests,
+      # net::HttpCache::GenerateCacheKey() prefixes the cache entry's key with
+      # the upload data stream's session unique identifier.
+      #
+      # It is fine to not patch these requests since when reopening Chrome,
+      # there is no way the entry can be reused since the upload data stream's
+      # identifier will be different.
+      #
+      # The fact that these entries are kept in the cache after closing Chrome
+      # properly by closing the Chrome tab as the ChromeControler.SetSlowDeath()
+      # do is a known Chrome bug (crbug.com/610725).
+      #
+      # TODO(gabadie): Add support in ValidateCacheArchiveContent() and in
+      #   VerifyBenchmarkOutputDirectory() for POST requests to be known as
+      #   impossible to use from cache.
+      if request.url not in cache_entries:
+        if request.method != 'POST':
+          raise RuntimeError('Unexpected method that is not found in cache.'
+                             ''.format(request.method))
+        continue
+      # Chrome prunes Set-Cookie from response headers before storing them in
+      # disk cache. Also, it adds implicit "Vary: cookie" header to all redirect
+      # response headers. Sandwich manages the cache, but between recording the
+      # cache and benchmarking the cookie jar is invalidated. This leads to
+      # invalidation of all cacheable redirects.
+      raw_headers = request.GetRawResponseHeaders()
+      cache_backend.UpdateRawResponseHeaders(request.url, raw_headers)
+      # NoState-Prefetch would only fetch the resources, but not parse them.
+      cache_backend.DeleteStreamForKey(request.url, 2)
+    chrome_cache.ZipDirectoryContent(cache_path, cache_archive_dest_path)
+    logging.info('Patched cache size: %d bytes' % cache_backend.GetSize())
+
+
 def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   """Extracts discoverable resource urls from a loading trace according to a
   sub-resource discoverer.
@@ -161,12 +218,15 @@ def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
   if ref_url_set == url_set:
     logging.info('  %d %s are matching.' % (len(ref_url_set), url_set_name))
     return
-  logging.error('  %s are not matching.' % url_set_name)
-  logging.error('    List of missing resources:')
-  for url in ref_url_set.difference(url_set):
+  missing_urls = ref_url_set.difference(url_set)
+  unexpected_urls = url_set.difference(ref_url_set)
+  logging.error('  %s are not matching (expected %d, had %d)' % \
+      (url_set_name, len(ref_url_set), len(url_set)))
+  logging.error('    List of %d missing resources:' % len(missing_urls))
+  for url in sorted(missing_urls):
     logging.error('-     ' + url)
-  logging.error('    List of unexpected resources:')
-  for url in url_set.difference(ref_url_set):
+  logging.error('    List of %d unexpected resources:' % len(unexpected_urls))
+  for url in sorted(unexpected_urls):
     logging.error('+     ' + url)
 
 
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 1d36c78..8fc01af 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -96,10 +96,11 @@ class SandwichTaskBuilder(task_manager.Builder):
     subresources (urls-resources.json).
 
     Here is the full dependency tree for the returned task:
-    common/cache-ref-validation.log
-      depends on: common/cache-ref.zip
-        depends on: common/webpages-patched.wpr
-          depends on: common/webpages.wpr
+    common/patched-cache-validation.log
+      depends on: common/patched-cache.zip
+        depends on: common/original-cache.zip
+          depends on: common/webpages-patched.wpr
+            depends on: common/webpages.wpr
       depends on: common/urls-resources.json
         depends on: common/urls-resources-run/
           depends on: common/webpages.wpr
@@ -113,14 +114,21 @@ class SandwichTaskBuilder(task_manager.Builder):
       shutil.copyfile(self._original_wpr_task.path, BuildPatchedWpr.path)
       sandwich_misc.PatchWpr(BuildPatchedWpr.path)
 
-    @self.RegisterTask('common/cache-ref.zip', [BuildPatchedWpr])
-    def BuildReferenceCache():
+    @self.RegisterTask('common/original-cache.zip', [BuildPatchedWpr])
+    def BuildOriginalCache():
       runner = self._CreateSandwichRunner()
       runner.wpr_archive_path = BuildPatchedWpr.path
-      runner.cache_archive_path = BuildReferenceCache.path
+      runner.cache_archive_path = BuildOriginalCache.path
       runner.cache_operation = 'save'
-      runner.trace_output_directory = BuildReferenceCache.path[:-4] + '-run'
+      runner.trace_output_directory = BuildOriginalCache.run_path
       runner.Run()
+    BuildOriginalCache.run_path = BuildOriginalCache.path[:-4] + '-run'
+
+    @self.RegisterTask('common/patched-cache.zip', [BuildOriginalCache])
+    def BuildPatchedCache():
+      sandwich_misc.PatchCacheArchive(BuildOriginalCache.path,
+          os.path.join(BuildOriginalCache.run_path, '0', 'trace.json'),
+          BuildPatchedCache.path)
 
     @self.RegisterTask('common/subresources-for-urls-run/',
                        dependencies=[self._original_wpr_task])
@@ -138,23 +146,23 @@ class SandwichTaskBuilder(task_manager.Builder):
       with open(ListUrlsResources.path, 'w') as output:
         json.dump(json_content, output)
 
-    @self.RegisterTask('common/cache-ref-validation.log',
-                       [BuildReferenceCache, ListUrlsResources])
-    def ValidateReferenceCache():
+    @self.RegisterTask('common/patched-cache-validation.log',
+                       [BuildPatchedCache, ListUrlsResources])
+    def ValidatePatchedCache():
       json_content = json.load(open(ListUrlsResources.path))
       ref_urls = set()
       for urls in json_content.values():
         ref_urls.update(set(urls))
       sandwich_misc.ValidateCacheArchiveContent(
-          ref_urls, BuildReferenceCache.path)
+          ref_urls, BuildPatchedCache.path)
 
     self._patched_wpr_task = BuildPatchedWpr
-    self._reference_cache_task = BuildReferenceCache
+    self._reference_cache_task = BuildPatchedCache
     self._subresources_for_urls_run_task = UrlsResourcesRun
     self._subresources_for_urls_task = ListUrlsResources
 
-    self._default_final_tasks.append(ValidateReferenceCache)
-    return ValidateReferenceCache
+    self._default_final_tasks.append(ValidatePatchedCache)
+    return ValidatePatchedCache
 
   def PopulateLoadBenchmark(self, subresource_discoverer,
                             transformer_list_name, transformer_list):

commit 1527f66f5082d56574e6202a131fbb6d8d64f400
Author: blundell <blundell@chromium.org>
Date:   Fri May 13 01:37:59 2016 -0700

    tools/android/loading: Add URL to LoadingReport
    
    This CL adds the URL of a trace to the LoadingReport generated for that
    trace.
    
    Review-Url: https://codereview.chromium.org/1972993002
    Cr-Original-Commit-Position: refs/heads/master@{#393477}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a6ea2f67fa437099ba7f7eaf51fb1165b0227eec

diff --git a/loading/report.py b/loading/report.py
index 3918723..a38ed5d 100644
--- a/loading/report.py
+++ b/loading/report.py
@@ -36,6 +36,7 @@ class LoadingReport(object):
   def GenerateReport(self):
     """Returns a report as a dict."""
     return {
+        'url': self.trace.url,
         'first_text_ms': self._text_msec - self._base_msec,
         'contentful_paint_ms': self._contentful_paint_msec - self._base_msec,
         'significant_paint_ms': self._significant_paint_msec - self._base_msec,
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
index 3475681..4d4fb3f 100644
--- a/loading/report_unittest.py
+++ b/loading/report_unittest.py
@@ -48,6 +48,7 @@ class LoadingReportTestCase(unittest.TestCase):
   def testGenerateReport(self):
     trace = self._MakeTrace()
     loading_report = report.LoadingReport(trace).GenerateReport()
+    self.assertEqual(trace.url, loading_report['url'])
     self.assertEqual(self._TEXT_PAINT - self._FIRST_REQUEST_TIME,
                      loading_report['first_text_ms'])
     self.assertEqual(self._SIGNIFICANT_PAINT - self._FIRST_REQUEST_TIME,
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 5bc4715..4d0bda7 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -31,6 +31,7 @@ class TraceCreator(object):
     loading_trace = test_utils.LoadingTraceFromEvents(
         requests, trace_events=events)
     loading_trace.tracing_track.SetMainFrameID(main_frame_id)
+    loading_trace.url = 'http://www.dummy.com'
     return loading_trace
 
 

commit 9e54e0ffbda81d28acfc51c09f67db6f93926b86
Author: gabadie <gabadie@chromium.org>
Date:   Thu May 12 10:33:15 2016 -0700

    tools/android/loading: Get ChromeControllerMetadataGatherer working outside of git
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1972953003
    Cr-Original-Commit-Position: refs/heads/master@{#393286}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7448dac70f1ffdc633d6cf56112c14f4414ebb38

diff --git a/loading/controller.py b/loading/controller.py
index 56d8fd8..e9f5995 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -54,9 +54,12 @@ class ChromeControllerMetadataGatherer(object):
     if self._chromium_commit is None:
       def _GitCommand(subcmd):
         return subprocess.check_output(['git', '-C', _SRC_DIR] + subcmd).strip()
-      self._chromium_commit = _GitCommand(['merge-base', 'master', 'HEAD'])
-      if self._chromium_commit != _GitCommand(['rev-parse', 'HEAD']):
-        self._chromium_commit = 'unknown'
+      try:
+        self._chromium_commit = _GitCommand(['merge-base', 'master', 'HEAD'])
+        if self._chromium_commit != _GitCommand(['rev-parse', 'HEAD']):
+          self._chromium_commit = 'unknown'
+      except subprocess.CalledProcessError:
+        self._chromium_commit = 'git_error'
     return {
       'chromium_commit': self._chromium_commit,
       'date': datetime.datetime.utcnow().isoformat(),

commit 443f9735ee85b225cbf0a1fa1faf94fdc21de55d
Author: gabadie <gabadie@chromium.org>
Date:   Thu May 12 05:44:15 2016 -0700

    sandwich: Add platform infos into the CSVs
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1951033002
    Cr-Original-Commit-Position: refs/heads/master@{#393233}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e860e1d12970b7867e086acaa429b97778bf9171

diff --git a/loading/controller.py b/loading/controller.py
index 8b36d2b..56d8fd8 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -15,6 +15,7 @@ import datetime
 import errno
 import logging
 import os
+import platform
 import shutil
 import socket
 import subprocess
@@ -42,6 +43,27 @@ sys.path.append(
 import websocket
 
 
+class ChromeControllerMetadataGatherer(object):
+  """Gather metadata for the ChromeControllerBase."""
+
+  def __init__(self):
+    self._chromium_commit = None
+
+  def GetMetadata(self):
+    """Gets metadata to update in the ChromeControllerBase"""
+    if self._chromium_commit is None:
+      def _GitCommand(subcmd):
+        return subprocess.check_output(['git', '-C', _SRC_DIR] + subcmd).strip()
+      self._chromium_commit = _GitCommand(['merge-base', 'master', 'HEAD'])
+      if self._chromium_commit != _GitCommand(['rev-parse', 'HEAD']):
+        self._chromium_commit = 'unknown'
+    return {
+      'chromium_commit': self._chromium_commit,
+      'date': datetime.datetime.utcnow().isoformat(),
+      'seconds_since_epoch': time.time()
+    }
+
+
 class ChromeControllerInternalError(Exception):
   pass
 
@@ -92,6 +114,7 @@ class ChromeControllerBase(object):
 
   Defines common operations but should not be created directly.
   """
+  METADATA_GATHERER = ChromeControllerMetadataGatherer()
   DEVTOOLS_CONNECTION_ATTEMPTS = 10
   DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
 
@@ -240,8 +263,7 @@ class ChromeControllerBase(object):
     else:
       self._metadata['network_emulation'] = \
           {k: 'disabled' for k in ['name', 'download', 'upload', 'latency']}
-    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
-                          seconds_since_epoch=time.time())
+    self._metadata.update(self.METADATA_GATHERER.GetMetadata())
     logging.info('Devtools connection success')
 
   def _GetChromeArguments(self):
@@ -268,6 +290,10 @@ class RemoteChromeController(ChromeControllerBase):
     super(RemoteChromeController, self).__init__()
     self._device = device
     self._device.EnableRoot()
+    self._metadata['platform'] = {
+        'os': 'A-' + device.build_id,
+        'product_model': device.product_model
+    }
 
   def GetDevice(self):
     """Overridden android device."""
@@ -363,6 +389,10 @@ class LocalChromeController(ChromeControllerBase):
     if self._using_temp_profile_dir:
       self._profile_dir = tempfile.mkdtemp(suffix='.profile')
     self._headless = False
+    self._metadata['platform'] = {
+        'os': platform.system()[0] + '-' + platform.release(),
+        'product_model': 'unknown'
+    }
 
   def __del__(self):
     if self._using_temp_profile_dir:
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index b472ce6..6fbf1c8 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -35,6 +35,8 @@ import tracing
 CSV_FIELD_NAMES = [
     'repeat_id',
     'url',
+    'chromium_commit',
+    'platform',
     'subresource_discoverer',
     'subresource_count',
     # The amount of subresources detected at SetupBenchmark step.
@@ -290,7 +292,12 @@ def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
   trace_path = os.path.join(run_directory_path, 'trace.json')
   logging.info('processing trace \'%s\'' % trace_path)
   loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
-  run_metrics = {'url': loading_trace.url}
+  run_metrics = {
+      'url': loading_trace.url,
+      'chromium_commit': loading_trace.metadata['chromium_commit'],
+      'platform': (loading_trace.metadata['platform']['os'] + '-' +
+          loading_trace.metadata['platform']['product_model'])
+  }
   run_metrics.update(_ExtractDefaultMetrics(loading_trace))
   run_metrics.update(_ExtractMemoryMetrics(loading_trace))
   run_metrics.update(

commit d7f2a4009b18b4c2553bf169e51f43bd7797e31b
Author: gab <gab@chromium.org>
Date:   Wed May 11 12:40:11 2016 -0700

    Fix include path for moved thread_task_runner_handle.h header in tools/
    
    Changes made by tools/git/move_source_file.py
    
    BUG=610438
    TBR=scottmg
    
    Review-Url: https://codereview.chromium.org/1969863002
    Cr-Original-Commit-Position: refs/heads/master@{#393030}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d3d8663862fb3e5d7f2b75e135c5dd39a96722de

diff --git a/forwarder2/device_controller.cc b/forwarder2/device_controller.cc
index a3c5505..7236baf 100644
--- a/forwarder2/device_controller.cc
+++ b/forwarder2/device_controller.cc
@@ -11,7 +11,7 @@
 #include "base/callback_helpers.h"
 #include "base/logging.h"
 #include "base/single_thread_task_runner.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/device_listener.h"
 #include "tools/android/forwarder2/socket.h"
diff --git a/forwarder2/device_listener.cc b/forwarder2/device_listener.cc
index 98459f0..31f9687 100644
--- a/forwarder2/device_listener.cc
+++ b/forwarder2/device_listener.cc
@@ -12,7 +12,7 @@
 #include "base/callback.h"
 #include "base/logging.h"
 #include "base/single_thread_task_runner.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/forwarder.h"
 #include "tools/android/forwarder2/socket.h"
diff --git a/forwarder2/host_controller.cc b/forwarder2/host_controller.cc
index 0510890..25b253a 100644
--- a/forwarder2/host_controller.cc
+++ b/forwarder2/host_controller.cc
@@ -11,7 +11,7 @@
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/logging.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/forwarder.h"
 #include "tools/android/forwarder2/socket.h"
diff --git a/forwarder2/self_deleter_helper.h b/forwarder2/self_deleter_helper.h
index 089df03..582f87b 100644
--- a/forwarder2/self_deleter_helper.h
+++ b/forwarder2/self_deleter_helper.h
@@ -15,7 +15,7 @@
 #include "base/memory/ptr_util.h"
 #include "base/memory/ref_counted.h"
 #include "base/memory/weak_ptr.h"
-#include "base/thread_task_runner_handle.h"
+#include "base/threading/thread_task_runner_handle.h"
 
 namespace base {
 

commit b5e0c67f40b3ed9bf4e02927c40748ebefcca592
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 11 09:17:52 2016 -0700

    sandwich: Ignore XmlHttpRequest that have received a response after trace recording
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1972493003
    Cr-Original-Commit-Position: refs/heads/master@{#392944}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 73b8dd6d253cc7ccee8eaa5b4a65324dd81ebc6f

diff --git a/loading/request_track.py b/loading/request_track.py
index 8248af1..5da2abe 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -296,6 +296,9 @@ class Request(object):
   def IsDataRequest(self):
     return self.protocol == 'data'
 
+  def HasReceivedResponse(self):
+    return self.status is not None
+
   def GetCacheControlDirective(self, directive_name):
     """Returns the value of a Cache-Control directive, or None."""
     cache_control_str = self.GetHTTPResponseHeader('Cache-Control')
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 3b44e05..bbd2e72 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -87,8 +87,15 @@ def PatchWpr(wpr_archive_path):
   wpr_archive.Persist()
 
 
-def _FilterOutDataRequests(requests):
+def _FilterOutDataAndIncompleteRequests(requests):
   for request in filter(lambda r: not r.IsDataRequest(), requests):
+    # The protocol is only known once the response has been received. But the
+    # trace recording might have been stopped with still some JavaScript
+    # originated requests that have not received any responses yet.
+    if request.protocol is None:
+      assert not request.HasReceivedResponse()
+      assert request.initiator['type'] == 'script'
+      continue
     if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
       raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
     yield request
@@ -133,10 +140,9 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   else:
     assert False
 
-  # Prune out data:// requests.
   whitelisted_urls = set()
   logging.info('white-listing %s' % first_resource_request.url)
-  for request in _FilterOutDataRequests(discovered_requests):
+  for request in _FilterOutDataAndIncompleteRequests(discovered_requests):
     logging.info('white-listing %s' % request.url)
     whitelisted_urls.add(request.url)
   return whitelisted_urls
@@ -179,7 +185,8 @@ def ListUrlRequests(trace, request_kind):
     set([str])
   """
   urls = set()
-  for request_event in _FilterOutDataRequests(trace.request_track.GetEvents()):
+  for request_event in _FilterOutDataAndIncompleteRequests(
+      trace.request_track.GetEvents()):
     if (request_kind == RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
@@ -297,7 +304,7 @@ def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
       continue
     logging.info('lists resources of %s from %s' % (trace.url, trace_path))
     urls_set = set()
-    for request_event in _FilterOutDataRequests(
+    for request_event in _FilterOutDataAndIncompleteRequests(
         trace.request_track.GetEvents()):
       if request_event.url not in urls_set:
         logging.info('  %s' % request_event.url)

commit e392b111be64f92f664f035eb7a2c75498c887a9
Author: lizeb <lizeb@chromium.org>
Date:   Tue May 10 11:23:37 2016 -0700

    clovis: Skeleton loading report.
    
    Review-Url: https://codereview.chromium.org/1946223002
    Cr-Original-Commit-Position: refs/heads/master@{#392655}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 44fe62959cffedd9fc96b1573d84f61a64bdb5aa

diff --git a/loading/report.py b/loading/report.py
new file mode 100644
index 0000000..3918723
--- /dev/null
+++ b/loading/report.py
@@ -0,0 +1,58 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Generates a loading report.
+
+When executed as a script, takes a trace filename and print the report.
+"""
+
+import loading_trace
+from user_satisfied_lens import (
+    FirstTextPaintLens, FirstContentfulPaintLens, FirstSignificantPaintLens)
+
+
+class LoadingReport(object):
+  """Generates a loading report from a loading trace."""
+  def __init__(self, trace):
+    """Constructor.
+
+    Args:
+      trace: (LoadingTrace) a loading trace.
+    """
+    self.trace = trace
+    self._text_msec = FirstTextPaintLens(self.trace).SatisfiedMs()
+    self._contentful_paint_msec = (
+        FirstContentfulPaintLens(self.trace).SatisfiedMs())
+    self._significant_paint_msec = (
+        FirstSignificantPaintLens(self.trace).SatisfiedMs())
+    self._base_msec = min(
+        r.start_msec for r in self.trace.request_track.GetEvents())
+    # TODO(lizeb): This is not PLT. Should correlate with
+    # RenderFrameImpl::didStopLoading.
+    self._max_msec = max(
+        r.end_msec or -1 for r in self.trace.request_track.GetEvents())
+
+  def GenerateReport(self):
+    """Returns a report as a dict."""
+    return {
+        'first_text_ms': self._text_msec - self._base_msec,
+        'contentful_paint_ms': self._contentful_paint_msec - self._base_msec,
+        'significant_paint_ms': self._significant_paint_msec - self._base_msec,
+        'plt_ms': self._max_msec - self._base_msec}
+
+  @classmethod
+  def FromTraceFilename(cls, filename):
+    """Returns a LoadingReport from a trace filename."""
+    trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+    return LoadingReport(trace)
+
+
+if __name__ == '__main__':
+  import sys
+  import json
+
+  trace_filename = sys.argv[1]
+  print json.dumps(
+      LoadingReport.FromTraceFilename(trace_filename).GenerateReport(),
+      indent=2)
diff --git a/loading/report_unittest.py b/loading/report_unittest.py
new file mode 100644
index 0000000..3475681
--- /dev/null
+++ b/loading/report_unittest.py
@@ -0,0 +1,62 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import report
+import test_utils
+import user_satisfied_lens_unittest
+
+
+class LoadingReportTestCase(unittest.TestCase):
+  MILLI_TO_MICRO = 1000
+  _FIRST_REQUEST_TIME = 15
+  _CONTENTFUL_PAINT = 120
+  _TEXT_PAINT = 30
+  _SIGNIFICANT_PAINT = 50
+  _DURATION = 400
+  _REQUEST_OFFSET = 5
+
+  @classmethod
+  def _MakeTrace(cls):
+    trace_creator = user_satisfied_lens_unittest.TraceCreator()
+    requests = [trace_creator.RequestAt(cls._FIRST_REQUEST_TIME),
+                trace_creator.RequestAt(
+                    cls._FIRST_REQUEST_TIME + cls._REQUEST_OFFSET,
+                    cls._DURATION)]
+    trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': cls._CONTENTFUL_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': 1}},
+         {'ts': cls._TEXT_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstPaint',
+          'args': {'frame': 1}},
+         {'ts': 90 * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': cls._SIGNIFICANT_PAINT * cls.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 10
+          }}}], 1)
+    return trace
+
+  def testGenerateReport(self):
+    trace = self._MakeTrace()
+    loading_report = report.LoadingReport(trace).GenerateReport()
+    self.assertEqual(self._TEXT_PAINT - self._FIRST_REQUEST_TIME,
+                     loading_report['first_text_ms'])
+    self.assertEqual(self._SIGNIFICANT_PAINT - self._FIRST_REQUEST_TIME,
+                     loading_report['significant_paint_ms'])
+    self.assertEqual(self._CONTENTFUL_PAINT - self._FIRST_REQUEST_TIME,
+                     loading_report['contentful_paint_ms'])
+    self.assertEqual(self._REQUEST_OFFSET + self._DURATION,
+                     loading_report['plt_ms'])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 523a240..14fdfc3 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -56,6 +56,14 @@ class _UserSatisfiedLens(object):
     """
     return 0
 
+  def SatisfiedMs(self):
+    """Returns user satisfied timestamp, in ms.
+
+    This is *not* a unix timestamp. It is relative to the same point in time
+    as the request_time field in request_track.Timing.
+    """
+    return self._satisfied_msec
+
 
 class RequestFingerprintLens(_UserSatisfiedLens):
   """A lens built using requests in a trace that match a set of fingerprints."""
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index c603bbe..5bc4715 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -8,16 +8,12 @@ import request_track
 import test_utils
 import user_satisfied_lens
 
-class UserSatisfiedLensTestCase(unittest.TestCase):
-  # We track all times in milliseconds, but raw trace events are in
-  # microseconds.
-  MILLI_TO_MICRO = 1000
 
-  def setUp(self):
-    super(UserSatisfiedLensTestCase, self).setUp()
+class TraceCreator(object):
+  def __init__(self):
     self._request_index = 1
 
-  def _RequestAt(self, timestamp_msec, duration=1):
+  def RequestAt(self, timestamp_msec, duration=1):
     timestamp_sec = float(timestamp_msec) / 1000
     rq = request_track.Request.FromJsonDict({
         'url': 'http://bla-%s-.com' % timestamp_msec,
@@ -31,38 +27,58 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     self._request_index += 1
     return rq
 
+  def CreateTrace(self, requests, events, main_frame_id):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        requests, trace_events=events)
+    loading_trace.tracing_track.SetMainFrameID(main_frame_id)
+    return loading_trace
+
+
+class UserSatisfiedLensTestCase(unittest.TestCase):
+  # We track all times in milliseconds, but raw trace events are in
+  # microseconds.
+  MILLI_TO_MICRO = 1000
+
+  def setUp(self):
+    super(UserSatisfiedLensTestCase, self).setUp()
+
   def testFirstContentfulPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink.some_other_user_timing',
-                       'name': 'firstContentfulPaint'},
-                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstDiscontentPaint'},
-                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': SUBFRAME} },
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': MAINFRAME}}])
-    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink.some_other_user_timing',
+          'name': 'firstContentfulPaint'},
+         {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstDiscontentPaint'},
+         {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': SUBFRAME} },
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testCantGetNoSatisfaction(self):
     MAINFRAME = 1
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'not_my_cat',
-                       'name': 'someEvent',
-                       'args': {'frame': MAINFRAME}}])
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'not_my_cat',
+          'name': 'someEvent',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequestIds())
@@ -71,60 +87,65 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testFirstTextPaintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink.some_other_user_timing',
-                       'name': 'firstPaint'},
-                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstishPaint',
-                       'args': {'frame': MAINFRAME}},
-                      {'ts': 3 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstPaint',
-                       'args': {'frame': SUBFRAME}},
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstPaint',
-                       'args': {'frame': MAINFRAME}}])
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink.some_other_user_timing',
+          'name': 'firstPaint'},
+         {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstishPaint',
+          'args': {'frame': MAINFRAME}},
+         {'ts': 3 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstPaint',
+          'args': {'frame': SUBFRAME}},
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstPaint',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstTextPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testFirstSignificantPaintLens(self):
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10),
-         self._RequestAt(15), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink',
-                       'name': 'firstPaint'},
-                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'FrameView::synchronizedPaint'},
-                      {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink',
-                       'name': 'FrameView::synchronizedPaint'},
-                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink',
-                       'name': 'FrameView::synchronizedPaint'},
-
-                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'foobar', 'name': 'biz',
-                       'args': {'counters': {
-                           'LayoutObjectsThatHadNeverHadLayout': 10
-                       } } },
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'foobar', 'name': 'biz',
-                       'args': {'counters': {
-                           'LayoutObjectsThatHadNeverHadLayout': 12
-                       } } },
-                      {'ts': 15 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'foobar', 'name': 'biz',
-                       'args': {'counters': {
-                           'LayoutObjectsThatHadNeverHadLayout': 10
-                       } } } ])
+    MAINFRAME = 1
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(15), trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'firstPaint'},
+         {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink',
+          'name': 'FrameView::synchronizedPaint'},
+         {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 10
+          } } },
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 12
+          } } },
+         {'ts': 15 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'foobar', 'name': 'biz',
+          'args': {'counters': {
+              'LayoutObjectsThatHadNeverHadLayout': 10
+          } } } ], MAINFRAME)
     lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(7, lens.PostloadTimeMsec())
@@ -132,23 +153,25 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
   def testRequestFingerprintLens(self):
     MAINFRAME = 1
     SUBFRAME = 2
-    loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
-        trace_events=[{'ts': 0, 'ph': 'I',
-                       'cat': 'blink.some_other_user_timing',
-                       'name': 'firstContentfulPaint'},
-                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstDiscontentPaint'},
-                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': SUBFRAME} },
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
-                       'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint',
-                       'args': {'frame': MAINFRAME}}])
-    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
+    trace_creator = TraceCreator()
+    requests = [trace_creator.RequestAt(1), trace_creator.RequestAt(10),
+                trace_creator.RequestAt(20)]
+    loading_trace = trace_creator.CreateTrace(
+        requests,
+        [{'ts': 0, 'ph': 'I',
+          'cat': 'blink.some_other_user_timing',
+          'name': 'firstContentfulPaint'},
+         {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstDiscontentPaint'},
+         {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': SUBFRAME} },
+         {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+          'cat': 'blink.user_timing',
+          'name': 'firstContentfulPaint',
+          'args': {'frame': MAINFRAME}}], MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())

commit 3adba710bd4834761e19619f0bd0f80c5932f2d8
Author: droger <droger@chromium.org>
Date:   Tue May 10 08:36:33 2016 -0700

    tools/android/loading Add a dirty bit to the failure database
    
    This is a code simplification, which will be useful when
    adding more actions to the backend.
    
    This is only refactoring, no significant change in behavior.
    
    Review-Url: https://codereview.chromium.org/1964873002
    Cr-Original-Commit-Position: refs/heads/master@{#392616}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 38c3770b5a466fbb027e5b48d1672fcc5189a660

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
index da5de14..00ba434 100644
--- a/loading/cloud/backend/clovis_task_handler.py
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -32,15 +32,11 @@ class ClovisTaskHandler(object):
 
     Args:
       clovis_task(ClovisTask): The task to run.
-
-    Returns:
-      boolean: True in case of success, False if a failure has been added to the
-               failure database.
     """
     handler = self._handlers.get(clovis_task.Action())
     if not handler:
       self._logger.error('Unsupported task action: %s' % clovis_task.Action())
       self._failure_database.AddFailure('unsupported_action',
                                         clovis_task.Action())
-      return False
-    return handler.Run(clovis_task)
+      return
+    handler.Run(clovis_task)
diff --git a/loading/cloud/backend/failure_database.py b/loading/cloud/backend/failure_database.py
index ac134af..a044f04 100644
--- a/loading/cloud/backend/failure_database.py
+++ b/loading/cloud/backend/failure_database.py
@@ -11,6 +11,7 @@ class FailureDatabase(object):
 
   def __init__(self, json_string=None):
     """Loads a FailureDatabase from a string returned by ToJsonString()."""
+    self.is_dirty = False
     if json_string:
       self._failures_dict = json.loads(json_string)
     else:
@@ -27,12 +28,14 @@ class FailureDatabase(object):
   def AddFailure(self, failure_name, failure_content=None):
     """Adds a failure with the given name and content. If the failure already
     exists, it will increment the associated count.
+    Sets the 'is_dirty' bit to True.
 
     Args:
       failure_name (str): name of the failure.
       failure_content (str): content of the failure (e.g. the URL or task that
                              is failing).
     """
+    self.is_dirty = True
     content = failure_content if failure_content else 'error_count'
     if failure_name not in self._failures_dict:
       self._failures_dict[failure_name] = {}
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
index a3acd33..25aff4f 100644
--- a/loading/cloud/backend/trace_task_handler.py
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -146,10 +146,11 @@ class TraceTaskHandler(object):
           remote_trace_location)
       self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
     else:
+      url = trace_metadata['url']
+      self._logger.warning('Trace generation failed for URL: %s' % url)
       failures_dir = os.path.join(self._base_path, 'failures')
       remote_trace_location = os.path.join(failures_dir, remote_filename)
-      self._failure_database.AddFailure('trace_collection',
-                                        trace_metadata['url'])
+      self._failure_database.AddFailure('trace_collection', url)
 
     if os.path.isfile(local_filename):
       self._logger.debug('Uploading: %s' % remote_trace_location)
@@ -167,16 +168,12 @@ class TraceTaskHandler(object):
 
     Args:
       clovis_task(ClovisTask): The task to run.
-
-    Returns:
-      boolean: True in case of success, False if a failure has been added to the
-               failure database.
     """
     if clovis_task.Action() != 'trace':
       self._logger.error('Unsupported task action: %s' % clovis_task.Action())
       self._failure_database.AddFailure(FailureDatabase.CRITICAL_ERROR,
                                         'trace_task_handler_run')
-      return False
+      return
 
     # Extract the task parameters.
     params = clovis_task.ActionParams()
@@ -189,7 +186,6 @@ class TraceTaskHandler(object):
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
 
-    failure_happened = False
     success_happened = False
 
     while len(urls) > 0:
@@ -201,9 +197,6 @@ class TraceTaskHandler(object):
             url, emulate_device, emulate_network, local_filename, log_filename)
         if trace_metadata['succeeded']:
           success_happened = True
-        else:
-          self._logger.warning('Trace generation failed for URL: %s' % url)
-          failure_happened = True
         remote_filename = os.path.join(local_filename, str(repeat))
         self._HandleTraceGenerationResults(
             local_filename, log_filename, remote_filename, trace_metadata)
@@ -211,5 +204,3 @@ class TraceTaskHandler(object):
     if success_happened:
       self._UploadTraceDatabase()
 
-    return not failure_happened
-
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index b1f6d7c..e5777c8 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -74,8 +74,7 @@ class Worker(object):
         self._google_storage_accessor, config['binaries_path'], self._logger,
         self._instance_name)
 
-    if self._failure_database.ToJsonDict():
-      self._UploadFailureDatabase()
+    self._UploadFailureDatabase()
 
   def Start(self):
     """Main worker loop.
@@ -98,8 +97,8 @@ class Worker(object):
         break
 
       self._logger.info('Processing task %s' % task_id)
-      if not self._clovis_task_handler.Run(clovis_task):
-        self._UploadFailureDatabase()
+      self._clovis_task_handler.Run(clovis_task)
+      self._UploadFailureDatabase()
       self._logger.debug('Deleting task %s' % task_id)
       task_api.tasks().delete(project=project, taskqueue=queue_name,
                               task=task_id).execute()
@@ -115,10 +114,13 @@ class Worker(object):
 
   def _UploadFailureDatabase(self):
     """Uploads the failure database to CloudStorage."""
+    if not self._failure_database.is_dirty:
+      return
     self._logger.info('Uploading failure database')
     self._google_storage_accessor.UploadString(
         self._failure_database.ToJsonString(),
         self._failure_database_path)
+    self._failure_database.is_dirty = False
 
   def _FetchClovisTask(self, project_name, task_api, queue_name):
     """Fetches a ClovisTask from the task queue.

commit 2592374a9346a71558f3a4c7cd93b03230c54bdb
Author: droger <droger@chromium.org>
Date:   Tue May 10 07:42:08 2016 -0700

    tools/android/loading Abstract the processing of a ClovisTask
    
    To prepare for the support of multiple type of tasks, this CL abstracts
    the handling of a task.
    
    Two classes are introduced:
    - TraceTaskHandler: Handles a 'trace' task
    - ClovisTaskHandler: Owns the handlers for the individual task types.
                         For now only owns a TraceTaskHandler, but others
                         can be added to handle more actions.
    
    A lot of code is moved from worker.py to trace_task_handler.py.
    
    Review-Url: https://codereview.chromium.org/1945303002
    Cr-Original-Commit-Position: refs/heads/master@{#392601}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f0bdd43b3825dcb275bce24b52216fb4821f1f74

diff --git a/loading/cloud/backend/clovis_task_handler.py b/loading/cloud/backend/clovis_task_handler.py
new file mode 100644
index 0000000..da5de14
--- /dev/null
+++ b/loading/cloud/backend/clovis_task_handler.py
@@ -0,0 +1,46 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+
+from cloud.common.clovis_task import ClovisTask
+from failure_database import FailureDatabase
+from trace_task_handler import TraceTaskHandler
+
+
+class ClovisTaskHandler(object):
+  """Handles all the supported clovis tasks."""
+
+  def __init__(self, base_path, failure_database, google_storage_accessor,
+               binaries_path, logger, instance_name=None):
+    """Creates a ClovisTaskHandler.
+
+    Args:
+      base_path(str): Base path where results are written.
+      binaries_path(str): Path to the directory where Chrome executables are.
+      instance_name(str, optional): Name of the ComputeEngine instance.
+    """
+    self._failure_database = failure_database
+    self._handlers = {
+        'trace': TraceTaskHandler(
+            os.path.join(base_path, 'trace'), failure_database,
+            google_storage_accessor, binaries_path, logger, instance_name)}
+
+  def Run(self, clovis_task):
+    """Runs a clovis_task.
+
+    Args:
+      clovis_task(ClovisTask): The task to run.
+
+    Returns:
+      boolean: True in case of success, False if a failure has been added to the
+               failure database.
+    """
+    handler = self._handlers.get(clovis_task.Action())
+    if not handler:
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      self._failure_database.AddFailure('unsupported_action',
+                                        clovis_task.Action())
+      return False
+    return handler.Run(clovis_task)
diff --git a/loading/cloud/backend/failure_database.py b/loading/cloud/backend/failure_database.py
index 4fdfde7..ac134af 100644
--- a/loading/cloud/backend/failure_database.py
+++ b/loading/cloud/backend/failure_database.py
@@ -6,6 +6,8 @@ import json
 
 class FailureDatabase(object):
   """Logs the failures happening in the Clovis backend."""
+  DIRTY_STATE_ERROR = 'startup_with_dirty_state'
+  CRITICAL_ERROR = 'critical_error'
 
   def __init__(self, json_string=None):
     """Loads a FailureDatabase from a string returned by ToJsonString()."""
diff --git a/loading/cloud/backend/google_storage_accessor.py b/loading/cloud/backend/google_storage_accessor.py
index ded3fe8..c95d742 100644
--- a/loading/cloud/backend/google_storage_accessor.py
+++ b/loading/cloud/backend/google_storage_accessor.py
@@ -18,13 +18,17 @@ class GoogleStorageAccessor(object):
     self._bucket_name = bucket_name
 
   def _GetStorageClient(self):
-    """Returns the storage client associated with the project"""
+    """Returns the storage client associated with the project."""
     return gcloud.storage.Client(project = self._project_name,
                                  credentials = self._credentials)
 
   def _GetStorageBucket(self, storage_client):
     return storage_client.get_bucket(self._bucket_name)
 
+  def BucketName(self):
+    """Returns the name of the bucket associated with this instance."""
+    return self._bucket_name
+
   def DownloadAsString(self, remote_filename):
     """Returns the content of a remote file as a string, or None if the file
     does not exist.
@@ -40,11 +44,11 @@ class GoogleStorageAccessor(object):
       return None
 
   def UploadFile(self, filename_src, filename_dest):
-    """Uploads a file to Google Cloud Storage
+    """Uploads a file to Google Cloud Storage.
 
     Args:
-      filename_src: name of the local file
-      filename_dest: name of the file in Google Cloud Storage
+      filename_src: name of the local file.
+      filename_dest: name of the file in Google Cloud Storage.
 
     Returns:
       The URL of the file in Google Cloud Storage.
@@ -57,11 +61,11 @@ class GoogleStorageAccessor(object):
     return blob.public_url
 
   def UploadString(self, data_string, filename_dest):
-    """Uploads a string to Google Cloud Storage
+    """Uploads a string to Google Cloud Storage.
 
     Args:
-      data_string: the contents of the file to be uploaded
-      filename_dest: name of the file in Google Cloud Storage
+      data_string: the contents of the file to be uploaded.
+      filename_dest: name of the file in Google Cloud Storage.
 
     Returns:
       The URL of the file in Google Cloud Storage.
diff --git a/loading/cloud/backend/trace_task_handler.py b/loading/cloud/backend/trace_task_handler.py
new file mode 100644
index 0000000..a3acd33
--- /dev/null
+++ b/loading/cloud/backend/trace_task_handler.py
@@ -0,0 +1,215 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import os
+import re
+import sys
+
+from cloud.common.clovis_task import ClovisTask
+import controller
+from failure_database import FailureDatabase
+import loading_trace
+from loading_trace_database import LoadingTraceDatabase
+import options
+
+
+class TraceTaskHandler(object):
+  """Handles 'trace' tasks."""
+
+  def __init__(self, base_path, failure_database,
+               google_storage_accessor, binaries_path, logger,
+               instance_name=None):
+    """Args:
+      base_path(str): Base path where results are written.
+      binaries_path(str): Path to the directory where Chrome executables are.
+      instance_name(str, optional): Name of the ComputeEngine instance.
+    """
+    self._failure_database = failure_database
+    self._logger = logger
+    self._google_storage_accessor = google_storage_accessor
+    self._base_path = base_path
+    if instance_name:
+      trace_database_filename = 'trace_database_%s.json' % instance_name
+    else:
+      trace_database_filename = 'trace_database.json'
+    self._trace_database_path = os.path.join(base_path, trace_database_filename)
+
+    # Recover any existing traces in case the worker died.
+    self._DownloadTraceDatabase()
+    if self._trace_database.ToJsonDict():
+      # Script is restarting after a crash, or there are already files from a
+      # previous run in the directory.
+      self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
+                                        'trace_database')
+
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs(['--local_build_dir', binaries_path])
+
+  def _DownloadTraceDatabase(self):
+    """Downloads the trace database from CloudStorage."""
+    self._logger.info('Downloading trace database')
+    trace_database_string = self._google_storage_accessor.DownloadAsString(
+        self._trace_database_path) or '{}'
+    self._trace_database = LoadingTraceDatabase.FromJsonString(
+        trace_database_string)
+
+  def _UploadTraceDatabase(self):
+    """Uploads the trace database to CloudStorage."""
+    self._logger.info('Uploading trace database')
+    self._google_storage_accessor.UploadString(
+        self._trace_database.ToJsonString(),
+        self._trace_database_path)
+
+  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
+                     log_filename):
+    """ Generates a trace.
+
+    Args:
+      url: URL as a string.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
+      filename: Name of the file where the trace is saved.
+      log_filename: Name of the file where standard output and errors are
+                    logged.
+
+    Returns:
+      A dictionary of metadata about the trace, including a 'succeeded' field
+      indicating whether the trace was successfully generated.
+    """
+    try:
+      os.remove(filename)  # Remove any existing trace for this URL.
+    except OSError:
+      pass  # Nothing to remove.
+
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
+
+    old_stdout = sys.stdout
+    old_stderr = sys.stderr
+
+    trace_metadata = { 'succeeded' : False, 'url' : url }
+    trace = None
+    with open(log_filename, 'w') as sys.stdout:
+      try:
+        sys.stderr = sys.stdout
+
+        # Set up the controller.
+        chrome_ctl = controller.LocalChromeController()
+        chrome_ctl.SetHeadless(True)
+        if emulate_device:
+          chrome_ctl.SetDeviceEmulation(emulate_device)
+        if emulate_network:
+          chrome_ctl.SetNetworkEmulation(emulate_network)
+
+        # Record and write the trace.
+        with chrome_ctl.Open() as connection:
+          connection.ClearCache()
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url, connection, chrome_ctl.ChromeMetadata())
+          trace_metadata['succeeded'] = True
+          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+      except controller.ChromeControllerError as e:
+        e.Dump(sys.stderr)
+      except Exception as e:
+        sys.stderr.write(str(e))
+
+      if trace:
+        with open(filename, 'w') as f:
+          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+
+    sys.stdout = old_stdout
+    sys.stderr = old_stderr
+
+    return trace_metadata
+
+  def _HandleTraceGenerationResults(self, local_filename, log_filename,
+                                    remote_filename, trace_metadata):
+    """Updates the trace database and the failure database after a trace
+    generation. Uploads the trace and the log.
+    Results related to successful traces are uploaded in the 'traces' directory,
+    and failures are uploaded in the 'failures' directory.
+
+    Args:
+      local_filename (str): Path to the local file containing the trace.
+      log_filename (str): Path to the local file containing the log.
+      remote_filename (str): Name of the target remote file where the trace and
+                             the log (with a .log extension added) are uploaded.
+      trace_metadata (dict): Metadata associated with the trace generation.
+    """
+    if trace_metadata['succeeded']:
+      traces_dir = os.path.join(self._base_path, 'traces')
+      remote_trace_location = os.path.join(traces_dir, remote_filename)
+      full_cloud_storage_path = os.path.join(
+          'gs://' + self._google_storage_accessor.BucketName(),
+          remote_trace_location)
+      self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
+    else:
+      failures_dir = os.path.join(self._base_path, 'failures')
+      remote_trace_location = os.path.join(failures_dir, remote_filename)
+      self._failure_database.AddFailure('trace_collection',
+                                        trace_metadata['url'])
+
+    if os.path.isfile(local_filename):
+      self._logger.debug('Uploading: %s' % remote_trace_location)
+      self._google_storage_accessor.UploadFile(local_filename,
+                                               remote_trace_location)
+    else:
+      self._logger.warning('No trace found at: ' + local_filename)
+
+    self._logger.debug('Uploading analyze log')
+    remote_log_location = remote_trace_location + '.log'
+    self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
+
+  def Run(self, clovis_task):
+    """Runs a 'trace' clovis_task.
+
+    Args:
+      clovis_task(ClovisTask): The task to run.
+
+    Returns:
+      boolean: True in case of success, False if a failure has been added to the
+               failure database.
+    """
+    if clovis_task.Action() != 'trace':
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      self._failure_database.AddFailure(FailureDatabase.CRITICAL_ERROR,
+                                        'trace_task_handler_run')
+      return False
+
+    # Extract the task parameters.
+    params = clovis_task.ActionParams()
+    urls = params['urls']
+    repeat_count = params.get('repeat_count', 1)
+    emulate_device = params.get('emulate_device')
+    emulate_network = params.get('emulate_network')
+
+    log_filename = 'analyze.log'
+    # Avoid special characters in storage object names
+    pattern = re.compile(r"[#\?\[\]\*/]")
+
+    failure_happened = False
+    success_happened = False
+
+    while len(urls) > 0:
+      url = urls.pop()
+      local_filename = pattern.sub('_', url)
+      for repeat in range(repeat_count):
+        self._logger.debug('Generating trace for URL: %s' % url)
+        trace_metadata = self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename)
+        if trace_metadata['succeeded']:
+          success_happened = True
+        else:
+          self._logger.warning('Trace generation failed for URL: %s' % url)
+          failure_happened = True
+        remote_filename = os.path.join(local_filename, str(repeat))
+        self._HandleTraceGenerationResults(
+            local_filename, log_filename, remote_filename, trace_metadata)
+
+    if success_happened:
+      self._UploadTraceDatabase()
+
+    return not failure_happened
+
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 05e8498..b1f6d7c 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -6,7 +6,6 @@ import argparse
 import json
 import logging
 import os
-import re
 import sys
 
 from googleapiclient import discovery
@@ -18,14 +17,11 @@ from oauth2client.client import GoogleCredentials
 sys.path.insert(0,
     os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir,
                  os.pardir))
-import controller
 from cloud.common.clovis_task import ClovisTask
 from cloud.common.google_instance_helper import GoogleInstanceHelper
+from clovis_task_handler import ClovisTaskHandler
 from failure_database import FailureDatabase
 from google_storage_accessor import GoogleStorageAccessor
-import loading_trace
-from loading_trace_database import LoadingTraceDatabase
-import options
 
 
 class Worker(object):
@@ -57,31 +53,29 @@ class Worker(object):
         bucket_name=self._bucket_name)
 
     if self._instance_name:
-      trace_database_filename = 'trace_database_%s.json' % self._instance_name
       failure_database_filename = \
           'failure_database_%s.json' % self._instance_name
     else:
-      trace_database_filename = 'trace_database.json'
       failure_database_filename = 'failure_dabatase.json'
-    self._traces_dir = os.path.join(self._base_path_in_bucket, 'traces')
-    self._trace_database_path = os.path.join(self._traces_dir,
-                                             trace_database_filename)
-    self._failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
-    self._failure_database_path = os.path.join(self._failures_dir,
+    self._failure_database_path = os.path.join(self._base_path_in_bucket,
                                                failure_database_filename)
 
-    # Recover any existing trace database and failures in case the worker died.
-    self._DownloadTraceDatabase()
+    # Recover any existing failures in case the worker died.
     self._DownloadFailureDatabase()
 
-    if self._trace_database.ToJsonDict() or self._failure_database.ToJsonDict():
+    if self._failure_database.ToJsonDict():
       # Script is restarting after a crash, or there are already files from a
       # previous run in the directory.
-      self._failure_database.AddFailure('startup_with_dirty_state')
-      self._UploadFailureDatabase()
+      self._failure_database.AddFailure(FailureDatabase.DIRTY_STATE_ERROR,
+                                        'failure_database')
+
+    self._clovis_task_handler = ClovisTaskHandler(
+        self._base_path_in_bucket, self._failure_database,
+        self._google_storage_accessor, config['binaries_path'], self._logger,
+        self._instance_name)
 
-    # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs(['--local_build_dir', config['binaries_path']])
+    if self._failure_database.ToJsonDict():
+      self._UploadFailureDatabase()
 
   def Start(self):
     """Main worker loop.
@@ -104,28 +98,14 @@ class Worker(object):
         break
 
       self._logger.info('Processing task %s' % task_id)
-      self._ProcessClovisTask(clovis_task)
+      if not self._clovis_task_handler.Run(clovis_task):
+        self._UploadFailureDatabase()
       self._logger.debug('Deleting task %s' % task_id)
       task_api.tasks().delete(project=project, taskqueue=queue_name,
                               task=task_id).execute()
       self._logger.info('Finished task %s' % task_id)
     self._Finalize()
 
-  def _DownloadTraceDatabase(self):
-    """Downloads the trace database from CloudStorage."""
-    self._logger.info('Downloading trace database')
-    trace_database_string = self._google_storage_accessor.DownloadAsString(
-        self._trace_database_path) or '{}'
-    self._trace_database = LoadingTraceDatabase.FromJsonString(
-        trace_database_string)
-
-  def _UploadTraceDatabase(self):
-    """Uploads the trace database to CloudStorage."""
-    self._logger.info('Uploading trace database')
-    self._google_storage_accessor.UploadString(
-        self._trace_database.ToJsonString(),
-        self._trace_database_path)
-
   def _DownloadFailureDatabase(self):
     """Downloads the failure database from CloudStorage."""
     self._logger.info('Downloading failure database')
@@ -206,144 +186,6 @@ class Worker(object):
     # Do not add anything after this line, as the instance might be killed at
     # any time.
 
-  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
-                     log_filename):
-    """ Generates a trace.
-
-    Args:
-      url: URL as a string.
-      emulate_device: Name of the device to emulate. Empty for no emulation.
-      emulate_network: Type of network emulation. Empty for no emulation.
-      filename: Name of the file where the trace is saved.
-      log_filename: Name of the file where standard output and errors are
-                    logged.
-
-    Returns:
-      A dictionary of metadata about the trace, including a 'succeeded' field
-      indicating whether the trace was successfully generated.
-    """
-    try:
-      os.remove(filename)  # Remove any existing trace for this URL.
-    except OSError:
-      pass  # Nothing to remove.
-
-    if not url.startswith('http') and not url.startswith('file'):
-      url = 'http://' + url
-
-    old_stdout = sys.stdout
-    old_stderr = sys.stderr
-
-    trace_metadata = { 'succeeded' : False, 'url' : url }
-    trace = None
-    with open(log_filename, 'w') as sys.stdout:
-      try:
-        sys.stderr = sys.stdout
-
-        # Set up the controller.
-        chrome_ctl = controller.LocalChromeController()
-        chrome_ctl.SetHeadless(True)
-        if emulate_device:
-          chrome_ctl.SetDeviceEmulation(emulate_device)
-        if emulate_network:
-          chrome_ctl.SetNetworkEmulation(emulate_network)
-
-        # Record and write the trace.
-        with chrome_ctl.Open() as connection:
-          connection.ClearCache()
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url, connection, chrome_ctl.ChromeMetadata())
-          trace_metadata['succeeded'] = True
-          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
-      except controller.ChromeControllerError as e:
-        e.Dump(sys.stderr)
-      except Exception as e:
-        sys.stderr.write(str(e))
-
-      if trace:
-        with open(filename, 'w') as f:
-          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
-
-    sys.stdout = old_stdout
-    sys.stderr = old_stderr
-
-    return trace_metadata
-
-  def _HandleTraceGenerationResults(self, local_filename, log_filename,
-                                    remote_filename, trace_metadata):
-    """Updates the trace database and the failure database after a trace
-    generation. Uploads the trace and the log.
-    Results related to successful traces are uploaded in the _traces_dir
-    directory, and failures are uploaded in the _failures_dir directory.
-
-    Args:
-      local_filename (str): Path to the local file containing the trace.
-      log_filename (str): Path to the local file containing the log.
-      remote_filename (str): Name of the target remote file where the trace and
-                             the log (with a .log extension added) are uploaded.
-      trace_metadata (dict): Metadata associated with the trace generation.
-    """
-    if trace_metadata['succeeded']:
-      remote_trace_location = os.path.join(self._traces_dir, remote_filename)
-      full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
-                                             remote_trace_location)
-      self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
-    else:
-      remote_trace_location = os.path.join(self._failures_dir, remote_filename)
-      self._failure_database.AddFailure('trace_collection',
-                                        trace_metadata['url'])
-
-    if os.path.isfile(local_filename):
-      self._logger.debug('Uploading: %s' % remote_trace_location)
-      self._google_storage_accessor.UploadFile(local_filename,
-                                               remote_trace_location)
-    else:
-      self._logger.warning('No trace found at: ' + local_filename)
-
-    self._logger.debug('Uploading analyze log')
-    remote_log_location = remote_trace_location + '.log'
-    self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
-
-  def _ProcessClovisTask(self, clovis_task):
-    """Processes one clovis_task."""
-    if clovis_task.Action() != 'trace':
-      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
-      return
-
-    # Extract the task parameters.
-    params = clovis_task.ActionParams()
-    urls = params['urls']
-    repeat_count = params.get('repeat_count', 1)
-    emulate_device = params.get('emulate_device')
-    emulate_network = params.get('emulate_network')
-
-    log_filename = 'analyze.log'
-    # Avoid special characters in storage object names
-    pattern = re.compile(r"[#\?\[\]\*/]")
-
-    failure_happened = False
-    success_happened = False
-
-    while len(urls) > 0:
-      url = urls.pop()
-      local_filename = pattern.sub('_', url)
-      for repeat in range(repeat_count):
-        self._logger.debug('Generating trace for URL: %s' % url)
-        trace_metadata = self._GenerateTrace(
-            url, emulate_device, emulate_network, local_filename, log_filename)
-        if trace_metadata['succeeded']:
-          success_happened = True
-        else:
-          self._logger.warning('Trace generation failed for URL: %s' % url)
-          failure_happened = True
-        remote_filename = os.path.join(local_filename, str(repeat))
-        self._HandleTraceGenerationResults(
-            local_filename, log_filename, remote_filename, trace_metadata)
-
-    if success_happened:
-      self._UploadTraceDatabase()
-    if failure_happened:
-      self._UploadFailureDatabase()
-
 if __name__ == '__main__':
   parser = argparse.ArgumentParser(
       description='ComputeEngine Worker for Clovis')

commit c8ae7ecb67de547346cc37f75a0a579da120176a
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 9 03:07:27 2016 -0700

    sandwich: Blacklist some keywords from Vary and Pragma response header
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1937213002
    Cr-Original-Commit-Position: refs/heads/master@{#392295}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 814d734820f33ec5e5aaa05e834671394621df1c

diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 3c7c740..3b44e05 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -60,7 +60,6 @@ def PatchWpr(wpr_archive_path):
     logging.info('patching %s' % url_entry.url)
     # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
     # TODO(gabadie): may need to delete ETag.
-    # TODO(gabadie): may need to patch Vary.
     # TODO(gabadie): may need to take care of x-cache.
     #
     # Override the cache-control header to set the resources max age to MAX_AGE.
@@ -72,6 +71,19 @@ def PatchWpr(wpr_archive_path):
     # choices but save absolutely all cached resources on disk so they survive
     # after killing chrome for cache save, modification and push.
     url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
+
+    # TODO(gabadie): May need to extend Vary blacklist (referer?)
+    #
+    # All of these Vary and Pragma possibilities need to be removed from
+    # response headers in order for Chrome to store a resource in HTTP cache and
+    # not to invalidate it.
+    #
+    # Note: HttpVaryData::Init() in Chrome adds an implicit 'Vary: cookie'
+    # header to any redirect.
+    # TODO(gabadie): Find a way to work around this issue.
+    url_entry.RemoveResponseHeaderDirectives('vary', {'*', 'cookie'})
+    url_entry.RemoveResponseHeaderDirectives('pragma', {'no-cache'})
+
   wpr_archive.Persist()
 
 
diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
index a24d282..04410ad 100644
--- a/loading/wpr_backend.py
+++ b/loading/wpr_backend.py
@@ -91,6 +91,28 @@ class WprUrlEntry(object):
     self._wpr_response.headers = \
         [x for x in self._wpr_response.headers if x[0].lower() != name]
 
+  def RemoveResponseHeaderDirectives(self, name, directives_blacklist):
+    """Removed a set of directives from response headers.
+
+    Also removes the cache header in case no more directives are left.
+    It is useful, for example, to remove 'no-cache' from 'pragma: no-cache'.
+
+    Args:
+      name: The name of the response header field to modify.
+      directives_blacklist: Set of lowered directives to remove from list.
+    """
+    response_headers = self.GetResponseHeadersDict()
+    if name not in response_headers:
+      return
+    new_value = []
+    for header_name in response_headers[name].split(','):
+      if header_name.strip().lower() not in directives_blacklist:
+        new_value.append(header_name)
+    if new_value:
+      self.SetResponseHeader(name, ','.join(new_value))
+    else:
+      self.DeleteResponseHeader(name)
+
   @classmethod
   def _ExtractUrl(cls, request_string):
     match = _PARSE_WPR_REQUEST_REGEX.match(request_string)
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
index f744c10..f4c2fe6 100644
--- a/loading/wpr_backend_unittest.py
+++ b/loading/wpr_backend_unittest.py
@@ -29,7 +29,7 @@ class WprUrlEntryTest(unittest.TestCase):
     wpr_response = MockWprResponse(headers)
     return WprUrlEntry('GET http://a.com/', wpr_response)
 
-  def test_ExtractUrl(self):
+  def testExtractUrl(self):
     self.assertEquals('http://aa.bb/c',
                       WprUrlEntry._ExtractUrl('GET http://aa.bb/c'))
     self.assertEquals('http://aa.b/c',
@@ -43,7 +43,7 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('http://aa.bb',
                       WprUrlEntry._ExtractUrl('GET http://aa.bb FOO BAR'))
 
-  def test_GetResponseHeadersDict(self):
+  def testGetResponseHeadersDict(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
                                      ('header0', 'value2'),
@@ -57,7 +57,7 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('value3', headers['header2'])
     self.assertEquals('VaLue4', headers['header3'])
 
-  def test_SetResponseHeader(self):
+  def testSetResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1')])
     entry.SetResponseHeader('new_header0', 'new_value0')
@@ -112,7 +112,7 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('header3', entry._wpr_response.headers[3][0])
     self.assertEquals('value4', entry._wpr_response.headers[3][1])
 
-  def test_DeleteResponseHeader(self):
+  def testDeleteResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
                                      ('header0', 'value2'),
@@ -132,6 +132,23 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertNotIn('header1', entry.GetResponseHeadersDict())
     self.assertEquals(2, len(entry.GetResponseHeadersDict()))
 
+  def testRemoveResponseHeaderDirectives(self):
+    entry = self._CreateWprUrlEntry([('hEAder0', 'keyWOrd0,KEYword1'),
+                                     ('heaDER1', 'value1'),
+                                     ('headeR2', 'value3')])
+    entry.RemoveResponseHeaderDirectives('header0', {'keyword1', 'keyword0'})
+    self.assertNotIn('header0', entry.GetResponseHeadersDict())
+
+    entry = self._CreateWprUrlEntry([('heADEr0', 'keYWOrd0'),
+                                     ('hEADERr1', 'value1'),
+                                     ('HEAder0', 'keywoRD1,keYwoRd2'),
+                                     ('hEADer2', 'value3')])
+    entry.RemoveResponseHeaderDirectives('header0', {'keyword1'})
+    self.assertEquals(
+        'keYWOrd0,keYwoRd2', entry.GetResponseHeadersDict()['header0'])
+    self.assertEquals(3, len(entry._wpr_response.headers))
+    self.assertEquals('keYWOrd0,keYwoRd2', entry._wpr_response.headers[0][1])
+
 
 class WprHostTest(unittest.TestCase):
   def setUp(self):

commit 03702ec1f703956a053b7423092e23200bec290f
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 4 11:46:45 2016 -0700

    sandwich: Log more effectively error in VerifyBenchmarkOutputDirectory()
    
    Before, sandwich_misc.VerifyBenchmarkOutputDirectory() was just
    comparing theoretical requests sets with in practice one. However
    there was some issues. As an example, the missing requests (may be
    because the URL as changed because of some JavaScript) were getting
    logged several times: list of all requests, list of {,un}cached
    requests. When URLs of requests are getting long (because of the
    JS that have generated them for instance), it is becoming difficult
    for the human eye to see if a such URL was missing from the expected
    {,un}cache resources because was missing from the all the requests.
    
    This CL fixes this issue by modifying the expected sets of url that
    were {,not} served from cache according to the expected and missing
    requests, so that a given miss-behaving URL gets logged at most once.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1949023004
    Cr-Original-Commit-Position: refs/heads/master@{#391583}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5f05514899b1a64200bb26fd23f04ef4b25f6f8a

diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index a99f973..3c7c740 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -191,7 +191,9 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
   # TODO(gabadie): What's the best way of propagating errors happening in here?
   benchmark_setup = json.load(open(benchmark_setup_path))
   cache_whitelist = set(benchmark_setup['cache_whitelist'])
-  url_resources = set(benchmark_setup['url_resources'])
+  original_requests = set(benchmark_setup['url_resources'])
+  original_cached_requests = original_requests.intersection(cache_whitelist)
+  original_uncached_requests = original_requests.difference(cache_whitelist)
   all_sent_url_requests = set()
 
   # Verify requests from traces.
@@ -207,16 +209,29 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
       continue
     trace = LoadingTrace.FromJsonFile(trace_path)
     logging.info('verifying %s from %s' % (trace.url, trace_path))
-    _PrintUrlSetComparison(url_resources,
-        ListUrlRequests(trace, RequestOutcome.All), 'All resources')
-    _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
-        ListUrlRequests(trace, RequestOutcome.ServedFromCache),
-        'Cached resources')
-    sent_url_requests = \
+
+    effective_requests = ListUrlRequests(trace, RequestOutcome.All)
+    effective_cached_requests = \
+        ListUrlRequests(trace, RequestOutcome.ServedFromCache)
+    effective_uncached_requests = \
         ListUrlRequests(trace, RequestOutcome.NotServedFromCache)
-    _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
-        sent_url_requests, 'Non cached resources')
-    all_sent_url_requests.update(sent_url_requests)
+
+    missing_requests = original_requests.difference(effective_requests)
+    unexpected_requests = effective_requests.difference(original_requests)
+    expected_cached_requests = \
+        original_cached_requests.difference(missing_requests)
+    missing_cached_requests = \
+        expected_cached_requests.difference(effective_cached_requests)
+    expected_uncached_requests = original_uncached_requests.union(
+        unexpected_requests).union(missing_cached_requests)
+    all_sent_url_requests.update(effective_uncached_requests)
+
+    _PrintUrlSetComparison(original_requests, effective_requests,
+                           'All resources')
+    _PrintUrlSetComparison(expected_cached_requests, effective_cached_requests,
+                           'Cached resources')
+    _PrintUrlSetComparison(expected_uncached_requests,
+                           effective_uncached_requests, 'Non cached resources')
 
   # Verify requests from WPR.
   wpr_log_path = os.path.join(
@@ -241,7 +256,7 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
   _PrintUrlSetComparison(set(), wpr_command_colliding_urls,
                          'Distinct resources colliding to WPR commands')
   _PrintUrlSetComparison(all_wpr_urls, all_sent_url_requests,
-                         'Distinct resources requests to WPR')
+                         'Distinct resource requests to WPR')
 
 
 def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 318f869..1d36c78 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -119,6 +119,7 @@ class SandwichTaskBuilder(task_manager.Builder):
       runner.wpr_archive_path = BuildPatchedWpr.path
       runner.cache_archive_path = BuildReferenceCache.path
       runner.cache_operation = 'save'
+      runner.trace_output_directory = BuildReferenceCache.path[:-4] + '-run'
       runner.Run()
 
     @self.RegisterTask('common/subresources-for-urls-run/',

commit 8d70cfe624fcf89969e22de7c2a5cd5a8a9cd4b5
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 4 10:55:24 2016 -0700

    tools/android/loading: Implements ChromeControllerError class
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1941743002
    Cr-Original-Commit-Position: refs/heads/master@{#391555}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8c50c742fe8f82a73b8488034b5cbf24833c6480

diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 07faf79..05e8498 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -248,13 +248,14 @@ class Worker(object):
           chrome_ctl.SetNetworkEmulation(emulate_network)
 
         # Record and write the trace.
-        with chrome_ctl.OpenWithRedirection(sys.stdout,
-                                            sys.stderr) as connection:
+        with chrome_ctl.Open() as connection:
           connection.ClearCache()
           trace = loading_trace.LoadingTrace.RecordUrlNavigation(
               url, connection, chrome_ctl.ChromeMetadata())
           trace_metadata['succeeded'] = True
           trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+      except controller.ChromeControllerError as e:
+        e.Dump(sys.stderr)
       except Exception as e:
         sys.stderr.write(str(e))
 
diff --git a/loading/controller.py b/loading/controller.py
index 6ff1389..8b36d2b 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -21,21 +21,70 @@ import subprocess
 import sys
 import tempfile
 import time
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
+import traceback
 
 import chrome_cache
 import common_util
 import device_setup
 import devtools_monitor
 import emulation
-import options
+from options import OPTIONS
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+_CATAPULT_DIR = os.path.join(_SRC_DIR, 'third_party', 'catapult')
 
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+sys.path.append(os.path.join(_CATAPULT_DIR, 'devil'))
 from devil.android.sdk import intent
 
-OPTIONS = options.OPTIONS
+sys.path.append(
+    os.path.join(_CATAPULT_DIR, 'telemetry', 'third_party', 'websocket-client'))
+import websocket
+
+
+class ChromeControllerInternalError(Exception):
+  pass
+
+
+class ChromeControllerError(Exception):
+  """Chrome error with detailed log.
+
+  Note:
+    Some of these errors might be known intermittent errors that can usually be
+    retried by the caller after re-doing any specific setup again.
+  """
+  _INTERMITTENT_WHITE_LIST = {websocket.WebSocketTimeoutException}
+
+  def __init__(self, log):
+    """Constructor
+
+    Args:
+      log: String containing the log of the running Chrome instance that was
+          running. It will be interleaved with xvfb with headless desktop or
+          interleaved with any other running Android package.
+    """
+    self.error_type, self.error_value, self.error_traceback = sys.exc_info()
+    super(ChromeControllerError, self).__init__(repr(self.error_value))
+    self.parent_stack = traceback.extract_stack()
+    self.log = log
+
+  def Dump(self, output):
+    """Dumps the entire error's infos into file-like object."""
+    output.write('-' * 60 + ' {}:\n'.format(self.__class__.__name__))
+    output.write(repr(self) + '\n')
+    output.write('{} is {}known as intermittent.\n'.format(
+        self.error_type.__name__, '' if self.IsIntermittent() else 'NOT '))
+    output.write(
+        '-' * 60 + ' {}\'s full traceback:\n'.format(self.error_type.__name__))
+    output.write(''.join(traceback.format_list(self.parent_stack)))
+    traceback.print_tb(self.error_traceback, file=output)
+    output.write('-' * 60 + ' Begin log\n')
+    output.write(self.log)
+    output.write('-' * 60 + ' End log\n')
+
+  def IsIntermittent(self):
+    """Returns whether the error is an known intermittent error."""
+    return self.error_type in self._INTERMITTENT_WHITE_LIST
 
 
 class ChromeControllerBase(object):
@@ -67,6 +116,10 @@ class ChromeControllerBase(object):
         # Tests & dev-tools related stuff.
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
+
+        # Detailed log.
+        '--enable-logging=stderr',
+        '--v=1',
     ]
     self._wpr_attributes = None
     self._metadata = {}
@@ -240,6 +293,7 @@ class RemoteChromeController(ChromeControllerBase):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
+      self._device.adb.Logcat(clear=True, dump=True)
       self._device.StartActivity(start_intent, blocking=True)
       try:
         for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
@@ -262,8 +316,12 @@ class RemoteChromeController(ChromeControllerBase):
               time.sleep(self.TIME_TO_IDLE_SECONDS)
             break
         else:
-          raise RuntimeError('Failed to connect to chrome devtools after {} '
-                             'attempts.'.format(attempt_id))
+          raise ChromeControllerInternalError(
+              'Failed to connect to Chrome devtools after {} '
+              'attempts.'.format(self.DEVTOOLS_CONNECTION_ATTEMPTS))
+      except:
+        logcat = ''.join([l + '\n' for l in self._device.adb.Logcat(dump=True)])
+        raise ChromeControllerError(log=logcat)
       finally:
         self._device.ForceStop(package_info.package)
 
@@ -319,47 +377,50 @@ class LocalChromeController(ChromeControllerBase):
     self._headless = headless
 
   @contextlib.contextmanager
-  def OpenWithRedirection(self, stdout, stderr):
-    """Override for connection context. stdout and stderr are passed to the
-       child processes used to run Chrome and XVFB."""
+  def Open(self):
+    """Overridden connection creation."""
     chrome_cmd = [OPTIONS.LocalBinary('chrome')]
     chrome_cmd.extend(self._GetChromeArguments())
     # Force use of simple cache.
     chrome_cmd.append('--use-simple-cache-backend=on')
     chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
-    chrome_cmd.extend(['--enable-logging=stderr', '--v=1'])
     # Navigates to about:blank for couples of reasons:
     #   - To find the correct target descriptor at devtool connection;
     #   - To avoid cache and WPR pollution by the NTP.
     chrome_cmd.append('about:blank')
 
-    chrome_env_override = {}
-    if self._wpr_attributes:
-      chrome_env_override.update(self._wpr_attributes.chrome_env_override)
-
-    if self._headless:
-      assert 'DISPLAY' not in chrome_env_override, \
-          'DISPLAY environment variable is reserved for headless.'
-      chrome_env_override['DISPLAY'] = 'localhost:99'
-      xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
-      logging.info(common_util.GetCommandLineForLogging(xvfb_cmd))
-      xvfb_process = subprocess.Popen(xvfb_cmd, stdout=stdout, stderr=stderr)
-
-    # Launch chrome.
-    logging.info(
-        common_util.GetCommandLineForLogging(chrome_cmd, chrome_env_override))
-    chrome_env = os.environ.copy()
-    chrome_env.update(chrome_env_override)
-    chrome_process = subprocess.Popen(chrome_cmd, stdout=stdout, stderr=stderr,
-                                      env=chrome_env)
-    connection = None
+    tmp_log = \
+        tempfile.NamedTemporaryFile(prefix="chrome_controller_", suffix='.log')
+    chrome_process = None
     try:
+      chrome_env_override = {}
+      if self._wpr_attributes:
+        chrome_env_override.update(self._wpr_attributes.chrome_env_override)
+
+      if self._headless:
+        assert 'DISPLAY' not in chrome_env_override, \
+            'DISPLAY environment variable is reserved for headless.'
+        chrome_env_override['DISPLAY'] = 'localhost:99'
+        xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
+        logging.info(common_util.GetCommandLineForLogging(xvfb_cmd))
+        xvfb_process = \
+            subprocess.Popen(xvfb_cmd, stdout=tmp_log.file, stderr=tmp_log.file)
+
+      chrome_env = os.environ.copy()
+      chrome_env.update(chrome_env_override)
+
+      # Launch Chrome.
+      logging.info(common_util.GetCommandLineForLogging(chrome_cmd,
+                                                        chrome_env_override))
+      chrome_process = subprocess.Popen(chrome_cmd, stdout=tmp_log.file,
+                                        stderr=tmp_log.file, env=chrome_env)
       # Attempt to connect to Chrome's devtools
       for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
         logging.info('Devtools connection attempt %d' % attempt_id)
         process_result = chrome_process.poll()
         if process_result is not None:
-          raise RuntimeError('Unexpected Chrome exit: %s', process_result)
+          raise ChromeControllerInternalError(
+              'Unexpected Chrome exit: {}'.format(process_result))
         try:
           connection = devtools_monitor.DevToolsConnection(
               OPTIONS.devtools_hostname, OPTIONS.devtools_port)
@@ -369,8 +430,9 @@ class LocalChromeController(ChromeControllerBase):
             raise
           time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
       else:
-        raise RuntimeError('Failed to connect to Chrome devtools after {} '
-                           'attempts.'.format(attempt_id))
+        raise ChromeControllerInternalError(
+            'Failed to connect to Chrome devtools after {} '
+            'attempts.'.format(self.DEVTOOLS_CONNECTION_ATTEMPTS))
       # Start and yield the devtool connection.
       self._StartConnection(connection)
       yield connection
@@ -378,19 +440,17 @@ class LocalChromeController(ChromeControllerBase):
         connection.Close()
         chrome_process.wait()
         chrome_process = None
+    except:
+      raise ChromeControllerError(log=open(tmp_log.name).read())
     finally:
+      if OPTIONS.local_noisy:
+        sys.stderr.write(open(tmp_log.name).read())
+      del tmp_log
       if chrome_process:
         chrome_process.kill()
       if self._headless:
         xvfb_process.kill()
 
-  def Open(self):
-    """Wrapper around the more-specialized version of Open() above that sets
-    the value of stdout/stderr based on the value of OPTIONS.local_noisy."""
-    stdout = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-    stderr = stdout
-    return self.OpenWithRedirection(stdout, stderr)
-
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
     self._EnsureProfileDirectory()
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index ead9bce..4952d10 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -379,7 +379,9 @@ class DevToolsConnection(object):
       if target_descriptor['type'] == 'page':
         self._target_descriptor = target_descriptor
         break
-    assert self._target_descriptor['url'] == 'about:blank'
+    if self._target_descriptor['url'] != 'about:blank':
+      raise DevToolsConnectionException(
+          'Looks like devtools connection was made to a different instance.')
     self._ws = inspector_websocket.InspectorWebsocket()
     self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'],
                      timeout=_WEBSOCKET_TIMEOUT_SECONDS)

commit 110689d593f74bf296535e57b98506903ecb6f8a
Author: gabadie <gabadie@chromium.org>
Date:   Wed May 4 08:06:43 2016 -0700

    tools/android/loading: Replace --local_binary by --local_build_dir in OPTIONS
    
    Before, we could only configures optionally the location of the
    local chrome binary with the --local_binary. But the cachetool or
    content_decoder_tool binaries path were set using an environment
    variable.
    
    This CL refactor these issue behind a replacing flag
    --local_build_dir to fix the inconsistency. One important
    difference is that this new flag doesn't have a default value to
    remove the out of the box magic that can sometimes cause issue if
    the program use a different binary from the one the command line
    expect.
    
    This CL also remove some analyze.py specific command line
    arguments from OPTIONS.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1941833002
    Cr-Original-Commit-Position: refs/heads/master@{#391510}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8ae350aa3a53e1eb62b7dddb63d66140caf46aa6

diff --git a/loading/analyze.py b/loading/analyze.py
index 82cc071..05b0a9d 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -288,6 +288,14 @@ COMMAND_MAP = {
 def main():
   logging.basicConfig(level=logging.WARNING)
   OPTIONS.AddGlobalArgument(
+      'clear_cache', True, 'clear browser cache before loading')
+  OPTIONS.AddGlobalArgument(
+      'emulate_device', '',
+      'Name of the device to emulate. Must be present '
+      'in --devices_file, or empty for no emulation.')
+  OPTIONS.AddGlobalArgument('emulate_network', '',
+      'Type of network emulation. Empty for no emulation.')
+  OPTIONS.AddGlobalArgument(
       'local', False,
       'run against local desktop chrome rather than device '
       '(see also --local_binary and local_profile_dir)')
diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 4c1f226..df8f955 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -31,17 +31,6 @@ OPTIONS = options.OPTIONS
 # Cache back-end types supported by cachetool.
 BACKEND_TYPES = ['simple']
 
-# Default build output directory.
-OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
-    os.path.dirname(__file__), '../../../out/Release'))
-
-# Default cachetool binary location.
-CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
-
-# Default content_decoder_tool binary location.
-CONTENT_DECODER_TOOL_BIN_PATH = os.path.join(OUT_DIRECTORY,
-                                             'content_decoder_tool')
-
 # Regex used to parse HTTP headers line by line.
 HEADER_PARSING_REGEX = re.compile(r'^(?P<header>\S+):(?P<value>.*)$')
 
@@ -247,22 +236,18 @@ class CacheBackend(object):
   """Takes care of reading and deleting cached keys.
   """
 
-  def __init__(self, cache_directory_path, cache_backend_type,
-               cachetool_bin_path=CACHETOOL_BIN_PATH):
+  def __init__(self, cache_directory_path, cache_backend_type):
     """Chrome cache back-end constructor.
 
     Args:
       cache_directory_path: The directory path where the cache is locally
         stored.
       cache_backend_type: A cache back-end type in BACKEND_TYPES.
-      cachetool_bin_path: Path of the cachetool binary.
     """
     assert os.path.isdir(cache_directory_path)
     assert cache_backend_type in BACKEND_TYPES
-    assert os.path.isfile(cachetool_bin_path), 'invalid ' + cachetool_bin_path
     self._cache_directory_path = cache_directory_path
     self._cache_backend_type = cache_backend_type
-    self._cachetool_bin_path = cachetool_bin_path
     # Make sure cache_directory_path is a valid cache.
     self._CachetoolCmd('validate')
 
@@ -308,7 +293,7 @@ class CacheBackend(object):
       Cachetool's stdout string.
     """
     editor_tool_cmd = [
-        self._cachetool_bin_path,
+        OPTIONS.LocalBinary('cachetool'),
         self._cache_directory_path,
         self._cache_backend_type,
         operation]
@@ -345,7 +330,7 @@ class CacheBackend(object):
     if content_encoding == None:
       return encoded_content
 
-    cmd = [CONTENT_DECODER_TOOL_BIN_PATH]
+    cmd = [OPTIONS.LocalBinary('content_decoder_tool')]
     cmd.extend([s.strip() for s in content_encoding.split(',')])
     process = subprocess.Popen(cmd,
                                stdin=subprocess.PIPE,
diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 49fc0ca..4e906cc 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -95,7 +95,7 @@ dictionary with the keys:
 -   `project_name` (string): Name of the Google Cloud project
 -   `cloud_storage_path` (string): Path in Google Storage where generated traces
     will be stored.
--   `chrome_path` (string): Path to the Chrome executable.
+-   `binaries_path` (string): Path to the executables (Containing chrome).
 -   `src_path` (string): Path to the Chromium source directory.
 -   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
     `clovis-queue`.
@@ -161,7 +161,7 @@ cat >$CONFIG_FILE << EOF
 {
   "project_name" : "$PROJECT_NAME",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "$CHROME_PATH",
+  "binaries_path" : "$BUILD_DIR",
   "src_path" : "$CHROMIUM_SRC",
   "taskqueue_tag" : "some-tag"
 }
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index 23cedb0..9660fc0 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -83,7 +83,7 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
   "instance_name" : "$INSTANCE_NAME",
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "/opt/app/clovis/binaries/chrome",
+  "binaries_path" : "/opt/app/clovis/binaries",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
   "worker_log_path" : "$WORKER_LOG_PATH",
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 038f251..07faf79 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -81,8 +81,7 @@ class Worker(object):
       self._UploadFailureDatabase()
 
     # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs([])
-    options.OPTIONS.local_binary = config['chrome_path']
+    options.OPTIONS.ParseArgs(['--local_build_dir', config['binaries_path']])
 
   def Start(self):
     """Main worker loop.
diff --git a/loading/controller.py b/loading/controller.py
index 8a2d4a6..6ff1389 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -322,7 +322,7 @@ class LocalChromeController(ChromeControllerBase):
   def OpenWithRedirection(self, stdout, stderr):
     """Override for connection context. stdout and stderr are passed to the
        child processes used to run Chrome and XVFB."""
-    chrome_cmd = [OPTIONS.local_binary]
+    chrome_cmd = [OPTIONS.LocalBinary('chrome')]
     chrome_cmd.extend(self._GetChromeArguments())
     # Force use of simple cache.
     chrome_cmd.append('--use-simple-cache-backend=on')
diff --git a/loading/options.py b/loading/options.py
index f7e8ee2..b7f03a2 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -18,9 +18,7 @@ class Options(object):
   be available as instance attributes (eg, OPTIONS.clear_cache).
   """
   # Tuples of (argument name, default value, help string).
-  _ARGS = [ ('clear_cache', True,
-             'clear browser cache before loading'),
-            ('clear_device_data', False,
+  _ARGS = [ ('clear_device_data', False,
              'Clear Chrome data from device before loading'),
             ('chrome_package_name', 'chrome',
              'build/android/pylib/constants package description'),
@@ -28,8 +26,8 @@ class Options(object):
              'hostname for devtools websocket connection'),
             ('devtools_port', 9222,
              'port for devtools websocket connection'),
-            ('local_binary', os.path.join(_SRC_DIR, 'out/Release/chrome'),
-             'chrome binary for local runs'),
+            ('local_build_dir', None,
+             'Build directory for local binary files such as chrome'),
             ('local_noisy', False,
              'Enable local chrome console output'),
             ('local_profile_dir', None,
@@ -40,11 +38,7 @@ class Options(object):
              'docs/linux_suid_sandbox_development.md)'),
             ('devices_file', _SRC_DIR + '/third_party/WebKit/Source/devtools'
              '/front_end/emulated_devices/module.json', 'File containing a'
-             ' list of emulated devices characteristics.'),
-            ('emulate_device', '', 'Name of the device to emulate. Must be '
-             'present in --devices_file, or empty for no emulation.'),
-            ('emulate_network', '', 'Type of network emulation. Empty for no'
-             ' emulation.')
+             ' list of emulated devices characteristics.')
           ]
 
 
@@ -170,4 +164,13 @@ class Options(object):
   def ChromePackage(self):
     return constants.PACKAGE_INFO[self.chrome_package_name]
 
+  def LocalBinary(self, binary_name):
+    """Get local binary path from its name."""
+    assert self.local_build_dir, '--local_build_dir needs to be set.'
+    path = os.path.join(self.local_build_dir, binary_name)
+    assert os.path.isfile(path), \
+        'Missing binary file {} (wrong --local_build_dir?).'.format(path)
+    return path
+
+
 OPTIONS = Options()

commit 55a41569c137c7476da8f7da79c23eb4414d4eb9
Author: droger <droger@chromium.org>
Date:   Wed May 4 06:39:01 2016 -0700

    tools/android/loading UI improvements for Clovis frontend
    
    This CL adds a menu at the top of the pages, a favicon, and some
    basic UI features such as a monospace font for logs.
    
    All pages can now inherit from base.html and share a basic
    template.
    
    Review-Url: https://codereview.chromium.org/1949543002
    Cr-Original-Commit-Position: refs/heads/master@{#391494}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fb948f8eedd6a59e656618fb622a7d2ebaafe7d6

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index b5379c6..33f14d9 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -29,9 +29,17 @@ instance_helper = common.google_instance_helper.GoogleInstanceHelper(
 app = flask.Flask(__name__)
 
 
-def Render(message, memory_logs):
-  return flask.render_template(
-      'log.html', body=message, log=memory_logs.Flush().split('\n'))
+def Render(message, memory_logs=None):
+  """Renders the log.html template.
+
+  Args:
+    message (str): Main content of the page.
+    memory_logs (MemoryLogs): Optional logs.
+  """
+  log = None
+  if memory_logs:
+    log = memory_logs.Flush().split('\n')
+  return flask.render_template('log.html', body=message, log=log)
 
 
 def PollWorkers(tag, start_time, timeout_hours, email_address, task_url):
@@ -243,8 +251,8 @@ def EnqueueTasks(tasks, task_tag):
 
 @app.route('/')
 def Root():
-  """Home page: redirect to the static form."""
-  return flask.redirect('/static/form.html')
+  """Home page: show the new task form."""
+  return flask.render_template('form.html')
 
 
 @app.route('/form_sent', methods=['POST'])
@@ -252,7 +260,7 @@ def StartFromForm():
   """HTML form endpoint."""
   data_stream = flask.request.files.get('json_task')
   if not data_stream:
-    return 'failed'
+    return Render('Failed, no content.')
   http_body_str = data_stream.read()
   return StartFromJsonString(http_body_str)
 
diff --git a/loading/cloud/frontend/static/base.css b/loading/cloud/frontend/static/base.css
new file mode 100644
index 0000000..3a5dfbf
--- /dev/null
+++ b/loading/cloud/frontend/static/base.css
@@ -0,0 +1,46 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file. */
+
+body {
+  font-family:'Arial', sans-serif;
+}
+
+.menu {
+  margin-bottom: 1em;
+  --padding: 14px 16px;
+  border: 1px solid #e7e7e7;
+}
+
+.menu div {
+  padding: var(--padding);
+  font-weight:bold;
+  color: #ffca00;
+  background-color: #cc0000;
+  cursor: default;
+}
+
+.menu ul {
+  list-style-type: none;
+  margin: 0;
+  padding: 0;
+  overflow: hidden;
+}
+
+.menu li {
+  float: left;
+  text-align: center;
+}
+
+.menu li a {
+  color: #666;
+  background-color: #f3f3f3;
+  text-decoration: none;
+  display: block;
+  padding: var(--padding);
+}
+
+.menu li a:hover {
+  background-color: #ddd;
+}
+
diff --git a/loading/cloud/frontend/static/crown_icon.png b/loading/cloud/frontend/static/crown_icon.png
new file mode 100644
index 0000000..2d5ac45
Binary files /dev/null and b/loading/cloud/frontend/static/crown_icon.png differ
diff --git a/loading/cloud/frontend/static/form.html b/loading/cloud/frontend/static/form.html
deleted file mode 100644
index 927cf2b..0000000
--- a/loading/cloud/frontend/static/form.html
+++ /dev/null
@@ -1,17 +0,0 @@
-<!DOCTYPE html>
-<html>
-
-<head>
-<meta charset="utf-8">
-<title>Submmit</title>
-</head>
-
-<body>
-<p> Select JSON file </p>
-<form action="/form_sent" method="POST" enctype="multipart/form-data">
-<input type="file" name="json_task"/>
-<input type="submit" name="submit" value="Upload"/>
-</form>
-</body>
-
-</html>
diff --git a/loading/cloud/frontend/templates/base.html b/loading/cloud/frontend/templates/base.html
new file mode 100644
index 0000000..611f2e7
--- /dev/null
+++ b/loading/cloud/frontend/templates/base.html
@@ -0,0 +1,34 @@
+{# Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file.
+#}
+
+<!DOCTYPE html>
+<html>
+
+<head>
+  <meta charset="utf-8">
+  <title>Clovis</title>
+  <link rel="stylesheet" type="text/css" href="/static/base.css"/>
+  <link rel="icon" href="/static/crown_icon.png"/>
+</head>
+
+<body>
+<header>
+  <div class="menu">
+    <ul style="border: 1px solid #e7e7e7; background-color: #f3f3f3;">
+      <li> <div> Clovis </div>
+      <li> <a href="/">New Task</a>
+      <li> <a href="https://chromium.googlesource.com/chromium/src/+/master/tools/android/loading/cloud/frontend/README.md">
+             Documentation
+           </a>
+    </ul>
+  </div>
+</header>
+
+{# The main content of the page goes here #}
+{% block content %}
+{% endblock %}
+
+</body>
+</html>
diff --git a/loading/cloud/frontend/templates/form.html b/loading/cloud/frontend/templates/form.html
new file mode 100644
index 0000000..9003259
--- /dev/null
+++ b/loading/cloud/frontend/templates/form.html
@@ -0,0 +1,15 @@
+{# Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file.
+#}
+
+{% extends "base.html" %}
+
+{% block content %}
+<h2>Submit New Task</h2>
+<p> Select JSON file </p>
+<form action="/form_sent" method="POST" enctype="multipart/form-data">
+  <input type="file" name="json_task"/>
+  <input type="submit" name="submit" value="Upload"/>
+</form>
+{% endblock %}
diff --git a/loading/cloud/frontend/templates/log.html b/loading/cloud/frontend/templates/log.html
index 4717896..d6b6dcf 100644
--- a/loading/cloud/frontend/templates/log.html
+++ b/loading/cloud/frontend/templates/log.html
@@ -1,14 +1,17 @@
+{# Copyright 2016 The Chromium Authors. All rights reserved.
+   Use of this source code is governed by a BSD-style license that can be
+   found in the LICENSE file.
+#}
+
 {# Template for a page displaying a body and logs (optional) under a collapsible
    section.
 #}
-<!DOCTYPE html>
-<html>
-<head>
-<meta charset="utf-8">
-<title>Clovis</title>
-</head>
-
-<body>
+{% extends "base.html" %}
+
+{% block content %}
+
+<h2> Task Creation Status </h2>
+
 {{ body }}
 
 {% if log %}
@@ -16,7 +19,9 @@
 <p><a onclick="javascript:ShowHide('HiddenDiv'); return false;" href="#">
   Show/hide details
 </a></p>
-<div id="HiddenDiv" style="display:none";>
+
+<div id="HiddenDiv"
+  style="display:none; font: 0.8em 'Droid Sans Mono', monospace;">
 
 {# Loop over the lines of the log to add linebreaks. #}
 {%- for line in log -%}
@@ -34,6 +39,4 @@ function ShowHide(divId) {
 
 {% endif %}
 
-</body>
-
-</html>
+{% endblock %}

commit d5b6e2272dafec4f84677537588f21e477735fb4
Author: gabadie <gabadie@chromium.org>
Date:   Tue May 3 13:18:47 2016 -0700

    tools/android/loading: Get protocol and status of redirected requests
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1923693004
    Cr-Original-Commit-Position: refs/heads/master@{#391340}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0268a16d1779e910d9dfef4869dc5a071efa1a5d

diff --git a/loading/request_track.py b/loading/request_track.py
index 8bd62da..8248af1 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -664,7 +664,8 @@ class RequestTrack(devtools_monitor.Track):
     _CopyFromDictToObject(redirect_response, r,
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
-                           ('fromDiskCache', 'from_disk_cache')))
+                           ('fromDiskCache', 'from_disk_cache'),
+                           ('protocol', 'protocol'), ('status', 'status')))
     r.timing = Timing.FromDevToolsDict(redirect_response['timing'])
 
     redirect_index = self._redirects_count_by_id[request_id]
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 9adb2e5..a99f973 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -75,6 +75,13 @@ def PatchWpr(wpr_archive_path):
   wpr_archive.Persist()
 
 
+def _FilterOutDataRequests(requests):
+  for request in filter(lambda r: not r.IsDataRequest(), requests):
+    if request.protocol not in {'http/0.9', 'http/1.0', 'http/1.1'}:
+      raise RuntimeError('Unknown request protocol {}'.format(request.protocol))
+    yield request
+
+
 def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   """Extracts discoverable resource urls from a loading trace according to a
   sub-resource discoverer.
@@ -117,16 +124,7 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   # Prune out data:// requests.
   whitelisted_urls = set()
   logging.info('white-listing %s' % first_resource_request.url)
-  for request in discovered_requests:
-    # Work-around where the protocol may be none for an unclear reason yet.
-    # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
-    #   this work-around.
-    if not request.protocol:
-      logging.warning('ignoring %s (no protocol)' % request.url)
-      continue
-    # Ignore data protocols.
-    if not request.protocol.startswith('http'):
-      continue
+  for request in _FilterOutDataRequests(discovered_requests):
     logging.info('white-listing %s' % request.url)
     whitelisted_urls.add(request.url)
   return whitelisted_urls
@@ -169,13 +167,7 @@ def ListUrlRequests(trace, request_kind):
     set([str])
   """
   urls = set()
-  for request_event in trace.request_track.GetEvents():
-    if request_event.protocol == None:
-      continue
-    if request_event.protocol.startswith('data'):
-      continue
-    if not request_event.protocol.startswith('http'):
-      raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
+  for request_event in _FilterOutDataRequests(trace.request_track.GetEvents()):
     if (request_kind == RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
@@ -278,9 +270,8 @@ def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
       continue
     logging.info('lists resources of %s from %s' % (trace.url, trace_path))
     urls_set = set()
-    for request_event in trace.request_track.GetEvents():
-      if not request_event.protocol.startswith('http'):
-        continue
+    for request_event in _FilterOutDataRequests(
+        trace.request_track.GetEvents()):
       if request_event.url not in urls_set:
         logging.info('  %s' % request_event.url)
         urls_set.add(request_event.url)

commit 9a0dba4dfefeb08ab049d19cf00ab06f88f2badb
Author: droger <droger@chromium.org>
Date:   Tue May 3 05:47:22 2016 -0700

    tools/android/loading Upload worker log
    
    This is the log from worker.py (including any crash stack
    traces).
    
    Add the -u option to python so that the output is not buffered,
    to avoid missing some parts of the log.
    
    Review-Url: https://codereview.chromium.org/1939033002
    Cr-Original-Commit-Position: refs/heads/master@{#391211}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fe8ef82212c043a0d73e5666e9002f591c32b27a

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 4e9248b..49fc0ca 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -101,6 +101,8 @@ dictionary with the keys:
     `clovis-queue`.
 -   `instance_name` (string, optional): Name of the Compute Engine instance this
     script is running on.
+-   `worker_log_file` (string, optional): Path to the log file capturing the
+    output of `worker.py`, to be uploaded to Cloud Storage.
 -   `self_destruct` (boolean, optional): Whether the worker will destroy the
     Compute Engine instance when there are no remaining tasks to process. This
     is only relevant when running in the cloud, and requires `instance_name` to
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index a1df3f3..23cedb0 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -65,7 +65,7 @@ cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
 chown root:root /usr/local/sbin/chrome-devel-sandbox
 chmod 4755 /usr/local/sbin/chrome-devel-sandbox
 
-# Make sure the pythonapp user owns the application code
+# Make sure the pythonapp user owns the application code.
 chown -R pythonapp:pythonapp /opt/app
 
 # Create the configuration file for this deployment.
@@ -76,6 +76,7 @@ if [ "$(get_instance_metadata self-destruct)" == "false" ]; then
 else
   SELF_DESTRUCT="True"
 fi
+WORKER_LOG_PATH=/opt/app/clovis/worker.log
 
 cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
@@ -85,6 +86,7 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
+  "worker_log_path" : "$WORKER_LOG_PATH",
   "self_destruct" : "$SELF_DESTRUCT"
 }
 EOF
@@ -101,7 +103,7 @@ fi
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
 directory=/opt/app/clovis/src/tools/android/loading/cloud/backend
-command=python worker.py --config $DEPLOYMENT_CONFIG_PATH
+command=python -u worker.py --config $DEPLOYMENT_CONFIG_PATH
 autostart=true
 autorestart=unexpected
 user=pythonapp
@@ -111,8 +113,8 @@ environment=VIRTUAL_ENV="/opt/app/clovis/env", \
     PATH="/opt/app/clovis/env/bin:/usr/bin", \
     HOME="/home/pythonapp",USER="pythonapp", \
     CHROME_DEVEL_SANDBOX="/usr/local/sbin/chrome-devel-sandbox"
-stdout_logfile=syslog
-stderr_logfile=syslog
+stdout_logfile=$WORKER_LOG_PATH
+stderr_logfile=$WORKER_LOG_PATH
 EOF
 
 supervisorctl reread
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 5b2ad60..038f251 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -35,6 +35,7 @@ class Worker(object):
     self._taskqueue_tag = config['taskqueue_tag']
     self._src_path = config['src_path']
     self._instance_name = config.get('instance_name')
+    self._worker_log_path = config.get('worker_log_path')
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
     self._self_destruct = config.get('self_destruct')
@@ -186,6 +187,14 @@ class Worker(object):
   def _Finalize(self):
     """Called before exiting."""
     self._logger.info('Done')
+    # Upload the worker log.
+    if self._worker_log_path:
+      self._logger.info('Uploading worker log.')
+      remote_log_path = os.path.join(self._base_path_in_bucket, 'worker_log')
+      if self._instance_name:
+        remote_log_path += '_' + self._instance_name
+      self._google_storage_accessor.UploadFile(self._worker_log_path,
+                                               remote_log_path)
     # Self destruct.
     if self._self_destruct:
       self._logger.info('Starting instance destruction: ' + self._instance_name)

commit a19fa11c045899c181e6d3f0ac04e0267947e441
Author: droger <droger@chromium.org>
Date:   Tue May 3 04:57:12 2016 -0700

    tools/android/loading Improve logging of failures
    
    Adds a failure database that keeps track of various failures:
    - trace generation failure,
    - TaskQueue failure,
    - worker.py starting up with an dirty state, most likely
      because it crashed and was restarted.
    
    The trace database is uploaded to Cloud Storage.
    
    As part of this CL, some code from _ProcessClovisTask() is
    split to a separate function _HandleTraceGenerationResults(),
    to make the code cleaner.
    
    The "autorestart" parameter for supervisor is changed
    to "unexpected" so that the worker script is restarted
    only if it crashed, and not if it exits cleanly.
    This avoids the worker to be restarted if there are no
    tasks to process, which has the side effect of triggering
    a "startup_with_dirty_state" error.
    
    The "destruct_instance_name" config field is split in
    "instance_name" (string) and "self_destruct" (bool).
    "trace_database_filename" is removed and is now inferred
    from the instance name.
    
    Review-Url: https://codereview.chromium.org/1930223003
    Cr-Original-Commit-Position: refs/heads/master@{#391203}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8cc5d09a2b6230b88b6e0a747f59e277a22ba145

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index e9e1a9a..4e9248b 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -99,12 +99,12 @@ dictionary with the keys:
 -   `src_path` (string): Path to the Chromium source directory.
 -   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
     `clovis-queue`.
--   `trace_database_filename` (string, optional): Filename for the trace
-    database in Cloud Storage. Must be unique per worker to avoid concurrent
-    access. Defaults to `trace_database.json`.
--   `destruct_instance_name` (string, optional): Name of the instance the worker
-    will destroy when there are no remaining tasks to process. This is only
-    relevant when running in the cloud.
+-   `instance_name` (string, optional): Name of the Compute Engine instance this
+    script is running on.
+-   `self_destruct` (boolean, optional): Whether the worker will destroy the
+    Compute Engine instance when there are no remaining tasks to process. This
+    is only relevant when running in the cloud, and requires `instance_name` to
+    be defined.
 
 ## Use the app
 
diff --git a/loading/cloud/backend/failure_database.py b/loading/cloud/backend/failure_database.py
new file mode 100644
index 0000000..4fdfde7
--- /dev/null
+++ b/loading/cloud/backend/failure_database.py
@@ -0,0 +1,39 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+
+class FailureDatabase(object):
+  """Logs the failures happening in the Clovis backend."""
+
+  def __init__(self, json_string=None):
+    """Loads a FailureDatabase from a string returned by ToJsonString()."""
+    if json_string:
+      self._failures_dict = json.loads(json_string)
+    else:
+      self._failures_dict = {}
+
+  def ToJsonDict(self):
+    """Returns a dict representing this instance."""
+    return self._failures_dict
+
+  def ToJsonString(self):
+    """Returns a string representing this instance."""
+    return json.dumps(self.ToJsonDict(), indent=2)
+
+  def AddFailure(self, failure_name, failure_content=None):
+    """Adds a failure with the given name and content. If the failure already
+    exists, it will increment the associated count.
+
+    Args:
+      failure_name (str): name of the failure.
+      failure_content (str): content of the failure (e.g. the URL or task that
+                             is failing).
+    """
+    content = failure_content if failure_content else 'error_count'
+    if failure_name not in self._failures_dict:
+      self._failures_dict[failure_name] = {}
+    error_count = self._failures_dict[failure_name].get(content, 0)
+    self._failures_dict[failure_name][content] = error_count + 1
+
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index 69cb7d9..a1df3f3 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -72,20 +72,20 @@ chown -R pythonapp:pythonapp /opt/app
 DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
 TASKQUEUE_TAG=`get_instance_metadata taskqueue-tag`
 if [ "$(get_instance_metadata self-destruct)" == "false" ]; then
-  SELF_DESTRUCT_CONFIG_LINE=""
+  SELF_DESTRUCT="False"
 else
-  SELF_DESTRUCT_CONFIG_LINE="\"destruct_instance_name\" : \"$INSTANCE_NAME\","
+  SELF_DESTRUCT="True"
 fi
 
 cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
-  $SELF_DESTRUCT_CONFIG_LINE
+  "instance_name" : "$INSTANCE_NAME",
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
-  "trace_database_filename" : "trace_database_${INSTANCE_NAME}.json"
+  "self_destruct" : "$SELF_DESTRUCT"
 }
 EOF
 
@@ -103,7 +103,7 @@ cat >/etc/supervisor/conf.d/python-app.conf << EOF
 directory=/opt/app/clovis/src/tools/android/loading/cloud/backend
 command=python worker.py --config $DEPLOYMENT_CONFIG_PATH
 autostart=true
-autorestart=true
+autorestart=unexpected
 user=pythonapp
 # Environment variables ensure that the application runs inside of the
 # configured virtualenv.
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index cef7044..5b2ad60 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -21,6 +21,7 @@ sys.path.insert(0,
 import controller
 from cloud.common.clovis_task import ClovisTask
 from cloud.common.google_instance_helper import GoogleInstanceHelper
+from failure_database import FailureDatabase
 from google_storage_accessor import GoogleStorageAccessor
 import loading_trace
 from loading_trace_database import LoadingTraceDatabase
@@ -33,9 +34,12 @@ class Worker(object):
     self._project_name = config['project_name']
     self._taskqueue_tag = config['taskqueue_tag']
     self._src_path = config['src_path']
-    self._destruct_instance_name = config.get('destruct_instance_name')
+    self._instance_name = config.get('instance_name')
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
+    self._self_destruct = config.get('self_destruct')
+    if self._self_destruct and not self._instance_name:
+      self._logger.error('Self destruction requires an instance name.')
 
     # Separate the cloud storage path into the bucket and the base path under
     # the bucket.
@@ -51,13 +55,29 @@ class Worker(object):
         credentials=self._credentials, project_name=self._project_name,
         bucket_name=self._bucket_name)
 
+    if self._instance_name:
+      trace_database_filename = 'trace_database_%s.json' % self._instance_name
+      failure_database_filename = \
+          'failure_database_%s.json' % self._instance_name
+    else:
+      trace_database_filename = 'trace_database.json'
+      failure_database_filename = 'failure_dabatase.json'
     self._traces_dir = os.path.join(self._base_path_in_bucket, 'traces')
-    self._trace_database_path = os.path.join(
-        self._traces_dir,
-        config.get('trace_database_filename', 'trace_database.json'))
+    self._trace_database_path = os.path.join(self._traces_dir,
+                                             trace_database_filename)
+    self._failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
+    self._failure_database_path = os.path.join(self._failures_dir,
+                                               failure_database_filename)
 
-    # Recover any existing trace database in case the worker died.
+    # Recover any existing trace database and failures in case the worker died.
     self._DownloadTraceDatabase()
+    self._DownloadFailureDatabase()
+
+    if self._trace_database.ToJsonDict() or self._failure_database.ToJsonDict():
+      # Script is restarting after a crash, or there are already files from a
+      # previous run in the directory.
+      self._failure_database.AddFailure('startup_with_dirty_state')
+      self._UploadFailureDatabase()
 
     # Initialize the global options that will be used during trace generation.
     options.OPTIONS.ParseArgs([])
@@ -96,16 +116,30 @@ class Worker(object):
     self._logger.info('Downloading trace database')
     trace_database_string = self._google_storage_accessor.DownloadAsString(
         self._trace_database_path) or '{}'
-    trace_database_dict = json.loads(trace_database_string)
-    self._trace_database = LoadingTraceDatabase(trace_database_dict)
+    self._trace_database = LoadingTraceDatabase.FromJsonString(
+        trace_database_string)
 
   def _UploadTraceDatabase(self):
     """Uploads the trace database to CloudStorage."""
     self._logger.info('Uploading trace database')
     self._google_storage_accessor.UploadString(
-        json.dumps(self._trace_database.ToJsonDict(), indent=2),
+        self._trace_database.ToJsonString(),
         self._trace_database_path)
 
+  def _DownloadFailureDatabase(self):
+    """Downloads the failure database from CloudStorage."""
+    self._logger.info('Downloading failure database')
+    failure_database_string = self._google_storage_accessor.DownloadAsString(
+        self._failure_database_path)
+    self._failure_database = FailureDatabase(failure_database_string)
+
+  def _UploadFailureDatabase(self):
+    """Uploads the failure database to CloudStorage."""
+    self._logger.info('Uploading failure database')
+    self._google_storage_accessor.UploadString(
+        self._failure_database.ToJsonString(),
+        self._failure_database_path)
+
   def _FetchClovisTask(self, project_name, task_api, queue_name):
     """Fetches a ClovisTask from the task queue.
 
@@ -132,27 +166,35 @@ class Worker(object):
     # once it is fixed.
     retry_count = google_task['retry_count']
     max_retry_count = 3
-    if retry_count >= max_retry_count:
+    skip_task = retry_count >= max_retry_count
+    if skip_task:
       task_api.tasks().delete(project=project_name, taskqueue=queue_name,
                               task=task_id).execute()
-      return self._FetchClovisTask(project_name, task_api, queue_name)
 
     clovis_task = ClovisTask.FromBase64(google_task['payloadBase64'])
+
+    if retry_count > 0:
+      self._failure_database.AddFailure('task_queue_retry',
+                                        clovis_task.ToJsonString())
+      self._UploadFailureDatabase()
+
+    if skip_task:
+      return self._FetchClovisTask(project_name, task_api, queue_name)
+
     return (clovis_task, task_id)
 
   def _Finalize(self):
     """Called before exiting."""
     self._logger.info('Done')
     # Self destruct.
-    if self._destruct_instance_name:
-      self._logger.info('Starting instance destruction: ' +
-                        self._destruct_instance_name)
+    if self._self_destruct:
+      self._logger.info('Starting instance destruction: ' + self._instance_name)
       google_instance_helper = GoogleInstanceHelper(
           self._credentials, self._project_name, self._logger)
-      success = google_instance_helper.DeleteInstance(
-          self._taskqueue_tag, self._destruct_instance_name)
+      success = google_instance_helper.DeleteInstance(self._taskqueue_tag,
+                                                      self._instance_name)
       if not success:
-        self._logger.error('Self destruction failed')
+        self._logger.error('Self destruction failed.')
     # Do not add anything after this line, as the instance might be killed at
     # any time.
 
@@ -217,6 +259,41 @@ class Worker(object):
 
     return trace_metadata
 
+  def _HandleTraceGenerationResults(self, local_filename, log_filename,
+                                    remote_filename, trace_metadata):
+    """Updates the trace database and the failure database after a trace
+    generation. Uploads the trace and the log.
+    Results related to successful traces are uploaded in the _traces_dir
+    directory, and failures are uploaded in the _failures_dir directory.
+
+    Args:
+      local_filename (str): Path to the local file containing the trace.
+      log_filename (str): Path to the local file containing the log.
+      remote_filename (str): Name of the target remote file where the trace and
+                             the log (with a .log extension added) are uploaded.
+      trace_metadata (dict): Metadata associated with the trace generation.
+    """
+    if trace_metadata['succeeded']:
+      remote_trace_location = os.path.join(self._traces_dir, remote_filename)
+      full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
+                                             remote_trace_location)
+      self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
+    else:
+      remote_trace_location = os.path.join(self._failures_dir, remote_filename)
+      self._failure_database.AddFailure('trace_collection',
+                                        trace_metadata['url'])
+
+    if os.path.isfile(local_filename):
+      self._logger.debug('Uploading: %s' % remote_trace_location)
+      self._google_storage_accessor.UploadFile(local_filename,
+                                               remote_trace_location)
+    else:
+      self._logger.warning('No trace found at: ' + local_filename)
+
+    self._logger.debug('Uploading analyze log')
+    remote_log_location = remote_trace_location + '.log'
+    self._google_storage_accessor.UploadFile(log_filename, remote_log_location)
+
   def _ProcessClovisTask(self, clovis_task):
     """Processes one clovis_task."""
     if clovis_task.Action() != 'trace':
@@ -230,39 +307,33 @@ class Worker(object):
     emulate_device = params.get('emulate_device')
     emulate_network = params.get('emulate_network')
 
-    failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
-    # TODO(blundell): Fix this up.
-    logs_dir = os.path.join(self._base_path_in_bucket, 'analyze_logs')
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
 
+    failure_happened = False
+    success_happened = False
+
     while len(urls) > 0:
       url = urls.pop()
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         self._logger.debug('Generating trace for URL: %s' % url)
-        remote_filename = os.path.join(local_filename, str(repeat))
         trace_metadata = self._GenerateTrace(
             url, emulate_device, emulate_network, local_filename, log_filename)
         if trace_metadata['succeeded']:
-          self._logger.debug('Uploading: %s' % remote_filename)
-          remote_trace_location = os.path.join(self._traces_dir,
-                                               remote_filename)
-          self._google_storage_accessor.UploadFile(local_filename,
-                                                   remote_trace_location)
-          full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
-                                                 remote_trace_location)
-          self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
+          success_happened = True
         else:
           self._logger.warning('Trace generation failed for URL: %s' % url)
-          if os.path.isfile(local_filename):
-            self._google_storage_accessor.UploadFile(
-                local_filename, os.path.join(failures_dir, remote_filename))
-        self._logger.debug('Uploading analyze log')
-        self._google_storage_accessor.UploadFile(
-            log_filename, os.path.join(logs_dir, remote_filename))
-    self._UploadTraceDatabase()
+          failure_happened = True
+        remote_filename = os.path.join(local_filename, str(repeat))
+        self._HandleTraceGenerationResults(
+            local_filename, log_filename, remote_filename, trace_metadata)
+
+    if success_happened:
+      self._UploadTraceDatabase()
+    if failure_happened:
+      self._UploadFailureDatabase()
 
 if __name__ == '__main__':
   parser = argparse.ArgumentParser(
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index dad6153..91705aa 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -31,6 +31,10 @@ class LoadingTraceDatabase(object):
     """Returns a dict representing this instance."""
     return self._traces_dict
 
+  def ToJsonString(self):
+    """Returns a string representing this instance."""
+    return json.dumps(self._traces_dict, indent=2)
+
   def ToJsonFile(self, json_path):
     """Saves a json file representing this instance."""
     json_dict = self.ToJsonDict()
@@ -43,6 +47,11 @@ class LoadingTraceDatabase(object):
     return LoadingTraceDatabase(json_dict)
 
   @classmethod
+  def FromJsonString(cls, json_string):
+    """Returns an instance from a string returned by ToJsonString()."""
+    return LoadingTraceDatabase(json.loads(json_string))
+
+  @classmethod
   def FromJsonFile(cls, json_path):
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:

commit 2b7f2680cbf25573526a620df590c58ced904fe3
Author: droger <droger@chromium.org>
Date:   Tue May 3 03:14:39 2016 -0700

    tools/android/loading Add missing argument to PollWorkers() call
    
    This CL fixes a crash happening because a PollWorkers() call was missing
    the timeout argument.
    
    Review-Url: https://codereview.chromium.org/1936073002
    Cr-Original-Commit-Position: refs/heads/master@{#391190}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1e591d0097ad9903666d75e182cbc29ea032274b

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index cb2f2c2..b5379c6 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -63,8 +63,8 @@ def PollWorkers(tag, start_time, timeout_hours, email_address, task_url):
   if live_instance_count > 0 or live_instance_count == -1:
     clovis_logger.info('Retry later, instances still alive for tag: ' + tag)
     poll_interval_minutes = 10
-    deferred.defer(PollWorkers, tag, start_time, email_address, task_url,
-                   _countdown=(60 * poll_interval_minutes))
+    deferred.defer(PollWorkers, tag, start_time, timeout_hours, email_address,
+                   task_url, _countdown=(60 * poll_interval_minutes))
     return
 
   Finalize(tag, email_address, 'SUCCESS', task_url)

commit 9b37e4422dc6c9eda2ffb31d3aeae9d38f3ec7de
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 2 12:28:23 2016 -0700

    sandwich: Add benchmark statistics into CSVs
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1939013002
    Cr-Original-Commit-Position: refs/heads/master@{#391024}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 269460a9ad0db2b669ee312cf220fb4e1849a2b0

diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 339cf72..b472ce6 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -8,6 +8,7 @@ python pull_sandwich_metrics.py -h
 """
 
 import collections
+import json
 import logging
 import os
 import shutil
@@ -27,12 +28,21 @@ from telemetry.util import rgba_color
 
 import loading_trace as loading_trace_module
 import sandwich_runner
+import sandwich_misc
 import tracing
 
 
 CSV_FIELD_NAMES = [
     'repeat_id',
     'url',
+    'subresource_discoverer',
+    'subresource_count',
+    # The amount of subresources detected at SetupBenchmark step.
+    'subresource_count_theoretic',
+    # Amount of subresources for caching as suggested by the subresource
+    # discoverer.
+    'cached_subresource_count_theoretic',
+    'cached_subresource_count',
     'total_load',
     'js_onload_event',
     'browser_malloc_avg',
@@ -182,6 +192,30 @@ def _ExtractMemoryMetrics(loading_trace):
   }
 
 
+def _ExtractBenchmarkStatistics(benchmark_setup, loading_trace):
+  """Extracts some useful statistics from a benchmark run.
+
+  Args:
+    benchmark_setup: benchmark_setup: dict representing the benchmark setup
+        JSON. The JSON format is according to:
+            SandwichTaskBuilder.PopulateLoadBenchmark.SetupBenchmark.
+    loading_trace: loading_trace_module.LoadingTrace.
+
+  Returns:
+    Dictionary with all extracted fields set.
+  """
+  return {
+    'subresource_discoverer': benchmark_setup['subresource_discoverer'],
+    'subresource_count': len(sandwich_misc.ListUrlRequests(
+        loading_trace, sandwich_misc.RequestOutcome.All)),
+    'subresource_count_theoretic': len(benchmark_setup['url_resources']),
+    'cached_subresource_count': len(sandwich_misc.ListUrlRequests(
+        loading_trace, sandwich_misc.RequestOutcome.ServedFromCache)),
+    'cached_subresource_count_theoretic':
+        len(benchmark_setup['cache_whitelist']),
+  }
+
+
 def _ExtractCompletenessRecordFromVideo(video_path):
   """Extracts the completeness record from a video.
 
@@ -241,10 +275,13 @@ def ComputeSpeedIndex(completeness_record):
   return speed_index
 
 
-def _ExtractMetricsFromRunDirectory(run_directory_path):
+def _ExtractMetricsFromRunDirectory(benchmark_setup, run_directory_path):
   """Extracts all the metrics from traces and video of a sandwich run.
 
   Args:
+    benchmark_setup: benchmark_setup: dict representing the benchmark setup
+        JSON. The JSON format is according to:
+            SandwichTaskBuilder.PopulateLoadBenchmark.SetupBenchmark.
     run_directory_path: Path of the run directory.
 
   Returns:
@@ -256,6 +293,8 @@ def _ExtractMetricsFromRunDirectory(run_directory_path):
   run_metrics = {'url': loading_trace.url}
   run_metrics.update(_ExtractDefaultMetrics(loading_trace))
   run_metrics.update(_ExtractMemoryMetrics(loading_trace))
+  run_metrics.update(
+      _ExtractBenchmarkStatistics(benchmark_setup, loading_trace))
   video_path = os.path.join(run_directory_path, 'video.mp4')
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
@@ -268,17 +307,20 @@ def _ExtractMetricsFromRunDirectory(run_directory_path):
   return run_metrics
 
 
-def ExtractMetricsFromRunnerOutputDirectory(output_directory_path):
+def ExtractMetricsFromRunnerOutputDirectory(benchmark_setup_path,
+                                            output_directory_path):
   """Extracts all the metrics from all the traces of a sandwich runner output
   directory.
 
   Args:
+    benchmark_setup_path: Path of the JSON of the benchmark setup.
     output_directory_path: The sandwich runner's output directory to extract the
         metrics from.
 
   Returns:
     List of dictionaries.
   """
+  benchmark_setup = json.load(open(benchmark_setup_path))
   assert os.path.isdir(output_directory_path)
   metrics = []
   for node_name in os.listdir(output_directory_path):
@@ -289,7 +331,8 @@ def ExtractMetricsFromRunnerOutputDirectory(output_directory_path):
     except ValueError:
       continue
     run_directory_path = os.path.join(output_directory_path, node_name)
-    run_metrics = _ExtractMetricsFromRunDirectory(run_directory_path)
+    run_metrics = _ExtractMetricsFromRunDirectory(
+        benchmark_setup, run_directory_path)
     run_metrics['repeat_id'] = repeat_id
     assert set(run_metrics.keys()) == set(CSV_FIELD_NAMES)
     metrics.append(run_metrics)
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index e7553bc..ea813e2 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -232,19 +232,6 @@ class PageTrackTest(unittest.TestCase):
     with self.assertRaises(ValueError):
       puller.ComputeSpeedIndex(completness_record)
 
-  def testCommandLine(self):
-    tmp_dir = tempfile.mkdtemp()
-    for dirname in ['1', '2', 'whatever']:
-      os.mkdir(os.path.join(tmp_dir, dirname))
-      LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
-          os.path.join(tmp_dir, dirname, sandwich_runner.TRACE_FILENAME))
-
-    process = subprocess.Popen(['python', puller.__file__, tmp_dir])
-    process.wait()
-    shutil.rmtree(tmp_dir)
-
-    self.assertEquals(0, process.returncode)
-
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 5ecc064..9adb2e5 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -154,16 +154,16 @@ def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
     logging.error('+     ' + url)
 
 
-class _RequestOutcome:
+class RequestOutcome:
   All, ServedFromCache, NotServedFromCache = range(3)
 
 
-def _ListUrlRequests(trace, request_kind):
+def ListUrlRequests(trace, request_kind):
   """Lists requested URLs from a trace.
 
   Args:
     trace: (LoadingTrace) loading trace.
-    request_kind: _RequestOutcome indicating the subset of requests to output.
+    request_kind: RequestOutcome indicating the subset of requests to output.
 
   Returns:
     set([str])
@@ -176,13 +176,13 @@ def _ListUrlRequests(trace, request_kind):
       continue
     if not request_event.protocol.startswith('http'):
       raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
-    if (request_kind == _RequestOutcome.ServedFromCache and
+    if (request_kind == RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
       urls.add(request_event.url)
-    elif (request_kind == _RequestOutcome.NotServedFromCache and
+    elif (request_kind == RequestOutcome.NotServedFromCache and
         not request_event.from_disk_cache):
       urls.add(request_event.url)
-    elif request_kind == _RequestOutcome.All:
+    elif request_kind == RequestOutcome.All:
       urls.add(request_event.url)
   return urls
 
@@ -216,12 +216,12 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
     trace = LoadingTrace.FromJsonFile(trace_path)
     logging.info('verifying %s from %s' % (trace.url, trace_path))
     _PrintUrlSetComparison(url_resources,
-        _ListUrlRequests(trace, _RequestOutcome.All), 'All resources')
+        ListUrlRequests(trace, RequestOutcome.All), 'All resources')
     _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
-        _ListUrlRequests(trace, _RequestOutcome.ServedFromCache),
+        ListUrlRequests(trace, RequestOutcome.ServedFromCache),
         'Cached resources')
     sent_url_requests = \
-        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache)
+        ListUrlRequests(trace, RequestOutcome.NotServedFromCache)
     _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
         sent_url_requests, 'Non cached resources')
     all_sent_url_requests.update(sent_url_requests)
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index ea763ec..318f869 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -201,6 +201,7 @@ class SandwichTaskBuilder(task_manager.Builder):
       with open(SetupBenchmark.path, 'w') as output:
         json.dump({
             'cache_whitelist': [url for url in whitelisted_urls],
+            'subresource_discoverer': subresource_discoverer,
             'url_resources': url_resources,
           }, output)
 
@@ -234,7 +235,7 @@ class SandwichTaskBuilder(task_manager.Builder):
           SetupBenchmark.path, RunBenchmark.path)
       trace_metrics_list = \
           sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
-              RunBenchmark.path)
+              SetupBenchmark.path, RunBenchmark.path)
       trace_metrics_list.sort(key=lambda e: e['repeat_id'])
       with open(ExtractMetrics.path, 'w') as csv_file:
         writer = csv.DictWriter(csv_file,

commit c58a3ccb2cf3c10a78283e9f968489850b00d69b
Author: gabadie <gabadie@chromium.org>
Date:   Mon May 2 08:36:59 2016 -0700

    tools/android/loading: make DevToolsConnection._HttpRequest() more reliable
    
    Sometimes, HTTPConnection.getresponse() can raise a BadStatusLine exception
    failing the connection to Chrome devtools. This CL add a retry mechanism in
    DevtoolsConnection._HttpRequest() to address this issue, increasing success
    rate of loading trace recording.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1941753002
    Cr-Original-Commit-Position: refs/heads/master@{#390946}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 243b15b6cad3b601e34bb6a6cb457f25a4ed602d

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index e8930e9..ead9bce 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -10,6 +10,7 @@ import json
 import logging
 import os
 import sys
+import time
 
 file_dir = os.path.dirname(__file__)
 sys.path.append(os.path.join(file_dir, '..', '..', 'perf'))
@@ -88,6 +89,8 @@ class DevToolsConnection(object):
   TRACING_DONE_EVENT = 'Tracing.tracingComplete'
   TRACING_STREAM_EVENT = 'Tracing.tracingComplete'  # Same as TRACING_DONE.
   TRACING_TIMEOUT = 300
+  HTTP_ATTEMPTS = 10
+  HTTP_ATTEMPT_INTERVAL_SECONDS = 0.1
 
   def __init__(self, hostname, port):
     """Initializes the connection with a DevTools server.
@@ -352,17 +355,22 @@ class DevToolsConnection(object):
 
   def _HttpRequest(self, path):
     assert path[0] == '/'
-    r = httplib.HTTPConnection(self._http_hostname, self._http_port)
-    try:
-      r.request('GET', '/json' + path)
-      response = r.getresponse()
-      if response.status != 200:
-        raise DevToolsConnectionException(
-            'Cannot connect to DevTools, reponse code %d' % response.status)
-      raw_response = response.read()
-    finally:
-      r.close()
-    return raw_response
+    for _ in xrange(self.HTTP_ATTEMPTS):
+      r = httplib.HTTPConnection(self._http_hostname, self._http_port)
+      try:
+        r.request('GET', '/json' + path)
+        response = r.getresponse()
+        if response.status != 200:
+          raise DevToolsConnectionException(
+              'Cannot connect to DevTools, reponse code %d' % response.status)
+        return response.read()
+      except httplib.BadStatusLine as exception:
+        logging.warning('Devtools HTTP connection failed: %s' % repr(exception))
+        time.sleep(self.HTTP_ATTEMPT_INTERVAL_SECONDS)
+      finally:
+        r.close()
+    # Raise the exception that has failed the last attempt.
+    raise
 
   def _Connect(self):
     assert not self._ws

commit 8877aa29ec94ac725a88653e304b9fa8b4b23980
Author: droger <droger@chromium.org>
Date:   Fri Apr 29 09:17:54 2016 -0700

    tools/android/loading Delete tasks that failed multiple times.
    
    The strategy for handling task failures was implemented using the task
    retry options.
    However, it seems that these don't work at all and the task is never
    actually removed from the queue.
    
    Google internal bug b/28442122 is tracking this.
    
    This CL implements the handling of the failed task in the backend
    worker instead, as a workaround.
    
    Review-Url: https://codereview.chromium.org/1931873002
    Cr-Original-Commit-Position: refs/heads/master@{#390664}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9db2c2e89cd4fa7d444f75e41b3e8fd2ab05d03e

diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index ff0d473..cef7044 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -8,7 +8,6 @@ import logging
 import os
 import re
 import sys
-import time
 
 from googleapiclient import discovery
 from oauth2client.client import GoogleCredentials
@@ -123,10 +122,21 @@ class Worker(object):
         project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=600,
         groupByTag=True, tag=self._taskqueue_tag).execute()
     if (not response.get('items')) or (len(response['items']) < 1):
-      return (None, None)
+      return (None, None)  # The task queue is empty.
 
     google_task = response['items'][0]
     task_id = google_task['id']
+
+    # Delete the task without processing if it already failed multiple times.
+    # TODO(droger): This is a workaround for internal bug b/28442122, revisit
+    # once it is fixed.
+    retry_count = google_task['retry_count']
+    max_retry_count = 3
+    if retry_count >= max_retry_count:
+      task_api.tasks().delete(project=project_name, taskqueue=queue_name,
+                              task=task_id).execute()
+      return self._FetchClovisTask(project_name, task_api, queue_name)
+
     clovis_task = ClovisTask.FromBase64(google_task['payloadBase64'])
     return (clovis_task, task_id)
 
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 42b7b09..cb2f2c2 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -217,7 +217,6 @@ def EnqueueTasks(tasks, task_tag):
   Google Compute Engine.
   """
   q = taskqueue.Queue('clovis-queue')
-  retry_options = taskqueue.TaskRetryOptions(task_retry_limit=3)
   # Add tasks to the queue by groups.
   # TODO(droger): This supports thousands of tasks, but maybe not millions.
   # Defer the enqueuing if it times out.
@@ -228,7 +227,7 @@ def EnqueueTasks(tasks, task_tag):
       group = tasks[i:i+group_size]
       taskqueue_tasks = [
           taskqueue.Task(payload=task.ToJsonString(), method='PULL',
-                         tag=task_tag, retry_options=retry_options)
+                         tag=task_tag)
           for task in group]
       rpc = taskqueue.create_rpc()
       q.add_async(task=taskqueue_tasks, rpc=rpc)

commit a48350800ef9604981b4feb68809b7848499affe
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 29 06:04:34 2016 -0700

    sandwich: List requests that couldn't be served by web page replay.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1922923004
    Cr-Original-Commit-Position: refs/heads/master@{#390634}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a181d8d4fb6ca04b9ed2707a67046f08618a1387

diff --git a/loading/options.py b/loading/options.py
index 3ea91b7..f7e8ee2 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -113,6 +113,7 @@ class Options(object):
     self._parsed_args = parsed_args
 
   def _MakeParser(self, description=None, extra=None, group=None):
+    self._arg_set = set()
     add_help = True if group is None else False
     parser = argparse.ArgumentParser(
         description=description, add_help=add_help)
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 5df6351..5ecc064 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -5,6 +5,7 @@
 import logging
 import json
 import os
+from urlparse import urlparse
 
 import chrome_cache
 import common_util
@@ -199,6 +200,7 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
   benchmark_setup = json.load(open(benchmark_setup_path))
   cache_whitelist = set(benchmark_setup['cache_whitelist'])
   url_resources = set(benchmark_setup['url_resources'])
+  all_sent_url_requests = set()
 
   # Verify requests from traces.
   run_id = -1
@@ -218,9 +220,36 @@ def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
     _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
         _ListUrlRequests(trace, _RequestOutcome.ServedFromCache),
         'Cached resources')
+    sent_url_requests = \
+        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache)
     _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
-        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache),
-        'Non cached resources')
+        sent_url_requests, 'Non cached resources')
+    all_sent_url_requests.update(sent_url_requests)
+
+  # Verify requests from WPR.
+  wpr_log_path = os.path.join(
+      benchmark_output_directory_path, sandwich_runner.WPR_LOG_FILENAME)
+  logging.info('verifying requests from %s' % wpr_log_path)
+  all_wpr_requests = wpr_backend.ExtractRequestsFromLog(wpr_log_path)
+  all_wpr_urls = set()
+  unserved_wpr_urls = set()
+  wpr_command_colliding_urls = set()
+
+  for request in all_wpr_requests:
+    if request.is_wpr_host:
+      continue
+    if urlparse(request.url).path.startswith('/web-page-replay'):
+      wpr_command_colliding_urls.add(request.url)
+    elif request.is_served is False:
+      unserved_wpr_urls.add(request.url)
+    all_wpr_urls.add(request.url)
+
+  _PrintUrlSetComparison(set(), unserved_wpr_urls,
+                         'Distinct unserved resources from WPR')
+  _PrintUrlSetComparison(set(), wpr_command_colliding_urls,
+                         'Distinct resources colliding to WPR commands')
+  _PrintUrlSetComparison(all_wpr_urls, all_sent_url_requests,
+                         'Distinct resources requests to WPR')
 
 
 def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 3dc0415..665824b 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -25,6 +25,7 @@ import loading_trace
 # Standard filenames in the sandwich runner's output directory.
 TRACE_FILENAME = 'trace.json'
 VIDEO_FILENAME = 'video.mp4'
+WPR_LOG_FILENAME = 'wpr.log'
 
 # Memory dump category used to get memory metrics.
 MEMORY_DUMP_CATEGORY = 'disabled-by-default-memory-infra'
@@ -264,12 +265,16 @@ class SandwichRunner(object):
       chrome_cache.UnzipDirectoryContent(
           self.cache_archive_path, self._local_cache_directory_path)
 
+    out_log_path = None
+    if self.trace_output_directory:
+      out_log_path = os.path.join(self.trace_output_directory, WPR_LOG_FILENAME)
+
     ran_urls = []
     with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
         record=self.wpr_record,
         network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
-        disable_script_injection=self.disable_wpr_script_injection
-        ):
+        disable_script_injection=self.disable_wpr_script_injection,
+        out_log_path=out_log_path):
       for _ in xrange(self.job_repeat):
         for url in self.urls:
           self._RunUrl(url, run_id=len(ran_urls))
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index d687b13..f4b232c 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -24,8 +24,7 @@ OPTIONS = options.OPTIONS
 
 class WebServerTestCase(unittest.TestCase):
   def setUp(self):
-    if not OPTIONS._parsed_args:
-      OPTIONS.ParseArgs('', extra=[('--noisy', False)])
+    OPTIONS.ParseArgs('', extra=[('--noisy', False)])
     self._temp_dir = tempfile.mkdtemp()
     self._server = webserver_test.WebServer(self._temp_dir, self._temp_dir)
 
diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
index e983572..a24d282 100644
--- a/loading/wpr_backend.py
+++ b/loading/wpr_backend.py
@@ -9,6 +9,7 @@ import collections
 import os
 import re
 import sys
+from urlparse import urlparse
 
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -23,6 +24,15 @@ import httparchive
 # Regex used to parse httparchive.py stdout's when listing all urls.
 _PARSE_WPR_REQUEST_REGEX = re.compile(r'^\S+\s+(?P<url>\S+)')
 
+# Regex used to extract WPR domain from WPR log.
+_PARSE_WPR_DOMAIN_REGEX = re.compile(r'^\(WARNING\)\s.*\sHTTP server started on'
+                                     r' (?P<netloc>\S+)\s*$')
+
+# Regex used to extract URLs requests from WPR log.
+_PARSE_WPR_URL_REGEX = re.compile(
+    r'^\((?P<level>\S+)\)\s.*\shttpproxy\..*\s(?P<method>[A-Z]+)\s+'
+    r'(?P<url>https?://[a-zA-Z0-9\-_:.]+/?\S*)\s.*$')
+
 
 class WprUrlEntry(object):
   """Wpr url entry holding request and response infos. """
@@ -114,6 +124,51 @@ class WprArchiveBackend(object):
     self._http_archive.Persist(self._wpr_archive_path)
 
 
+# WPR request seen by the WPR's HTTP proxy.
+#   is_served: Boolean whether WPR has found a matching resource in the archive.
+#   method: HTTP method of the request ['GET', 'POST' and so on...].
+#   url: The requested URL.
+#   is_wpr_host: Whether the requested url have WPR has an host such as:
+#     http://127.0.0.1:<WPR's HTTP listening port>/web-page-replay-command-exit
+WprRequest = collections.namedtuple('WprRequest',
+    ['is_served', 'method', 'url', 'is_wpr_host'])
+
+
+def ExtractRequestsFromLog(log_path):
+  """Extract list of requested handled by the WPR's HTTP proxy from a WPR log.
+
+  Args:
+    log_path: The path of the WPR log to parse.
+
+  Returns:
+    List of WprRequest.
+  """
+  requests = []
+  wpr_http_netloc = None
+  with open(log_path) as log_file:
+    for line in log_file.readlines():
+      # Extract WPR's HTTP proxy's listening network location.
+      match = _PARSE_WPR_DOMAIN_REGEX.match(line)
+      if match:
+        wpr_http_netloc = match.group('netloc')
+        assert wpr_http_netloc.startswith('127.0.0.1:')
+        continue
+      # Extract the WPR requested URLs.
+      match = _PARSE_WPR_URL_REGEX.match(line)
+      if match:
+        parsed_url = urlparse(match.group('url'))
+        # Ignore strange URL requests such as http://ousvtzkizg/
+        # TODO(gabadie): Find and terminate the location where they are queried.
+        if '.' not in parsed_url.netloc and ':' not in parsed_url.netloc:
+          continue
+        assert wpr_http_netloc
+        request = WprRequest(is_served=(match.group('level') == 'DEBUG'),
+            method=match.group('method'), url=match.group('url'),
+            is_wpr_host=parsed_url.netloc == wpr_http_netloc)
+        requests.append(request)
+  return requests
+
+
 if __name__ == '__main__':
   import argparse
   parser = argparse.ArgumentParser(description='Tests cache back-end.')
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
index 3cdf581..f744c10 100644
--- a/loading/wpr_backend_unittest.py
+++ b/loading/wpr_backend_unittest.py
@@ -2,9 +2,20 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import contextlib
+import httplib
+import os
+import shutil
+import tempfile
 import unittest
 
-from wpr_backend import WprUrlEntry
+from device_setup import _WprHost
+from options import OPTIONS
+from trace_test.webserver_test import WebServer
+from wpr_backend import WprUrlEntry, WprRequest, ExtractRequestsFromLog
+
+
+LOADING_DIR = os.path.dirname(__file__)
 
 
 class MockWprResponse(object):
@@ -122,5 +133,123 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals(2, len(entry.GetResponseHeadersDict()))
 
 
+class WprHostTest(unittest.TestCase):
+  def setUp(self):
+    OPTIONS.ParseArgs([])
+    self._server_address = None
+    self._wpr_http_port = None
+    self._tmp_directory = tempfile.mkdtemp(prefix='tmp_test_')
+
+  def tearDown(self):
+    shutil.rmtree(self._tmp_directory)
+
+  def _TmpPath(self, name):
+    return os.path.join(self._tmp_directory, name)
+
+  def _LogPath(self):
+    return self._TmpPath('wpr.log')
+
+  def _ArchivePath(self):
+    return self._TmpPath('wpr')
+
+  @contextlib.contextmanager
+  def RunWebServer(self):
+    assert self._server_address is None
+    with WebServer.Context(
+        source_dir=os.path.join(LOADING_DIR, 'trace_test', 'tests'),
+        communication_dir=self._tmp_directory) as server:
+      self._server_address = server.Address()
+      yield
+
+  @contextlib.contextmanager
+  def RunWpr(self, record):
+    assert self._server_address is not None
+    assert self._wpr_http_port is None
+    with _WprHost(self._ArchivePath(), record=record,
+                  out_log_path=self._LogPath()) as (http_port, https_port):
+      del https_port # unused
+      self._wpr_http_port = http_port
+      yield http_port
+
+  def DoHttpRequest(self, path, expected_status=200, destination='wpr'):
+    assert self._server_address is not None
+    if destination == 'wpr':
+      assert self._wpr_http_port is not None
+      connection = httplib.HTTPConnection('127.0.0.1', self._wpr_http_port)
+    elif destination == 'server':
+      connection = httplib.HTTPConnection(self._server_address)
+    else:
+      assert False
+    try:
+      connection.request(
+          "GET", '/' + path, headers={'Host': self._server_address})
+      response = connection.getresponse()
+    finally:
+      connection.close()
+    self.assertEquals(expected_status, response.status)
+
+  def _GenRawWprRequest(self, path):
+    assert self._wpr_http_port is not None
+    url = 'http://127.0.0.1:{}/web-page-replay-{}'.format(
+        self._wpr_http_port, path)
+    return WprRequest(is_served=True, method='GET', is_wpr_host=True, url=url)
+
+  def GenRawRequest(self, path, is_served):
+    assert self._server_address is not None
+    return WprRequest(is_served=is_served, method='GET', is_wpr_host=False,
+        url='http://{}/{}'.format(self._server_address, path))
+
+  def AssertWprParsedRequests(self, ref_requests):
+    all_ref_requests = []
+    all_ref_requests.append(self._GenRawWprRequest('generate-200'))
+    all_ref_requests.extend(ref_requests)
+    all_ref_requests.append(self._GenRawWprRequest('generate-200'))
+    all_ref_requests.append(self._GenRawWprRequest('command-exit'))
+    requests = ExtractRequestsFromLog(self._LogPath())
+    self.assertEquals(all_ref_requests, requests)
+    self._wpr_http_port = None
+
+  def testExtractRequestsFromLog(self):
+    with self.RunWebServer():
+      with self.RunWpr(record=True):
+        self.DoHttpRequest('1.html')
+        self.DoHttpRequest('2.html')
+        ref_requests = [
+            self.GenRawRequest('1.html', is_served=True),
+            self.GenRawRequest('2.html', is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+    with self.RunWpr(record=False):
+      self.DoHttpRequest('2.html')
+      self.DoHttpRequest('1.html')
+      ref_requests = [
+          self.GenRawRequest('2.html', is_served=True),
+          self.GenRawRequest('1.html', is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+  def testExtractRequestsFromLogHaveCorrectIsServed(self):
+    with self.RunWebServer():
+      with self.RunWpr(record=True):
+        self.DoHttpRequest('4.html', expected_status=404)
+        ref_requests = [self.GenRawRequest('4.html', is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+    with self.RunWpr(record=False):
+      self.DoHttpRequest('4.html', expected_status=404)
+      self.DoHttpRequest('5.html', expected_status=404)
+      ref_requests = [self.GenRawRequest('4.html', is_served=True),
+                      self.GenRawRequest('5.html', is_served=False)]
+    self.AssertWprParsedRequests(ref_requests)
+
+  def testExtractRequestsFromLogHaveCorrectIsWprHost(self):
+    PATH = 'web-page-replay-generate-200'
+    with self.RunWebServer():
+      self.DoHttpRequest(PATH, expected_status=404, destination='server')
+      with self.RunWpr(record=True):
+        self.DoHttpRequest(PATH)
+      ref_requests = [self.GenRawRequest(PATH, is_served=True)]
+    self.AssertWprParsedRequests(ref_requests)
+
+
 if __name__ == '__main__':
   unittest.main()

commit 3ab408b5262299d9a8cf4d3f9a1c515885d2262f
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 29 04:11:29 2016 -0700

    sandwich: Remove deprecated sub-command and rename `run-all` to `run`
    
    Sandwich has been migrated to the task manager that is going to let
    us do more complex operations than the deprecated sub-commands that
    required to be executed in a sequence of different processes.
    
    This CL simply remove the old sub-commands to reduce test coverage
    and release us from non necessary constrains on further sandwich
    improvments.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1930023002
    Cr-Original-Commit-Position: refs/heads/master@{#390616}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bc8010bd66683defcad902d7b51868ed1c16538c

diff --git a/loading/sandwich.py b/loading/sandwich.py
index d7ec1a1..0b72c73 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -50,16 +50,14 @@ _MEMORY_MEASUREMENT = 'memory'
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
-  # Command parser when dealing with jobs.
-  common_job_parser = argparse.ArgumentParser(add_help=False)
-  common_job_parser.add_argument('--job', required=True,
-                                 help='JSON file with job description.')
-  common_job_parser.add_argument('--android', default=None, type=str,
-                                 dest='android_device_serial',
-                                 help='Android device\'s serial to use.')
-
   task_parser = task_manager.CommandLineParser()
 
+  # Command parser when dealing with SandwichRunner.
+  sandwich_runner_parser = argparse.ArgumentParser(add_help=False)
+  sandwich_runner_parser.add_argument('--android', default=None, type=str,
+                                      dest='android_device_serial',
+                                      help='Android device\'s serial to use.')
+
   # Plumbing parser to configure OPTIONS.
   plumbing_parser = OPTIONS.GetParentParser('plumbing options')
 
@@ -67,109 +65,12 @@ def _ArgumentParser():
   parser = argparse.ArgumentParser(parents=[plumbing_parser])
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
-  # Record WPR subcommand.
-  record_wpr = subparsers.add_parser('record-wpr', parents=[common_job_parser],
-                                     help='Record WPR from sandwich job.')
-  record_wpr.add_argument('--wpr-archive', required=True, type=str,
-                          dest='wpr_archive_path',
-                          help='Web page replay archive to generate.')
-
-  # Patch WPR subcommand.
-  patch_wpr = subparsers.add_parser('patch-wpr',
-                                     help='Patch WPR response headers.')
-  patch_wpr.add_argument('--wpr-archive', required=True, type=str,
-                         dest='wpr_archive_path',
-                         help='Web page replay archive to patch.')
-
-  # Create cache subcommand.
-  create_cache_parser = subparsers.add_parser('create-cache',
-      parents=[common_job_parser],
-      help='Create cache from sandwich job.')
-  create_cache_parser.add_argument('--cache-archive', required=True, type=str,
-                                   dest='cache_archive_path',
-                                   help='Cache archive destination path.')
-  create_cache_parser.add_argument('--wpr-archive', default=None, type=str,
-                                   dest='wpr_archive_path',
-                                   help='Web page replay archive to create ' +
-                                       'the cache from.')
-
-  # Run subcommand.
-  run_parser = subparsers.add_parser('run', parents=[common_job_parser],
-                                     help='Run sandwich benchmark.')
-  run_parser.add_argument('--output', required=True, type=str,
-                          dest='trace_output_directory',
-                          help='Path of output directory to create.')
-  run_parser.add_argument('--cache-archive', type=str,
-                          dest='cache_archive_path',
-                          help='Cache archive destination path.')
-  run_parser.add_argument('--cache-op',
-                          choices=['clear', 'push', 'reload'],
-                          dest='cache_operation',
-                          default='clear',
-                          help='Configures cache operation to do before '
-                              +'launching Chrome. (Default is clear). The push'
-                              +' cache operation requires --cache-archive to '
-                              +'set.')
-  run_parser.add_argument('--disable-wpr-script-injection',
-                          action='store_true',
-                          help='Disable WPR default script injection such as ' +
-                              'overriding javascript\'s Math.random() and ' +
-                              'Date() with deterministic implementations.')
-  run_parser.add_argument('--network-condition', default=None,
-      choices=sorted(emulation.NETWORK_CONDITIONS.keys()),
-      help='Set a network profile.')
-  run_parser.add_argument('--network-emulator', default='browser',
-      choices=['browser', 'wpr'],
-      help='Set which component is emulating the network condition.' +
-          ' (Default to browser). Wpr network emulator requires --wpr-archive' +
-          ' to be set.')
-  run_parser.add_argument('--job-repeat', default=1, type=int,
-                          help='How many times to run the job.')
-  run_parser.add_argument('--record-video', action='store_true',
-                          help='Configures either to record or not a video of '
-                              +'chrome loading the web pages.')
-  run_parser.add_argument('--wpr-archive', default=None, type=str,
-                          dest='wpr_archive_path',
-                          help='Web page replay archive to load job\'s urls ' +
-                              'from.')
-
-  # Pull metrics subcommand.
-  create_cache_parser = subparsers.add_parser('extract-metrics',
-      help='Extracts metrics from a loading trace and saves as CSV.')
-  create_cache_parser.add_argument('--trace-directory', required=True,
-                                   dest='trace_output_directory', type=str,
-                                   help='Path of loading traces directory.')
-  create_cache_parser.add_argument('--out-metrics', default=None, type=str,
-                                   dest='metrics_csv_path',
-                                   help='Path where to save the metrics\'s '+
-                                      'CSV.')
-
-  # Filter cache subcommand.
-  filter_cache_parser = subparsers.add_parser('filter-cache',
-      help='Cache filtering that keeps only resources discoverable by the HTML'+
-          ' document parser.')
-  filter_cache_parser.add_argument('--cache-archive', type=str, required=True,
-                                   dest='cache_archive_path',
-                                   help='Path of the cache archive to filter.')
-  filter_cache_parser.add_argument('--subresource-discoverer', required=True,
-      help='Strategy for populating the cache with a subset of resources, '
-           'according to the way they can be discovered',
-      choices=sandwich_misc.SUBRESOURCE_DISCOVERERS)
-  filter_cache_parser.add_argument('--output', type=str, required=True,
-                                   dest='output_cache_archive_path',
-                                   help='Path of filtered cache archive.')
-  filter_cache_parser.add_argument('loading_trace_paths', type=str, nargs='+',
-      metavar='LOADING_TRACE',
-      help='A list of loading traces generated by a sandwich run for a given' +
-          ' url. This is used to have a resource dependency graph to white-' +
-          'list the ones discoverable by the HTML pre-scanner for that given ' +
-          'url.')
-
   # Record test trace subcommand.
   record_trace_parser = subparsers.add_parser('record-test-trace',
+      parents=[sandwich_runner_parser],
       help='Record a test trace using the trace_test.webserver_test.')
   record_trace_parser.add_argument('--source-dir', type=str, required=True,
-                                   help='Base path where the files are opened'
+                                   help='Base path where the files are opened '
                                         'by the web server.')
   record_trace_parser.add_argument('--page', type=str, required=True,
                                    help='Source page in source-dir to navigate '
@@ -177,93 +78,38 @@ def _ArgumentParser():
   record_trace_parser.add_argument('-o', '--output', type=str, required=True,
                                    help='Output path of the generated trace.')
 
-  # Run all subcommand.
-  run_all = subparsers.add_parser('run-all',
-                       parents=[common_job_parser, task_parser],
-                       help='Run all steps using the task manager '
-                            'infrastructure.')
-  run_all.add_argument('-m', '--measure', default=[], dest='optional_measures',
-                       choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
-                       nargs='+', help='Enable optional measurements.')
-  run_all.add_argument('-g', '--gen-full', action='store_true',
-                       help='Generate the full graph with all possible'
-                            'benchmarks.')
-  run_all.add_argument('--wpr-archive', default=None, type=str,
-                       dest='wpr_archive_path',
-                       help='WebPageReplay archive to use, instead of '
-                            'generating one.')
-  run_all.add_argument('--url-repeat', default=1, type=int,
-                       help='How many times to repeat the urls.')
+  # Run subcommand.
+  run_parser = subparsers.add_parser('run',
+      parents=[sandwich_runner_parser, task_parser],
+      help='Run all steps using the task manager infrastructure.')
+  run_parser.add_argument('-g', '--gen-full', action='store_true',
+                          help='Generate the full graph with all possible '
+                               'benchmarks.')
+  run_parser.add_argument('--job', required=True,
+      help='JSON file with job description such as in sandwich_jobs/.')
+  run_parser.add_argument('-m', '--measure', default=[], nargs='+',
+      choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
+      dest='optional_measures', help='Enable optional measurements.')
+  run_parser.add_argument('--wpr-archive', default=None, type=str,
+                          dest='wpr_archive_path',
+                          help='WebPageReplay archive to use, instead of '
+                               'generating one.')
+  run_parser.add_argument('--url-repeat', default=1, type=int,
+                          help='How many times to repeat the urls.')
 
   return parser
 
 
-def _CreateSandwichRunner(args):
-  sandwich_runner = SandwichRunner()
-  sandwich_runner.LoadJob(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
-  if args.android_device_serial is not None:
-    sandwich_runner.android_device = \
-        device_setup.GetDeviceFromSerial(args.android_device_serial)
-  return sandwich_runner
-
-
-def _RecordWprMain(args):
-  sandwich_runner = _CreateSandwichRunner(args)
-  sandwich_runner.wpr_record = True
-  sandwich_runner.PrintConfig()
-  if not os.path.isdir(os.path.dirname(args.wpr_archive_path)):
-    os.makedirs(os.path.dirname(args.wpr_archive_path))
-  sandwich_runner.Run()
-  return 0
-
-
-def _CreateCacheMain(args):
-  sandwich_runner = _CreateSandwichRunner(args)
-  sandwich_runner.cache_operation = 'save'
-  sandwich_runner.PrintConfig()
-  if not os.path.isdir(os.path.dirname(args.cache_archive_path)):
-    os.makedirs(os.path.dirname(args.cache_archive_path))
-  sandwich_runner.Run()
-  return 0
-
-
-def _RunJobMain(args):
-  sandwich_runner = _CreateSandwichRunner(args)
-  sandwich_runner.PrintConfig()
-  sandwich_runner.Run()
-  return 0
-
-
-def _ExtractMetricsMain(args):
-  run_metrics_list = sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
-      args.trace_output_directory)
-  run_metrics_list.sort(key=lambda e: e['repeat_id'])
-  with open(args.metrics_csv_path, 'w') as csv_file:
-    writer = csv.DictWriter(csv_file,
-                            fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
-    writer.writeheader()
-    for run_metrics in run_metrics_list:
-      writer.writerow(run_metrics)
-  return 0
-
-
-def _FilterCacheMain(args):
-  whitelisted_urls = set()
-  for loading_trace_path in args.loading_trace_paths:
-    whitelisted_urls.update(sandwich_misc.ExtractDiscoverableUrls(
-        loading_trace_path, args.subresource_discoverer))
-  if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
-    os.makedirs(os.path.dirname(args.output_cache_archive_path))
-  chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
-                                               whitelisted_urls,
-                                               args.output_cache_archive_path)
-  return 0
+def _GetAndroidDeviceFromArgs(args):
+  if args.android_device_serial:
+    return device_setup.GetDeviceFromSerial(args.android_device_serial)
+  return None
 
 
 def _RecordWebServerTestTrace(args):
   with common_util.TemporaryDirectory() as out_path:
     sandwich_runner = SandwichRunner()
+    sandwich_runner.android_device = _GetAndroidDeviceFromArgs(args)
     # Reuse the WPR's forwarding to access the webpage from Android.
     sandwich_runner.wpr_record = True
     sandwich_runner.wpr_archive_path = os.path.join(out_path, 'wpr')
@@ -280,13 +126,9 @@ def _RecordWebServerTestTrace(args):
 
 
 def _RunAllMain(args):
-  android_device = None
-  if args.android_device_serial:
-    android_device = \
-        device_setup.GetDeviceFromSerial(args.android_device_serial)
   builder = sandwich_task_builder.SandwichTaskBuilder(
       output_directory=args.output,
-      android_device=android_device,
+      android_device=_GetAndroidDeviceFromArgs(args),
       job_path=args.job)
   if args.wpr_archive_path:
     builder.OverridePathToWprArchive(args.wpr_archive_path)
@@ -330,22 +172,9 @@ def main(command_line_args):
   args = _ArgumentParser().parse_args(command_line_args)
   OPTIONS.SetParsedArgs(args)
 
-  if args.subcommand == 'record-wpr':
-    return _RecordWprMain(args)
-  if args.subcommand == 'patch-wpr':
-    sandwich_misc.PatchWpr(args.wpr_archive_path)
-    return 0
-  if args.subcommand == 'create-cache':
-    return _CreateCacheMain(args)
-  if args.subcommand == 'run':
-    return _RunJobMain(args)
-  if args.subcommand == 'extract-metrics':
-    return _ExtractMetricsMain(args)
-  if args.subcommand == 'filter-cache':
-    return _FilterCacheMain(args)
   if args.subcommand == 'record-test-trace':
     return _RecordWebServerTestTrace(args)
-  if args.subcommand == 'run-all':
+  if args.subcommand == 'run':
     return _RunAllMain(args)
   assert False
 

commit ba3731d5f642416a1191cb0e2860606e4aa1a853
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 28 10:02:26 2016 -0700

    sandwich: Make speed-index and memory measurement optional from run-all
    
    Before, the run-all sub-command was not recording the speed-index
    video, and the memory measurements were always done using the memory
    dump trace events.
    
    Memory dump trace event are large and may impact loading performance
    on low-end devices. This CL makes memory measurement optional and
    also add the option to measure speed-index in the new command line
    flag --measure.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1925803003
    Cr-Original-Commit-Position: refs/heads/master@{#390407}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 413a55d3b89527353ae88aa8f9881b791379f652

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 6dda5d0..d7ec1a1 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -44,6 +44,9 @@ from trace_test.webserver_test import WebServer
 # Use options layer to access constants.
 OPTIONS = options.OPTIONS
 
+_SPEED_INDEX_MEASUREMENT = 'speed-index'
+_MEMORY_MEASUREMENT = 'memory'
+
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
@@ -179,6 +182,9 @@ def _ArgumentParser():
                        parents=[common_job_parser, task_parser],
                        help='Run all steps using the task manager '
                             'infrastructure.')
+  run_all.add_argument('-m', '--measure', default=[], dest='optional_measures',
+                       choices=[_SPEED_INDEX_MEASUREMENT, _MEMORY_MEASUREMENT],
+                       nargs='+', help='Enable optional measurements.')
   run_all.add_argument('-g', '--gen-full', action='store_true',
                        help='Generate the full graph with all possible'
                             'benchmarks.')
@@ -281,31 +287,37 @@ def _RunAllMain(args):
   builder = sandwich_task_builder.SandwichTaskBuilder(
       output_directory=args.output,
       android_device=android_device,
-      job_path=args.job,
-      url_repeat=args.url_repeat)
+      job_path=args.job)
   if args.wpr_archive_path:
     builder.OverridePathToWprArchive(args.wpr_archive_path)
   else:
     builder.PopulateWprRecordingTask()
   builder.PopulateCommonPipelines()
 
-  runner_transformer_name = 'no-network-emulation'
-  runner_transformer = lambda arg: None
+  def MainTransformer(runner):
+    runner.record_video = _SPEED_INDEX_MEASUREMENT in args.optional_measures
+    runner.record_memory_dumps = _MEMORY_MEASUREMENT in args.optional_measures
+    runner.job_repeat = args.url_repeat
+
+  transformer_list_name = 'no-network-emulation'
   builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
-                                runner_transformer_name, runner_transformer)
+                                transformer_list_name,
+                                transformer_list=[MainTransformer])
   builder.PopulateLoadBenchmark(sandwich_misc.FULL_CACHE_DISCOVERER,
-                                runner_transformer_name, runner_transformer)
+                                transformer_list_name,
+                                transformer_list=[MainTransformer])
 
   if args.gen_full:
-    for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
-      if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
-        continue
-      for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
-        runner_transformer_name = network_condition.lower()
-        runner_transformer = sandwich_task_builder.NetworkSimulationTransformer(
-            network_condition)
-        builder.PopulateLoadBenchmark(
-            subresource_discoverer, runner_transformer_name, runner_transformer)
+    for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
+      transformer_list_name = network_condition.lower()
+      network_transformer = \
+          sandwich_task_builder.NetworkSimulationTransformer(network_condition)
+      transformer_list = [MainTransformer, network_transformer]
+      for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
+        if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
+          continue
+        builder.PopulateLoadBenchmark(subresource_discoverer,
+            transformer_list_name, transformer_list)
 
   return task_manager.ExecuteWithCommandLine(
       args, builder.tasks.values(), builder.default_final_tasks)
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 1324b75..339cf72 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -43,6 +43,8 @@ CSV_FIELD_NAMES = [
     'net_emul.upload',
     'net_emul.latency']
 
+_UNAVAILABLE_CSV_VALUE = 'unavailable'
+
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
 # Points of a completeness record.
@@ -81,6 +83,7 @@ def _GetBrowserDumpEvents(tracing_track):
   Returns:
     List of memory dump events.
   """
+  assert sandwich_runner.MEMORY_DUMP_CATEGORY in tracing_track.Categories()
   browser_pid = _GetBrowserPID(tracing_track)
   browser_dumps_events = []
   for event in tracing_track.GetEvents():
@@ -130,8 +133,8 @@ def _GetWebPageTrackedEvents(tracing_track):
   return tracked_events
 
 
-def _ExtractMetricsFromLoadingTrace(loading_trace):
-  """Pulls all the metrics from a given trace.
+def _ExtractDefaultMetrics(loading_trace):
+  """Extracts all the default metrics from a given trace.
 
   Args:
     loading_trace: loading_trace_module.LoadingTrace.
@@ -139,15 +142,32 @@ def _ExtractMetricsFromLoadingTrace(loading_trace):
   Returns:
     Dictionary with all trace extracted fields set.
   """
-  assert all(
-      cat in loading_trace.tracing_track.Categories()
-      for cat in sandwich_runner.ADDITIONAL_CATEGORIES), (
-          'This trace was not generated with the required set of categories '
-          'to be processed by this script.')
-  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   web_page_tracked_events = _GetWebPageTrackedEvents(
       loading_trace.tracing_track)
+  return {
+    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
+                   web_page_tracked_events['requestStart'].start_msec),
+    'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
+                        web_page_tracked_events['loadEventStart'].start_msec)
+  }
+
+
+def _ExtractMemoryMetrics(loading_trace):
+  """Extracts all the memory metrics from a given trace.
+
+  Args:
+    loading_trace: loading_trace_module.LoadingTrace.
 
+  Returns:
+    Dictionary with all trace extracted fields set.
+  """
+  if (sandwich_runner.MEMORY_DUMP_CATEGORY not in
+          loading_trace.tracing_track.Categories()):
+    return {
+      'browser_malloc_avg': _UNAVAILABLE_CSV_VALUE,
+      'browser_malloc_max': _UNAVAILABLE_CSV_VALUE
+    }
+  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   browser_malloc_sum = 0
   browser_malloc_max = 0
   for dump_event in browser_dump_events:
@@ -156,12 +176,7 @@ def _ExtractMetricsFromLoadingTrace(loading_trace):
     size = int(attr['value'], 16)
     browser_malloc_sum += size
     browser_malloc_max = max(browser_malloc_max, size)
-
   return {
-    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
-                   web_page_tracked_events['requestStart'].start_msec),
-    'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
-                        web_page_tracked_events['loadEventStart'].start_msec),
     'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
     'browser_malloc_max': browser_malloc_max
   }
@@ -239,14 +254,15 @@ def _ExtractMetricsFromRunDirectory(run_directory_path):
   logging.info('processing trace \'%s\'' % trace_path)
   loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
   run_metrics = {'url': loading_trace.url}
-  run_metrics.update(_ExtractMetricsFromLoadingTrace(loading_trace))
+  run_metrics.update(_ExtractDefaultMetrics(loading_trace))
+  run_metrics.update(_ExtractMemoryMetrics(loading_trace))
   video_path = os.path.join(run_directory_path, 'video.mp4')
   if os.path.isfile(video_path):
     logging.info('processing speed-index video \'%s\'' % video_path)
     completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
     run_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
   else:
-    run_metrics['speed_index'] = 'disabled'
+    run_metrics['speed_index'] = _UNAVAILABLE_CSV_VALUE
   for key, value in loading_trace.metadata['network_emulation'].iteritems():
     run_metrics['net_emul.' + key] = value
   return run_metrics
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 73c8f3e..e7553bc 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -19,7 +19,7 @@ import tracing
 
 
 _BLINK_CAT = 'blink.user_timing'
-_MEM_CAT = 'disabled-by-default-memory-infra'
+_MEM_CAT = sandwich_runner.MEMORY_DUMP_CATEGORY
 _START='requestStart'
 _LOADS='loadEventStart'
 _LOADE='loadEventEnd'
@@ -48,7 +48,7 @@ def TracingTrack(events):
   return tracing.TracingTrack.FromJsonDict({
       'events': events,
       'categories': (tracing.INITIAL_CATEGORIES +
-          sandwich_runner.ADDITIONAL_CATEGORIES)})
+          (sandwich_runner.MEMORY_DUMP_CATEGORY,))})
 
 
 def LoadingTrace(events):
@@ -107,8 +107,6 @@ class PageTrackTest(unittest.TestCase):
         {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
         {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
-    self.assertTrue(_MEM_CAT in sandwich_runner.ADDITIONAL_CATEGORIES)
-
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
     self.assertEquals(5, bump_events[0].start_msec)
@@ -187,12 +185,17 @@ class PageTrackTest(unittest.TestCase):
     self.assertEquals(17, trace_events['loadEventStart'].start_msec)
     self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
 
-  def testPullMetricsFromLoadingTrace(self):
-    metrics = puller._ExtractMetricsFromLoadingTrace(LoadingTrace(
+  def testExtractDefaultMetrics(self):
+    metrics = puller._ExtractDefaultMetrics(LoadingTrace(
         _MINIMALIST_TRACE_EVENTS))
-    self.assertEquals(4, len(metrics))
+    self.assertEquals(2, len(metrics))
     self.assertEquals(20, metrics['total_load'])
     self.assertEquals(5, metrics['js_onload_event'])
+
+  def testExtractMemoryMetrics(self):
+    metrics = puller._ExtractMemoryMetrics(LoadingTrace(
+        _MINIMALIST_TRACE_EVENTS))
+    self.assertEquals(2, len(metrics))
     self.assertEquals(30971, metrics['browser_malloc_avg'])
     self.assertEquals(55044, metrics['browser_malloc_max'])
 
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 3a56b59..3dc0415 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -26,9 +26,8 @@ import loading_trace
 TRACE_FILENAME = 'trace.json'
 VIDEO_FILENAME = 'video.mp4'
 
-# List of selected trace event categories when running chrome.
-ADDITIONAL_CATEGORIES = (
-    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
+# Memory dump category used to get memory metrics.
+MEMORY_DUMP_CATEGORY = 'disabled-by-default-memory-infra'
 
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
@@ -124,6 +123,9 @@ class SandwichRunner(object):
     # Configures whether to record speed-index video.
     self.record_video = False
 
+    # Configures whether to record memory dumps.
+    self.record_memory_dumps = False
+
     # Path to the WPR archive to load or save. Is str or None.
     self.wpr_archive_path = None
 
@@ -189,13 +191,17 @@ class SandwichRunner(object):
         os.makedirs(run_path)
     self._chrome_ctl.SetNetworkEmulation(
         self._GetEmulatorNetworkCondition('browser'))
+    additional_categories = []
+    if self.record_memory_dumps:
+      additional_categories = [MEMORY_DUMP_CATEGORY]
     # TODO(gabadie): add a way to avoid recording a trace.
     with self._chrome_ctl.Open() as connection:
       if clear_cache:
         connection.ClearCache()
       if run_path is not None and self.record_video:
         device = self._chrome_ctl.GetDevice()
-        assert device, 'Can only record video on a remote device.'
+        if device is None:
+          raise RuntimeError('Can only record video on a remote device.')
         video_recording_path = os.path.join(run_path, VIDEO_FILENAME)
         with device_setup.RemoteSpeedIndexRecorder(device, connection,
                                                    video_recording_path):
@@ -203,14 +209,14 @@ class SandwichRunner(object):
               url=url,
               connection=connection,
               chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              additional_categories=ADDITIONAL_CATEGORIES,
+              additional_categories=additional_categories,
               timeout_seconds=_DEVTOOLS_TIMEOUT)
       else:
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
             url=url,
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            additional_categories=ADDITIONAL_CATEGORIES,
+            additional_categories=additional_categories,
             timeout_seconds=_DEVTOOLS_TIMEOUT)
     if run_path is not None:
       trace_path = os.path.join(run_path, TRACE_FILENAME)
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 954c67e..ea763ec 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -37,7 +37,7 @@ class SandwichTaskBuilder(task_manager.Builder):
   """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
   """
 
-  def __init__(self, output_directory, android_device, job_path, url_repeat):
+  def __init__(self, output_directory, android_device, job_path):
     """Constructor.
 
     Args:
@@ -45,13 +45,10 @@ class SandwichTaskBuilder(task_manager.Builder):
       android_device: The android DeviceUtils to run sandwich on or None to run
         it locally.
       job_path: Path of the sandwich's job.
-      url_repeat: Non null integer controlling how many times the URLs should be
-        repeated in the benchmarks.
     """
     task_manager.Builder.__init__(self, output_directory)
     self._android_device = android_device
     self._job_path = job_path
-    self._url_repeat = url_repeat
     self._default_final_tasks = []
 
     self._original_wpr_task = None
@@ -159,20 +156,20 @@ class SandwichTaskBuilder(task_manager.Builder):
     return ValidateReferenceCache
 
   def PopulateLoadBenchmark(self, subresource_discoverer,
-                            runner_transformer_name, runner_transformer):
+                            transformer_list_name, transformer_list):
     """Populate benchmarking tasks from its setup tasks.
 
     Args:
       subresource_discoverer: Name of a subresources discoverer.
-      runner_transformer: A function that takes an instance of SandwichRunner as
-          parameter, would be applied immediately before SandwichRunner.Run().
-      runner_transformer_name: Name of the runner transformer used to generate
-          task names.
-      benchmark_name: The benchmark's name for that runner modifier.
+      transformer_list_name: A string describing the transformers, will be used
+          in Task names (prefer names without spaces and special characters).
+      transformer_list: An ordered list of function that takes an instance of
+          SandwichRunner as parameter, would be applied immediately before
+          SandwichRunner.Run() in the given order.
 
     Here is the full dependency of the added tree for the returned task:
-    <runner_transformer_name>/<subresource_discoverer>-metrics.csv
-      depends on: <runner_transformer_name>/<subresource_discoverer>-run/
+    <transformer_list_name>/<subresource_discoverer>-metrics.csv
+      depends on: <transformer_list_name>/<subresource_discoverer>-run/
         depends on: common/<subresource_discoverer>-cache.zip
           depends on: some tasks saved by PopulateCommonPipelines()
           depends on: common/<subresource_discoverer>-setup.json
@@ -180,12 +177,12 @@ class SandwichTaskBuilder(task_manager.Builder):
 
     Returns:
       task_manager.Task for
-          <runner_transformer_name>/<subresource_discoverer>-metrics.csv
+          <transformer_list_name>/<subresource_discoverer>-metrics.csv
     """
     assert subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS
     assert 'common' not in sandwich_misc.SUBRESOURCE_DISCOVERERS
     shared_task_prefix = os.path.join('common', subresource_discoverer)
-    task_prefix = os.path.join(runner_transformer_name, subresource_discoverer)
+    task_prefix = os.path.join(transformer_list_name, subresource_discoverer)
 
     @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
                        dependencies=[self._subresources_for_urls_task])
@@ -221,9 +218,8 @@ class SandwichTaskBuilder(task_manager.Builder):
                        dependencies=[BuildBenchmarkCacheArchive])
     def RunBenchmark():
       runner = self._CreateSandwichRunner()
-      # runner.record_video = True
-      runner.job_repeat = self._url_repeat
-      runner_transformer(runner)
+      for transformer in transformer_list:
+        transformer(runner)
       runner.wpr_archive_path = self._patched_wpr_task.path
       runner.wpr_out_log_path = os.path.join(RunBenchmark.path, 'wpr.log')
       runner.cache_archive_path = BuildBenchmarkCacheArchive.path

commit e5560ea60ce190f3e9809216390a96e8a6060da1
Author: droger <droger@chromium.org>
Date:   Thu Apr 28 08:06:14 2016 -0700

    tools/android/loading Send email notification when the task is finished
    
    Review-Url: https://codereview.chromium.org/1926693004
    Cr-Original-Commit-Position: refs/heads/master@{#390381}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b8ecf4fd6e15fe92dcf0608b8c997f919b5d6a67

diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index ffef8be..42b7b09 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -14,6 +14,7 @@ from oauth2client.client import GoogleCredentials
 
 from common.clovis_task import ClovisTask
 import common.google_instance_helper
+import email_helper
 from memory_logs import MemoryLogs
 
 
@@ -33,7 +34,7 @@ def Render(message, memory_logs):
       'log.html', body=message, log=memory_logs.Flush().split('\n'))
 
 
-def PollWorkers(tag, start_time, timeout_hours):
+def PollWorkers(tag, start_time, timeout_hours, email_address, task_url):
   """Checks if there are workers associated with tag, by polling the instance
   group. When all workers are finished, the instance group and the instance
   template are destroyed.
@@ -46,10 +47,12 @@ def PollWorkers(tag, start_time, timeout_hours):
     start_time (float): Time when the polling started, as returned by
                         time.time().
     timeout_hours (int): Timeout after which workers are terminated.
+    email_address (str): Email address to notify when the task is complete.
+    task_url (str): URL where the results of the task can be found.
   """
   if (time.time() - start_time) > (3600 * timeout_hours):
     clovis_logger.error('Worker timeout for tag %s, shuting down.' % tag)
-    deferred.defer(DeleteInstanceGroup, tag)
+    Finalize(tag, email_address, 'TIMEOUT', task_url)
     return
 
   clovis_logger.info('Polling workers for tag: ' + tag)
@@ -60,10 +63,26 @@ def PollWorkers(tag, start_time, timeout_hours):
   if live_instance_count > 0 or live_instance_count == -1:
     clovis_logger.info('Retry later, instances still alive for tag: ' + tag)
     poll_interval_minutes = 10
-    deferred.defer(PollWorkers, tag, start_time,
+    deferred.defer(PollWorkers, tag, start_time, email_address, task_url,
                    _countdown=(60 * poll_interval_minutes))
     return
 
+  Finalize(tag, email_address, 'SUCCESS', task_url)
+
+
+def Finalize(tag, email_address, status, task_url):
+  """Cleans up the remaining ComputeEngine resources and notifies the user.
+
+  Args:
+    tag (str): Tag of the task to finalize.
+    email_address (str): Email address of the user to be notified.
+    status (str): Status of the task, indicating the success or the cause of
+                  failure.
+    task_url (str): URL where the results of the task can be found.
+  """
+  email_helper.SendEmailTaskComplete(
+      to_address=email_address, tag=tag, status=status, task_url=task_url,
+      logger=clovis_logger)
   clovis_logger.info('Scheduling instance group destruction for tag: ' + tag)
   deferred.defer(DeleteInstanceGroup, tag)
 
@@ -142,7 +161,11 @@ def StartFromJsonString(http_body_str):
 
   # Split the task in smaller tasks.
   sub_tasks = []
+  task_url = None
   if task.Action() == 'trace':
+    bucket = task.BackendParams().get('storage_bucket')
+    if bucket:
+      task_url = 'https://console.cloud.google.com/storage/' + bucket
     sub_tasks = SplitTraceTask(task)
   else:
     error_string = 'Unsupported action: %s.' % task.Action()
@@ -160,10 +183,14 @@ def StartFromJsonString(http_body_str):
   clovis_logger.info('Creating worker polling task.')
   first_poll_delay_minutes = 10
   timeout_hours = task.BackendParams().get('timeout_hours', 5)
-  deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours,
-                 _countdown=(60 * first_poll_delay_minutes))
-
-  return Render('Success', memory_logs)
+  user_email = email_helper.GetUserEmail()
+  deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours, user_email,
+                 task_url, _countdown=(60 * first_poll_delay_minutes))
+
+  return Render(flask.Markup(
+      'Success!<br>Your task %s has started.<br>'
+      'You will be notified at %s when completed.') % (task_tag, user_email),
+      memory_logs)
 
 
 def SplitTraceTask(task):
diff --git a/loading/cloud/frontend/email_helper.py b/loading/cloud/frontend/email_helper.py
new file mode 100644
index 0000000..dd82ee7
--- /dev/null
+++ b/loading/cloud/frontend/email_helper.py
@@ -0,0 +1,42 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from google.appengine.api import (mail, users)
+
+
+def GetUserEmail():
+  """Returns the email address of the user currently making the request or None.
+  """
+  user = users.get_current_user()
+  if user:
+    return user.email()
+  return None
+
+
+def SendEmailTaskComplete(to_address, tag, status, task_url, logger):
+  """Sends an email to to_address notifying that the task identified by tag is
+  complete.
+
+  Args:
+    to_address (str): The email address to notify.
+    tag (str): The tag of the task.
+    status (str): Status of the task.
+    task_url (str): URL where the results of the task can be found.
+    logger (logging.logger): Used for logging.
+  """
+  if not to_address:
+    logger.error('No email address to notify for task ' + tag)
+    return
+
+  logger.info('Notify task %s complete to %s.' % (tag, to_address))
+  # The sender address must be in the "Email API authorized senders", configured
+  # in the Application Settings of AppEngine.
+  sender_address = 'clovis-noreply@google.com'
+  subject = 'Task %s complete' % tag
+  body = 'Your Clovis task %s is now complete with status: %s.' % (tag, status)
+  if task_url:
+    body += '\nCheck the results at ' + task_url
+  mail.send_mail(sender=sender_address, to=to_address, subject=subject,
+                 body=body)
+

commit bf86f9bc6f1d4d7f8fdb6d746bc842a20b2599eb
Author: mattcary <mattcary@chromium.org>
Date:   Thu Apr 28 05:47:26 2016 -0700

    Clovis: contentful paint upgrades.
    
    Use last contentful paint event for timing, and emit request fingerprints so
    that critical resources can be collated across runs. This adds the implied lens
    that defines critical resources from a fingerprint list.
    
    Review-Url: https://codereview.chromium.org/1888343003
    Cr-Original-Commit-Position: refs/heads/master@{#390359}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 89dbec666c4357c58b3b7ba748cfb842059f90b8

diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 0df9025..cbd882a 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -52,7 +52,7 @@ class PrefetchSimulationView(object):
     if trace is None:
       return
     requests = trace.request_track.GetEvents()
-    critical_requests_ids = user_lens.CriticalRequests()
+    critical_requests_ids = user_lens.CriticalRequestIds()
     self.postload_msec = user_lens.PostloadTimeMsec()
     self.graph = dependency_graph.RequestDependencyGraph(
         requests, dependencies_lens, node_class=RequestNode)
diff --git a/loading/request_track.py b/loading/request_track.py
index b4e3a15..8bd62da 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -12,6 +12,7 @@ import collections
 import copy
 import datetime
 import email.utils
+import hashlib
 import json
 import logging
 import re
@@ -213,6 +214,12 @@ class Request(object):
       return None
     return self.start_msec + self.timing.LargestOffset()
 
+  @property
+  def fingerprint(self):
+    h = hashlib.sha256()
+    h.update(self.url)
+    return h.hexdigest()[:10]
+
   def _TimestampOffsetFromStartMs(self, timestamp):
     assert self.timing.request_time != -1
     request_time = self.timing.request_time
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 8f61969..10385b5 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -221,7 +221,7 @@ class MockConnection(object):
     return response
 
 
-class MockUserSatisfiedLens(user_satisfied_lens._UserSatisfiedLens):
+class MockUserSatisfiedLens(user_satisfied_lens._FirstEventLens):
   def _CalculateTimes(self, _):
     self._satisfied_msec = float('inf')
     self._event_msec = float('inf')
diff --git a/loading/tracing.py b/loading/tracing.py
index 068c5b6..55009a6 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -92,10 +92,26 @@ class TracingTrack(devtools_monitor.Track):
 
   def GetMatchingMainFrameEvents(self, category, name):
     """Gets events matching |category| and |name| that occur in the main frame.
-    Assumes that the events in question have a 'frame' key in their |args|."""
+
+    Events without a 'frame' key in their |args| are discarded.
+    """
     matching_events = self.GetMatchingEvents(category, name)
     return [e for e in matching_events
-        if e.args['frame'] == self._GetMainFrameID()]
+        if 'frame' in e.args and e.args['frame'] == self.GetMainFrameID()]
+
+  def GetMainFrameID(self):
+    """Returns the main frame ID."""
+    if not self._main_frame_id:
+      navigation_start_events = [e for e in self.GetEvents()
+          if e.Matches('blink.user_timing', 'navigationStart')]
+      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
+      self._main_frame_id = first_event.args['frame']
+
+    return self._main_frame_id
+
+  def SetMainFrameID(self, frame_id):
+    """Set the main frame ID. Normally this is used only for testing."""
+    self._main_frame_id = frame_id
 
   def EventsAt(self, msec):
     """Gets events active at a timestamp.
@@ -197,16 +213,6 @@ class TracingTrack(devtools_monitor.Track):
         return event
     return None
 
-  def _GetMainFrameID(self):
-    """Returns the main frame ID."""
-    if not self._main_frame_id:
-      navigation_start_events = [e for e in self.GetEvents()
-          if e.Matches('blink.user_timing', 'navigationStart')]
-      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
-      self._main_frame_id = first_event.args['frame']
-
-    return self._main_frame_id
-
   def _IndexEvents(self, strict=False):
     if self._interval_tree:
       return
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index cce5809..95acef8 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -309,7 +309,7 @@ class TracingTrackTestCase(unittest.TestCase):
          'args': {'frame': _MAIN_FRAME_ID}},
         ]
     self._HandleEvents(events)
-    self.assertEquals(_MAIN_FRAME_ID, self.track._GetMainFrameID())
+    self.assertEquals(_MAIN_FRAME_ID, self.track.GetMainFrameID())
 
   def testGetMatchingEvents(self):
     _MAIN_FRAME_ID = 0xffff
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index dd99d79..523a240 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -6,6 +6,9 @@
 
 Several lenses are defined, for example FirstTextPaintLens and
 FirstSignificantPaintLens.
+
+When run from the command line, takes a lens name and a trace, and prints the
+fingerprints of the critical resources to stdout.
 """
 import logging
 import operator
@@ -24,6 +27,53 @@ class _UserSatisfiedLens(object):
   _ATTRS = ['_satisfied_msec', '_event_msec', '_postload_msec',
             '_critical_request_ids']
 
+  def CriticalRequests(self):
+    """Critical requests.
+
+    Returns:
+      A sequence of request_track.Request objects representing an estimate of
+      all requests that are necessary for the user satisfaction defined by this
+      class.
+    """
+    raise NotImplementedError
+
+  def CriticalRequestIds(self):
+    """Ids of critical requests."""
+    return set(rq.request_id for rq in self.CriticalRequests())
+
+  def CriticalFingerprints(self):
+    """Fingerprints of critical requests."""
+    return set(rq.fingerprint for rq in self.CriticalRequests())
+
+  def PostloadTimeMsec(self):
+    """Return postload time.
+
+    The postload time is an estimate of the amount of time needed by chrome to
+    transform the critical results into the satisfying event.
+
+    Returns:
+      Postload time in milliseconds.
+    """
+    return 0
+
+
+class RequestFingerprintLens(_UserSatisfiedLens):
+  """A lens built using requests in a trace that match a set of fingerprints."""
+  def __init__(self, trace, fingerprints):
+    fingerprints = set(fingerprints)
+    self._critical_requests = [rq for rq in trace.request_track.GetEvents()
+                               if rq.fingerprint in fingerprints]
+
+  def CriticalRequests(self):
+    """Ids of critical requests."""
+    return set(self._critical_requests)
+
+
+class _FirstEventLens(_UserSatisfiedLens):
+  """Helper abstract subclass that defines users first event manipulations."""
+  # pylint can't handle abstract subclasses.
+  # pylint: disable=abstract-method
+
   def __init__(self, trace):
     """Initialize the lens.
 
@@ -36,34 +86,23 @@ class _UserSatisfiedLens(object):
     self._critical_request_ids = None
     if trace is None:
       return
-    self._CalculateTimes(trace.tracing_track)
-    critical_requests = self._RequestsBefore(
+    self._CalculateTimes(trace)
+    self._critical_requests = self._RequestsBefore(
         trace.request_track, self._satisfied_msec)
-    self._critical_request_ids = set(rq.request_id for rq in critical_requests)
-    if critical_requests:
-      last_load = max(rq.end_msec for rq in critical_requests)
+    self._critical_request_ids = set(rq.request_id
+                                     for rq in self._critical_requests)
+    if self._critical_requests:
+      last_load = max(rq.end_msec for rq in self._critical_requests)
     else:
       last_load = float('inf')
     self._postload_msec = self._event_msec - last_load
 
   def CriticalRequests(self):
-    """Request ids of critical requests.
-
-    Returns:
-      A set of request ids (as strings) of an estimate of all requests that are
-      necessary for the user satisfaction defined by this class.
-    """
-    return self._critical_request_ids
+    """Override."""
+    return self._critical_requests
 
   def PostloadTimeMsec(self):
-    """Return postload time.
-
-    The postload time is an estimate of the amount of time needed by chrome to
-    transform the critical results into the satisfying event.
-
-    Returns:
-      Postload time in milliseconds.
-    """
+    """Override."""
     return self._postload_msec
 
   def ToJsonDict(self):
@@ -75,7 +114,7 @@ class _UserSatisfiedLens(object):
     return common_util.DeserializeAttributesFromJsonDict(
         json_dict, result, cls._ATTRS)
 
-  def _CalculateTimes(self, tracing_track):
+  def _CalculateTimes(self, trace):
     """Subclasses should implement to set _satisfied_msec and _event_msec."""
     raise NotImplementedError
 
@@ -84,26 +123,19 @@ class _UserSatisfiedLens(object):
     return [rq for rq in request_track.GetEvents()
             if rq.end_msec <= time_ms]
 
-
-class _FirstEventLens(_UserSatisfiedLens):
-  """Helper abstract subclass that defines users first event manipulations."""
-  # pylint can't handle abstract subclasses.
-  # pylint: disable=abstract-method
-
   @classmethod
   def _CheckCategory(cls, tracing_track, category):
     assert category in tracing_track.Categories(), (
         'The "%s" category must be enabled.' % category)
 
   @classmethod
-  def _ExtractFirstTiming(cls, times):
+  def _ExtractBestTiming(cls, times):
     if not times:
       return float('inf')
-    if len(times) != 1:
-      # TODO(mattcary): in some cases a trace has two first paint events. Why?
-      logging.error('%d %s with spread of %s', len(times),
-                    str(cls), max(times) - min(times))
-    return float(min(times))
+    assert len(times) == 1, \
+        'Unexpected duplicate {}: {} with spread of {}'.format(
+            str(cls), len(times), max(times) - min(times))
+    return float(max(times))
 
 
 class FirstTextPaintLens(_FirstEventLens):
@@ -112,12 +144,13 @@ class FirstTextPaintLens(_FirstEventLens):
   This event is taken directly from a trace.
   """
   _EVENT_CATEGORY = 'blink.user_timing'
-  def _CalculateTimes(self, tracing_track):
-    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
-    first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches(self._EVENT_CATEGORY, 'firstPaint')]
+  def _CalculateTimes(self, trace):
+    self._CheckCategory(trace.tracing_track, self._EVENT_CATEGORY)
+    first_paints = [
+        e.start_msec for e in trace.tracing_track.GetMatchingMainFrameEvents(
+            'blink.user_timing', 'firstPaint')]
     self._satisfied_msec = self._event_msec = \
-        self._ExtractFirstTiming(first_paints)
+        self._ExtractBestTiming(first_paints)
 
 
 class FirstContentfulPaintLens(_FirstEventLens):
@@ -127,12 +160,13 @@ class FirstContentfulPaintLens(_FirstEventLens):
   by filtering out things like background paint from firstPaint.
   """
   _EVENT_CATEGORY = 'blink.user_timing'
-  def _CalculateTimes(self, tracing_track):
-    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
-    first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches(self._EVENT_CATEGORY, 'firstContentfulPaint')]
+  def _CalculateTimes(self, trace):
+    self._CheckCategory(trace.tracing_track, self._EVENT_CATEGORY)
+    first_paints = [
+        e.start_msec for e in trace.tracing_track.GetMatchingMainFrameEvents(
+            'blink.user_timing', 'firstContentfulPaint')]
     self._satisfied_msec = self._event_msec = \
-       self._ExtractFirstTiming(first_paints)
+       self._ExtractBestTiming(first_paints)
 
 
 class FirstSignificantPaintLens(_FirstEventLens):
@@ -143,12 +177,18 @@ class FirstSignificantPaintLens(_FirstEventLens):
   that is the observable event.
   """
   _FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
-  _EVENT_CATEGORY = 'disabled-by-default-blink.debug.layout'
-  def _CalculateTimes(self, tracing_track):
-    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
+  _EVENT_CATEGORIES = ['blink', 'disabled-by-default-blink.debug.layout']
+  def _CalculateTimes(self, trace):
+    for cat in self._EVENT_CATEGORIES:
+      self._CheckCategory(trace.tracing_track, cat)
     sync_paint_times = []
     layouts = []  # (layout item count, msec).
-    for e in tracing_track.GetEvents():
+    for e in trace.tracing_track.GetEvents():
+      if ('frame' in e.args and
+          e.args['frame'] != trace.tracing_track.GetMainFrameID()):
+        continue
+      # If we don't know have a frame id, we assume it applies to all events.
+
       # TODO(mattcary): is this the right paint event? Check if synchronized
       # paints appear at the same time as the first*Paint events, above.
       if e.Matches('blink', 'FrameView::synchronizedPaint'):
@@ -158,7 +198,25 @@ class FirstSignificantPaintLens(_FirstEventLens):
         layouts.append((e.args['counters'][self._FIRST_LAYOUT_COUNTER],
                         e.start_msec))
     assert layouts, 'No layout events'
+    assert sync_paint_times,'No sync paint times'
     layouts.sort(key=operator.itemgetter(0), reverse=True)
     self._satisfied_msec = layouts[0][1]
-    self._event_msec = self._ExtractFirstTiming([
-        min(t for t in sync_paint_times if t > self._satisfied_msec)])
+    self._event_msec = min(t for t in sync_paint_times
+                           if t > self._satisfied_msec)
+
+
+def main(lens_name, trace_file):
+  assert (lens_name in globals() and
+          not lens_name.startswith('_') and
+          lens_name.endswith('Lens')), 'Bad lens %s' % lens_name
+  lens_cls = globals()[lens_name]
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_file)
+  lens = lens_cls(trace)
+  for fp in sorted(lens.CriticalFingerprints()):
+    print fp
+
+
+if __name__ == '__main__':
+  import sys
+  import loading_trace
+  main(sys.argv[1], sys.argv[2])
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index c7cdb10..c603bbe 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -32,51 +32,65 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     return rq
 
   def testFirstContentfulPaintLens(self):
+    MAINFRAME = 1
+    SUBFRAME = 2
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'blink.some_other_user_timing',
                        'name': 'firstContentfulPaint'},
-                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstDiscontentPaint'},
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint'},
-                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': SUBFRAME} },
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstContentfulPaint'}])
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testCantGetNoSatisfaction(self):
+    MAINFRAME = 1
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'not_my_cat',
-                       'name': 'someEvent'}])
+                       'name': 'someEvent',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequestIds())
     self.assertEqual(float('inf'), lens.PostloadTimeMsec())
 
   def testFirstTextPaintLens(self):
+    MAINFRAME = 1
+    SUBFRAME = 2
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'blink.some_other_user_timing',
                        'name': 'firstPaint'},
-                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstishPaint'},
-                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'name': 'firstishPaint',
+                       'args': {'frame': MAINFRAME}},
+                      {'ts': 3 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstPaint'},
-                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'name': 'firstPaint',
+                       'args': {'frame': SUBFRAME}},
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'firstPaint'}])
+                       'name': 'firstPaint',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
     lens = user_satisfied_lens.FirstTextPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(1, lens.PostloadTimeMsec())
 
   def testFirstSignificantPaintLens(self):
@@ -112,9 +126,37 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
                            'LayoutObjectsThatHadNeverHadLayout': 10
                        } } } ])
     lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
-    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
     self.assertEqual(7, lens.PostloadTimeMsec())
 
+  def testRequestFingerprintLens(self):
+    MAINFRAME = 1
+    SUBFRAME = 2
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink.some_other_user_timing',
+                       'name': 'firstContentfulPaint'},
+                      {'ts': 30 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstDiscontentPaint'},
+                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': SUBFRAME} },
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint',
+                       'args': {'frame': MAINFRAME}}])
+    loading_trace.tracing_track.SetMainFrameID(MAINFRAME)
+    lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequestIds())
+    self.assertEqual(1, lens.PostloadTimeMsec())
+    request_lens = user_satisfied_lens.RequestFingerprintLens(
+      loading_trace, lens.CriticalFingerprints())
+    self.assertEqual(set(['0.1', '0.2']), request_lens.CriticalRequestIds())
+    self.assertEqual(0, request_lens.PostloadTimeMsec())
+
 
 if __name__ == '__main__':
   unittest.main()

commit b890ad44b219a347bcf8010055138ab89167ab74
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 28 05:15:08 2016 -0700

    tools/android/loading: Force to use of simple cache on desktop.
    
    Before on desktop, the cache backend type of the HTTP cache was
    choosen by the SimpleCacheTrial on field trial. This can cause
    issues with sandwich were the cache backend type might change
    between runs.
    
    This CL override the cache backend type to use in Chrome on
    Desktop to not have to handle the two backend types in the cache
    archive processing.
    
    BUG=582080
    
    Review-Url: https://codereview.chromium.org/1931523002
    Cr-Original-Commit-Position: refs/heads/master@{#390352}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0e6c8e0d72463672af06ecd6a634e6d9e5078927

diff --git a/loading/controller.py b/loading/controller.py
index d8c20e0..8a2d4a6 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -324,6 +324,8 @@ class LocalChromeController(ChromeControllerBase):
        child processes used to run Chrome and XVFB."""
     chrome_cmd = [OPTIONS.local_binary]
     chrome_cmd.extend(self._GetChromeArguments())
+    # Force use of simple cache.
+    chrome_cmd.append('--use-simple-cache-backend=on')
     chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
     chrome_cmd.extend(['--enable-logging=stderr', '--v=1'])
     # Navigates to about:blank for couples of reasons:

commit 83f74d8859ac674cece0588735f672d4bd10f70c
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 28 02:11:20 2016 -0700

    clovis: Don't pretty-print JSON traces.
    
    Size:
    - before: 167MB
    - after:  45MB
    
    Review-Url: https://codereview.chromium.org/1923823003
    Cr-Original-Commit-Position: refs/heads/master@{#390335}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2382627811eeea2398c38feaa36077cbc1bd973c

diff --git a/loading/analyze.py b/loading/analyze.py
index db0106f..82cc071 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -59,16 +59,6 @@ def _LoadPage(device, url):
   device.StartActivity(load_intent, blocking=True)
 
 
-def _WriteJson(output, json_data):
-  """Write JSON data in a nice way.
-
-  Args:
-    output: a file object
-    json_data: JSON data as a dict.
-  """
-  json.dump(json_data, output, sort_keys=True, indent=2)
-
-
 def _GetPrefetchHtml(graph_view, name=None):
   """Generate prefetch page for the resources in resource graph.
 
@@ -154,14 +144,14 @@ def _FullFetch(url, json_output, prefetch):
     logging.warning('Warm fetch')
     warm_data = _LogRequests(url, clear_cache_override=False)
     with open(json_output, 'w') as f:
-      _WriteJson(f, warm_data)
+      json.dump(warm_data, f)
     logging.warning('Wrote ' + json_output)
     with open(json_output + '.cold', 'w') as f:
-      _WriteJson(f, cold_data)
+      json.dump(cold_data, f)
     logging.warning('Wrote ' + json_output + '.cold')
   else:
     with open(json_output, 'w') as f:
-      _WriteJson(f, cold_data)
+      json.dump(cold_data, f)
     logging.warning('Wrote ' + json_output)
 
 
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 2a291e5..0804e27 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -52,7 +52,7 @@ class LoadingTrace(object):
     """Save a json file representing this instance."""
     json_dict = self.ToJsonDict()
     with open(json_path, 'w') as output_file:
-       json.dump(json_dict, output_file, indent=2)
+       json.dump(json_dict, output_file)
 
   @classmethod
   def FromJsonDict(cls, json_dict):

commit 41fc05c86971ee57e815e9d1bff10d461f3c5a29
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 28 02:10:33 2016 -0700

    clovis: about:tracing traces don't need to be gzipped.
    
    Makes trace_to_chrome_trace.py go from >1m to 20s.
    
    Review-Url: https://codereview.chromium.org/1926513003
    Cr-Original-Commit-Position: refs/heads/master@{#390334}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a01c59e9c11f164302c7fbfbb21912876e98157f

diff --git a/loading/trace_to_chrome_trace.py b/loading/trace_to_chrome_trace.py
index 382d87d..23c3632 100755
--- a/loading/trace_to_chrome_trace.py
+++ b/loading/trace_to_chrome_trace.py
@@ -5,12 +5,11 @@
 
 """Convert trace output for Chrome.
 
-Takes a loading trace from 'analyze.py log_requests' and outputs a zip'd json
+Takes a loading trace from 'analyze.py log_requests' and outputs a json file
 that can be loaded by chrome's about:tracing..
 """
 
 import argparse
-import gzip
 import json
 
 if __name__ == '__main__':
@@ -18,6 +17,6 @@ if __name__ == '__main__':
   parser.add_argument('input')
   parser.add_argument('output')
   args = parser.parse_args()
-  with gzip.GzipFile(args.output, 'w') as output_f, file(args.input) as input_f:
+  with file(args.output, 'w') as output_f, file(args.input) as input_f:
     events = json.load(input_f)['tracing_track']['events']
     json.dump({'traceEvents': events, 'metadata': {}}, output_f)

commit 370155bd6c0191b15aa019c4a2d4848ba2ad0359
Author: droger <droger@chromium.org>
Date:   Wed Apr 27 12:40:52 2016 -0700

    tools/android/loading Cleanup cloud resources when workers are done.
    
    The frontend now polls the backend to check if the work is finished, and
    then destroys the instance group and the instance template.
    
    Review-Url: https://codereview.chromium.org/1920093004
    Cr-Original-Commit-Position: refs/heads/master@{#390151}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 35480ea8a3d068c38e4917b48d231e5a2d5ff192

diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 3f766dd..746512d 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -21,12 +21,16 @@ class GoogleInstanceHelper(object):
     self._logger = logger
 
   def _ExecuteApiRequest(self, request, retry_count=3):
-    """ Executes a Compute API request and returns True on success."""
+    """ Executes a Compute API request and returns True on success.
+
+    Returns:
+      (True, Response) in case of success, or (False, error_content) otherwise.
+    """
     self._logger.info('Compute API request:\n' + request.to_json())
     try:
       response = request.execute()
-      self._logger.info('Compute API response:\n' + response)
-      return True
+      self._logger.info('Compute API response:\n' + str(response))
+      return (True, response)
     except errors.HttpError as err:
       error_content = self._GetErrorContent(err)
       error_reason = self._GetErrorReason(error_content)
@@ -42,7 +46,7 @@ class GoogleInstanceHelper(object):
             error_reason, err))
         if error_content:
           self._logger.error('Error details:\n%s' % error_content)
-        return False
+        return (False, error_content)
 
   def _GetTemplateName(self, tag):
     """Returns the name of the instance template associated with tag."""
@@ -112,16 +116,23 @@ class GoogleInstanceHelper(object):
                 {'key': 'taskqueue-tag', 'value': tag}]}}}
     request = self._compute_api.instanceTemplates().insert(
         project=self._project, body=request_body)
-    return self._ExecuteApiRequest(request)
+    return self._ExecuteApiRequest(request)[0]
 
   def DeleteTemplate(self, tag):
     """Deletes the instance template associated with tag. Returns True if
     successful.
     """
+    template_name = self._GetTemplateName(tag)
     request = self._compute_api.instanceTemplates().delete(
-        project=self._project,
-        instanceTemplate=self._GetTemplateName(tag))
-    return self._ExecuteApiRequest(request)
+        project=self._project, instanceTemplate=template_name)
+    (success, result) = self._ExecuteApiRequest(request)
+    if success:
+      return True
+    if self._GetErrorReason(result) == 'notFound':
+      # The template does not exist, nothing to do.
+      self._logger.warning('Template not found: ' + template_name)
+      return True
+    return False
 
   def CreateInstances(self, tag, instance_count):
     """Creates an instance group associated with tag. The instance template must
@@ -137,10 +148,10 @@ class GoogleInstanceHelper(object):
     request = self._compute_api.instanceGroupManagers().insert(
         project=self._project, zone=self._zone,
         body=request_body)
-    return self._ExecuteApiRequest(request)
+    return self._ExecuteApiRequest(request)[0]
 
   def DeleteInstance(self, tag, instance_hostname):
-    """Deletes one instance from the instance group identified with tag. Returns
+    """Deletes one instance from the instance group identified by tag. Returns
     True if successful.
     """
     # The instance hostname may be of the form <name>.c.<project>.internal but
@@ -152,4 +163,34 @@ class GoogleInstanceHelper(object):
         project=self._project, zone=self._zone,
         instanceGroupManager=self._GetInstanceGroupName(tag),
         body={'instances': [instance_url]})
-    return self._ExecuteApiRequest(request)
+    return self._ExecuteApiRequest(request)[0]
+
+  def DeleteInstanceGroup(self, tag):
+    """Deletes the instance group identified by tag. If instances are still
+    running in this group, they are deleted as well.
+    """
+    group_name = self._GetInstanceGroupName(tag)
+    request = self._compute_api.instanceGroupManagers().delete(
+        project=self._project, zone=self._zone,
+        instanceGroupManager=group_name)
+    (success, result) = self._ExecuteApiRequest(request)
+    if success:
+      return True
+    if self._GetErrorReason(result) == 'notFound':
+      # The group does not exist, nothing to do.
+      self._logger.warning('Instance group not found: ' + group_name)
+      return True
+    return False
+
+  def GetInstanceCount(self, tag):
+    """Returns the number of instances in the instance group identified by
+    tag, or -1 in case of failure.
+    """
+    request = self._compute_api.instanceGroupManagers().listManagedInstances(
+        project=self._project, zone=self._zone,
+        instanceGroupManager=self._GetInstanceGroupName(tag))
+    (success, response) = self._ExecuteApiRequest(request)
+    if not success:
+      return -1
+    return len(response.get('managedInstances', []))
+
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index 7720203..7e06898 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -24,6 +24,9 @@ following keys:
     ComputeEngine instances. This parameter should not be set in general, as it
     is mostly exposed for development purposes. If this parameter is not
     specified, a unique tag will be generated.
+-   `timeout_hours` (int, optional): if workers are still alive after this
+    delay, they will be forcibly killed, to avoid wasting Compute Engine
+    resources. Defaults to `5`.
 
 ### Parameters for the `trace` action
 
diff --git a/loading/cloud/frontend/app.yaml b/loading/cloud/frontend/app.yaml
index 6544088..2801e9d 100644
--- a/loading/cloud/frontend/app.yaml
+++ b/loading/cloud/frontend/app.yaml
@@ -2,8 +2,18 @@ runtime: python27
 api_version: 1
 threadsafe: yes
 
+builtins:
+- deferred: on
+
 handlers:
+
+- url: /_ah/queue/deferred
+  # For the deferred API (https://cloud.google.com/appengine/articles/deferred).
+  script: google.appengine.ext.deferred.deferred.application
+  login: admin
+
 - url: /static
+  # Static content.
   static_dir: static
 
 - url: .*
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 656fa9d..ffef8be 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -5,9 +5,11 @@
 import logging
 import os
 import sys
+import time
 
 import flask
 from google.appengine.api import (app_identity, taskqueue)
+from google.appengine.ext import deferred
 from oauth2client.client import GoogleCredentials
 
 from common.clovis_task import ClovisTask
@@ -31,6 +33,41 @@ def Render(message, memory_logs):
       'log.html', body=message, log=memory_logs.Flush().split('\n'))
 
 
+def PollWorkers(tag, start_time, timeout_hours):
+  """Checks if there are workers associated with tag, by polling the instance
+  group. When all workers are finished, the instance group and the instance
+  template are destroyed.
+  After some timeout delay, the instance group is destroyed even if there are
+  still workers associated to it, which has the effect of killing all these
+  workers.
+
+  Args:
+    tag (string): Tag of the task that is polled.
+    start_time (float): Time when the polling started, as returned by
+                        time.time().
+    timeout_hours (int): Timeout after which workers are terminated.
+  """
+  if (time.time() - start_time) > (3600 * timeout_hours):
+    clovis_logger.error('Worker timeout for tag %s, shuting down.' % tag)
+    deferred.defer(DeleteInstanceGroup, tag)
+    return
+
+  clovis_logger.info('Polling workers for tag: ' + tag)
+  live_instance_count = instance_helper.GetInstanceCount(tag)
+  clovis_logger.info('%i live instances for tag %s.' % (
+      live_instance_count, tag))
+
+  if live_instance_count > 0 or live_instance_count == -1:
+    clovis_logger.info('Retry later, instances still alive for tag: ' + tag)
+    poll_interval_minutes = 10
+    deferred.defer(PollWorkers, tag, start_time,
+                   _countdown=(60 * poll_interval_minutes))
+    return
+
+  clovis_logger.info('Scheduling instance group destruction for tag: ' + tag)
+  deferred.defer(DeleteInstanceGroup, tag)
+
+
 def CreateInstanceTemplate(task):
   """Create the Compute Engine instance template that will be used to create the
   instances.
@@ -48,7 +85,7 @@ def CreateInstanceTemplate(task):
 
 
 def CreateInstances(task):
-  """Creates the Compute engine requested by the task"""
+  """Creates the Compute engine requested by the task."""
   backend_params = task.BackendParams()
   instance_count = backend_params.get('instance_count', 0)
   if instance_count <= 0:
@@ -57,8 +94,36 @@ def CreateInstances(task):
   return instance_helper.CreateInstances(backend_params['tag'], instance_count)
 
 
+def DeleteInstanceGroup(tag, try_count=0):
+  """Deletes the instance group associated with tag, and schedules the deletion
+  of the instance template."""
+  clovis_logger.info('Instance group destruction for tag: ' + tag)
+  if not instance_helper.DeleteInstanceGroup(tag):
+    clovis_logger.info('Instance group destruction failed for: ' + tag)
+    if try_count <= 5:
+      deferred.defer(DeleteInstanceGroup, tag, try_count + 1, _countdown=60)
+      return
+    clovis_logger.error('Giving up group destruction for: ' + tag)
+  clovis_logger.info('Scheduling instance template destruction for tag: ' + tag)
+  # Wait a little before deleting the instance template, because it may still be
+  # considered in use, causing failures.
+  deferred.defer(DeleteInstanceTemplate, tag, _countdown=30)
+
+
+def DeleteInstanceTemplate(tag, try_count=0):
+  """Deletes the instance template associated with tag."""
+  clovis_logger.info('Instance template destruction for tag: ' + tag)
+  if not instance_helper.DeleteTemplate(tag):
+    clovis_logger.info('Instance template destruction failed for: ' + tag)
+    if try_count <= 5:
+      deferred.defer(DeleteInstanceTemplate, tag, try_count + 1, _countdown=60)
+      return
+    clovis_logger.error('Giving up template destruction for: ' + tag)
+  clovis_logger.info('Cleanup complete for tag: ' + tag)
+
+
 def StartFromJsonString(http_body_str):
-  """Main function handling a JSON task posted by the user"""
+  """Main function handling a JSON task posted by the user."""
   # Set up logging.
   memory_logs = MemoryLogs(clovis_logger)
   memory_logs.Start()
@@ -85,11 +150,18 @@ def StartFromJsonString(http_body_str):
     return Render(error_string, memory_logs)
 
   if not EnqueueTasks(sub_tasks, task_tag):
-    return Render('Task creation failed', memory_logs)
+    return Render('Task creation failed.', memory_logs)
 
   # Start the instances if required.
   if not CreateInstances(task):
-    return Render('Instance creation failed', memory_logs)
+    return Render('Instance creation failed.', memory_logs)
+
+  # Start polling the progress.
+  clovis_logger.info('Creating worker polling task.')
+  first_poll_delay_minutes = 10
+  timeout_hours = task.BackendParams().get('timeout_hours', 5)
+  deferred.defer(PollWorkers, task_tag, time.time(), timeout_hours,
+                 _countdown=(60 * first_poll_delay_minutes))
 
   return Render('Success', memory_logs)
 
@@ -139,7 +211,7 @@ def EnqueueTasks(tasks, task_tag):
   except Exception as e:
     clovis_logger.error('Exception:' + type(e).__name__ + ' ' + str(e.args))
     return False
-  clovis_logger.info('Pushed %i tasks with tag: %s' % (len(tasks), task_tag))
+  clovis_logger.info('Pushed %i tasks with tag: %s.' % (len(tasks), task_tag))
   return True
 
 
@@ -151,7 +223,7 @@ def Root():
 
 @app.route('/form_sent', methods=['POST'])
 def StartFromForm():
-  """HTML form endpoint"""
+  """HTML form endpoint."""
   data_stream = flask.request.files.get('json_task')
   if not data_stream:
     return 'failed'

commit e693c0735729a44fbaccbb7bb5c88af478b973f0
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 08:05:39 2016 -0700

    sandwich: Output further information in metrics' CSV
    
    To make sure there is not misinterpretation of the metrics
    from a sandwich CSV, this CL adds the network emulation column
    in the CSVs.
    
    This CL is also the opportunity to get ride of the sandwich's
    run_info.json that is completely replaceable with LoadingTrace's
    metadata dictionary.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1891183003
    
    Cr-Original-Commit-Position: refs/heads/master@{#390078}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 929aa5f0811b7e9a3c66d8b59fb4ff2b95f0a243

diff --git a/loading/controller.py b/loading/controller.py
index 1eb2f03..d8c20e0 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,6 +10,7 @@ desktop-specific versions.
 """
 
 import contextlib
+import copy
 import datetime
 import errno
 import logging
@@ -70,7 +71,7 @@ class ChromeControllerBase(object):
     self._wpr_attributes = None
     self._metadata = {}
     self._emulated_device = None
-    self._emulated_network = None
+    self._network_name = None
     self._slow_death = False
 
   def AddChromeArgument(self, arg):
@@ -115,10 +116,8 @@ class ChromeControllerBase(object):
       network_name: (str) Key from emulation.NETWORK_CONDITIONS or None to
         disable network emulation.
     """
-    if network_name:
-      self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
-    else:
-      self._emulated_network = None
+    assert network_name in emulation.NETWORK_CONDITIONS or network_name is None
+    self._network_name = network_name
 
   def PushBrowserCache(self, cache_path):
     """Pushes the HTTP chrome cache to the profile directory.
@@ -177,9 +176,17 @@ class ChromeControllerBase(object):
     if self._emulated_device:
       self._metadata.update(emulation.SetUpDeviceEmulationAndReturnMetadata(
           connection, self._emulated_device))
-    if self._emulated_network:
-      emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
-      self._metadata.update(self._emulated_network)
+    if self._network_name:
+      network_condition = emulation.NETWORK_CONDITIONS[self._network_name]
+      logging.info('Set up network emulation %s (latency=%dms, down=%d, up=%d)'
+          % (self._network_name, network_condition['latency'],
+              network_condition['download'], network_condition['upload']))
+      emulation.SetUpNetworkEmulation(connection, **network_condition)
+      self._metadata['network_emulation'] = copy.copy(network_condition)
+      self._metadata['network_emulation']['name'] = self._network_name
+    else:
+      self._metadata['network_emulation'] = \
+          {k: 'disabled' for k in ['name', 'download', 'upload', 'latency']}
     self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
                           seconds_since_epoch=time.time())
     logging.info('Devtools connection success')
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index bfbf00f..e8930e9 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -236,6 +236,7 @@ class DevToolsConnection(object):
                                  self._scoped_states[scoped_state][0])
     self._tearing_down_tracing = False
 
+    logging.info('Navigate to %s' % url)
     self.SendAndIgnoreResponse('Page.navigate', {'url': url})
 
     self._Dispatch(timeout=timeout_seconds)
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 55d61b4..6dda5d0 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -230,15 +230,15 @@ def _RunJobMain(args):
 
 
 def _ExtractMetricsMain(args):
-  trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
+  run_metrics_list = sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
       args.trace_output_directory)
-  trace_metrics_list.sort(key=lambda e: e['id'])
+  run_metrics_list.sort(key=lambda e: e['repeat_id'])
   with open(args.metrics_csv_path, 'w') as csv_file:
     writer = csv.DictWriter(csv_file,
                             fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
     writer.writeheader()
-    for trace_metrics in trace_metrics_list:
-      writer.writerow(trace_metrics)
+    for run_metrics in run_metrics_list:
+      writer.writerow(run_metrics)
   return 0
 
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index a30b327..1324b75 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -31,13 +31,17 @@ import tracing
 
 
 CSV_FIELD_NAMES = [
-    'id',
+    'repeat_id',
     'url',
     'total_load',
-    'onload',
+    'js_onload_event',
     'browser_malloc_avg',
     'browser_malloc_max',
-    'speed_index']
+    'speed_index',
+    'net_emul.name', # Should be in emulation.NETWORK_CONDITIONS.keys()
+    'net_emul.download',
+    'net_emul.upload',
+    'net_emul.latency']
 
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
@@ -126,14 +130,14 @@ def _GetWebPageTrackedEvents(tracing_track):
   return tracked_events
 
 
-def _PullMetricsFromLoadingTrace(loading_trace):
+def _ExtractMetricsFromLoadingTrace(loading_trace):
   """Pulls all the metrics from a given trace.
 
   Args:
     loading_trace: loading_trace_module.LoadingTrace.
 
   Returns:
-    Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
+    Dictionary with all trace extracted fields set.
   """
   assert all(
       cat in loading_trace.tracing_track.Categories()
@@ -156,8 +160,8 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   return {
     'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
                    web_page_tracked_events['requestStart'].start_msec),
-    'onload': (web_page_tracked_events['loadEventEnd'].start_msec -
-               web_page_tracked_events['loadEventStart'].start_msec),
+    'js_onload_event': (web_page_tracked_events['loadEventEnd'].start_msec -
+                        web_page_tracked_events['loadEventStart'].start_msec),
     'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
     'browser_malloc_max': browser_malloc_max
   }
@@ -222,15 +226,42 @@ def ComputeSpeedIndex(completeness_record):
   return speed_index
 
 
-def PullMetricsFromOutputDirectory(output_directory_path):
-  """Pulls all the metrics from all the traces of a sandwich run directory.
+def _ExtractMetricsFromRunDirectory(run_directory_path):
+  """Extracts all the metrics from traces and video of a sandwich run.
 
   Args:
-    output_directory_path: The sandwich run's output directory to pull the
+    run_directory_path: Path of the run directory.
+
+  Returns:
+    Dictionary of extracted metrics.
+  """
+  trace_path = os.path.join(run_directory_path, 'trace.json')
+  logging.info('processing trace \'%s\'' % trace_path)
+  loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
+  run_metrics = {'url': loading_trace.url}
+  run_metrics.update(_ExtractMetricsFromLoadingTrace(loading_trace))
+  video_path = os.path.join(run_directory_path, 'video.mp4')
+  if os.path.isfile(video_path):
+    logging.info('processing speed-index video \'%s\'' % video_path)
+    completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
+    run_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+  else:
+    run_metrics['speed_index'] = 'disabled'
+  for key, value in loading_trace.metadata['network_emulation'].iteritems():
+    run_metrics['net_emul.' + key] = value
+  return run_metrics
+
+
+def ExtractMetricsFromRunnerOutputDirectory(output_directory_path):
+  """Extracts all the metrics from all the traces of a sandwich runner output
+  directory.
+
+  Args:
+    output_directory_path: The sandwich runner's output directory to extract the
         metrics from.
 
   Returns:
-    List of dictionaries with all CSV_FIELD_NAMES's field set.
+    List of dictionaries.
   """
   assert os.path.isdir(output_directory_path)
   metrics = []
@@ -238,25 +269,14 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     if not os.path.isdir(os.path.join(output_directory_path, node_name)):
       continue
     try:
-      page_id = int(node_name)
+      repeat_id = int(node_name)
     except ValueError:
       continue
-    run_path = os.path.join(output_directory_path, node_name)
-    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
-    if not os.path.isfile(trace_path):
-      continue
-    logging.info('processing \'%s\'' % trace_path)
-    loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
-    row_metrics = {key: 'unavailable' for key in CSV_FIELD_NAMES}
-    row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
-    row_metrics['id'] = page_id
-    row_metrics['url'] = loading_trace.url
-    video_path = os.path.join(run_path, sandwich_runner.VIDEO_FILENAME)
-    if os.path.isfile(video_path):
-      logging.info('processing \'%s\'' % video_path)
-      completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
-      row_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
-    metrics.append(row_metrics)
-  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
-                            'run directory.').format(output_directory_path)
+    run_directory_path = os.path.join(output_directory_path, node_name)
+    run_metrics = _ExtractMetricsFromRunDirectory(run_directory_path)
+    run_metrics['repeat_id'] = repeat_id
+    assert set(run_metrics.keys()) == set(CSV_FIELD_NAMES)
+    metrics.append(run_metrics)
+  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich runner ' +
+                            'output directory.').format(output_directory_path)
   return metrics
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index d9c1700..73c8f3e 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -188,11 +188,11 @@ class PageTrackTest(unittest.TestCase):
     self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
 
   def testPullMetricsFromLoadingTrace(self):
-    metrics = puller._PullMetricsFromLoadingTrace(LoadingTrace(
+    metrics = puller._ExtractMetricsFromLoadingTrace(LoadingTrace(
         _MINIMALIST_TRACE_EVENTS))
     self.assertEquals(4, len(metrics))
     self.assertEquals(20, metrics['total_load'])
-    self.assertEquals(5, metrics['onload'])
+    self.assertEquals(5, metrics['js_onload_event'])
     self.assertEquals(30971, metrics['browser_malloc_avg'])
     self.assertEquals(55044, metrics['browser_malloc_max'])
 
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 571a76d..3a56b59 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -168,17 +168,6 @@ class SandwichRunner(object):
     else:
       _CleanPreviousTraces(self.trace_output_directory)
 
-  def _SaveRunInfos(self, urls):
-    assert self.trace_output_directory
-    run_infos = {
-      'cache-op': self.cache_operation,
-      'job_name': self.job_name,
-      'urls': urls
-    }
-    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
-              'w') as file_output:
-      json.dump(run_infos, file_output, indent=2)
-
   def _GetEmulatorNetworkCondition(self, emulator):
     if self.network_emulator == emulator:
       return self.network_condition
@@ -285,7 +274,5 @@ class SandwichRunner(object):
       self._local_cache_directory_path = None
     if self.cache_operation == 'save':
       self._PullCacheFromDevice()
-    if self.trace_output_directory:
-      self._SaveRunInfos(ran_urls)
 
     self._chrome_ctl = None
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 22572d2..954c67e 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -236,9 +236,10 @@ class SandwichTaskBuilder(task_manager.Builder):
     def ExtractMetrics():
       sandwich_misc.VerifyBenchmarkOutputDirectory(
           SetupBenchmark.path, RunBenchmark.path)
-      trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
-          RunBenchmark.path)
-      trace_metrics_list.sort(key=lambda e: e['id'])
+      trace_metrics_list = \
+          sandwich_metrics.ExtractMetricsFromRunnerOutputDirectory(
+              RunBenchmark.path)
+      trace_metrics_list.sort(key=lambda e: e['repeat_id'])
       with open(ExtractMetrics.path, 'w') as csv_file:
         writer = csv.DictWriter(csv_file,
                                 fieldnames=sandwich_metrics.CSV_FIELD_NAMES)

commit 707c46d99b18b2e4a4e8776a82e60f9c40cef5e3
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 07:07:35 2016 -0700

    sandwich: Add Desktop support
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1908173004
    
    Cr-Original-Commit-Position: refs/heads/master@{#390063}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3cab289c7572a4f94a078c9cc02d12fa48ad46b0

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 1322a66..c1b17d9 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -68,6 +68,16 @@ def GetFirstDevice():
   return devices[0]
 
 
+def GetDeviceFromSerial(android_device_serial):
+  """Returns the DeviceUtils instance."""
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  for device in devices:
+    if device.adb._device_serial == android_device_serial:
+      return device
+  raise DeviceSetupException(
+      'Device {} not found'.format(android_device_serial))
+
+
 def DeviceSubmitShellCommandQueue(device, command_queue):
   """Executes on the device a command queue.
 
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 30c3d65..55d61b4 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -30,6 +30,7 @@ import devil_chromium
 
 import chrome_cache
 import common_util
+import device_setup
 import emulation
 import options
 import sandwich_metrics
@@ -50,6 +51,9 @@ def _ArgumentParser():
   common_job_parser = argparse.ArgumentParser(add_help=False)
   common_job_parser.add_argument('--job', required=True,
                                  help='JSON file with job description.')
+  common_job_parser.add_argument('--android', default=None, type=str,
+                                 dest='android_device_serial',
+                                 help='Android device\'s serial to use.')
 
   task_parser = task_manager.CommandLineParser()
 
@@ -188,10 +192,18 @@ def _ArgumentParser():
   return parser
 
 
-def _RecordWprMain(args):
+def _CreateSandwichRunner(args):
   sandwich_runner = SandwichRunner()
   sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
+  if args.android_device_serial is not None:
+    sandwich_runner.android_device = \
+        device_setup.GetDeviceFromSerial(args.android_device_serial)
+  return sandwich_runner
+
+
+def _RecordWprMain(args):
+  sandwich_runner = _CreateSandwichRunner(args)
   sandwich_runner.wpr_record = True
   sandwich_runner.PrintConfig()
   if not os.path.isdir(os.path.dirname(args.wpr_archive_path)):
@@ -201,9 +213,7 @@ def _RecordWprMain(args):
 
 
 def _CreateCacheMain(args):
-  sandwich_runner = SandwichRunner()
-  sandwich_runner.LoadJob(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner = _CreateSandwichRunner(args)
   sandwich_runner.cache_operation = 'save'
   sandwich_runner.PrintConfig()
   if not os.path.isdir(os.path.dirname(args.cache_archive_path)):
@@ -213,9 +223,7 @@ def _CreateCacheMain(args):
 
 
 def _RunJobMain(args):
-  sandwich_runner = SandwichRunner()
-  sandwich_runner.LoadJob(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner = _CreateSandwichRunner(args)
   sandwich_runner.PrintConfig()
   sandwich_runner.Run()
   return 0
@@ -266,8 +274,13 @@ def _RecordWebServerTestTrace(args):
 
 
 def _RunAllMain(args):
+  android_device = None
+  if args.android_device_serial:
+    android_device = \
+        device_setup.GetDeviceFromSerial(args.android_device_serial)
   builder = sandwich_task_builder.SandwichTaskBuilder(
       output_directory=args.output,
+      android_device=android_device,
       job_path=args.job,
       url_repeat=args.url_repeat)
   if args.wpr_archive_path:
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 04520d5..5df6351 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -173,7 +173,7 @@ def _ListUrlRequests(trace, request_kind):
       continue
     if request_event.protocol.startswith('data'):
       continue
-    if request_event.protocol.startswith('http'):
+    if not request_event.protocol.startswith('http'):
       raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
     if (request_kind == _RequestOutcome.ServedFromCache and
         request_event.from_disk_cache):
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 0d6d105..571a76d 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -130,6 +130,9 @@ class SandwichRunner(object):
     # Configures whether the WPR archive should be read or generated.
     self.wpr_record = False
 
+    # The android DeviceUtils to run sandwich on or None to run it locally.
+    self.android_device = None
+
     self._chrome_ctl = None
     self._local_cache_directory_path = None
 
@@ -252,9 +255,10 @@ class SandwichRunner(object):
     if self.trace_output_directory:
       self._CleanTraceOutputDirectory()
 
-    # TODO(gabadie): Make sandwich working on desktop.
-    device = device_utils.DeviceUtils.HealthyDevices()[0]
-    self._chrome_ctl = controller.RemoteChromeController(device)
+    if self.android_device:
+      self._chrome_ctl = controller.RemoteChromeController(self.android_device)
+    else:
+      self._chrome_ctl = controller.LocalChromeController()
     self._chrome_ctl.AddChromeArgument('--disable-infobars')
     if self.cache_operation == 'save':
       self._chrome_ctl.SetSlowDeath()
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
index 1120d77..22572d2 100644
--- a/loading/sandwich_task_builder.py
+++ b/loading/sandwich_task_builder.py
@@ -37,16 +37,19 @@ class SandwichTaskBuilder(task_manager.Builder):
   """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
   """
 
-  def __init__(self, output_directory, job_path, url_repeat):
+  def __init__(self, output_directory, android_device, job_path, url_repeat):
     """Constructor.
 
     Args:
       output_directory: As in task_manager.Builder.__init__
+      android_device: The android DeviceUtils to run sandwich on or None to run
+        it locally.
       job_path: Path of the sandwich's job.
       url_repeat: Non null integer controlling how many times the URLs should be
         repeated in the benchmarks.
     """
     task_manager.Builder.__init__(self, output_directory)
+    self._android_device = android_device
     self._job_path = job_path
     self._url_repeat = url_repeat
     self._default_final_tasks = []
@@ -65,6 +68,7 @@ class SandwichTaskBuilder(task_manager.Builder):
     """Create a runner for non benchmark purposes."""
     runner = sandwich_runner.SandwichRunner()
     runner.LoadJob(self._job_path)
+    runner.android_device = self._android_device
     return runner
 
   def OverridePathToWprArchive(self, original_wpr_path):

commit 719ae362f823716ee81db19c1e3b09b094d3122f
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 05:41:57 2016 -0700

    tools/android/loading: Add HTTPS certificate authority support on Linux
    
    Before on Linux, Chrome was opened with the --ignore-certificate-errors
    command line argument to ignore HTTPS certificate failure and still load
    the web page. But the HTTPS resources were not getting cached by design,
    unabling sandwich to run on Linux.
    
    This CL launch Chrome with a different $HOME directory and use the
    certutil command line tool to fill up the standard $HOME/.pki/nssdb
    to let Chrome to trust the root certification authority's certificate
    of Web Page Replay for SSL connections.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1903773002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390053}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e4a8d81705b6a3cd3cfca976505e0481b9ae25ab

diff --git a/loading/common_util.py b/loading/common_util.py
index 350a418..c4eac2c 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -8,6 +8,7 @@ import logging
 import os
 import re
 import shutil
+import subprocess
 import sys
 import tempfile
 import time
@@ -80,11 +81,11 @@ def DeserializeAttributesFromJsonDict(json_dict, instance, attributes):
 
 
 @contextlib.contextmanager
-def TemporaryDirectory():
+def TemporaryDirectory(suffix='', prefix='tmp'):
   """Returns a freshly-created directory that gets automatically deleted after
   usage.
   """
-  name = tempfile.mkdtemp()
+  name = tempfile.mkdtemp(suffix=suffix, prefix=prefix)
   try:
     yield name
   finally:
@@ -96,3 +97,20 @@ def EnsureParentDirectoryExists(path):
   parent_directory_path = os.path.abspath(os.path.dirname(path))
   if not os.path.isdir(parent_directory_path):
     os.makedirs(parent_directory_path)
+
+
+def GetCommandLineForLogging(cmd, env_diff=None):
+  """Get command line string.
+
+  Args:
+    cmd: Command line argument
+    env_diff: Environment modification for the command line.
+
+  Returns:
+    Command line string.
+  """
+  cmd_str = ''
+  if env_diff:
+    for key, value in env_diff.iteritems():
+      cmd_str += '{}={} '.format(key, value)
+  return cmd_str + subprocess.list2cmdline(cmd)
diff --git a/loading/controller.py b/loading/controller.py
index 50595e4..1eb2f03 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -25,6 +25,7 @@ _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
 
 import chrome_cache
+import common_util
 import device_setup
 import devtools_monitor
 import emulation
@@ -66,7 +67,7 @@ class ChromeControllerBase(object):
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
     ]
-    self._chrome_wpr_specific_args = []
+    self._wpr_attributes = None
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
@@ -185,7 +186,10 @@ class ChromeControllerBase(object):
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
-    return self._chrome_args + self._chrome_wpr_specific_args
+    chrome_args = self._chrome_args[:]
+    if self._wpr_attributes:
+      chrome_args.extend(self._wpr_attributes.chrome_args)
+    return chrome_args
 
 
 class RemoteChromeController(ChromeControllerBase):
@@ -212,6 +216,9 @@ class RemoteChromeController(ChromeControllerBase):
   @contextlib.contextmanager
   def Open(self):
     """Overridden connection creation."""
+    if self._wpr_attributes:
+      assert self._wpr_attributes.chrome_env_override == {}, \
+          'Remote controller doesn\'t support chrome environment variables.'
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
     self._device.ForceStop(package_info.package)
@@ -268,15 +275,15 @@ class RemoteChromeController(ChromeControllerBase):
                   disable_script_injection=False,
                   out_log_path=None):
     """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
-    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    assert not self._wpr_attributes, 'WPR is already running.'
     with device_setup.RemoteWprHost(self._device, wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection,
-        out_log_path=out_log_path) as additional_flags:
-      self._chrome_wpr_specific_args = additional_flags
+        out_log_path=out_log_path) as wpr_attributes:
+      self._wpr_attributes = wpr_attributes
       yield
-    self._chrome_wpr_specific_args = []
+    self._wpr_attributes = None
 
 
 class LocalChromeController(ChromeControllerBase):
@@ -316,16 +323,26 @@ class LocalChromeController(ChromeControllerBase):
     #   - To find the correct target descriptor at devtool connection;
     #   - To avoid cache and WPR pollution by the NTP.
     chrome_cmd.append('about:blank')
-    environment = os.environ.copy()
+
+    chrome_env_override = {}
+    if self._wpr_attributes:
+      chrome_env_override.update(self._wpr_attributes.chrome_env_override)
+
     if self._headless:
-      environment['DISPLAY'] = 'localhost:99'
-      xvfb_process = subprocess.Popen(
-          ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
-          stdout=stdout, stderr=stderr)
-    logging.debug(subprocess.list2cmdline(chrome_cmd))
-    chrome_process = subprocess.Popen(chrome_cmd, shell=False,
-                                      stdout=stdout, stderr=stderr,
-                                      env=environment)
+      assert 'DISPLAY' not in chrome_env_override, \
+          'DISPLAY environment variable is reserved for headless.'
+      chrome_env_override['DISPLAY'] = 'localhost:99'
+      xvfb_cmd = ['Xvfb', ':99', '-screen', '0', '1600x1200x24']
+      logging.info(common_util.GetCommandLineForLogging(xvfb_cmd))
+      xvfb_process = subprocess.Popen(xvfb_cmd, stdout=stdout, stderr=stderr)
+
+    # Launch chrome.
+    logging.info(
+        common_util.GetCommandLineForLogging(chrome_cmd, chrome_env_override))
+    chrome_env = os.environ.copy()
+    chrome_env.update(chrome_env_override)
+    chrome_process = subprocess.Popen(chrome_cmd, stdout=stdout, stderr=stderr,
+                                      env=chrome_env)
     connection = None
     try:
       # Attempt to connect to Chrome's devtools
@@ -388,15 +405,15 @@ class LocalChromeController(ChromeControllerBase):
                   disable_script_injection=False,
                   out_log_path=None):
     """Override for WPR context."""
-    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    assert not self._wpr_attributes, 'WPR is already running.'
     with device_setup.LocalWprHost(wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection,
-        out_log_path=out_log_path) as additional_flags:
-      self._chrome_wpr_specific_args = additional_flags
+        out_log_path=out_log_path) as wpr_attributes:
+      self._wpr_attributes = wpr_attributes
       yield
-    self._chrome_wpr_specific_args = []
+    self._wpr_attributes = None
 
   def _EnsureProfileDirectory(self):
     if (not os.path.isdir(self._profile_dir) or
diff --git a/loading/device_setup.py b/loading/device_setup.py
index d1abaff..1322a66 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import collections
 import contextlib
 import logging
 import os
@@ -14,7 +15,8 @@ import time
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
 
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+_CATAPULT_DIR = os.path.join(_SRC_DIR, 'third_party', 'catapult')
+sys.path.append(os.path.join(_CATAPULT_DIR, 'devil'))
 from devil.android import device_utils
 from devil.android import flag_changer
 from devil.android import forwarder
@@ -31,10 +33,12 @@ sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.image_processing import video
 from telemetry.internal.util import webpagereplay
 
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
+sys.path.append(os.path.join(
+    _CATAPULT_DIR, 'telemetry', 'third_party', 'webpagereplay'))
 import adb_install_cert
 import certutils
 
+import common_util
 import devtools_monitor
 import emulation
 import options
@@ -127,6 +131,17 @@ def ForwardPort(device, local, remote):
     device.adb.ForwardRemove(local)
 
 
+# WPR specific attributes to set up chrome.
+#
+# Members:
+#   chrome_args: Additional flags list that may be used for chromium to load web
+#     page through the running web page replay host.
+#   chrome_env_override: Dictionary of environment variables to override at
+#     Chrome's launch time.
+WprAttribute = collections.namedtuple('WprAttribute',
+                                      ['chrome_args', 'chrome_env_override'])
+
+
 @contextlib.contextmanager
 def _WprHost(wpr_archive_path, record=False,
              network_condition_name=None,
@@ -222,24 +237,33 @@ def LocalWprHost(wpr_archive_path, record=False,
     out_log_path: Path of the WPR host's log.
 
   Returns:
-    Additional flags list that may be used for chromium to load web page through
-    the running web page replay host.
+    WprAttribute
   """
   if wpr_archive_path == None:
     _VerifySilentWprHost(record, network_condition_name)
     yield []
     return
-  with _WprHost(
-      wpr_archive_path,
-      record=record,
-      network_condition_name=network_condition_name,
-      disable_script_injection=disable_script_injection,
-      out_log_path=out_log_path) as (http_port, https_port):
-    chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
-                                                     escape=False)
-    # Certification authority is handled only available on Android.
-    chrome_args.append('--ignore-certificate-errors')
-    yield chrome_args
+
+  with common_util.TemporaryDirectory() as temp_home_dir:
+    # Generate a root certification authority certificate for WPR.
+    private_ca_cert_path = os.path.join(temp_home_dir, 'wpr.pem')
+    ca_cert_path = os.path.join(temp_home_dir, 'wpr-cert.pem')
+    certutils.write_dummy_ca_cert(*certutils.generate_dummy_ca_cert(),
+                                  cert_path=private_ca_cert_path)
+    assert os.path.isfile(ca_cert_path)
+    certutils.install_cert_in_nssdb(temp_home_dir, ca_cert_path)
+
+    with _WprHost(
+        wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection,
+        wpr_ca_cert_path=private_ca_cert_path,
+        out_log_path=out_log_path) as (http_port, https_port):
+      chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
+                                                       escape=False)
+      yield WprAttribute(chrome_args=chrome_args,
+                         chrome_env_override={'HOME': temp_home_dir})
 
 
 @contextlib.contextmanager
@@ -260,8 +284,7 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
     out_log_path: Path of the WPR host's log.
 
   Returns:
-    Additional flags list that may be used for chromium to load web page through
-    the running web page replay host.
+    WprAttribute
   """
   assert device
   if wpr_archive_path == None:
@@ -290,9 +313,10 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
       device_http_port = forwarder.Forwarder.DevicePortForHostPort(http_port)
       device_https_port = forwarder.Forwarder.DevicePortForHostPort(https_port)
       try:
-        yield _FormatWPRRelatedChromeArgumentFor(device_http_port,
-                                                 device_https_port,
-                                                 escape=True)
+        chrome_args = _FormatWPRRelatedChromeArgumentFor(device_http_port,
+                                                         device_https_port,
+                                                         escape=True)
+        yield WprAttribute(chrome_args=chrome_args, chrome_env_override={})
       finally:
         # Tear down the forwarder.
         forwarder.Forwarder.UnmapDevicePort(device_http_port, device)

commit ca71a96319c6ee52f1ed473713ee9a0049380431
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 04:49:40 2016 -0700

    sandwich: Implement SandwichTaskBuilder
    
    Sandwich automation is moving to the task_manager API. This CL
    implements the SandwichTaskBuilder that builds the sandwich's
    task dependency graph.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1872313002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390048}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e88c3b13065fac6f02ec54b95c3fe66ee89f0c37

diff --git a/loading/common_util.py b/loading/common_util.py
index 855284f..350a418 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -89,3 +89,10 @@ def TemporaryDirectory():
     yield name
   finally:
     shutil.rmtree(name)
+
+
+def EnsureParentDirectoryExists(path):
+  """Verifies that the parent directory exists or creates it if missing."""
+  parent_directory_path = os.path.abspath(os.path.dirname(path))
+  if not os.path.isdir(parent_directory_path):
+    os.makedirs(parent_directory_path)
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 5f61aac..30c3d65 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -35,6 +35,8 @@ import options
 import sandwich_metrics
 import sandwich_misc
 from sandwich_runner import SandwichRunner
+import sandwich_task_builder
+import task_manager
 from trace_test.webserver_test import WebServer
 
 
@@ -49,6 +51,8 @@ def _ArgumentParser():
   common_job_parser.add_argument('--job', required=True,
                                  help='JSON file with job description.')
 
+  task_parser = task_manager.CommandLineParser()
+
   # Plumbing parser to configure OPTIONS.
   plumbing_parser = OPTIONS.GetParentParser('plumbing options')
 
@@ -166,6 +170,21 @@ def _ArgumentParser():
   record_trace_parser.add_argument('-o', '--output', type=str, required=True,
                                    help='Output path of the generated trace.')
 
+  # Run all subcommand.
+  run_all = subparsers.add_parser('run-all',
+                       parents=[common_job_parser, task_parser],
+                       help='Run all steps using the task manager '
+                            'infrastructure.')
+  run_all.add_argument('-g', '--gen-full', action='store_true',
+                       help='Generate the full graph with all possible'
+                            'benchmarks.')
+  run_all.add_argument('--wpr-archive', default=None, type=str,
+                       dest='wpr_archive_path',
+                       help='WebPageReplay archive to use, instead of '
+                            'generating one.')
+  run_all.add_argument('--url-repeat', default=1, type=int,
+                       help='How many times to repeat the urls.')
+
   return parser
 
 
@@ -240,10 +259,45 @@ def _RecordWebServerTestTrace(args):
       address = server.Address()
       sandwich_runner.urls = ['http://%s/%s' % (address, args.page)]
       sandwich_runner.Run()
-    shutil.copy(os.path.join(out_path, 'run', '0', 'trace.json'), args.output)
+    trace_path = os.path.join(
+        out_path, 'run', '0', sandwich_runner.TRACE_FILENAME)
+    shutil.copy(trace_path, args.output)
   return 0
 
 
+def _RunAllMain(args):
+  builder = sandwich_task_builder.SandwichTaskBuilder(
+      output_directory=args.output,
+      job_path=args.job,
+      url_repeat=args.url_repeat)
+  if args.wpr_archive_path:
+    builder.OverridePathToWprArchive(args.wpr_archive_path)
+  else:
+    builder.PopulateWprRecordingTask()
+  builder.PopulateCommonPipelines()
+
+  runner_transformer_name = 'no-network-emulation'
+  runner_transformer = lambda arg: None
+  builder.PopulateLoadBenchmark(sandwich_misc.EMPTY_CACHE_DISCOVERER,
+                                runner_transformer_name, runner_transformer)
+  builder.PopulateLoadBenchmark(sandwich_misc.FULL_CACHE_DISCOVERER,
+                                runner_transformer_name, runner_transformer)
+
+  if args.gen_full:
+    for subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS:
+      if subresource_discoverer == sandwich_misc.FULL_CACHE_DISCOVERER:
+        continue
+      for network_condition in ['Regular4G', 'Regular3G', 'Regular2G']:
+        runner_transformer_name = network_condition.lower()
+        runner_transformer = sandwich_task_builder.NetworkSimulationTransformer(
+            network_condition)
+        builder.PopulateLoadBenchmark(
+            subresource_discoverer, runner_transformer_name, runner_transformer)
+
+  return task_manager.ExecuteWithCommandLine(
+      args, builder.tasks.values(), builder.default_final_tasks)
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -266,6 +320,8 @@ def main(command_line_args):
     return _FilterCacheMain(args)
   if args.subcommand == 'record-test-trace':
     return _RecordWebServerTestTrace(args)
+  if args.subcommand == 'run-all':
+    return _RunAllMain(args)
   assert False
 
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 98d6317..a30b327 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -26,13 +26,10 @@ from telemetry.util import image_util
 from telemetry.util import rgba_color
 
 import loading_trace as loading_trace_module
+import sandwich_runner
 import tracing
 
 
-# List of selected trace event categories when running chrome.
-ADDITIONAL_CATEGORIES = (
-    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
-
 CSV_FIELD_NAMES = [
     'id',
     'url',
@@ -140,7 +137,7 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   """
   assert all(
       cat in loading_trace.tracing_track.Categories()
-      for cat in ADDITIONAL_CATEGORIES), (
+      for cat in sandwich_runner.ADDITIONAL_CATEGORIES), (
           'This trace was not generated with the required set of categories '
           'to be processed by this script.')
   browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
@@ -245,7 +242,7 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     except ValueError:
       continue
     run_path = os.path.join(output_directory_path, node_name)
-    trace_path = os.path.join(run_path, 'trace.json')
+    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
     if not os.path.isfile(trace_path):
       continue
     logging.info('processing \'%s\'' % trace_path)
@@ -254,7 +251,7 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
     row_metrics['id'] = page_id
     row_metrics['url'] = loading_trace.url
-    video_path = os.path.join(run_path, 'video.mp4')
+    video_path = os.path.join(run_path, sandwich_runner.VIDEO_FILENAME)
     if os.path.isfile(video_path):
       logging.info('processing \'%s\'' % video_path)
       completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 1976462..d9c1700 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -13,6 +13,7 @@ import unittest
 import loading_trace
 import page_track
 import sandwich_metrics as puller
+import sandwich_runner
 import request_track
 import tracing
 
@@ -46,7 +47,8 @@ _MINIMALIST_TRACE_EVENTS = [
 def TracingTrack(events):
   return tracing.TracingTrack.FromJsonDict({
       'events': events,
-      'categories': tracing.INITIAL_CATEGORIES + puller.ADDITIONAL_CATEGORIES})
+      'categories': (tracing.INITIAL_CATEGORIES +
+          sandwich_runner.ADDITIONAL_CATEGORIES)})
 
 
 def LoadingTrace(events):
@@ -105,7 +107,7 @@ class PageTrackTest(unittest.TestCase):
         {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
         {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
-    self.assertTrue(_MEM_CAT in puller.ADDITIONAL_CATEGORIES)
+    self.assertTrue(_MEM_CAT in sandwich_runner.ADDITIONAL_CATEGORIES)
 
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
@@ -232,7 +234,7 @@ class PageTrackTest(unittest.TestCase):
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
       LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
-          os.path.join(tmp_dir, dirname, 'trace.json'))
+          os.path.join(tmp_dir, dirname, sandwich_runner.TRACE_FILENAME))
 
     process = subprocess.Popen(['python', puller.__file__, tmp_dir])
     process.wait()
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index e7b00c6..04520d5 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -3,14 +3,24 @@
 # found in the LICENSE file.
 
 import logging
+import json
+import os
 
+import chrome_cache
+import common_util
 from loading_trace import LoadingTrace
 from prefetch_view import PrefetchSimulationView
 from request_dependencies_lens import RequestDependencyLens
-from user_satisfied_lens import FirstContentfulPaintLens
+import sandwich_runner
 import wpr_backend
 
 
+# Do not prefetch anything.
+EMPTY_CACHE_DISCOVERER = 'empty-cache'
+
+# Prefetches everything to load fully from cache (impossible in practice).
+FULL_CACHE_DISCOVERER = 'full-cache'
+
 # Prefetches the first resource following the redirection chain.
 REDIRECTED_MAIN_DISCOVERER = 'redirected-main'
 
@@ -21,6 +31,8 @@ PARSER_DISCOVERER = 'parser'
 HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner'
 
 SUBRESOURCE_DISCOVERERS = set([
+  EMPTY_CACHE_DISCOVERER,
+  FULL_CACHE_DISCOVERER,
   REDIRECTED_MAIN_DISCOVERER,
   PARSER_DISCOVERER,
   HTML_PRELOAD_SCANNER_DISCOVERER
@@ -85,7 +97,11 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
 
   # Build the list of discovered requests according to the desired simulation.
   discovered_requests = []
-  if subresource_discoverer == REDIRECTED_MAIN_DISCOVERER:
+  if subresource_discoverer == EMPTY_CACHE_DISCOVERER:
+    pass
+  elif subresource_discoverer == FULL_CACHE_DISCOVERER:
+    discovered_requests = trace.request_track.GetEvents()
+  elif subresource_discoverer == REDIRECTED_MAIN_DISCOVERER:
     discovered_requests = \
         [dependencies_lens.GetRedirectChain(first_resource_request)[-1]]
   elif subresource_discoverer == PARSER_DISCOVERER:
@@ -100,7 +116,6 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
   # Prune out data:// requests.
   whitelisted_urls = set()
   logging.info('white-listing %s' % first_resource_request.url)
-  whitelisted_urls.add(first_resource_request.url)
   for request in discovered_requests:
     # Work-around where the protocol may be none for an unclear reason yet.
     # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
@@ -114,3 +129,147 @@ def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
     logging.info('white-listing %s' % request.url)
     whitelisted_urls.add(request.url)
   return whitelisted_urls
+
+
+def _PrintUrlSetComparison(ref_url_set, url_set, url_set_name):
+  """Compare URL sets and log the diffs.
+
+  Args:
+    ref_url_set: Set of reference urls.
+    url_set: Set of urls to compare to the reference.
+    url_set_name: The set name for logging purposes.
+  """
+  assert type(ref_url_set) == set
+  assert type(url_set) == set
+  if ref_url_set == url_set:
+    logging.info('  %d %s are matching.' % (len(ref_url_set), url_set_name))
+    return
+  logging.error('  %s are not matching.' % url_set_name)
+  logging.error('    List of missing resources:')
+  for url in ref_url_set.difference(url_set):
+    logging.error('-     ' + url)
+  logging.error('    List of unexpected resources:')
+  for url in url_set.difference(ref_url_set):
+    logging.error('+     ' + url)
+
+
+class _RequestOutcome:
+  All, ServedFromCache, NotServedFromCache = range(3)
+
+
+def _ListUrlRequests(trace, request_kind):
+  """Lists requested URLs from a trace.
+
+  Args:
+    trace: (LoadingTrace) loading trace.
+    request_kind: _RequestOutcome indicating the subset of requests to output.
+
+  Returns:
+    set([str])
+  """
+  urls = set()
+  for request_event in trace.request_track.GetEvents():
+    if request_event.protocol == None:
+      continue
+    if request_event.protocol.startswith('data'):
+      continue
+    if request_event.protocol.startswith('http'):
+      raise RuntimeError('Unknown protocol {}'.format(request_event.protocol))
+    if (request_kind == _RequestOutcome.ServedFromCache and
+        request_event.from_disk_cache):
+      urls.add(request_event.url)
+    elif (request_kind == _RequestOutcome.NotServedFromCache and
+        not request_event.from_disk_cache):
+      urls.add(request_event.url)
+    elif request_kind == _RequestOutcome.All:
+      urls.add(request_event.url)
+  return urls
+
+
+def VerifyBenchmarkOutputDirectory(benchmark_setup_path,
+                                   benchmark_output_directory_path):
+  """Verifies that all run inside the run_output_directory worked as expected.
+
+  Args:
+    benchmark_setup_path: Path of the JSON of the benchmark setup.
+    benchmark_output_directory_path: Path of the benchmark output directory to
+        verify.
+  """
+  # TODO(gabadie): What's the best way of propagating errors happening in here?
+  benchmark_setup = json.load(open(benchmark_setup_path))
+  cache_whitelist = set(benchmark_setup['cache_whitelist'])
+  url_resources = set(benchmark_setup['url_resources'])
+
+  # Verify requests from traces.
+  run_id = -1
+  while True:
+    run_id += 1
+    run_path = os.path.join(benchmark_output_directory_path, str(run_id))
+    if not os.path.isdir(run_path):
+      break
+    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
+    if not os.path.isfile(trace_path):
+      logging.error('missing trace %s' % trace_path)
+      continue
+    trace = LoadingTrace.FromJsonFile(trace_path)
+    logging.info('verifying %s from %s' % (trace.url, trace_path))
+    _PrintUrlSetComparison(url_resources,
+        _ListUrlRequests(trace, _RequestOutcome.All), 'All resources')
+    _PrintUrlSetComparison(url_resources.intersection(cache_whitelist),
+        _ListUrlRequests(trace, _RequestOutcome.ServedFromCache),
+        'Cached resources')
+    _PrintUrlSetComparison(url_resources.difference(cache_whitelist),
+        _ListUrlRequests(trace, _RequestOutcome.NotServedFromCache),
+        'Non cached resources')
+
+
+def ReadSubresourceMapFromBenchmarkOutput(benchmark_output_directory_path):
+  """Extracts a map URL-to-subresources for each navigation in benchmark
+  directory.
+
+  Args:
+    benchmark_output_directory_path: Path of the benchmark output directory to
+        verify.
+
+  Returns:
+    {url -> [URLs of sub-resources]}
+  """
+  url_subresources = {}
+  run_id = -1
+  while True:
+    run_id += 1
+    run_path = os.path.join(benchmark_output_directory_path, str(run_id))
+    if not os.path.isdir(run_path):
+      break
+    trace_path = os.path.join(run_path, sandwich_runner.TRACE_FILENAME)
+    if not os.path.isfile(trace_path):
+      continue
+    trace = LoadingTrace.FromJsonFile(trace_path)
+    if trace.url in url_subresources:
+      continue
+    logging.info('lists resources of %s from %s' % (trace.url, trace_path))
+    urls_set = set()
+    for request_event in trace.request_track.GetEvents():
+      if not request_event.protocol.startswith('http'):
+        continue
+      if request_event.url not in urls_set:
+        logging.info('  %s' % request_event.url)
+        urls_set.add(request_event.url)
+    url_subresources[trace.url] = [url for url in urls_set]
+  return url_subresources
+
+
+def ValidateCacheArchiveContent(ref_urls, cache_archive_path):
+  """Validates a cache archive content.
+
+  Args:
+    ref_urls: Reference list of urls.
+    cache_archive_path: Cache archive's path to validate.
+  """
+  # TODO(gabadie): What's the best way of propagating errors happening in here?
+  logging.info('lists cached urls from %s' % cache_archive_path)
+  with common_util.TemporaryDirectory() as cache_directory:
+    chrome_cache.UnzipDirectoryContent(cache_archive_path, cache_directory)
+    cached_urls = \
+        chrome_cache.CacheBackend(cache_directory, 'simple').ListKeys()
+  _PrintUrlSetComparison(set(ref_urls), set(cached_urls), 'cached resources')
diff --git a/loading/sandwich_misc_unittest.py b/loading/sandwich_misc_unittest.py
index f75288c..9e5e455 100644
--- a/loading/sandwich_misc_unittest.py
+++ b/loading/sandwich_misc_unittest.py
@@ -19,26 +19,40 @@ class SandwichMiscTest(unittest.TestCase):
   def GetResourceUrl(self, path):
     return urlparse.urljoin('http://l/', path)
 
+  def testNoDiscovererWhitelisting(self):
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.EMPTY_CACHE_DISCOVERER)
+    self.assertEquals(set(), url_set)
+
+  def testFullCacheWhitelisting(self):
+    reference_url_set = set([self.GetResourceUrl('./'),
+                             self.GetResourceUrl('0.png'),
+                             self.GetResourceUrl('1.png'),
+                             self.GetResourceUrl('favicon.ico')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.FULL_CACHE_DISCOVERER)
+    self.assertEquals(reference_url_set, url_set)
+
   def testRedirectedMainWhitelisting(self):
-    urls_set_ref = set([self.GetResourceUrl('./')])
-    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+    reference_url_set = set([self.GetResourceUrl('./')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
         self._TRACE_PATH, sandwich_misc.REDIRECTED_MAIN_DISCOVERER)
-    self.assertEquals(urls_set_ref, urls_set)
+    self.assertEquals(reference_url_set, url_set)
 
   def testParserDiscoverableWhitelisting(self):
-    urls_set_ref = set([self.GetResourceUrl('./'),
-                        self.GetResourceUrl('0.png'),
-                        self.GetResourceUrl('1.png')])
-    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+    reference_url_set = set([self.GetResourceUrl('./'),
+                             self.GetResourceUrl('0.png'),
+                             self.GetResourceUrl('1.png')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
         self._TRACE_PATH, sandwich_misc.PARSER_DISCOVERER)
-    self.assertEquals(urls_set_ref, urls_set)
+    self.assertEquals(reference_url_set, url_set)
 
   def testHTMLPreloadScannerWhitelisting(self):
-    urls_set_ref = set([self.GetResourceUrl('./'),
-                        self.GetResourceUrl('0.png')])
-    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+    reference_url_set = set([self.GetResourceUrl('./'),
+                             self.GetResourceUrl('0.png')])
+    url_set = sandwich_misc.ExtractDiscoverableUrls(
         self._TRACE_PATH, sandwich_misc.HTML_PRELOAD_SCANNER_DISCOVERER)
-    self.assertEquals(urls_set_ref, urls_set)
+    self.assertEquals(reference_url_set, url_set)
 
 
 if __name__ == '__main__':
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 39bf4d6..0d6d105 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -20,9 +20,16 @@ import controller
 import devtools_monitor
 import device_setup
 import loading_trace
-import sandwich_metrics
 
 
+# Standard filenames in the sandwich runner's output directory.
+TRACE_FILENAME = 'trace.json'
+VIDEO_FILENAME = 'video.mp4'
+
+# List of selected trace event categories when running chrome.
+ADDITIONAL_CATEGORIES = (
+    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
+
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
 # Devtools timeout of 1 minute to avoid websocket timeout on slow
@@ -197,24 +204,24 @@ class SandwichRunner(object):
       if run_path is not None and self.record_video:
         device = self._chrome_ctl.GetDevice()
         assert device, 'Can only record video on a remote device.'
-        video_recording_path = os.path.join(run_path, 'video.mp4')
+        video_recording_path = os.path.join(run_path, VIDEO_FILENAME)
         with device_setup.RemoteSpeedIndexRecorder(device, connection,
                                                    video_recording_path):
           trace = loading_trace.LoadingTrace.RecordUrlNavigation(
               url=url,
               connection=connection,
               chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
+              additional_categories=ADDITIONAL_CATEGORIES,
               timeout_seconds=_DEVTOOLS_TIMEOUT)
       else:
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
             url=url,
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
+            additional_categories=ADDITIONAL_CATEGORIES,
             timeout_seconds=_DEVTOOLS_TIMEOUT)
     if run_path is not None:
-      trace_path = os.path.join(run_path, 'trace.json')
+      trace_path = os.path.join(run_path, TRACE_FILENAME)
       trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, run_id):
diff --git a/loading/sandwich_task_builder.py b/loading/sandwich_task_builder.py
new file mode 100644
index 0000000..1120d77
--- /dev/null
+++ b/loading/sandwich_task_builder.py
@@ -0,0 +1,246 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import csv
+import json
+import os
+import shutil
+
+import chrome_cache
+import common_util
+import emulation
+import sandwich_metrics
+import sandwich_misc
+import sandwich_runner
+import task_manager
+
+
+def NetworkSimulationTransformer(network_condition):
+  """Creates a function that accepts a SandwichRunner as a parameter and sets
+  network emulation options on it.
+
+  Args:
+    network_condition: The network condition to apply to the sandwich runner.
+
+  Returns:
+    A callback transforming the SandwichRunner given in argument accordingly
+  """
+  assert network_condition in emulation.NETWORK_CONDITIONS
+  def Transformer(runner):
+    assert isinstance(runner, sandwich_runner.SandwichRunner)
+    runner.network_condition = network_condition
+  return Transformer
+
+
+class SandwichTaskBuilder(task_manager.Builder):
+  """A builder for a graph of tasks, each prepares or invokes a SandwichRunner.
+  """
+
+  def __init__(self, output_directory, job_path, url_repeat):
+    """Constructor.
+
+    Args:
+      output_directory: As in task_manager.Builder.__init__
+      job_path: Path of the sandwich's job.
+      url_repeat: Non null integer controlling how many times the URLs should be
+        repeated in the benchmarks.
+    """
+    task_manager.Builder.__init__(self, output_directory)
+    self._job_path = job_path
+    self._url_repeat = url_repeat
+    self._default_final_tasks = []
+
+    self._original_wpr_task = None
+    self._patched_wpr_task = None
+    self._reference_cache_task = None
+    self._subresources_for_urls_run_task = None
+    self._subresources_for_urls_task = None
+
+  @property
+  def default_final_tasks(self):
+      return self._default_final_tasks
+
+  def _CreateSandwichRunner(self):
+    """Create a runner for non benchmark purposes."""
+    runner = sandwich_runner.SandwichRunner()
+    runner.LoadJob(self._job_path)
+    return runner
+
+  def OverridePathToWprArchive(self, original_wpr_path):
+    """Sets the original WPR archive path's to be used.
+
+    Args:
+      original_wpr_path: Path of the original WPR archive to be used.
+    """
+    self._original_wpr_task = \
+        self.CreateStaticTask('common/webpages.wpr', original_wpr_path)
+
+  def PopulateWprRecordingTask(self):
+    """Records the original WPR archive."""
+    @self.RegisterTask('common/webpages.wpr')
+    def BuildOriginalWpr():
+      common_util.EnsureParentDirectoryExists(BuildOriginalWpr.path)
+      runner = self._CreateSandwichRunner()
+      runner.wpr_archive_path = BuildOriginalWpr.path
+      runner.wpr_record = True
+      runner.Run()
+
+    self._original_wpr_task = BuildOriginalWpr
+
+  def PopulateCommonPipelines(self):
+    """Creates necessary tasks to produce initial cache archive.
+
+    Also creates a task for producing a json file with a mapping of URLs to
+    subresources (urls-resources.json).
+
+    Here is the full dependency tree for the returned task:
+    common/cache-ref-validation.log
+      depends on: common/cache-ref.zip
+        depends on: common/webpages-patched.wpr
+          depends on: common/webpages.wpr
+      depends on: common/urls-resources.json
+        depends on: common/urls-resources-run/
+          depends on: common/webpages.wpr
+
+    Returns:
+      The last task of the pipeline.
+    """
+    @self.RegisterTask('common/webpages-patched.wpr', [self._original_wpr_task])
+    def BuildPatchedWpr():
+      common_util.EnsureParentDirectoryExists(BuildPatchedWpr.path)
+      shutil.copyfile(self._original_wpr_task.path, BuildPatchedWpr.path)
+      sandwich_misc.PatchWpr(BuildPatchedWpr.path)
+
+    @self.RegisterTask('common/cache-ref.zip', [BuildPatchedWpr])
+    def BuildReferenceCache():
+      runner = self._CreateSandwichRunner()
+      runner.wpr_archive_path = BuildPatchedWpr.path
+      runner.cache_archive_path = BuildReferenceCache.path
+      runner.cache_operation = 'save'
+      runner.Run()
+
+    @self.RegisterTask('common/subresources-for-urls-run/',
+                       dependencies=[self._original_wpr_task])
+    def UrlsResourcesRun():
+      runner = self._CreateSandwichRunner()
+      runner.wpr_archive_path = self._original_wpr_task.path
+      runner.cache_operation = 'clear'
+      runner.trace_output_directory = UrlsResourcesRun.path
+      runner.Run()
+
+    @self.RegisterTask('common/subresources-for-urls.json', [UrlsResourcesRun])
+    def ListUrlsResources():
+      json_content = sandwich_misc.ReadSubresourceMapFromBenchmarkOutput(
+          UrlsResourcesRun.path)
+      with open(ListUrlsResources.path, 'w') as output:
+        json.dump(json_content, output)
+
+    @self.RegisterTask('common/cache-ref-validation.log',
+                       [BuildReferenceCache, ListUrlsResources])
+    def ValidateReferenceCache():
+      json_content = json.load(open(ListUrlsResources.path))
+      ref_urls = set()
+      for urls in json_content.values():
+        ref_urls.update(set(urls))
+      sandwich_misc.ValidateCacheArchiveContent(
+          ref_urls, BuildReferenceCache.path)
+
+    self._patched_wpr_task = BuildPatchedWpr
+    self._reference_cache_task = BuildReferenceCache
+    self._subresources_for_urls_run_task = UrlsResourcesRun
+    self._subresources_for_urls_task = ListUrlsResources
+
+    self._default_final_tasks.append(ValidateReferenceCache)
+    return ValidateReferenceCache
+
+  def PopulateLoadBenchmark(self, subresource_discoverer,
+                            runner_transformer_name, runner_transformer):
+    """Populate benchmarking tasks from its setup tasks.
+
+    Args:
+      subresource_discoverer: Name of a subresources discoverer.
+      runner_transformer: A function that takes an instance of SandwichRunner as
+          parameter, would be applied immediately before SandwichRunner.Run().
+      runner_transformer_name: Name of the runner transformer used to generate
+          task names.
+      benchmark_name: The benchmark's name for that runner modifier.
+
+    Here is the full dependency of the added tree for the returned task:
+    <runner_transformer_name>/<subresource_discoverer>-metrics.csv
+      depends on: <runner_transformer_name>/<subresource_discoverer>-run/
+        depends on: common/<subresource_discoverer>-cache.zip
+          depends on: some tasks saved by PopulateCommonPipelines()
+          depends on: common/<subresource_discoverer>-setup.json
+            depends on: some tasks saved by PopulateCommonPipelines()
+
+    Returns:
+      task_manager.Task for
+          <runner_transformer_name>/<subresource_discoverer>-metrics.csv
+    """
+    assert subresource_discoverer in sandwich_misc.SUBRESOURCE_DISCOVERERS
+    assert 'common' not in sandwich_misc.SUBRESOURCE_DISCOVERERS
+    shared_task_prefix = os.path.join('common', subresource_discoverer)
+    task_prefix = os.path.join(runner_transformer_name, subresource_discoverer)
+
+    @self.RegisterTask(shared_task_prefix + '-setup.json', merge=True,
+                       dependencies=[self._subresources_for_urls_task])
+    def SetupBenchmark():
+      trace_path = os.path.join(self._subresources_for_urls_run_task.path, '0',
+                                sandwich_runner.TRACE_FILENAME)
+      whitelisted_urls = sandwich_misc.ExtractDiscoverableUrls(
+          trace_path, subresource_discoverer)
+
+      urls_resources = json.load(open(self._subresources_for_urls_task.path))
+      # TODO(gabadie): Implement support for multiple URLs in this Task.
+      assert len(urls_resources) == 1
+      url = urls_resources.keys()[0]
+      url_resources = urls_resources[url]
+      common_util.EnsureParentDirectoryExists(SetupBenchmark.path)
+      with open(SetupBenchmark.path, 'w') as output:
+        json.dump({
+            'cache_whitelist': [url for url in whitelisted_urls],
+            'url_resources': url_resources,
+          }, output)
+
+    @self.RegisterTask(shared_task_prefix + '-cache.zip', merge=True,
+                       dependencies=[
+                           SetupBenchmark, self._reference_cache_task])
+    def BuildBenchmarkCacheArchive():
+      setup = json.load(open(SetupBenchmark.path))
+      chrome_cache.ApplyUrlWhitelistToCacheArchive(
+          cache_archive_path=self._reference_cache_task.path,
+          whitelisted_urls=setup['cache_whitelist'],
+          output_cache_archive_path=BuildBenchmarkCacheArchive.path)
+
+    @self.RegisterTask(task_prefix + '-run/',
+                       dependencies=[BuildBenchmarkCacheArchive])
+    def RunBenchmark():
+      runner = self._CreateSandwichRunner()
+      # runner.record_video = True
+      runner.job_repeat = self._url_repeat
+      runner_transformer(runner)
+      runner.wpr_archive_path = self._patched_wpr_task.path
+      runner.wpr_out_log_path = os.path.join(RunBenchmark.path, 'wpr.log')
+      runner.cache_archive_path = BuildBenchmarkCacheArchive.path
+      runner.cache_operation = 'push'
+      runner.trace_output_directory = RunBenchmark.path
+      runner.Run()
+
+    @self.RegisterTask(task_prefix + '-metrics.csv',
+                       dependencies=[RunBenchmark])
+    def ExtractMetrics():
+      sandwich_misc.VerifyBenchmarkOutputDirectory(
+          SetupBenchmark.path, RunBenchmark.path)
+      trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
+          RunBenchmark.path)
+      trace_metrics_list.sort(key=lambda e: e['id'])
+      with open(ExtractMetrics.path, 'w') as csv_file:
+        writer = csv.DictWriter(csv_file,
+                                fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
+        writer.writeheader()
+        for trace_metrics in trace_metrics_list:
+          writer.writerow(trace_metrics)
+
+    self._default_final_tasks.append(ExtractMetrics)
+    return ExtractMetrics

commit 569f5b62652fd5049af0f1df00bf3e1015609039
Author: lizeb <lizeb@chromium.org>
Date:   Wed Apr 27 03:54:49 2016 -0700

    clovis: Enable the disabled-by-default-blink.debug.layout tracing category.
    
    This category is used by FirstSignificantPaintLens, as pointed out by mattcary@.
    
    Review URL: https://codereview.chromium.org/1924453002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390041}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 188dfe59bb04b695ee7186758a75b9ee8a1afce5

diff --git a/loading/tracing.py b/loading/tracing.py
index c1ecf71..068c5b6 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -14,9 +14,9 @@ import devtools_monitor
 
 _DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
 INITIAL_CATEGORIES = (
-    'toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
-    'blink.user_timing', 'blink.net') + tuple(
-        '-' + cat for cat in _DISABLED_CATEGORIES)
+    ('toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
+     'blink.user_timing', 'blink.net', 'disabled-by-default-blink.debug.layout')
+    + tuple('-' + cat for cat in _DISABLED_CATEGORIES))
 
 
 class TracingTrack(devtools_monitor.Track):
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 89ce7a1..dd99d79 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -91,6 +91,11 @@ class _FirstEventLens(_UserSatisfiedLens):
   # pylint: disable=abstract-method
 
   @classmethod
+  def _CheckCategory(cls, tracing_track, category):
+    assert category in tracing_track.Categories(), (
+        'The "%s" category must be enabled.' % category)
+
+  @classmethod
   def _ExtractFirstTiming(cls, times):
     if not times:
       return float('inf')
@@ -106,9 +111,11 @@ class FirstTextPaintLens(_FirstEventLens):
 
   This event is taken directly from a trace.
   """
+  _EVENT_CATEGORY = 'blink.user_timing'
   def _CalculateTimes(self, tracing_track):
+    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
     first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches('blink.user_timing', 'firstPaint')]
+                    if e.Matches(self._EVENT_CATEGORY, 'firstPaint')]
     self._satisfied_msec = self._event_msec = \
         self._ExtractFirstTiming(first_paints)
 
@@ -119,9 +126,11 @@ class FirstContentfulPaintLens(_FirstEventLens):
   This event is taken directly from a trace. Internally to chrome it's computed
   by filtering out things like background paint from firstPaint.
   """
+  _EVENT_CATEGORY = 'blink.user_timing'
   def _CalculateTimes(self, tracing_track):
+    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
     first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if e.Matches('blink.user_timing', 'firstContentfulPaint')]
+                    if e.Matches(self._EVENT_CATEGORY, 'firstContentfulPaint')]
     self._satisfied_msec = self._event_msec = \
        self._ExtractFirstTiming(first_paints)
 
@@ -133,22 +142,22 @@ class FirstSignificantPaintLens(_FirstEventLens):
   been loaded to compute the layout. Our event time is that of the next paint as
   that is the observable event.
   """
-  FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
-
+  _FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
+  _EVENT_CATEGORY = 'disabled-by-default-blink.debug.layout'
   def _CalculateTimes(self, tracing_track):
+    self._CheckCategory(tracing_track, self._EVENT_CATEGORY)
     sync_paint_times = []
     layouts = []  # (layout item count, msec).
     for e in tracing_track.GetEvents():
       # TODO(mattcary): is this the right paint event? Check if synchronized
       # paints appear at the same time as the first*Paint events, above.
-      if e.Matches('blink', 'FrameView::SynchronizedPaint'):
+      if e.Matches('blink', 'FrameView::synchronizedPaint'):
         sync_paint_times.append(e.start_msec)
       if ('counters' in e.args and
-          self.FIRST_LAYOUT_COUNTER in e.args['counters']):
-        layouts.append((e.args['counters'][self.FIRST_LAYOUT_COUNTER],
+          self._FIRST_LAYOUT_COUNTER in e.args['counters']):
+        layouts.append((e.args['counters'][self._FIRST_LAYOUT_COUNTER],
                         e.start_msec))
-    assert layouts, ('No layout events, was the disabled-by-default-blink'
-                     '.debug.layout category enabled?')
+    assert layouts, 'No layout events'
     layouts.sort(key=operator.itemgetter(0), reverse=True)
     self._satisfied_msec = layouts[0][1]
     self._event_msec = self._ExtractFirstTiming([
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 2e35a50..c7cdb10 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -88,13 +88,13 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
                        'name': 'firstPaint'},
                       {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
-                       'name': 'FrameView::SynchronizedPaint'},
+                       'name': 'FrameView::synchronizedPaint'},
                       {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink',
-                       'name': 'FrameView::SynchronizedPaint'},
+                       'name': 'FrameView::synchronizedPaint'},
                       {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink',
-                       'name': 'FrameView::SynchronizedPaint'},
+                       'name': 'FrameView::synchronizedPaint'},
 
                       {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'foobar', 'name': 'biz',
@@ -114,3 +114,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
     self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
     self.assertEqual(7, lens.PostloadTimeMsec())
+
+
+if __name__ == '__main__':
+  unittest.main()

commit d4b3a72bcaebbd31b864c13d5fa8ddd0ea21c629
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 27 01:45:36 2016 -0700

    tools/android/loading: Improve LocalChromeController's chrome launch
    
    Before, the LocalChromeController was waiting 10 seconds before
    connecting to chrome's devtools connection. This CL replace this
    sleep by a devtool connection attempt every one seconds.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1912123002
    
    Cr-Original-Commit-Position: refs/heads/master@{#390024}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 240be2d4535ca60df1b9e44cf6a4b70a3be7ce89

diff --git a/loading/controller.py b/loading/controller.py
index 96eec7f..50595e4 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,6 +10,8 @@ desktop-specific versions.
 """
 
 import contextlib
+import datetime
+import errno
 import logging
 import os
 import shutil
@@ -39,6 +41,9 @@ class ChromeControllerBase(object):
 
   Defines common operations but should not be created directly.
   """
+  DEVTOOLS_CONNECTION_ATTEMPTS = 10
+  DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
+
   def __init__(self):
     self._chrome_args = [
         # Disable backgound network requests that may pollute WPR archive,
@@ -174,6 +179,9 @@ class ChromeControllerBase(object):
     if self._emulated_network:
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
+    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
+                          seconds_since_epoch=time.time())
+    logging.info('Devtools connection success')
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
@@ -182,12 +190,6 @@ class ChromeControllerBase(object):
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
-  # Number of connection attempt to chrome's devtools.
-  DEVTOOLS_CONNECTION_ATTEMPTS = 10
-
-  # Time interval in seconds between chrome's devtools connection attempts.
-  DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
-
   # An estimate of time to wait for the device to become idle after expensive
   # operations, such as opening the launcher activity.
   TIME_TO_IDLE_SECONDS = 2
@@ -226,10 +228,7 @@ class RemoteChromeController(ChromeControllerBase):
           data='about:blank')
       self._device.StartActivity(start_intent, blocking=True)
       try:
-        for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS + 1):
-          if attempt_id == self.DEVTOOLS_CONNECTION_ATTEMPTS:
-            raise RuntimeError('Failed to connect to chrome devtools after {} '
-                               'attempts.'.format(attempt_id))
+        for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
           logging.info('Devtools connection attempt %d' % attempt_id)
           with device_setup.ForwardPort(
               self._device, 'tcp:%d' % OPTIONS.devtools_port,
@@ -239,15 +238,18 @@ class RemoteChromeController(ChromeControllerBase):
                   OPTIONS.devtools_hostname, OPTIONS.devtools_port)
               self._StartConnection(connection)
             except socket.error as e:
-              assert str(e).startswith('[Errno 104] Connection reset by peer')
+              if e.errno != errno.ECONNRESET:
+                raise
               time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
               continue
-            logging.info('Devtools connection success')
             yield connection
             if self._slow_death:
               self._device.adb.Shell('am start com.google.android.launcher')
               time.sleep(self.TIME_TO_IDLE_SECONDS)
             break
+        else:
+          raise RuntimeError('Failed to connect to chrome devtools after {} '
+                             'attempts.'.format(attempt_id))
       finally:
         self._device.ForceStop(package_info.package)
 
@@ -326,21 +328,32 @@ class LocalChromeController(ChromeControllerBase):
                                       env=environment)
     connection = None
     try:
-      time.sleep(10)
-      process_result = chrome_process.poll()
-      if process_result is not None:
-        logging.error('Unexpected process exit: %s', process_result)
+      # Attempt to connect to Chrome's devtools
+      for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS):
+        logging.info('Devtools connection attempt %d' % attempt_id)
+        process_result = chrome_process.poll()
+        if process_result is not None:
+          raise RuntimeError('Unexpected Chrome exit: %s', process_result)
+        try:
+          connection = devtools_monitor.DevToolsConnection(
+              OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+          break
+        except socket.error as e:
+          if e.errno != errno.ECONNREFUSED:
+            raise
+          time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
       else:
-        connection = devtools_monitor.DevToolsConnection(
-            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-        self._StartConnection(connection)
-        yield connection
-        if self._slow_death:
-          connection.Close()
-          connection = None
-          chrome_process.wait()
+        raise RuntimeError('Failed to connect to Chrome devtools after {} '
+                           'attempts.'.format(attempt_id))
+      # Start and yield the devtool connection.
+      self._StartConnection(connection)
+      yield connection
+      if self._slow_death:
+        connection.Close()
+        chrome_process.wait()
+        chrome_process = None
     finally:
-      if connection:
+      if chrome_process:
         chrome_process.kill()
       if self._headless:
         xvfb_process.kill()

commit 46ea801f27a7976d32ed47afdbe411cc63232733
Author: Newton Allen <newt@google.com>
Date:   Tue Apr 26 17:32:51 2016 -0700

    Remove newt@ from OWNERS and add replacements in a few cases.
    
    R=finnur@chromium.org, ianwen@chromium.org, sadrul@chromium.org, tedchoc@chromium.org, twellington@chromium.org
    
    Review URL: https://codereview.chromium.org/1915173002 .
    
    Cr-Original-Commit-Position: refs/heads/master@{#389956}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 963b18fb2197513651ad508974d9f78c7136867e

diff --git a/checkstyle/OWNERS b/checkstyle/OWNERS
deleted file mode 100644
index 577bf98..0000000
--- a/checkstyle/OWNERS
+++ /dev/null
@@ -1,2 +0,0 @@
-aurimas@chromium.org
-newt@chromium.org

commit 470798b904a09cc5aecab57867bbe95ecace6153
Author: lizeb <lizeb@chromium.org>
Date:   Tue Apr 26 10:33:36 2016 -0700

    clovis: Failed requests have no timings, don't crash in the serialization.
    
    Review URL: https://codereview.chromium.org/1915853007
    
    Cr-Original-Commit-Position: refs/heads/master@{#389815}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2f9e27bf548149fa3ba711f6151ef6b027ffc141

diff --git a/loading/request_track.py b/loading/request_track.py
index 31eca51..b4e3a15 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -220,7 +220,7 @@ class Request(object):
 
   def ToJsonDict(self):
     result = copy.deepcopy(self.__dict__)
-    result['timing'] = self.timing.ToJsonDict()
+    result['timing'] = self.timing.ToJsonDict() if self.timing else {}
     return result
 
   @classmethod

commit 1f80fd36042d5f238ccf4f4fbe81acd768186f9e
Author: jbudorick <jbudorick@chromium.org>
Date:   Tue Apr 26 10:11:20 2016 -0700

    [Android] Make tools/android/forwarder build with the current toolchain.
    
    BUG=604468
    
    Review URL: https://codereview.chromium.org/1915293005
    
    Cr-Original-Commit-Position: refs/heads/master@{#389812}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c2e43c5c93451f112a1aefc4d2677a98c1185f7e

diff --git a/forwarder/BUILD.gn b/forwarder/BUILD.gn
index 9cc99b4..c59cd90 100644
--- a/forwarder/BUILD.gn
+++ b/forwarder/BUILD.gn
@@ -4,21 +4,14 @@
 
 import("//build/symlink.gni")
 
-if (current_toolchain == host_toolchain) {
-  # GYP: //tools/android/forwarder/forwarder.gyp:forwarder
-  executable("forwarder") {
-    sources = [
-      "forwarder.cc",
-    ]
-    deps = [
-      "//base",
-      "//build/config/sanitizers:deps",
-      "//tools/android/common",
-    ]
-  }
-} else {
-  # Create a symlink from root_build_dir -> clang_x64/forwarder.
-  binary_symlink("forwarder") {
-    binary_label = ":$target_name($host_toolchain)"
-  }
+# GYP: //tools/android/forwarder/forwarder.gyp:forwarder
+executable("forwarder") {
+  sources = [
+    "forwarder.cc",
+  ]
+  deps = [
+    "//base",
+    "//build/config/sanitizers:deps",
+    "//tools/android/common",
+  ]
 }

commit 074b718c84b9b043f27e23c94a2da0d9eed8de5e
Author: droger <droger@chromium.org>
Date:   Tue Apr 26 06:10:19 2016 -0700

    tools/android/loading Cleanup clovis frontend
    
    Due to a bad rebase, CL https://codereview.chromium.org/1907233003/
    undid some of the changes from CL
    https://codereview.chromium.org/1920793003/.
    
    This new CL re-applies the changes.
    
    Review URL: https://codereview.chromium.org/1914163002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389764}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 488d24ccce7f9649be3abdbd6083068a0d4c0af6

diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index c00f43b..3f766dd 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -3,7 +3,6 @@
 # found in the LICENSE file.
 
 import json
-from googleapiclient import (discovery, errors)
 import time
 
 from googleapiclient import (discovery, errors)
@@ -12,22 +11,21 @@ from googleapiclient import (discovery, errors)
 class GoogleInstanceHelper(object):
   """Helper class for the Google Compute API, allowing to manage groups of
   instances more easily. Groups of instances are identified by a tag."""
+  _COMPUTE_API_ROOT = 'https://www.googleapis.com/compute/v1/projects/'
 
   def __init__(self, credentials, project, logger):
     self._compute_api = discovery.build('compute','v1', credentials=credentials)
     self._project = project
-    self._api_url = 'https://www.googleapis.com/compute/v1/projects/' + project
+    self._project_api_url = self._COMPUTE_API_ROOT + project
     self._zone = 'europe-west1-c'
     self._logger = logger
 
   def _ExecuteApiRequest(self, request, retry_count=3):
     """ Executes a Compute API request and returns True on success."""
-    self._logger.info('Compute API request:')
-    self._logger.info(request.to_json())
+    self._logger.info('Compute API request:\n' + request.to_json())
     try:
       response = request.execute()
-      self._logger.info('Compute API response:')
-      self._logger.info(response)
+      self._logger.info('Compute API response:\n' + response)
       return True
     except errors.HttpError as err:
       error_content = self._GetErrorContent(err)
@@ -56,7 +54,7 @@ class GoogleInstanceHelper(object):
 
   def _GetErrorContent(self, error):
     """Returns the contents of an error returned by the Compute API as a
-    dictionary.
+    dictionary or None.
     """
     if not error.resp.get('content-type', '').startswith('application/json'):
       return None
@@ -70,22 +68,22 @@ class GoogleInstanceHelper(object):
         not error_content['error'].get('errors')):
       return None
     error_list = error_content['error']['errors']
-    if len(error_list) == 0:
+    if not error_list:
       return None
-    return error_list[0].get('reason', '')
+    return error_list[0].get('reason')
 
   def CreateTemplate(self, tag, bucket):
     """Creates an instance template for instances identified by tag and using
     bucket for deployment. Returns True if successful.
     """
-    image_url = 'https://www.googleapis.com/compute/v1/projects/' \
+    image_url = self._COMPUTE_API_ROOT + \
                 'ubuntu-os-cloud/global/images/ubuntu-1404-trusty-v20160406'
     request_body = {
         'name': self._GetTemplateName(tag),
         'properties': {
             'machineType': 'n1-standard-1',
             'networkInterfaces': [{
-                'network': self._api_url + '/global/networks/default',
+                'network': self._project_api_url + '/global/networks/default',
                 'accessConfigs': [{
                     'name': 'external-IP',
                     'type': 'ONE_TO_ONE_NAT'
@@ -130,7 +128,7 @@ class GoogleInstanceHelper(object):
     exist for this to succeed. Returns True if successful.
     """
     template_url = '%s/global/instanceTemplates/%s' % (
-        self._api_url, self._GetTemplateName(tag))
+        self._project_api_url, self._GetTemplateName(tag))
     request_body = {
         'zone': self._zone, 'targetSize': instance_count,
         'baseInstanceName': 'instance-' + tag,
@@ -148,7 +146,7 @@ class GoogleInstanceHelper(object):
     # The instance hostname may be of the form <name>.c.<project>.internal but
     # only the <name> part should be passed to the compute API.
     name = instance_hostname.split('.')[0]
-    instance_url = self._api_url + (
+    instance_url = self._project_api_url + (
         "/zones/%s/instances/%s" % (self._zone, name))
     request = self._compute_api.instanceGroupManagers().deleteInstances(
         project=self._project, zone=self._zone,
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index b037a3d..656fa9d 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -2,13 +2,14 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-import flask
-from google.appengine.api import (app_identity, taskqueue)
-from oauth2client.client import GoogleCredentials
 import logging
 import os
 import sys
 
+import flask
+from google.appengine.api import (app_identity, taskqueue)
+from oauth2client.client import GoogleCredentials
+
 from common.clovis_task import ClovisTask
 import common.google_instance_helper
 from memory_logs import MemoryLogs

commit 3348b0077d92112bf998f1ac18b3f8718319d6e2
Author: droger <droger@chromium.org>
Date:   Tue Apr 26 03:19:39 2016 -0700

    tools/android/loading VM instances self destruct when finished
    
    Review URL: https://codereview.chromium.org/1919923002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389740}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6aa684686a2819515d31d39d873e06c72b195dca

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index b181cfa..e9e1a9a 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -1,16 +1,34 @@
 # Clovis in the Cloud: Developer Guide
 
-This document describes how to collect Chromium traces using Google Compute
-Engine.
+This document describes the backend-side of the trace collection, using Google
+Compute Engine.
+
+When the [frontend][3] spawns new tasks, it pushes them into a [TaskQueue][4]
+called `clovis-queue` with a unique tag.
+Then it creates backend instances (as an instance group) and passes them the
+TaskQueue tag.
+
+The backend instances then pull tasks from the TaskQueue and process them until
+it is empty. When there is no task left in the queue, the backend instances
+kill themselves.
+
+The main files for the backend are:
+
+-   `startup-script.sh`: initializes an instance (installs the dependencies,
+    downloads the code and the configuration).
+-   `worker.py`: the main worker script.
 
 [TOC]
 
-## Initial setup
+## Initial setup for development
 
 Install the [gcloud command line tool][1].
 
 ## Deploy the code
 
+This step deploys all the source code needed by the backend workers, as well as
+the Chromium binaries required for trace collection.
+
 ```shell
 # Build Chrome (do not use the component build).
 BUILD_DIR=out/Release
@@ -25,58 +43,97 @@ ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
 
 ## Start the app in the cloud
 
-Create an instance using latest ubuntu LTS:
+The application is automatically started by the frontend, and should not need to
+be started manually.
+
+If you really want to create an instance manually (when debugging for example),
+this can be done like this:
 
 ```shell
-gcloud compute instances create clovis-tracer-1 \
+gcloud compute instances create $INSTANCE_NAME \
  --machine-type n1-standard-1 \
  --image ubuntu-14-04 \
  --zone europe-west1-c \
  --scopes cloud-platform,https://www.googleapis.com/auth/cloud-taskqueue \
- --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,taskqueue_tag=some_tag \
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,taskqueue-tag=some_tag \
  --metadata-from-file \
      startup-script=$CHROMIUM_SRC/tools/android/loading/cloud/backend/startup-script.sh
 ```
 
-**Note:** To start an instance without automatically starting the app on it,
-add a `auto-start=false` metadata. This can be useful when doing iterative
-development on the instance, to be able to restart the app manually.
-
-This should output the IP address of the instance.
-Otherwise the IP address can be retrieved by doing:
+If you are debbugging, you probably want to set additional metadata:
+
+-   `auto-start=false`: to start an instance without automatically starting the
+    app on it. This can be useful when doing iterative development on the
+    instance using ssh, to be able to stop and restart the app manually.
+-   `self-destruct=false`: to prevent the instance from self-destructing when
+    the queue is empty.
+
+**Notes:**
+
+-   If you use `auto-start=false`, and then try to ssh on the instance and
+    launch `worker.py`, it will not work because of various issues, such as:
+    -   Environment variables defined by the startup script are not available
+        to your user and you will need to redefine them.
+    -   You will not have permissions to access the files, and need to run
+        `sudo chown` to give yourself permissions.
+    -   You need to activate `virtualenv`.
+    Get in touch with *droger@* if you need this or want to improve it.
+-   It can take a few minutes for the instance to start. You can follow the
+    progress of the startup script on the gcloud console web interface (menu
+    "Compute Engine" > "VM instances" then click on your instance and scroll
+    down to see the "Serial console output") or from the command line using:
 
 ```shell
-gcloud compute instances list
+gcloud compute instances get-serial-port-output $INSTANCE_NAME
 ```
 
-**Note:** It can take a few minutes for the instance to start. You can follow
-the progress of the startup script on the gcloud console web interface (menu
-"Compute Engine" > "VM instances" then click on your instance and scroll down to
-see the "Serial console output") or from the command line using:
+## `worker.py` configuration file
 
-```shell
-gcloud compute instances get-serial-port-output clovis-tracer-1
-```
+`worker.py` takes a configuration file as command line parameter. This is a JSON
+dictionary with the keys:
+
+-   `project_name` (string): Name of the Google Cloud project
+-   `cloud_storage_path` (string): Path in Google Storage where generated traces
+    will be stored.
+-   `chrome_path` (string): Path to the Chrome executable.
+-   `src_path` (string): Path to the Chromium source directory.
+-   `taskqueue_tag` (string): Tag used by the worker when pulling tasks from
+    `clovis-queue`.
+-   `trace_database_filename` (string, optional): Filename for the trace
+    database in Cloud Storage. Must be unique per worker to avoid concurrent
+    access. Defaults to `trace_database.json`.
+-   `destruct_instance_name` (string, optional): Name of the instance the worker
+    will destroy when there are no remaining tasks to process. This is only
+    relevant when running in the cloud.
 
 ## Use the app
 
 Create tasks from the associated AppEngine application, see [documentation][3].
-Make sure the `taskqueue_tag` of the AppEngine request matches the one of the
-ComputeEngine instances.
+
+If you want the frontend to send tasks to a particular instance that you created
+manually, make sure the `tag` and `storage_bucket` of the AppEngine request
+match the ones of your ComputeEngine instance, and set `instance_count` to `0`.
 
 ## Stop the app in the cloud
 
+To stop a single instance that you started manually, do:
+
 ```shell
-gcloud compute instances delete clovis-tracer-1
+gcloud compute instances delete $INSTANCE_NAME
 ```
 
+To stop instances that were created by the frontend, you must delete the
+instance group, not the individual instances. Otherwise the instance group will
+just recreate the deleted instances. You can do this from the Google Cloud
+console web interface, or using the `gcloud compute groups` commands.
+
 ## Connect to the instance with SSH
 
 ```shell
-gcloud compute ssh clovis-tracer-1
+gcloud compute ssh $INSTANCE_NAME
 ```
 
-## Use the app locally
+## Run the app locally
 
 From a new directory, set up a local environment:
 
@@ -95,20 +152,8 @@ gcloud beta auth application-default login --scopes \
     https://www.googleapis.com/auth/cloud-platform
 ```
 
-Create a JSON dictionary file describing the deployment configuration, with the
-keys:
-
--   `project_name` (string): Name of the Google Cloud project
--   `cloud_storage_path` (string): Path in Google Storage where generated traces
-    will be stored.
--   `chrome_path` (string): Path to the Chrome executable.
--   `src_path` (string): Path to the Chromium source directory.
--   `taskqueue_tag` (string):
--   `trace_database_filename` (string, optional): Filename for the trace
-    database in Cloud Storage. Must be unique per worker to avoid concurrent
-    access. Defaults to `trace_database.json`.
+Create a local configuration file for `worker.py`. Example:
 
-Example:
 ```shell
 cat >$CONFIG_FILE << EOF
 {
@@ -116,7 +161,7 @@ cat >$CONFIG_FILE << EOF
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "$CHROME_PATH",
   "src_path" : "$CHROMIUM_SRC",
-  "taskqueue_tag" : "some_tag"
+  "taskqueue_tag" : "some-tag"
 }
 EOF
 ```
@@ -139,3 +184,4 @@ deactivate
 [1]: https://cloud.google.com/sdk
 [2]: #Use-the-app
 [3]: ../frontend/README.md
+[4]: https://cloud.google.com/appengine/docs/python/taskqueue
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index 3d3084f..69cb7d9 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -17,7 +17,7 @@ PROJECTID=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
     -H "Metadata-Flavor: Google")
 
-INSTANCE_ID=$(curl -s \
+INSTANCE_NAME=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/instance/hostname" \
     -H "Metadata-Flavor: Google")
 
@@ -70,15 +70,22 @@ chown -R pythonapp:pythonapp /opt/app
 
 # Create the configuration file for this deployment.
 DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
-TASKQUEUE_TAG=`get_instance_metadata taskqueue_tag`
+TASKQUEUE_TAG=`get_instance_metadata taskqueue-tag`
+if [ "$(get_instance_metadata self-destruct)" == "false" ]; then
+  SELF_DESTRUCT_CONFIG_LINE=""
+else
+  SELF_DESTRUCT_CONFIG_LINE="\"destruct_instance_name\" : \"$INSTANCE_NAME\","
+fi
+
 cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
+  $SELF_DESTRUCT_CONFIG_LINE
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
   "taskqueue_tag" : "$TASKQUEUE_TAG",
-  "trace_database_filename" : "trace_database_${INSTANCE_ID}.json"
+  "trace_database_filename" : "trace_database_${INSTANCE_NAME}.json"
 }
 EOF
 
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index c724d2e..ff0d473 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -21,6 +21,7 @@ sys.path.insert(0,
                  os.pardir))
 import controller
 from cloud.common.clovis_task import ClovisTask
+from cloud.common.google_instance_helper import GoogleInstanceHelper
 from google_storage_accessor import GoogleStorageAccessor
 import loading_trace
 from loading_trace_database import LoadingTraceDatabase
@@ -33,6 +34,7 @@ class Worker(object):
     self._project_name = config['project_name']
     self._taskqueue_tag = config['taskqueue_tag']
     self._src_path = config['src_path']
+    self._destruct_instance_name = config.get('destruct_instance_name')
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
 
@@ -80,15 +82,7 @@ class Worker(object):
       (clovis_task, task_id) = self._FetchClovisTask(project, task_api,
                                                      queue_name)
       if not clovis_task:
-        if self._trace_database.ToJsonDict():
-          self._logger.info('No remaining tasks in the queue.')
-          break
-        else:
-          delay_seconds = 60
-          self._logger.info(
-              'Nothing in the queue, retrying in %i seconds.' % delay_seconds)
-          time.sleep(delay_seconds)
-          continue
+        break
 
       self._logger.info('Processing task %s' % task_id)
       self._ProcessClovisTask(clovis_task)
@@ -126,7 +120,7 @@ class Worker(object):
                          if no tasks are found.
     """
     response = task_api.tasks().lease(
-        project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=180,
+        project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=600,
         groupByTag=True, tag=self._taskqueue_tag).execute()
     if (not response.get('items')) or (len(response['items']) < 1):
       return (None, None)
@@ -138,8 +132,19 @@ class Worker(object):
 
   def _Finalize(self):
     """Called before exiting."""
-    # TODO(droger): Implement automatic instance destruction.
     self._logger.info('Done')
+    # Self destruct.
+    if self._destruct_instance_name:
+      self._logger.info('Starting instance destruction: ' +
+                        self._destruct_instance_name)
+      google_instance_helper = GoogleInstanceHelper(
+          self._credentials, self._project_name, self._logger)
+      success = google_instance_helper.DeleteInstance(
+          self._taskqueue_tag, self._destruct_instance_name)
+      if not success:
+        self._logger.error('Self destruction failed')
+    # Do not add anything after this line, as the instance might be killed at
+    # any time.
 
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index bd678b4..c00f43b 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -111,7 +111,7 @@ class GoogleInstanceHelper(object):
                  'value': bucket},
                 {'key': 'startup-script-url',
                  'value': 'gs://%s/deployment/startup-script.sh' % bucket},
-                {'key': 'taskqueue_tag', 'value': tag}]}}}
+                {'key': 'taskqueue-tag', 'value': tag}]}}}
     request = self._compute_api.instanceTemplates().insert(
         project=self._project, body=request_body)
     return self._ExecuteApiRequest(request)

commit 65b17a568116563bc3e74e717c42f00d9c8c2d1d
Author: lizeb <lizeb@chromium.org>
Date:   Tue Apr 26 01:05:56 2016 -0700

    clovis: Computes the amount of data downloaded due to ads.
    
    Review URL: https://codereview.chromium.org/1915093002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389723}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9e49615dccedf0d7e49515c7a754b81a94b2aaad

diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
index be05fcb..89f40e2 100644
--- a/loading/content_classification_lens.py
+++ b/loading/content_classification_lens.py
@@ -6,6 +6,7 @@
 
 import collections
 import logging
+import operator
 import os
 import urlparse
 
@@ -26,6 +27,7 @@ class ContentClassificationLens(object):
     """
     self._trace = trace
     self._requests = trace.request_track.GetEvents()
+    self._requests_by_id = {r.request_id: r for r in self._requests}
     self._main_frame_id = trace.page_track.GetEvents()[0]['frame_id']
     self._frame_to_requests = collections.defaultdict(list)
     self._ad_requests = set()
@@ -44,15 +46,32 @@ class ContentClassificationLens(object):
     """Returns True iff the request matches one of the tracking_rules."""
     return request.request_id in self._tracking_requests
 
-  def IsAdFrame(self, frame_id, ratio):
-    """A Frame is an Ad frame if more than |ratio| of its requests are
-    ad-related, and is not the main frame."""
-    if frame_id == self._main_frame_id:
+  def IsAdOrTrackingFrame(self, frame_id):
+    """A Frame is an Ad frame if it's not the main frame and its main resource
+    is ad or tracking-related.
+    """
+    if (frame_id not in self._frame_to_requests
+        or frame_id == self._main_frame_id):
       return False
-    ad_requests_count = sum(r in self._ad_requests
-                            for r in self._frame_to_requests[frame_id])
-    frame_requests_count = len(self._frame_to_requests[frame_id])
-    return (float(ad_requests_count) / frame_requests_count) > ratio
+    frame_requests = [self._requests_by_id[request_id]
+                      for request_id in self._frame_to_requests[frame_id]]
+    sorted_frame_resources = sorted(
+        frame_requests, key=operator.attrgetter('start_msec'))
+    frame_main_resource = sorted_frame_resources[0]
+    return (frame_main_resource.request_id in self._ad_requests
+            or frame_main_resource.request_id in self._tracking_requests)
+
+  def AdAndTrackingRequests(self):
+    """Returns a list of requests linked to ads and tracking.
+
+    Returns the union of:
+    - Requests tagged as ad or tracking.
+    - Requests originating from an ad frame.
+    """
+    frame_ids = {r.frame_id for r in self._requests}
+    ad_frame_ids = filter(self.IsAdOrTrackingFrame, frame_ids)
+    return filter(lambda r: self.IsAdRequest(r) or self.IsTrackingRequest(r)
+                  or r.frame_id in ad_frame_ids, self._requests)
 
   @classmethod
   def WithRulesFiles(cls, trace, ad_rules_filename, tracking_rules_filename):
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index 7d1631b..18025b0 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -14,14 +14,15 @@ import test_utils
 class ContentClassificationLensTestCase(unittest.TestCase):
   _DOCUMENT_URL = 'http://bla.com'
   _MAIN_FRAME_ID = '123.1'
-  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+  _REQUEST = Request.FromJsonDict({'url': _DOCUMENT_URL,
                                    'document_url': _DOCUMENT_URL,
                                    'request_id': '1234.1',
                                    'frame_id': _MAIN_FRAME_ID,
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
                                    'status': 200,
-                                   'timing': {}})
+                                   'timing': {},
+                                   'resource_type': 'Document'})
   _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
                    'frame_id': _MAIN_FRAME_ID},
                   {'method': 'Page.frameAttached',
@@ -74,19 +75,42 @@ class ContentClassificationLensTestCase(unittest.TestCase):
     self.assertFalse(lens.IsAdRequest(self._REQUEST))
     self.assertTrue(lens.IsTrackingRequest(self._REQUEST))
 
-  def testMainFrameIsNotAdFrame(self):
+  def testMainFrameIsNotAnAdFrame(self):
     trace = test_utils.LoadingTraceFromEvents(
-        [self._REQUEST] * 10, self._PAGE_EVENTS)
+        [self._REQUEST], self._PAGE_EVENTS)
     lens = ContentClassificationLens(trace, self._RULES, [])
-    self.assertFalse(lens.IsAdFrame(self._MAIN_FRAME_ID, .5))
+    self.assertFalse(lens.IsAdOrTrackingFrame(self._MAIN_FRAME_ID))
 
   def testAdFrame(self):
     request = copy.deepcopy(self._REQUEST)
+    request.request_id = '1234.2'
     request.frame_id = '123.123'
     trace = test_utils.LoadingTraceFromEvents(
-        [request] * 10 + [self._REQUEST] * 5, self._PAGE_EVENTS)
+        [self._REQUEST, request], self._PAGE_EVENTS)
     lens = ContentClassificationLens(trace, self._RULES, [])
-    self.assertTrue(lens.IsAdFrame(request.frame_id, .5))
+    self.assertTrue(lens.IsAdOrTrackingFrame(request.frame_id))
+
+  def testAdAndTrackingRequests(self):
+    ad_request = copy.deepcopy(self._REQUEST)
+    ad_request.request_id = '1234.2'
+    ad_request.frame_id = '123.123'
+    non_ad_request_non_ad_frame = copy.deepcopy(self._REQUEST)
+    non_ad_request_non_ad_frame.request_id = '1234.3'
+    non_ad_request_non_ad_frame.url = 'http://www.example.com'
+    non_ad_request_non_ad_frame.frame_id = '123.456'
+    non_ad_request_ad_frame = copy.deepcopy(self._REQUEST)
+    non_ad_request_ad_frame.request_id = '1234.4'
+    non_ad_request_ad_frame.url = 'http://www.example.com'
+    non_ad_request_ad_frame.frame_id = ad_request.frame_id
+
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST, ad_request, non_ad_request_non_ad_frame,
+         non_ad_request_ad_frame], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertSetEqual(
+        set([self._REQUEST, ad_request, non_ad_request_ad_frame]),
+        set(lens.AdAndTrackingRequests()))
+
 
 class _MatcherTestCase(unittest.TestCase):
   _RULES_WITH_WHITELIST = ['/thisisanad.', '@@myadvertisingdomain.com/*',
diff --git a/loading/metrics.py b/loading/metrics.py
index b56c54c..5f5e560 100644
--- a/loading/metrics.py
+++ b/loading/metrics.py
@@ -4,11 +4,12 @@
 
 """Descriptive metrics for Clovis.
 
-When executed as a script, shows a graph of the amount of data to download for
-a new visit to the same page, with a given time interval.
+When executed as a script, prints the amount of data attributed to Ads, and
+shows a graph of the amount of data to download for a new visit to the same
+page, with a given time interval.
 """
 
-
+import content_classification_lens
 from request_track import CachingPolicy
 
 
@@ -24,16 +25,22 @@ def _RequestTransferSize(request):
           'body': request.encoded_data_length}
 
 
-def TotalTransferSize(trace):
-  """Returns the total transfer size (uploaded, downloaded) from a trace.
+def _TransferSize(requests):
+  """Returns the total transfer size (uploaded, downloaded) of requests.
 
   This is an estimate as we assume:
   - 200s (for the size computation)
   - GET only.
+
+  Args:
+    requests: ([Request]) List of requests.
+
+  Returns:
+    (uploaded_bytes (int), downloaded_bytes (int))
   """
   uploaded_bytes = 0
   downloaded_bytes = 0
-  for request in trace.request_track.GetEvents():
+  for request in requests:
     request_bytes = _RequestTransferSize(request)
     uploaded_bytes += request_bytes['get'] + request_bytes['request_headers']
     downloaded_bytes += (len('HTTP/1.1 200 OK')
@@ -42,6 +49,11 @@ def TotalTransferSize(trace):
   return (uploaded_bytes, downloaded_bytes)
 
 
+def TotalTransferSize(trace):
+  """Returns the total transfer size (uploaded, downloaded) from a trace."""
+  return _TransferSize(trace.request_track.GetEvents())
+
+
 def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
   """Returns the amount of data transferred for a revisit.
 
@@ -74,6 +86,25 @@ def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
   return (uploaded_bytes, downloaded_bytes)
 
 
+def AdsAndTrackingTransferSize(trace, ad_rules_filename,
+                               tracking_rules_filename):
+  """Returns the transfer size attributed to ads and tracking.
+
+  Args:
+    trace: (LoadingTrace) a loading trace.
+    ad_rules_filename: (str) Path to an ad rules file.
+    tracking_rules_filename: (str) Path to a tracking rules file.
+
+  Returns:
+    (uploaded_bytes (int), downloaded_bytes (int))
+  """
+  content_lens = (
+      content_classification_lens.ContentClassificationLens.WithRulesFiles(
+          trace, ad_rules_filename, tracking_rules_filename))
+  requests = content_lens.AdAndTrackingRequests()
+  return _TransferSize(requests)
+
+
 def PlotTransferSizeVsTimeBetweenVisits(trace):
   times = [10, 60, 300, 600, 3600, 4 * 3600, 12 * 3600, 24 * 3600]
   labels = ['10s', '1m', '10m', '1h', '4h', '12h', '1d']
@@ -90,8 +121,14 @@ def PlotTransferSizeVsTimeBetweenVisits(trace):
   plt.show()
 
 
-def main(trace_filename):
+def main(trace_filename, ad_rules_filename, tracking_rules_filename):
   trace = loading_trace.LoadingTrace.FromJsonFile(trace_filename)
+  (_, ads_downloaded_bytes) = AdsAndTrackingTransferSize(
+      trace, ad_rules_filename, tracking_rules_filename)
+  (_, total_downloaded_bytes) = TotalTransferSize(trace)
+  print '%e bytes linked to Ads/Tracking (%.02f%%)' % (
+      ads_downloaded_bytes,
+      (100. * ads_downloaded_bytes) / total_downloaded_bytes)
   PlotTransferSizeVsTimeBetweenVisits(trace)
 
 
@@ -99,5 +136,9 @@ if __name__ == '__main__':
   import sys
   from matplotlib import pylab as plt
   import loading_trace
-
-  main(sys.argv[1])
+  if len(sys.argv) != 4:
+    print (
+        'Usage: %s trace_filename ad_rules_filename tracking_rules_filename'
+        % sys.argv[0])
+    sys.exit(0)
+  main(*sys.argv[1:])

commit 285ed29317561f77148e484a27a83543a216e9f7
Author: droger <droger@chromium.org>
Date:   Mon Apr 25 23:24:09 2016 -0700

    tools/android/loading Automatic creation of the backend by the frontend
    
    This CL adds a new 'backend_params' parameter in a task,
    which allows to control the settings for the backend (in
    particular the number of compute engine instances).
    
    The frontend uses these parameters to automatically create
    an instance template, an instance group, and starts the
    instances.
    
    A new HTML page template is added to display the results,
    including the log.
    
    Review URL: https://codereview.chromium.org/1907233003
    
    Cr-Original-Commit-Position: refs/heads/master@{#389707}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ff5ba7a68e23dfed0754faf2f29a542753bfd5ec

diff --git a/loading/cloud/backend/deploy.sh b/loading/cloud/backend/deploy.sh
index cbab438..00f19d7 100755
--- a/loading/cloud/backend/deploy.sh
+++ b/loading/cloud/backend/deploy.sh
@@ -55,6 +55,10 @@ chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
   $tmpdir/linux.zip
 gsutil cp $tmpdir/linux.zip gs://$deployment_gcs_path/binaries/linux.zip
 
+# Copy the startup script uncompressed so that it can be executed.
+gsutil cp tools/android/loading/cloud/backend/startup-script.sh \
+  gs://$deployment_gcs_path/
+
 # Generate and upload metadata about this deployment.
 CHROMIUM_REV=$(git merge-base HEAD origin/master)
 cat >$tmpdir/build_metadata.json << EOF
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index 9518171..c724d2e 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -209,7 +209,7 @@ class Worker(object):
       return
 
     # Extract the task parameters.
-    params = clovis_task.Params()
+    params = clovis_task.ActionParams()
     urls = params['urls']
     repeat_count = params.get('repeat_count', 1)
     emulate_device = params.get('emulate_device')
diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
index 8bc3127..20a5182 100644
--- a/loading/cloud/common/clovis_task.py
+++ b/loading/cloud/common/clovis_task.py
@@ -4,24 +4,33 @@
 
 import base64
 import json
+import uuid
 
 class ClovisTask(object):
   """Generic task, generated by the AppEngine frontend and consumed by the
   ComputeEngine backend.
   """
 
-  def __init__(self, action, params, taskqueue_tag):
-    """Params:
+  def __init__(self, action, action_params, backend_params):
+    """ See tools/android/loading/cloud/frontend/README.md for a specification
+    of the parameters.
+
+    Args:
       action(str): Action accomplished by this task.
-      params(dict): Parameters of task.
-      taskqueue_tag(str): Tag of the task. Optional.
+      action_params(dict): Parameters of task.
+      backend_params(dict): Parameters of the instances running the task.
+          If this is None, no instances are created. If this dictionary has no
+          'tag' key, a unique tag will be generated.
     """
     self._action = action
-    self._params = params
-    self._taskqueue_tag = taskqueue_tag
+    self._action_params = action_params
+    self._backend_params = backend_params or {}
+    # If no tag is specified, generate a unique tag.
+    if not self._backend_params.get('tag'):
+      self._backend_params.update({'tag': str(uuid.uuid1())})
 
   @classmethod
-  def FromJsonDict(cls, json_dict):
+  def FromJsonString(cls, json_dict):
     """Loads a ClovisTask from a JSON string.
 
     Returns:
@@ -30,37 +39,36 @@ class ClovisTask(object):
     try:
       data = json.loads(json_dict)
       action = data['action']
-      params = data['params']
-      tag = data.get('taskqueue_tag')
+      action_params = data['action_params']
       # Vaidate the format.
       if action == 'trace':
-        urls = params['urls']
+        urls = action_params['urls']
         if (type(urls) is not list) or (len(urls) == 0):
           return None
       else:
         # When more actions are supported, check that they are valid here.
         return None
-      return cls(action, params, tag)
+      return cls(action, action_params, data.get('backend_params'))
     except Exception:
       return None
 
   @classmethod
   def FromBase64(cls, base64_string):
     """Loads a ClovisTask from a base 64 string."""
-    return ClovisTask.FromJsonDict(base64.b64decode(base64_string))
+    return ClovisTask.FromJsonString(base64.b64decode(base64_string))
 
-  def ToJsonDict(self):
+  def ToJsonString(self):
     """Returns the JSON representation of the task."""
-    task_dict = { 'action': self._action, 'params': self._params }
-    if self._taskqueue_tag:
-      task_dict['taskqueue_tag'] = self._taskqueue_tag
+    task_dict = {'action': self._action, 'action_params': self._action_params,
+                 'backend_params': self._backend_params}
     return json.dumps(task_dict)
 
   def Action(self):
     return self._action
 
-  def Params(self):
-    return self._params
+  def ActionParams(self):
+    return self._action_params
+
+  def BackendParams(self):
+    return self._backend_params
 
-  def TaskqueueTag(self):
-    return self._taskqueue_tag
diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
index 21cf120..bd678b4 100644
--- a/loading/cloud/common/google_instance_helper.py
+++ b/loading/cloud/common/google_instance_helper.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import json
+from googleapiclient import (discovery, errors)
 import time
 
 from googleapiclient import (discovery, errors)
@@ -11,21 +12,22 @@ from googleapiclient import (discovery, errors)
 class GoogleInstanceHelper(object):
   """Helper class for the Google Compute API, allowing to manage groups of
   instances more easily. Groups of instances are identified by a tag."""
-  _COMPUTE_API_ROOT = 'https://www.googleapis.com/compute/v1/projects/'
 
   def __init__(self, credentials, project, logger):
     self._compute_api = discovery.build('compute','v1', credentials=credentials)
     self._project = project
-    self._project_api_url = self._COMPUTE_API_ROOT + project
+    self._api_url = 'https://www.googleapis.com/compute/v1/projects/' + project
     self._zone = 'europe-west1-c'
     self._logger = logger
 
   def _ExecuteApiRequest(self, request, retry_count=3):
     """ Executes a Compute API request and returns True on success."""
-    self._logger.info('Compute API request:\n' + request.to_json())
+    self._logger.info('Compute API request:')
+    self._logger.info(request.to_json())
     try:
       response = request.execute()
-      self._logger.info('Compute API response:\n' + response)
+      self._logger.info('Compute API response:')
+      self._logger.info(response)
       return True
     except errors.HttpError as err:
       error_content = self._GetErrorContent(err)
@@ -54,7 +56,7 @@ class GoogleInstanceHelper(object):
 
   def _GetErrorContent(self, error):
     """Returns the contents of an error returned by the Compute API as a
-    dictionary or None.
+    dictionary.
     """
     if not error.resp.get('content-type', '').startswith('application/json'):
       return None
@@ -68,22 +70,22 @@ class GoogleInstanceHelper(object):
         not error_content['error'].get('errors')):
       return None
     error_list = error_content['error']['errors']
-    if not error_list:
+    if len(error_list) == 0:
       return None
-    return error_list[0].get('reason')
+    return error_list[0].get('reason', '')
 
   def CreateTemplate(self, tag, bucket):
     """Creates an instance template for instances identified by tag and using
     bucket for deployment. Returns True if successful.
     """
-    image_url = self._COMPUTE_API_ROOT + \
+    image_url = 'https://www.googleapis.com/compute/v1/projects/' \
                 'ubuntu-os-cloud/global/images/ubuntu-1404-trusty-v20160406'
     request_body = {
         'name': self._GetTemplateName(tag),
         'properties': {
             'machineType': 'n1-standard-1',
             'networkInterfaces': [{
-                'network': self._project_api_url + '/global/networks/default',
+                'network': self._api_url + '/global/networks/default',
                 'accessConfigs': [{
                     'name': 'external-IP',
                     'type': 'ONE_TO_ONE_NAT'
@@ -128,7 +130,7 @@ class GoogleInstanceHelper(object):
     exist for this to succeed. Returns True if successful.
     """
     template_url = '%s/global/instanceTemplates/%s' % (
-        self._project_api_url, self._GetTemplateName(tag))
+        self._api_url, self._GetTemplateName(tag))
     request_body = {
         'zone': self._zone, 'targetSize': instance_count,
         'baseInstanceName': 'instance-' + tag,
@@ -146,7 +148,7 @@ class GoogleInstanceHelper(object):
     # The instance hostname may be of the form <name>.c.<project>.internal but
     # only the <name> part should be passed to the compute API.
     name = instance_hostname.split('.')[0]
-    instance_url = self._project_api_url + (
+    instance_url = self._api_url + (
         "/zones/%s/instances/%s" % (self._zone, name))
     request = self._compute_api.instanceGroupManagers().deleteInstances(
         project=self._project, zone=self._zone,
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
index f6f6f5f..7720203 100644
--- a/loading/cloud/frontend/README.md
+++ b/loading/cloud/frontend/README.md
@@ -8,13 +8,24 @@ Visit the application URL in your browser, and upload a JSON dictionary with the
 following keys:
 
 -   `action` (string): the action to perform. Only `trace` is supported.
--   `params` (dictionary): the parameters associated to the action. See below
-    for more details.
--   `taskqueue_tag` (string, optional): the [TaskQueue][2] tag internally used
-    to send the work from AppEngine to ComputeEngine.  If this parameter is not
-    specified, a unique tag will be created.
+-   `action_params` (dictionary): the parameters associated to the action.
+    See below for more details.
+-   `backend_params` (dictionary): the parameters configuring the backend for
+    this task. See below for more details.
 
-### Parameters for the `trace` action.
+### Parameters for `backend_params`
+
+-   `instance_count` (int): Number of Compute Engine instances that will be
+    started for this task.
+-   `storage_bucket` (string): Name of the storage bucket used by the backend
+    instances. Backend code and data must have been previously deployed to this
+    bucket using the `deploy.sh` [script][4].
+-   `tag` (string, optional): tag internally used to associate tasks to backend
+    ComputeEngine instances. This parameter should not be set in general, as it
+    is mostly exposed for development purposes. If this parameter is not
+    specified, a unique tag will be generated.
+
+### Parameters for the `trace` action
 
 -   `urls` (list of strings): the list of URLs to process.
 -   `repeat_count` (integer, optional): the number of traces to be generated
@@ -67,19 +78,20 @@ following keys:
 # Chromium checkout, see the cleanup intructions below.
 pip install -r requirements.txt -t lib
 # Start the local server.
-dev_appserver.py .
+dev_appserver.py -A $PROJECT_NAME .
 ```
 
 Visit the application [http://localhost:8080](http://localhost:8080).
 
 After you are done, cleanup your Chromium checkout:
+
 ```shell
 rm -rf $CHROMIUM_SRC/tools/android/loading/frontend/lib
 ```
 
 ### Deploy
 
-````shell
+```shell
 # Install dependencies in the lib/ directory.
 pip install -r requirements.txt -t lib
 # Deploy.
@@ -89,3 +101,4 @@ gcloud preview app deploy app.yaml
 [1]: https://cloud.google.com/sdk
 [2]: https://cloud.google.com/appengine/docs/python/taskqueue
 [3]: https://cloud.google.com/appengine/docs/python/config/queue
+[4]: ../backend/README.md#Deploy-the-code
diff --git a/loading/cloud/frontend/app.yaml b/loading/cloud/frontend/app.yaml
index 7dcc1d6..6544088 100644
--- a/loading/cloud/frontend/app.yaml
+++ b/loading/cloud/frontend/app.yaml
@@ -8,3 +8,7 @@ handlers:
 
 - url: .*
   script: clovis_frontend.app
+
+libraries:
+- name: ssl
+  version: latest
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
index 8eeee35..b037a3d 100644
--- a/loading/cloud/frontend/clovis_frontend.py
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -3,52 +3,112 @@
 # found in the LICENSE file.
 
 import flask
-from google.appengine.api import taskqueue
-import json
+from google.appengine.api import (app_identity, taskqueue)
+from oauth2client.client import GoogleCredentials
+import logging
 import os
 import sys
-import uuid
 
 from common.clovis_task import ClovisTask
+import common.google_instance_helper
+from memory_logs import MemoryLogs
+
+
+# Global variables.
+clovis_logger = logging.getLogger('clovis_frontend')
+clovis_logger.setLevel(logging.DEBUG)
+project_name = app_identity.get_application_id()
+instance_helper = common.google_instance_helper.GoogleInstanceHelper(
+    credentials=GoogleCredentials.get_application_default(),
+    project=project_name,
+    logger=clovis_logger)
+app = flask.Flask(__name__)
 
 
-app = flask.Flask(__name__)
+def Render(message, memory_logs):
+  return flask.render_template(
+      'log.html', body=message, log=memory_logs.Flush().split('\n'))
 
 
-def StartFromJson(http_body_str):
-  """Creates a new batch of tasks from its JSON representation."""
-  task = ClovisTask.FromJsonDict(http_body_str)
+def CreateInstanceTemplate(task):
+  """Create the Compute Engine instance template that will be used to create the
+  instances.
+  """
+  backend_params = task.BackendParams()
+  instance_count = backend_params.get('instance_count', 0)
+  if instance_count <= 0:
+    clovis_logger.info('No template required.')
+    return True
+  bucket = backend_params.get('storage_bucket')
+  if not bucket:
+    clovis_logger.error('Missing bucket in backend_params.')
+    return False
+  return instance_helper.CreateTemplate(task.BackendParams()['tag'], bucket)
+
+
+def CreateInstances(task):
+  """Creates the Compute engine requested by the task"""
+  backend_params = task.BackendParams()
+  instance_count = backend_params.get('instance_count', 0)
+  if instance_count <= 0:
+    clovis_logger.info('No instances to create.')
+    return True
+  return instance_helper.CreateInstances(backend_params['tag'], instance_count)
+
+
+def StartFromJsonString(http_body_str):
+  """Main function handling a JSON task posted by the user"""
+  # Set up logging.
+  memory_logs = MemoryLogs(clovis_logger)
+  memory_logs.Start()
+
+  # Load the task from JSON.
+  task = ClovisTask.FromJsonString(http_body_str)
   if not task:
-    return 'Invalid JSON task:\n%s\n' % http_body_str
+    clovis_logger.error('Invalid JSON task.')
+    return Render('Invalid JSON task:\n' + http_body_str, memory_logs)
 
-  task_tag = task.TaskqueueTag()
-  if not task_tag:
-    task_tag = uuid.uuid1()
+  task_tag = task.BackendParams()['tag']
 
+  # Create the instance template if required.
+  if not CreateInstanceTemplate(task):
+    return Render('Template creation failed.', memory_logs)
+
+  # Split the task in smaller tasks.
   sub_tasks = []
   if task.Action() == 'trace':
     sub_tasks = SplitTraceTask(task)
   else:
-    return 'Unsupported action: %s\n' % task.Action()
+    error_string = 'Unsupported action: %s.' % task.Action()
+    clovis_logger.error(error_string)
+    return Render(error_string, memory_logs)
+
+  if not EnqueueTasks(sub_tasks, task_tag):
+    return Render('Task creation failed', memory_logs)
+
+  # Start the instances if required.
+  if not CreateInstances(task):
+    return Render('Instance creation failed', memory_logs)
 
-  return EnqueueTasks(sub_tasks, task_tag)
+  return Render('Success', memory_logs)
 
 
 def SplitTraceTask(task):
-  """Split a tracing task with potentially many URLs into several tracing tasks
+  """Splits a tracing task with potentially many URLs into several tracing tasks
   with few URLs.
   """
-  params = task.Params()
-  urls = params['urls']
+  clovis_logger.debug('Splitting trace task.')
+  action_params = task.ActionParams()
+  urls = action_params['urls']
 
   # Split the task in smaller tasks with fewer URLs each.
   urls_per_task = 1
   sub_tasks = []
   for i in range(0, len(urls), urls_per_task):
-    sub_task_params = params.copy()
+    sub_task_params = action_params.copy()
     sub_task_params['urls'] = [url for url in urls[i:i+urls_per_task]]
     sub_tasks.append(ClovisTask(task.Action(), sub_task_params,
-                                task.TaskqueueTag()))
+                                task.BackendParams()))
   return sub_tasks
 
 
@@ -59,17 +119,16 @@ def EnqueueTasks(tasks, task_tag):
   q = taskqueue.Queue('clovis-queue')
   retry_options = taskqueue.TaskRetryOptions(task_retry_limit=3)
   # Add tasks to the queue by groups.
-  # TODO(droger): This support to thousands of tasks, but maybe not millions.
+  # TODO(droger): This supports thousands of tasks, but maybe not millions.
   # Defer the enqueuing if it times out.
-  # is too large.
   group_size = 100
   callbacks = []
   try:
     for i in range(0, len(tasks), group_size):
       group = tasks[i:i+group_size]
       taskqueue_tasks = [
-          taskqueue.Task(payload=task.ToJsonDict(), method='PULL', tag=task_tag,
-                         retry_options=retry_options)
+          taskqueue.Task(payload=task.ToJsonString(), method='PULL',
+                         tag=task_tag, retry_options=retry_options)
           for task in group]
       rpc = taskqueue.create_rpc()
       q.add_async(task=taskqueue_tasks, rpc=rpc)
@@ -77,8 +136,10 @@ def EnqueueTasks(tasks, task_tag):
     for callback in callbacks:
       callback.get_result()
   except Exception as e:
-    return 'Exception:' + type(e).__name__ + ' ' + str(e.args) + '\n'
-  return 'pushed %i tasks with tag: %s\n' % (len(tasks), task_tag)
+    clovis_logger.error('Exception:' + type(e).__name__ + ' ' + str(e.args))
+    return False
+  clovis_logger.info('Pushed %i tasks with tag: %s' % (len(tasks), task_tag))
+  return True
 
 
 @app.route('/')
@@ -94,7 +155,7 @@ def StartFromForm():
   if not data_stream:
     return 'failed'
   http_body_str = data_stream.read()
-  return StartFromJson(http_body_str)
+  return StartFromJsonString(http_body_str)
 
 
 @app.errorhandler(404)
diff --git a/loading/cloud/frontend/requirements.txt b/loading/cloud/frontend/requirements.txt
index 880a7bc..483195b 100644
--- a/loading/cloud/frontend/requirements.txt
+++ b/loading/cloud/frontend/requirements.txt
@@ -1 +1,2 @@
 Flask==0.10
+google-api-python-client
diff --git a/loading/cloud/frontend/templates/log.html b/loading/cloud/frontend/templates/log.html
new file mode 100644
index 0000000..4717896
--- /dev/null
+++ b/loading/cloud/frontend/templates/log.html
@@ -0,0 +1,39 @@
+{# Template for a page displaying a body and logs (optional) under a collapsible
+   section.
+#}
+<!DOCTYPE html>
+<html>
+<head>
+<meta charset="utf-8">
+<title>Clovis</title>
+</head>
+
+<body>
+{{ body }}
+
+{% if log %}
+
+<p><a onclick="javascript:ShowHide('HiddenDiv'); return false;" href="#">
+  Show/hide details
+</a></p>
+<div id="HiddenDiv" style="display:none";>
+
+{# Loop over the lines of the log to add linebreaks. #}
+{%- for line in log -%}
+  {{ line }}<br/>
+{%- endfor -%}
+
+</div>
+
+<script type="text/javascript">
+function ShowHide(divId) {
+  element = document.getElementById(divId)
+  element.style.display = (element.style.display == 'none') ? 'block' : 'none';
+}
+</script>
+
+{% endif %}
+
+</body>
+
+</html>

commit 01d8d51d6e00c9b73596c90d65d1dd2f39d03a4d
Author: droger <droger@chromium.org>
Date:   Mon Apr 25 16:24:53 2016 -0700

    tools/android/loading Helper to collect logs in a memory buffer
    
    Review URL: https://codereview.chromium.org/1921593002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389602}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3b3582e12c4c761637b8b8269895226f2c10512f

diff --git a/loading/cloud/frontend/memory_logs.py b/loading/cloud/frontend/memory_logs.py
new file mode 100644
index 0000000..afd1897
--- /dev/null
+++ b/loading/cloud/frontend/memory_logs.py
@@ -0,0 +1,34 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+from StringIO import StringIO
+
+
+class MemoryLogs(object):
+  """Collects logs in memory."""
+
+  def __init__(self, logger):
+    self._logger = logger
+    self._log_buffer = StringIO()
+    self._log_handler = logging.StreamHandler(self._log_buffer)
+    formatter = logging.Formatter("[%(asctime)s][%(levelname)s] %(message)s",
+                                  "%y-%m-%d %H:%M:%S")
+    self._log_handler.setFormatter(formatter)
+
+  def Start(self):
+    """Starts collecting the logs."""
+    self._logger.addHandler(self._log_handler)
+
+  def Flush(self):
+    """Stops collecting the logs and returns the logs collected since Start()
+    was called.
+    """
+    self._logger.removeHandler(self._log_handler)
+    self._log_handler.flush()
+    self._log_buffer.flush()
+    result = self._log_buffer.getvalue()
+    self._log_buffer.truncate(0)
+    return result
+

commit c79f039d0ec04d799bf044b42985ee9d6e30eb77
Author: droger <droger@chromium.org>
Date:   Mon Apr 25 15:03:38 2016 -0700

    tools/android/loading Helper to create Compute instances from Appengine
    
    Review URL: https://codereview.chromium.org/1920793003
    
    Cr-Original-Commit-Position: refs/heads/master@{#389577}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 93f1eeda75312f2ab5a027cd4a86353b4ab2ec5a

diff --git a/loading/cloud/common/google_instance_helper.py b/loading/cloud/common/google_instance_helper.py
new file mode 100644
index 0000000..21cf120
--- /dev/null
+++ b/loading/cloud/common/google_instance_helper.py
@@ -0,0 +1,155 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import time
+
+from googleapiclient import (discovery, errors)
+
+
+class GoogleInstanceHelper(object):
+  """Helper class for the Google Compute API, allowing to manage groups of
+  instances more easily. Groups of instances are identified by a tag."""
+  _COMPUTE_API_ROOT = 'https://www.googleapis.com/compute/v1/projects/'
+
+  def __init__(self, credentials, project, logger):
+    self._compute_api = discovery.build('compute','v1', credentials=credentials)
+    self._project = project
+    self._project_api_url = self._COMPUTE_API_ROOT + project
+    self._zone = 'europe-west1-c'
+    self._logger = logger
+
+  def _ExecuteApiRequest(self, request, retry_count=3):
+    """ Executes a Compute API request and returns True on success."""
+    self._logger.info('Compute API request:\n' + request.to_json())
+    try:
+      response = request.execute()
+      self._logger.info('Compute API response:\n' + response)
+      return True
+    except errors.HttpError as err:
+      error_content = self._GetErrorContent(err)
+      error_reason = self._GetErrorReason(error_content)
+      if error_reason == 'resourceNotReady' and retry_count > 0:
+        # Retry after a delay
+        delay_seconds = 1
+        self._logger.info(
+            'Resource not ready, retrying in %i seconds.' % delay_seconds)
+        time.sleep(delay_seconds)
+        return self._ExecuteApiRequest(request, retry_count - 1)
+      else:
+        self._logger.error('Compute API error (reason: "%s"):\n%s' % (
+            error_reason, err))
+        if error_content:
+          self._logger.error('Error details:\n%s' % error_content)
+        return False
+
+  def _GetTemplateName(self, tag):
+    """Returns the name of the instance template associated with tag."""
+    return 'template-' + tag
+
+  def _GetInstanceGroupName(self, tag):
+    """Returns the name of the instance group associated with tag."""
+    return 'group-' + tag
+
+  def _GetErrorContent(self, error):
+    """Returns the contents of an error returned by the Compute API as a
+    dictionary or None.
+    """
+    if not error.resp.get('content-type', '').startswith('application/json'):
+      return None
+    return json.loads(error.content)
+
+  def _GetErrorReason(self, error_content):
+    """Returns the error reason as a string."""
+    if not error_content:
+      return None
+    if (not error_content.get('error') or
+        not error_content['error'].get('errors')):
+      return None
+    error_list = error_content['error']['errors']
+    if not error_list:
+      return None
+    return error_list[0].get('reason')
+
+  def CreateTemplate(self, tag, bucket):
+    """Creates an instance template for instances identified by tag and using
+    bucket for deployment. Returns True if successful.
+    """
+    image_url = self._COMPUTE_API_ROOT + \
+                'ubuntu-os-cloud/global/images/ubuntu-1404-trusty-v20160406'
+    request_body = {
+        'name': self._GetTemplateName(tag),
+        'properties': {
+            'machineType': 'n1-standard-1',
+            'networkInterfaces': [{
+                'network': self._project_api_url + '/global/networks/default',
+                'accessConfigs': [{
+                    'name': 'external-IP',
+                    'type': 'ONE_TO_ONE_NAT'
+            }]}],
+            'disks': [{
+                'type': 'PERSISTENT',
+                'boot': True,
+                'autoDelete': True,
+                'mode': 'READ_WRITE',
+                'initializeParams': {'sourceImage': image_url}}],
+            'canIpForward': False,
+            'scheduling': {
+                'automaticRestart': True,
+                'onHostMaintenance': 'MIGRATE',
+                'preemptible': False},
+            'serviceAccounts': [{
+                'scopes': [
+                    'https://www.googleapis.com/auth/cloud-platform',
+                    'https://www.googleapis.com/auth/cloud-taskqueue'],
+                'email': 'default'}],
+            'metadata': { 'items': [
+                {'key': 'cloud-storage-path',
+                 'value': bucket},
+                {'key': 'startup-script-url',
+                 'value': 'gs://%s/deployment/startup-script.sh' % bucket},
+                {'key': 'taskqueue_tag', 'value': tag}]}}}
+    request = self._compute_api.instanceTemplates().insert(
+        project=self._project, body=request_body)
+    return self._ExecuteApiRequest(request)
+
+  def DeleteTemplate(self, tag):
+    """Deletes the instance template associated with tag. Returns True if
+    successful.
+    """
+    request = self._compute_api.instanceTemplates().delete(
+        project=self._project,
+        instanceTemplate=self._GetTemplateName(tag))
+    return self._ExecuteApiRequest(request)
+
+  def CreateInstances(self, tag, instance_count):
+    """Creates an instance group associated with tag. The instance template must
+    exist for this to succeed. Returns True if successful.
+    """
+    template_url = '%s/global/instanceTemplates/%s' % (
+        self._project_api_url, self._GetTemplateName(tag))
+    request_body = {
+        'zone': self._zone, 'targetSize': instance_count,
+        'baseInstanceName': 'instance-' + tag,
+        'instanceTemplate': template_url,
+        'name': self._GetInstanceGroupName(tag)}
+    request = self._compute_api.instanceGroupManagers().insert(
+        project=self._project, zone=self._zone,
+        body=request_body)
+    return self._ExecuteApiRequest(request)
+
+  def DeleteInstance(self, tag, instance_hostname):
+    """Deletes one instance from the instance group identified with tag. Returns
+    True if successful.
+    """
+    # The instance hostname may be of the form <name>.c.<project>.internal but
+    # only the <name> part should be passed to the compute API.
+    name = instance_hostname.split('.')[0]
+    instance_url = self._project_api_url + (
+        "/zones/%s/instances/%s" % (self._zone, name))
+    request = self._compute_api.instanceGroupManagers().deleteInstances(
+        project=self._project, zone=self._zone,
+        instanceGroupManager=self._GetInstanceGroupName(tag),
+        body={'instances': [instance_url]})
+    return self._ExecuteApiRequest(request)

commit c7668e8bfa06f5d01c9e6a7e0d4cb44c48339a04
Author: lizeb <lizeb@chromium.org>
Date:   Mon Apr 25 08:56:30 2016 -0700

    clovis: Computes the amount of data to re-download for repeated visits.
    
    This CL has two parts:
    - Replicate the caching policy mandated by the RFC (and Chrome's
      heuristics)
    - Use this to show the amount of data to re-download for repeated
      visits.
    
    Review URL: https://codereview.chromium.org/1916443002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389486}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 71705d9fc20cb8362d9962a25534c3cf59a579fb

diff --git a/loading/metrics.py b/loading/metrics.py
new file mode 100644
index 0000000..b56c54c
--- /dev/null
+++ b/loading/metrics.py
@@ -0,0 +1,103 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Descriptive metrics for Clovis.
+
+When executed as a script, shows a graph of the amount of data to download for
+a new visit to the same page, with a given time interval.
+"""
+
+
+from request_track import CachingPolicy
+
+
+def _RequestTransferSize(request):
+  def HeadersSize(headers):
+    # 4: ':', ' ', '\r', '\n'
+    return sum(len(k) + len(v) + 4 for (k, v) in headers.items())
+  if request.protocol == 'data':
+    return {'get': 0, 'request_headers': 0, 'response_headers': 0, 'body': 0}
+  return {'get': len('GET ') + len(request.url) + 2,
+          'request_headers': HeadersSize(request.request_headers or {}),
+          'response_headers': HeadersSize(request.response_headers or {}),
+          'body': request.encoded_data_length}
+
+
+def TotalTransferSize(trace):
+  """Returns the total transfer size (uploaded, downloaded) from a trace.
+
+  This is an estimate as we assume:
+  - 200s (for the size computation)
+  - GET only.
+  """
+  uploaded_bytes = 0
+  downloaded_bytes = 0
+  for request in trace.request_track.GetEvents():
+    request_bytes = _RequestTransferSize(request)
+    uploaded_bytes += request_bytes['get'] + request_bytes['request_headers']
+    downloaded_bytes += (len('HTTP/1.1 200 OK')
+                         + request_bytes['response_headers']
+                         + request_bytes['body'])
+  return (uploaded_bytes, downloaded_bytes)
+
+
+def TransferredDataRevisit(trace, after_time_s, assume_validation_ok=False):
+  """Returns the amount of data transferred for a revisit.
+
+  Args:
+    trace: (LoadingTrace) loading trace.
+    after_time_s: (float) Time in s after which the site is revisited.
+    assume_validation_ok: (bool) Assumes that the resources to validate return
+                          304s.
+
+  Returns:
+    (uploaded_bytes, downloaded_bytes)
+  """
+  uploaded_bytes = 0
+  downloaded_bytes = 0
+  for request in trace.request_track.GetEvents():
+    caching_policy = CachingPolicy(request)
+    policy = caching_policy.PolicyAtDate(request.wall_time + after_time_s)
+    request_bytes = _RequestTransferSize(request)
+    if policy == CachingPolicy.VALIDATION_NONE:
+      continue
+    uploaded_bytes += request_bytes['get'] + request_bytes['request_headers']
+    if (policy in (CachingPolicy.VALIDATION_SYNC,
+                   CachingPolicy.VALIDATION_ASYNC)
+        and caching_policy.HasValidators() and assume_validation_ok):
+      downloaded_bytes += len('HTTP/1.1 304 NOT MODIFIED\r\n')
+      continue
+    downloaded_bytes += (len('HTTP/1.1 200 OK\r\n')
+                         + request_bytes['response_headers']
+                         + request_bytes['body'])
+  return (uploaded_bytes, downloaded_bytes)
+
+
+def PlotTransferSizeVsTimeBetweenVisits(trace):
+  times = [10, 60, 300, 600, 3600, 4 * 3600, 12 * 3600, 24 * 3600]
+  labels = ['10s', '1m', '10m', '1h', '4h', '12h', '1d']
+  (_, total_downloaded) = TotalTransferSize(trace)
+  downloaded = [TransferredDataRevisit(trace, delta_t)[1] for delta_t in times]
+  plt.figure()
+  plt.title('Amount of data to download for a revisit - %s' % trace.url)
+  plt.xlabel('Time between visits (log)')
+  plt.ylabel('Amount of data (bytes)')
+  plt.plot(times, downloaded, 'k+--')
+  plt.axhline(total_downloaded, color='k', linewidth=2)
+  plt.xscale('log')
+  plt.xticks(times, labels)
+  plt.show()
+
+
+def main(trace_filename):
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_filename)
+  PlotTransferSizeVsTimeBetweenVisits(trace)
+
+
+if __name__ == '__main__':
+  import sys
+  from matplotlib import pylab as plt
+  import loading_trace
+
+  main(sys.argv[1])
diff --git a/loading/metrics_unittest.py b/loading/metrics_unittest.py
new file mode 100644
index 0000000..cc439ce
--- /dev/null
+++ b/loading/metrics_unittest.py
@@ -0,0 +1,75 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import unittest
+
+import metrics
+import request_track
+import test_utils
+
+
+class MetricsTestCase(unittest.TestCase):
+  _BODY_SIZE = 14187
+  _URL = 'http://www.example.com/'
+  _REQUEST_HEADERS_SIZE = (len(_URL) + len('GET ') + 2
+                           + len('Accept: Everything\r\n'))
+  _RESPONSE_HEADERS_SIZE = 124
+  _REQUEST = {
+      'encoded_data_length': _BODY_SIZE,
+      'request_id': '2291.1',
+      'request_headers': {
+          'Accept': 'Everything',
+      },
+      'response_headers': {
+          'Age': '866',
+          'Content-Length': str(_BODY_SIZE),
+          'Etag': 'ABCD',
+          'Date': 'Fri, 22 Apr 2016 08:56:19 -0200',
+          'Vary': 'Accept-Encoding',
+      },
+      'timestamp': 5535648.730768,
+      'timing': {
+          'receive_headers_end': 47.0650000497699,
+          'request_time': 5535648.73264,
+      },
+      'url': _URL,
+      'status': 200,
+      'wall_time': 1461322579.59422}
+
+  def testTransferredDataRevisitNoCache(self):
+    trace = self._MakeTrace()
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 10)
+    self.assertEqual(self._REQUEST_HEADERS_SIZE, uploaded)
+    self.assertEqual(self._BODY_SIZE + self._RESPONSE_HEADERS_SIZE, downloaded)
+
+  def testTransferredDataRevisitNoCacheAssumeValidates(self):
+    trace = self._MakeTrace()
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 10, True)
+    self.assertEqual(self._REQUEST_HEADERS_SIZE, uploaded)
+    not_modified_length = len('HTTP/1.1 304 NOT MODIFIED\r\n')
+    self.assertEqual(not_modified_length, downloaded)
+
+  def testTransferredDataRevisitCacheable(self):
+    trace = self._MakeTrace()
+    r = trace.request_track.GetEvents()[0]
+    r.response_headers['Cache-Control'] = 'max-age=1000'
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 10)
+    self.assertEqual(0, uploaded)
+    self.assertEqual(0, downloaded)
+    (uploaded, downloaded) = metrics.TransferredDataRevisit(trace, 1000)
+    self.assertEqual(self._REQUEST_HEADERS_SIZE, uploaded)
+    cache_control_length = len('Cache-Control: max-age=1000\r\n')
+    self.assertEqual(
+        self._BODY_SIZE + self._RESPONSE_HEADERS_SIZE + cache_control_length,
+        downloaded)
+
+  @classmethod
+  def _MakeTrace(cls):
+    request = request_track.Request.FromJsonDict(copy.deepcopy(cls._REQUEST))
+    return test_utils.LoadingTraceFromEvents([request])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index 613f6e0..31eca51 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -10,6 +10,8 @@ When executed, parses a JSON dump of DevTools messages.
 import bisect
 import collections
 import copy
+import datetime
+import email.utils
 import json
 import logging
 import re
@@ -248,6 +250,21 @@ class Request(object):
         break
     return result
 
+  def GetResponseHeaderValue(self, header, value):
+    """Returns True iff the response headers |header| contains |value|."""
+    header_values = self.GetHTTPResponseHeader(header)
+    if not header_values:
+      return None
+    values = header_values.split(',')
+    for header_value in values:
+      if header_value.lower() == value.lower():
+        return header_value
+    return None
+
+  def HasResponseHeaderValue(self, header, value):
+    """Returns True iff the response headers |header| contains |value|."""
+    return self.GetResponseHeaderValue(header, value) is not None
+
   def GetContentType(self):
     """Returns the content type, or None."""
     # Check for redirects. Use the "Location" header, because the HTTP status is
@@ -272,6 +289,21 @@ class Request(object):
   def IsDataRequest(self):
     return self.protocol == 'data'
 
+  def GetCacheControlDirective(self, directive_name):
+    """Returns the value of a Cache-Control directive, or None."""
+    cache_control_str = self.GetHTTPResponseHeader('Cache-Control')
+    if cache_control_str is None:
+      return None
+    directives = [s.strip() for s in cache_control_str.split(',')]
+    for directive in directives:
+      parts = directive.split('=')
+      if len(parts) == 1:
+        continue
+      (name, value) = parts
+      if name == directive_name:
+        return value
+    return None
+
   def MaxAge(self):
     """Returns the max-age of a resource, or -1."""
     # TODO(lizeb): Handle the "Expires" header as well.
@@ -292,11 +324,9 @@ class Request(object):
         or u'no-cache' in cache_control
         or len(cache_control) == 0):
       return -1
-    if 'max-age' in cache_control:
-      age_match = re.match(r'\s*(\d+)+', cache_control['max-age'])
-      if not age_match:
-        return -1
-      return int(age_match.group(1))
+    max_age = self.GetCacheControlDirective('max-age')
+    if max_age:
+      return int(max_age)
     return -1
 
   def Cost(self):
@@ -316,6 +346,126 @@ class Request(object):
     return json.dumps(self.ToJsonDict(), sort_keys=True, indent=2)
 
 
+class CachingPolicy(object):
+  """Represents the caching policy at an arbitrary time for a cached response.
+  """
+  FETCH = 'FETCH'
+  VALIDATION_NONE = 'VALIDATION_NONE'
+  VALIDATION_SYNC = 'VALIDATION_SYNC'
+  VALIDATION_ASYNC = 'VALIDATION_ASYNC'
+  POLICIES = (FETCH, VALIDATION_NONE, VALIDATION_SYNC, VALIDATION_ASYNC)
+  def __init__(self, request):
+    """Constructor.
+
+    Args:
+      request: (Request)
+    """
+    assert request.response_headers is not None
+    self.request = request
+    # This is incorrect, as the timestamp corresponds to when devtools is made
+    # aware of the request, not when it was sent. However, this is good enough
+    # for computing cache expiration, which doesn't need sub-second precision.
+    self._request_time = self.request.wall_time
+    # Used when the date is not available.
+    self._response_time = (
+        self._request_time + self.request.timing.receive_headers_end)
+
+  def HasValidators(self):
+    """Returns wether the request has a validator."""
+    # Assuming HTTP 1.1+.
+    return (self.request.GetHTTPResponseHeader('Last-Modified')
+            or self.request.GetHTTPResponseHeader('Etag'))
+
+  def IsCacheable(self):
+    """Returns whether the request could be stored in the cache."""
+    return not self.request.HasResponseHeaderValue('Cache-Control', 'no-store')
+
+  def PolicyAtDate(self, timestamp):
+    """Returns the caching policy at an aribitrary timestamp.
+
+    Args:
+      timestamp: (float) Seconds since Epoch.
+
+    Returns:
+      A policy in POLICIES.
+    """
+    # Note: the implementation is largely transcribed from
+    # net/http/http_response_headers.cc, itself following RFC 2616.
+    if not self.IsCacheable():
+      return self.FETCH
+    freshness = self._GetFreshnessLifetimes()
+    if freshness[0] == 0 and freshness[1] == 0:
+      return self.VALIDATION_SYNC
+    age = self._GetCurrentAge(timestamp)
+    if freshness[0] > age:
+      return self.VALIDATION_NONE
+    if freshness[1] > age:
+      return self.VALIDATION_ASYNC
+    return self.VALIDATION_SYNC
+
+  def _GetFreshnessLifetimes(self):
+    """Returns [freshness, stale-while-revalidate freshness] in seconds."""
+    # This is adapted from GetFreshnessLifetimes() in
+    # //net/http/http_response_headers.cc (which follows the RFC).
+    r = self.request
+    result = [0, 0]
+    if (r.HasResponseHeaderValue('Cache-Control', 'no-cache')
+        or r.HasResponseHeaderValue('Cache-Control', 'no-store')
+        or r.HasResponseHeaderValue('Vary', '*')):  # RFC 2616, 13.6.
+      return result
+    must_revalidate = r.HasResponseHeaderValue(
+        'Cache-Control', 'must-revalidate')
+    swr_header = r.GetCacheControlDirective('stale-while-revalidate')
+    if not must_revalidate and swr_header:
+      result[1] = int(swr_header)
+
+    max_age_header = r.GetCacheControlDirective('max-age')
+    if max_age_header:
+      result[0] = int(max_age_header)
+      return result
+
+    date = self._GetDateValue('Date') or self._response_time
+    expires = self._GetDateValue('Expires')
+    if expires:
+      result[0] = expires - date
+      return result
+
+    if self.request.status in (200, 203, 206) and not must_revalidate:
+      last_modified = self._GetDateValue('Last-Modified')
+      if last_modified and last_modified < date:
+        result[0] = (date - last_modified) / 10
+        return result
+
+    if self.request.status in (300, 301, 308, 410):
+      return [2**48, 0] # ~forever.
+    # No header -> not fresh.
+    return result
+
+  def _GetDateValue(self, name):
+    date_str = self.request.GetHTTPResponseHeader(name)
+    if not date_str:
+      return None
+    parsed_date = email.utils.parsedate_tz(date_str)
+    if parsed_date is None:
+      return None
+    return email.utils.mktime_tz(parsed_date)
+
+  def _GetCurrentAge(self, current_time):
+    # See GetCurrentAge() in //net/http/http_response_headers.cc.
+    r = self.request
+    date_value = self._GetDateValue('Date') or self._response_time
+    age_value = int(r.GetHTTPResponseHeader('Age') or '0')
+
+    apparent_age = max(0, self._response_time - date_value)
+    corrected_received_age = max(apparent_age, age_value)
+    response_delay = self._response_time - self._request_time
+    corrected_initial_age = corrected_received_age + response_delay
+    resident_time = current_time - self._response_time
+    current_age = corrected_initial_age + resident_time
+
+    return current_age
+
+
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
   _REDIRECT_SUFFIX = '.redirect'
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index a613397..19f7b4a 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -6,7 +6,8 @@ import copy
 import json
 import unittest
 
-from request_track import (TimeBetween, Request, RequestTrack, Timing)
+from request_track import (TimeBetween, Request, CachingPolicy, RequestTrack,
+                           Timing)
 
 
 class TimeBetweenTestCase(unittest.TestCase):
@@ -74,6 +75,133 @@ class RequestTestCase(unittest.TestCase):
     self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
 
 
+class CachingPolicyTestCase(unittest.TestCase):
+  _REQUEST = {
+      'encoded_data_length': 14726,
+      'request_id': '2291.1',
+      'response_headers': {
+          'Age': '866',
+          'Content-Length': '14187',
+          'Date': 'Fri, 22 Apr 2016 08:56:19 -0200',
+          'Vary': 'Accept-Encoding',
+      },
+      'timestamp': 5535648.730768,
+      'timing': {
+          'connect_end': 34.0510001406074,
+          'connect_start': 21.6859998181462,
+          'dns_end': 21.6859998181462,
+          'dns_start': 0,
+          'loading_finished': 58.76399949193001,
+          'receive_headers_end': 47.0650000497699,
+          'request_time': 5535648.73264,
+          'send_end': 34.6099995076656,
+          'send_start': 34.2979999259114
+      },
+      'url': 'http://www.example.com/',
+      'status': 200,
+      'wall_time': 1461322579.59422}
+
+  def testHasValidators(self):
+    r = self._MakeRequest()
+    self.assertFalse(CachingPolicy(r).HasValidators())
+    r.response_headers['Last-Modified'] = 'Yesterday all my troubles'
+    self.assertTrue(CachingPolicy(r).HasValidators())
+    r = self._MakeRequest()
+    r.response_headers['ETAG'] = 'ABC'
+    self.assertTrue(CachingPolicy(r).HasValidators())
+
+  def testIsCacheable(self):
+    r = self._MakeRequest()
+    self.assertTrue(CachingPolicy(r).IsCacheable())
+    r.response_headers['Cache-Control'] = 'Whatever,no-store'
+    self.assertFalse(CachingPolicy(r).IsCacheable())
+
+  def testPolicyNoStore(self):
+    r = self._MakeRequest()
+    r.response_headers['Cache-Control'] = 'Whatever,no-store'
+    self.assertEqual(CachingPolicy.FETCH, CachingPolicy(r).PolicyAtDate(0))
+
+  def testPolicyMaxAge(self):
+    r = self._MakeRequest()
+    r.response_headers['Cache-Control'] = 'whatever,max-age=1000,whatever'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 10000))
+    # Take current age into account.
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 500))
+    # Max-Age before Expires.
+    r.response_headers['Expires'] = 'Thu, 21 Apr 2016 00:00:00 -0200'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    # Max-Age < age
+    r.response_headers['Cache-Control'] = 'whatever,max-age=100,whatever'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 2))
+
+  def testPolicyExpires(self):
+    r = self._MakeRequest()
+    # Already expired
+    r.response_headers['Expires'] = 'Thu, 21 Apr 2016 00:00:00 -0200'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    r.response_headers['Expires'] = 'Thu, 25 Apr 2016 00:00:00 -0200'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,\
+        CachingPolicy(r).PolicyAtDate(r.wall_time))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 86400))
+    self.assertEqual(CachingPolicy.VALIDATION_SYNC,
+                     CachingPolicy(r).PolicyAtDate(r.wall_time + 86400 * 5))
+
+  def testStaleWhileRevalidate(self):
+    r = self._MakeRequest()
+    r.response_headers['Cache-Control'] = (
+        'whatever,max-age=100,stale-while-revalidate=2000')
+    self.assertEqual(
+        CachingPolicy.VALIDATION_ASYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 200))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 2000))
+    # must-revalidate overrides stale-while-revalidate.
+    r.response_headers['Cache-Control'] += ',must-revalidate'
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 200))
+
+  def test301NeverExpires(self):
+    r = self._MakeRequest()
+    r.status = 301
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 2000))
+
+  def testLastModifiedHeuristic(self):
+    r = self._MakeRequest()
+    # 8 hours ago.
+    r.response_headers['Last-Modified'] = 'Fri, 22 Apr 2016 00:56:19 -0200'
+    del r.response_headers['Age']
+    self.assertEqual(
+        CachingPolicy.VALIDATION_NONE,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 60))
+    self.assertEqual(
+        CachingPolicy.VALIDATION_SYNC,
+        CachingPolicy(r).PolicyAtDate(r.wall_time + 3600))
+
+  @classmethod
+  def _MakeRequest(cls):
+    return Request.FromJsonDict(copy.deepcopy(cls._REQUEST))
+
+
 class RequestTrackTestCase(unittest.TestCase):
   _REQUEST_WILL_BE_SENT = {
       'method': 'Network.requestWillBeSent',

commit 49ae5151989d0d7e72f98753ebb2e0130875f09f
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 22 02:09:03 2016 -0700

    clovis: Record the trace collection time when the navigation starts.
    
    Trace collection time is useful when looking at "Expires" headers. It is
    currently recorded when the DevTools connection is established, which is
    not when the page starts loading. This moves it later.
    
    Review URL: https://codereview.chromium.org/1912703002
    
    Cr-Original-Commit-Position: refs/heads/master@{#389058}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: efe753e724d5b30a7359a345bc76c031ca30c8fe

diff --git a/loading/controller.py b/loading/controller.py
index 0511998..96eec7f 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,7 +10,6 @@ desktop-specific versions.
 """
 
 import contextlib
-import datetime
 import logging
 import os
 import shutil
@@ -175,8 +174,6 @@ class ChromeControllerBase(object):
     if self._emulated_network:
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
-    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
-                          seconds_since_epoch=time.time())
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index d4139f7..2a291e5 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -4,7 +4,9 @@
 
 """Represents the trace of a page load."""
 
+import datetime
 import json
+import time
 
 import devtools_monitor
 import page_track
@@ -94,8 +96,13 @@ class LoadingTrace(object):
     trace = tracing.TracingTrack(
         connection,
         additional_categories=additional_categories)
+    start_date_str = datetime.datetime.utcnow().isoformat()
+    seconds_since_epoch=time.time()
     connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
-    return cls(url, chrome_metadata, page, request, trace)
+    trace = cls(url, chrome_metadata, page, request, trace)
+    trace.metadata.update(date=start_date_str,
+                          seconds_since_epoch=seconds_since_epoch)
+    return trace
 
   @property
   def tracing_track(self):

commit da4f7a74a97f93eefc29c328cfebdc3627f0d15b
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 21 04:23:56 2016 -0700

    clovis: Convert Timing to a proper class.
    
    This overdue change remove some headaches, and more importantly,
    preserves compatibility with past traces when the timing events list is
    enriched.
    
    Review URL: https://codereview.chromium.org/1909893002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388742}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cf759fbe8f87d50598eb169df75522395300d2e6

diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index 6a64c86..7d1631b 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -7,7 +7,7 @@ import unittest
 
 from content_classification_lens import (ContentClassificationLens,
                                          _RulesMatcher)
-from request_track import (Request, TimingFromDict)
+from request_track import Request
 import test_utils
 
 
@@ -21,7 +21,7 @@ class ContentClassificationLensTestCase(unittest.TestCase):
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
                                    'status': 200,
-                                   'timing': TimingFromDict({})})
+                                   'timing': {}})
   _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
                    'frame_id': _MAIN_FRAME_ID},
                   {'method': 'Page.frameAttached',
@@ -100,7 +100,7 @@ class _MatcherTestCase(unittest.TestCase):
        'frame_id': '123.1',
        'initiator': {'type': 'other'},
        'timestamp': 2,
-       'timing': TimingFromDict({})})
+       'timing': {}})
 
   def testRemovesWhitelistRules(self):
     matcher = _RulesMatcher(self._RULES_WITH_WHITELIST, False)
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
index 68433f5..23441f4 100644
--- a/loading/dependency_graph_unittest.py
+++ b/loading/dependency_graph_unittest.py
@@ -18,7 +18,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
 
   def testUpdateRequestCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
-    requests[0].timing = request_track.TimingFromDict(
+    requests[0].timing = request_track.Timing.FromDevToolsDict(
         {'requestTime': 12, 'loadingFinished': 10})
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
@@ -35,7 +35,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
   def testCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
     for (index, request) in enumerate(requests):
-      request.timing = request_track.TimingFromDict(
+      request.timing = request_track.Timing.FromDevToolsDict(
           {'requestTime': index, 'receiveHeadersEnd': 10,
            'loadingFinished': 10})
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 08e9c17..67ab3f5 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -7,7 +7,7 @@ import unittest
 import devtools_monitor
 from loading_trace import LoadingTrace
 from request_dependencies_lens import RequestDependencyLens
-from request_track import (Request, TimingFromDict)
+from request_track import Request
 import test_utils
 
 
@@ -15,12 +15,12 @@ class TestRequests(object):
   FIRST_REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com', 'request_id': '1234.redirect.1',
        'initiator': {'type': 'other'},
-       'timestamp': 0.5, 'timing': TimingFromDict({})})
+       'timestamp': 0.5, 'timing': {}})
   SECOND_REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com/redirect1', 'request_id': '1234.redirect.2',
        'initiator': {'type': 'redirect',
                      'initiating_request': '1234.redirect.1'},
-       'timestamp': 1, 'timing': TimingFromDict({})})
+       'timestamp': 1, 'timing': {}})
   REDIRECTED_REQUEST = Request.FromJsonDict({
       'url': 'http://bla.com/index.html',
       'request_id': '1234.1',
@@ -28,13 +28,13 @@ class TestRequests(object):
       'initiator': {'type': 'redirect',
                     'initiating_request': '1234.redirect.2'},
       'timestamp': 2,
-      'timing': TimingFromDict({})})
+      'timing': {}})
   REQUEST = Request.FromJsonDict({'url': 'http://bla.com/index.html',
                                   'request_id': '1234.1',
                                   'frame_id': '123.1',
                                   'initiator': {'type': 'other'},
                                   'timestamp': 2,
-                                  'timing': TimingFromDict({})})
+                                  'timing': {}})
   JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
                                      'request_id': '1234.12',
                                      'frame_id': '123.123',
@@ -42,21 +42,21 @@ class TestRequests(object):
                                          'type': 'parser',
                                          'url': 'http://bla.com/index.html'},
                                      'timestamp': 3,
-                                     'timing': TimingFromDict({})})
+                                     'timing': {}})
   JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
        'request_id': '1234.42',
        'frame_id': '123.13',
        'initiator': {'type': 'parser',
                      'url': 'http://bla.com/index.html'},
-       'timestamp': 4, 'timing': TimingFromDict({})})
+       'timestamp': 4, 'timing': {}})
   JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
        'request_id': '1234.56',
        'frame_id': '123.99',
        'initiator': {'type': 'parser',
                      'url': 'http://bla.com/index.html'},
-       'timestamp': 5, 'timing': TimingFromDict({})})
+       'timestamp': 5, 'timing': {}})
   JS_REQUEST_2 = Request.FromJsonDict(
       {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
        'frame_id': '123.123',
@@ -64,7 +64,7 @@ class TestRequests(object):
                      'stack': {'callFrames': [
                          {'url': 'unknown'},
                          {'url': 'http://bla.com/nyancat.js'}]}},
-       'timestamp': 10, 'timing': TimingFromDict({})})
+       'timestamp': 10, 'timing': {}})
   PAGE_EVENTS = [{'method': 'Page.frameAttached',
                    'frame_id': '123.13', 'parent_frame_id': '123.1'},
                  {'method': 'Page.frameAttached',
@@ -139,7 +139,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
              'stack': {'callFrames': [],
                        'parent': {'callFrames': [
                                       {'url': 'http://bla.com/nyancat.js'}]}}},
-         'timestamp': 10, 'timing': TimingFromDict({})})
+         'timestamp': 10, 'timing': {}})
     loading_trace = test_utils.LoadingTraceFromEvents(
         [TestRequests.JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
diff --git a/loading/request_track.py b/loading/request_track.py
index 9f513d2..613f6e0 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -18,17 +18,55 @@ import urlparse
 import devtools_monitor
 
 
-_TIMING_NAMES_MAPPING = {
-    'connectEnd': 'connect_end', 'connectStart': 'connect_start',
-    'dnsEnd': 'dns_end', 'dnsStart': 'dns_start', 'proxyEnd': 'proxy_end',
-    'proxyStart': 'proxy_start', 'receiveHeadersEnd': 'receive_headers_end',
-    'requestTime': 'request_time', 'sendEnd': 'send_end',
-    'sendStart': 'send_start', 'sslEnd': 'ssl_end', 'sslStart': 'ssl_start',
-    'workerReady': 'worker_ready', 'workerStart': 'worker_start',
-    'loadingFinished': 'loading_finished', 'pushStart' : 'push_start',
-    'pushEnd' : 'push_end'}
+class Timing(object):
+  """Collects the timing data for a request."""
+  _TIMING_NAMES = (
+      ('connectEnd', 'connect_end'), ('connectStart', 'connect_start'),
+      ('dnsEnd', 'dns_end'), ('dnsStart', 'dns_start'),
+      ('proxyEnd', 'proxy_end'), ('proxyStart', 'proxy_start'),
+      ('receiveHeadersEnd', 'receive_headers_end'),
+      ('requestTime', 'request_time'), ('sendEnd', 'send_end'),
+      ('sendStart', 'send_start'), ('sslEnd', 'ssl_end'),
+      ('sslStart', 'ssl_start'), ('workerReady', 'worker_ready'),
+      ('workerStart', 'worker_start'),
+      ('loadingFinished', 'loading_finished'), ('pushStart', 'push_start'),
+      ('pushEnd', 'push_end'))
+  _TIMING_NAMES_MAPPING = dict(_TIMING_NAMES)
+  __slots__ = tuple(x[1] for x in _TIMING_NAMES)
+
+  def __init__(self, **kwargs):
+    """Constructor.
+
+    Initialize with keywords arguments from __slots__.
+    """
+    for slot in self.__slots__:
+      setattr(self, slot, -1)
+    for (attr, value) in kwargs.items():
+      setattr(self, attr, value)
 
-Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
+  def __eq__(self, o):
+    return all(getattr(self, attr) == getattr(o, attr)
+               for attr in self.__slots__)
+
+  def LargestOffset(self):
+    """Returns the largest offset in the available timings."""
+    return max(0, max(
+        getattr(self, attr) for attr in self.__slots__
+        if attr != 'request_time'))
+
+  def ToJsonDict(self):
+    return {attr: getattr(self, attr)
+            for attr in self.__slots__ if getattr(self, attr) != -1}
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    return cls(**json_dict)
+
+  @classmethod
+  def FromDevToolsDict(cls, json_dict):
+    """Returns an instance of Timing from a dict, as passed by DevTools."""
+    timing_dict = {
+        cls._TIMING_NAMES_MAPPING[k]: v for (k, v) in json_dict.items()}
+    return cls(**timing_dict)
 
 
 def ShortName(url):
@@ -70,9 +108,7 @@ def IntervalBetween(first, second, reason):
   if reason == 'parser':
     first_offset_ms = first.timing.receive_headers_end
   else:
-    first_offset_ms = max(
-        [0] + [t for f, t in first.timing._asdict().iteritems()
-               if f != 'request_time'])
+    first_offset_ms = first.timing.LargestOffset()
   return (first.timing.request_time * 1000 + first_offset_ms, second_ms)
 
 
@@ -173,10 +209,7 @@ class Request(object):
   def end_msec(self):
     if self.start_msec is None:
       return None
-    return self.start_msec + max(
-        [0] + [t for f, t in self.timing._asdict().iteritems()
-               if f != 'request_time'])
-
+    return self.start_msec + self.timing.LargestOffset()
 
   def _TimestampOffsetFromStartMs(self, timestamp):
     assert self.timing.request_time != -1
@@ -184,7 +217,9 @@ class Request(object):
     return (timestamp - request_time) * 1000
 
   def ToJsonDict(self):
-    return copy.deepcopy(self.__dict__)
+    result = copy.deepcopy(self.__dict__)
+    result['timing'] = self.timing.ToJsonDict()
+    return result
 
   @classmethod
   def FromJsonDict(cls, data_dict):
@@ -194,9 +229,9 @@ class Request(object):
     if not result.response_headers:
       result.response_headers = {}
     if result.timing:
-      result.timing = Timing(*result.timing)
+      result.timing = Timing.FromJsonDict(result.timing)
     else:
-      result.timing = TimingFromDict({'requestTime': result.timestamp})
+      result.timing = Timing(request_time=result.timestamp)
     return result
 
   def GetHTTPResponseHeader(self, header_name):
@@ -269,8 +304,7 @@ class Request(object):
     request_time and the latest timing event.
     """
     # All fields in timing are millis relative to request_time.
-    return max([0] + [t for f, t in self.timing._asdict().iteritems()
-                      if f != 'request_time'])
+    return self.timing.LargestOffset()
 
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
@@ -474,7 +508,7 @@ class RequestTrack(devtools_monitor.Track):
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache')))
-    r.timing = TimingFromDict(redirect_response['timing'])
+    r.timing = Timing.FromDevToolsDict(redirect_response['timing'])
 
     redirect_index = self._redirects_count_by_id[request_id]
     self._redirects_count_by_id[request_id] += 1
@@ -530,7 +564,7 @@ class RequestTrack(devtools_monitor.Track):
       timing_dict = {'requestTime': r.timestamp}
     else:
       timing_dict = response['timing']
-    r.timing = TimingFromDict(timing_dict)
+    r.timing = Timing.FromDevToolsDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
     self._request_id_to_response_received[request_id] = params
 
@@ -548,8 +582,8 @@ class RequestTrack(devtools_monitor.Track):
     assert (status == RequestTrack._STATUS_RESPONSE
             or status == RequestTrack._STATUS_DATA)
     r.encoded_data_length = params['encodedDataLength']
-    r.timing = r.timing._replace(
-        loading_finished=r._TimestampOffsetFromStartMs(params['timestamp']))
+    r.timing.loading_finished = r._TimestampOffsetFromStartMs(
+        params['timestamp'])
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
     self._FinalizeRequest(request_id)
 
@@ -581,15 +615,6 @@ RequestTrack._METHOD_TO_HANDLER = {
     'Network.loadingFailed': RequestTrack._LoadingFailed}
 
 
-def TimingFromDict(timing_dict):
-  """Returns an instance of Timing from an () dict."""
-  complete_timing_dict = {field: -1 for field in Timing._fields}
-  timing_dict_mapped = {
-      _TIMING_NAMES_MAPPING[k]: v for (k, v) in timing_dict.items()}
-  complete_timing_dict.update(timing_dict_mapped)
-  return Timing(**complete_timing_dict)
-
-
 def _CopyFromDictToObject(d, o, key_attrs):
   for (key, attr) in key_attrs:
     if key in d:
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 6e54208..a613397 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -6,7 +6,7 @@ import copy
 import json
 import unittest
 
-from request_track import (TimeBetween, Request, RequestTrack, TimingFromDict)
+from request_track import (TimeBetween, Request, RequestTrack, Timing)
 
 
 class TimeBetweenTestCase(unittest.TestCase):
@@ -15,17 +15,17 @@ class TimeBetweenTestCase(unittest.TestCase):
                                    'frame_id': '123.1',
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
-                                   'timing': TimingFromDict({})})
+                                   'timing': {}})
   def setUp(self):
     super(TimeBetweenTestCase, self).setUp()
     self.first = copy.deepcopy(self._REQUEST)
-    self.first.timing = TimingFromDict({'requestTime': 123456,
-                                        'receiveHeadersEnd': 100,
-                                        'loadingFinished': 500})
+    self.first.timing = Timing.FromDevToolsDict({'requestTime': 123456,
+                                                 'receiveHeadersEnd': 100,
+                                                 'loadingFinished': 500})
     self.second = copy.deepcopy(self._REQUEST)
-    self.second.timing = TimingFromDict({'requestTime': 123456 + 1,
-                                        'receiveHeadersEnd': 200,
-                                        'loadingFinished': 600})
+    self.second.timing = Timing.FromDevToolsDict({'requestTime': 123456 + 1,
+                                                  'receiveHeadersEnd': 200,
+                                                  'loadingFinished': 600})
 
   def testTimeBetweenParser(self):
     self.assertEquals(900, TimeBetween(self.first, self.second, 'parser'))
@@ -329,11 +329,11 @@ class RequestTrackTestCase(unittest.TestCase):
     self.assertEquals(False, r.served_from_cache)
     self.assertEquals(False, r.from_disk_cache)
     self.assertEquals(False, r.from_service_worker)
-    timing = TimingFromDict(response['timing'])
+    timing = Timing.FromDevToolsDict(response['timing'])
     loading_finished = RequestTrackTestCase._LOADING_FINISHED['params']
     loading_finished_offset = r._TimestampOffsetFromStartMs(
         loading_finished['timestamp'])
-    timing = timing._replace(loading_finished=loading_finished_offset)
+    timing.loading_finished = loading_finished_offset
     self.assertEquals(timing, r.timing)
     self.assertEquals(200, r.status)
     self.assertEquals(
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 8ae919f..8f61969 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -16,7 +16,9 @@ import user_satisfied_lens
 class FakeRequestTrack(devtools_monitor.Track):
   def __init__(self, events):
     super(FakeRequestTrack, self).__init__(None)
-    self._events = [self._RewriteEvent(e) for e in events]
+    self._events = events
+    for e in self._events:
+      e.timing.request_time = e.timestamp
 
   def Handle(self, _method, _msg):
     assert False  # Should never be called.
@@ -32,12 +34,6 @@ class FakeRequestTrack(devtools_monitor.Track):
                 cls._DUPLICATES_KEY: 0,
                 cls._INCONSISTENT_INITIATORS_KEY: 0}}
 
-  def _RewriteEvent(self, event):
-    # This modifies the instance used across tests, so this method
-    # must be idempotent.
-    event.timing = event.timing._replace(request_time=event.timestamp)
-    return event
-
 
 class FakePageTrack(devtools_monitor.Track):
   def __init__(self, events):
@@ -70,14 +66,14 @@ def MakeRequestWithTiming(
     source_url: a url or number which will be used as the source (initiating)
       url. If the source url is not present, then url will be a root. The
       convention in tests is to use a source_url of 'null' in this case.
-    timing_dict: (dict) Suitable to be passed to request_track.TimingFromDict().
+    timing_dict: (dict) Suitable to be passed to request_track.Timing().
     initiator_type: the initiator type to use.
 
   Returns:
     A request_track.Request.
   """
   assert initiator_type in ('other', 'parser')
-  timing = request_track.TimingFromDict(timing_dict)
+  timing = request_track.Timing.FromDevToolsDict(timing_dict)
   rq = request_track.Request.FromJsonDict({
       'timestamp': timing.request_time,
       'request_id': str(MakeRequestWithTiming._next_request_id),
@@ -86,7 +82,7 @@ def MakeRequestWithTiming(
       'response_headers': {'Content-Type':
                            'null' if not magic_content_type
                            else 'magic-debug-content' },
-      'timing': request_track.TimingAsList(timing)
+      'timing': timing.ToJsonDict()
   })
   MakeRequestWithTiming._next_request_id += 1
   return rq
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 29cf3d5..2e35a50 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -25,9 +25,8 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
         'frame_id': '123.%s' % timestamp_msec,
         'initiator': {'type': 'other'},
         'timestamp': timestamp_sec,
-        'timing': request_track.TimingFromDict({
-            'requestTime': timestamp_sec,
-            'loadingFinished': duration})
+        'timing': {'request_time': timestamp_sec,
+                   'loading_finished': duration}
         })
     self._request_index += 1
     return rq

commit 0c70b224115419437422751155bb9384d0aa0acb
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 21 01:54:14 2016 -0700

    tools/android/loading: rm testdata/rollingstone.trace.gz
    
    ./testdata/rollingstone.trace.gz has become out of date, causing
    ./loading_trace_analyzer_unittest.py to fail. This CL removes both
    files to get ./run_tests passing again.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1907603003
    
    Cr-Original-Commit-Position: refs/heads/master@{#388728}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c799dc85bde528d0bbf39e5e432a6ecb8bb22eca

diff --git a/loading/loading_trace_analyzer_unittest.py b/loading/loading_trace_analyzer_unittest.py
deleted file mode 100644
index b01aab2..0000000
--- a/loading/loading_trace_analyzer_unittest.py
+++ /dev/null
@@ -1,59 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import gzip
-import os
-import re
-import shutil
-import subprocess
-import tempfile
-import unittest
-
-import loading_trace_analyzer
-
-LOADING_DIR = os.path.dirname(__file__)
-TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
-
-
-class LoadingTraceAnalyzerTest(unittest.TestCase):
-  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
-
-  def setUp(self):
-    self._temp_dir = tempfile.mkdtemp()
-    self.trace_path = self._TmpPath('trace.json')
-    with gzip.GzipFile(self._ROLLING_STONE) as f:
-      with open(self.trace_path, 'w') as g:
-        g.write(f.read())
-
-  def tearDown(self):
-    shutil.rmtree(self._temp_dir)
-
-  def _TmpPath(self, name):
-    return os.path.join(self._temp_dir, name)
-
-  def testRequestsCmd(self):
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path)]
-    self.assertNotEqual(0, len(lines))
-
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
-        output_format='hello {protocol} world {url}')]
-    self.assertNotEqual(0, len(lines))
-    for line in lines:
-      self.assertTrue(re.match(r'^hello \S+ world \S+$', line))
-
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
-        where_format='{url}', where_statement=r'^http://.*$')]
-    self.assertNotEqual(0, len(lines))
-    for line in lines:
-      self.assertTrue(line.startswith('http://'))
-
-    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
-        where_format='{url}', where_statement=r'^https://.*$')]
-    self.assertNotEqual(0, len(lines))
-    for line in lines:
-      self.assertTrue(line.startswith('https://'))
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/testdata/rollingstone.trace.gz b/loading/testdata/rollingstone.trace.gz
deleted file mode 100644
index f4a239e..0000000
Binary files a/loading/testdata/rollingstone.trace.gz and /dev/null differ

commit ba5a071e1f45f139cab91a69103366d911970349
Author: droger <droger@chromium.org>
Date:   Wed Apr 20 10:52:07 2016 -0700

    tools/android/loading Loading trace database improvements
    
    The loading trace database is now reloaded from the cloud when the
    worker starts, which prevents losing data when the worker restarts after
    a failure.
    
    There is also now one trace database per worker, which solves the issues
    of concurrent access to the database.
    
    The CL also adds supports for the 'pushStart' and 'pushEnd'
    events that were added by CL:
    https://codereview.chromium.org/1828203005/
    
    Review URL: https://codereview.chromium.org/1908483002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388530}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4a50ba25fd9177163031cfc403e2d7e4bf036d99

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
index 5f534f2..b181cfa 100644
--- a/loading/cloud/backend/README.md
+++ b/loading/cloud/backend/README.md
@@ -95,15 +95,21 @@ gcloud beta auth application-default login --scopes \
     https://www.googleapis.com/auth/cloud-platform
 ```
 
-Create a JSON file describing the deployment configuration:
-
+Create a JSON dictionary file describing the deployment configuration, with the
+keys:
+
+-   `project_name` (string): Name of the Google Cloud project
+-   `cloud_storage_path` (string): Path in Google Storage where generated traces
+    will be stored.
+-   `chrome_path` (string): Path to the Chrome executable.
+-   `src_path` (string): Path to the Chromium source directory.
+-   `taskqueue_tag` (string):
+-   `trace_database_filename` (string, optional): Filename for the trace
+    database in Cloud Storage. Must be unique per worker to avoid concurrent
+    access. Defaults to `trace_database.json`.
+
+Example:
 ```shell
-# CONFIG_FILE is the output json file.
-# PROJECT_NAME is the Google Cloud project.
-# CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
-# be stored.
-# CHROME_PATH is the path to the Chrome executable on the host.
-# CHROMIUM_SRC is the Chromium src directory.
 cat >$CONFIG_FILE << EOF
 {
   "project_name" : "$PROJECT_NAME",
diff --git a/loading/cloud/backend/google_storage_accessor.py b/loading/cloud/backend/google_storage_accessor.py
index 86e238e..ded3fe8 100644
--- a/loading/cloud/backend/google_storage_accessor.py
+++ b/loading/cloud/backend/google_storage_accessor.py
@@ -2,7 +2,8 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-from gcloud import storage
+import gcloud.exceptions
+import gcloud.storage
 
 
 class GoogleStorageAccessor(object):
@@ -18,12 +19,26 @@ class GoogleStorageAccessor(object):
 
   def _GetStorageClient(self):
     """Returns the storage client associated with the project"""
-    return storage.Client(project = self._project_name,
-                          credentials = self._credentials)
+    return gcloud.storage.Client(project = self._project_name,
+                                 credentials = self._credentials)
 
   def _GetStorageBucket(self, storage_client):
     return storage_client.get_bucket(self._bucket_name)
 
+  def DownloadAsString(self, remote_filename):
+    """Returns the content of a remote file as a string, or None if the file
+    does not exist.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.get_blob(remote_filename)
+    if not blob:
+      return None
+    try:
+      return blob.download_as_string()
+    except gcloud.exceptions.NotFound:
+      return None
+
   def UploadFile(self, filename_src, filename_dest):
     """Uploads a file to Google Cloud Storage
 
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
index e429c1e..3d3084f 100644
--- a/loading/cloud/backend/startup-script.sh
+++ b/loading/cloud/backend/startup-script.sh
@@ -12,11 +12,15 @@ get_instance_metadata() {
       -H "Metadata-Flavor: Google"
 }
 
-# Talk to the metadata server to get the project id
+# Talk to the metadata server to get the project id and the instance id
 PROJECTID=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
     -H "Metadata-Flavor: Google")
 
+INSTANCE_ID=$(curl -s \
+    "http://metadata.google.internal/computeMetadata/v1/instance/hostname" \
+    -H "Metadata-Flavor: Google")
+
 # Install dependencies from apt
 apt-get update
 # Basic dependencies
@@ -73,7 +77,8 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
   "chrome_path" : "/opt/app/clovis/binaries/chrome",
   "src_path" : "/opt/app/clovis/src",
-  "taskqueue_tag" : "$TASKQUEUE_TAG"
+  "taskqueue_tag" : "$TASKQUEUE_TAG",
+  "trace_database_filename" : "trace_database_${INSTANCE_ID}.json"
 }
 EOF
 
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
index b9900ce..9518171 100644
--- a/loading/cloud/backend/worker.py
+++ b/loading/cloud/backend/worker.py
@@ -32,6 +32,7 @@ class Worker(object):
     """See README.md for the config format."""
     self._project_name = config['project_name']
     self._taskqueue_tag = config['taskqueue_tag']
+    self._src_path = config['src_path']
     self._credentials = GoogleCredentials.get_application_default()
     self._logger = logger
 
@@ -45,15 +46,18 @@ class Worker(object):
       if not self._base_path_in_bucket.endswith('/'):
         self._base_path_in_bucket += '/'
 
-    # TODO: improve the trace database to support concurrent access.
-    self._traces_dir = self._base_path_in_bucket + 'traces/'
-    self._trace_database = LoadingTraceDatabase({})
-
-    self._src_path = config['src_path']
     self._google_storage_accessor = GoogleStorageAccessor(
         credentials=self._credentials, project_name=self._project_name,
         bucket_name=self._bucket_name)
 
+    self._traces_dir = os.path.join(self._base_path_in_bucket, 'traces')
+    self._trace_database_path = os.path.join(
+        self._traces_dir,
+        config.get('trace_database_filename', 'trace_database.json'))
+
+    # Recover any existing trace database in case the worker died.
+    self._DownloadTraceDatabase()
+
     # Initialize the global options that will be used during trace generation.
     options.OPTIONS.ParseArgs([])
     options.OPTIONS.local_binary = config['chrome_path']
@@ -94,6 +98,21 @@ class Worker(object):
       self._logger.info('Finished task %s' % task_id)
     self._Finalize()
 
+  def _DownloadTraceDatabase(self):
+    """Downloads the trace database from CloudStorage."""
+    self._logger.info('Downloading trace database')
+    trace_database_string = self._google_storage_accessor.DownloadAsString(
+        self._trace_database_path) or '{}'
+    trace_database_dict = json.loads(trace_database_string)
+    self._trace_database = LoadingTraceDatabase(trace_database_dict)
+
+  def _UploadTraceDatabase(self):
+    """Uploads the trace database to CloudStorage."""
+    self._logger.info('Uploading trace database')
+    self._google_storage_accessor.UploadString(
+        json.dumps(self._trace_database.ToJsonDict(), indent=2),
+        self._trace_database_path)
+
   def _FetchClovisTask(self, project_name, task_api, queue_name):
     """Fetches a ClovisTask from the task queue.
 
@@ -119,14 +138,9 @@ class Worker(object):
 
   def _Finalize(self):
     """Called before exiting."""
-    self._logger.info('Uploading trace database')
-    self._google_storage_accessor.UploadString(
-        json.dumps(self._trace_database.ToJsonDict(), indent=2),
-        self._traces_dir + 'trace_database.json')
     # TODO(droger): Implement automatic instance destruction.
     self._logger.info('Done')
 
-
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
     """ Generates a trace.
@@ -201,9 +215,9 @@ class Worker(object):
     emulate_device = params.get('emulate_device')
     emulate_network = params.get('emulate_network')
 
-    failures_dir = self._base_path_in_bucket + 'failures/'
+    failures_dir = os.path.join(self._base_path_in_bucket, 'failures')
     # TODO(blundell): Fix this up.
-    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
+    logs_dir = os.path.join(self._base_path_in_bucket, 'analyze_logs')
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
@@ -213,27 +227,27 @@ class Worker(object):
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         self._logger.debug('Generating trace for URL: %s' % url)
-        remote_filename = local_filename + '/' + str(repeat)
+        remote_filename = os.path.join(local_filename, str(repeat))
         trace_metadata = self._GenerateTrace(
             url, emulate_device, emulate_network, local_filename, log_filename)
         if trace_metadata['succeeded']:
           self._logger.debug('Uploading: %s' % remote_filename)
-          remote_trace_location = self._traces_dir + remote_filename
+          remote_trace_location = os.path.join(self._traces_dir,
+                                               remote_filename)
           self._google_storage_accessor.UploadFile(local_filename,
                                                    remote_trace_location)
-          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
-              remote_trace_location)
-          self._trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
+          full_cloud_storage_path = os.path.join('gs://' + self._bucket_name,
+                                                 remote_trace_location)
+          self._trace_database.SetTrace(full_cloud_storage_path, trace_metadata)
         else:
           self._logger.warning('Trace generation failed for URL: %s' % url)
-          # TODO: upload the failure
           if os.path.isfile(local_filename):
-            self._google_storage_accessor.UploadFile(local_filename,
-                                            failures_dir + remote_filename)
-            self._logger.debug('Uploading log')
-        self._google_storage_accessor.UploadFile(log_filename,
-                                                 logs_dir + remote_filename)
-
+            self._google_storage_accessor.UploadFile(
+                local_filename, os.path.join(failures_dir, remote_filename))
+        self._logger.debug('Uploading analyze log')
+        self._google_storage_accessor.UploadFile(
+            log_filename, os.path.join(logs_dir, remote_filename))
+    self._UploadTraceDatabase()
 
 if __name__ == '__main__':
   parser = argparse.ArgumentParser(
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index 2ef33b0..dad6153 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -14,9 +14,10 @@ class LoadingTraceDatabase(object):
        about those traces."""
     self._traces_dict = traces_dict
 
-  def AddTrace(self, filename, trace_dict):
-    """Adds a mapping from |filename| to |trace_dict| into the database."""
-    assert filename not in self._traces_dict
+  def SetTrace(self, filename, trace_dict):
+    """Sets a mapping from |filename| to |trace_dict| into the database.
+    If there is an existing mapping for filename, it is replaced.
+    """
     self._traces_dict[filename] = trace_dict
 
   def GetTraceFilesForURL(self, url):
diff --git a/loading/loading_trace_database_unittest.py b/loading/loading_trace_database_unittest.py
index 31dc9b3..e64fb8c 100644
--- a/loading/loading_trace_database_unittest.py
+++ b/loading/loading_trace_database_unittest.py
@@ -32,11 +32,11 @@ class LoadingTraceDatabaseUnittest(unittest.TestCase):
     self.assertEqual(
         self._JSON_DATABASE, self.database.ToJsonDict())
 
-  def testAddTrace(self):
+  def testSetTrace(self):
     dummy_url = "http://dummy.com"
     new_trace_file = "traces/new_trace.json"
     self.assertEqual(self.database.GetTraceFilesForURL(dummy_url), [])
-    self.database.AddTrace(new_trace_file, {"url" : dummy_url})
+    self.database.SetTrace(new_trace_file, {"url" : dummy_url})
     self.assertEqual(self.database.GetTraceFilesForURL(dummy_url),
                      [new_trace_file])
 
diff --git a/loading/request_track.py b/loading/request_track.py
index bf19fbb..9f513d2 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -25,7 +25,8 @@ _TIMING_NAMES_MAPPING = {
     'requestTime': 'request_time', 'sendEnd': 'send_end',
     'sendStart': 'send_start', 'sslEnd': 'ssl_end', 'sslStart': 'ssl_start',
     'workerReady': 'worker_ready', 'workerStart': 'worker_start',
-    'loadingFinished': 'loading_finished'}
+    'loadingFinished': 'loading_finished', 'pushStart' : 'push_start',
+    'pushEnd' : 'push_end'}
 
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 

commit fb08d39e524f66c5f9ed3561b4dc5408711fb230
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 20 08:21:32 2016 -0700

    tools/android/loading: Fixes a bug in chrome_cache.UnzipDirectoryContent
    
    os.utime(file_path, ...) modifies modification time of file_path's parent
    directories. Therefore we now call os.utime on files and directories that
    have longer relative paths first.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1899843006
    
    Cr-Original-Commit-Position: refs/heads/master@{#388501}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 74668f8f0ab3a1c27997f5023aa463907d75fd2a

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 9d486e9..4c1f226 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -219,7 +219,11 @@ def UnzipDirectoryContent(archive_path, directory_dest_path):
           f.write(zip_input.read(file_archive_name))
 
     assert timestamps
-    for relative_path, stats in timestamps.iteritems():
+    # os.utime(file_path, ...) modifies modification time of file_path's parent
+    # directories. Therefore we call os.utime on files and directories that have
+    # longer relative paths first.
+    for relative_path in sorted(timestamps.keys(), key=len, reverse=True):
+      stats = timestamps[relative_path]
       output_path = os.path.join(directory_dest_path, relative_path)
       if not os.path.exists(output_path):
         os.makedirs(output_path)

commit 30f4a22ec0afb0ed8f3115f76a319775915e0d15
Author: gabadie <gabadie@chromium.org>
Date:   Wed Apr 20 02:45:15 2016 -0700

    tools/android/loading: Fix cache pushing non deterministic bug on Android
    
    On Android, Sandwich create a cache of all resources by navigating to
    the URLs and then give a slow death to chrome: switch it to background
    so it can persist its cache on disk before killing it. However the next
    sandwich run was getting non deterministic cache use, because chrome
    was previously not killed enough.
    
    This CL fix this non deterministic bug by using am force-stop command
    line instead of kill -9 <pids>.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1902133002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388457}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: be415ce063ec0fb6799a1c4fe1b27845f7f2a30d

diff --git a/loading/controller.py b/loading/controller.py
index 5944711..0511998 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -215,7 +215,7 @@ class RemoteChromeController(ChromeControllerBase):
     """Overridden connection creation."""
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
-    self._device.KillAll(package_info.package, quiet=True)
+    self._device.ForceStop(package_info.package)
     if OPTIONS.clear_device_data:
       logging.info('Clear Chrome data')
       self._device.adb.Shell('pm clear ' + package_info.package)
@@ -252,7 +252,7 @@ class RemoteChromeController(ChromeControllerBase):
               time.sleep(self.TIME_TO_IDLE_SECONDS)
             break
       finally:
-        self._device.KillAll(package_info.package, quiet=True)
+        self._device.ForceStop(package_info.package)
 
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""

commit 0645d7f8da640b5fba042215ea8d3609334f47d4
Author: lizeb <lizeb@chromium.org>
Date:   Wed Apr 20 01:38:39 2016 -0700

    clovis: Improve the handling of tracing categories.
    
    The baseline categories exclude 'cc', drastically reducing the size of a
    loading trace (85 -> 39MB in one sample case), and are suitable for all
    tools but sandwich. This one adds an extra category which is too heavy
    to be in the default set.
    
    Review URL: https://codereview.chromium.org/1901653002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388446}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a3bccc967be8d4d4e27f2f7449b9c63ad5fc440a

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index ff8b8c5..d4139f7 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -74,7 +74,7 @@ class LoadingTrace(object):
 
   @classmethod
   def RecordUrlNavigation(
-      cls, url, connection, chrome_metadata, categories=None,
+      cls, url, connection, chrome_metadata, additional_categories=None,
       timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
     """Create a loading trace by using controller to fetch url.
 
@@ -82,7 +82,8 @@ class LoadingTrace(object):
       url: (str) url to fetch.
       connection: An opened devtools connection.
       chrome_metadata: Dictionary of chrome metadata.
-      categories: TracingTrack categories to capture.
+      additional_categories: ([str] or None) TracingTrack additional categories
+                             to capture.
       timeout_seconds: monitoring connection timeout in seconds.
 
     Returns:
@@ -92,8 +93,7 @@ class LoadingTrace(object):
     request = request_track.RequestTrack(connection)
     trace = tracing.TracingTrack(
         connection,
-        categories=(tracing.DEFAULT_CATEGORIES if categories is None
-                    else categories))
+        additional_categories=additional_categories)
     connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
     return cls(url, chrome_metadata, page, request, trace)
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index 1a9e96d..98d6317 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -30,15 +30,8 @@ import tracing
 
 
 # List of selected trace event categories when running chrome.
-CATEGORIES = [
-    # Need blink network trace events for prefetch_view.PrefetchSimulationView
-    'blink.net',
-
-    # Need to get mark trace events for _GetWebPageTrackedEvents()
-    'blink.user_timing',
-
-    # Need to memory dump trace event for _GetBrowserDumpEvents()
-    'disabled-by-default-memory-infra']
+ADDITIONAL_CATEGORIES = (
+    'disabled-by-default-memory-infra',)  # Used by _GetBrowserDumpEvents()
 
 CSV_FIELD_NAMES = [
     'id',
@@ -145,6 +138,11 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   Returns:
     Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
   """
+  assert all(
+      cat in loading_trace.tracing_track.Categories()
+      for cat in ADDITIONAL_CATEGORIES), (
+          'This trace was not generated with the required set of categories '
+          'to be processed by this script.')
   browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   web_page_tracked_events = _GetWebPageTrackedEvents(
       loading_trace.tracing_track)
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
index 7cbe795..1976462 100644
--- a/loading/sandwich_metrics_unittest.py
+++ b/loading/sandwich_metrics_unittest.py
@@ -44,7 +44,9 @@ _MINIMALIST_TRACE_EVENTS = [
 
 
 def TracingTrack(events):
-  return tracing.TracingTrack.FromJsonDict({'events': events})
+  return tracing.TracingTrack.FromJsonDict({
+      'events': events,
+      'categories': tracing.INITIAL_CATEGORIES + puller.ADDITIONAL_CATEGORIES})
 
 
 def LoadingTrace(events):
@@ -103,7 +105,7 @@ class PageTrackTest(unittest.TestCase):
         {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
         {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
-    self.assertTrue(_MEM_CAT in puller.CATEGORIES)
+    self.assertTrue(_MEM_CAT in puller.ADDITIONAL_CATEGORIES)
 
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
@@ -124,8 +126,6 @@ class PageTrackTest(unittest.TestCase):
       RunHelper(TRACE_EVENTS, 895)
 
   def testGetWebPageTrackedEvents(self):
-    self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
-
     trace_events = puller._GetWebPageTrackedEvents(TracingTrack([
         {'ph': 'R', 'ts':  0000, 'args': {},             'cat': 'whatever',
             'name': _START},
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index ebef772..39bf4d6 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -204,14 +204,14 @@ class SandwichRunner(object):
               url=url,
               connection=connection,
               chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              categories=sandwich_metrics.CATEGORIES,
+              additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
               timeout_seconds=_DEVTOOLS_TIMEOUT)
       else:
         trace = loading_trace.LoadingTrace.RecordUrlNavigation(
             url=url,
             connection=connection,
             chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            categories=sandwich_metrics.CATEGORIES,
+            additional_categories=sandwich_metrics.ADDITIONAL_CATEGORIES,
             timeout_seconds=_DEVTOOLS_TIMEOUT)
     if run_path is not None:
       trace_path = os.path.join(run_path, 'trace.json')
diff --git a/loading/testdata/scanner_vs_parser.trace b/loading/testdata/scanner_vs_parser.trace
index 627d141..75d10a2 100644
--- a/loading/testdata/scanner_vs_parser.trace
+++ b/loading/testdata/scanner_vs_parser.trace
@@ -52,6 +52,7 @@
     }
   },
   "tracing_track": {
+   "categories": [],
     "events": [
       {
         "args": {
diff --git a/loading/tracing.py b/loading/tracing.py
index 4891d3e..c1ecf71 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -12,7 +12,11 @@ import operator
 import devtools_monitor
 
 
-DEFAULT_CATEGORIES = None
+_DISABLED_CATEGORIES = ('cc',) # Contains a lot of events, none of which we use.
+INITIAL_CATEGORIES = (
+    'toplevel', 'blink', 'v8', 'java', 'devtools.timeline',
+    'blink.user_timing', 'blink.net') + tuple(
+        '-' + cat for cat in _DISABLED_CATEGORIES)
 
 
 class TracingTrack(devtools_monitor.Track):
@@ -20,16 +24,16 @@ class TracingTrack(devtools_monitor.Track):
 
   See https://goo.gl/Qabkqk for details on the protocol.
   """
-  def __init__(self, connection,
-               categories=DEFAULT_CATEGORIES,
+  def __init__(self, connection, additional_categories=None,
                fetch_stream=False):
     """Initialize this TracingTrack.
 
     Args:
       connection: a DevToolsConnection.
-      categories: None, or a string, or list of strings, of tracing categories
-        to filter.
-
+      additional_categories: ([str] or None) If set, a list of additional
+                             categories to add. This cannot be used to re-enable
+                             a category which is disabled by default (see
+                             INITIAL_CATEGORIES), nor to disable a category.
       fetch_stream: if true, use a websocket stream to fetch tracing data rather
         than dataCollected events. It appears based on very limited testing that
         a stream is slower than the default reporting as dataCollected events.
@@ -37,10 +41,15 @@ class TracingTrack(devtools_monitor.Track):
     super(TracingTrack, self).__init__(connection)
     if connection:
       connection.RegisterListener('Tracing.dataCollected', self)
+    extra_categories = additional_categories or []
+    assert not (set(extra_categories) & set(_DISABLED_CATEGORIES)), (
+        'Cannot enable a disabled category')
+    assert not any(cat.startswith('-') for cat in extra_categories), (
+        'Cannot disable a category')
+    self._categories = set(
+        itertools.chain(INITIAL_CATEGORIES, extra_categories))
     params = {}
-    if categories:
-      params['categories'] = (categories if type(categories) is str
-                              else ','.join(categories))
+    params['categories'] = ','.join(self._categories)
     if fetch_stream:
       params['transferMode'] = 'ReturnAsStream'
 
@@ -62,15 +71,9 @@ class TracingTrack(devtools_monitor.Track):
     # update.
     self._interval_tree = None
 
-  def _GetMainFrameID(self):
-    """Returns the main frame ID."""
-    if not self._main_frame_id:
-      navigation_start_events = [e for e in self.GetEvents()
-          if e.Matches('blink.user_timing', 'navigationStart')]
-      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
-      self._main_frame_id = first_event.args['frame']
-
-    return self._main_frame_id
+  def Categories(self):
+    """Returns the set of categories in this trace."""
+    return self._categories
 
   def GetFirstEventMillis(self):
     """Find the canonical start time for this track.
@@ -110,9 +113,6 @@ class TracingTrack(devtools_monitor.Track):
     self._IndexEvents()
     return self._interval_tree.EventsAt(msec)
 
-  def ToJsonDict(self):
-    return {'events': [e.ToJsonDict() for e in self._events]}
-
   def Filter(self, pid=None, tid=None, categories=None):
     """Returns a new TracingTrack with a subset of the events.
 
@@ -133,8 +133,15 @@ class TracingTrack(devtools_monitor.Track):
           events)
     tracing_track = TracingTrack(None)
     tracing_track._events = events
+    tracing_track._categories = self._categories
+    if categories is not None:
+      tracing_track._categories = self._categories.intersection(categories)
     return tracing_track
 
+  def ToJsonDict(self):
+    return {'categories': list(self._categories),
+            'events': [e.ToJsonDict() for e in self._events]}
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     if not json_dict:
@@ -142,6 +149,7 @@ class TracingTrack(devtools_monitor.Track):
     assert 'events' in json_dict
     events = [Event(e) for e in json_dict['events']]
     tracing_track = TracingTrack(None)
+    tracing_track._categories = set(json_dict.get('categories', []))
     tracing_track._events = events
     tracing_track._base_msec = events[0].start_msec if events else 0
     for e in events[1:]:
@@ -189,6 +197,16 @@ class TracingTrack(devtools_monitor.Track):
         return event
     return None
 
+  def _GetMainFrameID(self):
+    """Returns the main frame ID."""
+    if not self._main_frame_id:
+      navigation_start_events = [e for e in self.GetEvents()
+          if e.Matches('blink.user_timing', 'navigationStart')]
+      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
+      self._main_frame_id = first_event.args['frame']
+
+    return self._main_frame_id
+
   def _IndexEvents(self, strict=False):
     if self._interval_tree:
       return
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 0d7ba27..cce5809 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -42,7 +42,7 @@ class TracingTrackTestCase(unittest.TestCase):
   def setUp(self):
     self.tree_threshold = _IntervalTree._TRESHOLD
     _IntervalTree._TRESHOLD = 2  # Expose more edge cases in the tree.
-    self.track = TracingTrack(None)
+    self.track = TracingTrack(None, additional_categories=('A', 'B', 'C', 'D'))
 
   def tearDown(self):
     _IntervalTree._TRESHOLD = self.tree_threshold
@@ -357,6 +357,18 @@ class TracingTrackTestCase(unittest.TestCase):
     filtered_events = self.track.Filter(categories=set(['B', 'C'])).GetEvents()
     self.assertEquals(3, len(filtered_events))
     self.assertListEqual(tracing_events[1:], filtered_events)
+    self.assertSetEqual(
+        set('A'), self.track.Filter(categories=set('A')).Categories())
+
+  def testAdditionalCategories(self):
+    track = TracingTrack(None, additional_categories=('best-category-ever',))
+    self.assertIn('best-category-ever', track.Categories())
+    # Cannot re-enable a category.
+    with self.assertRaises(AssertionError):
+      TracingTrack(None, additional_categories=('cc',))
+    # Cannot disable categories.
+    with self.assertRaises(AssertionError):
+      TracingTrack(None, additional_categories=('-best-category-ever',))
 
   def _HandleEvents(self, events):
     self.track.Handle('Tracing.dataCollected', {'params': {'value': [

commit c7ec82ca8984405fd9d8ece58c7f2c0bd2bb23e1
Author: shaktisahu <shaktisahu@chromium.org>
Date:   Tue Apr 19 20:05:17 2016 -0700

    Allow Blimp user to choose assigner
    
    Currently the assigner URL is hard-coded. This patch will enable user to select an assigner from a list of assigner URLs. In the About Blimp page, the user can tap on the assigner URL and a list of assigners would show up. Once the new assigner is selected, it is saved to SharedPreference and a dialog is displayed to the user to restart the app. Upon restart, the new assigner takes into effect.
    
    Also fixed the java class path for blimp client in the eclipse .classpath file
    
    BUG=597141
    
    Review URL: https://codereview.chromium.org/1884293002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388416}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 430ff7cb9e5c3ea32dfb4142a28a255d3fdf338f

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 125f04e..a6d7e26 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -24,8 +24,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="base/android/java/src"/>
     <classpathentry kind="src" path="base/android/javatests/src"/>
     <classpathentry kind="src" path="base/test/android/javatests/src"/>
-    <classpathentry kind="src" path="blimp/client/android/java/src"/>
-    <classpathentry kind="src" path="blimp/client/android/javatests/src"/>
+    <classpathentry kind="src" path="blimp/client/app/android/java/src"/>
+    <classpathentry kind="src" path="blimp/client/app/android/javatests/src"/>
     <classpathentry kind="src" path="chrome/android/java/src"/>
     <classpathentry kind="src" path="chrome/android/javatests/src"/>
     <classpathentry kind="src" path="chrome/android/sync_shell/javatests/src"/>

commit a49482eda2e83616805c8f00946a8bc88546b8d3
Author: droger <droger@chromium.org>
Date:   Tue Apr 19 12:10:43 2016 -0700

    tools/android/loading Switch the GCE worker to pull queues
    
    The GCE worker now uses pull queues instead of HTTP requests.
    
    Review URL: https://codereview.chromium.org/1895033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#388265}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 56df5e65bb133509403661c090e18617b1910d36

diff --git a/loading/cloud/backend/README.md b/loading/cloud/backend/README.md
new file mode 100644
index 0000000..5f534f2
--- /dev/null
+++ b/loading/cloud/backend/README.md
@@ -0,0 +1,135 @@
+# Clovis in the Cloud: Developer Guide
+
+This document describes how to collect Chromium traces using Google Compute
+Engine.
+
+[TOC]
+
+## Initial setup
+
+Install the [gcloud command line tool][1].
+
+## Deploy the code
+
+```shell
+# Build Chrome (do not use the component build).
+BUILD_DIR=out/Release
+ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
+
+# Deploy to GCE
+# CLOUD_STORAGE_PATH is the path in Google Cloud Storage under which the
+# Clovis deployment will be uploaded.
+
+./tools/android/loading/cloud/backend/deploy.sh $BUILD_DIR $CLOUD_STORAGE_PATH
+```
+
+## Start the app in the cloud
+
+Create an instance using latest ubuntu LTS:
+
+```shell
+gcloud compute instances create clovis-tracer-1 \
+ --machine-type n1-standard-1 \
+ --image ubuntu-14-04 \
+ --zone europe-west1-c \
+ --scopes cloud-platform,https://www.googleapis.com/auth/cloud-taskqueue \
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,taskqueue_tag=some_tag \
+ --metadata-from-file \
+     startup-script=$CHROMIUM_SRC/tools/android/loading/cloud/backend/startup-script.sh
+```
+
+**Note:** To start an instance without automatically starting the app on it,
+add a `auto-start=false` metadata. This can be useful when doing iterative
+development on the instance, to be able to restart the app manually.
+
+This should output the IP address of the instance.
+Otherwise the IP address can be retrieved by doing:
+
+```shell
+gcloud compute instances list
+```
+
+**Note:** It can take a few minutes for the instance to start. You can follow
+the progress of the startup script on the gcloud console web interface (menu
+"Compute Engine" > "VM instances" then click on your instance and scroll down to
+see the "Serial console output") or from the command line using:
+
+```shell
+gcloud compute instances get-serial-port-output clovis-tracer-1
+```
+
+## Use the app
+
+Create tasks from the associated AppEngine application, see [documentation][3].
+Make sure the `taskqueue_tag` of the AppEngine request matches the one of the
+ComputeEngine instances.
+
+## Stop the app in the cloud
+
+```shell
+gcloud compute instances delete clovis-tracer-1
+```
+
+## Connect to the instance with SSH
+
+```shell
+gcloud compute ssh clovis-tracer-1
+```
+
+## Use the app locally
+
+From a new directory, set up a local environment:
+
+```shell
+virtualenv env
+source env/bin/activate
+pip install -r \
+    $CHROMIUM_SRC/tools/android/loading/cloud/backend/pip_requirements.txt
+```
+
+The first time, you may need to get more access tokens:
+
+```shell
+gcloud beta auth application-default login --scopes \
+    https://www.googleapis.com/auth/cloud-taskqueue \
+    https://www.googleapis.com/auth/cloud-platform
+```
+
+Create a JSON file describing the deployment configuration:
+
+```shell
+# CONFIG_FILE is the output json file.
+# PROJECT_NAME is the Google Cloud project.
+# CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
+# be stored.
+# CHROME_PATH is the path to the Chrome executable on the host.
+# CHROMIUM_SRC is the Chromium src directory.
+cat >$CONFIG_FILE << EOF
+{
+  "project_name" : "$PROJECT_NAME",
+  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
+  "chrome_path" : "$CHROME_PATH",
+  "src_path" : "$CHROMIUM_SRC",
+  "taskqueue_tag" : "some_tag"
+}
+EOF
+```
+
+Launch the app, passing the path to the deployment configuration file:
+
+```shell
+python $CHROMIUM_SRC/tools/android/loading/cloud/backend/worker.py \
+    --config $CONFIG_FILE
+```
+
+You can now [use the app][2].
+
+Tear down the local environment:
+
+```shell
+deactivate
+```
+
+[1]: https://cloud.google.com/sdk
+[2]: #Use-the-app
+[3]: ../frontend/README.md
diff --git a/loading/cloud/backend/deploy.sh b/loading/cloud/backend/deploy.sh
new file mode 100755
index 0000000..cbab438
--- /dev/null
+++ b/loading/cloud/backend/deploy.sh
@@ -0,0 +1,67 @@
+#!/bin/bash
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# This script copies all dependencies required for trace collection.
+# Usage:
+#   deploy.sh builddir gcs_path
+#
+# Where:
+#   builddir is the build directory for Chrome
+#   gcs_path is the Google Storage bucket under which the deployment is
+#   installed
+
+builddir=$1
+tmpdir=`mktemp -d`
+deployment_gcs_path=$2/deployment
+
+# Extract needed sources.
+src_suffix=src
+tmp_src_dir=$tmpdir/$src_suffix
+
+# Copy files from tools/android/loading.
+mkdir -p $tmp_src_dir/tools/android/loading/cloud
+cp -r tools/android/loading/cloud/backend \
+  $tmp_src_dir/tools/android/loading/cloud/
+cp -r tools/android/loading/cloud/common \
+  $tmp_src_dir/tools/android/loading/cloud/
+cp tools/android/loading/*.py $tmp_src_dir/tools/android/loading
+cp tools/android/loading/cloud/*.py $tmp_src_dir/tools/android/loading/cloud
+
+# Copy other dependencies.
+mkdir $tmp_src_dir/third_party
+rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
+  third_party/catapult $tmp_src_dir/third_party
+mkdir $tmp_src_dir/tools/perf
+cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
+mkdir -p $tmp_src_dir/build/android
+cp build/android/devil_chromium.py $tmp_src_dir/build/android/
+cp build/android/video_recorder.py $tmp_src_dir/build/android/
+cp build/android/devil_chromium.json $tmp_src_dir/build/android/
+cp -r build/android/pylib $tmp_src_dir/build/android/
+mkdir -p \
+  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
+cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \
+  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices/
+
+# Tar up the source and copy it to Google Cloud Storage.
+source_tarball=$tmpdir/source.tgz
+tar -cvzf $source_tarball -C $tmpdir $src_suffix
+gsutil cp $source_tarball gs://$deployment_gcs_path/source/
+
+# Copy the chrome executable to Google Cloud Storage.
+chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
+  $tmpdir/linux.zip
+gsutil cp $tmpdir/linux.zip gs://$deployment_gcs_path/binaries/linux.zip
+
+# Generate and upload metadata about this deployment.
+CHROMIUM_REV=$(git merge-base HEAD origin/master)
+cat >$tmpdir/build_metadata.json << EOF
+{
+  "chromium_rev": "$CHROMIUM_REV"
+}
+EOF
+gsutil cp $tmpdir/build_metadata.json \
+  gs://$deployment_gcs_path/deployment_metadata.json
+rm -rf $tmpdir
diff --git a/loading/cloud/backend/google_storage_accessor.py b/loading/cloud/backend/google_storage_accessor.py
new file mode 100644
index 0000000..86e238e
--- /dev/null
+++ b/loading/cloud/backend/google_storage_accessor.py
@@ -0,0 +1,59 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from gcloud import storage
+
+
+class GoogleStorageAccessor(object):
+  """Utility class providing helpers for Google Cloud Storage.
+  """
+  def __init__(self, credentials, project_name, bucket_name):
+    """project_name is the name of the Google Cloud project.
+    bucket_name is the name of the bucket that is used for Cloud Storage calls.
+    """
+    self._credentials = credentials
+    self._project_name = project_name
+    self._bucket_name = bucket_name
+
+  def _GetStorageClient(self):
+    """Returns the storage client associated with the project"""
+    return storage.Client(project = self._project_name,
+                          credentials = self._credentials)
+
+  def _GetStorageBucket(self, storage_client):
+    return storage_client.get_bucket(self._bucket_name)
+
+  def UploadFile(self, filename_src, filename_dest):
+    """Uploads a file to Google Cloud Storage
+
+    Args:
+      filename_src: name of the local file
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    with open(filename_src) as file_src:
+      blob.upload_from_file(file_src)
+    return blob.public_url
+
+  def UploadString(self, data_string, filename_dest):
+    """Uploads a string to Google Cloud Storage
+
+    Args:
+      data_string: the contents of the file to be uploaded
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    blob.upload_from_string(data_string)
+    return blob.public_url
+
diff --git a/loading/cloud/backend/pip_requirements.txt b/loading/cloud/backend/pip_requirements.txt
new file mode 100644
index 0000000..390863a
--- /dev/null
+++ b/loading/cloud/backend/pip_requirements.txt
@@ -0,0 +1,3 @@
+gcloud==0.10.1
+google-api-python-client==1.5.0
+psutil==4.1.0
diff --git a/loading/cloud/backend/startup-script.sh b/loading/cloud/backend/startup-script.sh
new file mode 100644
index 0000000..e429c1e
--- /dev/null
+++ b/loading/cloud/backend/startup-script.sh
@@ -0,0 +1,108 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Script executed at instance startup. It installs the required dependencies,
+# downloads the source code, and starts a web server.
+
+set -v
+
+get_instance_metadata() {
+  curl -fs http://metadata/computeMetadata/v1/instance/attributes/$1 \
+      -H "Metadata-Flavor: Google"
+}
+
+# Talk to the metadata server to get the project id
+PROJECTID=$(curl -s \
+    "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
+    -H "Metadata-Flavor: Google")
+
+# Install dependencies from apt
+apt-get update
+# Basic dependencies
+apt-get install -yq git supervisor python-pip python-dev unzip
+# Web server dependencies
+apt-get install -yq libffi-dev libssl-dev
+# Chrome dependencies
+apt-get install -yq libpangocairo-1.0-0 libXcomposite1 libXcursor1 libXdamage1 \
+    libXi6 libXtst6 libnss3 libcups2 libgconf2-4 libXss1 libXrandr2 \
+    libatk1.0-0 libasound2 libgtk2.0-0
+# Trace collection dependencies
+apt-get install -yq xvfb
+
+# Create a pythonapp user. The application will run as this user.
+useradd -m -d /home/pythonapp pythonapp
+
+# pip from apt is out of date, so make it update itself and install virtualenv.
+pip install --upgrade pip virtualenv
+
+# Download the Clovis deployment from Google Cloud Storage and unzip it.
+# It is expected that the contents of the deployment have been generated using
+# the tools/android/loading/cloud/backend/deploy.sh script.
+CLOUD_STORAGE_PATH=`get_instance_metadata cloud-storage-path`
+DEPLOYMENT_PATH=$CLOUD_STORAGE_PATH/deployment
+
+mkdir -p /opt/app/clovis
+gsutil cp gs://$DEPLOYMENT_PATH/source/source.tgz /opt/app/clovis/source.tgz
+tar xvf /opt/app/clovis/source.tgz -C /opt/app/clovis
+rm /opt/app/clovis/source.tgz
+
+# Install app dependencies
+virtualenv /opt/app/clovis/env
+/opt/app/clovis/env/bin/pip install -r \
+   /opt/app/clovis/src/tools/android/loading/cloud/backend/pip_requirements.txt
+
+mkdir /opt/app/clovis/binaries
+gsutil cp gs://$DEPLOYMENT_PATH/binaries/* /opt/app/clovis/binaries/
+unzip /opt/app/clovis/binaries/linux.zip -d /opt/app/clovis/binaries/
+
+# Install the Chrome sandbox
+cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
+chown root:root /usr/local/sbin/chrome-devel-sandbox
+chmod 4755 /usr/local/sbin/chrome-devel-sandbox
+
+# Make sure the pythonapp user owns the application code
+chown -R pythonapp:pythonapp /opt/app
+
+# Create the configuration file for this deployment.
+DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
+TASKQUEUE_TAG=`get_instance_metadata taskqueue_tag`
+cat >$DEPLOYMENT_CONFIG_PATH << EOF
+{
+  "project_name" : "$PROJECTID",
+  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
+  "chrome_path" : "/opt/app/clovis/binaries/chrome",
+  "src_path" : "/opt/app/clovis/src",
+  "taskqueue_tag" : "$TASKQUEUE_TAG"
+}
+EOF
+
+# Check if auto-start is enabled
+AUTO_START=`get_instance_metadata auto-start`
+
+# Exit early if auto start is not enabled.
+if [ "$AUTO_START" == "false" ]; then
+  exit 1
+fi
+
+# Configure supervisor to start the worker inside of our virtualenv.
+cat >/etc/supervisor/conf.d/python-app.conf << EOF
+[program:pythonapp]
+directory=/opt/app/clovis/src/tools/android/loading/cloud/backend
+command=python worker.py --config $DEPLOYMENT_CONFIG_PATH
+autostart=true
+autorestart=true
+user=pythonapp
+# Environment variables ensure that the application runs inside of the
+# configured virtualenv.
+environment=VIRTUAL_ENV="/opt/app/clovis/env", \
+    PATH="/opt/app/clovis/env/bin:/usr/bin", \
+    HOME="/home/pythonapp",USER="pythonapp", \
+    CHROME_DEVEL_SANDBOX="/usr/local/sbin/chrome-devel-sandbox"
+stdout_logfile=syslog
+stderr_logfile=syslog
+EOF
+
+supervisorctl reread
+supervisorctl update
+
diff --git a/loading/cloud/backend/worker.py b/loading/cloud/backend/worker.py
new file mode 100644
index 0000000..b9900ce
--- /dev/null
+++ b/loading/cloud/backend/worker.py
@@ -0,0 +1,254 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import json
+import logging
+import os
+import re
+import sys
+import time
+
+from googleapiclient import discovery
+from oauth2client.client import GoogleCredentials
+
+# NOTE: The parent directory needs to be first in sys.path to avoid conflicts
+# with catapult modules that have colliding names, as catapult inserts itself
+# into the path as the second element. This is an ugly and fragile hack.
+sys.path.insert(0,
+    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir,
+                 os.pardir))
+import controller
+from cloud.common.clovis_task import ClovisTask
+from google_storage_accessor import GoogleStorageAccessor
+import loading_trace
+from loading_trace_database import LoadingTraceDatabase
+import options
+
+
+class Worker(object):
+  def __init__(self, config, logger):
+    """See README.md for the config format."""
+    self._project_name = config['project_name']
+    self._taskqueue_tag = config['taskqueue_tag']
+    self._credentials = GoogleCredentials.get_application_default()
+    self._logger = logger
+
+    # Separate the cloud storage path into the bucket and the base path under
+    # the bucket.
+    storage_path_components = config['cloud_storage_path'].split('/')
+    self._bucket_name = storage_path_components[0]
+    self._base_path_in_bucket = ''
+    if len(storage_path_components) > 1:
+      self._base_path_in_bucket = '/'.join(storage_path_components[1:])
+      if not self._base_path_in_bucket.endswith('/'):
+        self._base_path_in_bucket += '/'
+
+    # TODO: improve the trace database to support concurrent access.
+    self._traces_dir = self._base_path_in_bucket + 'traces/'
+    self._trace_database = LoadingTraceDatabase({})
+
+    self._src_path = config['src_path']
+    self._google_storage_accessor = GoogleStorageAccessor(
+        credentials=self._credentials, project_name=self._project_name,
+        bucket_name=self._bucket_name)
+
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs([])
+    options.OPTIONS.local_binary = config['chrome_path']
+
+  def Start(self):
+    """Main worker loop.
+
+    Repeatedly pulls tasks from the task queue and processes them. Returns when
+    the queue is empty.
+    """
+    task_api = discovery.build('taskqueue', 'v1beta2',
+                               credentials=self._credentials)
+    queue_name = 'clovis-queue'
+    # Workaround for
+    # https://code.google.com/p/googleappengine/issues/detail?id=10199
+    project = 's~' + self._project_name
+
+    while True:
+      self._logger.debug('Fetching new task.')
+      (clovis_task, task_id) = self._FetchClovisTask(project, task_api,
+                                                     queue_name)
+      if not clovis_task:
+        if self._trace_database.ToJsonDict():
+          self._logger.info('No remaining tasks in the queue.')
+          break
+        else:
+          delay_seconds = 60
+          self._logger.info(
+              'Nothing in the queue, retrying in %i seconds.' % delay_seconds)
+          time.sleep(delay_seconds)
+          continue
+
+      self._logger.info('Processing task %s' % task_id)
+      self._ProcessClovisTask(clovis_task)
+      self._logger.debug('Deleting task %s' % task_id)
+      task_api.tasks().delete(project=project, taskqueue=queue_name,
+                              task=task_id).execute()
+      self._logger.info('Finished task %s' % task_id)
+    self._Finalize()
+
+  def _FetchClovisTask(self, project_name, task_api, queue_name):
+    """Fetches a ClovisTask from the task queue.
+
+    Params:
+      project_name(str): The name of the Google Cloud project.
+      task_api: The TaskQueue service.
+      queue_name(str): The name of the task queue.
+
+    Returns:
+      (ClovisTask, str): The fetched ClovisTask and its task ID, or (None, None)
+                         if no tasks are found.
+    """
+    response = task_api.tasks().lease(
+        project=project_name, taskqueue=queue_name, numTasks=1, leaseSecs=180,
+        groupByTag=True, tag=self._taskqueue_tag).execute()
+    if (not response.get('items')) or (len(response['items']) < 1):
+      return (None, None)
+
+    google_task = response['items'][0]
+    task_id = google_task['id']
+    clovis_task = ClovisTask.FromBase64(google_task['payloadBase64'])
+    return (clovis_task, task_id)
+
+  def _Finalize(self):
+    """Called before exiting."""
+    self._logger.info('Uploading trace database')
+    self._google_storage_accessor.UploadString(
+        json.dumps(self._trace_database.ToJsonDict(), indent=2),
+        self._traces_dir + 'trace_database.json')
+    # TODO(droger): Implement automatic instance destruction.
+    self._logger.info('Done')
+
+
+  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
+                     log_filename):
+    """ Generates a trace.
+
+    Args:
+      url: URL as a string.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
+      filename: Name of the file where the trace is saved.
+      log_filename: Name of the file where standard output and errors are
+                    logged.
+
+    Returns:
+      A dictionary of metadata about the trace, including a 'succeeded' field
+      indicating whether the trace was successfully generated.
+    """
+    try:
+      os.remove(filename)  # Remove any existing trace for this URL.
+    except OSError:
+      pass  # Nothing to remove.
+
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
+
+    old_stdout = sys.stdout
+    old_stderr = sys.stderr
+
+    trace_metadata = { 'succeeded' : False, 'url' : url }
+    trace = None
+    with open(log_filename, 'w') as sys.stdout:
+      try:
+        sys.stderr = sys.stdout
+
+        # Set up the controller.
+        chrome_ctl = controller.LocalChromeController()
+        chrome_ctl.SetHeadless(True)
+        if emulate_device:
+          chrome_ctl.SetDeviceEmulation(emulate_device)
+        if emulate_network:
+          chrome_ctl.SetNetworkEmulation(emulate_network)
+
+        # Record and write the trace.
+        with chrome_ctl.OpenWithRedirection(sys.stdout,
+                                            sys.stderr) as connection:
+          connection.ClearCache()
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url, connection, chrome_ctl.ChromeMetadata())
+          trace_metadata['succeeded'] = True
+          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
+      except Exception as e:
+        sys.stderr.write(str(e))
+
+      if trace:
+        with open(filename, 'w') as f:
+          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+
+    sys.stdout = old_stdout
+    sys.stderr = old_stderr
+
+    return trace_metadata
+
+  def _ProcessClovisTask(self, clovis_task):
+    """Processes one clovis_task."""
+    if clovis_task.Action() != 'trace':
+      self._logger.error('Unsupported task action: %s' % clovis_task.Action())
+      return
+
+    # Extract the task parameters.
+    params = clovis_task.Params()
+    urls = params['urls']
+    repeat_count = params.get('repeat_count', 1)
+    emulate_device = params.get('emulate_device')
+    emulate_network = params.get('emulate_network')
+
+    failures_dir = self._base_path_in_bucket + 'failures/'
+    # TODO(blundell): Fix this up.
+    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
+    log_filename = 'analyze.log'
+    # Avoid special characters in storage object names
+    pattern = re.compile(r"[#\?\[\]\*/]")
+
+    while len(urls) > 0:
+      url = urls.pop()
+      local_filename = pattern.sub('_', url)
+      for repeat in range(repeat_count):
+        self._logger.debug('Generating trace for URL: %s' % url)
+        remote_filename = local_filename + '/' + str(repeat)
+        trace_metadata = self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename)
+        if trace_metadata['succeeded']:
+          self._logger.debug('Uploading: %s' % remote_filename)
+          remote_trace_location = self._traces_dir + remote_filename
+          self._google_storage_accessor.UploadFile(local_filename,
+                                                   remote_trace_location)
+          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
+              remote_trace_location)
+          self._trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
+        else:
+          self._logger.warning('Trace generation failed for URL: %s' % url)
+          # TODO: upload the failure
+          if os.path.isfile(local_filename):
+            self._google_storage_accessor.UploadFile(local_filename,
+                                            failures_dir + remote_filename)
+            self._logger.debug('Uploading log')
+        self._google_storage_accessor.UploadFile(log_filename,
+                                                 logs_dir + remote_filename)
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser(
+      description='ComputeEngine Worker for Clovis')
+  parser.add_argument('--config', required=True,
+                      help='Path to the configuration file.')
+  args = parser.parse_args()
+
+  # Configure logging.
+  logging.basicConfig(level=logging.WARNING)
+  worker_logger = logging.getLogger('worker')
+  worker_logger.setLevel(logging.INFO)
+
+  worker_logger.info('Reading configuration')
+  with open(args.config) as config_json:
+    worker = Worker(json.load(config_json), worker_logger)
+    worker.Start()
+
diff --git a/loading/cloud/frontend/lib/common b/loading/cloud/frontend/lib/common
new file mode 120000
index 0000000..dc879ab
--- /dev/null
+++ b/loading/cloud/frontend/lib/common
@@ -0,0 +1 @@
+../../common
\ No newline at end of file
diff --git a/loading/gce/README.md b/loading/gce/README.md
deleted file mode 100644
index 2bbd06b..0000000
--- a/loading/gce/README.md
+++ /dev/null
@@ -1,140 +0,0 @@
-# Clovis in the Cloud: Developer Guide
-
-This document describes how to collect Chromium traces using Google Compute
-Engine.
-
-[TOC]
-
-## Initial setup
-
-Install the [gcloud command line tool][1].
-
-## Deploy the code
-
-```shell
-# Build Chrome (do not use the component build).
-BUILD_DIR=out/Release
-ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
-
-# Deploy to GCE
-# CLOUD_STORAGE_PATH is the path in Google Cloud Storage under which the
-# Clovis deployment will be uploaded.
-
-./tools/android/loading/gce/deploy.sh $BUILD_DIR $CLOUD_STORAGE_PATH
-```
-
-## Start the app in the cloud
-
-Create an instance using latest ubuntu LTS:
-
-```shell
-gcloud compute instances create clovis-tracer-1 \
- --machine-type n1-standard-1 \
- --image ubuntu-14-04 \
- --zone europe-west1-c \
- --scopes cloud-platform \
- --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,auto-start=true \
- --metadata-from-file \
-     startup-script=$CHROMIUM_SRC/tools/android/loading/gce/startup-script.sh
-```
-
-**Note:** To start an instance without automatically starting the app on it,
-remove the `--metadata auto-start=true` argument. This can be useful when doing
-iterative development on the instance, to be able to restart the app manually.
-
-This should output the IP address of the instance.
-Otherwise the IP address can be retrieved by doing:
-
-```shell
-gcloud compute instances list
-```
-
-**Note:** It can take a few minutes for the instance to start. You can follow
-the progress of the startup script on the gcloud console web interface (menu
-"Compute Engine" > "VM instances" then click on your instance and scroll down to
-see the "Serial console output") or from the command line using:
-
-```shell
-gcloud compute instances get-serial-port-output clovis-tracer-1
-```
-
-## Use the app
-
-Check that `http://<instance-ip>:8080/test` prints `hello` when opened in a
-browser.
-
-To send a list of URLs to process:
-
-```shell
-curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
-```
-
-where `urls.json` is a JSON dictionary with the keys:
-
-*   `urls`: array of URLs
-*   `repeat_count`: Number of times each URL will be loaded. Each load of a URL
-    generates a separate trace file. Optional.
-*   `emulate_device`: Name of the device to emulate. Optional.
-*   `emulate_network`: Type of network emulation. Optional.
-
-You can follow the progress at `http://<instance-ip>:8080/status`.
-
-## Stop the app in the cloud
-
-```shell
-gcloud compute instances delete clovis-tracer-1
-```
-
-## Connect to the instance with SSH
-
-```shell
-gcloud compute ssh clovis-tracer-1
-```
-
-## Use the app locally
-
-From a new directory, set up a local environment:
-
-```shell
-virtualenv env
-source env/bin/activate
-pip install -r $CHROMIUM_SRC/tools/android/loading/gce/pip_requirements.txt
-```
-
-Create a JSON file describing the deployment configuration:
-
-```shell
-# CONFIG_FILE is the output json file.
-# PROJECT_NAME is the Google Cloud project.
-# CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
-# be stored.
-# CHROME_PATH is the path to the Chrome executable on the host.
-# CHROMIUM_SRC is the Chromium src directory.
-cat >$CONFIG_FILE << EOF
-{
-  "project_name" : "$PROJECT_NAME",
-  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "$CHROME_PATH",
-  "src_path" : "$CHROMIUM_SRC"
-}
-EOF
-```
-
-Launch the app, passing the path to the deployment configuration file:
-
-```shell
-gunicorn --workers=1 --bind 127.0.0.1:8080 \
-    --pythonpath $CHROMIUM_SRC/tools/android/loading/gce \
-    'main:StartApp('\"$CONFIG_FILE\"')'
-```
-
-You can now [use the app][2], which is located at http://localhost:8080.
-
-Tear down the local environment:
-
-```shell
-deactivate
-```
-
-[1]: https://cloud.google.com/sdk
-[2]: #Use-the-app
diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
deleted file mode 100755
index 2362414..0000000
--- a/loading/gce/deploy.sh
+++ /dev/null
@@ -1,63 +0,0 @@
-#!/bin/bash
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-# This script copies all dependencies required for trace collection.
-# Usage:
-#   deploy.sh builddir gcs_path
-#
-# Where:
-#   builddir is the build directory for Chrome
-#   gcs_path is the Google Storage bucket under which the deployment is
-#   installed
-
-builddir=$1
-tmpdir=`mktemp -d`
-deployment_gcs_path=$2/deployment
-
-# Extract needed sources.
-src_suffix=src
-tmp_src_dir=$tmpdir/$src_suffix
-
-# Copy files from tools/android/loading.
-mkdir -p $tmp_src_dir/tools/android/loading
-cp tools/android/loading/*.py $tmp_src_dir/tools/android/loading
-cp -r tools/android/loading/gce $tmp_src_dir/tools/android/loading
-
-# Copy other dependencies.
-mkdir $tmp_src_dir/third_party
-rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
-  third_party/catapult $tmp_src_dir/third_party
-mkdir $tmp_src_dir/tools/perf
-cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
-mkdir -p $tmp_src_dir/build/android
-cp build/android/devil_chromium.py $tmp_src_dir/build/android/
-cp build/android/video_recorder.py $tmp_src_dir/build/android/
-cp build/android/devil_chromium.json $tmp_src_dir/build/android/
-cp -r build/android/pylib $tmp_src_dir/build/android/
-mkdir -p \
-  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
-cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \
-  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices/
-
-# Tar up the source and copy it to Google Cloud Storage.
-source_tarball=$tmpdir/source.tgz
-tar -cvzf $source_tarball -C $tmpdir $src_suffix
-gsutil cp $source_tarball gs://$deployment_gcs_path/source/
-
-# Copy the chrome executable to Google Cloud Storage.
-chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
-  $tmpdir/linux.zip
-gsutil cp $tmpdir/linux.zip gs://$deployment_gcs_path/binaries/linux.zip
-
-# Generate and upload metadata about this deployment.
-CHROMIUM_REV=$(git merge-base HEAD origin/master)
-cat >$tmpdir/build_metadata.json << EOF
-{
-  "chromium_rev": "$CHROMIUM_REV"
-}
-EOF
-gsutil cp $tmpdir/build_metadata.json \
-  gs://$deployment_gcs_path/deployment_metadata.json
-rm -rf $tmpdir
diff --git a/loading/gce/google_storage_accessor.py b/loading/gce/google_storage_accessor.py
deleted file mode 100644
index 59c47da..0000000
--- a/loading/gce/google_storage_accessor.py
+++ /dev/null
@@ -1,60 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-from gcloud import storage
-from oauth2client.client import GoogleCredentials
-
-
-class GoogleStorageAccessor(object):
-  """Utility class providing helpers for Google Cloud Storage.
-  """
-  def __init__(self, project_name, bucket_name):
-    """project_name is the name of the Google Cloud project.
-    bucket_name is the name of the bucket that is used for Cloud Storage calls.
-    """
-    self._credentials = GoogleCredentials.get_application_default()
-    self._project_name = project_name
-    self._bucket_name = bucket_name
-
-  def _GetStorageClient(self):
-    """Returns the storage client associated with the project"""
-    return storage.Client(project = self._project_name,
-                          credentials = self._credentials)
-
-  def _GetStorageBucket(self, storage_client):
-    return storage_client.get_bucket(self._bucket_name)
-
-  def UploadFile(self, filename_src, filename_dest):
-    """Uploads a file to Google Cloud Storage
-
-    Args:
-      filename_src: name of the local file
-      filename_dest: name of the file in Google Cloud Storage
-
-    Returns:
-      The URL of the file in Google Cloud Storage.
-    """
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename_dest)
-    with open(filename_src) as file_src:
-      blob.upload_from_file(file_src)
-    return blob.public_url
-
-  def UploadString(self, data_string, filename_dest):
-    """Uploads a string to Google Cloud Storage
-
-    Args:
-      data_string: the contents of the file to be uploaded
-      filename_dest: name of the file in Google Cloud Storage
-
-    Returns:
-      The URL of the file in Google Cloud Storage.
-    """
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename_dest)
-    blob.upload_from_string(data_string)
-    return blob.public_url
-
diff --git a/loading/gce/main.py b/loading/gce/main.py
deleted file mode 100644
index 0ef2cae..0000000
--- a/loading/gce/main.py
+++ /dev/null
@@ -1,280 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import json
-import os
-import re
-import threading
-import time
-import subprocess
-import sys
-
-# NOTE: The parent directory needs to be first in sys.path to avoid conflicts
-# with catapult modules that have colliding names, as catapult inserts itself
-# into the path as the second element. This is an ugly and fragile hack.
-sys.path.insert(0,
-    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
-import controller
-from google_storage_accessor import GoogleStorageAccessor
-import loading_trace
-from loading_trace_database import LoadingTraceDatabase
-import options
-
-
-class ServerApp(object):
-  """Simple web server application, collecting traces and writing them in
-  Google Cloud Storage.
-  """
-
-  def __init__(self, configuration_file):
-    """|configuration_file| is a path to a file containing JSON as described in
-    README.md.
-    """
-    self._tasks = []  # List of remaining tasks, only modified by _thread.
-    self._failed_tasks = []  # Failed tasks, only modified by _thread.
-    self._thread = None
-    self._tasks_lock = threading.Lock()  # Protects _tasks and _failed_tasks.
-    self._initial_task_count = -1
-    self._start_time = None
-    print 'Reading configuration'
-    with open(configuration_file) as config_json:
-       config = json.load(config_json)
-
-       # Separate the cloud storage path into the bucket and the base path under
-       # the bucket.
-       storage_path_components = config['cloud_storage_path'].split('/')
-       self._bucket_name = storage_path_components[0]
-       self._base_path_in_bucket = ''
-       if len(storage_path_components) > 1:
-         self._base_path_in_bucket = '/'.join(storage_path_components[1:])
-         if not self._base_path_in_bucket.endswith('/'):
-           self._base_path_in_bucket += '/'
-
-       self._src_path = config['src_path']
-       self._google_storage_accessor = GoogleStorageAccessor(
-           project_name=config['project_name'], bucket_name=self._bucket_name)
-
-    # Initialize the global options that will be used during trace generation.
-    options.OPTIONS.ParseArgs([])
-    options.OPTIONS.local_binary = config['chrome_path']
-
-  def _IsProcessingTasks(self):
-    """Returns True if the application is currently processing tasks."""
-    return self._thread is not None and self._thread.is_alive()
-
-  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
-                     log_filename):
-    """ Generates a trace on _thread.
-
-    Args:
-      url: URL as a string.
-      emulate_device: Name of the device to emulate. Empty for no emulation.
-      emulate_network: Type of network emulation. Empty for no emulation.
-      filename: Name of the file where the trace is saved.
-      log_filename: Name of the file where standard output and errors are logged
-
-    Returns:
-      A dictionary of metadata about the trace, including a 'succeeded' field
-      indicating whether the trace was successfully generated.
-    """
-    try:
-      os.remove(filename)  # Remove any existing trace for this URL.
-    except OSError:
-      pass  # Nothing to remove.
-
-    if not url.startswith('http') and not url.startswith('file'):
-      url = 'http://' + url
-
-    old_stdout = sys.stdout
-    old_stderr = sys.stderr
-
-    trace_metadata = { 'succeeded' : False, 'url' : url }
-    trace = None
-    with open(log_filename, 'w') as sys.stdout:
-      try:
-        sys.stderr = sys.stdout
-
-        # Set up the controller.
-        chrome_ctl = controller.LocalChromeController()
-        chrome_ctl.SetHeadless(True)
-        if emulate_device:
-          chrome_ctl.SetDeviceEmulation(emulate_device)
-        if emulate_network:
-          chrome_ctl.SetNetworkEmulation(emulate_network)
-
-        # Record and write the trace.
-        with chrome_ctl.OpenWithRedirection(sys.stdout,
-                                            sys.stderr) as connection:
-          connection.ClearCache()
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url, connection, chrome_ctl.ChromeMetadata())
-          trace_metadata['succeeded'] = True
-          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
-      except Exception as e:
-        sys.stderr.write(str(e))
-
-      if trace:
-        with open(filename, 'w') as f:
-          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
-
-    sys.stdout = old_stdout
-    sys.stderr = old_stderr
-
-    return trace_metadata
-
-  def _GetCurrentTaskCount(self):
-    """Returns the number of remaining tasks. Thread safe."""
-    self._tasks_lock.acquire()
-    task_count = len(self._tasks)
-    self._tasks_lock.release()
-    return task_count
-
-  def _ProcessTasks(self, tasks, repeat_count, emulate_device, emulate_network):
-    """Iterates over _task, generating a trace for each of them. Uploads the
-    resulting traces to Google Cloud Storage.  Runs on _thread.
-
-    Args:
-      tasks: The list of URLs to process.
-      repeat_count: The number of traces generated for each URL.
-      emulate_device: Name of the device to emulate. Empty for no emulation.
-      emulate_network: Type of network emulation. Empty for no emulation.
-    """
-    # The main thread might be reading the task lists, take the lock to modify.
-    self._tasks_lock.acquire()
-    self._tasks = tasks
-    self._failed_tasks = []
-    self._tasks_lock.release()
-    failures_dir = self._base_path_in_bucket + 'failures/'
-    traces_dir = self._base_path_in_bucket + 'traces/'
-
-    trace_database = LoadingTraceDatabase({})
-
-    # TODO(blundell): Fix this up.
-    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
-    log_filename = 'analyze.log'
-    # Avoid special characters in storage object names
-    pattern = re.compile(r"[#\?\[\]\*/]")
-    while len(self._tasks) > 0:
-      url = self._tasks[-1]
-      local_filename = pattern.sub('_', url)
-      for repeat in range(repeat_count):
-        print 'Generating trace for URL: %s' % url
-        remote_filename = local_filename + '/' + str(repeat)
-        trace_metadata = self._GenerateTrace(
-            url, emulate_device, emulate_network, local_filename, log_filename)
-        if trace_metadata['succeeded']:
-          print 'Uploading: %s' % remote_filename
-          remote_trace_location = traces_dir + remote_filename
-          self._google_storage_accessor.UploadFile(local_filename,
-                                           remote_trace_location)
-          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
-              remote_trace_location)
-          trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
-        else:
-          print 'Trace generation failed for URL: %s' % url
-          self._tasks_lock.acquire()
-          self._failed_tasks.append({ "url": url, "repeat": repeat})
-          self._tasks_lock.release()
-          if os.path.isfile(local_filename):
-            self._google_storage_accessor.UploadFile(local_filename,
-                                            failures_dir + remote_filename)
-        print 'Uploading log'
-        self._google_storage_accessor.UploadFile(log_filename,
-                                         logs_dir + remote_filename)
-      # Pop once task is finished, for accurate status tracking.
-      self._tasks_lock.acquire()
-      url = self._tasks.pop()
-      self._tasks_lock.release()
-
-    self._google_storage_accessor.UploadString(
-        json.dumps(trace_database.ToJsonDict(), indent=2),
-        traces_dir + 'trace_database.json')
-
-    if len(self._failed_tasks) > 0:
-      print 'Uploading failing URLs'
-      self._google_storage_accessor.UploadString(
-          json.dumps(self._failed_tasks, indent=2),
-          failures_dir + 'failures.json')
-
-  def _SetTaskList(self, http_body):
-    """Sets the list of tasks and starts processing them
-
-    Args:
-      http_body: JSON dictionary. See README.md for a description of the format.
-
-    Returns:
-      A string to be sent back to the client, describing the success status of
-      the request.
-    """
-    if self._IsProcessingTasks():
-      return 'Error: Already running\n'
-
-    load_parameters = json.loads(http_body)
-    try:
-      tasks = load_parameters['urls']
-    except KeyError:
-      return 'Error: invalid urls\n'
-    # Optional parameters.
-    try:
-      repeat_count = int(load_parameters.get('repeat_count', '1'))
-    except ValueError:
-      return 'Error: invalid repeat_count\n'
-    emulate_device = load_parameters.get('emulate_device', '')
-    emulate_network = load_parameters.get('emulate_network', '')
-
-    if len(tasks) == 0:
-      return 'Error: Empty task list\n'
-    else:
-      self._initial_task_count = len(tasks)
-      self._start_time = time.time()
-      self._thread = threading.Thread(
-          target = self._ProcessTasks,
-          args = (tasks, repeat_count, emulate_device, emulate_network))
-      self._thread.start()
-      return 'Starting generation of %s tasks\n' % str(self._initial_task_count)
-
-  def __call__(self, environ, start_response):
-    path = environ['PATH_INFO']
-
-    if path == '/set_tasks':
-      # Get the tasks from the HTTP body.
-      try:
-        body_size = int(environ.get('CONTENT_LENGTH', 0))
-      except (ValueError):
-        body_size = 0
-      body = environ['wsgi.input'].read(body_size)
-      data = self._SetTaskList(body)
-    elif path == '/test':
-      data = 'hello\n'
-    elif path == '/status':
-      if not self._IsProcessingTasks():
-        data = 'Idle\n'
-      else:
-        task_count = self._GetCurrentTaskCount()
-        if task_count == 0:
-          data = '%s tasks complete. Finalizing.\n' % self._initial_task_count
-        else:
-          data = 'Remaining tasks: %s / %s\n' % (
-              task_count, self._initial_task_count)
-        elapsed = time.time() - self._start_time
-        data += 'Elapsed time: %s seconds\n' % str(elapsed)
-        self._tasks_lock.acquire()
-        failed_tasks = self._failed_tasks
-        self._tasks_lock.release()
-        data += '%s failed tasks:\n' % len(failed_tasks)
-        data += json.dumps(failed_tasks, indent=2)
-    else:
-      start_response('404 NOT FOUND', [('Content-Length', '0')])
-      return iter([''])
-
-    response_headers = [
-        ('Content-type','text/plain'),
-        ('Content-Length', str(len(data)))
-    ]
-    start_response('200 OK', response_headers)
-    return iter([data])
-
-
-def StartApp(configuration_file):
-  return ServerApp(configuration_file)
diff --git a/loading/gce/pip_requirements.txt b/loading/gce/pip_requirements.txt
deleted file mode 100644
index 7d97e12..0000000
--- a/loading/gce/pip_requirements.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-gunicorn==19.4.5
-gcloud==0.10.1
-psutil==4.1.0
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
deleted file mode 100644
index 94edb22..0000000
--- a/loading/gce/startup-script.sh
+++ /dev/null
@@ -1,108 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-# Script executed at instance startup. It installs the required dependencies,
-# downloads the source code, and starts a web server.
-
-set -v
-
-get_instance_metadata() {
-  curl -fs http://metadata/computeMetadata/v1/instance/attributes/$1 \
-      -H "Metadata-Flavor: Google"
-}
-
-# Talk to the metadata server to get the project id
-PROJECTID=$(curl -s \
-    "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
-    -H "Metadata-Flavor: Google")
-
-# Install dependencies from apt
-apt-get update
-# Basic dependencies
-apt-get install -yq git supervisor python-pip python-dev unzip
-# Web server dependencies
-apt-get install -yq libffi-dev libssl-dev
-# Chrome dependencies
-apt-get install -yq libpangocairo-1.0-0 libXcomposite1 libXcursor1 libXdamage1 \
-    libXi6 libXtst6 libnss3 libcups2 libgconf2-4 libXss1 libXrandr2 \
-    libatk1.0-0 libasound2 libgtk2.0-0
-# Trace collection dependencies
-apt-get install -yq xvfb
-
-# Create a pythonapp user. The application will run as this user.
-useradd -m -d /home/pythonapp pythonapp
-
-# pip from apt is out of date, so make it update itself and install virtualenv.
-pip install --upgrade pip virtualenv
-
-# Download the Clovis deployment from Google Cloud Storage and unzip it.
-# It is expected that the contents of the deployment have been generated using
-# the tools/android/loading/gce/deploy.sh script.
-CLOUD_STORAGE_PATH=`get_instance_metadata cloud-storage-path`
-DEPLOYMENT_PATH=$CLOUD_STORAGE_PATH/deployment
-
-mkdir -p /opt/app/clovis
-gsutil cp gs://$DEPLOYMENT_PATH/source/source.tgz /opt/app/clovis/source.tgz
-tar xvf /opt/app/clovis/source.tgz -C /opt/app/clovis
-rm /opt/app/clovis/source.tgz
-
-# Install app dependencies
-virtualenv /opt/app/clovis/env
-/opt/app/clovis/env/bin/pip install \
-    -r /opt/app/clovis/src/tools/android/loading/gce/pip_requirements.txt
-
-mkdir /opt/app/clovis/binaries
-gsutil cp gs://$DEPLOYMENT_PATH/binaries/* /opt/app/clovis/binaries/
-unzip /opt/app/clovis/binaries/linux.zip -d /opt/app/clovis/binaries/
-
-# Install the Chrome sandbox
-cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
-chown root:root /usr/local/sbin/chrome-devel-sandbox
-chmod 4755 /usr/local/sbin/chrome-devel-sandbox
-
-# Make sure the pythonapp user owns the application code
-chown -R pythonapp:pythonapp /opt/app
-
-# Create the configuration file for this deployment.
-DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
-cat >$DEPLOYMENT_CONFIG_PATH << EOF
-{
-  "project_name" : "$PROJECTID",
-  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "/opt/app/clovis/binaries/chrome",
-  "src_path" : "/opt/app/clovis/src"
-}
-EOF
-
-# Check if auto-start is enabled
-AUTO_START=`get_instance_metadata auto-start`
-
-# Exit early if auto start is not enabled.
-if [ "$AUTO_START" != "true" ]; then
-  exit 1
-fi
-
-# Configure supervisor to start gunicorn inside of our virtualenv and run the
-# applicaiton.
-cat >/etc/supervisor/conf.d/python-app.conf << EOF
-[program:pythonapp]
-directory=/opt/app/clovis/src/tools/android/loading/gce
-command=/opt/app/clovis/env/bin/gunicorn --workers=1 --bind 0.0.0.0:8080 \
-    'main:StartApp('\"$DEPLOYMENT_CONFIG_PATH\"')'
-autostart=true
-autorestart=true
-user=pythonapp
-# Environment variables ensure that the application runs inside of the
-# configured virtualenv.
-environment=VIRTUAL_ENV="/opt/app/clovis/env", \
-    PATH="/opt/app/clovis/env/bin:/usr/bin", \
-    HOME="/home/pythonapp",USER="pythonapp", \
-    CHROME_DEVEL_SANDBOX="/usr/local/sbin/chrome-devel-sandbox"
-stdout_logfile=syslog
-stderr_logfile=syslog
-EOF
-
-supervisorctl reread
-supervisorctl update
-

commit 3c3a0aacf519579124717d2d076d4027b8eef718
Author: agrieve <agrieve@chromium.org>
Date:   Tue Apr 19 10:33:02 2016 -0700

    Fix various findbugs warnings found when building "all" with GN
    
    TBR=mikecase
    BUG=604456
    
    Review URL: https://codereview.chromium.org/1903473003
    
    Cr-Original-Commit-Position: refs/heads/master@{#388234}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 794f776af830155360e6a24f575b6f9f1e763bec

diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
index 7030b8f..d8e3696 100644
--- a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
@@ -38,6 +38,8 @@ public class AudioFocusGrabberActivity extends Activity {
             case R.id.button_hide_notification:
                 intent.setAction(AudioFocusGrabberListenerService.ACTION_HIDE_NOTIFICATION);
                 break;
+            default:
+                break;
         }
         startService(intent);
     }
diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
index 9981fd8..44b9806 100644
--- a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
@@ -144,6 +144,8 @@ public class AudioFocusGrabberListenerService extends Service {
                             mMediaPlayer.setVolume(0.1f, 0.1f);
                             mIsDucking = true;
                             break;
+                        default:
+                            break;
                     }
                 }
             };

commit 700de317cc9b79ee4cfa9cf7e8b69c4768b50450
Author: droger <droger@chromium.org>
Date:   Tue Apr 19 10:23:18 2016 -0700

    tools/android/loading Add appengine frontend for Clovis
    
    Review URL: https://codereview.chromium.org/1878943013
    
    Cr-Original-Commit-Position: refs/heads/master@{#388229}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0ee8eea8d5648db41fecddac32edca241d00fa44

diff --git a/loading/PRESUBMIT.py b/loading/PRESUBMIT.py
index b9bcc19..cd740ef 100644
--- a/loading/PRESUBMIT.py
+++ b/loading/PRESUBMIT.py
@@ -11,7 +11,7 @@ for more details on the presubmit API built into depot_tools.
 
 def CommonChecks(input_api, output_api):
   output = []
-  blacklist = []
+  blacklist = [r'cloud/frontend/lib/*']
   output.extend(input_api.canned_checks.RunPylint(
       input_api, output_api, black_list=blacklist))
   output.extend(input_api.canned_checks.RunUnitTests(
diff --git a/loading/cloud/__init__.py b/loading/cloud/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/loading/cloud/common/__init__.py b/loading/cloud/common/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/loading/cloud/common/clovis_task.py b/loading/cloud/common/clovis_task.py
new file mode 100644
index 0000000..8bc3127
--- /dev/null
+++ b/loading/cloud/common/clovis_task.py
@@ -0,0 +1,66 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import base64
+import json
+
+class ClovisTask(object):
+  """Generic task, generated by the AppEngine frontend and consumed by the
+  ComputeEngine backend.
+  """
+
+  def __init__(self, action, params, taskqueue_tag):
+    """Params:
+      action(str): Action accomplished by this task.
+      params(dict): Parameters of task.
+      taskqueue_tag(str): Tag of the task. Optional.
+    """
+    self._action = action
+    self._params = params
+    self._taskqueue_tag = taskqueue_tag
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Loads a ClovisTask from a JSON string.
+
+    Returns:
+      ClovisTask: The task, or None if the string is invalid.
+    """
+    try:
+      data = json.loads(json_dict)
+      action = data['action']
+      params = data['params']
+      tag = data.get('taskqueue_tag')
+      # Vaidate the format.
+      if action == 'trace':
+        urls = params['urls']
+        if (type(urls) is not list) or (len(urls) == 0):
+          return None
+      else:
+        # When more actions are supported, check that they are valid here.
+        return None
+      return cls(action, params, tag)
+    except Exception:
+      return None
+
+  @classmethod
+  def FromBase64(cls, base64_string):
+    """Loads a ClovisTask from a base 64 string."""
+    return ClovisTask.FromJsonDict(base64.b64decode(base64_string))
+
+  def ToJsonDict(self):
+    """Returns the JSON representation of the task."""
+    task_dict = { 'action': self._action, 'params': self._params }
+    if self._taskqueue_tag:
+      task_dict['taskqueue_tag'] = self._taskqueue_tag
+    return json.dumps(task_dict)
+
+  def Action(self):
+    return self._action
+
+  def Params(self):
+    return self._params
+
+  def TaskqueueTag(self):
+    return self._taskqueue_tag
diff --git a/loading/cloud/frontend/.gitignore b/loading/cloud/frontend/.gitignore
new file mode 100644
index 0000000..c3af857
--- /dev/null
+++ b/loading/cloud/frontend/.gitignore
@@ -0,0 +1 @@
+lib/
diff --git a/loading/cloud/frontend/README.md b/loading/cloud/frontend/README.md
new file mode 100644
index 0000000..f6f6f5f
--- /dev/null
+++ b/loading/cloud/frontend/README.md
@@ -0,0 +1,91 @@
+# Appengine Frontend for Clovis
+
+[TOC]
+
+## Usage
+
+Visit the application URL in your browser, and upload a JSON dictionary with the
+following keys:
+
+-   `action` (string): the action to perform. Only `trace` is supported.
+-   `params` (dictionary): the parameters associated to the action. See below
+    for more details.
+-   `taskqueue_tag` (string, optional): the [TaskQueue][2] tag internally used
+    to send the work from AppEngine to ComputeEngine.  If this parameter is not
+    specified, a unique tag will be created.
+
+### Parameters for the `trace` action.
+
+-   `urls` (list of strings): the list of URLs to process.
+-   `repeat_count` (integer, optional): the number of traces to be generated
+    for each URL. Defaults to 1.
+-   `emulate_device` (string, optional): the device to emulate (e.g. `Nexus 4`).
+-   `emulate_network` (string, optional): the network to emulate.
+
+## Development
+
+### Design overview
+
+-   Appengine configuration:
+    -   `app.yaml` defines the handlers. There is a static handler for all URLs
+    in the `static/` directory, and all other URLs are handled by the
+    `clovis_frontend.py` script.
+    -   `queue.yaml` defines the task queues associated with the application. In
+        particular, the `clovis-queue` is a pull-queue where tasks are added by
+        the AppEngine frontend and consummed by the ComputeEngine backend.
+        See the [TaskQueue documentation][2] for more details.
+-   `static/form.html` is a static HTML document allowing the user to upload a
+    JSON file. `clovis_frontend.py` is then invoked with the contents of the
+    file (see the `/form_sent` handler).
+-   `clovis_task.py` defines a task to be run by the backend. It is sent through
+    the `clovis-queue` task queue.
+-   `clovis_frontend.py` is the script that processes the file uploaded by the
+    form, creates the tasks and enqueues them in `clovis-queue`.
+
+### Prerequisites
+
+-   Install the gcloud [tool][1]
+-   Add a `queue.yaml` file in the application directory (i.e. next to
+    `app.yaml`) defining a `clovis-queue` pull queue that can be accessed by the
+    ComputeEngine service worker associated to the project. Add your email too
+    if you want to run the application locally. See the [TaskQueue configuration
+    documentation][3] for more details. Example:
+
+```
+# queue.yaml
+- name: clovis-queue
+  mode: pull
+  acl:
+    - user_email: me@address.com
+    - user_email: 123456789-compute@developer.gserviceaccount.com
+```
+
+### Run Locally
+
+```shell
+# Install dependencies in the lib/ directory. Note that this will pollute your
+# Chromium checkout, see the cleanup intructions below.
+pip install -r requirements.txt -t lib
+# Start the local server.
+dev_appserver.py .
+```
+
+Visit the application [http://localhost:8080](http://localhost:8080).
+
+After you are done, cleanup your Chromium checkout:
+```shell
+rm -rf $CHROMIUM_SRC/tools/android/loading/frontend/lib
+```
+
+### Deploy
+
+````shell
+# Install dependencies in the lib/ directory.
+pip install -r requirements.txt -t lib
+# Deploy.
+gcloud preview app deploy app.yaml
+```
+
+[1]: https://cloud.google.com/sdk
+[2]: https://cloud.google.com/appengine/docs/python/taskqueue
+[3]: https://cloud.google.com/appengine/docs/python/config/queue
diff --git a/loading/cloud/frontend/app.yaml b/loading/cloud/frontend/app.yaml
new file mode 100644
index 0000000..7dcc1d6
--- /dev/null
+++ b/loading/cloud/frontend/app.yaml
@@ -0,0 +1,10 @@
+runtime: python27
+api_version: 1
+threadsafe: yes
+
+handlers:
+- url: /static
+  static_dir: static
+
+- url: .*
+  script: clovis_frontend.app
diff --git a/loading/cloud/frontend/appengine_config.py b/loading/cloud/frontend/appengine_config.py
new file mode 100644
index 0000000..608cd73
--- /dev/null
+++ b/loading/cloud/frontend/appengine_config.py
@@ -0,0 +1,6 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from google.appengine.ext import vendor
+vendor.add('lib')
diff --git a/loading/cloud/frontend/clovis_frontend.py b/loading/cloud/frontend/clovis_frontend.py
new file mode 100644
index 0000000..8eeee35
--- /dev/null
+++ b/loading/cloud/frontend/clovis_frontend.py
@@ -0,0 +1,109 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import flask
+from google.appengine.api import taskqueue
+import json
+import os
+import sys
+import uuid
+
+from common.clovis_task import ClovisTask
+
+
+app = flask.Flask(__name__)
+
+
+def StartFromJson(http_body_str):
+  """Creates a new batch of tasks from its JSON representation."""
+  task = ClovisTask.FromJsonDict(http_body_str)
+  if not task:
+    return 'Invalid JSON task:\n%s\n' % http_body_str
+
+  task_tag = task.TaskqueueTag()
+  if not task_tag:
+    task_tag = uuid.uuid1()
+
+  sub_tasks = []
+  if task.Action() == 'trace':
+    sub_tasks = SplitTraceTask(task)
+  else:
+    return 'Unsupported action: %s\n' % task.Action()
+
+  return EnqueueTasks(sub_tasks, task_tag)
+
+
+def SplitTraceTask(task):
+  """Split a tracing task with potentially many URLs into several tracing tasks
+  with few URLs.
+  """
+  params = task.Params()
+  urls = params['urls']
+
+  # Split the task in smaller tasks with fewer URLs each.
+  urls_per_task = 1
+  sub_tasks = []
+  for i in range(0, len(urls), urls_per_task):
+    sub_task_params = params.copy()
+    sub_task_params['urls'] = [url for url in urls[i:i+urls_per_task]]
+    sub_tasks.append(ClovisTask(task.Action(), sub_task_params,
+                                task.TaskqueueTag()))
+  return sub_tasks
+
+
+def EnqueueTasks(tasks, task_tag):
+  """Enqueues a list of tasks in the Google Cloud task queue, for consumption by
+  Google Compute Engine.
+  """
+  q = taskqueue.Queue('clovis-queue')
+  retry_options = taskqueue.TaskRetryOptions(task_retry_limit=3)
+  # Add tasks to the queue by groups.
+  # TODO(droger): This support to thousands of tasks, but maybe not millions.
+  # Defer the enqueuing if it times out.
+  # is too large.
+  group_size = 100
+  callbacks = []
+  try:
+    for i in range(0, len(tasks), group_size):
+      group = tasks[i:i+group_size]
+      taskqueue_tasks = [
+          taskqueue.Task(payload=task.ToJsonDict(), method='PULL', tag=task_tag,
+                         retry_options=retry_options)
+          for task in group]
+      rpc = taskqueue.create_rpc()
+      q.add_async(task=taskqueue_tasks, rpc=rpc)
+      callbacks.append(rpc)
+    for callback in callbacks:
+      callback.get_result()
+  except Exception as e:
+    return 'Exception:' + type(e).__name__ + ' ' + str(e.args) + '\n'
+  return 'pushed %i tasks with tag: %s\n' % (len(tasks), task_tag)
+
+
+@app.route('/')
+def Root():
+  """Home page: redirect to the static form."""
+  return flask.redirect('/static/form.html')
+
+
+@app.route('/form_sent', methods=['POST'])
+def StartFromForm():
+  """HTML form endpoint"""
+  data_stream = flask.request.files.get('json_task')
+  if not data_stream:
+    return 'failed'
+  http_body_str = data_stream.read()
+  return StartFromJson(http_body_str)
+
+
+@app.errorhandler(404)
+def PageNotFound(e):  # pylint: disable=unused-argument
+  """Return a custom 404 error."""
+  return 'Sorry, Nothing at this URL.', 404
+
+
+@app.errorhandler(500)
+def ApplicationError(e):
+  """Return a custom 500 error."""
+  return 'Sorry, unexpected error: {}'.format(e), 499
diff --git a/loading/cloud/frontend/common b/loading/cloud/frontend/common
new file mode 120000
index 0000000..60d3b0a
--- /dev/null
+++ b/loading/cloud/frontend/common
@@ -0,0 +1 @@
+../common
\ No newline at end of file
diff --git a/loading/cloud/frontend/requirements.txt b/loading/cloud/frontend/requirements.txt
new file mode 100644
index 0000000..880a7bc
--- /dev/null
+++ b/loading/cloud/frontend/requirements.txt
@@ -0,0 +1 @@
+Flask==0.10
diff --git a/loading/cloud/frontend/static/form.html b/loading/cloud/frontend/static/form.html
new file mode 100644
index 0000000..927cf2b
--- /dev/null
+++ b/loading/cloud/frontend/static/form.html
@@ -0,0 +1,17 @@
+<!DOCTYPE html>
+<html>
+
+<head>
+<meta charset="utf-8">
+<title>Submmit</title>
+</head>
+
+<body>
+<p> Select JSON file </p>
+<form action="/form_sent" method="POST" enctype="multipart/form-data">
+<input type="file" name="json_task"/>
+<input type="submit" name="submit" value="Upload"/>
+</form>
+</body>
+
+</html>

commit a79cac683210973f39ab62dfb89e88799b803f7d
Author: blundell <blundell@chromium.org>
Date:   Tue Apr 19 07:54:52 2016 -0700

    tools/android/loading: Add ability to match only main frame events
    
    This CL adds the ability to get tracing events that occurred in the
    main frame (as opposed to in any frame). This functionality works for
    events whose 'args' have a 'frame' key (e.g.,
    'blink.user_timing.firstContentfulPaint'). The ID of the main frame is
    defined to be that of the earliest 'blink.user_timing.navigationStart'
    event in the trace.
    
    This functionality will be used to e.g. find the first contentful paint
    of the main frame.
    
    Review URL: https://codereview.chromium.org/1899843004
    
    Cr-Original-Commit-Position: refs/heads/master@{#388206}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8faf66ee0a7dd136ab4ec28b0821b1ec5c57a26c

diff --git a/loading/tracing.py b/loading/tracing.py
index c8db1df..4891d3e 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -50,6 +50,7 @@ class TracingTrack(devtools_monitor.Track):
     self._events = []
     self._base_msec = None
     self._interval_tree = None
+    self._main_frame_id = None
 
   def Handle(self, method, event):
     for e in event['params']['value']:
@@ -61,6 +62,16 @@ class TracingTrack(devtools_monitor.Track):
     # update.
     self._interval_tree = None
 
+  def _GetMainFrameID(self):
+    """Returns the main frame ID."""
+    if not self._main_frame_id:
+      navigation_start_events = [e for e in self.GetEvents()
+          if e.Matches('blink.user_timing', 'navigationStart')]
+      first_event = min(navigation_start_events, key=lambda e: e.start_msec)
+      self._main_frame_id = first_event.args['frame']
+
+    return self._main_frame_id
+
   def GetFirstEventMillis(self):
     """Find the canonical start time for this track.
 
@@ -72,6 +83,17 @@ class TracingTrack(devtools_monitor.Track):
   def GetEvents(self):
     return self._events
 
+  def GetMatchingEvents(self, category, name):
+    """Gets events matching |category| and |name|."""
+    return [e for e in self.GetEvents() if e.Matches(category, name)]
+
+  def GetMatchingMainFrameEvents(self, category, name):
+    """Gets events matching |category| and |name| that occur in the main frame.
+    Assumes that the events in question have a 'frame' key in their |args|."""
+    matching_events = self.GetMatchingEvents(category, name)
+    return [e for e in matching_events
+        if e.args['frame'] == self._GetMainFrameID()]
+
   def EventsAt(self, msec):
     """Gets events active at a timestamp.
 
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 1bf0273..0d7ba27 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -295,6 +295,48 @@ class TracingTrackTestCase(unittest.TestCase):
     tracing_track = self.track.Filter(2, 42)
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
+  def testGetMainFrameID(self):
+    _MAIN_FRAME_ID = 0xffff
+    _SUBFRAME_ID = 0xaaaa
+    events = [
+        {'ts': 7, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x123',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _SUBFRAME_ID}},
+        {'ts': 8, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 1, 'id': '0x12343',
+        'name': 'A'},
+        {'ts': 3, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x125',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _MAIN_FRAME_ID}},
+        ]
+    self._HandleEvents(events)
+    self.assertEquals(_MAIN_FRAME_ID, self.track._GetMainFrameID())
+
+  def testGetMatchingEvents(self):
+    _MAIN_FRAME_ID = 0xffff
+    _SUBFRAME_ID = 0xaaaa
+    events = [
+        {'ts': 7, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x123',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _SUBFRAME_ID}},
+        {'ts': 8, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 1, 'id': '0x12343',
+        'name': 'A'},
+        {'ts': 3, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x125',
+         'name': 'navigationStart', 'cat': 'blink.user_timing',
+         'args': {'frame': _MAIN_FRAME_ID}},
+        ]
+    self._HandleEvents(events)
+    matching_events = self.track.GetMatchingEvents('blink.user_timing',
+                                                   'navigationStart')
+    self.assertEquals(2, len(matching_events))
+    self.assertListEqual([self.track.GetEvents()[0],
+                         self.track.GetEvents()[2]], matching_events)
+
+    matching_main_frame_events = self.track.GetMatchingMainFrameEvents(
+        'blink.user_timing', 'navigationStart')
+    self.assertEquals(1, len(matching_main_frame_events))
+    self.assertListEqual([self.track.GetEvents()[2]],
+                         matching_main_frame_events)
+
   def testFilterCategories(self):
     events = [
         {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'A'},

commit a452684c7bda908ebf10e58769ea1e773982d291
Author: blundell <blundell@chromium.org>
Date:   Mon Apr 18 04:31:55 2016 -0700

    Add ability to execute loading graph view visualization
    
    Patch originally by Matt Cary (mattcary@chromium.org).
    
    Review URL: https://codereview.chromium.org/1900573002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387891}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 386a6486443ce0d5b6641fa6dc7f2b0624d69b7d

diff --git a/loading/loading_graph_view_visualization.py b/loading/loading_graph_view_visualization.py
index d083575..7a89f6b 100644
--- a/loading/loading_graph_view_visualization.py
+++ b/loading/loading_graph_view_visualization.py
@@ -2,8 +2,12 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Visualize a loading_graph_view.LoadingGraphView."""
+"""Visualize a loading_graph_view.LoadingGraphView.
 
+When executed as a script, takes a loading trace and generates a png of the
+loading graph."""
+
+import activity_lens
 import request_track
 
 
@@ -167,3 +171,28 @@ class LoadingGraphViewVisualization(object):
     from_request_id = edge.from_node.request.request_id
     to_request_id = edge.to_node.request.request_id
     return '"%s" -> "%s" %s;\n' % (from_request_id, to_request_id, arrow)
+
+def main(trace_file):
+  import subprocess
+
+  import loading_graph_view
+  import loading_trace
+  import request_dependencies_lens
+
+  trace = loading_trace.LoadingTrace.FromJsonFile(trace_file)
+  dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
+  activity = activity_lens.ActivityLens(trace)
+  graph_view = loading_graph_view.LoadingGraphView(trace, dependencies_lens,
+                                                   activity=activity)
+  visualization = LoadingGraphViewVisualization(graph_view)
+
+  dotfile = trace_file + '.dot'
+  pngfile = trace_file + '.png'
+  with file(dotfile, 'w') as output:
+    visualization.OutputDot(output)
+  subprocess.check_call(['dot', '-Tpng', dotfile, '-o', pngfile])
+
+
+if __name__ == '__main__':
+  import sys
+  main(sys.argv[1])

commit cef414409a92489b0a749b8eb348c620ff93da7e
Author: gabadie <gabadie@chromium.org>
Date:   Mon Apr 18 03:34:56 2016 -0700

    tools/android/loading: Fixes inspector_websocket's missing timeouts
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1895733002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387884}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6893a0e7dbb3f68ee5c8ce3f7554e9922466a13b

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index aade097..bfbf00f 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -22,7 +22,9 @@ from telemetry.internal.backends.chrome_inspector import websocket
 import common_util
 
 
-DEFAULT_TIMEOUT_SECONDS = 10 # seconds
+DEFAULT_TIMEOUT_SECONDS = 10
+
+_WEBSOCKET_TIMEOUT_SECONDS = 10
 
 
 class DevToolsConnectionException(Exception):
@@ -177,7 +179,7 @@ class DevToolsConnection(object):
     request = {'method': method}
     if params:
       request['params'] = params
-    return self._ws.SyncRequest(request)
+    return self._ws.SyncRequest(request, timeout=_WEBSOCKET_TIMEOUT_SECONDS)
 
   def SendAndIgnoreResponse(self, method, params=None):
     """Issues a request to the DevTools server, do not wait for the response.
@@ -370,7 +372,8 @@ class DevToolsConnection(object):
         break
     assert self._target_descriptor['url'] == 'about:blank'
     self._ws = inspector_websocket.InspectorWebsocket()
-    self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'])
+    self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'],
+                     timeout=_WEBSOCKET_TIMEOUT_SECONDS)
 
 
 class Listener(object):

commit 4715535a451ec7d4e23fbcbad113dc708a12b2fc
Author: droger <droger@chromium.org>
Date:   Mon Apr 18 01:41:02 2016 -0700

    tools/android/loading Split GoogleAPI wrappers to a separate library
    
    This is in preparation of the migration to a master/slave
    architecture.
    
    Review URL: https://codereview.chromium.org/1882793006
    
    Cr-Original-Commit-Position: refs/heads/master@{#387873}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d452ab4c620c15a11333b36d8e5984248e158e35

diff --git a/loading/gce/google_storage_accessor.py b/loading/gce/google_storage_accessor.py
new file mode 100644
index 0000000..59c47da
--- /dev/null
+++ b/loading/gce/google_storage_accessor.py
@@ -0,0 +1,60 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+from gcloud import storage
+from oauth2client.client import GoogleCredentials
+
+
+class GoogleStorageAccessor(object):
+  """Utility class providing helpers for Google Cloud Storage.
+  """
+  def __init__(self, project_name, bucket_name):
+    """project_name is the name of the Google Cloud project.
+    bucket_name is the name of the bucket that is used for Cloud Storage calls.
+    """
+    self._credentials = GoogleCredentials.get_application_default()
+    self._project_name = project_name
+    self._bucket_name = bucket_name
+
+  def _GetStorageClient(self):
+    """Returns the storage client associated with the project"""
+    return storage.Client(project = self._project_name,
+                          credentials = self._credentials)
+
+  def _GetStorageBucket(self, storage_client):
+    return storage_client.get_bucket(self._bucket_name)
+
+  def UploadFile(self, filename_src, filename_dest):
+    """Uploads a file to Google Cloud Storage
+
+    Args:
+      filename_src: name of the local file
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    with open(filename_src) as file_src:
+      blob.upload_from_file(file_src)
+    return blob.public_url
+
+  def UploadString(self, data_string, filename_dest):
+    """Uploads a string to Google Cloud Storage
+
+    Args:
+      data_string: the contents of the file to be uploaded
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    blob.upload_from_string(data_string)
+    return blob.public_url
+
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 2988113..0ef2cae 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -10,19 +10,16 @@ import time
 import subprocess
 import sys
 
-from gcloud import storage
-from gcloud.exceptions import NotFound
-from oauth2client.client import GoogleCredentials
-
 # NOTE: The parent directory needs to be first in sys.path to avoid conflicts
 # with catapult modules that have colliding names, as catapult inserts itself
 # into the path as the second element. This is an ugly and fragile hack.
 sys.path.insert(0,
     os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
 import controller
+from google_storage_accessor import GoogleStorageAccessor
 import loading_trace
-import options
 from loading_trace_database import LoadingTraceDatabase
+import options
 
 
 class ServerApp(object):
@@ -40,12 +37,9 @@ class ServerApp(object):
     self._tasks_lock = threading.Lock()  # Protects _tasks and _failed_tasks.
     self._initial_task_count = -1
     self._start_time = None
-    print 'Initializing credentials'
-    self._credentials = GoogleCredentials.get_application_default()
     print 'Reading configuration'
     with open(configuration_file) as config_json:
        config = json.load(config_json)
-       self._project_name = config['project_name']
 
        # Separate the cloud storage path into the bucket and the base path under
        # the bucket.
@@ -58,6 +52,8 @@ class ServerApp(object):
            self._base_path_in_bucket += '/'
 
        self._src_path = config['src_path']
+       self._google_storage_accessor = GoogleStorageAccessor(
+           project_name=config['project_name'], bucket_name=self._bucket_name)
 
     # Initialize the global options that will be used during trace generation.
     options.OPTIONS.ParseArgs([])
@@ -67,46 +63,6 @@ class ServerApp(object):
     """Returns True if the application is currently processing tasks."""
     return self._thread is not None and self._thread.is_alive()
 
-  def _GetStorageClient(self):
-    return storage.Client(project = self._project_name,
-                          credentials = self._credentials)
-
-  def _GetStorageBucket(self, storage_client):
-    return storage_client.get_bucket(self._bucket_name)
-
-  def _UploadFile(self, filename_src, filename_dest):
-    """Uploads a file to Google Cloud Storage
-
-    Args:
-      filename_src: name of the local file
-      filename_dest: name of the file in Google Cloud Storage
-
-    Returns:
-      The URL of the file in Google Cloud Storage.
-    """
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename_dest)
-    with open(filename_src) as file_src:
-      blob.upload_from_file(file_src)
-    return blob.public_url
-
-  def _UploadString(self, data_string, filename_dest):
-    """Uploads a string to Google Cloud Storage
-
-    Args:
-      data_string: the contents of the file to be uploaded
-      filename_dest: name of the file in Google Cloud Storage
-
-    Returns:
-      The URL of the file in Google Cloud Storage.
-    """
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename_dest)
-    blob.upload_from_string(data_string)
-    return blob.public_url
-
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
     """ Generates a trace on _thread.
@@ -210,7 +166,8 @@ class ServerApp(object):
         if trace_metadata['succeeded']:
           print 'Uploading: %s' % remote_filename
           remote_trace_location = traces_dir + remote_filename
-          self._UploadFile(local_filename, remote_trace_location)
+          self._google_storage_accessor.UploadFile(local_filename,
+                                           remote_trace_location)
           full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
               remote_trace_location)
           trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
@@ -220,21 +177,25 @@ class ServerApp(object):
           self._failed_tasks.append({ "url": url, "repeat": repeat})
           self._tasks_lock.release()
           if os.path.isfile(local_filename):
-            self._UploadFile(local_filename, failures_dir + remote_filename)
+            self._google_storage_accessor.UploadFile(local_filename,
+                                            failures_dir + remote_filename)
         print 'Uploading log'
-        self._UploadFile(log_filename, logs_dir + remote_filename)
+        self._google_storage_accessor.UploadFile(log_filename,
+                                         logs_dir + remote_filename)
       # Pop once task is finished, for accurate status tracking.
       self._tasks_lock.acquire()
       url = self._tasks.pop()
       self._tasks_lock.release()
 
-    self._UploadString(json.dumps(trace_database.ToJsonDict(), indent=2),
-                       traces_dir + 'trace_database.json')
+    self._google_storage_accessor.UploadString(
+        json.dumps(trace_database.ToJsonDict(), indent=2),
+        traces_dir + 'trace_database.json')
 
     if len(self._failed_tasks) > 0:
       print 'Uploading failing URLs'
-      self._UploadString(json.dumps(self._failed_tasks, indent=2),
-                         failures_dir + 'failures.json')
+      self._google_storage_accessor.UploadString(
+          json.dumps(self._failed_tasks, indent=2),
+          failures_dir + 'failures.json')
 
   def _SetTaskList(self, http_body):
     """Sets the list of tasks and starts processing them

commit ac2ec40d93eb96e3f62610d657ed00c836135ec1
Author: johnme <johnme@chromium.org>
Date:   Fri Apr 15 11:17:44 2016 -0700

    Implement InstanceIDAndroid using InstanceIDWithSubtype.java
    
    Replaces the previous stub implementation with a fully functional
    implementation backed by InstanceIDWithSubtype.java.
    
    Part of a series of patches:
    1. https://codereview.chromium.org/1832833002 adds InstanceIDWithSubtype
    2. this patch
    3. https://codereview.chromium.org/1829023002 adds fake and test
    4. https://codereview.chromium.org/1854093002 enables InstanceID by default
    5. https://codereview.chromium.org/1851423003 switches Push to InstanceIDs
    
    BUG=589461
    
    Review URL: https://codereview.chromium.org/1830983002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387646}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2f8daf9d61b0bd6260dda858f852b258e03341f4

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 0495984..125f04e 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -216,6 +216,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="lib" path="out/Debug/lib.java/components/dom_distiller/android/dom_distiller_core_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/external_video_surface/java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/gcm_driver/android/gcm_driver_java.jar"/>
+    <classpathentry kind="lib" path="out/Debug/lib.java/components/gcm_driver/instance_id/android/instance_id_driver_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/invalidation/impl/java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/invalidation/impl/proto_java.jar"/>
     <classpathentry kind="lib" path="out/Debug/lib.java/components/navigation_interception/android/navigation_interception_java.jar"/>

commit b281445db2995a6e582f8e4bfb74c708c65ac18e
Author: mattcary <mattcary@chromium.org>
Date:   Fri Apr 15 08:37:01 2016 -0700

    Clovis: Update prefetch view to accept changed location of url field.
    
    Review URL: https://codereview.chromium.org/1890813004
    
    Cr-Original-Commit-Position: refs/heads/master@{#387605}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8cc4a1d8c3c03d52676ecb2407654e9b2c7c0fd3

diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 057ac0f..0df9025 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -154,7 +154,7 @@ class PrefetchSimulationView(object):
     for preload_step_event in preload_step_events:
       preload_event = resource_events.EventFromStep(preload_step_event)
       if preload_event:
-        preloaded_urls.add(preload_event.args['url'])
+        preloaded_urls.add(preload_event.args['data']['url'])
     parser_requests = cls.ParserDiscoverableRequests(
         request, dependencies_lens)
     preloaded_root_requests = filter(
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index be97809..1f85019 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -38,7 +38,7 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
         first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([first_request], preloaded_requests)
     self._SetUp(
-        [{'args': {'url': 'http://bla.com/nyancat.js'},
+        [{'args': {'data': {'url': 'http://bla.com/nyancat.js'}},
           'cat': 'blink.net', 'id': '0xaf9f14fa9dd6c314', 'name': 'Resource',
           'ph': 'X', 'ts': 1, 'dur': 120, 'pid': 12, 'tid': 12},
          {'args': {'step': 'Preload'}, 'cat': 'blink.net',
diff --git a/loading/testdata/scanner_vs_parser.trace b/loading/testdata/scanner_vs_parser.trace
index 427cdc8..627d141 100644
--- a/loading/testdata/scanner_vs_parser.trace
+++ b/loading/testdata/scanner_vs_parser.trace
@@ -55,8 +55,10 @@
     "events": [
       {
         "args": {
-          "priority": 4,
-          "url": "http://l/"
+          "data": {
+            "priority": 4,
+            "url": "http://l/"
+          }
         },
         "cat": "blink.net",
         "name": "Resource",
@@ -74,8 +76,10 @@
       },
       {
         "args": {
-          "priority": 1,
-          "url": "http://l/0.png"
+          "data": {
+            "priority": 1,
+            "url": "http://l/0.png"
+          }
         },
         "cat": "blink.net",
         "name": "Resource",
@@ -95,8 +99,10 @@
       },
       {
         "args": {
-          "priority": 1,
-          "url": "http://l/1.png"
+          "data": {
+            "priority": 1,
+            "url": "http://l/1.png"
+          }
         },
         "cat": "blink.net",
         "name": "Resource",

commit d0eb4e867e2ec64702b31af05b973ba72c4d66d0
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 15 08:32:25 2016 -0700

    tools/android/loading: Show cmd to resume sandwich in case of task failure.
    
    Sandwich is moving to the task_manager API. In case of a task failing,
    ExecuteWithCommandLine() was only helping by giving the command line
    to re-execute this task. This CL lets ExecuteWithCommandLine() also
    giving the command line argument to set in order to resume the initial
    command line, avoiding the re-executing previous tasks that went
    smoothly.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1878593002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387603}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 230c2bb243e9ce74981cc9cd7eafa092fabaa37d

diff --git a/loading/task_manager.py b/loading/task_manager.py
index 79a22fd..88a473d 100644
--- a/loading/task_manager.py
+++ b/loading/task_manager.py
@@ -216,6 +216,40 @@ def GenerateScenario(final_tasks, frozen_tasks):
   return scenario
 
 
+def ListResumingTasksToFreeze(scenario, final_tasks, failed_task):
+  """Lists the tasks that one needs to freeze to be able to resume the scenario
+  after failure.
+
+  Args:
+    scenario: The scenario (list of Task) to be resumed.
+    final_tasks: The list of final Task used to generate the scenario.
+    failed_task: A Task that have failed in the scenario.
+
+  Returns:
+    set(Task)
+  """
+  task_to_id = {t: i for i, t in enumerate(scenario)}
+  assert failed_task in task_to_id
+  frozen_tasks = set()
+  walked_tasks = set()
+
+  def InternalWalk(task):
+    if task.IsStatic() or task in walked_tasks:
+      return
+    walked_tasks.add(task)
+    if task not in task_to_id:
+      frozen_tasks.add(task)
+    elif task_to_id[task] < task_to_id[failed_task]:
+      frozen_tasks.add(task)
+    else:
+      for dependency in task._dependencies:
+        InternalWalk(dependency)
+
+  for final_task in final_tasks:
+    InternalWalk(final_task)
+  return frozen_tasks
+
+
 def OutputGraphViz(scenario, final_tasks, output):
   """Outputs the build dependency graph covered by this scenario.
 
@@ -285,6 +319,17 @@ def CommandLineParser():
   return parser
 
 
+def _GetCommandLineArgumentsStr(final_task_regexes, frozen_tasks):
+  arguments = []
+  if frozen_tasks:
+    arguments.append('-f')
+    arguments.extend([task.name for task in frozen_tasks])
+  if final_task_regexes:
+    arguments.append('-e')
+    arguments.extend(final_task_regexes)
+  return subprocess.list2cmdline(arguments)
+
+
 def ExecuteWithCommandLine(args, tasks, default_final_tasks):
   """Helper to execute tasks using command line arguments.
 
@@ -353,11 +398,11 @@ def ExecuteWithCommandLine(args, tasks, default_final_tasks):
         print '# Looks like something went wrong in \'{}\''.format(task.name)
         print '#'
         print '# To re-execute only this task, add the following parameters:'
-        suggested_flags = []
-        if task._dependencies:
-          suggested_flags.append('-f')
-          suggested_flags.extend([dep.name for dep in task._dependencies])
-        suggested_flags.extend(['-e', task.name])
-        print '#   ' + subprocess.list2cmdline(suggested_flags)
+        print '#   ' + _GetCommandLineArgumentsStr(
+            [task.name], task._dependencies)
+        print '#'
+        print '# To resume from this task, add the following parameters:'
+        print '#   ' + _GetCommandLineArgumentsStr(args.run_regexes,
+            ListResumingTasksToFreeze(scenario, final_tasks, task))
         raise
   return 0
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
index 2217aef..68ae1ce 100644
--- a/loading/task_manager_unittest.py
+++ b/loading/task_manager_unittest.py
@@ -2,10 +2,12 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import contextlib
 import os
 import re
 import shutil
 import StringIO
+import sys
 import tempfile
 import unittest
 
@@ -27,6 +29,26 @@ _GOLDEN_GRAPHVIZ = """digraph graphname {
 }\n"""
 
 
+@contextlib.contextmanager
+def EatStdoutAndStderr():
+  """Overrides sys.std{out,err} to intercept write calls."""
+  sys.stdout.flush()
+  sys.stderr.flush()
+  original_stdout = sys.stdout
+  original_stderr = sys.stderr
+  try:
+    sys.stdout = StringIO.StringIO()
+    sys.stderr = StringIO.StringIO()
+    yield
+  finally:
+    sys.stdout = original_stdout
+    sys.stderr = original_stderr
+
+
+class TestException(Exception):
+  pass
+
+
 class TaskManagerTestCase(unittest.TestCase):
   def setUp(self):
     self.output_directory = tempfile.mkdtemp()
@@ -244,6 +266,52 @@ class GenerateScenarioTest(TaskManagerTestCase):
     task_manager.OutputGraphViz(scenario, [TaskF], output)
     self.assertEqual(_GOLDEN_GRAPHVIZ, output.getvalue())
 
+  def testListResumingTasksToFreeze(self):
+    TaskManagerTestCase.setUp(self)
+    builder = task_manager.Builder(self.output_directory)
+    static_task = builder.CreateStaticTask('static', __file__)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[static_task])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskA, TaskB])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskA])
+    def TaskD():
+      pass
+    @builder.RegisterTask('e', dependencies=[TaskC])
+    def TaskE():
+      pass
+    @builder.RegisterTask('f', dependencies=[TaskC])
+    def TaskF():
+      pass
+
+    for k in 'abcdef':
+      self.TouchOutputFile(k)
+
+    def RunSubTest(final_tasks, initial_frozen_tasks, failed_task, reference):
+      scenario = \
+          task_manager.GenerateScenario(final_tasks, initial_frozen_tasks)
+      resume_frozen_tasks = task_manager.ListResumingTasksToFreeze(
+          scenario, final_tasks, failed_task)
+      self.assertEqual(reference, resume_frozen_tasks)
+
+      failed_pos = scenario.index(failed_task)
+      new_scenario = \
+          task_manager.GenerateScenario(final_tasks, resume_frozen_tasks)
+      self.assertEqual(scenario[failed_pos:], new_scenario)
+
+    RunSubTest([TaskA], set([]), TaskA, set([]))
+    RunSubTest([TaskD], set([]), TaskA, set([]))
+    RunSubTest([TaskD], set([]), TaskD, set([TaskA]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskB, set([TaskA]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskC, set([TaskA, TaskB]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskE, set([TaskC]))
+    RunSubTest([TaskE, TaskF], set([TaskA]), TaskF, set([TaskC, TaskE]))
+
 
 class CommandLineControlledExecutionTest(TaskManagerTestCase):
   def Execute(self, *command_line_args):
@@ -265,15 +333,16 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
       pass
     @builder.RegisterTask('raise_exception', dependencies=[TaskB])
     def TaskF():
-      raise Exception('Expected error.')
+      raise TestException('Expected error.')
 
     default_final_tasks = [TaskD, TaskE]
     parser = task_manager.CommandLineParser()
     cmd = ['-o', self.output_directory]
     cmd.extend([i for i in command_line_args])
     args = parser.parse_args(cmd)
-    return task_manager.ExecuteWithCommandLine(
-        args, [TaskA, TaskB, TaskC, TaskD, TaskE, TaskF], default_final_tasks)
+    with EatStdoutAndStderr():
+      return task_manager.ExecuteWithCommandLine(
+          args, [TaskA, TaskB, TaskC, TaskD, TaskE, TaskF], default_final_tasks)
 
   def testSimple(self):
     self.assertEqual(0, self.Execute())
@@ -291,7 +360,7 @@ class CommandLineControlledExecutionTest(TaskManagerTestCase):
     self.assertEqual(0, self.Execute('-f', 'c'))
 
   def testTaskFailure(self):
-    with self.assertRaisesRegexp(Exception, r'^Expected error\.$'):
+    with self.assertRaisesRegexp(TestException, r'^Expected error\.$'):
       self.Execute('-e', 'raise_exception')
 
 

commit f83730cefa69b0731afcc6ba2813b9e700e33f36
Author: mattcary <mattcary@chromium.org>
Date:   Fri Apr 15 06:57:47 2016 -0700

    Clovis: unittesting fix and some general tweaks.
    
    The root problem was that dependency_graph_unittest called
    request_dependencies_lens.TestReqeusts.CreateLoadingTrace, and then changed some
    of the timing of those requests. As those requests were global, that meant that
    if a different unittest (eg, prefetch_view_unittest) happened to use the same
    TestRequests, it would see the changed timing, and so run differently than if it
    were run in isolation.
    
    A nice way to fix that is to serialize and deserialize the trace, which exposed
    some other holes in our organization, namely abstract classes that defined empty
    methods instead of abstract methods (in the python world, that means using
    "pass" instead of "raise NotImplementedError") and then some other gaps in our
    serialization methods.
    
    In order to diagnose & fix this, run_tests was extended to allow for multiple
    tests to be specified. This does not change existing behavior when a single
    argument is passed to run_tests, and does the right thing for multiple arguments
    instead of silently ignoring them.
    
    Review URL: https://codereview.chromium.org/1892073002
    
    Cr-Original-Commit-Position: refs/heads/master@{#387590}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 046e2f3a95ac7bb7afd3b16bb18007c2bf71fd8e

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index ab78f8e..aade097 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -390,14 +390,14 @@ class Listener(object):
       event_name: (str) Event name, as registered.
       event: (dict) complete event.
     """
-    pass
+    raise NotImplementedError
 
 
 class Track(Listener):
   """Collects data from a DevTools server."""
   def GetEvents(self):
     """Returns a list of collected events, finalizing the state if necessary."""
-    pass
+    raise NotImplementedError
 
   def ToJsonDict(self):
     """Serializes to a dictionary, to be dumped as JSON.
@@ -406,10 +406,10 @@ class Track(Listener):
       A dict that can be dumped by the json module, and loaded by
       FromJsonDict().
     """
-    pass
+    raise NotImplementedError
 
   @classmethod
-  def FromJsonDict(cls, json_dict):
+  def FromJsonDict(cls, _json_dict):
     """Returns a Track instance constructed from data dumped by
        Track.ToJsonDict().
 
@@ -419,4 +419,8 @@ class Track(Listener):
     Returns:
       a Track instance.
     """
-    pass
+    # There is no sensible way to deserialize this abstract class, but
+    # subclasses are not required to define a deserialization method. For
+    # example, for testing we have a FakeRequestTrack which is never
+    # deserialized; instead fake instances are deserialized as RequestTracks.
+    assert False
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 7630bcc..ff8b8c5 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -42,7 +42,8 @@ class LoadingTrace(object):
     result = {self._URL_KEY: self.url, self._METADATA_KEY: self.metadata,
               self._PAGE_KEY: self.page_track.ToJsonDict(),
               self._REQUEST_KEY: self.request_track.ToJsonDict(),
-              self._TRACING_KEY: self.tracing_track.ToJsonDict()}
+              self._TRACING_KEY: (self.tracing_track.ToJsonDict()
+                                  if self.tracing_track else None)}
     return result
 
   def ToJsonFile(self, json_path):
@@ -111,7 +112,8 @@ class LoadingTrace(object):
     self._tracing_track = None
 
   def _RestoreTracingTrack(self):
-    assert self._tracing_json_str
+    if not self._tracing_json_str:
+      return None
     self._tracing_track = tracing.TracingTrack.FromJsonDict(
         json.loads(self._tracing_json_str))
     self._tracing_json_str = None
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 773af44..08e9c17 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -72,11 +72,14 @@ class TestRequests(object):
 
   @classmethod
   def CreateLoadingTrace(cls, trace_events=None):
-    return test_utils.LoadingTraceFromEvents(
+    trace = test_utils.LoadingTraceFromEvents(
         [cls.FIRST_REDIRECT_REQUEST, cls.SECOND_REDIRECT_REQUEST,
          cls.REDIRECTED_REQUEST, cls.REQUEST, cls.JS_REQUEST, cls.JS_REQUEST_2,
          cls.JS_REQUEST_OTHER_FRAME, cls.JS_REQUEST_UNRELATED_FRAME],
         cls.PAGE_EVENTS, trace_events)
+    # Serialize and deserialize so that clients can change events without
+    # affecting future tests.
+    return LoadingTrace.FromJsonDict(trace.ToJsonDict())
 
 
 class RequestDependencyLensTestCase(unittest.TestCase):
diff --git a/loading/run_tests b/loading/run_tests
index 7f182a7..1f04f05 100755
--- a/loading/run_tests
+++ b/loading/run_tests
@@ -16,9 +16,15 @@ if __name__ == '__main__':
 
   suite = unittest.TestSuite()
   loader = unittest.TestLoader()
-  pattern = '*%s*_unittest.py' % ('' if len(sys.argv) < 2 else sys.argv[1])
   root_dir = os.path.dirname(os.path.realpath(__file__))
-  suite.addTests(loader.discover(start_dir=root_dir, pattern=pattern))
+  if len(sys.argv) < 2:
+    cases = loader.discover(start_dir=root_dir, pattern='*_unittest.py')
+  else:
+    cases = []
+    for module in sys.argv[1:]:
+      pattern = '{}_unittest.py'.format(module)
+      cases.extend(loader.discover(start_dir=root_dir, pattern=pattern))
+  suite.addTests(cases)
   res = unittest.TextTestRunner(verbosity=2).run(suite)
   if res.wasSuccessful():
     sys.exit(0)
diff --git a/loading/test_utils.py b/loading/test_utils.py
index a0b5b16..8ae919f 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -18,9 +18,20 @@ class FakeRequestTrack(devtools_monitor.Track):
     super(FakeRequestTrack, self).__init__(None)
     self._events = [self._RewriteEvent(e) for e in events]
 
+  def Handle(self, _method, _msg):
+    assert False  # Should never be called.
+
   def GetEvents(self):
     return self._events
 
+  def ToJsonDict(self):
+    cls = request_track.RequestTrack
+    return {cls._EVENTS_KEY: [
+        rq.ToJsonDict() for rq in self.GetEvents()],
+            cls._METADATA_KEY: {
+                cls._DUPLICATES_KEY: 0,
+                cls._INCONSISTENT_INITIATORS_KEY: 0}}
+
   def _RewriteEvent(self, event):
     # This modifies the instance used across tests, so this method
     # must be idempotent.
@@ -33,6 +44,9 @@ class FakePageTrack(devtools_monitor.Track):
     super(FakePageTrack, self).__init__(None)
     self._events = events
 
+  def Handle(self, _method, _msg):
+    assert False  # Should never be called.
+
   def GetEvents(self):
     return self._events
 
@@ -42,6 +56,9 @@ class FakePageTrack(devtools_monitor.Track):
     assert event['method'] == page_track.PageTrack.FRAME_STARTED_LOADING
     return event['frame_id']
 
+  def ToJsonDict(self):
+    return {'events': [event for event in self._events]}
+
 
 def MakeRequestWithTiming(
     url, source_url, timing_dict, magic_content_type=False,
diff --git a/loading/tracing.py b/loading/tracing.py
index fb54d70..c8db1df 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -115,6 +115,8 @@ class TracingTrack(devtools_monitor.Track):
 
   @classmethod
   def FromJsonDict(cls, json_dict):
+    if not json_dict:
+      return None
     assert 'events' in json_dict
     events = [Event(e) for e in json_dict['events']]
     tracing_track = TracingTrack(None)

commit c75ed61aa96c817351cee960123030ac0c713a6c
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 14 08:58:57 2016 -0700

    sandwich: Remove some non necessary commas in sandwich_misc.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1890883003
    
    Cr-Original-Commit-Position: refs/heads/master@{#387329}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f3cb0fefb2586a52be3e562378a6d93b128d636b

diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index 13988c3..e7b00c6 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -15,10 +15,10 @@ import wpr_backend
 REDIRECTED_MAIN_DISCOVERER = 'redirected-main'
 
 # All resources which are fetched from the main document and their redirections.
-PARSER_DISCOVERER = 'parser',
+PARSER_DISCOVERER = 'parser'
 
 # Simulation of HTMLPreloadScanner on the main document and their redirections.
-HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner',
+HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner'
 
 SUBRESOURCE_DISCOVERERS = set([
   REDIRECTED_MAIN_DISCOVERER,

commit 89bf0a5ede47303dcea43e76f44037e772f80be1
Author: agrieve <agrieve@chromium.org>
Date:   Tue Apr 12 17:05:39 2016 -0700

    Fix GN deps needed by third_party/WebKit/Tools/Scripts/run-webkit-tests
    
    BUG=587083
    TBR=mkwst
    
    Review URL: https://codereview.chromium.org/1882533004
    
    Cr-Original-Commit-Position: refs/heads/master@{#386870}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 993374cf1ac90ef6fbb188017ea2fb445cf0eb09

diff --git a/forwarder/BUILD.gn b/forwarder/BUILD.gn
new file mode 100644
index 0000000..9cc99b4
--- /dev/null
+++ b/forwarder/BUILD.gn
@@ -0,0 +1,24 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import("//build/symlink.gni")
+
+if (current_toolchain == host_toolchain) {
+  # GYP: //tools/android/forwarder/forwarder.gyp:forwarder
+  executable("forwarder") {
+    sources = [
+      "forwarder.cc",
+    ]
+    deps = [
+      "//base",
+      "//build/config/sanitizers:deps",
+      "//tools/android/common",
+    ]
+  }
+} else {
+  # Create a symlink from root_build_dir -> clang_x64/forwarder.
+  binary_symlink("forwarder") {
+    binary_label = ":$target_name($host_toolchain)"
+  }
+}
diff --git a/forwarder2/BUILD.gn b/forwarder2/BUILD.gn
index 6899a7e..d2af725 100644
--- a/forwarder2/BUILD.gn
+++ b/forwarder2/BUILD.gn
@@ -93,6 +93,6 @@ if (current_toolchain != default_toolchain) {
 } else {
   # Create a symlink from root_build_dir -> clang_x64/host_forwarder.
   binary_symlink("host_forwarder") {
-    binary_label = ":host_forwarder($host_toolchain)"
+    binary_label = ":$target_name($host_toolchain)"
   }
 }
diff --git a/md5sum/BUILD.gn b/md5sum/BUILD.gn
index 7cce4c9..35c386b 100644
--- a/md5sum/BUILD.gn
+++ b/md5sum/BUILD.gn
@@ -46,5 +46,6 @@ if (current_toolchain == default_toolchain) {
   # GYP: //tools/android/md5sum/md5sum.gyp:md5sum_bin_host
   binary_symlink("md5sum_bin_host") {
     binary_label = ":md5sum_bin($host_toolchain)"
+    output_name = "md5sum_bin_host"
   }
 }

commit fd1eca3c26d6f0e01d45f442468b61a5e6580161
Author: kjellander <kjellander@chromium.org>
Date:   Tue Apr 12 03:58:25 2016 -0700

    Revert of [Devil] Replace generated Devil config with jinja template. (patchset #8 id:140001 of https://codereview.chromium.org/1812383003/ )
    
    Reason for revert:
    I believe this change breaks content_browsertests on every Android tester in chromium.android.
    
    Examples:
    
    https://build.chromium.org/p/chromium.android/builders/Lollipop%20Phone%20Tester/builds/4031
    https://build.chromium.org/p/chromium.android/builders/Marshmallow%2064%20bit%20Tester/builds/1457
    https://build.chromium.org/p/chromium.android/builders/KitKat%20Tablet%20Tester/builds/3759
    
    Original issue's description:
    > [Devil] Replace generated Devil config with jinja template.
    >
    > This change will hopefully allow us to configure devil more based
    > on the build config. For example, it will let us use the same
    > android_sdk_tools to run tests that we use to build with.
    >
    > BUG=
    >
    > Committed: https://crrev.com/c2aa4243a9e00a2d0e254d9f6f73b41c00cb644d
    > Cr-Commit-Position: refs/heads/master@{#386461}
    
    TBR=droger@chromium.org,jbudorick@chromium.org,mikecase@chromium.org
    # Skipping CQ checks because original CL landed less than 1 days ago.
    NOPRESUBMIT=true
    NOTREECHECKS=true
    NOTRY=true
    BUG=
    
    Review URL: https://codereview.chromium.org/1885503002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386648}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d408713145a9b75e155e1bf106105803ad40b1bd

diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 80f0f29..2362414 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -34,9 +34,8 @@ cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
 mkdir -p $tmp_src_dir/build/android
 cp build/android/devil_chromium.py $tmp_src_dir/build/android/
 cp build/android/video_recorder.py $tmp_src_dir/build/android/
+cp build/android/devil_chromium.json $tmp_src_dir/build/android/
 cp -r build/android/pylib $tmp_src_dir/build/android/
-mkdir -p $tmp_src_dir/$builddir/gen/
-cp $builddir/gen/devil_chromium.json $tmp_src_dir/$builddir/gen/
 mkdir -p \
   $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
 cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \

commit 85d20d0922c72586e8ffc03495dbb9e9ff694d87
Author: mikecase <mikecase@chromium.org>
Date:   Mon Apr 11 13:26:41 2016 -0700

    [Devil] Replace generated Devil config with jinja template.
    
    This change will hopefully allow us to configure devil more based
    on the build config. For example, it will let us use the same
    android_sdk_tools to run tests that we use to build with.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1812383003
    
    Cr-Original-Commit-Position: refs/heads/master@{#386461}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c2aa4243a9e00a2d0e254d9f6f73b41c00cb644d

diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 2362414..80f0f29 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -34,8 +34,9 @@ cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
 mkdir -p $tmp_src_dir/build/android
 cp build/android/devil_chromium.py $tmp_src_dir/build/android/
 cp build/android/video_recorder.py $tmp_src_dir/build/android/
-cp build/android/devil_chromium.json $tmp_src_dir/build/android/
 cp -r build/android/pylib $tmp_src_dir/build/android/
+mkdir -p $tmp_src_dir/$builddir/gen/
+cp $builddir/gen/devil_chromium.json $tmp_src_dir/$builddir/gen/
 mkdir -p \
   $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
 cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \

commit d40f7000570b3ee63a5a19eb99058558046d134d
Author: gabadie <gabadie@chromium.org>
Date:   Mon Apr 11 02:08:28 2016 -0700

    tools/android/loading: Implements task_manager.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1869703002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386353}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c0fa3be7a5d809a3a0291cd97a3169fd028ff701

diff --git a/loading/common_util.py b/loading/common_util.py
index ca06da2..855284f 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -3,12 +3,25 @@
 # found in the LICENSE file.
 
 import contextlib
+import json
 import logging
+import os
+import re
 import shutil
+import sys
 import tempfile
 import time
 
 
+def VerboseCompileRegexOrAbort(regex):
+  """Compiles a user-provided regular expression, exits the program on error."""
+  try:
+    return re.compile(regex)
+  except re.error as e:
+    sys.stderr.write('invalid regex: {}\n{}\n'.format(regex, e))
+    sys.exit(2)
+
+
 def PollFor(condition, condition_name, interval=5):
   """Polls for a function to return true.
 
diff --git a/loading/task_manager.py b/loading/task_manager.py
new file mode 100644
index 0000000..79a22fd
--- /dev/null
+++ b/loading/task_manager.py
@@ -0,0 +1,363 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""API that build and execute recipes wrapped into a task dependency graph.
+
+A Task consists of a 'recipe' (a closure to be executed) and a list of refs to
+tasks that should be executed prior to executing this Task (i.e. dependencies).
+
+A Task can be either 'static' or 'dynamic'. A static tasks only represents an
+existing file on the filesystem, its recipe is a no-op. The responsibility of
+the recipe of a dynamic task is to produce the file with the name assigned at
+task creation.
+
+A scenario is a ordered list of dynamic tasks to execute such that the
+dependencies of a given task are execute before the said task. The scenario is
+built from a list of final tasks and a list of frozen tasks:
+  - A final task is a task to execute ultimately. Therefore the scenario is
+    composed of final tasks and their required intermediary tasks.
+  - A frozen task is dynamic task to not execute. This is a mechanism to morph a
+    dynamic task that may have dependencies to a static task with no dependency
+    at scenario generation time, injecting what the dynamic task have already
+    produced before as an input of the smaller tasks dependency graph covered
+    by the scenario.
+
+Example:
+  # -------------------------------------------------- Build my dependency graph
+  builder = Builder('my/output/dir')
+  input0 = builder.CreateStaticTask('input0', 'path/to/input/file0')
+  input1 = builder.CreateStaticTask('input1', 'path/to/input/file1')
+  input2 = builder.CreateStaticTask('input2', 'path/to/input/file2')
+  input3 = builder.CreateStaticTask('input3', 'path/to/input/file3')
+
+  @builder.RegisterTask('out0', dependencies=[input0, input2])
+  def BuildOut0():
+    DoStuff(input0.path, input2.path, out=BuildOut0.path)
+
+  @builder.RegisterTask('out1', dependencies=[input1, input3])
+  def BuildOut1():
+    DoStuff(input1.path, input3.path, out=BuildOut1.path)
+
+  @builder.RegisterTask('out2', dependencies=[BuildOut0, BuildOut1])
+  def BuildOut2():
+    DoStuff(BuildOut0.path, BuildOut1.path, out=BuildOut2.path)
+
+  @builder.RegisterTask('out3', dependencies=[BuildOut0])
+  def BuildOut3():
+    DoStuff(BuildOut0.path, out=BuildOut3.path)
+
+  # ---------------------------- Case 1: Execute BuildOut3 and its dependencies.
+  for task in GenerateScenario(final_tasks=[BuildOut3], frozen_tasks=[])
+    task.Execute()
+
+  # ---------- Case 2: Execute BuildOut2 and its dependencies but not BuildOut1.
+  # It is required that BuildOut1.path is already existing.
+  for task in GenerateScenario(final_tasks=[BuildOut2],
+                               frozen_tasks=[BuildOut1])
+    task.Execute()
+"""
+
+
+import argparse
+import logging
+import os
+import subprocess
+import sys
+
+import common_util
+
+
+_TASK_GRAPH_DOTFILE_NAME = 'tasks_graph.dot'
+_TASK_GRAPH_PNG_NAME = 'tasks_graph.png'
+
+
+class TaskError(Exception):
+  pass
+
+
+class Task(object):
+  """Task that can be either a static task or dynamic with a recipe."""
+
+  def __init__(self, name, path, dependencies, recipe):
+    """Constructor.
+
+    Args:
+      name: The name of the  task.
+      path: Path to the file or directory that this task produces.
+      dependencies: List of parent task to execute before.
+      recipe: Function to execute if a dynamic task or None if a static task.
+    """
+    self.name = name
+    self.path = path
+    self._dependencies = dependencies
+    self._recipe = recipe
+    self._is_done = recipe == None
+
+  def Execute(self):
+    """Executes this task."""
+    if self.IsStatic():
+      raise TaskError('Task {} is static.'.format(self.name))
+    if not self._is_done:
+      self._recipe()
+    self._is_done = True
+
+  def IsStatic(self):
+    """Returns whether this task is a static task."""
+    return self._recipe == None
+
+
+class Builder(object):
+  """Utilities for creating sub-graphs of tasks with dependencies."""
+
+  def __init__(self, output_directory):
+    """Constructor.
+
+    Args:
+      output_directory: Output directory where the dynamic tasks work.
+    """
+    self.output_directory = output_directory
+    self.tasks = {}
+
+  def CreateStaticTask(self, task_name, path):
+    if not os.path.exists(path):
+      raise TaskError('Error while creating task {}: File not found: {}'.format(
+          task_name, path))
+    if task_name in self.tasks:
+      raise TaskError('Task {} already exists.'.format(task_name))
+    task = Task(task_name, path, [], None)
+    self.tasks[task_name] = task
+    return task
+
+  # Caution:
+  #   This decorator may not create a dynamic task in the case where
+  #   merge=True and another dynamic target having the same name have already
+  #   been created. In this case, it will just reuse the former task. This is at
+  #   the user responsibility to ensure that merged tasks would do the exact
+  #   same thing.
+  #
+  #     @builder.RegisterTask('hello')
+  #     def TaskA():
+  #       my_object.a = 1
+  #
+  #     @builder.RegisterTask('hello', merge=True)
+  #     def TaskB():
+  #       # This function won't be executed ever.
+  #       my_object.a = 2 # <------- Wrong because different from what TaskA do.
+  #
+  #     assert TaskA == TaskB
+  #     TaskB.Execute() # Sets set my_object.a == 1
+  def RegisterTask(self, task_name, dependencies=None, merge=False):
+    """Decorator that wraps a function into a dynamic task.
+
+    Args:
+      task_name: The name of this new task to register.
+      dependencies: List of SandwichTarget to build before this task.
+      merge: If a task already have this name, don't create a new one and
+        reuse the existing one.
+
+    Returns:
+      A Task that was created by wrapping the function or an existing registered
+      wrapper (that have wrapped a different function).
+    """
+    dependencies = dependencies or []
+    def InnerAddTaskWithNewPath(recipe):
+      if task_name in self.tasks:
+        if not merge:
+          raise TaskError('Task {} already exists.'.format(task_name))
+        task = self.tasks[task_name]
+        if task.IsStatic():
+          raise TaskError('Should not merge dynamic task {} with the already '
+                          'existing static one.'.format(task_name))
+        return task
+      task_path = os.path.join(self.output_directory, task_name)
+      task = Task(task_name, task_path, dependencies, recipe)
+      self.tasks[task_name] = task
+      return task
+    return InnerAddTaskWithNewPath
+
+
+def GenerateScenario(final_tasks, frozen_tasks):
+  """Generates a list of tasks to execute in order of dependencies-first.
+
+  Args:
+    final_tasks: The final tasks to generate the scenario from.
+    frozen_tasks: Sets of task to freeze.
+
+  Returns:
+    [Task]
+  """
+  scenario = []
+  task_paths = {}
+  def InternalAppendTarget(task):
+    if task.IsStatic():
+      return
+    if task in frozen_tasks:
+      if not os.path.exists(task.path):
+        raise TaskError('Frozen target `{}`\'s path doesn\'t exist.'.format(
+            task.name))
+      return
+    if task.path in task_paths:
+      if task_paths[task.path] == None:
+        raise TaskError('Target `{}` depends on itself.'.format(task.name))
+      if task_paths[task.path] != task:
+        raise TaskError(
+            'Tasks `{}` and `{}` produce the same file: `{}`.'.format(
+                task.name, task_paths[task.path].name, task.path))
+      return
+    task_paths[task.path] = None
+    for dependency in task._dependencies:
+      InternalAppendTarget(dependency)
+    task_paths[task.path] = task
+    scenario.append(task)
+
+  for final_task in final_tasks:
+    InternalAppendTarget(final_task)
+  return scenario
+
+
+def OutputGraphViz(scenario, final_tasks, output):
+  """Outputs the build dependency graph covered by this scenario.
+
+  Args:
+    scenario: The generated scenario.
+    final_tasks: The final tasks used to generate the scenario.
+    output: A file-like output stream to receive the dot file.
+
+  Graph interpretations:
+    - Static tasks are shape less.
+    - Final tasks (the one that where directly appended) are box shaped.
+    - Non final dynamic tasks are ellipse shaped.
+    - Frozen dynamic tasks have a blue shape.
+  """
+  task_execution_ids = {t: i for i, t in enumerate(scenario)}
+  tasks_node_ids = dict()
+
+  def GetTaskNodeId(task):
+    if task in tasks_node_ids:
+      return tasks_node_ids[task]
+    node_id = len(tasks_node_ids)
+    node_label = task.name
+    node_color = 'blue'
+    node_shape = 'ellipse'
+    if task.IsStatic():
+      node_shape = 'plaintext'
+    elif task in task_execution_ids:
+      node_color = 'black'
+      node_label = str(task_execution_ids[task]) + ': ' + node_label
+    if task in final_tasks:
+      node_shape = 'box'
+    output.write('  n{} [label="{}", color={}, shape={}];\n'.format(
+        node_id, node_label, node_color, node_shape))
+    tasks_node_ids[task] = node_id
+    return node_id
+
+  output.write('digraph graphname {\n')
+  for task in scenario:
+    task_node_id = GetTaskNodeId(task)
+    for dep in task._dependencies:
+      dep_node_id = GetTaskNodeId(dep)
+      output.write('  n{} -> n{};\n'.format(dep_node_id, task_node_id))
+  output.write('}\n')
+
+
+def CommandLineParser():
+  """Creates command line arguments parser meant to be used as a parent parser
+  for any entry point that use the ExecuteWithCommandLine() function.
+
+  Returns:
+    The command line arguments parser.
+  """
+  parser = argparse.ArgumentParser(add_help=False)
+  parser.add_argument('-d', '--dry-run', action='store_true',
+                      help='Only prints the deps of tasks to build.')
+  parser.add_argument('-e', '--to-execute', metavar='REGEX', type=str,
+                      nargs='+', dest='run_regexes', default=[],
+                      help='Regex selecting tasks to execute.')
+  parser.add_argument('-f', '--to-freeze', metavar='REGEX', type=str,
+                      nargs='+', dest='frozen_regexes', default=[],
+                      help='Regex selecting tasks to not execute.')
+  parser.add_argument('-o', '--output', type=str, required=True,
+                      help='Path of the output directory.')
+  parser.add_argument('-v', '--output-graphviz', action='store_true',
+      help='Outputs the {} and {} file in the output directory.'
+           ''.format(_TASK_GRAPH_DOTFILE_NAME, _TASK_GRAPH_PNG_NAME))
+  return parser
+
+
+def ExecuteWithCommandLine(args, tasks, default_final_tasks):
+  """Helper to execute tasks using command line arguments.
+
+  Args:
+    args: Command line argument parsed with CommandLineParser().
+    tasks: Unordered list of tasks to publish to command line regexes.
+    default_final_tasks: Default final tasks if there is no -r command
+      line arguments.
+
+  Returns:
+    0 if success or 1 otherwise
+  """
+  frozen_regexes = [common_util.VerboseCompileRegexOrAbort(e)
+                      for e in args.frozen_regexes]
+  run_regexes = [common_util.VerboseCompileRegexOrAbort(e)
+                   for e in args.run_regexes]
+
+  # Lists frozen tasks
+  frozen_tasks = set()
+  if frozen_regexes:
+    for task in tasks:
+      for regex in frozen_regexes:
+        if regex.search(task.name):
+          frozen_tasks.add(task)
+          break
+
+  # Lists final tasks.
+  final_tasks = default_final_tasks
+  if run_regexes:
+    final_tasks = []
+    for task in tasks:
+      for regex in run_regexes:
+        if regex.search(task.name):
+          final_tasks.append(task)
+          break
+
+  # Create the scenario.
+  scenario = GenerateScenario(final_tasks, frozen_tasks)
+
+  if len(scenario) == 0:
+    logging.error('No tasks to build.')
+    return 1
+
+  if not os.path.isdir(args.output):
+    os.makedirs(args.output)
+
+  # Print the task dependency graph visualization.
+  if args.output_graphviz:
+    graphviz_path = os.path.join(args.output, _TASK_GRAPH_DOTFILE_NAME)
+    png_graph_path = os.path.join(args.output, _TASK_GRAPH_PNG_NAME)
+    with open(graphviz_path, 'w') as output:
+      OutputGraphViz(scenario, final_tasks, output)
+    subprocess.check_call(['dot', '-Tpng', graphviz_path, '-o', png_graph_path])
+
+  # Use the build scenario.
+  if args.dry_run:
+    for task in scenario:
+      print '{}:{}'.format(
+          task.name, ' '.join([' \\\n  ' + d.name for d in task._dependencies]))
+  else:
+    for task in scenario:
+      logging.info('%s %s' % ('-' * 60, task.name))
+      try:
+        task.Execute()
+      except:
+        print '# Looks like something went wrong in \'{}\''.format(task.name)
+        print '#'
+        print '# To re-execute only this task, add the following parameters:'
+        suggested_flags = []
+        if task._dependencies:
+          suggested_flags.append('-f')
+          suggested_flags.extend([dep.name for dep in task._dependencies])
+        suggested_flags.extend(['-e', task.name])
+        print '#   ' + subprocess.list2cmdline(suggested_flags)
+        raise
+  return 0
diff --git a/loading/task_manager_unittest.py b/loading/task_manager_unittest.py
new file mode 100644
index 0000000..2217aef
--- /dev/null
+++ b/loading/task_manager_unittest.py
@@ -0,0 +1,299 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import re
+import shutil
+import StringIO
+import tempfile
+import unittest
+
+import task_manager
+
+
+_GOLDEN_GRAPHVIZ = """digraph graphname {
+  n0 [label="0: b", color=black, shape=ellipse];
+  n1 [label="1: c", color=black, shape=ellipse];
+  n0 -> n1;
+  n2 [label="a", color=blue, shape=plaintext];
+  n2 -> n1;
+  n3 [label="2: d", color=black, shape=ellipse];
+  n1 -> n3;
+  n4 [label="3: f", color=black, shape=box];
+  n3 -> n4;
+  n5 [label="e", color=blue, shape=ellipse];
+  n5 -> n4;
+}\n"""
+
+
+class TaskManagerTestCase(unittest.TestCase):
+  def setUp(self):
+    self.output_directory = tempfile.mkdtemp()
+
+  def tearDown(self):
+    shutil.rmtree(self.output_directory)
+
+  def TouchOutputFile(self, file_path):
+    with open(os.path.join(self.output_directory, file_path), 'w') as output:
+      output.write(file_path + '\n')
+
+
+class TaskTest(TaskManagerTestCase):
+  def testStaticTask(self):
+    task = task_manager.Task('hello.json', 'what/ever/hello.json', [], None)
+    self.assertTrue(task.IsStatic())
+    self.assertTrue(task._is_done)
+    with self.assertRaises(task_manager.TaskError):
+      task.Execute()
+
+  def testDynamicTask(self):
+    def Recipe():
+      Recipe.counter += 1
+    Recipe.counter = 0
+    task = task_manager.Task('hello.json', 'what/ever/hello.json', [], Recipe)
+    self.assertFalse(task.IsStatic())
+    self.assertFalse(task._is_done)
+    self.assertEqual(0, Recipe.counter)
+    task.Execute()
+    self.assertEqual(1, Recipe.counter)
+    task.Execute()
+    self.assertEqual(1, Recipe.counter)
+
+  def testDynamicTaskWithUnexecutedDeps(self):
+    def RecipeA():
+      self.fail()
+
+    def RecipeB():
+      RecipeB.counter += 1
+    RecipeB.counter = 0
+
+    a = task_manager.Task('hello.json', 'out/hello.json', [], RecipeA)
+    b = task_manager.Task('hello.json', 'out/hello.json', [a], RecipeB)
+    self.assertEqual(0, RecipeB.counter)
+    b.Execute()
+    self.assertEqual(1, RecipeB.counter)
+
+
+class BuilderTest(TaskManagerTestCase):
+  def testCreateUnexistingStaticTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    with self.assertRaises(task_manager.TaskError):
+      builder.CreateStaticTask('hello.txt', '/__unexisting/file/path')
+
+  def testCreateStaticTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    task = builder.CreateStaticTask('hello.py', __file__)
+    self.assertTrue(task.IsStatic())
+
+  def testDuplicateStaticTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    builder.CreateStaticTask('hello.py', __file__)
+    with self.assertRaises(task_manager.TaskError):
+      builder.CreateStaticTask('hello.py', __file__)
+
+  def testRegisterTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('hello.txt')
+    def TaskA():
+      TaskA.executed = True
+    TaskA.executed = False
+    self.assertFalse(TaskA.IsStatic())
+    self.assertEqual(os.path.join(self.output_directory, 'hello.txt'),
+                     TaskA.path)
+    self.assertFalse(TaskA.executed)
+    TaskA.Execute()
+    self.assertTrue(TaskA.executed)
+
+  def testRegisterDuplicateTask(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('hello.txt')
+    def TaskA():
+      pass
+    del TaskA # unused
+    with self.assertRaises(task_manager.TaskError):
+      @builder.RegisterTask('hello.txt')
+      def TaskB():
+        pass
+      del TaskB # unused
+
+  def testTaskMerging(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('hello.txt')
+    def TaskA():
+      pass
+    @builder.RegisterTask('hello.txt', merge=True)
+    def TaskB():
+      pass
+    self.assertEqual(TaskA, TaskB)
+
+  def testStaticTaskMergingError(self):
+    builder = task_manager.Builder(self.output_directory)
+    builder.CreateStaticTask('hello.py', __file__)
+    with self.assertRaises(task_manager.TaskError):
+      @builder.RegisterTask('hello.py', merge=True)
+      def TaskA():
+        pass
+      del TaskA # unused
+
+
+class GenerateScenarioTest(TaskManagerTestCase):
+  def testParents(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[TaskA])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskB])
+    def TaskC():
+      pass
+    scenario = task_manager.GenerateScenario([TaskA, TaskB, TaskC], set())
+    self.assertListEqual([TaskA, TaskB, TaskC], scenario)
+
+    scenario = task_manager.GenerateScenario([TaskB], set())
+    self.assertListEqual([TaskA, TaskB], scenario)
+
+    scenario = task_manager.GenerateScenario([TaskC], set())
+    self.assertListEqual([TaskA, TaskB, TaskC], scenario)
+
+    scenario = task_manager.GenerateScenario([TaskC, TaskB], set())
+    self.assertListEqual([TaskA, TaskB, TaskC], scenario)
+
+  def testFreezing(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[TaskA])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c')
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskB, TaskC])
+    def TaskD():
+      pass
+
+    # assert no exception raised.
+    task_manager.GenerateScenario([TaskB], set([TaskC]))
+
+    with self.assertRaises(task_manager.TaskError):
+      task_manager.GenerateScenario([TaskD], set([TaskA]))
+
+    self.TouchOutputFile('a')
+    scenario = task_manager.GenerateScenario([TaskD], set([TaskA]))
+    self.assertListEqual([TaskB, TaskC, TaskD], scenario)
+
+    self.TouchOutputFile('b')
+    scenario = task_manager.GenerateScenario([TaskD], set([TaskB]))
+    self.assertListEqual([TaskC, TaskD], scenario)
+
+  def testCycleError(self):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b', dependencies=[TaskA])
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskB])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskC])
+    def TaskD():
+      pass
+    TaskA._dependencies.append(TaskC)
+    with self.assertRaises(task_manager.TaskError):
+      task_manager.GenerateScenario([TaskD], set())
+
+  def testCollisionError(self):
+    builder_a = task_manager.Builder(self.output_directory)
+    builder_b = task_manager.Builder(self.output_directory)
+    @builder_a.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder_b.RegisterTask('a')
+    def TaskB():
+      pass
+    with self.assertRaises(task_manager.TaskError):
+      task_manager.GenerateScenario([TaskA, TaskB], set())
+
+  def testGraphVizOutput(self):
+    builder = task_manager.Builder(self.output_directory)
+    static_task = builder.CreateStaticTask('a', __file__)
+    @builder.RegisterTask('b')
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskB, static_task])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskC])
+    def TaskD():
+      pass
+    @builder.RegisterTask('e')
+    def TaskE():
+      pass
+    @builder.RegisterTask('f', dependencies=[TaskD, TaskE])
+    def TaskF():
+      pass
+    self.TouchOutputFile('e')
+    scenario = task_manager.GenerateScenario([TaskF], set([TaskE]))
+    output = StringIO.StringIO()
+    task_manager.OutputGraphViz(scenario, [TaskF], output)
+    self.assertEqual(_GOLDEN_GRAPHVIZ, output.getvalue())
+
+
+class CommandLineControlledExecutionTest(TaskManagerTestCase):
+  def Execute(self, *command_line_args):
+    builder = task_manager.Builder(self.output_directory)
+    @builder.RegisterTask('a')
+    def TaskA():
+      pass
+    @builder.RegisterTask('b')
+    def TaskB():
+      pass
+    @builder.RegisterTask('c', dependencies=[TaskA, TaskB])
+    def TaskC():
+      pass
+    @builder.RegisterTask('d', dependencies=[TaskA])
+    def TaskD():
+      pass
+    @builder.RegisterTask('e', dependencies=[TaskC])
+    def TaskE():
+      pass
+    @builder.RegisterTask('raise_exception', dependencies=[TaskB])
+    def TaskF():
+      raise Exception('Expected error.')
+
+    default_final_tasks = [TaskD, TaskE]
+    parser = task_manager.CommandLineParser()
+    cmd = ['-o', self.output_directory]
+    cmd.extend([i for i in command_line_args])
+    args = parser.parse_args(cmd)
+    return task_manager.ExecuteWithCommandLine(
+        args, [TaskA, TaskB, TaskC, TaskD, TaskE, TaskF], default_final_tasks)
+
+  def testSimple(self):
+    self.assertEqual(0, self.Execute())
+
+  def testDryRun(self):
+    self.assertEqual(0, self.Execute('-d'))
+
+  def testRegex(self):
+    self.assertEqual(0, self.Execute('-e', 'b', 'd'))
+    self.assertEqual(1, self.Execute('-e', r'\d'))
+
+  def testFreezing(self):
+    self.assertEqual(0, self.Execute('-f', r'\d'))
+    self.TouchOutputFile('c')
+    self.assertEqual(0, self.Execute('-f', 'c'))
+
+  def testTaskFailure(self):
+    with self.assertRaisesRegexp(Exception, r'^Expected error\.$'):
+      self.Execute('-e', 'raise_exception')
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 3588c91b93bd9b72853ecff2dc59a300da7f0ea2
Author: dcheng <dcheng@chromium.org>
Date:   Fri Apr 8 12:55:42 2016 -0700

    Convert //tools to use std::unique_ptr
    
    BUG=554298
    
    Review URL: https://codereview.chromium.org/1869503004
    
    Cr-Original-Commit-Position: refs/heads/master@{#386168}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a500b69aa06c2bc084a3b1867b9824f4218014de

diff --git a/forwarder2/daemon.cc b/forwarder2/daemon.cc
index e5ebe07..1ec390d 100644
--- a/forwarder2/daemon.cc
+++ b/forwarder2/daemon.cc
@@ -13,15 +13,16 @@
 #include <sys/types.h>
 #include <sys/wait.h>
 #include <unistd.h>
+
 #include <cstdlib>
 #include <cstring>
+#include <memory>
 #include <string>
 #include <utility>
 
 #include "base/files/file_path.h"
 #include "base/files/file_util.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/posix/eintr_wrapper.h"
 #include "base/strings/string_number_conversions.h"
 #include "base/strings/stringprintf.h"
@@ -53,7 +54,7 @@ bool RunServerAcceptLoop(const std::string& welcome_message,
                          Daemon::ServerDelegate* server_delegate) {
   bool failed = false;
   for (;;) {
-    scoped_ptr<Socket> client_socket(new Socket());
+    std::unique_ptr<Socket> client_socket(new Socket());
     if (!server_socket->Accept(client_socket.get())) {
       if (server_socket->DidReceiveEvent())
         break;
@@ -97,13 +98,13 @@ void SigChildHandler(int signal_number) {
   SIGNAL_SAFE_LOG(ERROR, string_builder.buffer());
 }
 
-scoped_ptr<Socket> ConnectToUnixDomainSocket(
+std::unique_ptr<Socket> ConnectToUnixDomainSocket(
     const std::string& socket_name,
     int tries_count,
     int idle_time_msec,
     const std::string& expected_welcome_message) {
   for (int i = 0; i < tries_count; ++i) {
-    scoped_ptr<Socket> socket(new Socket());
+    std::unique_ptr<Socket> socket(new Socket());
     if (!socket->ConnectUnix(socket_name)) {
       if (idle_time_msec)
         usleep(idle_time_msec * 1000);
@@ -122,7 +123,7 @@ scoped_ptr<Socket> ConnectToUnixDomainSocket(
     }
     return socket;
   }
-  return scoped_ptr<Socket>();
+  return nullptr;
 }
 
 }  // namespace
@@ -147,7 +148,7 @@ Daemon::~Daemon() {}
 bool Daemon::SpawnIfNeeded() {
   const int kSingleTry = 1;
   const int kNoIdleTime = 0;
-  scoped_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
+  std::unique_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
       identifier_, kSingleTry, kNoIdleTime, identifier_);
   if (!client_socket) {
     switch (fork()) {
@@ -170,7 +171,7 @@ bool Daemon::SpawnIfNeeded() {
         CHECK_EQ(dup(null_fd), STDERR_FILENO);
         Socket command_socket;
         if (!command_socket.BindUnix(identifier_)) {
-          scoped_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
+          std::unique_ptr<Socket> client_socket = ConnectToUnixDomainSocket(
               identifier_, kSingleTry, kNoIdleTime, identifier_);
           if (client_socket.get()) {
             // The daemon was spawned by a concurrent process.
diff --git a/forwarder2/daemon.h b/forwarder2/daemon.h
index f983a2e..54285df 100644
--- a/forwarder2/daemon.h
+++ b/forwarder2/daemon.h
@@ -5,10 +5,10 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_DAEMON_H_
 #define TOOLS_ANDROID_FORWARDER2_DAEMON_H_
 
+#include <memory>
 #include <string>
 
 #include "base/macros.h"
-#include "base/memory/scoped_ptr.h"
 
 namespace forwarder2 {
 
@@ -37,7 +37,7 @@ class Daemon {
     // setup signal handlers or perform global initialization.
     virtual void Init() = 0;
 
-    virtual void OnClientConnected(scoped_ptr<Socket> client_socket) = 0;
+    virtual void OnClientConnected(std::unique_ptr<Socket> client_socket) = 0;
   };
 
   // |identifier| should be a unique string identifier. It is used to
diff --git a/forwarder2/device_controller.cc b/forwarder2/device_controller.cc
index 7ad38d9..a3c5505 100644
--- a/forwarder2/device_controller.cc
+++ b/forwarder2/device_controller.cc
@@ -4,12 +4,12 @@
 
 #include "tools/android/forwarder2/device_controller.h"
 
+#include <memory>
 #include <utility>
 
 #include "base/bind.h"
 #include "base/callback_helpers.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/single_thread_task_runner.h"
 #include "base/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
@@ -20,11 +20,11 @@
 namespace forwarder2 {
 
 // static
-scoped_ptr<DeviceController> DeviceController::Create(
+std::unique_ptr<DeviceController> DeviceController::Create(
     const std::string& adb_unix_socket,
     int exit_notifier_fd) {
-  scoped_ptr<DeviceController> device_controller;
-  scoped_ptr<Socket> host_socket(new Socket());
+  std::unique_ptr<DeviceController> device_controller;
+  std::unique_ptr<Socket> host_socket(new Socket());
   if (!host_socket->BindUnix(adb_unix_socket)) {
     PLOG(ERROR) << "Could not BindAndListen DeviceController socket on port "
                 << adb_unix_socket << ": ";
@@ -44,7 +44,7 @@ void DeviceController::Start() {
   AcceptHostCommandSoon();
 }
 
-DeviceController::DeviceController(scoped_ptr<Socket> host_socket,
+DeviceController::DeviceController(std::unique_ptr<Socket> host_socket,
                                    int exit_notifier_fd)
     : host_socket_(std::move(host_socket)),
       exit_notifier_fd_(exit_notifier_fd),
@@ -60,7 +60,7 @@ void DeviceController::AcceptHostCommandSoon() {
 }
 
 void DeviceController::AcceptHostCommandInternal() {
-  scoped_ptr<Socket> socket(new Socket);
+  std::unique_ptr<Socket> socket(new Socket);
   if (!host_socket_->Accept(socket.get())) {
     if (!host_socket_->DidReceiveEvent())
       PLOG(ERROR) << "Could not Accept DeviceController socket";
@@ -89,7 +89,7 @@ void DeviceController::AcceptHostCommandInternal() {
                      << ". Attempting to restart the listener.\n";
         DeleteRefCountedValueInMapFromIterator(listener_it, &listeners_);
       }
-      scoped_ptr<DeviceListener> new_listener(DeviceListener::Create(
+      std::unique_ptr<DeviceListener> new_listener(DeviceListener::Create(
           std::move(socket), port,
           base::Bind(&DeviceController::DeleteListenerOnError,
                      weak_ptr_factory_.GetWeakPtr())));
@@ -136,8 +136,8 @@ void DeviceController::AcceptHostCommandInternal() {
 
 // static
 void DeviceController::DeleteListenerOnError(
-      const base::WeakPtr<DeviceController>& device_controller_ptr,
-      scoped_ptr<DeviceListener> device_listener) {
+    const base::WeakPtr<DeviceController>& device_controller_ptr,
+    std::unique_ptr<DeviceListener> device_listener) {
   DeviceListener* const listener = device_listener.release();
   DeviceController* const controller = device_controller_ptr.get();
   if (!controller) {
diff --git a/forwarder2/device_controller.h b/forwarder2/device_controller.h
index 5a31e7b..28fbea4 100644
--- a/forwarder2/device_controller.h
+++ b/forwarder2/device_controller.h
@@ -5,13 +5,13 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_DEVICE_CONTROLLER_H_
 #define TOOLS_ANDROID_FORWARDER2_DEVICE_CONTROLLER_H_
 
+#include <memory>
 #include <string>
 
 #include "base/containers/hash_tables.h"
 #include "base/macros.h"
 #include "base/memory/linked_ptr.h"
 #include "base/memory/ref_counted.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/memory/weak_ptr.h"
 #include "tools/android/forwarder2/socket.h"
 
@@ -28,8 +28,9 @@ class DeviceListener;
 // DeviceListener each).
 class DeviceController {
  public:
-  static scoped_ptr<DeviceController> Create(const std::string& adb_unix_socket,
-                                             int exit_notifier_fd);
+  static std::unique_ptr<DeviceController> Create(
+      const std::string& adb_unix_socket,
+      int exit_notifier_fd);
   ~DeviceController();
 
   void Start();
@@ -38,7 +39,7 @@ class DeviceController {
   typedef base::hash_map<
       int /* port */, linked_ptr<DeviceListener> > ListenersMap;
 
-  DeviceController(scoped_ptr<Socket> host_socket, int exit_notifier_fd);
+  DeviceController(std::unique_ptr<Socket> host_socket, int exit_notifier_fd);
 
   void AcceptHostCommandSoon();
   void AcceptHostCommandInternal();
@@ -47,9 +48,9 @@ class DeviceController {
   // destroyed which is why a weak pointer is used.
   static void DeleteListenerOnError(
       const base::WeakPtr<DeviceController>& device_controller_ptr,
-      scoped_ptr<DeviceListener> device_listener);
+      std::unique_ptr<DeviceListener> device_listener);
 
-  const scoped_ptr<Socket> host_socket_;
+  const std::unique_ptr<Socket> host_socket_;
   // Used to notify the controller to exit.
   const int exit_notifier_fd_;
   // Lets ensure DeviceListener instances are deleted on the thread they were
diff --git a/forwarder2/device_forwarder_main.cc b/forwarder2/device_forwarder_main.cc
index 09790f9..af796b3 100644
--- a/forwarder2/device_forwarder_main.cc
+++ b/forwarder2/device_forwarder_main.cc
@@ -67,7 +67,7 @@ class ServerDelegate : public Daemon::ServerDelegate {
     controller_thread_->Start();
   }
 
-  void OnClientConnected(scoped_ptr<Socket> client_socket) override {
+  void OnClientConnected(std::unique_ptr<Socket> client_socket) override {
     if (initialized_) {
       client_socket->WriteString("OK");
       return;
@@ -80,9 +80,10 @@ class ServerDelegate : public Daemon::ServerDelegate {
   }
 
  private:
-  void StartController(int exit_notifier_fd, scoped_ptr<Socket> client_socket) {
+  void StartController(int exit_notifier_fd,
+                       std::unique_ptr<Socket> client_socket) {
     DCHECK(!controller_.get());
-    scoped_ptr<DeviceController> controller(
+    std::unique_ptr<DeviceController> controller(
         DeviceController::Create(kUnixDomainSocketPath, exit_notifier_fd));
     if (!controller.get()) {
       client_socket->WriteString(
@@ -97,8 +98,8 @@ class ServerDelegate : public Daemon::ServerDelegate {
     client_socket->Close();
   }
 
-  scoped_ptr<DeviceController> controller_;
-  scoped_ptr<base::Thread> controller_thread_;
+  std::unique_ptr<DeviceController> controller_;
+  std::unique_ptr<base::Thread> controller_thread_;
   bool initialized_;
 };
 
diff --git a/forwarder2/device_listener.cc b/forwarder2/device_listener.cc
index 9825c3c..98459f0 100644
--- a/forwarder2/device_listener.cc
+++ b/forwarder2/device_listener.cc
@@ -4,13 +4,13 @@
 
 #include "tools/android/forwarder2/device_listener.h"
 
+#include <memory>
 #include <utility>
 
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/callback.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/single_thread_task_runner.h"
 #include "base/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
@@ -20,12 +20,12 @@
 namespace forwarder2 {
 
 // static
-scoped_ptr<DeviceListener> DeviceListener::Create(
-    scoped_ptr<Socket> host_socket,
+std::unique_ptr<DeviceListener> DeviceListener::Create(
+    std::unique_ptr<Socket> host_socket,
     int listener_port,
     const ErrorCallback& error_callback) {
-  scoped_ptr<Socket> listener_socket(new Socket());
-  scoped_ptr<DeviceListener> device_listener;
+  std::unique_ptr<Socket> listener_socket(new Socket());
+  std::unique_ptr<DeviceListener> device_listener;
   if (!listener_socket->BindTcp("", listener_port)) {
     LOG(ERROR) << "Device could not bind and listen to local port "
                << listener_port;
@@ -52,15 +52,15 @@ void DeviceListener::Start() {
   AcceptNextClientSoon();
 }
 
-void DeviceListener::SetAdbDataSocket(scoped_ptr<Socket> adb_data_socket) {
+void DeviceListener::SetAdbDataSocket(std::unique_ptr<Socket> adb_data_socket) {
   thread_.task_runner()->PostTask(
       FROM_HERE,
       base::Bind(&DeviceListener::OnAdbDataSocketReceivedOnInternalThread,
                  base::Unretained(this), base::Passed(&adb_data_socket)));
 }
 
-DeviceListener::DeviceListener(scoped_ptr<Socket> listener_socket,
-                               scoped_ptr<Socket> host_socket,
+DeviceListener::DeviceListener(std::unique_ptr<Socket> listener_socket,
+                               std::unique_ptr<Socket> host_socket,
                                int port,
                                const ErrorCallback& error_callback)
     : self_deleter_helper_(this, error_callback),
@@ -116,7 +116,7 @@ void DeviceListener::AcceptClientOnInternalThread() {
 }
 
 void DeviceListener::OnAdbDataSocketReceivedOnInternalThread(
-    scoped_ptr<Socket> adb_data_socket) {
+    std::unique_ptr<Socket> adb_data_socket) {
   DCHECK(adb_data_socket);
   SendCommand(command::ADB_DATA_SOCKET_SUCCESS, listener_port_,
               host_socket_.get());
diff --git a/forwarder2/device_listener.h b/forwarder2/device_listener.h
index 2b1a5b2..c235518 100644
--- a/forwarder2/device_listener.h
+++ b/forwarder2/device_listener.h
@@ -5,12 +5,13 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_DEVICE_LISTENER_H_
 #define TOOLS_ANDROID_FORWARDER2_DEVICE_LISTENER_H_
 
+#include <memory>
+
 #include "base/callback.h"
 #include "base/compiler_specific.h"
 #include "base/logging.h"
 #include "base/macros.h"
 #include "base/memory/ref_counted.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/threading/thread.h"
 #include "tools/android/forwarder2/forwarders_manager.h"
 #include "tools/android/forwarder2/pipe_notifier.h"
@@ -45,23 +46,24 @@ class DeviceListener {
   // Callback that is used for self-deletion on error to let the device
   // controller perform some additional cleanup work (e.g. removing the device
   // listener instance from its internal map before deleting it).
-  typedef base::Callback<void (scoped_ptr<DeviceListener>)> ErrorCallback;
+  typedef base::Callback<void(std::unique_ptr<DeviceListener>)> ErrorCallback;
 
-  static scoped_ptr<DeviceListener> Create(scoped_ptr<Socket> host_socket,
-                                           int port,
-                                           const ErrorCallback& error_callback);
+  static std::unique_ptr<DeviceListener> Create(
+      std::unique_ptr<Socket> host_socket,
+      int port,
+      const ErrorCallback& error_callback);
 
   ~DeviceListener();
 
   void Start();
 
-  void SetAdbDataSocket(scoped_ptr<Socket> adb_data_socket);
+  void SetAdbDataSocket(std::unique_ptr<Socket> adb_data_socket);
 
   int listener_port() const { return listener_port_; }
 
  private:
-  DeviceListener(scoped_ptr<Socket> listener_socket,
-                 scoped_ptr<Socket> host_socket,
+  DeviceListener(std::unique_ptr<Socket> listener_socket,
+                 std::unique_ptr<Socket> host_socket,
                  int port,
                  const ErrorCallback& error_callback);
 
@@ -72,7 +74,7 @@ class DeviceListener {
   void AcceptClientOnInternalThread();
 
   void OnAdbDataSocketReceivedOnInternalThread(
-      scoped_ptr<Socket> adb_data_socket);
+      std::unique_ptr<Socket> adb_data_socket);
 
   void OnInternalThreadError();
 
@@ -87,10 +89,10 @@ class DeviceListener {
   PipeNotifier deletion_notifier_;
   // The local device listener socket for accepting connections from the local
   // port (listener_port_).
-  const scoped_ptr<Socket> listener_socket_;
+  const std::unique_ptr<Socket> listener_socket_;
   // The listener socket for sending control commands.
-  const scoped_ptr<Socket> host_socket_;
-  scoped_ptr<Socket> device_data_socket_;
+  const std::unique_ptr<Socket> host_socket_;
+  std::unique_ptr<Socket> device_data_socket_;
   const int listener_port_;
   // Task runner used for deletion set at construction time (i.e. the object is
   // deleted on the same thread it is created on).
diff --git a/forwarder2/forwarder.cc b/forwarder2/forwarder.cc
index 9945674..3c49eef 100644
--- a/forwarder2/forwarder.cc
+++ b/forwarder2/forwarder.cc
@@ -223,7 +223,8 @@ class Forwarder::BufferedCopier {
   DISALLOW_COPY_AND_ASSIGN(BufferedCopier);
 };
 
-Forwarder::Forwarder(scoped_ptr<Socket> socket1, scoped_ptr<Socket> socket2)
+Forwarder::Forwarder(std::unique_ptr<Socket> socket1,
+                     std::unique_ptr<Socket> socket2)
     : socket1_(std::move(socket1)),
       socket2_(std::move(socket2)),
       buffer1_(new BufferedCopier(socket1_.get(), socket2_.get())),
diff --git a/forwarder2/forwarder.h b/forwarder2/forwarder.h
index 0be86fc..857babd 100644
--- a/forwarder2/forwarder.h
+++ b/forwarder2/forwarder.h
@@ -7,7 +7,8 @@
 
 #include <sys/select.h>
 
-#include "base/memory/scoped_ptr.h"
+#include <memory>
+
 #include "base/threading/thread_checker.h"
 
 namespace forwarder2 {
@@ -18,7 +19,7 @@ class Socket;
 // that this class is not thread-safe.
 class Forwarder {
  public:
-  Forwarder(scoped_ptr<Socket> socket1, scoped_ptr<Socket> socket2);
+  Forwarder(std::unique_ptr<Socket> socket1, std::unique_ptr<Socket> socket2);
 
   ~Forwarder();
 
@@ -34,12 +35,12 @@ class Forwarder {
   class BufferedCopier;
 
   base::ThreadChecker thread_checker_;
-  const scoped_ptr<Socket> socket1_;
-  const scoped_ptr<Socket> socket2_;
+  const std::unique_ptr<Socket> socket1_;
+  const std::unique_ptr<Socket> socket2_;
   // Copies data from socket1 to socket2.
-  const scoped_ptr<BufferedCopier> buffer1_;
+  const std::unique_ptr<BufferedCopier> buffer1_;
   // Copies data from socket2 to socket1.
-  const scoped_ptr<BufferedCopier> buffer2_;
+  const std::unique_ptr<BufferedCopier> buffer2_;
 };
 
 }  // namespace forwarder2
diff --git a/forwarder2/forwarders_manager.cc b/forwarder2/forwarders_manager.cc
index fb6890f..c3dd026 100644
--- a/forwarder2/forwarders_manager.cc
+++ b/forwarder2/forwarders_manager.cc
@@ -31,8 +31,9 @@ ForwardersManager::~ForwardersManager() {
   deletion_notifier_.Notify();
 }
 
-void ForwardersManager::CreateAndStartNewForwarder(scoped_ptr<Socket> socket1,
-                                                   scoped_ptr<Socket> socket2) {
+void ForwardersManager::CreateAndStartNewForwarder(
+    std::unique_ptr<Socket> socket1,
+    std::unique_ptr<Socket> socket2) {
   // Note that the internal Forwarder vector is populated on the internal thread
   // which is the only thread from which it's accessed.
   thread_.task_runner()->PostTask(
@@ -47,8 +48,8 @@ void ForwardersManager::CreateAndStartNewForwarder(scoped_ptr<Socket> socket1,
 }
 
 void ForwardersManager::CreateNewForwarderOnInternalThread(
-    scoped_ptr<Socket> socket1,
-    scoped_ptr<Socket> socket2) {
+    std::unique_ptr<Socket> socket1,
+    std::unique_ptr<Socket> socket2) {
   DCHECK(thread_.task_runner()->RunsTasksOnCurrentThread());
   forwarders_.push_back(new Forwarder(std::move(socket1), std::move(socket2)));
 }
diff --git a/forwarder2/forwarders_manager.h b/forwarder2/forwarders_manager.h
index 4c6dea6..2cdafdb 100644
--- a/forwarder2/forwarders_manager.h
+++ b/forwarder2/forwarders_manager.h
@@ -5,7 +5,8 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_FORWARDERS_MANAGER_H_
 #define TOOLS_ANDROID_FORWARDER2_FORWARDERS_MANAGER_H_
 
-#include "base/memory/scoped_ptr.h"
+#include <memory>
+
 #include "base/memory/scoped_vector.h"
 #include "base/threading/thread.h"
 #include "tools/android/forwarder2/pipe_notifier.h"
@@ -24,12 +25,12 @@ class ForwardersManager {
   ~ForwardersManager();
 
   // Can be called on any thread.
-  void CreateAndStartNewForwarder(scoped_ptr<Socket> socket1,
-                                  scoped_ptr<Socket> socket2);
+  void CreateAndStartNewForwarder(std::unique_ptr<Socket> socket1,
+                                  std::unique_ptr<Socket> socket2);
 
  private:
-  void CreateNewForwarderOnInternalThread(scoped_ptr<Socket> socket1,
-                                          scoped_ptr<Socket> socket2);
+  void CreateNewForwarderOnInternalThread(std::unique_ptr<Socket> socket1,
+                                          std::unique_ptr<Socket> socket2);
 
   void WaitForEventsOnInternalThreadSoon();
   void WaitForEventsOnInternalThread();
diff --git a/forwarder2/host_controller.cc b/forwarder2/host_controller.cc
index 68394df..0510890 100644
--- a/forwarder2/host_controller.cc
+++ b/forwarder2/host_controller.cc
@@ -4,13 +4,13 @@
 
 #include "tools/android/forwarder2/host_controller.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/logging.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/thread_task_runner_handle.h"
 #include "tools/android/forwarder2/command.h"
 #include "tools/android/forwarder2/forwarder.h"
@@ -19,15 +19,15 @@
 namespace forwarder2 {
 
 // static
-scoped_ptr<HostController> HostController::Create(
+std::unique_ptr<HostController> HostController::Create(
     int device_port,
     int host_port,
     int adb_port,
     int exit_notifier_fd,
     const ErrorCallback& error_callback) {
-  scoped_ptr<HostController> host_controller;
-  scoped_ptr<PipeNotifier> delete_controller_notifier(new PipeNotifier());
-  scoped_ptr<Socket> adb_control_socket(new Socket());
+  std::unique_ptr<HostController> host_controller;
+  std::unique_ptr<PipeNotifier> delete_controller_notifier(new PipeNotifier());
+  std::unique_ptr<Socket> adb_control_socket(new Socket());
   adb_control_socket->AddEventFd(exit_notifier_fd);
   adb_control_socket->AddEventFd(delete_controller_notifier->receiver_fd());
   if (!adb_control_socket->ConnectTcp(std::string(), adb_port)) {
@@ -70,8 +70,8 @@ HostController::HostController(
     int adb_port,
     int exit_notifier_fd,
     const ErrorCallback& error_callback,
-    scoped_ptr<Socket> adb_control_socket,
-    scoped_ptr<PipeNotifier> delete_controller_notifier)
+    std::unique_ptr<Socket> adb_control_socket,
+    std::unique_ptr<PipeNotifier> delete_controller_notifier)
     : self_deleter_helper_(this, error_callback),
       device_port_(device_port),
       host_port_(host_port),
@@ -97,7 +97,7 @@ void HostController::ReadCommandOnInternalThread() {
     return;
   }
   // Try to connect to host server.
-  scoped_ptr<Socket> host_server_data_socket(new Socket());
+  std::unique_ptr<Socket> host_server_data_socket(new Socket());
   if (!host_server_data_socket->ConnectTcp(std::string(), host_port_)) {
     LOG(ERROR) << "Could not Connect HostServerData socket on port: "
                << host_port_;
@@ -121,8 +121,8 @@ void HostController::ReadCommandOnInternalThread() {
 }
 
 void HostController::StartForwarder(
-    scoped_ptr<Socket> host_server_data_socket) {
-  scoped_ptr<Socket> adb_data_socket(new Socket());
+    std::unique_ptr<Socket> host_server_data_socket) {
+  std::unique_ptr<Socket> adb_data_socket(new Socket());
   if (!adb_data_socket->ConnectTcp("", adb_port_)) {
     LOG(ERROR) << "Could not connect AdbDataSocket on port: " << adb_port_;
     OnInternalThreadError();
diff --git a/forwarder2/host_controller.h b/forwarder2/host_controller.h
index 484adac..7328245 100644
--- a/forwarder2/host_controller.h
+++ b/forwarder2/host_controller.h
@@ -5,12 +5,12 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_HOST_CONTROLLER_H_
 #define TOOLS_ANDROID_FORWARDER2_HOST_CONTROLLER_H_
 
+#include <memory>
 #include <string>
 
 #include "base/callback.h"
 #include "base/compiler_specific.h"
 #include "base/macros.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/memory/weak_ptr.h"
 #include "base/threading/thread.h"
 #include "tools/android/forwarder2/forwarders_manager.h"
@@ -34,15 +34,16 @@ class HostController {
  public:
   // Callback used for self-deletion when an error happens so that the client
   // can perform some cleanup work before deleting the HostController instance.
-  typedef base::Callback<void (scoped_ptr<HostController>)> ErrorCallback;
+  typedef base::Callback<void(std::unique_ptr<HostController>)> ErrorCallback;
 
   // If |device_port| is zero then a dynamic port is allocated (and retrievable
   // through device_port() below).
-  static scoped_ptr<HostController> Create(int device_port,
-                                           int host_port,
-                                           int adb_port,
-                                           int exit_notifier_fd,
-                                           const ErrorCallback& error_callback);
+  static std::unique_ptr<HostController> Create(
+      int device_port,
+      int host_port,
+      int adb_port,
+      int exit_notifier_fd,
+      const ErrorCallback& error_callback);
 
   ~HostController();
 
@@ -59,13 +60,13 @@ class HostController {
                  int adb_port,
                  int exit_notifier_fd,
                  const ErrorCallback& error_callback,
-                 scoped_ptr<Socket> adb_control_socket,
-                 scoped_ptr<PipeNotifier> delete_controller_notifier);
+                 std::unique_ptr<Socket> adb_control_socket,
+                 std::unique_ptr<PipeNotifier> delete_controller_notifier);
 
   void ReadNextCommandSoon();
   void ReadCommandOnInternalThread();
 
-  void StartForwarder(scoped_ptr<Socket> host_server_data_socket);
+  void StartForwarder(std::unique_ptr<Socket> host_server_data_socket);
 
   // Note that this gets also called when ~HostController() is invoked.
   void OnInternalThreadError();
@@ -78,10 +79,10 @@ class HostController {
   const int adb_port_;
   // Used to notify the controller when the process is killed.
   const int global_exit_notifier_fd_;
-  scoped_ptr<Socket> adb_control_socket_;
+  std::unique_ptr<Socket> adb_control_socket_;
   // Used to cancel the pending blocking IO operations when the host controller
   // instance is deleted.
-  scoped_ptr<PipeNotifier> delete_controller_notifier_;
+  std::unique_ptr<PipeNotifier> delete_controller_notifier_;
   // Task runner used for deletion set at deletion time (i.e. the object is
   // deleted on the same thread it is created on).
   const scoped_refptr<base::SingleThreadTaskRunner> deletion_task_runner_;
diff --git a/forwarder2/host_forwarder_main.cc b/forwarder2/host_forwarder_main.cc
index 04684f0..2fae616 100644
--- a/forwarder2/host_forwarder_main.cc
+++ b/forwarder2/host_forwarder_main.cc
@@ -102,7 +102,7 @@ class HostControllersManager {
                      const std::string& device_serial,
                      int device_port,
                      int host_port,
-                     scoped_ptr<Socket> client_socket) {
+                     std::unique_ptr<Socket> client_socket) {
     // Lazy initialize so that the CLI process doesn't get this thread created.
     InitOnce();
     thread_->task_runner()->PostTask(
@@ -135,7 +135,7 @@ class HostControllersManager {
   // controller manager was destroyed which is why a weak pointer is used.
   static void DeleteHostController(
       const base::WeakPtr<HostControllersManager>& manager_ptr,
-      scoped_ptr<HostController> host_controller) {
+      std::unique_ptr<HostController> host_controller) {
     HostController* const controller = host_controller.release();
     HostControllersManager* const manager = manager_ptr.get();
     if (!manager) {
@@ -156,7 +156,7 @@ class HostControllersManager {
                                      const std::string& device_serial,
                                      int device_port,
                                      int host_port,
-                                     scoped_ptr<Socket> client_socket) {
+                                     std::unique_ptr<Socket> client_socket) {
     const int adb_port = GetAdbPortForDevice(adb_path, device_serial);
     if (adb_port < 0) {
       SendMessage(
@@ -198,11 +198,10 @@ class HostControllersManager {
       }
     }
     // Create a new host controller.
-    scoped_ptr<HostController> host_controller(
-        HostController::Create(
-            device_port, host_port, adb_port, GetExitNotifierFD(),
-            base::Bind(&HostControllersManager::DeleteHostController,
-                       weak_ptr_factory_.GetWeakPtr())));
+    std::unique_ptr<HostController> host_controller(HostController::Create(
+        device_port, host_port, adb_port, GetExitNotifierFD(),
+        base::Bind(&HostControllersManager::DeleteHostController,
+                   weak_ptr_factory_.GetWeakPtr())));
     if (!host_controller.get()) {
       has_failed_ = true;
       SendMessage("ERROR: Connection to device failed.", client_socket.get());
@@ -222,7 +221,7 @@ class HostControllersManager {
                        linked_ptr<HostController>(host_controller.release())));
   }
 
-  void LogExistingControllers(const scoped_ptr<Socket>& client_socket) {
+  void LogExistingControllers(const std::unique_ptr<Socket>& client_socket) {
     SendMessage("ERROR: Existing controllers:", client_socket.get());
     for (const auto& controller : *controllers_) {
       SendMessage(base::StringPrintf("ERROR:   %s", controller.first.c_str()),
@@ -309,10 +308,11 @@ class HostControllersManager {
   }
 
   base::hash_map<std::string, int> device_serial_to_adb_port_map_;
-  scoped_ptr<HostControllerMap> controllers_;
+  std::unique_ptr<HostControllerMap> controllers_;
   bool has_failed_;
-  scoped_ptr<base::AtExitManager> at_exit_manager_;  // Needed by base::Thread.
-  scoped_ptr<base::Thread> thread_;
+  std::unique_ptr<base::AtExitManager>
+      at_exit_manager_;  // Needed by base::Thread.
+  std::unique_ptr<base::Thread> thread_;
   base::WeakPtrFactory<HostControllersManager> weak_ptr_factory_;
 };
 
@@ -334,7 +334,7 @@ class ServerDelegate : public Daemon::ServerDelegate {
     signal(SIGINT, KillHandler);
   }
 
-  void OnClientConnected(scoped_ptr<Socket> client_socket) override {
+  void OnClientConnected(std::unique_ptr<Socket> client_socket) override {
     char buf[kBufSize];
     const int bytes_read = client_socket->Read(buf, sizeof(buf));
     if (bytes_read <= 0) {
diff --git a/forwarder2/self_deleter_helper.h b/forwarder2/self_deleter_helper.h
index 9739bd0..089df03 100644
--- a/forwarder2/self_deleter_helper.h
+++ b/forwarder2/self_deleter_helper.h
@@ -5,13 +5,15 @@
 #ifndef TOOLS_ANDROID_FORWARDER2_SELF_DELETER_HELPER_H_
 #define TOOLS_ANDROID_FORWARDER2_SELF_DELETER_HELPER_H_
 
+#include <memory>
+
 #include "base/bind.h"
 #include "base/callback.h"
 #include "base/location.h"
 #include "base/logging.h"
 #include "base/macros.h"
+#include "base/memory/ptr_util.h"
 #include "base/memory/ref_counted.h"
-#include "base/memory/scoped_ptr.h"
 #include "base/memory/weak_ptr.h"
 #include "base/thread_task_runner_handle.h"
 
@@ -42,7 +44,7 @@ namespace forwarder2 {
 // Usage example:
 // class Object {
 //  public:
-//   typedef base::Callback<void (scoped_ptr<Object>)> ErrorCallback;
+//   typedef base::Callback<void (std::unique_ptr<Object>)> ErrorCallback;
 //
 //   Object(const ErrorCallback& error_callback)
 //       : self_deleter_helper_(this, error_callback) {
@@ -80,7 +82,7 @@ namespace forwarder2 {
 //   }
 //
 //  private:
-//   void DeleteObjectOnError(scoped_ptr<Object> object) {
+//   void DeleteObjectOnError(std::unique_ptr<Object> object) {
 //     DCHECK(thread_checker_.CalledOnValidThread());
 //     DCHECK_EQ(object_, object);
 //     // Do some extra work with |object| before it gets deleted...
@@ -89,13 +91,13 @@ namespace forwarder2 {
 //   }
 //
 //   base::ThreadChecker thread_checker_;
-//   scoped_ptr<Object> object_;
+//   std::unique_ptr<Object> object_;
 // };
 //
 template <typename T>
 class SelfDeleterHelper {
  public:
-  typedef base::Callback<void (scoped_ptr<T>)> DeletionCallback;
+  typedef base::Callback<void(std::unique_ptr<T>)> DeletionCallback;
 
   SelfDeleterHelper(T* self_deleting_object,
                     const DeletionCallback& deletion_callback)
@@ -119,7 +121,7 @@ class SelfDeleterHelper {
  private:
   void SelfDelete() {
     DCHECK(construction_runner_->RunsTasksOnCurrentThread());
-    deletion_callback_.Run(make_scoped_ptr(self_deleting_object_));
+    deletion_callback_.Run(base::WrapUnique(self_deleting_object_));
   }
 
   const scoped_refptr<base::SingleThreadTaskRunner> construction_runner_;
diff --git a/md5sum/md5sum.cc b/md5sum/md5sum.cc
index 94efa10..eaee434 100644
--- a/md5sum/md5sum.cc
+++ b/md5sum/md5sum.cc
@@ -9,6 +9,7 @@
 
 #include <fstream>
 #include <iostream>
+#include <memory>
 #include <set>
 #include <string>
 
@@ -17,7 +18,6 @@
 #include "base/files/file_util.h"
 #include "base/logging.h"
 #include "base/md5.h"
-#include "base/memory/scoped_ptr.h"
 
 namespace {
 
@@ -31,7 +31,7 @@ bool MD5Sum(const char* path, std::string* digest_string) {
   base::MD5Context ctx;
   base::MD5Init(&ctx);
   const size_t kBufferSize = 1 << 16;
-  scoped_ptr<char[]> buf(new char[kBufferSize]);
+  std::unique_ptr<char[]> buf(new char[kBufferSize]);
   size_t len;
   while ((len = fread(buf.get(), 1, kBufferSize, file.get())) > 0)
     base::MD5Update(&ctx, base::StringPiece(buf.get(), len));

commit 9d5d8d8c35e56cf10cc1b9a3a61df3a919985910
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 08:23:49 2016 -0700

    tools/android/loading GCE validation: Add delay after analyze failures
    
    Previously, when a adb connection error happenned it was typically
    causing several traces to fail in a row.
    This CL adds a 3 seconds delay when analyze fails, so that the adb
    connection has some time to recover.
    As a result, trace collection fails less often.
    
    Review URL: https://codereview.chromium.org/1873753002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386082}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5f90425cf39d2a58987cf409113d1aa32e9b8e24

diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
index b324a0b..3dc6c81 100755
--- a/loading/unmaintained/gce_validation_collect.sh
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -41,5 +41,9 @@ EOF
       --devtools_port 9222 \
       --url $site \
       --output $outdir/${output_subdir}/${run}
+   if [ $? -ne 0 ]; then
+    echo "Analyze failed. Wait a bit for device to recover."
+    sleep 3
+   fi
  done
 done

commit 7c85de8450c59c8f7e68a1de3edcc8451738bf8e
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 07:53:58 2016 -0700

    tools/android/loading Add argument to clear device data
    
    This option uses adb to clear all Chrome data from the device.
    The intent is to ensure that a fresh profile is used.
    
    Review URL: https://codereview.chromium.org/1869673003
    
    Cr-Original-Commit-Position: refs/heads/master@{#386074}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8770093f8cdc207286be2745eed7b42606fddeee

diff --git a/loading/controller.py b/loading/controller.py
index a0f6dff..5944711 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -216,6 +216,9 @@ class RemoteChromeController(ChromeControllerBase):
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
     self._device.KillAll(package_info.package, quiet=True)
+    if OPTIONS.clear_device_data:
+      logging.info('Clear Chrome data')
+      self._device.adb.Shell('pm clear ' + package_info.package)
     chrome_args = self._GetChromeArguments()
     logging.info('Launching %s with flags: %s' % (package_info.package,
         subprocess.list2cmdline(chrome_args)))
diff --git a/loading/options.py b/loading/options.py
index 021377c..3ea91b7 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -20,6 +20,8 @@ class Options(object):
   # Tuples of (argument name, default value, help string).
   _ARGS = [ ('clear_cache', True,
              'clear browser cache before loading'),
+            ('clear_device_data', False,
+             'Clear Chrome data from device before loading'),
             ('chrome_package_name', 'chrome',
              'build/android/pylib/constants package description'),
             ('devtools_hostname', 'localhost',
diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
index 41d8417..b324a0b 100755
--- a/loading/unmaintained/gce_validation_collect.sh
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -37,7 +37,7 @@ EOF
  for ((run=0;run<$repeat_count;++run)); do
    echo '****'  $run
    tools/android/loading/analyze.py log_requests \
-      --clear_cache \
+      --clear_device_data \
       --devtools_port 9222 \
       --url $site \
       --output $outdir/${output_subdir}/${run}

commit 85d14c9250c83ca5df2393cbb0b1ada6d28dfc76
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 8 07:35:21 2016 -0700

    clovis: Fix the main renderer thread detection.
    
    Also returns the figure instead of saving it in network_cpu_activity_view.py.
    
    Review URL: https://codereview.chromium.org/1869373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386071}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0fe5fae10dc8bfecc3847c43f114f8c692377e6c

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index bf6610d..7fdd9af 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -57,6 +57,9 @@ class ActivityLens(object):
           and tracing_event['name'] == 'thread_name'
           and event.args['name'] == 'CrRendererMain'):
         main_renderer_thread_ids.add((pid, tid))
+    events_count_per_pid_tid = {
+        pid_tid: count for (pid_tid, count) in events_count_per_pid_tid.items()
+        if pid_tid in main_renderer_thread_ids}
     pid_tid_events_counts = sorted(events_count_per_pid_tid.items(),
                                    key=operator.itemgetter(1), reverse=True)
     if (len(pid_tid_events_counts) > 1
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index f0c1275..839809a 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -211,6 +211,13 @@ class ActivityLensTestCase(unittest.TestCase):
          u'ph': u'X',
          u'pid': 1,
          u'tid': 1,
+         u'ts': 0},
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 1,
+         u'tid': 1,
          u'ts': 0}]
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'parser')
diff --git a/loading/network_cpu_activity_view.py b/loading/network_cpu_activity_view.py
index 09450e1..a118823 100755
--- a/loading/network_cpu_activity_view.py
+++ b/loading/network_cpu_activity_view.py
@@ -27,12 +27,14 @@ def _CpuActivityTimeline(cpu_lens, start_msec, end_msec, granularity):
   return (cpu_timestamps[:-1], np.array(busy_percentage))
 
 
-def GraphTimelines(trace, output_filename):
-  """Creates and saves a graph of Network and CPU activity for a trace.
+def GraphTimelines(trace):
+  """Creates a figure of Network and CPU activity for a trace.
 
   Args:
     trace: (LoadingTrace)
-    output_filename: (str) Path of the output graph.
+
+  Returns:
+    A matplotlib.pylab.figure.
   """
   cpu_lens = activity_lens.ActivityLens(trace)
   network_lens = network_activity_lens.NetworkActivityLens(trace)
@@ -56,13 +58,15 @@ def GraphTimelines(trace, output_filename):
   cpu.set_ylim(ymin=0, ymax=100)
   cpu.set_xlabel('Time (ms)')
   cpu.set_ylabel('Main Renderer Thread Busyness (%)')
-  figure.savefig(output_filename, dpi=300)
+  return figure
 
 
 def main():
   filename = sys.argv[1]
   trace = loading_trace.LoadingTrace.FromJsonFile(filename)
-  GraphTimelines(trace, filename + '.pdf')
+  figure = GraphTimelines(trace, filename + '.pdf')
+  output_filename = filename + '.pdf'
+  figure.savefig(output_filename, dpi=300)
 
 
 if __name__ == '__main__':

commit c382b512efb7c6c8f4e1cf8286302fa85a5a40c7
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 07:26:05 2016 -0700

    tools/android/loading Fix logging crash in main.py
    
    trace.ToJsonDict() was sometimes trying to write to a closed log file,
    resulting in crashes.
    Some exceptions were also not caught in the log file, and the
    exception handling code was itself crashing.
    
    This CL reorganizes the handling of log files and exceptions handling.
    
    NO_DEPENDENCY_CHECKS=true
    
    Review URL: https://codereview.chromium.org/1871023002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386067}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e4bb66b3be2c5f84b5c48b1d99e1a77f5bfbbca2

diff --git a/loading/gce/main.py b/loading/gce/main.py
index ad4410c..2988113 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -134,8 +134,9 @@ class ServerApp(object):
     old_stderr = sys.stderr
 
     trace_metadata = { 'succeeded' : False, 'url' : url }
-    try:
-      with open(log_filename, 'w') as sys.stdout:
+    trace = None
+    with open(log_filename, 'w') as sys.stdout:
+      try:
         sys.stderr = sys.stdout
 
         # Set up the controller.
@@ -154,15 +155,16 @@ class ServerApp(object):
               url, connection, chrome_ctl.ChromeMetadata())
           trace_metadata['succeeded'] = True
           trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
-    except Exception as e:
-      sys.stderr.write(e)
+      except Exception as e:
+        sys.stderr.write(str(e))
+
+      if trace:
+        with open(filename, 'w') as f:
+          json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
 
     sys.stdout = old_stdout
     sys.stderr = old_stderr
 
-    with open(filename, 'w') as f:
-      json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
-
     return trace_metadata
 
   def _GetCurrentTaskCount(self):

commit 024c76a86a903c7b8f36fbfd108719681d17b835
Author: droger <droger@chromium.org>
Date:   Fri Apr 8 07:23:16 2016 -0700

    tools/android/loading Improve GCE status reporting
    
    There is now only one way to check the status of the application.
    Previously, there was one used by the /status request and another used
    by the /set_tasks request.
    This was leading to inconsistencies where /status reported that the app
    is idle, but then /set_tasks failed because it considered the app is
    still busy.
    
    NO_DEPENDENCY_CHECKS=true
    
    Review URL: https://codereview.chromium.org/1874673002
    
    Cr-Original-Commit-Position: refs/heads/master@{#386066}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9a1f6d3bb92a29c23652c48724cdc269173608ab

diff --git a/loading/gce/main.py b/loading/gce/main.py
index dc1da8e..ad4410c 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -63,6 +63,10 @@ class ServerApp(object):
     options.OPTIONS.ParseArgs([])
     options.OPTIONS.local_binary = config['chrome_path']
 
+  def _IsProcessingTasks(self):
+    """Returns True if the application is currently processing tasks."""
+    return self._thread is not None and self._thread.is_alive()
+
   def _GetStorageClient(self):
     return storage.Client(project = self._project_name,
                           credentials = self._credentials)
@@ -240,7 +244,7 @@ class ServerApp(object):
       A string to be sent back to the client, describing the success status of
       the request.
     """
-    if self._thread is not None and self._thread.is_alive():
+    if self._IsProcessingTasks():
       return 'Error: Already running\n'
 
     load_parameters = json.loads(http_body)
@@ -281,12 +285,15 @@ class ServerApp(object):
     elif path == '/test':
       data = 'hello\n'
     elif path == '/status':
-      task_count = self._GetCurrentTaskCount()
-      if task_count == 0:
+      if not self._IsProcessingTasks():
         data = 'Idle\n'
       else:
-        data = 'Remaining tasks: %s / %s\n' % (
-            task_count, self._initial_task_count)
+        task_count = self._GetCurrentTaskCount()
+        if task_count == 0:
+          data = '%s tasks complete. Finalizing.\n' % self._initial_task_count
+        else:
+          data = 'Remaining tasks: %s / %s\n' % (
+              task_count, self._initial_task_count)
         elapsed = time.time() - self._start_time
         data += 'Elapsed time: %s seconds\n' % str(elapsed)
         self._tasks_lock.acquire()

commit e2edaa9d5b784a97b30854073bd004adfb43b29f
Author: gabadie <gabadie@chromium.org>
Date:   Fri Apr 8 02:55:56 2016 -0700

    tools/android/loading: Add WPR logging support in the ChromeController
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1865803005
    
    Cr-Original-Commit-Position: refs/heads/master@{#386030}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5b980b6c398d38685a82957cb0205aaa4eac5de3

diff --git a/loading/controller.py b/loading/controller.py
index 567bcb7..a0f6dff 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -152,7 +152,8 @@ class ChromeControllerBase(object):
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
     """Opens a Web Page Replay host context.
 
     Args:
@@ -162,6 +163,7 @@ class ChromeControllerBase(object):
           emulation.NETWORK_CONDITIONS.
       disable_script_injection: Disable JavaScript file injections that is
         fighting against resources name entropy.
+      out_log_path: Path of the WPR host's log.
     """
     raise NotImplementedError
 
@@ -261,13 +263,15 @@ class RemoteChromeController(ChromeControllerBase):
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
     """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
     assert not self._chrome_wpr_specific_args, 'WPR is already running.'
     with device_setup.RemoteWprHost(self._device, wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
-        disable_script_injection=disable_script_injection) as additional_flags:
+        disable_script_injection=disable_script_injection,
+        out_log_path=out_log_path) as additional_flags:
       self._chrome_wpr_specific_args = additional_flags
       yield
     self._chrome_wpr_specific_args = []
@@ -368,14 +372,15 @@ class LocalChromeController(ChromeControllerBase):
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
     """Override for WPR context."""
     assert not self._chrome_wpr_specific_args, 'WPR is already running.'
     with device_setup.LocalWprHost(wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
-        disable_script_injection=disable_script_injection
-        ) as additional_flags:
+        disable_script_injection=disable_script_injection,
+        out_log_path=out_log_path) as additional_flags:
       self._chrome_wpr_specific_args = additional_flags
       yield
     self._chrome_wpr_specific_args = []
diff --git a/loading/device_setup.py b/loading/device_setup.py
index d2c466d..d1abaff 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -131,8 +131,16 @@ def ForwardPort(device, local, remote):
 def _WprHost(wpr_archive_path, record=False,
              network_condition_name=None,
              disable_script_injection=False,
-             wpr_ca_cert_path=None):
+             wpr_ca_cert_path=None,
+             out_log_path=None):
   assert wpr_archive_path
+
+  def PathWorkaround(path):
+    # webpagereplay.ReplayServer is doing a os.path.exist(os.path.dirname(p))
+    # that fails if p = 'my_file.txt' because os.path.dirname(p) = '' != '.'.
+    # This workaround just sends absolute path to work around this bug.
+    return os.path.abspath(path)
+
   wpr_server_args = ['--use_closest_match']
   if record:
     wpr_server_args.append('--record')
@@ -157,10 +165,17 @@ def _WprHost(wpr_archive_path, record=False,
     wpr_server_args.extend(['--inject_scripts', ''])
   if wpr_ca_cert_path:
     wpr_server_args.extend(['--should_generate_certs',
-                            '--https_root_ca_cert_path=' + wpr_ca_cert_path])
+        '--https_root_ca_cert_path=' + PathWorkaround(wpr_ca_cert_path)])
+  if out_log_path:
+    # --log_level debug to extract the served URLs requests from the log.
+    wpr_server_args.extend(['--log_level', 'debug',
+                            '--log_file', PathWorkaround(out_log_path)])
+    # Don't append to previously existing log.
+    if os.path.exists(out_log_path):
+      os.remove(out_log_path)
 
   # Set up WPR server and device forwarder.
-  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
+  wpr_server = webpagereplay.ReplayServer(PathWorkaround(wpr_archive_path),
       '127.0.0.1', 0, 0, None, wpr_server_args)
   http_port, https_port = wpr_server.StartServer()[:-1]
 
@@ -193,7 +208,8 @@ def _FormatWPRRelatedChromeArgumentFor(http_port, https_port, escape):
 @contextlib.contextmanager
 def LocalWprHost(wpr_archive_path, record=False,
                  network_condition_name=None,
-                 disable_script_injection=False):
+                 disable_script_injection=False,
+                 out_log_path=None):
   """Launches web page replay host.
 
   Args:
@@ -203,6 +219,7 @@ def LocalWprHost(wpr_archive_path, record=False,
         emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
+    out_log_path: Path of the WPR host's log.
 
   Returns:
     Additional flags list that may be used for chromium to load web page through
@@ -216,8 +233,8 @@ def LocalWprHost(wpr_archive_path, record=False,
       wpr_archive_path,
       record=record,
       network_condition_name=network_condition_name,
-      disable_script_injection=disable_script_injection
-      ) as (http_port, https_port):
+      disable_script_injection=disable_script_injection,
+      out_log_path=out_log_path) as (http_port, https_port):
     chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
                                                      escape=False)
     # Certification authority is handled only available on Android.
@@ -228,7 +245,8 @@ def LocalWprHost(wpr_archive_path, record=False,
 @contextlib.contextmanager
 def RemoteWprHost(device, wpr_archive_path, record=False,
                   network_condition_name=None,
-                  disable_script_injection=False):
+                  disable_script_injection=False,
+                  out_log_path=None):
   """Launches web page replay host.
 
   Args:
@@ -239,6 +257,7 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
         emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
+    out_log_path: Path of the WPR host's log.
 
   Returns:
     Additional flags list that may be used for chromium to load web page through
@@ -264,8 +283,8 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection,
-        wpr_ca_cert_path=wpr_ca_cert_path
-        ) as (http_port, https_port):
+        wpr_ca_cert_path=wpr_ca_cert_path,
+        out_log_path=out_log_path) as (http_port, https_port):
       # Set up the forwarder.
       forwarder.Forwarder.Map([(0, http_port), (0, https_port)], device)
       device_http_port = forwarder.Forwarder.DevicePortForHostPort(http_port)

commit e76e5d0324e5ad1e7070bc60638b3d6d70ce3179
Author: iceman <iceman@yandex-team.ru>
Date:   Thu Apr 7 12:36:33 2016 -0700

    Use StringPiece as an argument for Base64Encode() instead of std::string.
    
    It is not necessary to make a copy of characters when calling
    Base64Encode() funcion.
    
    No behavior changes.
    
    BUG=
    CQ_INCLUDE_TRYBOTS=tryserver.blink:linux_blink_rel
    
    Review URL: https://codereview.chromium.org/1841703002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385840}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 35ea753e261641df335585e8a4dc060cb1e456e7

diff --git a/memdump/memdump.cc b/memdump/memdump.cc
index 7f135ee..572a838 100644
--- a/memdump/memdump.cc
+++ b/memdump/memdump.cc
@@ -56,9 +56,8 @@ class BitSet {
     size_t end = data_.size();
     while (end > 0 && data_[end - 1] == '\0')
       --end;
-    std::string bits(&data_[0], end);
     std::string b64_string;
-    base::Base64Encode(bits, &b64_string);
+    base::Base64Encode(base::StringPiece(data_.data(), end), &b64_string);
     return b64_string;
   }
 

commit 73e0d89654b2dc09c544a08e5aaa0ba8e843d0d8
Author: gabadie <gabadie@chromium.org>
Date:   Thu Apr 7 08:57:06 2016 -0700

    Sandwich: Implement filter-cache's --subresource-discoverer flag.
    
    Before this CL, sandwich's cache filtering operation was whitelisting
    resources only discoverable by the parser. This CL add different
    strategies to white-list resources to keep in the cache for benchmark
    metrics comparison.
    
    Moreover, this CL also add the record-test-trace sandwich sub-command
    generating sandwich compatible loading traces using the
    webserver_test module, and also add the loading_trace_analyzer.py's
    prune subcommand to make loading traces thinner such as the
    scanner_vs_parser.trace in this CL.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1859553002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385771}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0c6569f5a9cd03222930efa4593f9fe2c8ff9800

diff --git a/loading/common_util.py b/loading/common_util.py
index 9384df0..ca06da2 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -2,7 +2,10 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import contextlib
 import logging
+import shutil
+import tempfile
 import time
 
 
@@ -61,3 +64,15 @@ def DeserializeAttributesFromJsonDict(json_dict, instance, attributes):
     getattr(instance, attr) # To raise AttributeError if attr doesn't exist.
     setattr(instance, attr, json_dict[attr])
   return instance
+
+
+@contextlib.contextmanager
+def TemporaryDirectory():
+  """Returns a freshly-created directory that gets automatically deleted after
+  usage.
+  """
+  name = tempfile.mkdtemp()
+  try:
+    yield name
+  finally:
+    shutil.rmtree(name)
diff --git a/loading/loading_trace_analyzer.py b/loading/loading_trace_analyzer.py
index 1821001..3c7882d 100755
--- a/loading/loading_trace_analyzer.py
+++ b/loading/loading_trace_analyzer.py
@@ -33,6 +33,23 @@ def _ArgumentParser():
       dest='where_statement', type=str,
       nargs=2, metavar=('FORMAT', 'REGEX'), default=[],
       help='Where statement to filter such as: --where "{protocol}" "https?"')
+
+  # requests listing subcommand.
+  prune_parser = subparsers.add_parser('prune',
+      help='Prunes some stuff from traces to make them small.')
+  prune_parser.add_argument('loading_trace', type=file,
+      help='Input path of the loading trace.')
+  prune_parser.add_argument('-t', '--trace-filters',
+      type=str, nargs='+', metavar='REGEX', default=[],
+      help='Regex filters to whitelist trace events.')
+  prune_parser.add_argument('-r', '--request-member-filter',
+      type=str, nargs='+', metavar='REGEX', default=[],
+      help='Regex filters to whitelist requests\' members.')
+  prune_parser.add_argument('-i', '--indent', type=int, default=2,
+      help='Number of space to indent the output.')
+  prune_parser.add_argument('-o', '--output',
+      type=argparse.FileType('w'), default=sys.stdout,
+      help='Output destination path if different from stdout.')
   return parser
 
 
@@ -70,9 +87,70 @@ def ListRequests(loading_trace_path,
     yield output_format.format(**request_event_json)
 
 
-def main(command_line_args):
-  """Command line tool entry point.
+def _PruneMain(args):
+  """`loading_trace_analyzer.py requests` Command line tool entry point.
+
+  Args:
+    args: Command line parsed arguments.
+
+  Example:
+    Keep only blink.net trace event category:
+      ... prune -t "blink.net"
+
+    Keep only requestStart trace events:
+      ... prune -t "requestStart"
+
+    Keep only requestStart trace events of the blink.user_timing category:
+      ... prune -t "blink.user_timing:requestStart"
+
+    Keep only all blink trace event categories:
+      ... prune -t "^blink\.*"
+
+    Keep only requests' url member:
+      ... prune -r "^url$"
+
+    Keep only requests' url and document_url members:
+      ... prune -r "^./url$"
+
+    Keep only requests' url, document_url and initiator members:
+      ... prune -r "^./url$" "initiator"
   """
+  trace_json = json.load(args.loading_trace)
+
+  # Filter trace events.
+  regexes = [re.compile(f) for f in args.trace_filters]
+  events = []
+  for event in trace_json['tracing_track']['events']:
+    prune = True
+    for cat in event['cat'].split(','):
+      event_name = cat + ':' + event['name']
+      for regex in regexes:
+        if regex.search(event_name):
+          prune = False
+          break
+      if not prune:
+        events.append(event)
+        break
+  trace_json['tracing_track']['events'] = events
+
+  # Filter members of requests.
+  regexes = [re.compile(f) for f in args.request_member_filter]
+  for request in trace_json['request_track']['events']:
+    for key in request.keys():
+      prune = True
+      for regex in regexes:
+        if regex.search(key):
+          prune = False
+          break
+      if prune:
+        del request[key]
+
+  json.dump(trace_json, args.output, indent=args.indent)
+  return 0
+
+
+def main(command_line_args):
+  """Command line tool entry point."""
   args = _ArgumentParser().parse_args(command_line_args)
   if args.subcommand == 'requests':
     try:
@@ -91,6 +169,8 @@ def main(command_line_args):
       sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
           where_statement[1], str(e)))
     return 1
+  elif args.subcommand == 'prune':
+    return _PruneMain(args)
   assert False
 
 
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 85dd48d..057ac0f 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -56,7 +56,7 @@ class PrefetchSimulationView(object):
     self.postload_msec = user_lens.PostloadTimeMsec()
     self.graph = dependency_graph.RequestDependencyGraph(
         requests, dependencies_lens, node_class=RequestNode)
-    preloaded_requests = [r.request_id for r in self._PreloadedRequests(
+    preloaded_requests = [r.request_id for r in self.PreloadedRequests(
         requests[0], dependencies_lens, trace)]
     self._AnnotateNodes(self.graph.graph.Nodes(), preloaded_requests,
                         critical_requests_ids)
@@ -102,8 +102,8 @@ class PrefetchSimulationView(object):
       node.before = node.request.request_id in critical_requests_ids
 
   @classmethod
-  def _ParserDiscoverableRequests(
-      cls, dependencies_lens, request, recurse=False):
+  def ParserDiscoverableRequests(
+      cls, request, dependencies_lens, recurse=False):
     """Returns a list of requests IDs dicovered by the parser.
 
     Args:
@@ -128,7 +128,7 @@ class PrefetchSimulationView(object):
         [dependencies_lens.GetRedirectChain(r) for r in requests]))
 
   @classmethod
-  def _PreloadedRequests(cls, request, dependencies_lens, trace):
+  def PreloadedRequests(cls, request, dependencies_lens, trace):
     """Returns the requests that have been preloaded from a given request.
 
     This list is the set of request that are:
@@ -142,7 +142,7 @@ class PrefetchSimulationView(object):
 
     Returns:
       A list of Request. Does not include the root request. This list is a
-      subset of the one returned by _ParserDiscoverableRequests().
+      subset of the one returned by ParserDiscoverableRequests().
     """
     # Preload step events are emitted in ResourceFetcher::preloadStarted().
     resource_events = trace.tracing_track.Filter(
@@ -155,8 +155,8 @@ class PrefetchSimulationView(object):
       preload_event = resource_events.EventFromStep(preload_step_event)
       if preload_event:
         preloaded_urls.add(preload_event.args['url'])
-    parser_requests = cls._ParserDiscoverableRequests(
-        dependencies_lens, request)
+    parser_requests = cls.ParserDiscoverableRequests(
+        request, dependencies_lens)
     preloaded_root_requests = filter(
         lambda r: r.url in preloaded_urls, parser_requests)
     # We can actually fetch the whole redirect chain.
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index e222bf5..be97809 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -25,8 +25,8 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
 
   def testParserDiscoverableRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    discovered_requests = PrefetchSimulationView._ParserDiscoverableRequests(
-        self.dependencies_lens, first_request)
+    discovered_requests = PrefetchSimulationView.ParserDiscoverableRequests(
+        first_request, self.dependencies_lens)
     self.assertListEqual(
         [TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
@@ -34,7 +34,7 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
 
   def testPreloadedRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+    preloaded_requests = PrefetchSimulationView.PreloadedRequests(
         first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([first_request], preloaded_requests)
     self._SetUp(
@@ -44,7 +44,7 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
          {'args': {'step': 'Preload'}, 'cat': 'blink.net',
           'id': '0xaf9f14fa9dd6c314', 'name': 'Resource', 'ph': 'T',
           'ts': 12, 'pid': 12, 'tid': 12}])
-    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+    preloaded_requests = PrefetchSimulationView.PreloadedRequests(
         first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
diff --git a/loading/request_track.py b/loading/request_track.py
index 47b874f..bf19fbb 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -333,6 +333,9 @@ class RequestTrack(devtools_monitor.Track):
                       % len(self._requests_in_flight))
     return self._requests
 
+  def GetFirstResourceRequest(self):
+    return self.GetEvents()[0]
+
   def GetFirstRequestMillis(self):
     """Find the canonical start time for this track.
 
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 3fac221..5f61aac 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -15,6 +15,7 @@ import argparse
 import csv
 import logging
 import os
+import shutil
 import sys
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -28,11 +29,13 @@ from pylib import constants
 import devil_chromium
 
 import chrome_cache
+import common_util
 import emulation
 import options
 import sandwich_metrics
 import sandwich_misc
 from sandwich_runner import SandwichRunner
+from trace_test.webserver_test import WebServer
 
 
 # Use options layer to access constants.
@@ -137,6 +140,10 @@ def _ArgumentParser():
   filter_cache_parser.add_argument('--cache-archive', type=str, required=True,
                                    dest='cache_archive_path',
                                    help='Path of the cache archive to filter.')
+  filter_cache_parser.add_argument('--subresource-discoverer', required=True,
+      help='Strategy for populating the cache with a subset of resources, '
+           'according to the way they can be discovered',
+      choices=sandwich_misc.SUBRESOURCE_DISCOVERERS)
   filter_cache_parser.add_argument('--output', type=str, required=True,
                                    dest='output_cache_archive_path',
                                    help='Path of filtered cache archive.')
@@ -147,11 +154,24 @@ def _ArgumentParser():
           'list the ones discoverable by the HTML pre-scanner for that given ' +
           'url.')
 
+  # Record test trace subcommand.
+  record_trace_parser = subparsers.add_parser('record-test-trace',
+      help='Record a test trace using the trace_test.webserver_test.')
+  record_trace_parser.add_argument('--source-dir', type=str, required=True,
+                                   help='Base path where the files are opened'
+                                        'by the web server.')
+  record_trace_parser.add_argument('--page', type=str, required=True,
+                                   help='Source page in source-dir to navigate '
+                                        'to.')
+  record_trace_parser.add_argument('-o', '--output', type=str, required=True,
+                                   help='Output path of the generated trace.')
+
   return parser
 
 
 def _RecordWprMain(args):
-  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner = SandwichRunner()
+  sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.wpr_record = True
   sandwich_runner.PrintConfig()
@@ -162,7 +182,8 @@ def _RecordWprMain(args):
 
 
 def _CreateCacheMain(args):
-  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner = SandwichRunner()
+  sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.cache_operation = 'save'
   sandwich_runner.PrintConfig()
@@ -173,7 +194,8 @@ def _CreateCacheMain(args):
 
 
 def _RunJobMain(args):
-  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner = SandwichRunner()
+  sandwich_runner.LoadJob(args.job)
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.PrintConfig()
   sandwich_runner.Run()
@@ -196,8 +218,8 @@ def _ExtractMetricsMain(args):
 def _FilterCacheMain(args):
   whitelisted_urls = set()
   for loading_trace_path in args.loading_trace_paths:
-    whitelisted_urls.update(
-        sandwich_misc.ExtractParserDiscoverableResources(loading_trace_path))
+    whitelisted_urls.update(sandwich_misc.ExtractDiscoverableUrls(
+        loading_trace_path, args.subresource_discoverer))
   if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
     os.makedirs(os.path.dirname(args.output_cache_archive_path))
   chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
@@ -206,6 +228,22 @@ def _FilterCacheMain(args):
   return 0
 
 
+def _RecordWebServerTestTrace(args):
+  with common_util.TemporaryDirectory() as out_path:
+    sandwich_runner = SandwichRunner()
+    # Reuse the WPR's forwarding to access the webpage from Android.
+    sandwich_runner.wpr_record = True
+    sandwich_runner.wpr_archive_path = os.path.join(out_path, 'wpr')
+    sandwich_runner.trace_output_directory = os.path.join(out_path, 'run')
+    with WebServer.Context(
+        source_dir=args.source_dir, communication_dir=out_path) as server:
+      address = server.Address()
+      sandwich_runner.urls = ['http://%s/%s' % (address, args.page)]
+      sandwich_runner.Run()
+    shutil.copy(os.path.join(out_path, 'run', '0', 'trace.json'), args.output)
+  return 0
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -226,6 +264,8 @@ def main(command_line_args):
     return _ExtractMetricsMain(args)
   if args.subcommand == 'filter-cache':
     return _FilterCacheMain(args)
+  if args.subcommand == 'record-test-trace':
+    return _RecordWebServerTestTrace(args)
   assert False
 
 
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
index e30232c..1a9e96d 100644
--- a/loading/sandwich_metrics.py
+++ b/loading/sandwich_metrics.py
@@ -29,7 +29,16 @@ import loading_trace as loading_trace_module
 import tracing
 
 
-CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
+# List of selected trace event categories when running chrome.
+CATEGORIES = [
+    # Need blink network trace events for prefetch_view.PrefetchSimulationView
+    'blink.net',
+
+    # Need to get mark trace events for _GetWebPageTrackedEvents()
+    'blink.user_timing',
+
+    # Need to memory dump trace event for _GetBrowserDumpEvents()
+    'disabled-by-default-memory-infra']
 
 CSV_FIELD_NAMES = [
     'id',
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
index a15f5ab..13988c3 100644
--- a/loading/sandwich_misc.py
+++ b/loading/sandwich_misc.py
@@ -4,9 +4,27 @@
 
 import logging
 
+from loading_trace import LoadingTrace
+from prefetch_view import PrefetchSimulationView
+from request_dependencies_lens import RequestDependencyLens
+from user_satisfied_lens import FirstContentfulPaintLens
 import wpr_backend
-import loading_trace
-import request_dependencies_lens
+
+
+# Prefetches the first resource following the redirection chain.
+REDIRECTED_MAIN_DISCOVERER = 'redirected-main'
+
+# All resources which are fetched from the main document and their redirections.
+PARSER_DISCOVERER = 'parser',
+
+# Simulation of HTMLPreloadScanner on the main document and their redirections.
+HTML_PRELOAD_SCANNER_DISCOVERER = 'html-scanner',
+
+SUBRESOURCE_DISCOVERERS = set([
+  REDIRECTED_MAIN_DISCOVERER,
+  PARSER_DISCOVERER,
+  HTML_PRELOAD_SCANNER_DISCOVERER
+])
 
 
 def PatchWpr(wpr_archive_path):
@@ -44,38 +62,55 @@ def PatchWpr(wpr_archive_path):
   wpr_archive.Persist()
 
 
-def ExtractParserDiscoverableResources(loading_trace_path):
-  """Extracts the parser discoverable resources from a loading trace.
+def ExtractDiscoverableUrls(loading_trace_path, subresource_discoverer):
+  """Extracts discoverable resource urls from a loading trace according to a
+  sub-resource discoverer.
 
   Args:
     loading_trace_path: The loading trace's path.
+    subresource_discoverer: The sub-resources discoverer that should white-list
+      the resources to keep in cache for the NoState-Prefetch benchmarks.
 
   Returns:
     A set of urls.
   """
-  whitelisted_urls = set()
+  assert subresource_discoverer in SUBRESOURCE_DISCOVERERS, \
+      'unknown prefetch simulation {}'.format(subresource_discoverer)
+
+  # Load trace and related infos.
   logging.info('loading %s' % loading_trace_path)
-  trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
-  requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
-  deps = requests_lens.GetRequestDependencies()
-
-  main_resource_request = deps[0][0]
-  logging.info('white-listing %s' % main_resource_request.url)
-  whitelisted_urls.add(main_resource_request.url)
-  for (first, second, reason) in deps:
+  trace = LoadingTrace.FromJsonFile(loading_trace_path)
+  dependencies_lens = RequestDependencyLens(trace)
+  first_resource_request = trace.request_track.GetFirstResourceRequest()
+
+  # Build the list of discovered requests according to the desired simulation.
+  discovered_requests = []
+  if subresource_discoverer == REDIRECTED_MAIN_DISCOVERER:
+    discovered_requests = \
+        [dependencies_lens.GetRedirectChain(first_resource_request)[-1]]
+  elif subresource_discoverer == PARSER_DISCOVERER:
+    discovered_requests = PrefetchSimulationView.ParserDiscoverableRequests(
+        first_resource_request, dependencies_lens)
+  elif subresource_discoverer == HTML_PRELOAD_SCANNER_DISCOVERER:
+    discovered_requests = PrefetchSimulationView.PreloadedRequests(
+        first_resource_request, dependencies_lens, trace)
+  else:
+    assert False
+
+  # Prune out data:// requests.
+  whitelisted_urls = set()
+  logging.info('white-listing %s' % first_resource_request.url)
+  whitelisted_urls.add(first_resource_request.url)
+  for request in discovered_requests:
     # Work-around where the protocol may be none for an unclear reason yet.
     # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
     #   this work-around.
-    if not second.protocol:
-      logging.info('ignoring %s (no protocol)' % second.url)
+    if not request.protocol:
+      logging.warning('ignoring %s (no protocol)' % request.url)
       continue
     # Ignore data protocols.
-    if not second.protocol.startswith('http'):
-      logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
-          second.url, second.protocol))
+    if not request.protocol.startswith('http'):
       continue
-    if (first.request_id == main_resource_request.request_id and
-        reason == 'parser' and second.url not in whitelisted_urls):
-      logging.info('white-listing %s' % second.url)
-      whitelisted_urls.add(second.url)
+    logging.info('white-listing %s' % request.url)
+    whitelisted_urls.add(request.url)
   return whitelisted_urls
diff --git a/loading/sandwich_misc_unittest.py b/loading/sandwich_misc_unittest.py
new file mode 100644
index 0000000..f75288c
--- /dev/null
+++ b/loading/sandwich_misc_unittest.py
@@ -0,0 +1,45 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import unittest
+import urlparse
+
+import sandwich_misc
+
+
+LOADING_DIR = os.path.dirname(__file__)
+TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
+
+
+class SandwichMiscTest(unittest.TestCase):
+  _TRACE_PATH = os.path.join(TEST_DATA_DIR, 'scanner_vs_parser.trace')
+
+  def GetResourceUrl(self, path):
+    return urlparse.urljoin('http://l/', path)
+
+  def testRedirectedMainWhitelisting(self):
+    urls_set_ref = set([self.GetResourceUrl('./')])
+    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.REDIRECTED_MAIN_DISCOVERER)
+    self.assertEquals(urls_set_ref, urls_set)
+
+  def testParserDiscoverableWhitelisting(self):
+    urls_set_ref = set([self.GetResourceUrl('./'),
+                        self.GetResourceUrl('0.png'),
+                        self.GetResourceUrl('1.png')])
+    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.PARSER_DISCOVERER)
+    self.assertEquals(urls_set_ref, urls_set)
+
+  def testHTMLPreloadScannerWhitelisting(self):
+    urls_set_ref = set([self.GetResourceUrl('./'),
+                        self.GetResourceUrl('0.png')])
+    urls_set = sandwich_misc.ExtractDiscoverableUrls(
+        self._TRACE_PATH, sandwich_misc.HTML_PRELOAD_SCANNER_DISCOVERER)
+    self.assertEquals(urls_set_ref, urls_set)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
index 5dd7ba2..ebef772 100644
--- a/loading/sandwich_runner.py
+++ b/loading/sandwich_runner.py
@@ -78,7 +78,7 @@ class SandwichRunner(object):
   command line flags have `dest` parameter set to existing runner members.
   """
 
-  def __init__(self, job_name):
+  def __init__(self):
     """Configures a sandwich runner out of the box.
 
     Public members are meant to be configured as wished before calling Run().
@@ -97,7 +97,7 @@ class SandwichRunner(object):
     self.disable_wpr_script_injection = False
 
     # The job name. Is str.
-    self.job_name = job_name
+    self.job_name = '__unknown_job'
 
     # Number of times to repeat the job.
     self.job_repeat = 1
@@ -112,7 +112,7 @@ class SandwichRunner(object):
     self.trace_output_directory = None
 
     # List of urls to run.
-    self.urls = _ReadUrlsFromJobDescription(job_name)
+    self.urls = []
 
     # Configures whether to record speed-index video.
     self.record_video = False
@@ -126,6 +126,10 @@ class SandwichRunner(object):
     self._chrome_ctl = None
     self._local_cache_directory_path = None
 
+  def LoadJob(self, job_name):
+    self.job_name = job_name
+    self.urls = _ReadUrlsFromJobDescription(job_name)
+
   def PullConfigFromArgs(self, args):
     """Configures the sandwich runner from parsed command line argument.
 
diff --git a/loading/testdata/scanner_vs_parser.trace b/loading/testdata/scanner_vs_parser.trace
new file mode 100644
index 0000000..427cdc8
--- /dev/null
+++ b/loading/testdata/scanner_vs_parser.trace
@@ -0,0 +1,159 @@
+{
+  "metadata": {},
+  "page_track": {
+    "events": []
+  },
+  "request_track": {
+    "events": [
+      {
+        "initiator": {
+          "type": "other"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/"
+      },
+      {
+        "initiator": {
+          "lineNumber": 28,
+          "type": "parser",
+          "url": "http://l/"
+        },
+        "protocol": "data",
+        "url": "data:image/png;base64,iVBO[PRUNED]"
+      },
+      {
+        "initiator": {
+          "lineNumber": 21,
+          "type": "parser",
+          "url": "http://l/"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/0.png"
+      },
+      {
+        "initiator": {
+          "type": "parser",
+          "url": "http://l/"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/1.png"
+      },
+      {
+        "initiator": {
+          "type": "other"
+        },
+        "protocol": "http/1.0",
+        "url": "http://l/favicon.ico"
+      }
+    ],
+    "metadata": {
+      "duplicates_count": 0,
+      "inconsistent_initiators": 0
+    }
+  },
+  "tracing_track": {
+    "events": [
+      {
+        "args": {
+          "priority": 4,
+          "url": "http://l/"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "S",
+        "pid": 3,
+        "ts": 1213697828839
+      },
+      {
+        "args": {},
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "F",
+        "pid": 3,
+        "ts": 1213697889955
+      },
+      {
+        "args": {
+          "priority": 1,
+          "url": "http://l/0.png"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "S",
+        "pid": 3,
+        "ts": 1213697891911
+      },
+      {
+        "args": {
+          "step": "Preload"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697892658
+      },
+      {
+        "args": {
+          "priority": 1,
+          "url": "http://l/1.png"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "S",
+        "pid": 3,
+        "ts": 1213697934273
+      },
+      {
+        "args": {},
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "F",
+        "pid": 3,
+        "ts": 1213697943810
+      },
+      {
+        "args": {
+          "priority": 3,
+          "step": "ChangePriority"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697984606
+      },
+      {
+        "args": {
+          "priority": 3,
+          "step": "ChangePriority"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697984875
+      },
+      {
+        "args": {
+          "priority": 3,
+          "step": "ChangePriority"
+        },
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "T",
+        "pid": 3,
+        "ts": 1213697985346
+      },
+      {
+        "args": {},
+        "cat": "blink.net",
+        "name": "Resource",
+        "ph": "F",
+        "pid": 3,
+        "ts": 1213698035637
+      }
+    ]
+  },
+  "url": "http://l/"
+}

commit 51d74298761b0430ce97a3a2c50496704d6eeebc
Author: blundell <blundell@chromium.org>
Date:   Thu Apr 7 07:03:15 2016 -0700

    tools/android/loading: Write trace database when generating traces on GCE
    
    This CL adds the generation of a trace database file when generating traces
    via GCE. The trace database file is located in
    "gs://path/to/traces/trace_database.json" and indexes all of the
    successfully-generated traces in "gs://path/to/traces". To access the
    database in Python, do the following:
    
    from loading_trace_database import LoadingTraceDatabase
    db = LoadingTraceDatabase.FromJsonFileInGoogleStorage(
        "gs://path/to/traces/trace_database.json")
    
    Review URL: https://codereview.chromium.org/1866103003
    
    Cr-Original-Commit-Position: refs/heads/master@{#385745}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f7564ce44cc27d4a39890c2bca6e073c710dfe4c

diff --git a/loading/gce/main.py b/loading/gce/main.py
index c20f0ae..dc1da8e 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -22,6 +22,7 @@ sys.path.insert(0,
 import controller
 import loading_trace
 import options
+from loading_trace_database import LoadingTraceDatabase
 
 
 class ServerApp(object):
@@ -114,7 +115,8 @@ class ServerApp(object):
       log_filename: Name of the file where standard output and errors are logged
 
     Returns:
-      True if the trace was generated successfully.
+      A dictionary of metadata about the trace, including a 'succeeded' field
+      indicating whether the trace was successfully generated.
     """
     try:
       os.remove(filename)  # Remove any existing trace for this URL.
@@ -127,7 +129,7 @@ class ServerApp(object):
     old_stdout = sys.stdout
     old_stderr = sys.stderr
 
-    succeeded = True
+    trace_metadata = { 'succeeded' : False, 'url' : url }
     try:
       with open(log_filename, 'w') as sys.stdout:
         sys.stderr = sys.stdout
@@ -146,8 +148,9 @@ class ServerApp(object):
           connection.ClearCache()
           trace = loading_trace.LoadingTrace.RecordUrlNavigation(
               url, connection, chrome_ctl.ChromeMetadata())
+          trace_metadata['succeeded'] = True
+          trace_metadata.update(trace.ToJsonDict()[trace._METADATA_KEY])
     except Exception as e:
-      succeeded = False
       sys.stderr.write(e)
 
     sys.stdout = old_stdout
@@ -156,7 +159,7 @@ class ServerApp(object):
     with open(filename, 'w') as f:
       json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
 
-    return succeeded
+    return trace_metadata
 
   def _GetCurrentTaskCount(self):
     """Returns the number of remaining tasks. Thread safe."""
@@ -183,6 +186,8 @@ class ServerApp(object):
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
 
+    trace_database = LoadingTraceDatabase({})
+
     # TODO(blundell): Fix this up.
     logs_dir = self._base_path_in_bucket + 'analyze_logs/'
     log_filename = 'analyze.log'
@@ -194,10 +199,15 @@ class ServerApp(object):
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
         remote_filename = local_filename + '/' + str(repeat)
-        if self._GenerateTrace(
-            url, emulate_device, emulate_network, local_filename, log_filename):
+        trace_metadata = self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename)
+        if trace_metadata['succeeded']:
           print 'Uploading: %s' % remote_filename
-          self._UploadFile(local_filename, traces_dir + remote_filename)
+          remote_trace_location = traces_dir + remote_filename
+          self._UploadFile(local_filename, remote_trace_location)
+          full_cloud_storage_path = ('gs://' + self._bucket_name + '/' +
+              remote_trace_location)
+          trace_database.AddTrace(full_cloud_storage_path, trace_metadata)
         else:
           print 'Trace generation failed for URL: %s' % url
           self._tasks_lock.acquire()
@@ -212,6 +222,9 @@ class ServerApp(object):
       url = self._tasks.pop()
       self._tasks_lock.release()
 
+    self._UploadString(json.dumps(trace_database.ToJsonDict(), indent=2),
+                       traces_dir + 'trace_database.json')
+
     if len(self._failed_tasks) > 0:
       print 'Uploading failing URLs'
       self._UploadString(json.dumps(self._failed_tasks, indent=2),
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index ac8f855..2ef33b0 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -14,6 +14,11 @@ class LoadingTraceDatabase(object):
        about those traces."""
     self._traces_dict = traces_dict
 
+  def AddTrace(self, filename, trace_dict):
+    """Adds a mapping from |filename| to |trace_dict| into the database."""
+    assert filename not in self._traces_dict
+    self._traces_dict[filename] = trace_dict
+
   def GetTraceFilesForURL(self, url):
     """Given a URL, returns the set of filenames of traces that were generated
        for this URL."""
diff --git a/loading/loading_trace_database_unittest.py b/loading/loading_trace_database_unittest.py
index 775638c..31dc9b3 100644
--- a/loading/loading_trace_database_unittest.py
+++ b/loading/loading_trace_database_unittest.py
@@ -32,6 +32,14 @@ class LoadingTraceDatabaseUnittest(unittest.TestCase):
     self.assertEqual(
         self._JSON_DATABASE, self.database.ToJsonDict())
 
+  def testAddTrace(self):
+    dummy_url = "http://dummy.com"
+    new_trace_file = "traces/new_trace.json"
+    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url), [])
+    self.database.AddTrace(new_trace_file, {"url" : dummy_url})
+    self.assertEqual(self.database.GetTraceFilesForURL(dummy_url),
+                     [new_trace_file])
+
 
 if __name__ == '__main__':
   unittest.main()

commit 831e96e58edf8af2eaa0ead5fd1d4cd49163da8e
Author: lizeb <lizeb@chromium.org>
Date:   Thu Apr 7 05:17:59 2016 -0700

    clovis: Fix a typo (redundant self parameter).
    
    Also make a class inherit from object.
    
    Review URL: https://codereview.chromium.org/1868923004
    
    Cr-Original-Commit-Position: refs/heads/master@{#385721}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2a9c63b81cd26207ddfd14674f7c492887583eec

diff --git a/loading/controller.py b/loading/controller.py
index b6616f9..567bcb7 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -346,7 +346,7 @@ class LocalChromeController(ChromeControllerBase):
     the value of stdout/stderr based on the value of OPTIONS.local_noisy."""
     stdout = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     stderr = stdout
-    return self.OpenWithRedirection(self, stdout, stderr)
+    return self.OpenWithRedirection(stdout, stderr)
 
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index d2e4910..ac8f855 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -7,8 +7,8 @@
 import json
 from google_storage_util import ReadFromGoogleStorage
 
-class LoadingTraceDatabase:
 
+class LoadingTraceDatabase(object):
   def __init__(self, traces_dict):
     """traces_dict is a dictionary mapping filenames of traces to metadata
        about those traces."""

commit c99ddca2895e08071438c31fd9b6f6983e4cad42
Author: droger <droger@chromium.org>
Date:   Thu Apr 7 03:37:44 2016 -0700

    tools/android/loading Experimental scripts for clovis validation
    
    This CL adds experimental scripts to collect and compare traces on a
    device and on GCE.
    
    Review URL: https://codereview.chromium.org/1865233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385705}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4af98546321dcd74207f781c9e37415ce4264e79

diff --git a/loading/unmaintained/README.md b/loading/unmaintained/README.md
new file mode 100644
index 0000000..295323b
--- /dev/null
+++ b/loading/unmaintained/README.md
@@ -0,0 +1,2 @@
+This directory contains unmaintained code that still has value, such as
+experimental or temporary scripts.
diff --git a/loading/unmaintained/gce_validation_collect.sh b/loading/unmaintained/gce_validation_collect.sh
new file mode 100755
index 0000000..41d8417
--- /dev/null
+++ b/loading/unmaintained/gce_validation_collect.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Takes a list of URLs (infile), and runs analyse.py on them in  parallel on a
+# device and on GCE, in a sychronized manner (the task is started on both
+# platforms at the same time).
+
+infile=$1
+outdir=$2
+instance_ip=$3
+repeat_count=$4
+
+for site in $(< $infile); do
+ echo $site
+ output_subdir=$(echo "$site"|tr "/:" "_")
+ echo 'Start remote task'
+ cat >urls.json << EOF
+ {
+  "urls" : [
+    "$site"
+  ],
+  "repeat_count" : "$repeat_count",
+  "emulate_device" : "Nexus 4"
+ }
+EOF
+
+ while [ "$(curl http://$instance_ip:8080/status)" != "Idle" ]; do
+   echo 'Waiting for instance to be ready, retry in 5s'
+   sleep 5
+ done
+ curl -X POST -d @urls.json http://$instance_ip:8080/set_tasks
+
+ echo 'Run on device'
+ mkdir $outdir/$output_subdir
+ for ((run=0;run<$repeat_count;++run)); do
+   echo '****'  $run
+   tools/android/loading/analyze.py log_requests \
+      --clear_cache \
+      --devtools_port 9222 \
+      --url $site \
+      --output $outdir/${output_subdir}/${run}
+ done
+done
diff --git a/loading/unmaintained/gce_validation_compare.sh b/loading/unmaintained/gce_validation_compare.sh
new file mode 100755
index 0000000..88e26df
--- /dev/null
+++ b/loading/unmaintained/gce_validation_compare.sh
@@ -0,0 +1,75 @@
+#!/bin/bash
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# gce_validation_compare.sh rootdir compare_filename
+#   root_dir: root directory for the experiment.
+#   compare_filename: file where the comparison breakdown is output.
+#
+# Computes core sets from GCE and device experiment resutls, and compare them.
+# The expected directory structure is:
+#
+# root_dir/
+#   cloud/
+#     url1/         # Can be any name as long as it is mirrored under device/.
+#       run1.trace  # Can be any name.
+#       run2.trace
+#       ...
+#     url2/
+#     ...
+#   device/
+#     url1/
+#       run1.trace
+#       run2.trace
+#       ...
+#     url2/
+#     ...
+
+root_dir=$1
+compare_filename=$2
+
+rm $compare_filename
+
+# Check directory structure.
+if [ ! -d $root_dir/cloud ]; then
+  echo "$root_dir/cloud missing!"
+  exit 1
+fi
+
+if [ ! -d $root_dir/device ]; then
+  echo "$root_dir/device missing!"
+  exit 1
+fi
+
+for device_file in $root_dir/device/*/  ; do
+  cloud_file=$root_dir/cloud/$(basename $device_file)
+  if [ ! -d $cloud_file ]; then
+    echo "$cloud_file not found"
+  fi
+done
+
+for cloud_file in $root_dir/cloud/*/  ; do
+  device_file=$root_dir/device/$(basename $device_file)
+  if [ ! -d $device_file ]; then
+    echo "$device_file not found"
+  fi
+done
+
+# Loop through all the subdirectories, compute the core sets and compare them.
+for device_file in $root_dir/device/*/  ; do
+  base_name=$(basename $device_file)
+  python tools/android/loading/core_set.py page_core --sets device/$base_name \
+    --output $device_file/core_set.json --prefix $device_file
+
+  cloud_file=$root_dir/cloud/$base_name
+  if [ -d $cloud_file ]; then
+    python tools/android/loading/core_set.py page_core --sets cloud/$base_name \
+      --output $cloud_file/core_set.json --prefix $cloud_file
+
+    compare_result=$(python tools/android/loading/core_set.py compare \
+      --a $cloud_file/core_set.json --b $device_file/core_set.json)
+    compare_result+=" $base_name"
+    echo $compare_result >> $compare_filename
+  fi
+done

commit f01c7a0340c2a838d42611f69530fc228aa79ad8
Author: lizeb <lizeb@chromium.org>
Date:   Wed Apr 6 09:22:26 2016 -0700

    clovis: Make PrefetchSimulationView serializable.
    
    PrefetchSimulationView keeps a reference to the trace, making it heavy
    in memory, and costly to instanciate (hundreds of MB, and up to
    10s). This CL only keeps the relevant data, that can then be stored in a
    JSON file.
    
    Review URL: https://codereview.chromium.org/1863613002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385474}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9faed16cc5a57221063baff2d1bad04a8cf396b1

diff --git a/loading/common_util.py b/loading/common_util.py
index 5b62ce0..9384df0 100644
--- a/loading/common_util.py
+++ b/loading/common_util.py
@@ -24,3 +24,40 @@ def PollFor(condition, condition_name, interval=5):
     if result:
       return result
     time.sleep(interval)
+
+
+def SerializeAttributesToJsonDict(json_dict, instance, attributes):
+  """Adds the |attributes| from |instance| to a |json_dict|.
+
+  Args:
+    json_dict: (dict) Dict to update.
+    instance: (object) instance to take the values from.
+    attributes: ([str]) List of attributes to serialize.
+
+  Returns:
+    json_dict
+  """
+  json_dict.update({attr: getattr(instance, attr) for attr in attributes})
+  return json_dict
+
+
+def DeserializeAttributesFromJsonDict(json_dict, instance, attributes):
+  """Sets a list of |attributes| in |instance| according to their value in
+    |json_dict|.
+
+  Args:
+    json_dict: (dict) Dict containing values dumped by
+               SerializeAttributesToJsonDict.
+    instance: (object) instance to modify.
+    attributes: ([str]) List of attributes to set.
+
+  Raises:
+    AttributeError if one of the attribute doesn't exist in |instance|.
+
+  Returns:
+    instance
+  """
+  for attr in attributes:
+    getattr(instance, attr) # To raise AttributeError if attr doesn't exist.
+    setattr(instance, attr, json_dict[attr])
+  return instance
diff --git a/loading/common_util_unittest.py b/loading/common_util_unittest.py
new file mode 100644
index 0000000..8984881
--- /dev/null
+++ b/loading/common_util_unittest.py
@@ -0,0 +1,51 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import common_util
+
+
+class SerializeAttributesTestCase(unittest.TestCase):
+  class Foo(object):
+    def __init__(self, foo_fighters, whisky_bar):
+      # Pylint doesn't like foo and bar, but I guess musical references are OK.
+      self.foo_fighters = foo_fighters
+      self.whisky_bar = whisky_bar
+
+  def testSerialization(self):
+    foo_fighters = self.Foo('1', 2)
+    json_dict = common_util.SerializeAttributesToJsonDict(
+        {}, foo_fighters, ['foo_fighters', 'whisky_bar'])
+    self.assertDictEqual({'foo_fighters': '1', 'whisky_bar': 2}, json_dict)
+    # Partial update
+    json_dict = common_util.SerializeAttributesToJsonDict(
+        {'baz': 42}, foo_fighters, ['whisky_bar'])
+    self.assertDictEqual({'baz': 42, 'whisky_bar': 2}, json_dict)
+    # Non-existing attribute.
+    with self.assertRaises(AttributeError):
+      json_dict = common_util.SerializeAttributesToJsonDict(
+          {}, foo_fighters, ['foo_fighters', 'whisky_bar', 'baz'])
+
+  def testDeserialization(self):
+    foo_fighters = self.Foo('hello', 'world')
+    json_dict = {'foo_fighters': 12, 'whisky_bar': 42}
+    # Partial.
+    foo_fighters = common_util.DeserializeAttributesFromJsonDict(
+        json_dict, foo_fighters, ['foo_fighters'])
+    self.assertEqual(12, foo_fighters.foo_fighters)
+    self.assertEqual('world', foo_fighters.whisky_bar)
+    # Complete.
+    foo_fighters = common_util.DeserializeAttributesFromJsonDict(
+        json_dict, foo_fighters, ['foo_fighters', 'whisky_bar'])
+    self.assertEqual(42, foo_fighters.whisky_bar)
+    # Non-existing attribute.
+    with self.assertRaises(AttributeError):
+      json_dict['baz'] = 'bad'
+      foo_fighters = common_util.DeserializeAttributesFromJsonDict(
+          json_dict, foo_fighters, ['foo_fighters', 'whisky_bar', 'baz'])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index 955177c..da55ad2 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -7,25 +7,54 @@
 import logging
 import sys
 
+import common_util
 import graph
 import request_track
 
 
 class RequestNode(graph.Node):
-  def __init__(self, request):
+  def __init__(self, request=None):
     super(RequestNode, self).__init__()
     self.request = request
-    self.cost = request.Cost()
+    self.cost = request.Cost() if request else None # Deserialization.
+
+  def ToJsonDict(self):
+    json_dict = super(RequestNode, self).ToJsonDict()
+    json_dict.update({'request': self.request.ToJsonDict()})
+    return json_dict
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = super(RequestNode, cls).FromJsonDict(json_dict)
+    result.request = request_track.Request.FromJsonDict(json_dict['request'])
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['cost'])
 
 
 class Edge(graph.Edge):
-  def __init__(self, from_node, to_node, reason):
+  def __init__(self, from_node, to_node, reason=None):
     super(Edge, self).__init__(from_node, to_node)
     self.reason = reason
+    self.cost = None
+    self.is_timing = None
+    if from_node is None: # Deserialization.
+      return
+    self.reason = reason
     self.cost = request_track.TimeBetween(
         self.from_node.request, self.to_node.request, self.reason)
     self.is_timing = False
 
+  def ToJsonDict(self):
+    result = {}
+    return common_util.SerializeAttributesToJsonDict(
+        result, self, ['reason', 'cost', 'is_timing'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = cls(None, None, None)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['reason', 'cost', 'is_timing'])
+
 
 class RequestDependencyGraph(object):
   """Request dependency graph."""
@@ -45,6 +74,12 @@ class RequestDependencyGraph(object):
       node_class: (subclass of RequestNode)
       edge_class: (subclass of Edge)
     """
+    self._requests = None
+    self._first_request_node = None
+    self._deps_graph = None
+    self._nodes_by_id = None
+    if requests is None: # Deserialization.
+      return
     assert issubclass(node_class, RequestNode)
     assert issubclass(edge_class, Edge)
     self._requests = requests
@@ -175,3 +210,22 @@ class RequestDependencyGraph(object):
         self._deps_graph.UpdateEdge(
             current, edges_by_end_time[end_mark].to_node,
             current.to_node)
+
+  def ToJsonDict(self):
+    result = {'graph': self.graph.ToJsonDict()}
+    result['requests'] = [r.ToJsonDict() for r in self._requests]
+    return result
+
+  @classmethod
+  def FromJsonDict(cls, json_dict, node_class, edge_class):
+    result = cls(None, None)
+    graph_dict = json_dict['graph']
+    g = graph.DirectedGraph.FromJsonDict(graph_dict, node_class, edge_class)
+    result._requests = [request_track.Request.FromJsonDict(r)
+                        for r in json_dict['requests']]
+    result._nodes_by_id = {node.request.request_id: node
+                           for node in g.Nodes()}
+    result._first_request_node = result._nodes_by_id[
+        result._requests[0].request_id]
+    result._deps_graph = g
+    return result
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
index 4f60995..68433f5 100644
--- a/loading/dependency_graph_unittest.py
+++ b/loading/dependency_graph_unittest.py
@@ -16,13 +16,15 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     super(RequestDependencyGraphTestCase, self).setUp()
     self.trace = TestRequests.CreateLoadingTrace()
 
-  def testUpdateRequestCost(self):
+  def testUpdateRequestCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
     requests[0].timing = request_track.TimingFromDict(
         {'requestTime': 12, 'loadingFinished': 10})
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
     g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     self.assertEqual(10, g.Cost())
     request_id = requests[0].request_id
     g.UpdateRequestsCost({request_id: 100})
@@ -30,7 +32,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     g.UpdateRequestsCost({'unrelated_id': 1000})
     self.assertEqual(100, g.Cost())
 
-  def testCost(self):
+  def testCost(self, serialize=False):
     requests = self.trace.request_track.GetEvents()
     for (index, request) in enumerate(requests):
       request.timing = request_track.TimingFromDict(
@@ -39,6 +41,8 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
     g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     # First redirect -> Second redirect -> Redirected Request -> Request ->
     # JS Request 2
     self.assertEqual(7010, g.Cost())
@@ -50,7 +54,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     g.UpdateRequestsCost({TestRequests.SECOND_REDIRECT_REQUEST.request_id: 0})
     self.assertEqual(6990, g.Cost())
 
-  def testHandleTimingDependencies(self):
+  def testHandleTimingDependencies(self, serialize=False):
     # Timing adds node 1 as a parent to 2 but not 3.
     requests = [
         test_utils.MakeRequest(0, 'null', 100, 110, 110,
@@ -65,6 +69,8 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
         test_utils.MakeRequest(5, 2, 122, 126, 126)]
 
     g = self._GraphFromRequests(requests)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     self.assertSetEqual(
         self._Successors(g, requests[0]), set([requests[1], requests[3]]))
     self.assertSetEqual(
@@ -106,7 +112,7 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     self.assertSetEqual(self._Successors(g, requests[5]), set())
     self.assertSetEqual(self._Successors(g, requests[6]), set([requests[3]]))
 
-  def testHandleTimingDependenciesImages(self):
+  def testHandleTimingDependenciesImages(self, serialize=False):
     # If we're all image types, then we shouldn't split by timing.
     requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
                 test_utils.MakeRequest(1, 0, 115, 120, 120),
@@ -117,6 +123,8 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     for r in requests:
       r.response_headers['Content-Type'] = 'image/gif'
     g = self._GraphFromRequests(requests)
+    if serialize:
+      g = self._SerializeDeserialize(g)
     self.assertSetEqual(self._Successors(g, requests[0]),
                         set([requests[1], requests[2], requests[3]]))
     self.assertSetEqual(self._Successors(g, requests[1]), set())
@@ -126,6 +134,19 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     self.assertSetEqual(self._Successors(g, requests[4]), set())
     self.assertSetEqual(self._Successors(g, requests[5]), set())
 
+  def testSerializeDeserialize(self):
+    # Redo the tests, with a graph that has been serialized / deserialized.
+    self.testUpdateRequestCost(True)
+    self.testCost(True)
+    self.testHandleTimingDependencies(True)
+    self.testHandleTimingDependenciesImages(True)
+
+  @classmethod
+  def _SerializeDeserialize(cls, g):
+    json_dict = g.ToJsonDict()
+    return dependency_graph.RequestDependencyGraph.FromJsonDict(
+        json_dict, dependency_graph.RequestNode, dependency_graph.Edge)
+
   @classmethod
   def _GraphFromRequests(cls, requests):
     trace = test_utils.LoadingTraceFromEvents(requests)
diff --git a/loading/graph.py b/loading/graph.py
index d051d58..7c2d275 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -6,6 +6,8 @@
 
 import collections
 
+import common_util
+
 
 class Node(object):
   """A node in a Graph.
@@ -16,6 +18,14 @@ class Node(object):
     """Create a new node."""
     self.cost = 0
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict({}, self, ['cost'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, cls(), ['cost'])
+
 
 class Edge(object):
   """Represents an edge in a graph."""
@@ -30,6 +40,16 @@ class Edge(object):
     self.to_node = to_node
     self.cost = 0
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict(
+        {}, self, ['from_node', 'to_node', 'cost'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = cls(None, None)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['from_node', 'to_node', 'cost'])
+
 
 class DirectedGraph(object):
   """Directed graph.
@@ -37,6 +57,10 @@ class DirectedGraph(object):
   A graph is identified by a list of nodes and a list of edges. It does not need
   to be acyclic, but then some methods will fail.
   """
+  __GRAPH_NODE_INDEX = '__graph_node_index'
+  __TO_NODE_INDEX = '__to_node_index'
+  __FROM_NODE_INDEX = '__from_node_index'
+
   def __init__(self, nodes, edges):
     """Builds a graph from a set of node and edges.
 
@@ -185,3 +209,40 @@ class DirectedGraph(object):
             else costliest_node, predecessors)
         path_list.insert(0, node)
     return max_cost
+
+  def ToJsonDict(self):
+    node_dicts = []
+    node_to_index = {node: index for (index, node) in enumerate(self._nodes)}
+    for (node, index) in node_to_index.items():
+      node_dict = node.ToJsonDict()
+      assert self.__GRAPH_NODE_INDEX not in node_dict
+      node_dict.update({self.__GRAPH_NODE_INDEX: index})
+      node_dicts.append(node_dict)
+    edge_dicts = []
+    for edge in self._edges:
+      edge_dict = edge.ToJsonDict()
+      assert self.__TO_NODE_INDEX not in edge_dict
+      assert self.__FROM_NODE_INDEX not in edge_dict
+      edge_dict.update({self.__TO_NODE_INDEX: node_to_index[edge.to_node],
+                        self.__FROM_NODE_INDEX: node_to_index[edge.from_node]})
+      edge_dicts.append(edge_dict)
+    return {'nodes': node_dicts, 'edges': edge_dicts}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict, node_class, edge_class):
+    """Returns an instance from a dict.
+
+    Note that the classes of the nodes and edges need to be specified here.
+    This is done to reduce the likelihood of error.
+    """
+    index_to_node = {
+        node_dict[cls.__GRAPH_NODE_INDEX]: node_class.FromJsonDict(node_dict)
+        for node_dict in json_dict['nodes']}
+    edges = []
+    for edge_dict in json_dict['edges']:
+      edge = edge_class.FromJsonDict(edge_dict)
+      edge.from_node = index_to_node[edge_dict[cls.__FROM_NODE_INDEX]]
+      edge.to_node = index_to_node[edge_dict[cls.__TO_NODE_INDEX]]
+      edges.append(edge)
+    result = DirectedGraph(index_to_node.values(), edges)
+    return result
diff --git a/loading/graph_unittest.py b/loading/graph_unittest.py
index e8ef761..e0e5f5b 100644
--- a/loading/graph_unittest.py
+++ b/loading/graph_unittest.py
@@ -7,42 +7,61 @@ import os
 import sys
 import unittest
 
+import common_util
 import graph
 
 
 class _IndexedNode(graph.Node):
-  def __init__(self, index):
+  def __init__(self, index=None):
     super(_IndexedNode, self).__init__()
     self.index = index
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict(
+        super(_IndexedNode, self).ToJsonDict(), self, ['index'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = super(_IndexedNode, cls).FromJsonDict(json_dict)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['index'])
+
 
 class GraphTestCase(unittest.TestCase):
   @classmethod
-  def MakeGraph(cls, count, edge_tuples):
+  def MakeGraph(cls, count, edge_tuples, serialize=False):
     """Makes a graph from a list of edges.
 
     Args:
       count: Number of nodes.
       edge_tuples: (from_index, to_index). Both indices must be in [0, count),
-                   and uniquely identify a node.
+                   and uniquely identify a node. Must be sorted
+                   lexicographically by node indices.
     """
     nodes = [_IndexedNode(i) for i in xrange(count)]
     edges = [graph.Edge(nodes[from_index], nodes[to_index])
              for (from_index, to_index) in edge_tuples]
-    return (nodes, edges, graph.DirectedGraph(nodes, edges))
+    g = graph.DirectedGraph(nodes, edges)
+    if serialize:
+      g = graph.DirectedGraph.FromJsonDict(
+          g.ToJsonDict(), _IndexedNode, graph.Edge)
+      nodes = sorted(g.Nodes(), key=operator.attrgetter('index'))
+      edges = sorted(g.Edges(), key=operator.attrgetter(
+          'from_node.index', 'to_node.index'))
+    return (nodes, edges, g)
 
   @classmethod
   def _NodesIndices(cls, g):
     return map(operator.attrgetter('index'), g.Nodes())
 
-  def testBuildGraph(self):
+  def testBuildGraph(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
     self.assertSetEqual(set(edges), set(g.Edges()))
 
@@ -72,14 +91,14 @@ class GraphTestCase(unittest.TestCase):
     self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
     self.assertEqual(5, len(g.Edges()))
 
-  def testUpdateEdge(self):
+  def testUpdateEdge(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     edge = edges[1]
     self.assertTrue(edge in g.OutEdges(nodes[0]))
     self.assertTrue(edge in g.InEdges(nodes[2]))
@@ -89,28 +108,28 @@ class GraphTestCase(unittest.TestCase):
     self.assertTrue(edge in g.OutEdges(nodes[2]))
     self.assertTrue(edge in g.InEdges(nodes[3]))
 
-  def testTopologicalSort(self):
+  def testTopologicalSort(self, serialize=False):
     (_, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     sorted_nodes = g.TopologicalSort()
     node_to_sorted_index = dict(zip(sorted_nodes, xrange(len(sorted_nodes))))
     for e in edges:
       self.assertTrue(
           node_to_sorted_index[e.from_node] < node_to_sorted_index[e.to_node])
 
-  def testReachableNodes(self):
+  def testReachableNodes(self, serialize=False):
     (nodes, _, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     self.assertSetEqual(
         set([0, 1, 2, 3, 4]),
         set(n.index for n in g.ReachableNodes([nodes[0]])))
@@ -124,14 +143,14 @@ class GraphTestCase(unittest.TestCase):
         set([6]),
         set(n.index for n in g.ReachableNodes([nodes[6]])))
 
-  def testCost(self):
+  def testCost(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     for (i, node) in enumerate(nodes):
       node.cost = i + 1
     nodes[6].cost = 6
@@ -146,14 +165,14 @@ class GraphTestCase(unittest.TestCase):
     g.Cost(path_list=path_list)
     self.assertListEqual([nodes[i] for i in (5, 6)], path_list)
 
-  def testCostWithRoots(self):
+  def testCostWithRoots(self, serialize=False):
     (nodes, edges, g) = self.MakeGraph(
         7,
         [(0, 1),
          (0, 2),
          (1, 3),
          (3, 4),
-         (5, 6)])
+         (5, 6)], serialize)
     for (i, node) in enumerate(nodes):
       node.cost = i + 1
     nodes[6].cost = 9
@@ -165,6 +184,15 @@ class GraphTestCase(unittest.TestCase):
     self.assertEqual(15, g.Cost(roots=[nodes[0]], path_list=path_list))
     self.assertListEqual([nodes[i] for i in (0, 1, 3, 4)], path_list)
 
+  def testSerialize(self):
+    # Re-do tests with a deserialized graph.
+    self.testBuildGraph(True)
+    self.testUpdateEdge(True)
+    self.testTopologicalSort(True)
+    self.testReachableNodes(True)
+    self.testCost(True)
+    self.testCostWithRoots(True)
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 8a172bd..85dd48d 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -8,47 +8,103 @@ For example, this can be used to evaluate NoState Prefetch
 (https://goo.gl/B3nRUR).
 
 When executed as a script, takes a trace as a command-line arguments and shows
-how many requests were prefetched.
+statistics about it.
 """
 
 import itertools
 import operator
 
+import common_util
 import dependency_graph
+import graph
 import loading_trace
 import user_satisfied_lens
 import request_dependencies_lens
 import request_track
 
 
-class PrefetchSimulationView(object):
+class RequestNode(dependency_graph.RequestNode):
   """Simulates the effect of prefetching resources discoverable by the preload
   scanner.
   """
+  _ATTRS = ['preloaded', 'before']
+  def __init__(self, request=None):
+    super(RequestNode, self).__init__(request)
+    self.preloaded = False
+    self.before = False
+
+  def ToJsonDict(self):
+    result = super(RequestNode, self).ToJsonDict()
+    return common_util.SerializeAttributesToJsonDict(result, self, self._ATTRS)
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = super(RequestNode, cls).FromJsonDict(json_dict)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, cls._ATTRS)
+
+
+class PrefetchSimulationView(object):
+  """Simulates the effect of prefetch."""
   def __init__(self, trace, dependencies_lens, user_lens):
-    """Initializes an instance of PrefetchSimulationView.
+    self.postload_msec = None
+    self.graph = None
+    if trace is None:
+      return
+    requests = trace.request_track.GetEvents()
+    critical_requests_ids = user_lens.CriticalRequests()
+    self.postload_msec = user_lens.PostloadTimeMsec()
+    self.graph = dependency_graph.RequestDependencyGraph(
+        requests, dependencies_lens, node_class=RequestNode)
+    preloaded_requests = [r.request_id for r in self._PreloadedRequests(
+        requests[0], dependencies_lens, trace)]
+    self._AnnotateNodes(self.graph.graph.Nodes(), preloaded_requests,
+                        critical_requests_ids)
+
+  def Cost(self):
+    """Returns the cost of the graph, restricted to the critical requests."""
+    pruned_graph = self._PrunedGraph()
+    return pruned_graph.Cost() + self.postload_msec
+
+  def UpdateNodeCosts(self, node_to_cost):
+    """Updates the cost of nodes, according to |node_to_cost|.
 
     Args:
-      trace: (LoadingTrace) a loading trace.
-      dependencies_lens: (RequestDependencyLens) request dependencies.
-      user_lens: (UserSatisfiedLens) Lens used to compute costs.
+      node_to_cost: (Callable) RequestNode -> float. Callable returning the cost
+                    of a node.
     """
-    self.trace = trace
-    self.dependencies_lens = dependencies_lens
-    self._resource_events = self.trace.tracing_track.Filter(
-        categories=set([u'blink.net']))
-    assert len(self._resource_events.GetEvents()) > 0,\
-            'Was the "blink.net" category enabled at trace collection time?"'
-    self._user_lens = user_lens
-    request_ids = self._user_lens.CriticalRequests()
-    all_requests = self.trace.request_track.GetEvents()
-    self._first_request_node = all_requests[0].request_id
-    requests = [r for r in all_requests if r.request_id in request_ids]
-    self.graph = dependency_graph.RequestDependencyGraph(
-        requests, self.dependencies_lens)
-
-  def ParserDiscoverableRequests(self, request, recurse=False):
-    """Returns a list of requests discovered by the parser from a given request.
+    pruned_graph = self._PrunedGraph()
+    for node in pruned_graph.Nodes():
+      node.cost = node_to_cost(node)
+
+  def ToJsonDict(self):
+    """Returns a dict representing this instance."""
+    result = {'graph': self.graph.ToJsonDict()}
+    return common_util.SerializeAttributesToJsonDict(
+        result, self, ['postload_msec'])
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns an instance of PrefetchSimulationView from a dict dumped by
+    ToJSonDict().
+    """
+    result = cls(None, None, None)
+    result.graph = dependency_graph.RequestDependencyGraph.FromJsonDict(
+        json_dict['graph'], RequestNode, dependency_graph.Edge)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, ['postload_msec'])
+
+  @classmethod
+  def _AnnotateNodes(cls, nodes, preloaded_requests_ids,
+                     critical_requests_ids,):
+    for node in nodes:
+      node.preloaded = node.request.request_id in preloaded_requests_ids
+      node.before = node.request.request_id in critical_requests_ids
+
+  @classmethod
+  def _ParserDiscoverableRequests(
+      cls, dependencies_lens, request, recurse=False):
+    """Returns a list of requests IDs dicovered by the parser.
 
     Args:
       request: (Request) Root request.
@@ -59,18 +115,20 @@ class PrefetchSimulationView(object):
     # TODO(lizeb): handle the recursive case.
     assert not recurse
     discoverable_requests = [request]
-    first_request = self.dependencies_lens.GetRedirectChain(request)[-1]
-    deps = self.dependencies_lens.GetRequestDependencies()
+    first_request = dependencies_lens.GetRedirectChain(request)[-1]
+    deps = dependencies_lens.GetRequestDependencies()
     for (first, second, reason) in deps:
       if first.request_id == first_request.request_id and reason == 'parser':
         discoverable_requests.append(second)
     return discoverable_requests
 
-  def ExpandRedirectChains(self, requests):
+  @classmethod
+  def _ExpandRedirectChains(cls, requests, dependencies_lens):
     return list(itertools.chain.from_iterable(
-        [self.dependencies_lens.GetRedirectChain(r) for r in requests]))
+        [dependencies_lens.GetRedirectChain(r) for r in requests]))
 
-  def PreloadedRequests(self, request):
+  @classmethod
+  def _PreloadedRequests(cls, request, dependencies_lens, trace):
     """Returns the requests that have been preloaded from a given request.
 
     This list is the set of request that are:
@@ -84,51 +142,48 @@ class PrefetchSimulationView(object):
 
     Returns:
       A list of Request. Does not include the root request. This list is a
-      subset of the one returned by ParserDiscoverableRequests().
+      subset of the one returned by _ParserDiscoverableRequests().
     """
     # Preload step events are emitted in ResourceFetcher::preloadStarted().
+    resource_events = trace.tracing_track.Filter(
+        categories=set([u'blink.net']))
     preload_step_events = filter(
         lambda e:  e.args.get('step') == 'Preload',
-        self._resource_events.GetEvents())
+        resource_events.GetEvents())
     preloaded_urls = set()
     for preload_step_event in preload_step_events:
-      preload_event = self._resource_events.EventFromStep(preload_step_event)
+      preload_event = resource_events.EventFromStep(preload_step_event)
       if preload_event:
         preloaded_urls.add(preload_event.args['url'])
-    parser_requests = self.ParserDiscoverableRequests(request)
+    parser_requests = cls._ParserDiscoverableRequests(
+        dependencies_lens, request)
     preloaded_root_requests = filter(
         lambda r: r.url in preloaded_urls, parser_requests)
     # We can actually fetch the whole redirect chain.
     return [request] + list(itertools.chain.from_iterable(
-        [self.dependencies_lens.GetRedirectChain(r)
+        [dependencies_lens.GetRedirectChain(r)
          for r in preloaded_root_requests]))
 
+  def _PrunedGraph(self):
+    roots = self.graph.graph.RootNodes()
+    nodes = self.graph.graph.ReachableNodes(
+        roots, should_stop=lambda n: not n.before)
+    return graph.DirectedGraph(nodes, self.graph.graph.Edges())
 
-def _PrintSummary(prefetch_view, user_lens):
-  requests = prefetch_view.trace.request_track.GetEvents()
-  first_request = prefetch_view.trace.request_track.GetEvents()[0]
-  parser_requests = prefetch_view.ExpandRedirectChains(
-      prefetch_view.ParserDiscoverableRequests(first_request))
-  preloaded_requests = prefetch_view.ExpandRedirectChains(
-      prefetch_view.PreloadedRequests(first_request))
-  print '%d requests, %d parser from the main request, %d preloaded' % (
-      len(requests), len(parser_requests), len(preloaded_requests))
-  print 'Time to user satisfaction: %.02fms' % (
-      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
 
-  print 'With 0-cost prefetched resources...'
-  new_costs = {r.request_id: 0. for r in preloaded_requests}
-  prefetch_view.graph.UpdateRequestsCost(new_costs)
-  print 'Time to user satisfaction: %.02fms' % (
-      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
+def _PrintSumamry(trace, dependencies_lens, user_lens):
+  prefetch_view = PrefetchSimulationView(trace, dependencies_lens, user_lens)
+  print 'Time to First Contentful Paint = %.02fms' % prefetch_view.Cost()
+  print 'Set costs of prefetched requests to 0.'
+  prefetch_view.UpdateNodeCosts(lambda n: 0 if n.preloaded else n.cost)
+  print 'Time to First Contentful Paint = %.02fms' % prefetch_view.Cost()
 
 
 def main(filename):
   trace = loading_trace.LoadingTrace.FromJsonFile(filename)
   dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
   user_lens = user_satisfied_lens.FirstContentfulPaintLens(trace)
-  prefetch_view = PrefetchSimulationView(trace, dependencies_lens, user_lens)
-  _PrintSummary(prefetch_view, user_lens)
+  _PrintSumamry(trace, dependencies_lens, user_lens)
 
 
 if __name__ == '__main__':
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index ca8897d..e222bf5 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -4,7 +4,7 @@
 
 import unittest
 
-import prefetch_view
+from prefetch_view import PrefetchSimulationView
 import request_dependencies_lens
 from request_dependencies_lens_unittest import TestRequests
 import request_track
@@ -20,13 +20,13 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
     self.assertListEqual(
         [TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST],
-        self.prefetch_view.ExpandRedirectChains(
-            [TestRequests.FIRST_REDIRECT_REQUEST]))
+        PrefetchSimulationView._ExpandRedirectChains(
+            [TestRequests.FIRST_REDIRECT_REQUEST], self.dependencies_lens))
 
   def testParserDiscoverableRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    discovered_requests = self.prefetch_view.ParserDiscoverableRequests(
-        first_request)
+    discovered_requests = PrefetchSimulationView._ParserDiscoverableRequests(
+        self.dependencies_lens, first_request)
     self.assertListEqual(
         [TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
@@ -34,7 +34,8 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
 
   def testPreloadedRequests(self):
     first_request = TestRequests.FIRST_REDIRECT_REQUEST
-    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+        first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([first_request], preloaded_requests)
     self._SetUp(
         [{'args': {'url': 'http://bla.com/nyancat.js'},
@@ -43,22 +44,51 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
          {'args': {'step': 'Preload'}, 'cat': 'blink.net',
           'id': '0xaf9f14fa9dd6c314', 'name': 'Resource', 'ph': 'T',
           'ts': 12, 'pid': 12, 'tid': 12}])
-    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    preloaded_requests = PrefetchSimulationView._PreloadedRequests(
+        first_request, self.dependencies_lens, self.trace)
     self.assertListEqual([TestRequests.FIRST_REDIRECT_REQUEST,
          TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
          TestRequests.JS_REQUEST_UNRELATED_FRAME], preloaded_requests)
 
+  def testCost(self):
+    self.assertEqual(40 + 12, self.prefetch_view.Cost())
+
+  def testUpdateNodeCosts(self):
+    self.prefetch_view.UpdateNodeCosts(lambda _: 100)
+    self.assertEqual(500 + 40 + 12, self.prefetch_view.Cost())
+
+  def testUpdateNodeCostsPartial(self):
+    self.prefetch_view.UpdateNodeCosts(
+        lambda n: 100 if (n.request.request_id
+                          == TestRequests.REDIRECTED_REQUEST.request_id) else 0)
+    self.assertEqual(100 + 40 + 12, self.prefetch_view.Cost())
+
+  def testToFromJsonDict(self):
+    self.assertEqual(40 + 12, self.prefetch_view.Cost())
+    json_dict = self.prefetch_view.ToJsonDict()
+    new_view = PrefetchSimulationView.FromJsonDict(json_dict)
+    self.assertEqual(40 + 12, new_view.Cost())
+    # Updated Costs.
+    self.prefetch_view.UpdateNodeCosts(lambda _: 100)
+    self.assertEqual(500 + 40 + 12, self.prefetch_view.Cost())
+    json_dict = self.prefetch_view.ToJsonDict()
+    new_view = PrefetchSimulationView.FromJsonDict(json_dict)
+    self.assertEqual(500 + 40 + 12, new_view.Cost())
+
   def _SetUp(self, added_trace_events=None):
     trace_events = [
         {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'blink.net'}]
     if added_trace_events is not None:
       trace_events += added_trace_events
     self.trace = TestRequests.CreateLoadingTrace(trace_events)
-    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+    self.dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
     self.user_satisfied_lens = test_utils.MockUserSatisfiedLens(self.trace)
-    self.prefetch_view = prefetch_view.PrefetchSimulationView(
-        self.trace, dependencies_lens, self.user_satisfied_lens)
+    self.user_satisfied_lens._postload_msec = 12
+    self.prefetch_view = PrefetchSimulationView(
+        self.trace, self.dependencies_lens, self.user_satisfied_lens)
+    for e in self.prefetch_view.graph.graph.Edges():
+      e.cost = 10
 
 
 if __name__ == '__main__':
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 0115f8c..89ce7a1 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -10,6 +10,8 @@ FirstSignificantPaintLens.
 import logging
 import operator
 
+import common_util
+
 
 class _UserSatisfiedLens(object):
   """A base class for all user satisfaction metrics.
@@ -19,6 +21,9 @@ class _UserSatisfiedLens(object):
   event. Subclasses need only provide the time computation. The base class will
   use that to construct the request ids.
   """
+  _ATTRS = ['_satisfied_msec', '_event_msec', '_postload_msec',
+            '_critical_request_ids']
+
   def __init__(self, trace):
     """Initialize the lens.
 
@@ -27,6 +32,10 @@ class _UserSatisfiedLens(object):
     """
     self._satisfied_msec = None
     self._event_msec = None
+    self._postload_msec = None
+    self._critical_request_ids = None
+    if trace is None:
+      return
     self._CalculateTimes(trace.tracing_track)
     critical_requests = self._RequestsBefore(
         trace.request_track, self._satisfied_msec)
@@ -57,6 +66,15 @@ class _UserSatisfiedLens(object):
     """
     return self._postload_msec
 
+  def ToJsonDict(self):
+    return common_util.SerializeAttributesToJsonDict({}, self, self._ATTRS)
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    result = cls(None)
+    return common_util.DeserializeAttributesFromJsonDict(
+        json_dict, result, cls._ATTRS)
+
   def _CalculateTimes(self, tracing_track):
     """Subclasses should implement to set _satisfied_msec and _event_msec."""
     raise NotImplementedError

commit d14970e46ed5efc717475c20ce29a554f5e78c71
Author: blundell <blundell@chromium.org>
Date:   Wed Apr 6 05:55:41 2016 -0700

    tools/android/loading: Avoid using analyze.py in GCE code
    
    This CL changes tools/android/loading/gce/main.py to use the relevant
    tools/android/loading libraries directly rather than invoking analyze.py
    via subprocess. The concrete motivation is to be able to directly access
    LoadingTrace objects after they're generated.
    
    Review URL: https://codereview.chromium.org/1863603002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385443}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 178ce7d179e5dff0218e68ad826b25c0ae9bc4ca

diff --git a/loading/controller.py b/loading/controller.py
index 053e062..b6616f9 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -299,8 +299,9 @@ class LocalChromeController(ChromeControllerBase):
     self._headless = headless
 
   @contextlib.contextmanager
-  def Open(self):
-    """Override for connection context."""
+  def OpenWithRedirection(self, stdout, stderr):
+    """Override for connection context. stdout and stderr are passed to the
+       child processes used to run Chrome and XVFB."""
     chrome_cmd = [OPTIONS.local_binary]
     chrome_cmd.extend(self._GetChromeArguments())
     chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
@@ -309,16 +310,16 @@ class LocalChromeController(ChromeControllerBase):
     #   - To find the correct target descriptor at devtool connection;
     #   - To avoid cache and WPR pollution by the NTP.
     chrome_cmd.append('about:blank')
-    chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     environment = os.environ.copy()
     if self._headless:
       environment['DISPLAY'] = 'localhost:99'
       xvfb_process = subprocess.Popen(
           ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
-          stderr=chrome_out)
+          stdout=stdout, stderr=stderr)
     logging.debug(subprocess.list2cmdline(chrome_cmd))
     chrome_process = subprocess.Popen(chrome_cmd, shell=False,
-                                      stderr=chrome_out, env=environment)
+                                      stdout=stdout, stderr=stderr,
+                                      env=environment)
     connection = None
     try:
       time.sleep(10)
@@ -340,6 +341,13 @@ class LocalChromeController(ChromeControllerBase):
       if self._headless:
         xvfb_process.kill()
 
+  def Open(self):
+    """Wrapper around the more-specialized version of Open() above that sets
+    the value of stdout/stderr based on the value of OPTIONS.local_noisy."""
+    stdout = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+    stderr = stdout
+    return self.OpenWithRedirection(self, stdout, stderr)
+
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""
     self._EnsureProfileDirectory()
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 86d24a6..c20f0ae 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -8,11 +8,22 @@ import re
 import threading
 import time
 import subprocess
+import sys
 
 from gcloud import storage
 from gcloud.exceptions import NotFound
 from oauth2client.client import GoogleCredentials
 
+# NOTE: The parent directory needs to be first in sys.path to avoid conflicts
+# with catapult modules that have colliding names, as catapult inserts itself
+# into the path as the second element. This is an ugly and fragile hack.
+sys.path.insert(0,
+    os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
+import controller
+import loading_trace
+import options
+
+
 class ServerApp(object):
   """Simple web server application, collecting traces and writing them in
   Google Cloud Storage.
@@ -45,9 +56,11 @@ class ServerApp(object):
          if not self._base_path_in_bucket.endswith('/'):
            self._base_path_in_bucket += '/'
 
-       self._chrome_path = config['chrome_path']
        self._src_path = config['src_path']
 
+    # Initialize the global options that will be used during trace generation.
+    options.OPTIONS.ParseArgs([])
+    options.OPTIONS.local_binary = config['chrome_path']
 
   def _GetStorageClient(self):
     return storage.Client(project = self._project_name,
@@ -91,8 +104,7 @@ class ServerApp(object):
 
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
-    """ Generates a trace using analyze.py
-    Runs on _thread.
+    """ Generates a trace on _thread.
 
     Args:
       url: URL as a string.
@@ -108,18 +120,43 @@ class ServerApp(object):
       os.remove(filename)  # Remove any existing trace for this URL.
     except OSError:
       pass  # Nothing to remove.
-    analyze_path = self._src_path + '/tools/android/loading/analyze.py'
-    command_line = ['python', analyze_path, 'log_requests', '--local_noisy',
-        '--clear_cache', '--local', '--headless', '--local_binary',
-        self._chrome_path, '--url', url, '--output', filename]
-    if len(emulate_device):
-      command_line += ['--emulate_device', emulate_device]
-    if len(emulate_network):
-      command_line += ['--emulate_network', emulate_network]
-    with open(log_filename, 'w') as log_file:
-      ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
-                            stdout = log_file)
-    return ret == 0
+
+    if not url.startswith('http') and not url.startswith('file'):
+      url = 'http://' + url
+
+    old_stdout = sys.stdout
+    old_stderr = sys.stderr
+
+    succeeded = True
+    try:
+      with open(log_filename, 'w') as sys.stdout:
+        sys.stderr = sys.stdout
+
+        # Set up the controller.
+        chrome_ctl = controller.LocalChromeController()
+        chrome_ctl.SetHeadless(True)
+        if emulate_device:
+          chrome_ctl.SetDeviceEmulation(emulate_device)
+        if emulate_network:
+          chrome_ctl.SetNetworkEmulation(emulate_network)
+
+        # Record and write the trace.
+        with chrome_ctl.OpenWithRedirection(sys.stdout,
+                                            sys.stderr) as connection:
+          connection.ClearCache()
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url, connection, chrome_ctl.ChromeMetadata())
+    except Exception as e:
+      succeeded = False
+      sys.stderr.write(e)
+
+    sys.stdout = old_stdout
+    sys.stderr = old_stderr
+
+    with open(filename, 'w') as f:
+      json.dump(trace.ToJsonDict(), f, sort_keys=True, indent=2)
+
+    return succeeded
 
   def _GetCurrentTaskCount(self):
     """Returns the number of remaining tasks. Thread safe."""
@@ -129,9 +166,8 @@ class ServerApp(object):
     return task_count
 
   def _ProcessTasks(self, tasks, repeat_count, emulate_device, emulate_network):
-    """Iterates over _tasks and runs analyze.py on each of them. Uploads the
-    resulting traces to Google Cloud Storage.
-    Runs on _thread.
+    """Iterates over _task, generating a trace for each of them. Uploads the
+    resulting traces to Google Cloud Storage.  Runs on _thread.
 
     Args:
       tasks: The list of URLs to process.
@@ -146,6 +182,8 @@ class ServerApp(object):
     self._tasks_lock.release()
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
+
+    # TODO(blundell): Fix this up.
     logs_dir = self._base_path_in_bucket + 'analyze_logs/'
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
@@ -161,13 +199,13 @@ class ServerApp(object):
           print 'Uploading: %s' % remote_filename
           self._UploadFile(local_filename, traces_dir + remote_filename)
         else:
-          print 'analyze.py failed for URL: %s' % url
+          print 'Trace generation failed for URL: %s' % url
           self._tasks_lock.acquire()
           self._failed_tasks.append({ "url": url, "repeat": repeat})
           self._tasks_lock.release()
           if os.path.isfile(local_filename):
             self._UploadFile(local_filename, failures_dir + remote_filename)
-        print 'Uploading analyze log'
+        print 'Uploading log'
         self._UploadFile(log_filename, logs_dir + remote_filename)
       # Pop once task is finished, for accurate status tracking.
       self._tasks_lock.acquire()

commit 15a7665e8d3063d783c6b22c5f7daea752a56c83
Author: droger <droger@chromium.org>
Date:   Tue Apr 5 07:54:53 2016 -0700

    tools/android/loading Ignore 'p' trace events
    
    These events can happen in practice, and thus the analysis should not
    crash in that case. Instead, these events are now ignored.
    
    The error was:
    
    __init__ at tools/android/loading/tracing.py:318
      'Deprecated event: %s' % tracing_event)
    
    DevToolsConnectionException: Deprecated event: {u'name': u'EmbeddedWorkerInstance::Start', u'tts': 163020, u'args': {u'step': u'OnProcessAllocated', u'Is New Process': False}, u'pid': 13875, u'ts': 517201903, u'cat': u'ServiceWorker', u'tid': 13898, u'ph': u'p', u'id': u'0xaf5594c526a89982'}
    
    Review URL: https://codereview.chromium.org/1853753004
    
    Cr-Original-Commit-Position: refs/heads/master@{#385174}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6c2cfd7db70f6686439c78d8829a603c3605fb6e

diff --git a/loading/tracing.py b/loading/tracing.py
index 5d0a647..fb54d70 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -207,6 +207,7 @@ class TracingTrack(devtools_monitor.Track):
           'M': self._Ignore,
           'X': self._Ignore,
           'R': self._Ignore,
+          'p': self._Ignore,
           '(': self._Ignore, # Context events.
           ')': self._Ignore, # Ditto.
           None: self._Ignore,
@@ -313,9 +314,6 @@ class Event(object):
     if not synthetic and tracing_event['ph'] in ['s', 't', 'f']:
       raise devtools_monitor.DevToolsConnectionException(
           'Unsupported event: %s' % tracing_event)
-    if not synthetic and tracing_event['ph'] in ['p']:
-      raise devtools_monitor.DevToolsConnectionException(
-          'Deprecated event: %s' % tracing_event)
 
     self._tracing_event = tracing_event
     # Note tracing event times are in microseconds.

commit c0fa12c6235e61a83a7272ee7ea83b1fb1a49200
Author: droger <droger@chromium.org>
Date:   Tue Apr 5 07:45:44 2016 -0700

    tools/android/loading Fix crash when popping task in main.py
    
    The function popleft does not exist on list.
    Popping from the other end instead.
    
    Review URL: https://codereview.chromium.org/1858223002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385172}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 67d4b25201f8dd0b32b815b68372bf64085dbffd

diff --git a/loading/gce/main.py b/loading/gce/main.py
index 66bd7dd..86d24a6 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -151,7 +151,7 @@ class ServerApp(object):
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
     while len(self._tasks) > 0:
-      url = self._tasks[0]
+      url = self._tasks[-1]
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
@@ -171,7 +171,7 @@ class ServerApp(object):
         self._UploadFile(log_filename, logs_dir + remote_filename)
       # Pop once task is finished, for accurate status tracking.
       self._tasks_lock.acquire()
-      url = self._tasks.popleft()
+      url = self._tasks.pop()
       self._tasks_lock.release()
 
     if len(self._failed_tasks) > 0:

commit ad4e8ff5457b683afe3612b60132f659a594d4f9
Author: dgn <dgn@chromium.org>
Date:   Tue Apr 5 07:07:41 2016 -0700

    [tool] SpnegoAuth: add multi account, credential confirmation
    
    Add more features to the SpnegoAuthenticator:
    
     -  Set up up to 2 accounts.
     -  Account 1 will start authenticated.
     -  Account 2 will start unauthenticated. The first token request
        will require an additional confirmation step.
     -  Accounts can be added and removed from the Android account
        settings screen
    
    BUG=534293
    
    Review URL: https://codereview.chromium.org/1416443003
    
    Cr-Original-Commit-Position: refs/heads/master@{#385165}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e4cefacb5bc08b92c284a8d7a3deaaaa2545a04d

diff --git a/kerberos/README.md b/kerberos/README.md
index c8aecb2..4daa473 100644
--- a/kerberos/README.md
+++ b/kerberos/README.md
@@ -62,14 +62,22 @@ $CHROMIUM_SRC/tools/android/kerberos/negotiate_test_server.py
 
 ### SpnegoAuthenticator
 
-This app declares and sets up an account type to be used for Negotiate auth.
-When Chrome makes a request for the `org.chromium.tools.SpnegoAuthenticator`
-account type, it returns the dummy account, allowing Chrome to continue the
-request.
+This app declares and sets up an accounts to be used for Negotiate auth, as
+described in the chromium.org wiki
+([Writing a SPNEGO Authenticator for Chrome on Android][crwiki]).
+Those accounts use the type `org.chromium.tools.SpnegoAuthenticator`.
 
-See [Writing a SPNEGO Authenticator for Chrome on Android][crwiki] on
-chromium.org for more information.
+![Account administration activity preview][screenshot]
+
+Features:
+
+ -  Set up up to 2 accounts.
+ -  Account 1 will start authenticated.
+ -  Account 2 will start unauthenticated. The first token request will require
+    an additional confirmation step.
+ -  Accounts can be added and removed from the Android account settings screen
 
 [testdpc-play]: https://play.google.com/store/apps/details?id=com.sample.android.testdpc
 [testdpc-gh]: https://github.com/googlesamples/android-testdpc
 [crwiki]:https://sites.google.com/a/chromium.org/dev/developers/design-documents/http-authentication/writing-a-spnego-authenticator-for-chrome-on-android
+[screenshot]:SpnegoAuthenticator/preview.png
diff --git a/kerberos/SpnegoAuthenticator/AndroidManifest.xml b/kerberos/SpnegoAuthenticator/AndroidManifest.xml
index efd4f33..bb48308 100644
--- a/kerberos/SpnegoAuthenticator/AndroidManifest.xml
+++ b/kerberos/SpnegoAuthenticator/AndroidManifest.xml
@@ -9,6 +9,10 @@
 
     <uses-sdk android:minSdkVersion="21" android:targetSdkVersion="23" />
 
+    <!--
+      Deprecated permissions. Normal protection level, autogranted. Needed
+      for API level 22 and before.
+    -->
     <uses-permission android:name="android.permission.AUTHENTICATE_ACCOUNTS" />
     <uses-permission android:name="android.permission.MANAGE_ACCOUNTS" />
 
@@ -27,6 +31,14 @@
                 android:name="android.accounts.AccountAuthenticator"
                 android:resource="@xml/spnego_authenticator" />
         </service>
+
+        <!-- exported=true needed so that chrome can start the activity -->
+        <activity
+           android:exported="true"
+           android:label="@string/title_activity_account_authenticator"
+           android:name=".SpnegoAuthenticatorActivity"
+           android:noHistory="true">
+       </activity>
     </application>
 
 </manifest>
diff --git a/kerberos/SpnegoAuthenticator/BUILD.gn b/kerberos/SpnegoAuthenticator/BUILD.gn
index 3ce7ea4..e6e8f89 100644
--- a/kerberos/SpnegoAuthenticator/BUILD.gn
+++ b/kerberos/SpnegoAuthenticator/BUILD.gn
@@ -18,7 +18,10 @@ android_apk("spnego_authenticator_apk") {
   ]
 
   java_files = [
+    "src/org/chromium/tools/spnegoauthenticator/AccountData.java",
+    "src/org/chromium/tools/spnegoauthenticator/Constants.java",
     "src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java",
+    "src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java",
     "src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorService.java",
   ]
 }
diff --git a/kerberos/SpnegoAuthenticator/preview.png b/kerberos/SpnegoAuthenticator/preview.png
new file mode 100644
index 0000000..82fd331
Binary files /dev/null and b/kerberos/SpnegoAuthenticator/preview.png differ
diff --git a/kerberos/SpnegoAuthenticator/res/layout/activity_account_authenticator.xml b/kerberos/SpnegoAuthenticator/res/layout/activity_account_authenticator.xml
new file mode 100644
index 0000000..275384d
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/res/layout/activity_account_authenticator.xml
@@ -0,0 +1,53 @@
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+  xmlns:tools="http://schemas.android.com/tools"
+  android:layout_width="match_parent"
+  android:layout_height="match_parent"
+  android:paddingTop="16dp"
+  android:paddingBottom="16dp"
+  android:paddingStart="16dp"
+  android:paddingEnd="16dp"
+  android:gravity="center_horizontal"
+  android:orientation="vertical"
+  android:theme="@android:style/Theme.Material"
+  tools:context=".SpnegoAuthenticatorActivity">
+
+  <ScrollView
+    android:layout_width="match_parent"
+    android:layout_height="match_parent">
+
+    <LinearLayout
+      android:id="@+id/login_form"
+      android:layout_width="match_parent"
+      android:layout_height="wrap_content"
+      android:orientation="vertical">
+
+      <Button
+        android:id="@+id/sign_in_button_1"
+        style="?android:textAppearanceSmall"
+        android:textStyle="bold"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_marginTop="16dp"
+        android:text="@string/action_sign_in_1"/>
+
+      <Button
+        android:id="@+id/sign_in_button_2"
+        style="?android:textAppearanceSmall"
+        android:textStyle="bold"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_marginTop="16dp"
+        android:text="@string/action_sign_in_2"/>
+
+      <Button
+        android:id="@+id/confirm_credentials_button"
+        style="?android:textAppearanceSmall"
+        android:textStyle="bold"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_marginTop="16dp"
+        android:text="@string/action_confirm_credentials"/>
+
+    </LinearLayout>
+  </ScrollView>
+</LinearLayout>
diff --git a/kerberos/SpnegoAuthenticator/res/values/strings.xml b/kerberos/SpnegoAuthenticator/res/values/strings.xml
index 1fb5555..2427630 100644
--- a/kerberos/SpnegoAuthenticator/res/values/strings.xml
+++ b/kerberos/SpnegoAuthenticator/res/values/strings.xml
@@ -1,4 +1,8 @@
 <resources>
     <string name="app_name">SpnegoAuthenticator</string>
+    <string name="action_sign_in_1">Sign In Account 1</string>
+    <string name="action_sign_in_2">Sign In Account 2</string>
+    <string name="action_confirm_credentials">Confirm Credentials</string>
+    <string name="title_activity_account_authenticator">Spnego Authenticator Sign In</string>
     <string name="account_type">org.chromium.tools.SpnegoAuthenticator</string>
 </resources>
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/AccountData.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/AccountData.java
new file mode 100644
index 0000000..5c68378
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/AccountData.java
@@ -0,0 +1,100 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.spnegoauthenticator;
+
+import android.accounts.Account;
+import android.accounts.AccountManager;
+import android.content.Context;
+import android.content.Intent;
+import android.text.TextUtils;
+
+import org.chromium.base.Log;
+import org.chromium.net.HttpNegotiateConstants;
+
+/** Utility class to get and set account data. */
+class AccountData {
+    private static final String TAG = Constants.TAG;
+    private static final String OPT_KEY_AUTH = "isAuthenticated";
+    private static final String OPT_VALUE_AUTH = "YES";
+    private final Account mAccount;
+    private final String mPassword;
+    private boolean mIsAuthenticated;
+
+    private AccountData(Account account, boolean isAuthenticated) {
+        Log.d(TAG, "AccountData(name=%s, isAuthenticated=%s", account.name, isAuthenticated);
+        mAccount = account;
+        mIsAuthenticated = isAuthenticated;
+        mPassword = "userPass";
+    }
+
+    /** Creates some new account data. */
+    public static AccountData create(String name, Context context) {
+        Account account = new Account(name, context.getString(R.string.account_type));
+        boolean isAuthenticated = Constants.ACCOUNT_1_NAME.equals(name);
+
+        return new AccountData(account, isAuthenticated);
+    }
+
+    /**
+     * Creates a new {@link AccountData} object, looking at previously saved data to
+     * initialize it.
+     */
+    public static AccountData get(String accountName, Context context) {
+        Account account = new Account(accountName, context.getString(R.string.account_type));
+
+        AccountManager am = AccountManager.get(context);
+        String authValue = am.getUserData(account, OPT_KEY_AUTH);
+        boolean isAuthenticated = TextUtils.equals(authValue, OPT_VALUE_AUTH);
+
+        return new AccountData(account, isAuthenticated);
+    }
+
+    /**
+     * Saves the account data with the AccountManager. If the account did not previously
+     * exist, it will be created.
+     */
+    public void save(Context context) {
+        AccountManager am = AccountManager.get(context);
+
+        // Does nothing if the account already exists
+        am.addAccountExplicitly(mAccount, mPassword, null);
+
+        am.setUserData(mAccount, OPT_KEY_AUTH, mIsAuthenticated ? OPT_VALUE_AUTH : null);
+
+        // Is supposed to be send by AccountsService when accounts are modified, but it looks like
+        // the authenticator has to do it itself.
+        context.sendBroadcast(new Intent(AccountManager.LOGIN_ACCOUNTS_CHANGED_ACTION));
+    }
+
+    /** Returns an intent as expected for answers to {@link SpnegoAuthenticator#addAccount}. */
+    public Intent getAccountAddedIntent() {
+        Intent intent = new Intent();
+        intent.putExtra(AccountManager.KEY_ACCOUNT_NAME, mAccount.name);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_TYPE, mAccount.type);
+        return intent;
+    }
+
+    /** Returns an intent as expected for answers to {@link SpnegoAuthenticator#getAuthToken}. */
+    public Intent getCredentialsConfirmedIntent() {
+        Intent intent = new Intent();
+        intent.putExtra(AccountManager.KEY_ACCOUNT_NAME, mAccount.name);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_TYPE, mAccount.type);
+        intent.putExtra(AccountManager.KEY_AUTHTOKEN, Constants.AUTH_TOKEN);
+        intent.putExtra(HttpNegotiateConstants.KEY_SPNEGO_RESULT, 0);
+        return intent;
+    }
+
+    public boolean isAuthenticated() {
+        return mIsAuthenticated;
+    }
+
+    public void setIsAuthenticated(boolean value) {
+        mIsAuthenticated = value;
+    }
+
+    public Account getAccount() {
+        return mAccount;
+    }
+}
\ No newline at end of file
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/Constants.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/Constants.java
new file mode 100644
index 0000000..126bed0
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/Constants.java
@@ -0,0 +1,13 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.spnegoauthenticator;
+
+class Constants {
+    static final String TAG = "tools_SpnegoAuth";
+    static final String AUTH_TOKEN = "DummyAuthToken";
+    static final int CONFIRM_CREDENTIAL_NOTIFICATION_ID = 42;
+    static final String ACCOUNT_1_NAME = "Dummy SpnegoAccount 1";
+    static final String ACCOUNT_2_NAME = "Dummy SpnegoAccount 2";
+}
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java
index 16133ad..7d2c084 100644
--- a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticator.java
@@ -9,7 +9,11 @@ import android.accounts.Account;
 import android.accounts.AccountAuthenticatorResponse;
 import android.accounts.AccountManager;
 import android.accounts.NetworkErrorException;
+import android.app.Notification;
+import android.app.NotificationManager;
+import android.app.PendingIntent;
 import android.content.Context;
+import android.content.Intent;
 import android.os.Bundle;
 
 import org.chromium.base.Log;
@@ -18,66 +22,85 @@ import org.chromium.net.HttpNegotiateConstants;
 import java.util.Arrays;
 
 /**
- * AccountAuthenticator implementation that automatically creates a dummy account and returns a
- * dummy token when asked.
+ * AccountAuthenticator implementation
  */
 public class SpnegoAuthenticator extends AbstractAccountAuthenticator {
-    private static final String TAG = "tools_SpnegoAuth";
-    private static final String ACCOUNT_NAME = "DummySpnegoAccount";
+
+    private static final String TAG = Constants.TAG;
+    private final Context mContext;
 
     /**
      * @param context
      */
     public SpnegoAuthenticator(Context context) {
         super(context);
-        ensureTestAccountExists(context);
+        mContext = context;
     }
 
     @Override
-    public Bundle addAccount(AccountAuthenticatorResponse arg0, String accountType, String arg2,
-            String[] arg3, Bundle arg4) throws NetworkErrorException {
-        Log.w(TAG, "addAccount():  Not supported.");
-        Bundle result = new Bundle();
-        result.putInt(AccountManager.KEY_ERROR_CODE, AccountManager.ERROR_CODE_BAD_REQUEST);
-        result.putString(AccountManager.KEY_ERROR_MESSAGE, "Can't add new SPNEGO accounts");
-        return result;
+    public Bundle addAccount(AccountAuthenticatorResponse response, String accountType,
+            String authTokenType, String[] requiredFeatures, Bundle options)
+            throws NetworkErrorException {
+        Log.d(TAG, "addAccount()");
+
+        // Delegate to the activity to get the account information from the user.
+        Bundle bundle = new Bundle();
+        bundle.putParcelable(AccountManager.KEY_INTENT,
+                SpnegoAuthenticatorActivity.getAddAccountIntent(mContext, response));
+        return bundle;
     }
 
     @Override
-    public Bundle confirmCredentials(AccountAuthenticatorResponse arg0, Account arg1, Bundle arg2)
-            throws NetworkErrorException {
-        Bundle result = new Bundle();
-        result.putBoolean(AccountManager.KEY_BOOLEAN_RESULT, true);
-        return result;
+    public Bundle confirmCredentials(AccountAuthenticatorResponse response, Account account,
+            Bundle options) throws NetworkErrorException {
+        Log.d(TAG, "confirmCredentials(%s)", account.name);
+        return unsupportedOperationBundle("confirmCredentials");
     }
 
     @Override
-    public Bundle editProperties(AccountAuthenticatorResponse arg0, String arg1) {
-        return new Bundle();
+    public Bundle editProperties(AccountAuthenticatorResponse response, String accountType) {
+        Log.d(TAG, "editProperties(%s)", accountType);
+        return unsupportedOperationBundle("editProperties");
     }
 
     @Override
     public Bundle getAuthToken(AccountAuthenticatorResponse response, Account account,
             String authTokenType, Bundle options) throws NetworkErrorException {
+        Log.d(TAG, "getAuthToken(%s)", account.name);
+
         Bundle result = new Bundle();
-        result.putString(AccountManager.KEY_ACCOUNT_NAME, account.name);
-        result.putString(AccountManager.KEY_ACCOUNT_TYPE, account.type);
-        result.putString(AccountManager.KEY_AUTHTOKEN, "DummyAuthToken");
-        result.putInt(HttpNegotiateConstants.KEY_SPNEGO_RESULT, 0);
-        Log.d(TAG, "getAuthToken(): Returning dummy SPNEGO auth token");
+        if (AccountData.get(account.name, mContext).isAuthenticated()) {
+            Log.d(TAG, "getAuthToken(): Returning dummy SPNEGO auth token");
+            result.putString(AccountManager.KEY_ACCOUNT_NAME, account.name);
+            result.putString(AccountManager.KEY_ACCOUNT_TYPE, account.type);
+            result.putString(AccountManager.KEY_AUTHTOKEN, Constants.AUTH_TOKEN);
+            result.putInt(HttpNegotiateConstants.KEY_SPNEGO_RESULT, 0);
+        } else {
+            Log.d(TAG, "getAuthToken(): Asking for credentials confirmation");
+            Intent intent = SpnegoAuthenticatorActivity.getConfirmCredentialsIntent(
+                    mContext, account.name, response);
+            result.putParcelable(AccountManager.KEY_INTENT, intent);
+
+            // We need to show a notification in case the caller can't use the intent directly.
+            showConfirmCredentialsNotification(mContext, intent);
+        }
+
         return result;
     }
 
     @Override
     public String getAuthTokenLabel(String authTokenType) {
+        Log.d(TAG, "getAuthTokenLabel(%s)", authTokenType);
         return "Spnego " + authTokenType;
     }
 
     @Override
-    public Bundle hasFeatures(AccountAuthenticatorResponse arg0, Account arg1, String[] features)
-            throws NetworkErrorException {
+    public Bundle hasFeatures(AccountAuthenticatorResponse response, Account account,
+            String[] features) throws NetworkErrorException {
         Log.d(TAG, "hasFeatures(%s)", Arrays.asList(features));
         Bundle result = new Bundle();
+
+        // All our accounts only have the SPNEGO feature, other features are not supported.
         for (String feature : features) {
             if (!feature.equals(HttpNegotiateConstants.SPNEGO_FEATURE)) {
                 result.putBoolean(AccountManager.KEY_BOOLEAN_RESULT, false);
@@ -89,18 +112,34 @@ public class SpnegoAuthenticator extends AbstractAccountAuthenticator {
     }
 
     @Override
-    public Bundle updateCredentials(AccountAuthenticatorResponse arg0, Account arg1, String arg2,
-            Bundle arg3) throws NetworkErrorException {
-        Log.w(TAG, "updateCredentials(): Not supported.");
-        Bundle result = new Bundle();
-        result.putInt(AccountManager.KEY_ERROR_CODE, AccountManager.ERROR_CODE_BAD_REQUEST);
-        result.putString(AccountManager.KEY_ERROR_MESSAGE, "Can't update credentials.");
-        return result;
+    public Bundle updateCredentials(AccountAuthenticatorResponse response, Account account,
+            String authTokenType, Bundle options) throws NetworkErrorException {
+        Log.d(TAG, "updateCredentials(%s)", account.name);
+        return unsupportedOperationBundle("updateCredentials");
     }
 
-    private void ensureTestAccountExists(Context context) {
-        AccountManager am = AccountManager.get(context);
-        Account account = new Account(ACCOUNT_NAME, context.getString(R.string.account_type));
-        am.addAccountExplicitly(account, null, null);
+    private void showConfirmCredentialsNotification(Context context, Intent intent) {
+        PendingIntent notificationAction =
+                PendingIntent.getActivity(context, 0, intent, PendingIntent.FLAG_UPDATE_CURRENT);
+        Notification notification = new Notification.Builder(context)
+                .setContentTitle("Authentication required")
+                .setContentText("Credential confirmation required for the Spnego account")
+                .setSmallIcon(android.R.drawable.stat_sys_warning)
+                .setContentIntent(notificationAction)
+                .setAutoCancel(true).build();
+
+        NotificationManager notificationManager =
+                (NotificationManager) context.getSystemService(Context.NOTIFICATION_SERVICE);
+
+        notificationManager.notify(Constants.CONFIRM_CREDENTIAL_NOTIFICATION_ID, notification);
+    }
+
+    /** Returns a bundle containing a standard error response. */
+    private Bundle unsupportedOperationBundle(String operationName) {
+        Bundle result = new Bundle();
+        result.putInt(
+                AccountManager.KEY_ERROR_CODE, AccountManager.ERROR_CODE_UNSUPPORTED_OPERATION);
+        result.putString(AccountManager.KEY_ERROR_MESSAGE, "Unsupported method: " + operationName);
+        return result;
     }
 }
diff --git a/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java
new file mode 100644
index 0000000..1172a1e
--- /dev/null
+++ b/kerberos/SpnegoAuthenticator/src/org/chromium/tools/spnegoauthenticator/SpnegoAuthenticatorActivity.java
@@ -0,0 +1,131 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.spnegoauthenticator;
+
+import android.accounts.AccountAuthenticatorActivity;
+import android.accounts.AccountAuthenticatorResponse;
+import android.accounts.AccountManager;
+import android.app.NotificationManager;
+import android.content.Context;
+import android.content.Intent;
+import android.os.Bundle;
+import android.view.View;
+import android.view.View.OnClickListener;
+import android.widget.Button;
+
+import org.chromium.base.Log;
+
+/** Provides a UI to administrate the Spnego accounts. */
+public class SpnegoAuthenticatorActivity extends AccountAuthenticatorActivity {
+    private static final String TAG = Constants.TAG;
+
+    // Constants for passing information via intents.
+    private static final String KEY_MODE = "mode";
+    private static final String KEY_ACCOUNT = "account";
+    private static final int MODE_INVALID = 0;
+    private static final int MODE_ADD_ACCOUNT = 1;
+    private static final int MODE_CONFIRM_CREDENTIALS = 2;
+
+    @Override
+    protected void onCreate(Bundle savedInstanceState) {
+        super.onCreate(savedInstanceState);
+        setContentView(R.layout.activity_account_authenticator);
+
+        Intent intent = getIntent();
+        initUi(intent.getIntExtra(KEY_MODE, MODE_INVALID), intent.getStringExtra(KEY_ACCOUNT));
+    }
+
+    /** Returns an intent that can be used to start the activity in AddAcount mode */
+    public static Intent getAddAccountIntent(
+            Context context, AccountAuthenticatorResponse response) {
+        Intent intent = new Intent(context, SpnegoAuthenticatorActivity.class);
+        intent.putExtra(KEY_MODE, MODE_ADD_ACCOUNT);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_AUTHENTICATOR_RESPONSE, response);
+        return intent;
+    }
+
+    /** Returns an intent that can be used to start the activity in ConfirmCredentials mode */
+    public static Intent getConfirmCredentialsIntent(
+            Context context, String accountName, AccountAuthenticatorResponse response) {
+        Intent intent = new Intent(context, SpnegoAuthenticatorActivity.class);
+        intent.putExtra(KEY_MODE, MODE_CONFIRM_CREDENTIALS);
+        intent.putExtra(KEY_ACCOUNT, accountName);
+        intent.putExtra(AccountManager.KEY_ACCOUNT_AUTHENTICATOR_RESPONSE, response);
+        intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK | Intent.FLAG_ACTIVITY_EXCLUDE_FROM_RECENTS
+                | Intent.FLAG_ACTIVITY_NO_HISTORY);
+        return intent;
+    }
+
+    private void addAccount(String accountName) {
+        Log.d(TAG, "Adding account '%s'", accountName);
+
+        AccountData accountData = AccountData.create(accountName, this);
+        accountData.save(this);
+        Intent intent = accountData.getAccountAddedIntent();
+        intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK | Intent.FLAG_ACTIVITY_EXCLUDE_FROM_RECENTS
+                | Intent.FLAG_ACTIVITY_NO_HISTORY);
+        setAccountAuthenticatorResult(intent.getExtras());
+        setResult(RESULT_OK, intent);
+        finish();
+    }
+
+    private void confirmCredentials(String accountName) {
+        Log.d(TAG, "Confirming credentials for account '%s'", accountName);
+
+        NotificationManager nm = (NotificationManager) getSystemService(NOTIFICATION_SERVICE);
+        nm.cancel(Constants.CONFIRM_CREDENTIAL_NOTIFICATION_ID);
+
+        AccountData accountData = AccountData.get(accountName, this);
+        accountData.setIsAuthenticated(true);
+        accountData.save(this);
+
+        Intent intent = accountData.getCredentialsConfirmedIntent();
+        setAccountAuthenticatorResult(intent.getExtras());
+        setResult(RESULT_OK, intent);
+        finish();
+    }
+
+    private void initUi(final int mode, final String account) {
+        Button signInButton1 = (Button) findViewById(R.id.sign_in_button_1);
+        Button signInButton2 = (Button) findViewById(R.id.sign_in_button_2);
+        Button confirmCredentialsButton = (Button) findViewById(R.id.confirm_credentials_button);
+
+        switch (mode) {
+            case MODE_ADD_ACCOUNT:
+                signInButton1.setOnClickListener(new OnClickListener() {
+                    @Override
+                    public void onClick(View view) {
+                        addAccount(Constants.ACCOUNT_1_NAME);
+                    }
+                });
+                signInButton2.setOnClickListener(new OnClickListener() {
+                    @Override
+                    public void onClick(View view) {
+                        addAccount(Constants.ACCOUNT_2_NAME);
+                    }
+                });
+                confirmCredentialsButton.setEnabled(false);
+                break;
+
+            case MODE_CONFIRM_CREDENTIALS:
+                signInButton1.setEnabled(false);
+                signInButton2.setEnabled(false);
+                confirmCredentialsButton.setOnClickListener(new OnClickListener() {
+                    @Override
+                    public void onClick(View view) {
+                        confirmCredentials(account);
+                    }
+                });
+                break;
+
+            default:
+                Log.w(TAG, "Opened the activity in an invalid mode: %d", mode);
+                signInButton1.setEnabled(false);
+                signInButton2.setEnabled(false);
+                confirmCredentialsButton.setEnabled(false);
+                break;
+        }
+    }
+}

commit 3fecfb79db47fb118c534beadfcf8ade8f48a11a
Author: droger <droger@chromium.org>
Date:   Tue Apr 5 04:43:36 2016 -0700

    tools/android/loading Add a /status URL
    
    This URL can be used to track the progress of a job.
    
    Review URL: https://codereview.chromium.org/1857653003
    
    Cr-Original-Commit-Position: refs/heads/master@{#385152}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cf2eac75c6c4241c1cab6bb2423ab1c941f783b4

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 0399de0..2bbd06b 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -77,6 +77,8 @@ where `urls.json` is a JSON dictionary with the keys:
 *   `emulate_device`: Name of the device to emulate. Optional.
 *   `emulate_network`: Type of network emulation. Optional.
 
+You can follow the progress at `http://<instance-ip>:8080/status`.
+
 ## Stop the app in the cloud
 
 ```shell
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 95c18c5..66bd7dd 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -6,6 +6,7 @@ import json
 import os
 import re
 import threading
+import time
 import subprocess
 
 from gcloud import storage
@@ -21,8 +22,12 @@ class ServerApp(object):
     """|configuration_file| is a path to a file containing JSON as described in
     README.md.
     """
-    self._tasks = []
+    self._tasks = []  # List of remaining tasks, only modified by _thread.
+    self._failed_tasks = []  # Failed tasks, only modified by _thread.
     self._thread = None
+    self._tasks_lock = threading.Lock()  # Protects _tasks and _failed_tasks.
+    self._initial_task_count = -1
+    self._start_time = None
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
     print 'Reading configuration'
@@ -87,6 +92,7 @@ class ServerApp(object):
   def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
                      log_filename):
     """ Generates a trace using analyze.py
+    Runs on _thread.
 
     Args:
       url: URL as a string.
@@ -115,24 +121,37 @@ class ServerApp(object):
                             stdout = log_file)
     return ret == 0
 
-  def _ProcessTasks(self, repeat_count, emulate_device, emulate_network):
+  def _GetCurrentTaskCount(self):
+    """Returns the number of remaining tasks. Thread safe."""
+    self._tasks_lock.acquire()
+    task_count = len(self._tasks)
+    self._tasks_lock.release()
+    return task_count
+
+  def _ProcessTasks(self, tasks, repeat_count, emulate_device, emulate_network):
     """Iterates over _tasks and runs analyze.py on each of them. Uploads the
     resulting traces to Google Cloud Storage.
+    Runs on _thread.
 
     Args:
+      tasks: The list of URLs to process.
       repeat_count: The number of traces generated for each URL.
       emulate_device: Name of the device to emulate. Empty for no emulation.
       emulate_network: Type of network emulation. Empty for no emulation.
     """
+    # The main thread might be reading the task lists, take the lock to modify.
+    self._tasks_lock.acquire()
+    self._tasks = tasks
+    self._failed_tasks = []
+    self._tasks_lock.release()
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
     logs_dir = self._base_path_in_bucket + 'analyze_logs/'
     log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
-    failed_tasks = []
     while len(self._tasks) > 0:
-      url = self._tasks.pop()
+      url = self._tasks[0]
       local_filename = pattern.sub('_', url)
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
@@ -143,15 +162,21 @@ class ServerApp(object):
           self._UploadFile(local_filename, traces_dir + remote_filename)
         else:
           print 'analyze.py failed for URL: %s' % url
-          failed_tasks.append({ "url": url, "repeat": repeat})
+          self._tasks_lock.acquire()
+          self._failed_tasks.append({ "url": url, "repeat": repeat})
+          self._tasks_lock.release()
           if os.path.isfile(local_filename):
             self._UploadFile(local_filename, failures_dir + remote_filename)
         print 'Uploading analyze log'
         self._UploadFile(log_filename, logs_dir + remote_filename)
+      # Pop once task is finished, for accurate status tracking.
+      self._tasks_lock.acquire()
+      url = self._tasks.popleft()
+      self._tasks_lock.release()
 
-    if len(failed_tasks) > 0:
+    if len(self._failed_tasks) > 0:
       print 'Uploading failing URLs'
-      self._UploadString(json.dumps(failed_tasks),
+      self._UploadString(json.dumps(self._failed_tasks, indent=2),
                          failures_dir + 'failures.json')
 
   def _SetTaskList(self, http_body):
@@ -164,29 +189,32 @@ class ServerApp(object):
       A string to be sent back to the client, describing the success status of
       the request.
     """
+    if self._thread is not None and self._thread.is_alive():
+      return 'Error: Already running\n'
+
     load_parameters = json.loads(http_body)
     try:
-      self._tasks = load_parameters['urls']
+      tasks = load_parameters['urls']
     except KeyError:
-      return 'Error: invalid urls'
+      return 'Error: invalid urls\n'
     # Optional parameters.
     try:
       repeat_count = int(load_parameters.get('repeat_count', '1'))
     except ValueError:
-      return 'Error: invalid repeat_count'
+      return 'Error: invalid repeat_count\n'
     emulate_device = load_parameters.get('emulate_device', '')
     emulate_network = load_parameters.get('emulate_network', '')
 
-    if len(self._tasks) == 0:
-      return 'Error: Empty task list'
-    elif self._thread is not None and self._thread.is_alive():
-      return 'Error: Already running'
+    if len(tasks) == 0:
+      return 'Error: Empty task list\n'
     else:
+      self._initial_task_count = len(tasks)
+      self._start_time = time.time()
       self._thread = threading.Thread(
           target = self._ProcessTasks,
-          args = (repeat_count, emulate_device, emulate_network))
+          args = (tasks, repeat_count, emulate_device, emulate_network))
       self._thread.start()
-      return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
+      return 'Starting generation of %s tasks\n' % str(self._initial_task_count)
 
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
@@ -200,7 +228,21 @@ class ServerApp(object):
       body = environ['wsgi.input'].read(body_size)
       data = self._SetTaskList(body)
     elif path == '/test':
-      data = 'hello'
+      data = 'hello\n'
+    elif path == '/status':
+      task_count = self._GetCurrentTaskCount()
+      if task_count == 0:
+        data = 'Idle\n'
+      else:
+        data = 'Remaining tasks: %s / %s\n' % (
+            task_count, self._initial_task_count)
+        elapsed = time.time() - self._start_time
+        data += 'Elapsed time: %s seconds\n' % str(elapsed)
+        self._tasks_lock.acquire()
+        failed_tasks = self._failed_tasks
+        self._tasks_lock.release()
+        data += '%s failed tasks:\n' % len(failed_tasks)
+        data += json.dumps(failed_tasks, indent=2)
     else:
       start_response('404 NOT FOUND', [('Content-Length', '0')])
       return iter([''])

commit e53369781ea73d371a995c76dc4f1e5a6a53b9ac
Author: mattcary <mattcary@chromium.org>
Date:   Tue Apr 5 01:26:24 2016 -0700

    Clovis: Update core set output to use full URL instead of shortened label.
    
    This also includes a generalization for doing approximate name matching which is currently unused (that is, we still identify resources by their URLs).
    
    Review URL: https://codereview.chromium.org/1859563002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385133}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 581720eb88d9a3b0b973c5fe33e3894e2f7164a0

diff --git a/loading/core_set.py b/loading/core_set.py
index 3bbb66f..910c280 100644
--- a/loading/core_set.py
+++ b/loading/core_set.py
@@ -42,7 +42,15 @@ def _PageCore(prefix, graph_set_names, output):
       sack.ConsumeGraph(graph)
       name_graphs.append(graph)
     graph_sets.append(name_graphs)
-  json.dump({'page_core': [l for l in sack.CoreSet(*graph_sets)],
+  core = sack.CoreSet(*graph_sets)
+  json.dump({'page_core': [{'label': b.label,
+                            'name': b.name,
+                            'count': b.num_nodes}
+                           for b in core],
+             'non_core': [{'label': b.label,
+                           'name': b.name,
+                           'count': b.num_nodes}
+                          for b in sack.bags if b not in core],
              'threshold': sack.CORE_THRESHOLD},
             output, sort_keys=True, indent=2)
   output.write('\n')
@@ -80,39 +88,9 @@ def _Spawn(site_list_file, graph_sets, input_dir, output_dir, workers):
                              for s in sites])
 
 
-def _AllCores(prefix, graph_set_names, output, threshold):
-  """Compute all core sets (per-set and overall page core) for a site."""
-  core_sets = []
-  _Progress('Using threshold %s' % threshold)
-  big_sack = resource_sack.GraphSack()
-  graph_sets = []
-  for name in graph_set_names:
-    _Progress('Finding core set for %s' % name)
-    sack = resource_sack.GraphSack()
-    sack.CORE_THRESHOLD = threshold
-    this_set = []
-    for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
-      _Progress('Reading %s' % filename)
-      trace = loading_trace.LoadingTrace.FromJsonFile(filename)
-      graph = dependency_graph.RequestDependencyGraph(
-          trace.request_track.GetEvents(),
-          request_dependencies_lens.RequestDependencyLens(trace))
-      sack.ConsumeGraph(graph)
-      big_sack.ConsumeGraph(graph)
-      this_set.append(graph)
-    core_sets.append({
-        'set_name': name,
-        'core_set': [l for l in sack.CoreSet()]
-    })
-    graph_sets.append(this_set)
-  json.dump({'core_sets': core_sets,
-             'page_core': [l for l in big_sack.CoreSet(*graph_sets)]},
-            output, sort_keys=True, indent=2)
-
-
 def _ReadCoreSet(filename):
   data = json.load(open(filename))
-  return set(data['page_core'])
+  return set(page['name'] for page in data['page_core'])
 
 
 def _Compare(a_name, b_name, csv):
@@ -172,28 +150,9 @@ if __name__ == '__main__':
                            help='trace file prefix')
   page_core.add_argument('--output', required=True,
                            help='JSON output file name')
-  page_core.set_defaults(
-      executor=lambda args:
-      _PageCore(args.prefix, args.sets.split(','), file(args.output, 'w')))
-
-  all_cores = subparsers.add_parser(
-      'all_cores',
-      help=('compute core and page core sets. Computes the core for each set '
-            'in --sets and then the overall page core using trace files '
-            'of form {--prefix}{set}*.trace. Outputs all the sets as JSON'))
-  all_cores.add_argument('--sets', required=True,
-                         help='sets to combine, comma-separated')
-  all_cores.add_argument('--prefix', required=True,
-                         help='input file prefix')
-  all_cores.add_argument('--output', required=True,
-                         help='JSON output file name')
-  all_cores.add_argument('--threshold',
-                         default=resource_sack.GraphSack.CORE_THRESHOLD,
-                         type=float, help='core set threshold')
-  all_cores.set_defaults(
-      executor=lambda args:
-      _AllCores(args.prefix, args.sets.split(','), file(args.output, 'w'),
-                args.threshold))
+  page_core.set_defaults(executor=lambda args:
+                         _PageCore(args.prefix, args.sets.split(','),
+                                   file(args.output, 'w')))
 
   compare = subparsers.add_parser(
       'compare',
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 563bb77..d7fe331 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -47,9 +47,23 @@ class GraphSack(object):
     # Maps graph -> _GraphInfo structures for each graph we've consumed.
     self._graph_info = {}
 
+    # How we generate names.
+    self._name_generator = lambda n: n.request.url
+
     # Our graph, updated after each ConsumeGraph.
     self._graph = None
 
+  def SetNameGenerator(self, generator):
+    """Set the generator we use for names.
+
+    This will define the equivalence class of requests we use to define sacks.
+
+    Args:
+      generator: a function taking a RequestDependencyGraph node and returning a
+        string.
+    """
+    self._name_generator = generator
+
   def ConsumeGraph(self, request_graph):
     """Add a graph and process.
 
@@ -66,6 +80,10 @@ class GraphSack(object):
     # explicit graph creation from the client.
     self._graph = graph.DirectedGraph(self.bags, self._edges.itervalues())
 
+  def GetBag(self, node):
+    """Find the bag for a node, or None if not found."""
+    return self._name_to_bag.get(self._name_generator(node), None)
+
   def AddNode(self, request_graph, node):
     """Add a node to our collection.
 
@@ -76,7 +94,7 @@ class GraphSack(object):
     Returns:
       The Bag containing the node.
     """
-    sack_name = self._GetSackName(node)
+    sack_name = self._name_generator(node)
     if sack_name not in self._name_to_bag:
       self._name_to_bag[sack_name] = Bag(self, sack_name)
     bag = self._name_to_bag[sack_name]
@@ -107,7 +125,7 @@ class GraphSack(object):
         computed.
 
     Returns:
-      A set of bag labels (as strings) in the core set.
+      A set of bags in the core set.
     """
     if not graph_sets:
       graph_sets = [self._graph_info.keys()]
@@ -151,12 +169,9 @@ class GraphSack(object):
     for b in self.bags:
       count = sum([g in graph_set for g in b.graphs])
       if float(count) / num_graphs > self.CORE_THRESHOLD:
-        core.add(b.label)
+        core.add(b)
     return core
 
-  def _GetSackName(self, node):
-    return self._MakeShortname(node.request.url)
-
   @classmethod
   def _MakeShortname(cls, url):
     # TODO(lizeb): Move this method to a convenient common location.
@@ -173,14 +188,19 @@ class GraphSack(object):
 
 
 class Bag(graph.Node):
-  def __init__(self, sack, label):
+  def __init__(self, sack, name):
     super(Bag, self).__init__()
     self._sack = sack
-    self._label = label
+    self._name = name
+    self._label = GraphSack._MakeShortname(name)
     # Maps a ResourceGraph to its Nodes contained in this Bag.
     self._graphs = defaultdict(set)
 
   @property
+  def name(self):
+    return self._name
+
+  @property
   def label(self):
     return self._label
 
@@ -192,6 +212,9 @@ class Bag(graph.Node):
   def num_nodes(self):
     return sum(len(g) for g in self._graphs.itervalues())
 
+  def GraphNodes(self, g):
+    return self._graphs.get(g, set())
+
   def AddNode(self, request_graph, node):
     if node in self._graphs[request_graph]:
       return  # Already added.
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index 5c3e54d..30c1d0e 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -72,7 +72,9 @@ class ResourceSackTestCase(unittest.TestCase):
 
   def test_Core(self):
     # We will use a core threshold of 0.5 to make it easier to define
-    # graphs. Resources 0 and 1 are core and others are not.
+    # graphs. Resources 0 and 1 are core and others are not. We check full names
+    # and node counts as we output that for core set analysis. In subsequent
+    # tests we just check labels to make the tests easier to read.
     graphs = [self.SimpleGraph([0, 1, 2]),
               self.SimpleGraph([0, 1, 3]),
               self.SimpleGraph([0, 1, 4]),
@@ -81,7 +83,8 @@ class ResourceSackTestCase(unittest.TestCase):
     sack.CORE_THRESHOLD = 0.5
     for g in graphs:
       sack.ConsumeGraph(g)
-    self.assertEqual(set(['0/', '1/']), sack.CoreSet())
+    self.assertEqual(set([('http://0', 4), ('http://1', 3)]),
+                     set((b.name, b.num_nodes) for b in sack.CoreSet()))
 
   def test_IntersectingCore(self):
     # Graph set A has core set {0, 1} and B {0, 2} so the final core set should
@@ -96,10 +99,13 @@ class ResourceSackTestCase(unittest.TestCase):
     for g in set_A + set_B + set_C:
       sack.ConsumeGraph(g)
     self.assertEqual(set(), sack.CoreSet())
-    self.assertEqual(set(['0/', '1/']), sack.CoreSet(set_A))
-    self.assertEqual(set(['0/', '2/']), sack.CoreSet(set_B))
+    self.assertEqual(set(['0/', '1/']),
+                     set(b.label for b in sack.CoreSet(set_A)))
+    self.assertEqual(set(['0/', '2/']),
+                     set(b.label for b in sack.CoreSet(set_B)))
     self.assertEqual(set(), sack.CoreSet(set_C))
-    self.assertEqual(set(['0/']), sack.CoreSet(set_A, set_B))
+    self.assertEqual(set(['0/']),
+                     set(b.label for b in sack.CoreSet(set_A, set_B)))
     self.assertEqual(set(), sack.CoreSet(set_A, set_B, set_C))
 
   def test_Simililarity(self):

commit 0d9e17938a76dcd6742051b2b80f799f539656fe
Author: agrieve <agrieve@chromium.org>
Date:   Mon Apr 4 19:03:45 2016 -0700

    Reland 2 of GN: Make breakpad_unittests & sandbox_linux_unittests use test()
    
    This simplifies build rules for native tests, and allows us to get rid
    of ${target}_deps targets (once recipes are updated).
    
    This change fixes the generated wrapper scripts, which didn't work.
    
    TBR=jbudorick
    BUG=589318
    
    Review URL: https://codereview.chromium.org/1854233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#385084}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 62ab0028a0165eecd60568783531d1ad3d031a26

diff --git a/forwarder2/BUILD.gn b/forwarder2/BUILD.gn
index a118d65..6899a7e 100644
--- a/forwarder2/BUILD.gn
+++ b/forwarder2/BUILD.gn
@@ -54,7 +54,6 @@ if (current_toolchain == default_toolchain) {
   create_native_executable_dist("device_forwarder_prepare_dist") {
     dist_dir = "$root_build_dir/forwarder_dist"
     binary = "$root_build_dir/device_forwarder"
-    include_main_binary = true
     deps = [
       ":device_forwarder",
     ]
diff --git a/md5sum/BUILD.gn b/md5sum/BUILD.gn
index 391ffb8..7cce4c9 100644
--- a/md5sum/BUILD.gn
+++ b/md5sum/BUILD.gn
@@ -38,7 +38,6 @@ if (current_toolchain == default_toolchain) {
   create_native_executable_dist("md5sum_prepare_dist") {
     dist_dir = "$root_build_dir/md5sum_dist"
     binary = "$root_build_dir/md5sum_bin"
-    include_main_binary = true
     deps = [
       ":md5sum_bin",
     ]

commit 531b53710ce2d434a738fca06565807e7147fbd9
Author: blundell <blundell@chromium.org>
Date:   Mon Apr 4 08:09:59 2016 -0700

    tools/android/loading: Initialize loading trace DB from Google Storage path
    
    This CL adds the ability to initialize a LoadingTraceDatabase instance from
    a JSON file stored in Google Storage (as opposed to locally). This will be
    needed in order to access the serialized database files that will be generated
    at the time of generating traces.
    
    Review URL: https://codereview.chromium.org/1856743002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384910}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 352303571a8842aa7a25b072d68be5535e7c316d

diff --git a/loading/google_storage_util.py b/loading/google_storage_util.py
new file mode 100644
index 0000000..82a59a5
--- /dev/null
+++ b/loading/google_storage_util.py
@@ -0,0 +1,19 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Contains utility functions for interacting with Google Storage."""
+
+import subprocess
+
+def ReadFromGoogleStorage(path):
+  """Given a Google Storage path, returns the contents of the file at that path
+     as a string. Will fail if the user does not have authorization to access
+     the path or if the path does not exist. To gain authorization, follow the
+     instructions for installing gsutil and setting up credentials to access
+     protected data that are on this page:
+     https://cloud.google.com/storage/docs/gsutil_install"""
+  # TODO(blundell): Change this to use the gcloud Python module once
+  # https://github.com/GoogleCloudPlatform/gcloud-python/issues/14360 is fixed.
+  contents = subprocess.check_output(["gsutil", "cat", path])
+  return contents
diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
index 89d608a..d2e4910 100644
--- a/loading/loading_trace_database.py
+++ b/loading/loading_trace_database.py
@@ -5,7 +5,7 @@
 """Represents a database of on-disk traces."""
 
 import json
-
+from google_storage_util import ReadFromGoogleStorage
 
 class LoadingTraceDatabase:
 
@@ -26,7 +26,7 @@ class LoadingTraceDatabase:
     return self._traces_dict
 
   def ToJsonFile(self, json_path):
-    """Save a json file representing this instance."""
+    """Saves a json file representing this instance."""
     json_dict = self.ToJsonDict()
     with open(json_path, 'w') as output_file:
        json.dump(json_dict, output_file, indent=2)
@@ -41,3 +41,10 @@ class LoadingTraceDatabase:
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:
       return cls.FromJsonDict(json.load(input_file))
+
+  @classmethod
+  def FromJsonFileInGoogleStorage(cls, json_google_storage_path):
+    """Returns an instance from a json file in Google Storage whose contents
+       were generated by ToJsonFile()."""
+    json_string = ReadFromGoogleStorage(json_google_storage_path)
+    return cls.FromJsonDict(json.loads(json_string))

commit 8e655f52e41b128165b2c9ca48cfaaf65834d3db
Author: dcheng <dcheng@chromium.org>
Date:   Sat Apr 2 11:35:46 2016 -0700

    Fix IWYU violators that don't include scoped_ptr.h in Android build.
    
    This blocks the conversion of //base from scoped_ptr to std::unique_ptr.
    
    BUG=554298
    R=avi@chromium.org
    TBR=brettw@chromium.org
    
    Review URL: https://codereview.chromium.org/1847373005
    
    Cr-Original-Commit-Position: refs/heads/master@{#384813}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: af7ea1d22dca21a645f58d3c649aa9fd2b2d4e04

diff --git a/md5sum/md5sum.cc b/md5sum/md5sum.cc
index dacf8f4..94efa10 100644
--- a/md5sum/md5sum.cc
+++ b/md5sum/md5sum.cc
@@ -17,6 +17,7 @@
 #include "base/files/file_util.h"
 #include "base/logging.h"
 #include "base/md5.h"
+#include "base/memory/scoped_ptr.h"
 
 namespace {
 

commit 8ab51a0c0ea715e0d817a1b5056fc5e39f7b015b
Author: blundell <blundell@chromium.org>
Date:   Fri Apr 1 08:29:51 2016 -0700

    tools/android/loading: Add simple database for trace files
    
    This CL adds a class whose purpose will be to allow for search/indexing
    over a set of tracefiles along various dimensions of interest (e.g.,
    "give me all the tracefiles for a given domain"). This class can be
    initialized from and serialized to an on-disk representation of the
    database.
    
    Followup CLs will generate this on-disk representation as part of
    generating traces in the cloud and add support for initializing the
    database from a serialized file stored in the cloud.
    
    Review URL: https://codereview.chromium.org/1850203002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384592}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6a6bec0e71f127adafe56bf659769fb9aa97edf5

diff --git a/loading/loading_trace_database.py b/loading/loading_trace_database.py
new file mode 100644
index 0000000..89d608a
--- /dev/null
+++ b/loading/loading_trace_database.py
@@ -0,0 +1,43 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Represents a database of on-disk traces."""
+
+import json
+
+
+class LoadingTraceDatabase:
+
+  def __init__(self, traces_dict):
+    """traces_dict is a dictionary mapping filenames of traces to metadata
+       about those traces."""
+    self._traces_dict = traces_dict
+
+  def GetTraceFilesForURL(self, url):
+    """Given a URL, returns the set of filenames of traces that were generated
+       for this URL."""
+    trace_files = [f for f in self._traces_dict.keys()
+        if self._traces_dict[f]["url"] == url]
+    return trace_files
+
+  def ToJsonDict(self):
+    """Returns a dict representing this instance."""
+    return self._traces_dict
+
+  def ToJsonFile(self, json_path):
+    """Save a json file representing this instance."""
+    json_dict = self.ToJsonDict()
+    with open(json_path, 'w') as output_file:
+       json.dump(json_dict, output_file, indent=2)
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns an instance from a dict returned by ToJsonDict()."""
+    return LoadingTraceDatabase(json_dict)
+
+  @classmethod
+  def FromJsonFile(cls, json_path):
+    """Returns an instance from a json file saved by ToJsonFile()."""
+    with open(json_path) as input_file:
+      return cls.FromJsonDict(json.load(input_file))
diff --git a/loading/loading_trace_database_unittest.py b/loading/loading_trace_database_unittest.py
new file mode 100644
index 0000000..775638c
--- /dev/null
+++ b/loading/loading_trace_database_unittest.py
@@ -0,0 +1,37 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from loading_trace_database import LoadingTraceDatabase
+
+
+class LoadingTraceDatabaseUnittest(unittest.TestCase):
+  _JSON_DATABASE = {
+    "traces/trace1.json" : { "url" : "http://bar.html", },
+    "traces/trace2.json" : { "url" : "http://bar.html", },
+    "traces/trace3.json" : { "url" : "http://qux.html", },
+  }
+
+  def setUp(self):
+    self.database = LoadingTraceDatabase.FromJsonDict(self._JSON_DATABASE)
+
+  def testGetTraceFilesForURL(self):
+    # Test a URL with no matching traces.
+    self.assertEqual(
+        self.database.GetTraceFilesForURL("http://foo.html"),
+        [])
+
+    # Test a URL with matching traces.
+    self.assertEqual(
+        set(self.database.GetTraceFilesForURL("http://bar.html")),
+        set(["traces/trace1.json", "traces/trace2.json"]))
+
+  def testSerialization(self):
+    self.assertEqual(
+        self._JSON_DATABASE, self.database.ToJsonDict())
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 19ca7df295331bf924064a50b973936faf93ba6d
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 08:11:08 2016 -0700

    tools/android/loading Deploy module.json for device emulation on GCE
    
    Review URL: https://codereview.chromium.org/1847323003
    
    Cr-Original-Commit-Position: refs/heads/master@{#384586}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 076f05ee140ffeb19aec751590c4a434bb12fb94

diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 0a3a531..2362414 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -28,7 +28,7 @@ cp -r tools/android/loading/gce $tmp_src_dir/tools/android/loading
 # Copy other dependencies.
 mkdir $tmp_src_dir/third_party
 rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
-  --delete third_party/catapult $tmp_src_dir/third_party
+  third_party/catapult $tmp_src_dir/third_party
 mkdir $tmp_src_dir/tools/perf
 cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
 mkdir -p $tmp_src_dir/build/android
@@ -36,6 +36,10 @@ cp build/android/devil_chromium.py $tmp_src_dir/build/android/
 cp build/android/video_recorder.py $tmp_src_dir/build/android/
 cp build/android/devil_chromium.json $tmp_src_dir/build/android/
 cp -r build/android/pylib $tmp_src_dir/build/android/
+mkdir -p \
+  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices
+cp third_party/WebKit/Source/devtools/front_end/emulated_devices/module.json \
+  $tmp_src_dir/third_party/WebKit/Source/devtools/front_end/emulated_devices/
 
 # Tar up the source and copy it to Google Cloud Storage.
 source_tarball=$tmpdir/source.tgz

commit 92d05b8d0d1aed43a1ce5dd6e84d23f89733a24c
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 05:40:05 2016 -0700

    tools/android/loading Add GCE support for device and network emulation
    
    Review URL: https://codereview.chromium.org/1853653003
    
    Cr-Original-Commit-Position: refs/heads/master@{#384564}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cc06b8b66a7be3dfdb0421a4de864f4b55091d3b

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 52e0c82..0399de0 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -73,7 +73,9 @@ where `urls.json` is a JSON dictionary with the keys:
 
 *   `urls`: array of URLs
 *   `repeat_count`: Number of times each URL will be loaded. Each load of a URL
-    generates a separate trace file.
+    generates a separate trace file. Optional.
+*   `emulate_device`: Name of the device to emulate. Optional.
+*   `emulate_network`: Type of network emulation. Optional.
 
 ## Stop the app in the cloud
 
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 9b74ec6..95c18c5 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -84,13 +84,16 @@ class ServerApp(object):
     blob.upload_from_string(data_string)
     return blob.public_url
 
-  def _GenerateTrace(self, url, filename, log_filename):
+  def _GenerateTrace(self, url, emulate_device, emulate_network, filename,
+                     log_filename):
     """ Generates a trace using analyze.py
 
     Args:
-      url: url as a string.
-      filename: name of the file where the trace is saved.
-      log_filename: name of the file where standard output and errors are logged
+      url: URL as a string.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
+      filename: Name of the file where the trace is saved.
+      log_filename: Name of the file where standard output and errors are logged
 
     Returns:
       True if the trace was generated successfully.
@@ -103,17 +106,23 @@ class ServerApp(object):
     command_line = ['python', analyze_path, 'log_requests', '--local_noisy',
         '--clear_cache', '--local', '--headless', '--local_binary',
         self._chrome_path, '--url', url, '--output', filename]
+    if len(emulate_device):
+      command_line += ['--emulate_device', emulate_device]
+    if len(emulate_network):
+      command_line += ['--emulate_network', emulate_network]
     with open(log_filename, 'w') as log_file:
       ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
                             stdout = log_file)
     return ret == 0
 
-  def _ProcessTasks(self, repeat_count):
+  def _ProcessTasks(self, repeat_count, emulate_device, emulate_network):
     """Iterates over _tasks and runs analyze.py on each of them. Uploads the
     resulting traces to Google Cloud Storage.
 
     Args:
       repeat_count: The number of traces generated for each URL.
+      emulate_device: Name of the device to emulate. Empty for no emulation.
+      emulate_network: Type of network emulation. Empty for no emulation.
     """
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
@@ -128,7 +137,8 @@ class ServerApp(object):
       for repeat in range(repeat_count):
         print 'Generating trace for URL: %s' % url
         remote_filename = local_filename + '/' + str(repeat)
-        if self._GenerateTrace(url, local_filename, log_filename):
+        if self._GenerateTrace(
+            url, emulate_device, emulate_network, local_filename, log_filename):
           print 'Uploading: %s' % remote_filename
           self._UploadFile(local_filename, traces_dir + remote_filename)
         else:
@@ -159,18 +169,22 @@ class ServerApp(object):
       self._tasks = load_parameters['urls']
     except KeyError:
       return 'Error: invalid urls'
+    # Optional parameters.
     try:
-      repeat_count = int(load_parameters['repeat_count'])
-    except (KeyError, ValueError):
+      repeat_count = int(load_parameters.get('repeat_count', '1'))
+    except ValueError:
       return 'Error: invalid repeat_count'
+    emulate_device = load_parameters.get('emulate_device', '')
+    emulate_network = load_parameters.get('emulate_network', '')
 
     if len(self._tasks) == 0:
       return 'Error: Empty task list'
     elif self._thread is not None and self._thread.is_alive():
       return 'Error: Already running'
     else:
-      self._thread = threading.Thread(target = self._ProcessTasks,
-                                      args = (repeat_count,))
+      self._thread = threading.Thread(
+          target = self._ProcessTasks,
+          args = (repeat_count, emulate_device, emulate_network))
       self._thread.start()
       return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
 

commit 9a7fce0251cf143569be8cbb5ea4138b66b9357c
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 1 04:58:00 2016 -0700

    clovis: Replace loading_model with loading_graph_view.
    
    This completes the refactoring of loading_model.py.
    
    Review URL: https://codereview.chromium.org/1850683002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384556}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 09d7bc4db813cc546a5a890fb72a1f0c106f1568

diff --git a/loading/analyze.py b/loading/analyze.py
index fd1ba4e..db0106f 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -29,11 +29,12 @@ import content_classification_lens
 import controller
 import device_setup
 import frame_load_lens
-import loading_model
+import loading_graph_view
+import loading_graph_view_visualization
 import loading_trace
-import model_graph
 import options
-
+import request_dependencies_lens
+import request_track
 
 # TODO(mattcary): logging.info isn't that useful, as the whole (tools) world
 # uses logging info; we need to introduce logging modules to get finer-grained
@@ -68,11 +69,11 @@ def _WriteJson(output, json_data):
   json.dump(json_data, output, sort_keys=True, indent=2)
 
 
-def _GetPrefetchHtml(graph, name=None):
+def _GetPrefetchHtml(graph_view, name=None):
   """Generate prefetch page for the resources in resource graph.
 
   Args:
-    graph: a ResourceGraph.
+    graph_view: (LoadingGraphView)
     name: optional string used in the generated page.
 
   Returns:
@@ -89,8 +90,8 @@ def _GetPrefetchHtml(graph, name=None):
 <head>
 <title>%s</title>
 """ % title)
-  for info in graph.ResourceInfo():
-    output.append('<link rel="prefetch" href="%s">\n' % info.Url())
+  for node in graph_view.deps_graph.graph.Nodes():
+    output.append('<link rel="prefetch" href="%s">\n' % node.request.url)
   output.append("""</head>
 <body>%s</body>
 </html>
@@ -139,8 +140,7 @@ def _FullFetch(url, json_output, prefetch):
   if prefetch:
     assert not OPTIONS.local
     logging.warning('Generating prefetch')
-    prefetch_html = _GetPrefetchHtml(
-        loading_model.ResourceGraph(cold_data), name=url)
+    prefetch_html = _GetPrefetchHtml(_ProcessJsonTrace(cold_data), name=url)
     tmp = tempfile.NamedTemporaryFile()
     tmp.write(prefetch_html)
     tmp.flush()
@@ -165,19 +165,24 @@ def _FullFetch(url, json_output, prefetch):
     logging.warning('Wrote ' + json_output)
 
 
-def _ProcessRequests(filename):
+def _ProcessTraceFile(filename):
   with open(filename) as f:
-    trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
+    return _ProcessJsonTrace(json.load(f))
+
+
+def _ProcessJsonTrace(json_dict):
+    trace = loading_trace.LoadingTrace.FromJsonDict(json_dict)
     content_lens = (
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
             trace, OPTIONS.ad_rules, OPTIONS.tracking_rules))
     frame_lens = frame_load_lens.FrameLoadLens(trace)
     activity = activity_lens.ActivityLens(trace)
-    graph = loading_model.ResourceGraph(
-        trace, content_lens, frame_lens, activity)
+    deps_lens = request_dependencies_lens.RequestDependencyLens(trace)
+    graph_view = loading_graph_view.LoadingGraphView(
+        trace, deps_lens, content_lens, frame_lens, activity)
     if OPTIONS.noads:
-      graph.Set(node_filter=graph.FilterAds)
-    return graph
+      graph_view.RemoveAds()
+    return graph_view
 
 
 def InvalidCommand(cmd):
@@ -189,8 +194,10 @@ def DoPng(arg_str):
   OPTIONS.ParseArgs(arg_str, description='Generates a PNG from a trace',
                     extra=['request_json', ('--png_output', ''),
                            ('--eog', False)])
-  graph = _ProcessRequests(OPTIONS.request_json)
-  visualization = model_graph.GraphVisualization(graph)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
+  visualization = (
+      loading_graph_view_visualization.LoadingGraphViewVisualization(
+          graph_view))
   tmp = tempfile.NamedTemporaryFile()
   visualization.OutputDot(tmp)
   tmp.flush()
@@ -208,26 +215,13 @@ def DoPng(arg_str):
   tmp.close()
 
 
-def DoCompare(arg_str):
-  OPTIONS.ParseArgs(arg_str, description='Compares two traces',
-                    extra=['g1_json', 'g2_json'])
-  g1 = _ProcessRequests(OPTIONS.g1_json)
-  g2 = _ProcessRequests(OPTIONS.g2_json)
-  discrepancies = loading_model.ResourceGraph.CheckImageLoadConsistency(g1, g2)
-  if discrepancies:
-    print '%d discrepancies' % len(discrepancies)
-    print '\n'.join([str(r) for r in discrepancies])
-  else:
-    print 'Consistent!'
-
-
 def DoPrefetchSetup(arg_str):
   OPTIONS.ParseArgs(arg_str, description='Sets up prefetch',
                     extra=['request_json', 'target_html', ('--upload', False)])
-  graph = _ProcessRequests(OPTIONS.request_json)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
   with open(OPTIONS.target_html, 'w') as html:
     html.write(_GetPrefetchHtml(
-        graph, name=os.path.basename(OPTIONS.request_json)))
+        graph_view, name=os.path.basename(OPTIONS.request_json)))
   if OPTIONS.upload:
     device = device_setup.GetFirstDevice()
     destination = os.path.join('/sdcard/Download',
@@ -265,35 +259,34 @@ def DoFetch(arg_str):
 def DoLongPole(arg_str):
   OPTIONS.ParseArgs(arg_str, description='Calculates long pole',
                     extra='request_json')
-  graph = _ProcessRequests(OPTIONS.request_json)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
   path_list = []
-  cost = graph.Cost(path_list=path_list)
-  print '%s (%s)' % (path_list[-1], cost)
+  cost = graph_view.deps_graph.Cost(path_list=path_list)
+  print '%s (%s)' % (path_list[-1].request.url, cost)
 
 
 def DoNodeCost(arg_str):
   OPTIONS.ParseArgs(arg_str,
                     description='Calculates node cost',
                     extra='request_json')
-  graph = _ProcessRequests(OPTIONS.request_json)
-  print sum((n.NodeCost() for n in graph.Nodes()))
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
+  print sum((n.cost for n in graph_view.deps_graph.graph.Nodes()))
 
 
 def DoCost(arg_str):
   OPTIONS.ParseArgs(arg_str,
                     description='Calculates total cost',
                     extra=['request_json', ('--path', False)])
-  graph = _ProcessRequests(OPTIONS.request_json)
+  graph_view = _ProcessTraceFile(OPTIONS.request_json)
   path_list = []
-  print 'Graph cost: %s' % graph.Cost(path_list)
+  print 'Graph cost: %s' % graph_view.deps_graph.Cost(path_list=path_list)
   if OPTIONS.path:
-    for p in path_list:
-      print '  ' + p.ShortName()
+    for n in path_list:
+      print '  ' + request_track.ShortName(n.request.url)
 
 
 COMMAND_MAP = {
     'png': DoPng,
-    'compare': DoCompare,
     'prefetch_setup': DoPrefetchSetup,
     'log_requests': DoLogRequests,
     'longpole': DoLongPole,
diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index 13f5485..955177c 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -11,19 +11,20 @@ import graph
 import request_track
 
 
-class _RequestNode(graph.Node):
+class RequestNode(graph.Node):
   def __init__(self, request):
-    super(_RequestNode, self).__init__()
+    super(RequestNode, self).__init__()
     self.request = request
     self.cost = request.Cost()
 
 
-class _Edge(graph.Edge):
+class Edge(graph.Edge):
   def __init__(self, from_node, to_node, reason):
-    super(_Edge, self).__init__(from_node, to_node)
+    super(Edge, self).__init__(from_node, to_node)
     self.reason = reason
     self.cost = request_track.TimeBetween(
         self.from_node.request, self.to_node.request, self.reason)
+    self.is_timing = False
 
 
 class RequestDependencyGraph(object):
@@ -34,16 +35,21 @@ class RequestDependencyGraph(object):
   _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
   _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
 
-  def __init__(self, requests, dependencies_lens):
+  def __init__(self, requests, dependencies_lens,
+               node_class=RequestNode, edge_class=Edge):
     """Creates a request dependency graph.
 
     Args:
       requests: ([Request]) a list of requests.
       dependencies_lens: (RequestDependencyLens)
+      node_class: (subclass of RequestNode)
+      edge_class: (subclass of Edge)
     """
+    assert issubclass(node_class, RequestNode)
+    assert issubclass(edge_class, Edge)
     self._requests = requests
     deps = dependencies_lens.GetRequestDependencies()
-    self._nodes_by_id = {r.request_id : _RequestNode(r) for r in self._requests}
+    self._nodes_by_id = {r.request_id : node_class(r) for r in self._requests}
     edges = []
     for (parent_request, child_request, reason) in deps:
       if (parent_request.request_id not in self._nodes_by_id
@@ -51,7 +57,7 @@ class RequestDependencyGraph(object):
         continue
       parent_node = self._nodes_by_id[parent_request.request_id]
       child_node = self._nodes_by_id[child_request.request_id]
-      edges.append(_Edge(parent_node, child_node, reason))
+      edges.append(edge_class(parent_node, child_node, reason))
     self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
     self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
     self._HandleTimingDependencies()
@@ -73,17 +79,20 @@ class RequestDependencyGraph(object):
       if request_id in request_id_to_cost:
         node.cost = request_id_to_cost[request_id]
 
-  def Cost(self, from_first_request=True):
+  def Cost(self, from_first_request=True, path_list=None, costs_out=None):
     """Returns the cost of the graph, that is the costliest path.
 
     Args:
       from_first_request: (boolean) If True, only considers paths that originate
                           from the first request node.
+      path_list: (list) See graph.Cost().
+      costs_out: (list) See graph.Cost().
     """
     if from_first_request:
-      return self._deps_graph.Cost([self._first_request_node])
+      return self._deps_graph.Cost(
+          [self._first_request_node], path_list, costs_out)
     else:
-      return self._deps_graph.Cost()
+      return self._deps_graph.Cost(path_list=path_list, costs_out=costs_out)
 
   def _HandleTimingDependencies(self):
     try:
@@ -162,6 +171,7 @@ class RequestDependencyGraph(object):
                   # eligible.
       if (edges_by_end_time[end_mark].to_node.request.end_msec
           <= current.to_node.request.start_msec):
+        current.is_timing = True
         self._deps_graph.UpdateEdge(
             current, edges_by_end_time[end_mark].to_node,
             current.to_node)
diff --git a/loading/graph.py b/loading/graph.py
index 789d917..d051d58 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -75,6 +75,10 @@ class DirectedGraph(object):
     """Returns the set of edges of this graph."""
     return self._edges
 
+  def RootNodes(self):
+    """Returns an iterable of nodes that have no incoming edges."""
+    return filter(lambda n: not self.InEdges(n), self._nodes)
+
   def UpdateEdge(self, edge, new_from_node, new_to_node):
     """Updates an edge.
 
@@ -125,15 +129,23 @@ class DirectedGraph(object):
           sources.append(successor)
     return sorted_nodes
 
-  def ReachableNodes(self, roots):
-    """Returns a list of nodes from a set of root nodes."""
+  def ReachableNodes(self, roots, should_stop=lambda n: False):
+    """Returns a list of nodes from a set of root nodes.
+
+    Args:
+      roots: ([Node]) List of roots to start from.
+      should_stop: (callable) Returns True when a node should stop the
+                   exploration and be skipped.
+    """
     visited = set()
-    fifo = collections.deque(roots)
+    fifo = collections.deque([n for n in roots if not should_stop(n)])
     while len(fifo) != 0:
       node = fifo.pop()
+      if should_stop(node):
+        continue
       visited.add(node)
       for e in self.OutEdges(node):
-        if e.to_node not in visited:
+        if e.to_node not in visited and not should_stop(e.to_node):
           visited.add(e.to_node)
         fifo.appendleft(e.to_node)
     return list(visited)
diff --git a/loading/loading_graph_view.py b/loading/loading_graph_view.py
new file mode 100644
index 0000000..c312af0
--- /dev/null
+++ b/loading/loading_graph_view.py
@@ -0,0 +1,88 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Views a trace as an annotated request dependency graph."""
+
+import dependency_graph
+
+
+class RequestNode(dependency_graph.RequestNode):
+  """Represents a request in the graph.
+
+  is_ad and is_tracking are set according to the ContentClassificationLens
+  passed to LoadingGraphView.
+  """
+  def __init__(self, request):
+    super(RequestNode, self).__init__(request)
+    self.is_ad = False
+    self.is_tracking = False
+
+
+class Edge(dependency_graph.Edge):
+  """Represents a dependency between two nodes.
+
+  activity is set according to the ActivityLens passed to LoadingGraphView.
+  """
+  def __init__(self, from_node, to_node, reason):
+    super(Edge, self).__init__(from_node, to_node, reason)
+    self.activity = {}
+
+
+class LoadingGraphView(object):
+  """Represents a trace as a dependency graph. The graph is annotated using
+     optional lenses passed to it.
+  """
+  def __init__(self, trace, dependencies_lens, content_lens=None,
+               frame_lens=None, activity=None):
+    """Initalizes a LoadingGraphView instance.
+
+    Args:
+      trace: (LoadingTrace) a loading trace.
+      dependencies_lens: (RequestDependencyLens)
+      content_lens: (ContentClassificationLens)
+      frame_lens: (FrameLoadLens)
+      activity: (ActivityLens)
+    """
+    self._requests = trace.request_track.GetEvents()
+    self._deps_lens = dependencies_lens
+    self._content_lens = content_lens
+    self._frame_lens = frame_lens
+    self._activity_lens = activity
+    self._graph = None
+    self._BuildGraph()
+
+  def RemoveAds(self):
+    """Updates the graph to remove the Ads.
+
+    Nodes that are only reachable through ad nodes are excluded as well.
+    """
+    roots = self._graph.graph.RootNodes()
+    self._requests = [n.request for n in self._graph.graph.ReachableNodes(
+        roots, should_stop=lambda n: n.is_ad or n.is_tracking)]
+    self._BuildGraph()
+
+  @property
+  def deps_graph(self):
+    return self._graph
+
+  def _BuildGraph(self):
+    self._graph = dependency_graph.RequestDependencyGraph(
+        self._requests, self._deps_lens, RequestNode, Edge)
+    self._AnnotateNodes()
+    self._AnnotateEdges()
+
+  def _AnnotateNodes(self):
+    if self._content_lens is None:
+      return
+    for node in self._graph.graph.Nodes():
+      node.is_ad = self._content_lens.IsAdRequest(node.request)
+      node.is_tracking = self._content_lens.IsTrackingRequest(node.request)
+
+  def _AnnotateEdges(self):
+    if self._activity_lens is None:
+      return
+    for edge in self._graph.graph.Edges():
+      dep = (edge.from_node.request, edge.to_node.request, edge.reason)
+      activity = self._activity_lens.BreakdownEdgeActivityByInitiator(dep)
+      edge.activity = activity
diff --git a/loading/loading_graph_view_unittest.py b/loading/loading_graph_view_unittest.py
new file mode 100644
index 0000000..b1a93c0
--- /dev/null
+++ b/loading/loading_graph_view_unittest.py
@@ -0,0 +1,84 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import loading_graph_view
+import request_dependencies_lens
+from request_dependencies_lens_unittest import TestRequests
+
+
+class MockContentClassificationLens(object):
+  def __init__(self, ad_request_ids, tracking_request_ids):
+    self._ad_requests_ids = ad_request_ids
+    self._tracking_request_ids = tracking_request_ids
+
+  def IsAdRequest(self, request):
+    return request.request_id in self._ad_requests_ids
+
+  def IsTrackingRequest(self, request):
+    return request.request_id in self._tracking_request_ids
+
+
+class LoadingGraphViewTestCase(unittest.TestCase):
+  def setUp(self):
+    super(LoadingGraphViewTestCase, self).setUp()
+    self.trace = TestRequests.CreateLoadingTrace()
+    self.deps_lens = request_dependencies_lens.RequestDependencyLens(self.trace)
+
+  def testAnnotateNodesNoLenses(self):
+    graph_view = loading_graph_view.LoadingGraphView(self.trace, self.deps_lens)
+    for node in graph_view.deps_graph.graph.Nodes():
+      self.assertFalse(node.is_ad)
+      self.assertFalse(node.is_tracking)
+    for edge in graph_view.deps_graph.graph.Edges():
+      self.assertFalse(edge.is_timing)
+
+  def testAnnotateNodesContentLens(self):
+    ad_request_ids = set([TestRequests.JS_REQUEST_UNRELATED_FRAME.request_id])
+    tracking_request_ids = set([TestRequests.JS_REQUEST.request_id])
+    content_lens = MockContentClassificationLens(
+        ad_request_ids, tracking_request_ids)
+    graph_view = loading_graph_view.LoadingGraphView(self.trace, self.deps_lens,
+                                                     content_lens)
+    for node in graph_view.deps_graph.graph.Nodes():
+      request_id = node.request.request_id
+      self.assertEqual(request_id in ad_request_ids, node.is_ad)
+      self.assertEqual(request_id in tracking_request_ids, node.is_tracking)
+
+  def testRemoveAds(self):
+    ad_request_ids = set([TestRequests.JS_REQUEST_UNRELATED_FRAME.request_id])
+    tracking_request_ids = set([TestRequests.JS_REQUEST.request_id])
+    content_lens = MockContentClassificationLens(
+        ad_request_ids, tracking_request_ids)
+    graph_view = loading_graph_view.LoadingGraphView(self.trace, self.deps_lens,
+                                                     content_lens)
+    graph_view.RemoveAds()
+    request_ids = set([n.request.request_id
+                       for n in graph_view.deps_graph.graph.Nodes()])
+    expected_request_ids = set([r.request_id for r in [
+        TestRequests.FIRST_REDIRECT_REQUEST,
+        TestRequests.SECOND_REDIRECT_REQUEST,
+        TestRequests.REDIRECTED_REQUEST,
+        TestRequests.REQUEST,
+        TestRequests.JS_REQUEST_OTHER_FRAME]])
+    self.assertSetEqual(expected_request_ids, request_ids)
+
+  def testRemoveAdsPruneGraph(self):
+    ad_request_ids = set([TestRequests.SECOND_REDIRECT_REQUEST.request_id])
+    tracking_request_ids = set([])
+    content_lens = MockContentClassificationLens(
+        ad_request_ids, tracking_request_ids)
+    graph_view = loading_graph_view.LoadingGraphView(
+        self.trace, self.deps_lens, content_lens)
+    graph_view.RemoveAds()
+    request_ids = set([n.request.request_id
+                       for n in graph_view.deps_graph.graph.Nodes()])
+    expected_request_ids = set(
+        [TestRequests.FIRST_REDIRECT_REQUEST.request_id])
+    self.assertSetEqual(expected_request_ids, request_ids)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/loading_graph_view_visualization.py b/loading/loading_graph_view_visualization.py
new file mode 100644
index 0000000..d083575
--- /dev/null
+++ b/loading/loading_graph_view_visualization.py
@@ -0,0 +1,169 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Visualize a loading_graph_view.LoadingGraphView."""
+
+import request_track
+
+
+class LoadingGraphViewVisualization(object):
+  """Manipulate visual representations of a request graph.
+
+  Currently only DOT output is supported.
+  """
+  _LONG_EDGE_THRESHOLD_MS = 2000  # Time in milliseconds.
+
+  _CONTENT_KIND_TO_COLOR = {
+      'application':     'blue',      # Scripts.
+      'font':            'grey70',
+      'image':           'orange',    # This probably catches gifs?
+      'video':           'hotpink1',
+      'audio':           'hotpink2',
+      }
+
+  _CONTENT_TYPE_TO_COLOR = {
+      'html':            'red',
+      'css':             'green',
+      'script':          'blue',
+      'javascript':      'blue',
+      'json':            'purple',
+      'gif':             'grey',
+      'image':           'orange',
+      'jpeg':            'orange',
+      'ping':            'cyan',  # Empty response
+      'redirect':        'forestgreen',
+      'png':             'orange',
+      'plain':           'brown3',
+      'octet-stream':    'brown3',
+      'other':           'white',
+      }
+
+  _EDGE_REASON_TO_COLOR = {
+    'redirect': 'black',
+    'parser': 'red',
+    'script': 'blue',
+    'script_inferred': 'purple',
+  }
+
+  _ACTIVITY_TYPE_LABEL = (
+      ('idle', 'I'), ('unrelated_work', 'W'), ('script', 'S'),
+      ('parsing', 'P'), ('other_url', 'O'), ('unknown_url', 'U'))
+
+  def __init__(self, graph_view):
+    """Initialize.
+
+    Args:
+      graph_view: (loading_graph_view.LoadingGraphView) the graph to visualize.
+    """
+    self._graph_view = graph_view
+    self._global_start = None
+
+  def OutputDot(self, output):
+    """Output DOT (graphviz) representation.
+
+    Args:
+      output: a file-like output stream to receive the dot file.
+    """
+    nodes = self._graph_view.deps_graph.graph.Nodes()
+    self._global_start = min(n.request.start_msec for n in nodes)
+    g = self._graph_view.deps_graph.graph
+
+    output.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+
+    isolated_nodes = [
+        n for n in nodes if (
+            len(g.InEdges(n)) == 0 and len(g.OutEdges(n)) == 0)]
+    if isolated_nodes:
+      output.write("""subgraph cluster_isolated {
+                        color=black;
+                        label="Isolated Nodes";
+                   """)
+      for n in isolated_nodes:
+        output.write(self._DotNode(n))
+      output.write('}\n')
+
+    output.write("""subgraph cluster_nodes {
+                      color=invis;
+                 """)
+    for n in nodes:
+      if n in isolated_nodes:
+        continue
+      output.write(self._DotNode(n))
+
+    edges = g.Edges()
+    for edge in edges:
+      output.write(self._DotEdge(edge))
+
+    output.write('}\n')
+    output.write('}\n')
+
+  def _ContentTypeToColor(self, content_type):
+    if not content_type:
+      type_str = 'other'
+    elif '/' in content_type:
+      kind, type_str = content_type.split('/', 1)
+      if kind in self._CONTENT_KIND_TO_COLOR:
+        return self._CONTENT_KIND_TO_COLOR[kind]
+    else:
+      type_str = content_type
+    return self._CONTENT_TYPE_TO_COLOR[type_str]
+
+  def _DotNode(self, node):
+    """Returns a graphviz node description for a given node.
+
+    Args:
+      node: (RequestNode)
+
+    Returns:
+      A string describing the resource in graphviz format.
+      The resource is color-coded according to its content type, and its shape
+      is oval if its max-age is less than 300s (or if it's not cacheable).
+    """
+    color = self._ContentTypeToColor(node.request.GetContentType())
+    request = node.request
+    max_age = request.MaxAge()
+    shape = 'polygon' if max_age > 300 else 'oval'
+    styles = ['filled']
+    if node.is_ad or node.is_tracking:
+      styles += ['bold', 'diagonals']
+    return ('"%s" [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
+            'fillcolor = %s; shape = %s];\n'
+            % (request.request_id, request_track.ShortName(request.url),
+               request.start_msec - self._global_start,
+               request.end_msec - self._global_start,
+               request.end_msec - request.start_msec,
+               ','.join(styles), color, shape))
+
+  def _DotEdge(self, edge):
+    """Returns a graphviz edge description for a given edge.
+
+    Args:
+      edge: (Edge)
+
+    Returns:
+      A string encoding the graphviz representation of the edge.
+    """
+    style = {'color': 'orange'}
+    label = '%.02f' % edge.cost
+    if edge.is_timing:
+      style['style'] = 'dashed'
+    style['color'] = self._EDGE_REASON_TO_COLOR[edge.reason]
+    if edge.cost > self._LONG_EDGE_THRESHOLD_MS:
+      style['penwidth'] = '5'
+      style['weight'] = '2'
+    style_str = '; '.join('%s=%s' % (k, v) for (k, v) in style.items())
+
+    label = '%.02f' % edge.cost
+    if edge.activity:
+      separator = ' - '
+      for activity_type, activity_label in self._ACTIVITY_TYPE_LABEL:
+        label += '%s%s:%.02f ' % (
+            separator, activity_label, edge.activity[activity_type])
+        separator = ' '
+    arrow = '[%s; label="%s"]' % (style_str, label)
+    from_request_id = edge.from_node.request.request_id
+    to_request_id = edge.to_node.request.request_id
+    return '"%s" -> "%s" %s;\n' % (from_request_id, to_request_id, arrow)
diff --git a/loading/loading_model.py b/loading/loading_model.py
deleted file mode 100644
index b7f8002..0000000
--- a/loading/loading_model.py
+++ /dev/null
@@ -1,610 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Models for loading in chrome.
-
-(Redirect the following to the general model module once we have one)
-A model is an object with the following methods.
-  CostMs(): return the cost of the model in milliseconds.
-  Set(): set model-specific parameters.
-
-ResourceGraph
-  This creates a DAG of resource dependencies from loading.log_requests to model
-  loading time. The model may be parameterized by changing the loading time of
-  a particular or all resources.
-"""
-
-import logging
-import os
-
-import sys
-
-import activity_lens
-import dag
-import loading_trace
-import request_dependencies_lens
-import request_track
-
-class ResourceGraph(object):
-  """A model of loading by a DAG of resource dependencies.
-
-  See model parameters in Set().
-  """
-  # The lens to build request dependencies. Exposed here for subclasses in
-  # unittesting.
-  REQUEST_LENS = request_dependencies_lens.RequestDependencyLens
-
-  EDGE_KIND_KEY = 'edge_kind'
-  EDGE_KINDS = request_track.Request.INITIATORS + (
-      'script_inferred', 'after-load', 'before-load', 'timing')
-
-  def __init__(self, trace, content_lens=None, frame_lens=None,
-               activity=None):
-    """Create from a LoadingTrace (or json of a trace).
-
-    Args:
-      trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
-      content_lens: (ContentClassificationLens) Lens used to annotate the
-                    nodes, or None.
-      frame_lens: (FrameLoadLens) Lens used to augment graph with load nodes.
-      activity:   (ActivityLens) Lens used to augment the edges with the
-                   activity.
-    """
-    if type(trace) == dict:
-      trace = loading_trace.LoadingTrace.FromJsonDict(trace)
-    self._trace = trace
-    self._content_lens = content_lens
-    self._frame_lens = frame_lens
-    self._activity_lens = activity
-    self._BuildDag(trace)
-    # Sort before splitting children so that we can correctly dectect if a
-    # reparented child is actually a dependency for a child of its new parent.
-    try:
-      for n in dag.TopologicalSort(self._nodes):
-        self._SplitChildrenByTime(self._node_info[n.Index()])
-    except AssertionError as exc:
-      sys.stderr.write('Bad topological sort: %s\n'
-                       'Skipping child split\n' % str(exc))
-    self._cache_all = False
-    self._node_filter = lambda _: True
-
-  @classmethod
-  def CheckImageLoadConsistency(cls, g1, g2):
-    """Check that images have the same dependencies between ResourceGraphs.
-
-    Image resources are identified by their short names.
-
-    Args:
-      g1: a ResourceGraph instance
-      g2: a ResourceGraph instance
-
-    Returns:
-      A list of discrepancy tuples. If this list is empty, g1 and g2 are
-      consistent with respect to image load dependencies. Otherwise, each tuple
-      is of the form:
-        ( g1 resource short name or str(list of short names),
-          g2 resource short name or str(list of short names),
-          human-readable discrepancy reason )
-      Either or both of the g1 and g2 image resource short names may be None if
-      it's not applicable for the discrepancy reason.
-    """
-    discrepancies = []
-    g1_image_to_info = g1._ExtractImages()
-    g2_image_to_info = g2._ExtractImages()
-    for image in set(g1_image_to_info.keys()) - set(g2_image_to_info.keys()):
-      discrepancies.append((image, None, 'Missing in g2'))
-    for image in set(g2_image_to_info.keys()) - set(g1_image_to_info.keys()):
-      discrepancies.append((None, image, 'Missing in g1'))
-
-    for image in set(g1_image_to_info.keys()) & set(g2_image_to_info.keys()):
-      def PredecessorInfo(g, n):
-        info = [g._ShortName(p) for p in n.Node().Predecessors()]
-        info.sort()
-        return str(info)
-      g1_pred = PredecessorInfo(g1, g1_image_to_info[image])
-      g2_pred = PredecessorInfo(g2, g2_image_to_info[image])
-      if g1_pred != g2_pred:
-        discrepancies.append((g1_pred, g2_pred,
-                              'Predecessor mismatch for ' + image))
-
-    return discrepancies
-
-  def Set(self, cache_all=None, node_filter=None):
-    """Set model parameters.
-
-    TODO(mattcary): add parameters for caching certain types of resources (just
-    scripts, just cacheable, etc).
-
-    Args:
-      cache_all: boolean that if true ignores empirical resource load times for
-        all resources.
-      node_filter: a Node->boolean used to restrict the graph for most
-        operations.
-    """
-    if self._cache_all is not None:
-      self._cache_all = cache_all
-    if node_filter is not None:
-      self._node_filter = node_filter
-
-  def Nodes(self, sort=False):
-    """Return iterable of all nodes via their NodeInfos.
-
-    Args:
-      sort: if true, return nodes in sorted order. This may prune additional
-        nodes from the unsorted list (eg, non-root, non-ad nodes reachable only
-        by ad nodes)
-
-    Returns:
-      Iterable of node infos.
-
-    """
-    if sort:
-      return (self._node_info[n.Index()]
-              for n in dag.TopologicalSort(self._nodes, self._node_filter))
-    return (n for n in self._node_info if self._node_filter(n.Node()))
-
-  def EdgeCosts(self, node_filter=None):
-    """Edge costs.
-
-    Args:
-      node_filter: if not none, a Node->boolean filter to use instead of the
-      current one from Set.
-
-    Returns:
-      The total edge costs of our graph.
-
-    """
-    node_filter = self._node_filter if node_filter is None else node_filter
-    total = 0
-    for n in self._node_info:
-      if not node_filter(n.Node()):
-        continue
-      for s in n.Node().Successors():
-        if node_filter(s):
-          total += self.EdgeCost(n.Node(), s)
-    return total
-
-  def Intersect(self, other_nodes):
-    """Return iterable of nodes that intersect with another graph.
-
-    Args:
-      other_nodes: iterable of the nodes of another graph, eg from Nodes().
-
-    Returns:
-      an iterable of (mine, other) pairs for all nodes for which the URL is
-      identical.
-    """
-    other_map = {n.Url(): n for n in other_nodes}
-    for n in self._node_info:
-      if self._node_filter(n.Node()) and n.Url() in other_map:
-        yield(n, other_map[n.Url()])
-
-  def Cost(self, path_list=None, costs_out=None):
-    """Compute cost of current model.
-
-    Args:
-      path_list: if not None, gets a list of NodeInfo in the longest path.
-      costs_out: if not None, gets a vector of node costs by node index. Any
-        filtered nodes will have zero cost.
-
-    Returns:
-      Cost of the longest path.
-
-    """
-    costs = [0] * len(self._nodes)
-    for n in dag.TopologicalSort(self._nodes, self._node_filter):
-      cost = 0
-      if n.Predecessors():
-        cost = max([costs[p.Index()] + self.EdgeCost(p, n)
-                    for p in n.Predecessors()])
-      if not self._cache_all:
-        cost += self.NodeCost(n)
-      costs[n.Index()] = cost
-    max_cost = max(costs)
-    if costs_out is not None:
-      del costs_out[:]
-      costs_out.extend(costs)
-    assert max_cost > 0  # Otherwise probably the filter went awry.
-    if path_list is not None:
-      del path_list[:]
-      n = (i for i in self._nodes if costs[i.Index()] == max_cost).next()
-      path_list.append(self._node_info[n.Index()])
-      while n.Predecessors():
-        n = reduce(lambda costliest, next:
-                   next if (self._node_filter(next) and
-                            costs[next.Index()] > costs[costliest.Index()])
-                        else costliest,
-                   n.Predecessors())
-        path_list.insert(0, self._node_info[n.Index()])
-    return max_cost
-
-  def FilterAds(self, node):
-    """A filter for use in eg, Cost, to remove advertising nodes.
-
-    Args:
-      node: A dag.Node.
-
-    Returns:
-      True if the node is not ad-related.
-    """
-    node_info = self._node_info[node.Index()]
-    return not (node_info.IsAd() or node_info.IsTracking())
-
-  def ResourceInfo(self):
-    """Get resource info.
-
-    Returns:
-      A list of NodeInfo objects that describe the resources fetched.
-    """
-    return [n for n in self._node_info if n.Request() is not None]
-
-  def DebugString(self):
-    """Graph structure for debugging.
-
-    TODO(mattcary): this fails for graphs with more than one component or where
-    self._nodes[0] is not a root.
-
-    Returns:
-      A human-readable string of the graph.
-    """
-    output = []
-    queue = [self._nodes[0]]
-    visited = set()
-    while queue:
-      n = queue.pop(0)
-      assert n not in visited
-      visited.add(n)
-      children = n.SortedSuccessors()
-      output.append('%d -> [%s]' %
-                    (n.Index(), ' '.join([str(c.Index()) for c in children])))
-      for c in children:
-        assert n in c.Predecessors()  # Integrity checking
-        queue.append(c)
-    assert len(visited) == len(self._nodes)
-    return '\n'.join(output)
-
-  def NodeInfo(self, node):
-    """Return the node info for a graph node.
-
-    Args:
-      node: (int, dag.Node or NodeInfo) a node representation. An int is taken
-      to be the node's index.
-
-    Returns:
-      The NodeInfo instance corresponding to the node.
-    """
-    if type(node) is self._NodeInfo:
-      return node
-    elif type(node) is int:
-      return self._node_info[node]
-    return self._node_info[node.Index()]
-
-  def ShortName(self, node):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(node).ShortName()
-
-  def Url(self, node):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(node).Url()
-
-  def NodeCost(self, node):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(node).NodeCost()
-
-  def EdgeCost(self, parent, child):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(parent).EdgeCost(self.NodeInfo(child))
-
-  def EdgeAnnotations(self, parent, child):
-    """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(parent).EdgeAnnotations(self.NodeInfo(child))
-
-  ##
-  ## Internal items
-  ##
-
-  # This resource type may induce a timing dependency. See _SplitChildrenByTime
-  # for details.
-  # TODO(mattcary): are these right?
-  _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
-  _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
-
-  class _NodeInfo(object):
-    """Our internal class that adds cost and other information to nodes.
-
-    Costs are stored on the node as well as edges. Edge information is only
-    stored on successor edges and not predecessor, that is, you get them from
-    the parent and not the child.
-
-    We also store the request on the node, and expose request-derived
-    information like content type.
-    """
-    def __init__(self, node, request):
-      """Create a new node info.
-
-      Args:
-        node: The node to augment.
-        request: The request associated with this node, or an (index, msec)
-          tuple.
-      """
-      self._node = node
-      self._is_ad = False
-      self._is_tracking = False
-      self._edge_costs = {}
-      self._edge_annotations = {}
-
-      if type(request) == tuple:
-        self._request = None
-        self._node_cost = 0
-        self._shortname = 'LOAD %s' % request[0]
-        self._start_time = request[1]
-      else:
-        self._shortname = None
-        self._start_time = None
-        self._request = request
-        # All fields in timing are millis relative to request_time.
-        self._node_cost = max(
-            [0] + [t for f, t in request.timing._asdict().iteritems()
-                   if f != 'request_time'])
-
-    def __str__(self):
-      return self.ShortName()
-
-    def Node(self):
-      return self._node
-
-    def Index(self):
-      return self._node.Index()
-
-    def SetRequestContent(self, is_ad, is_tracking):
-      """Sets the kind of content the request relates to.
-
-      Args:
-        is_ad: (bool) Whether the request is an Ad.
-        is_tracking: (bool) Whether the request is related to tracking.
-      """
-      (self._is_ad, self._is_tracking) = (is_ad, is_tracking)
-
-    def IsAd(self):
-      return self._is_ad
-
-    def IsTracking(self):
-      return self._is_tracking
-
-    def Request(self):
-      return self._request
-
-    def NodeCost(self):
-      return self._node_cost
-
-    def EdgeCost(self, s):
-      return self._edge_costs.get(s, 0)
-
-    def StartTime(self):
-      if self._start_time:
-        return self._start_time
-      return self._request.timing.request_time * 1000
-
-    def EndTime(self):
-      return self.StartTime() + self._node_cost
-
-    def EdgeAnnotations(self, s):
-      assert s.Node() in self.Node().Successors()
-      return self._edge_annotations.get(s, {})
-
-    def ContentType(self):
-      if self._request is None:
-        return 'synthetic'
-      return self._request.GetContentType()
-
-    def ShortName(self):
-      """Returns either the hostname of the resource, or the filename,
-      or the end of the path. Tries to include the domain as much as possible.
-      """
-      if self._shortname:
-        return self._shortname
-      return request_track.ShortName(self._request.url)
-
-    def Url(self):
-      return self._request.url
-
-    def SetEdgeCost(self, child, cost):
-      assert child.Node() in self._node.Successors()
-      self._edge_costs[child] = cost
-
-    def AddEdgeAnnotations(self, s, annotations):
-      assert s.Node() in self._node.Successors()
-      self._edge_annotations.setdefault(s, {}).update(annotations)
-
-    def ReparentTo(self, old_parent, new_parent):
-      """Move costs and annotatations from old_parent to new_parent.
-
-      Also updates the underlying node connections, ie, do not call
-      old_parent.RemoveSuccessor(), etc.
-
-      Args:
-        old_parent: the NodeInfo of a current parent of self. We assert this
-          is actually a parent.
-        new_parent: the NodeInfo of the new parent. We assert it is not already
-          a parent.
-      """
-      assert old_parent.Node() in self.Node().Predecessors()
-      assert new_parent.Node() not in self.Node().Predecessors()
-      edge_annotations = old_parent._edge_annotations.pop(self, {})
-      edge_cost =  old_parent._edge_costs.pop(self)
-      old_parent.Node().RemoveSuccessor(self.Node())
-      new_parent.Node().AddSuccessor(self.Node())
-      new_parent.SetEdgeCost(self, edge_cost)
-      new_parent.AddEdgeAnnotations(self, edge_annotations)
-
-    def __eq__(self, o):
-      """Note this works whether o is a Node or a NodeInfo."""
-      return self.Index() == o.Index()
-
-    def __hash__(self):
-      return hash(self.Node().Index())
-
-  def _BuildDag(self, trace):
-    """Build DAG of resources.
-
-    Build a DAG from our requests and augment with NodeInfo (see above) in a
-    parallel array indexed by Node.Index().
-
-    Creates self._nodes and self._node_info.
-
-    Args:
-      trace: A LoadingTrace.
-    """
-    self._nodes = []
-    self._node_info = []
-    index_by_request = {}
-    for request in trace.request_track.GetEvents():
-      next_index = len(self._nodes)
-      assert request not in index_by_request
-      index_by_request[request] = next_index
-      node = dag.Node(next_index)
-      node_info = self._NodeInfo(node, request)
-      if self._content_lens:
-        node_info.SetRequestContent(
-            self._content_lens.IsAdRequest(request),
-            self._content_lens.IsTrackingRequest(request))
-      self._nodes.append(node)
-      self._node_info.append(node_info)
-
-    dependencies = self.REQUEST_LENS(trace).GetRequestDependencies()
-    for dep in dependencies:
-      (parent_rq, child_rq, reason) = dep
-      parent = self._node_info[index_by_request[parent_rq]]
-      child = self._node_info[index_by_request[child_rq]]
-      edge_cost = request_track.TimeBetween(parent_rq, child_rq, reason)
-      if edge_cost < 0:
-        edge_cost = 0
-        if child.StartTime() < parent.StartTime():
-          logging.error('Inverted dependency: %s->%s',
-                        parent.ShortName(), child.ShortName())
-          # Note that child.StartTime() < parent.EndTime() appears to happen a
-          # fair amount in practice.
-      parent.Node().AddSuccessor(child.Node())
-      parent.SetEdgeCost(child, edge_cost)
-      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: reason})
-      if self._activity_lens:
-        activity = self._activity_lens.BreakdownEdgeActivityByInitiator(dep)
-        parent.AddEdgeAnnotations(child, {'activity': activity})
-
-    self._AugmentFrameLoads(index_by_request)
-
-  def _AugmentFrameLoads(self, index_by_request):
-    if not self._frame_lens:
-      return
-    loads = self._frame_lens.GetFrameLoadInfo()
-    load_index_to_node = {}
-    for l in loads:
-      next_index = len(self._nodes)
-      node = dag.Node(next_index)
-      node_info = self._NodeInfo(node, (l.index, l.msec))
-      load_index_to_node[l.index] = next_index
-      self._nodes.append(node)
-      self._node_info.append(node_info)
-    frame_deps = self._frame_lens.GetFrameLoadDependencies()
-    for load_idx, rq in frame_deps[0]:
-      parent = self._node_info[load_index_to_node[load_idx]]
-      child = self._node_info[index_by_request[rq]]
-      parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'after-load'})
-    for rq, load_idx in frame_deps[1]:
-      child = self._node_info[load_index_to_node[load_idx]]
-      parent = self._node_info[index_by_request[rq]]
-      parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'before-load'})
-
-  def _SplitChildrenByTime(self, parent):
-    """Split children of a node by request times.
-
-    The initiator of a request may not be the true dependency of a request. For
-    example, a script may appear to load several resources independently, but in
-    fact one of them may be a JSON data file, and the remaining resources assets
-    described in the JSON. The assets should be dependent upon the JSON data
-    file, and not the original script.
-
-    This function approximates that by rearranging the children of a node
-    according to their request times. The predecessor of each child is made to
-    be the node with the greatest finishing time, that is before the start time
-    of the child.
-
-    We do this by sorting the nodes twice, once by start time and once by end
-    time. We mark the earliest end time, and then we walk the start time list,
-    advancing the end time mark when it is less than our current start time.
-
-    This is refined by only considering assets which we believe actually create
-    a dependency. We only split if the original parent is a script, and the new
-    parent a data file. We confirm these relationships heuristically by loading
-    pages multiple times and ensuring that dependencies do not change; see
-    CheckImageLoadConsistency() for details.
-
-    We incorporate this heuristic by skipping over any non-script/json resources
-    when moving the end mark.
-
-    TODO(mattcary): More heuristics, like incorporating cachability somehow, and
-    not just picking arbitrarily if there are two nodes with the same end time
-    (does that ever really happen?)
-
-    Args:
-      parent: the NodeInfo whose children we are going to rearrange.
-
-    """
-    if parent.ContentType() not in self._CAN_BE_TIMING_PARENT:
-      return  # No dependency changes.
-    children_by_start_time = [self._node_info[s.Index()]
-                              for s in parent.Node().Successors()]
-    children_by_start_time.sort(key=lambda c: c.StartTime())
-    children_by_end_time = [self._node_info[s.Index()]
-                            for s in parent.Node().Successors()]
-    children_by_end_time.sort(key=lambda c: c.EndTime())
-    end_mark = 0
-    for current in children_by_start_time:
-      if current.StartTime() < parent.EndTime() - 1e-5:
-        logging.warning('Child loaded before parent finished: %s -> %s',
-                        parent.ShortName(), current.ShortName())
-      go_to_next_child = False
-      while end_mark < len(children_by_end_time):
-        if children_by_end_time[end_mark] == current:
-          go_to_next_child = True
-          break
-        elif (children_by_end_time[end_mark].ContentType() not in
-            self._CAN_MAKE_TIMING_DEPENDENCE):
-          end_mark += 1
-        elif (end_mark < len(children_by_end_time) - 1 and
-              children_by_end_time[end_mark + 1].EndTime() <
-                  current.StartTime()):
-          end_mark += 1
-        else:
-          break
-      if end_mark >= len(children_by_end_time):
-        break  # It's not possible to rearrange any more children.
-      if go_to_next_child:
-        continue  # We can't rearrange this child, but the next child may be
-                  # eligible.
-      if children_by_end_time[end_mark].EndTime() <= current.StartTime():
-        current.ReparentTo(parent, children_by_end_time[end_mark])
-        children_by_end_time[end_mark].AddEdgeAnnotations(
-            current, {self.EDGE_KIND_KEY: 'timing'})
-
-  def _ExtractImages(self):
-    """Return interesting image resources.
-
-    Uninteresting image resources are things like ads that we don't expect to be
-    constant across fetches.
-
-    Returns:
-      Dict of image url + short name to NodeInfo.
-    """
-    image_to_info = {}
-    for n in self._node_info:
-      if (n.ContentType() is not None and
-          n.ContentType().startswith('image') and
-          self.FilterAds(n)):
-        key = str((n.Url(), n.ShortName(), n.StartTime()))
-        assert key not in image_to_info, n.Url()
-        image_to_info[key] = n
-    return image_to_info
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
deleted file mode 100644
index ee5e84d..0000000
--- a/loading/loading_model_unittest.py
+++ /dev/null
@@ -1,154 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import os
-import sys
-import unittest
-
-import dag
-import loading_model
-import request_track
-import request_dependencies_lens
-import test_utils
-
-
-class LoadingModelTestCase(unittest.TestCase):
-  def SortedIndicies(self, graph):
-    return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
-
-  def SuccessorIndicies(self, node):
-    return [c.Index() for c in node.SortedSuccessors()]
-
-  def test_DictConstruction(self):
-    graph = test_utils.TestResourceGraph(
-        {'request_track': {
-            'events': [
-                test_utils.MakeRequest(0, 'null', 100, 100.5, 101).ToJsonDict(),
-                test_utils.MakeRequest(1, 0, 102, 102.5, 103).ToJsonDict(),
-                test_utils.MakeRequest(2, 0, 102, 102.5, 103).ToJsonDict(),
-                test_utils.MakeRequest(3, 2, 104, 114.5, 105).ToJsonDict()],
-            'metadata': {
-                request_track.RequestTrack._DUPLICATES_KEY: 0,
-                request_track.RequestTrack._INCONSISTENT_INITIATORS_KEY: 0}},
-         'url': 'foo.com',
-         'tracing_track': {'events': []},
-         'page_track': {'events': []},
-         'metadata': {}})
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
-
-  def test_Costing(self):
-    requests = [test_utils.MakeRequest(0, 'null', 100, 105, 110),
-                test_utils.MakeRequest(1, 0, 115, 117, 120),
-                test_utils.MakeRequest(2, 0, 112, 116, 120),
-                test_utils.MakeRequest(3, 1, 122, 124, 126),
-                test_utils.MakeRequest(4, 3, 127, 127.5, 128),
-                test_utils.MakeRequest(5, 'null', 100, 103, 105),
-                test_utils.MakeRequest(6, 5, 105, 107, 110)]
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [4])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [6])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 5, 1, 2, 6, 3, 4])
-    self.assertEqual(28, graph.Cost())
-    graph.Set(cache_all=True)
-    self.assertEqual(8, graph.Cost())
-
-  def test_MaxPath(self):
-    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 111),
-                test_utils.MakeRequest(1, 0, 115, 120, 121),
-                test_utils.MakeRequest(2, 0, 112, 120, 121),
-                test_utils.MakeRequest(3, 1, 122, 126, 127),
-                test_utils.MakeRequest(4, 3, 127, 128, 129),
-                test_utils.MakeRequest(5, 'null', 100, 105, 106),
-                test_utils.MakeRequest(6, 5, 105, 110, 111)]
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    path_list = []
-    self.assertEqual(29, graph.Cost(path_list))
-    self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
-
-    # More interesting would be a test when a node has multiple predecessors,
-    # but it's not possible for us to construct such a graph from requests yet.
-
-  def test_TimingSplit(self):
-    # Timing adds node 1 as a parent to 2 but not 3.
-    requests = [
-        test_utils.MakeRequest(0, 'null', 100, 110, 110,
-                               magic_content_type=True),
-        test_utils.MakeRequest(1, 0, 115, 120, 120,
-                               magic_content_type=True),
-        test_utils.MakeRequest(2, 0, 121, 122, 122,
-                               magic_content_type=True),
-        test_utils.MakeRequest(3, 0, 112, 119, 119,
-                               magic_content_type=True),
-        test_utils.MakeRequest(4, 2, 122, 126, 126),
-        test_utils.MakeRequest(5, 2, 122, 126, 126)]
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
-
-    # Change node 1 so it is a parent of 3, which becomes the parent of 2.
-    requests[1] = test_utils.MakeRequest(
-        1, 0, 110, 111, 111, magic_content_type=True)
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
-
-    # Add an initiator dependence to 1 that will become the parent of 3.
-    requests[1] = test_utils.MakeRequest(
-        1, 0, 110, 111, 111, magic_content_type=True)
-    requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    # Check it doesn't change until we change the content type of 6.
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
-    requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
-                                         magic_content_type=True)
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [3])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 6, 3, 2, 4, 5])
-
-  def test_TimingSplitImage(self):
-    # If we're all image types, then we shouldn't split by timing.
-    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
-                test_utils.MakeRequest(1, 0, 115, 120, 120),
-                test_utils.MakeRequest(2, 0, 121, 122, 122),
-                test_utils.MakeRequest(3, 0, 112, 119, 119),
-                test_utils.MakeRequest(4, 2, 122, 126, 126),
-                test_utils.MakeRequest(5, 2, 122, 126, 126)]
-    for r in requests:
-      r.response_headers['Content-Type'] = 'image/gif'
-    graph = test_utils.TestResourceGraph.FromRequestList(requests)
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
-    self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5])
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/model_graph.py b/loading/model_graph.py
deleted file mode 100644
index eaded4a..0000000
--- a/loading/model_graph.py
+++ /dev/null
@@ -1,184 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Visualize a loading_model.ResourceGraph."""
-
-import dag
-import itertools
-
-import loading_model
-
-
-class GraphVisualization(object):
-  """Manipulate visual representations of a resource graph.
-
-  The output will change as the ResourceGraph is changed, for example by setting
-  filters.
-
-  Currently only DOT output is supported.
-  """
-  _LONG_EDGE_THRESHOLD_MS = 2000  # Time in milliseconds.
-
-  _CONTENT_KIND_TO_COLOR = {
-      'application':     'blue',      # Scripts.
-      'font':            'grey70',
-      'image':           'orange',    # This probably catches gifs?
-      'video':           'hotpink1',
-      'audio':           'hotpink2',
-      }
-
-  _CONTENT_TYPE_TO_COLOR = {
-      'html':            'red',
-      'css':             'green',
-      'script':          'blue',
-      'javascript':      'blue',
-      'json':            'purple',
-      'gif':             'grey',
-      'image':           'orange',
-      'jpeg':            'orange',
-      'ping':            'cyan',  # Empty response
-      'redirect':        'forestgreen',
-      'png':             'orange',
-      'plain':           'brown3',
-      'octet-stream':    'brown3',
-      'other':           'white',
-      'synthetic':       'yellow',
-      }
-
-  _EDGE_KIND_TO_COLOR = {
-    'redirect': 'black',
-    'parser': 'red',
-    'script': 'blue',
-    'script_inferred': 'purple',
-    'after-load': 'forestgreen',
-    'before-load': 'forestgreen',
-  }
-
-  _ACTIVITY_TYPE_LABEL = (
-      ('idle', 'I'), ('unrelated_work', 'W'), ('script', 'S'),
-      ('parsing', 'P'), ('other_url', 'O'), ('unknown_url', 'U'))
-
-  def __init__(self, graph):
-    """Initialize.
-
-    Args:
-      graph: (loading_model.ResourceGraph) the graph to visualize.
-    """
-    self._graph = graph
-    self._global_start = None
-
-  def OutputDot(self, output):
-    """Output DOT (graphviz) representation.
-
-    Args:
-      output: a file-like output stream to receive the dot file.
-    """
-    sorted_nodes = [n for n in self._graph.Nodes(sort=True)]
-    self._global_start = min([n.StartTime() for n in sorted_nodes])
-    visited_nodes = set([n for n in sorted_nodes])
-
-    output.write("""digraph dependencies {
-    rankdir = LR;
-    """)
-
-    orphans = set()
-    for n in sorted_nodes:
-      for s in itertools.chain(n.Node().Successors(),
-                               n.Node().Predecessors()):
-        if s in visited_nodes:
-          break
-      else:
-        orphans.add(n)
-    if orphans:
-      output.write("""subgraph cluster_orphans {
-                        color=black;
-                        label="Orphans";
-                   """)
-      for n in orphans:
-        # Ignore synthetic nodes for orphan display.
-        if not self._graph.NodeInfo(n).Request():
-          continue
-        output.write(self.DotNode(n))
-      output.write('}\n')
-
-    output.write("""subgraph cluster_nodes {
-                      color=invis;
-                 """)
-
-    for n in sorted_nodes:
-      if n in orphans:
-        continue
-      output.write(self.DotNode(n))
-
-    for n in visited_nodes:
-      for s in n.Node().Successors():
-        if s not in visited_nodes:
-          continue
-        style = 'color = orange'
-        label = '%.02f' % self._graph.EdgeCost(n, s)
-        annotations = self._graph.EdgeAnnotations(n, s)
-        edge_kind = annotations.get(
-            loading_model.ResourceGraph.EDGE_KIND_KEY, None)
-        assert ((edge_kind is None)
-                or (edge_kind in loading_model.ResourceGraph.EDGE_KINDS))
-        style = 'color = %s' % self._EDGE_KIND_TO_COLOR[edge_kind]
-        if edge_kind == 'timing':
-          style += '; style=dashed'
-        if self._graph.EdgeCost(n, s) > self._LONG_EDGE_THRESHOLD_MS:
-          style += '; penwidth=5; weight=2'
-
-        label = '%.02f' % self._graph.EdgeCost(n, s)
-        if 'activity' in annotations:
-          activity = annotations['activity']
-          separator = ' - '
-          for activity_type, activity_label in self._ACTIVITY_TYPE_LABEL:
-            label += '%s%s:%.02f ' % (
-                separator, activity_label, activity[activity_type])
-            separator = ' '
-        arrow = '[%s; label="%s"]' % (style, label)
-        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
-    output.write('}\n')
-
-    output.write('}\n')
-
-  def _ContentTypeToColor(self, content_type):
-    if not content_type:
-      type_str = 'other'
-    elif '/' in content_type:
-      kind, type_str = content_type.split('/', 1)
-      if kind in self._CONTENT_KIND_TO_COLOR:
-        return self._CONTENT_KIND_TO_COLOR[kind]
-    else:
-      type_str = content_type
-    return self._CONTENT_TYPE_TO_COLOR[type_str]
-
-  def DotNode(self, node):
-    """Returns a graphviz node description for a given node.
-
-    Args:
-      node: a dag.Node or ResourceGraph node info.
-
-    Returns:
-      A string describing the resource in graphviz format.
-      The resource is color-coded according to its content type, and its shape
-      is oval if its max-age is less than 300s (or if it's not cacheable).
-    """
-    if type(node) is dag.Node:
-      node = self._graph.NodeInfo(node)
-    color = self._ContentTypeToColor(node.ContentType())
-    if node.Request():
-      max_age = node.Request().MaxAge()
-      shape = 'polygon' if max_age > 300 else 'oval'
-    else:
-      shape = 'doubleoctagon'
-    styles = ['filled']
-    if node.IsAd() or node.IsTracking():
-      styles += ['bold', 'diagonals']
-    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
-            'fillcolor = %s; shape = %s];\n'
-            % (node.Index(), node.ShortName(),
-               node.StartTime() - self._global_start,
-               node.EndTime() - self._global_start,
-               node.EndTime() - node.StartTime(),
-               ','.join(styles), color, shape))
diff --git a/loading/model_graph_unittest.py b/loading/model_graph_unittest.py
deleted file mode 100644
index 831b1e1..0000000
--- a/loading/model_graph_unittest.py
+++ /dev/null
@@ -1,33 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import gzip
-import json
-import os.path
-import tempfile
-import unittest
-
-import frame_load_lens
-import loading_model
-import loading_trace
-import model_graph
-
-TEST_DATA_DIR = os.path.join(os.path.dirname(__file__), 'testdata')
-
-class ModelGraphTestCase(unittest.TestCase):
-  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
-
-  def test_EndToEnd(self):
-    # Test that we don't crash. This also runs through frame_load_lens.
-    tmp = tempfile.NamedTemporaryFile()
-    with gzip.GzipFile(self._ROLLING_STONE) as f:
-      trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
-      frame_lens = frame_load_lens.FrameLoadLens(trace)
-      graph = loading_model.ResourceGraph(trace=trace, frame_lens=frame_lens)
-      visualization = model_graph.GraphVisualization(graph)
-      visualization.OutputDot(tmp)
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 8e07826..a0b5b16 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -6,7 +6,6 @@
 
 import dependency_graph
 import devtools_monitor
-import loading_model
 import loading_trace
 import page_track
 import request_track
@@ -158,15 +157,6 @@ class SimpleLens(object):
     return deps
 
 
-class TestResourceGraph(loading_model.ResourceGraph):
-  """Replace the default request lens in a ResourceGraph with our SimpleLens."""
-  REQUEST_LENS = SimpleLens
-
-  @classmethod
-  def FromRequestList(cls, requests, page_events=None, trace_events=None):
-    return cls(LoadingTraceFromEvents(requests, page_events, trace_events))
-
-
 class TestDependencyGraph(dependency_graph.RequestDependencyGraph):
   """A dependency graph created from requests using a simple lens."""
   def __init__(self, requests):

commit 2db8db644141af35bb5cbae4c2a72988cbc6b08c
Author: lizeb <lizeb@chromium.org>
Date:   Fri Apr 1 04:32:37 2016 -0700

    clovis: Add a method to slim a trace by dropping large objects.
    
    JSON is less expensive than objects for trace events. A ~45MB trace file
    expands to more than 500MB of memory. This CL adds a method to return
    the trace to its JSON format. This increases the peak memory usage when
    processing a single trace (and the processing time, since serializing
    the trace is not free), but reduces the total memory cost when holding
    several traces at once.
    
    Review URL: https://codereview.chromium.org/1848203003
    
    Cr-Original-Commit-Position: refs/heads/master@{#384548}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 65f484b3674d177e98ed04aa2ec30482e05a6c3a

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 6dbd13d..7630bcc 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -34,7 +34,8 @@ class LoadingTrace(object):
     self.metadata = metadata
     self.page_track = page
     self.request_track = request
-    self.tracing_track = tracing_track
+    self._tracing_track = tracing_track
+    self._tracing_json_str = None
 
   def ToJsonDict(self):
     """Returns a dictionary representing this instance."""
@@ -94,3 +95,23 @@ class LoadingTrace(object):
                     else categories))
     connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
     return cls(url, chrome_metadata, page, request, trace)
+
+  @property
+  def tracing_track(self):
+    if not self._tracing_track:
+      self._RestoreTracingTrack()
+    return self._tracing_track
+
+  def Slim(self):
+    """Slims the memory usage of a trace by dropping the TraceEvents from it.
+
+    The tracing track is restored on-demand when accessed.
+    """
+    self._tracing_json_str = json.dumps(self._tracing_track.ToJsonDict())
+    self._tracing_track = None
+
+  def _RestoreTracingTrack(self):
+    assert self._tracing_json_str
+    self._tracing_track = tracing.TracingTrack.FromJsonDict(
+        json.loads(self._tracing_json_str))
+    self._tracing_json_str = None

commit fb8ed3b74b67e421d947a50da71b604a66d65988
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 04:22:14 2016 -0700

    tools/android/loading Use '--local_noisy' for trace collection
    
    This options might be useful to diagnose failures
    
    Review URL: https://codereview.chromium.org/1847383002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384545}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 81a4b6ff340a9544a45690982a61e13ed190e3a7

diff --git a/loading/gce/main.py b/loading/gce/main.py
index b950c54..9b74ec6 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -84,23 +84,6 @@ class ServerApp(object):
     blob.upload_from_string(data_string)
     return blob.public_url
 
-  def _DeleteFile(self, filename):
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    try:
-      bucket.delete_blob(filename)
-      return True
-    except NotFound:
-      return False
-
-  def _ReadFile(self, filename):
-    client = self._GetStorageClient()
-    bucket = self._GetStorageBucket(client)
-    blob = bucket.get_blob(filename)
-    if not blob:
-      return None
-    return blob.download_as_string()
-
   def _GenerateTrace(self, url, filename, log_filename):
     """ Generates a trace using analyze.py
 
@@ -117,7 +100,7 @@ class ServerApp(object):
     except OSError:
       pass  # Nothing to remove.
     analyze_path = self._src_path + '/tools/android/loading/analyze.py'
-    command_line = ['python', analyze_path, 'log_requests',
+    command_line = ['python', analyze_path, 'log_requests', '--local_noisy',
         '--clear_cache', '--local', '--headless', '--local_binary',
         self._chrome_path, '--url', url, '--output', filename]
     with open(log_filename, 'w') as log_file:

commit 38c7aea54ceee18a2765565dc5f3361c2c466c33
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 02:45:07 2016 -0700

    tools/android/loading Factor code to query metadata as a function
    
    Review URL: https://codereview.chromium.org/1847853002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384533}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fc66f57b93009510d05efa5943d6b5052fc9a3ca

diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index cf1fe03..94edb22 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -7,6 +7,11 @@
 
 set -v
 
+get_instance_metadata() {
+  curl -fs http://metadata/computeMetadata/v1/instance/attributes/$1 \
+      -H "Metadata-Flavor: Google"
+}
+
 # Talk to the metadata server to get the project id
 PROJECTID=$(curl -s \
     "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
@@ -34,9 +39,7 @@ pip install --upgrade pip virtualenv
 # Download the Clovis deployment from Google Cloud Storage and unzip it.
 # It is expected that the contents of the deployment have been generated using
 # the tools/android/loading/gce/deploy.sh script.
-CLOUD_STORAGE_PATH=$(curl -s \
-    "http://metadata/computeMetadata/v1/instance/attributes/cloud-storage-path" \
-    -H "Metadata-Flavor: Google")
+CLOUD_STORAGE_PATH=`get_instance_metadata cloud-storage-path`
 DEPLOYMENT_PATH=$CLOUD_STORAGE_PATH/deployment
 
 mkdir -p /opt/app/clovis
@@ -73,9 +76,7 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
 EOF
 
 # Check if auto-start is enabled
-AUTO_START=$(curl -s \
-    "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
-    -H "Metadata-Flavor: Google")
+AUTO_START=`get_instance_metadata auto-start`
 
 # Exit early if auto start is not enabled.
 if [ "$AUTO_START" != "true" ]; then

commit 8f1c84f3020b2af91fe89fe07ab1c99bb6e8fa0c
Author: droger <droger@chromium.org>
Date:   Fri Apr 1 01:50:34 2016 -0700

    tools/android/loading Run main.py from an arbitrary directory
    
    The path to analyze.py was defined as a relative path to the current
    directory.
    This CL changes this to get the path from the configuration instead.
    
    The goal is to be able to launch the app from a directory that is not
    under the Chromium checkout, and thus avoid polluting that checkout
    
    Review URL: https://codereview.chromium.org/1848773002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384523}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b75ab05b606f449a858c2cc421994cf1fd2a228b

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 1d0332c..52e0c82 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -12,7 +12,7 @@ Install the [gcloud command line tool][1].
 ## Deploy the code
 
 ```shell
-# Build Chrome
+# Build Chrome (do not use the component build).
 BUILD_DIR=out/Release
 ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
 
@@ -32,11 +32,10 @@ gcloud compute instances create clovis-tracer-1 \
  --machine-type n1-standard-1 \
  --image ubuntu-14-04 \
  --zone europe-west1-c \
- --tags clovis-http-server \
  --scopes cloud-platform \
- --metadata cloud-storage-path=$CLOUD_STORAGE_PATH
- --metadata auto-start=true \
- --metadata-from-file startup-script=tools/android/loading/gce/startup-script.sh
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH,auto-start=true \
+ --metadata-from-file \
+     startup-script=$CHROMIUM_SRC/tools/android/loading/gce/startup-script.sh
 ```
 
 **Note:** To start an instance without automatically starting the app on it,
@@ -90,12 +89,12 @@ gcloud compute ssh clovis-tracer-1
 
 ## Use the app locally
 
-Set up the local environment:
+From a new directory, set up a local environment:
 
 ```shell
 virtualenv env
 source env/bin/activate
-pip install -r pip_requirements.txt
+pip install -r $CHROMIUM_SRC/tools/android/loading/gce/pip_requirements.txt
 ```
 
 Create a JSON file describing the deployment configuration:
@@ -106,11 +105,13 @@ Create a JSON file describing the deployment configuration:
 # CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
 # be stored.
 # CHROME_PATH is the path to the Chrome executable on the host.
+# CHROMIUM_SRC is the Chromium src directory.
 cat >$CONFIG_FILE << EOF
 {
   "project_name" : "$PROJECT_NAME",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "$CHROME_PATH"
+  "chrome_path" : "$CHROME_PATH",
+  "src_path" : "$CHROMIUM_SRC"
 }
 EOF
 ```
@@ -118,7 +119,9 @@ EOF
 Launch the app, passing the path to the deployment configuration file:
 
 ```shell
-gunicorn --workers=1 --bind 127.0.0.1:8080 'main:StartApp('\"$CONFIG_FILE\"')'
+gunicorn --workers=1 --bind 127.0.0.1:8080 \
+    --pythonpath $CHROMIUM_SRC/tools/android/loading/gce \
+    'main:StartApp('\"$CONFIG_FILE\"')'
 ```
 
 You can now [use the app][2], which is located at http://localhost:8080.
@@ -129,28 +132,5 @@ Tear down the local environment:
 deactivate
 ```
 
-## Project-wide settings
-
-This is already setup, no need to do this again.
-Kept here for reference.
-
-### Firewall rule
-
-Firewall rule to allow access to the instance HTTP server from the outside:
-
-```shell
-gcloud compute firewall-rules create default-allow-http-8080 \
-    --allow tcp:8080 \
-    --source-ranges 0.0.0.0/0 \
-    --target-tags clovis-http-server \
-    --description "Allow port 8080 access to http-server"
-```
-
-The firewall rule can be disabled with:
-
-```shell
-gcloud compute firewall-rules delete default-allow-http-8080
-```
-
 [1]: https://cloud.google.com/sdk
 [2]: #Use-the-app
diff --git a/loading/gce/main.py b/loading/gce/main.py
index b926cad..b950c54 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -41,6 +41,7 @@ class ServerApp(object):
            self._base_path_in_bucket += '/'
 
        self._chrome_path = config['chrome_path']
+       self._src_path = config['src_path']
 
 
   def _GetStorageClient(self):
@@ -115,9 +116,10 @@ class ServerApp(object):
       os.remove(filename)  # Remove any existing trace for this URL.
     except OSError:
       pass  # Nothing to remove.
-    command_line = ['python', '../analyze.py', 'log_requests', '--clear_cache',
-        '--local', '--headless', '--local_binary', self._chrome_path, '--url',
-        url, '--output', filename]
+    analyze_path = self._src_path + '/tools/android/loading/analyze.py'
+    command_line = ['python', analyze_path, 'log_requests',
+        '--clear_cache', '--local', '--headless', '--local_binary',
+        self._chrome_path, '--url', url, '--output', filename]
     with open(log_filename, 'w') as log_file:
       ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
                             stdout = log_file)
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index ef3d24f..cf1fe03 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -67,7 +67,8 @@ cat >$DEPLOYMENT_CONFIG_PATH << EOF
 {
   "project_name" : "$PROJECTID",
   "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
-  "chrome_path" : "/opt/app/clovis/binaries/chrome"
+  "chrome_path" : "/opt/app/clovis/binaries/chrome",
+  "src_path" : "/opt/app/clovis/src"
 }
 EOF
 

commit 7aeb4b6dad4606929aa4be0a1ba7f0cde9e40325
Author: droger <droger@chromium.org>
Date:   Thu Mar 31 07:31:16 2016 -0700

    tools/android/loading Add a repeat_count parameter
    
    This CL adds support for generating multiple traces for the same URL.
    
    Review URL: https://codereview.chromium.org/1839053002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384272}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 34f3f023894244d92cfcd2395d6a14f767c8459e

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 5176d00..1d0332c 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -70,7 +70,11 @@ To send a list of URLs to process:
 curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
 ```
 
-where `urls.json` is a file containing URLs as a JSON array.
+where `urls.json` is a JSON dictionary with the keys:
+
+*   `urls`: array of URLs
+*   `repeat_count`: Number of times each URL will be loaded. Each load of a URL
+    generates a separate trace file.
 
 ## Stop the app in the cloud
 
diff --git a/loading/gce/main.py b/loading/gce/main.py
index f89c536..b926cad 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -123,9 +123,12 @@ class ServerApp(object):
                             stdout = log_file)
     return ret == 0
 
-  def _ProcessTasks(self):
+  def _ProcessTasks(self, repeat_count):
     """Iterates over _tasks and runs analyze.py on each of them. Uploads the
     resulting traces to Google Cloud Storage.
+
+    Args:
+      repeat_count: The number of traces generated for each URL.
     """
     failures_dir = self._base_path_in_bucket + 'failures/'
     traces_dir = self._base_path_in_bucket + 'traces/'
@@ -136,17 +139,20 @@ class ServerApp(object):
     failed_tasks = []
     while len(self._tasks) > 0:
       url = self._tasks.pop()
-      print 'Generating trace for URL: %s' % url
-      filename = pattern.sub('_', url)
-      if self._GenerateTrace(url, filename, log_filename):
-        self._UploadFile(filename, traces_dir + filename)
-      else:
-        print 'analyze.py failed'
-        failed_tasks.append(url)
-        if os.path.isfile(filename):
-          self._UploadFile(filename, failures_dir + filename)
-      print 'Uploading analyze log'
-      self._UploadFile(log_filename, logs_dir + filename)
+      local_filename = pattern.sub('_', url)
+      for repeat in range(repeat_count):
+        print 'Generating trace for URL: %s' % url
+        remote_filename = local_filename + '/' + str(repeat)
+        if self._GenerateTrace(url, local_filename, log_filename):
+          print 'Uploading: %s' % remote_filename
+          self._UploadFile(local_filename, traces_dir + remote_filename)
+        else:
+          print 'analyze.py failed for URL: %s' % url
+          failed_tasks.append({ "url": url, "repeat": repeat})
+          if os.path.isfile(local_filename):
+            self._UploadFile(local_filename, failures_dir + remote_filename)
+        print 'Uploading analyze log'
+        self._UploadFile(log_filename, logs_dir + remote_filename)
 
     if len(failed_tasks) > 0:
       print 'Uploading failing URLs'
@@ -157,19 +163,29 @@ class ServerApp(object):
     """Sets the list of tasks and starts processing them
 
     Args:
-      http_body: List of URLs as a string representing a JSON array.
+      http_body: JSON dictionary. See README.md for a description of the format.
 
     Returns:
       A string to be sent back to the client, describing the success status of
       the request.
     """
-    self._tasks = json.loads(http_body)
+    load_parameters = json.loads(http_body)
+    try:
+      self._tasks = load_parameters['urls']
+    except KeyError:
+      return 'Error: invalid urls'
+    try:
+      repeat_count = int(load_parameters['repeat_count'])
+    except (KeyError, ValueError):
+      return 'Error: invalid repeat_count'
+
     if len(self._tasks) == 0:
       return 'Error: Empty task list'
     elif self._thread is not None and self._thread.is_alive():
       return 'Error: Already running'
     else:
-      self._thread = threading.Thread(target = self._ProcessTasks)
+      self._thread = threading.Thread(target = self._ProcessTasks,
+                                      args = (repeat_count,))
       self._thread.start()
       return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
 

commit d36e479b9434cd70eaa79b805b78707860e1241b
Author: droger <droger@chromium.org>
Date:   Thu Mar 31 06:28:18 2016 -0700

    tools/android/loading Re-enable autostart in the GCE startup script.
    
    The test checking for the autostart parameter was assuming that GCE
    would return the empty string when a metadata was not defined.
    In fact, some error string is returned instead, and thus the value of
    the string has to be explicitly checked.
    
    Review URL: https://codereview.chromium.org/1844533002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384261}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 76aa326bb5c62e334d2cc508f70bbde56532e56b

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 60c29b2..5176d00 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -50,6 +50,15 @@ Otherwise the IP address can be retrieved by doing:
 gcloud compute instances list
 ```
 
+**Note:** It can take a few minutes for the instance to start. You can follow
+the progress of the startup script on the gcloud console web interface (menu
+"Compute Engine" > "VM instances" then click on your instance and scroll down to
+see the "Serial console output") or from the command line using:
+
+```shell
+gcloud compute instances get-serial-port-output clovis-tracer-1
+```
+
 ## Use the app
 
 Check that `http://<instance-ip>:8080/test` prints `hello` when opened in a
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index 9ea5fa4..ef3d24f 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -57,7 +57,6 @@ unzip /opt/app/clovis/binaries/linux.zip -d /opt/app/clovis/binaries/
 cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
 chown root:root /usr/local/sbin/chrome-devel-sandbox
 chmod 4755 /usr/local/sbin/chrome-devel-sandbox
-export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
 
 # Make sure the pythonapp user owns the application code
 chown -R pythonapp:pythonapp /opt/app
@@ -77,12 +76,8 @@ AUTO_START=$(curl -s \
     "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
     -H "Metadata-Flavor: Google")
 
-# TODO(droger): Figure out how to correctly restore check for auto-startup
-# as well as auto-startup code.
-exit 1
-
 # Exit early if auto start is not enabled.
-if [ -z "$AUTO_START" ]; then
+if [ "$AUTO_START" != "true" ]; then
   exit 1
 fi
 
@@ -100,7 +95,8 @@ user=pythonapp
 # configured virtualenv.
 environment=VIRTUAL_ENV="/opt/app/clovis/env", \
     PATH="/opt/app/clovis/env/bin:/usr/bin", \
-    HOME="/home/pythonapp",USER="pythonapp"
+    HOME="/home/pythonapp",USER="pythonapp", \
+    CHROME_DEVEL_SANDBOX="/usr/local/sbin/chrome-devel-sandbox"
 stdout_logfile=syslog
 stderr_logfile=syslog
 EOF

commit d4baaced2a9db67c6d1e35a542f9bea4a15b6b72
Author: droger <droger@chromium.org>
Date:   Thu Mar 31 05:38:54 2016 -0700

    tools/android/loading Cleanup main.py
    
    This CL merges the set_tasks and start API endpoints
    and cleanups the flow.
    
    It also addresses a TODO by uploading the list of
    failed tasks.
    
    Review URL: https://codereview.chromium.org/1831763004
    
    Cr-Original-Commit-Position: refs/heads/master@{#384256}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a0137660f2c93f7eba72e747ad5f3e730b0fe6fb

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 5302aaa..60c29b2 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -63,13 +63,6 @@ curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
 
 where `urls.json` is a file containing URLs as a JSON array.
 
-Start the processing by sending a request to `http://<instance-ip>:8080/start`,
-for example:
-
-```shell
-curl http://<instance-ip>:8080/start
-```
-
 ## Stop the app in the cloud
 
 ```shell
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 80f7f50..f89c536 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import json
+import os
 import re
 import threading
 import subprocess
@@ -37,7 +38,7 @@ class ServerApp(object):
        if len(storage_path_components) > 1:
          self._base_path_in_bucket = '/'.join(storage_path_components[1:])
          if not self._base_path_in_bucket.endswith('/'):
-           self._base_path_in_bucket.append('/')
+           self._base_path_in_bucket += '/'
 
        self._chrome_path = config['chrome_path']
 
@@ -57,15 +58,30 @@ class ServerApp(object):
       filename_dest: name of the file in Google Cloud Storage
 
     Returns:
-      The URL of the new file in Google Cloud Storage.
+      The URL of the file in Google Cloud Storage.
     """
     client = self._GetStorageClient()
     bucket = self._GetStorageBucket(client)
     blob = bucket.blob(filename_dest)
     with open(filename_src) as file_src:
       blob.upload_from_file(file_src)
-    url = blob.public_url
-    return url
+    return blob.public_url
+
+  def _UploadString(self, data_string, filename_dest):
+    """Uploads a string to Google Cloud Storage
+
+    Args:
+      data_string: the contents of the file to be uploaded
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the file in Google Cloud Storage.
+    """
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename_dest)
+    blob.upload_from_string(data_string)
+    return blob.public_url
 
   def _DeleteFile(self, filename):
     client = self._GetStorageClient()
@@ -84,40 +100,78 @@ class ServerApp(object):
       return None
     return blob.download_as_string()
 
-  def _SetTasks(self, task_list):
-    if len(self._tasks) > 0:
-      return False  # There are tasks already.
-    self._tasks = json.loads(task_list)
-    return len(self._tasks) != 0
-
-  def _GenerateTrace(self, url, filename):
+  def _GenerateTrace(self, url, filename, log_filename):
     """ Generates a trace using analyze.py
 
     Args:
       url: url as a string.
-      filename: name of the file where the output is saved.
+      filename: name of the file where the trace is saved.
+      log_filename: name of the file where standard output and errors are logged
 
     Returns:
       True if the trace was generated successfully.
     """
-    ret = subprocess.call(
-        ['python', '../analyze.py', 'log_requests', '--clear_cache', '--local',
-         '--headless', '--local_binary', self._chrome_path, '--url',
-         url, '--output', filename])
+    try:
+      os.remove(filename)  # Remove any existing trace for this URL.
+    except OSError:
+      pass  # Nothing to remove.
+    command_line = ['python', '../analyze.py', 'log_requests', '--clear_cache',
+        '--local', '--headless', '--local_binary', self._chrome_path, '--url',
+        url, '--output', filename]
+    with open(log_filename, 'w') as log_file:
+      ret = subprocess.call(command_line , stderr = subprocess.STDOUT,
+                            stdout = log_file)
     return ret == 0
 
   def _ProcessTasks(self):
+    """Iterates over _tasks and runs analyze.py on each of them. Uploads the
+    resulting traces to Google Cloud Storage.
+    """
+    failures_dir = self._base_path_in_bucket + 'failures/'
+    traces_dir = self._base_path_in_bucket + 'traces/'
+    logs_dir = self._base_path_in_bucket + 'analyze_logs/'
+    log_filename = 'analyze.log'
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
+    failed_tasks = []
     while len(self._tasks) > 0:
       url = self._tasks.pop()
+      print 'Generating trace for URL: %s' % url
       filename = pattern.sub('_', url)
-      if self._GenerateTrace(url, filename):
-        self._UploadFile(filename, self._base_path_in_bucket + 'traces/'
-                         + filename)
+      if self._GenerateTrace(url, filename, log_filename):
+        self._UploadFile(filename, traces_dir + filename)
       else:
-        # TODO(droger): Upload the list of urls that failed.
         print 'analyze.py failed'
+        failed_tasks.append(url)
+        if os.path.isfile(filename):
+          self._UploadFile(filename, failures_dir + filename)
+      print 'Uploading analyze log'
+      self._UploadFile(log_filename, logs_dir + filename)
+
+    if len(failed_tasks) > 0:
+      print 'Uploading failing URLs'
+      self._UploadString(json.dumps(failed_tasks),
+                         failures_dir + 'failures.json')
+
+  def _SetTaskList(self, http_body):
+    """Sets the list of tasks and starts processing them
+
+    Args:
+      http_body: List of URLs as a string representing a JSON array.
+
+    Returns:
+      A string to be sent back to the client, describing the success status of
+      the request.
+    """
+    self._tasks = json.loads(http_body)
+    if len(self._tasks) == 0:
+      return 'Error: Empty task list'
+    elif self._thread is not None and self._thread.is_alive():
+      return 'Error: Already running'
+    else:
+      self._thread = threading.Thread(target = self._ProcessTasks)
+      self._thread.start()
+      return 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
 
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
@@ -129,19 +183,7 @@ class ServerApp(object):
       except (ValueError):
         body_size = 0
       body = environ['wsgi.input'].read(body_size)
-      if self._SetTasks(body):
-        data = 'Set tasks: ' + str(len(self._tasks))
-      else:
-        data = 'Something went wrong'
-    elif path == '/start':
-      if len(self._tasks) == 0 :
-        data = 'Nothing to do!'
-      elif self._thread is not None and self._thread.is_alive():
-        data = 'Already running!'
-      else:
-        data = 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
-        self._thread = threading.Thread(target = self._ProcessTasks)
-        self._thread.start()
+      data = self._SetTaskList(body)
     elif path == '/test':
       data = 'hello'
     else:
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index 74b049a..9ea5fa4 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -98,7 +98,8 @@ autorestart=true
 user=pythonapp
 # Environment variables ensure that the application runs inside of the
 # configured virtualenv.
-environment=VIRTUAL_ENV="/opt/app/clovis/env",PATH="/opt/app/clovis/env/bin",\
+environment=VIRTUAL_ENV="/opt/app/clovis/env", \
+    PATH="/opt/app/clovis/env/bin:/usr/bin", \
     HOME="/home/pythonapp",USER="pythonapp"
 stdout_logfile=syslog
 stderr_logfile=syslog

commit 50d2611774ae0a45d25469a55a004a0c9536ac07
Author: blundell <blundell@chromium.org>
Date:   Thu Mar 31 03:41:49 2016 -0700

    tools/android/loading: Eliminate need for distinct GCE repo
    
    This CL eliminates the need to have a separate git repo for the
    GCE-related code. Instead, the code needed to run Clovis in the cloud
    is uploaded to Google Storage along with the binaries.
    
    Review URL: https://codereview.chromium.org/1836503002
    
    Cr-Original-Commit-Position: refs/heads/master@{#384243}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 19a7554f5cc09950603cdff94b2ff4a518a49737

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 453ca86..5302aaa 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -9,46 +9,20 @@ Engine.
 
 Install the [gcloud command line tool][1].
 
-Checkout the source:
-
-```shell
-mkdir clovis
-cd clovis
-gcloud init
-```
-
-When offered, accept to clone the Google Cloud repo.
-
-## Update or Change the code
-
-Make changes to the code, or copy the latest version from Chromium into your
-local Google Cloud repository:
+## Deploy the code
 
 ```shell
 # Build Chrome
 BUILD_DIR=out/Release
 ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
 
-GCE_DIR=~/dev/clovis/default
-
 # Deploy to GCE
-# CHROME_BUCKET_NAME is the name of the Google Cloud Storage bucket where the
-# Chrome build artifacts will be uploaded, and matches the value of
-# 'bucket_name' in server_config.json.
-./tools/android/loading/gce/deploy.sh $BUILD_DIR $GCE_DIR $CHROME_BUCKET_NAME
-
-cd $GCE_DIR
-
-# git add the relevant files
+# CLOUD_STORAGE_PATH is the path in Google Cloud Storage under which the
+# Clovis deployment will be uploaded.
 
-# commit and push:
-git commit
-git push -u origin master
+./tools/android/loading/gce/deploy.sh $BUILD_DIR $CLOUD_STORAGE_PATH
 ```
 
-If there are instances already running, they need to be restarted for this to
-take effect.
-
 ## Start the app in the cloud
 
 Create an instance using latest ubuntu LTS:
@@ -60,6 +34,7 @@ gcloud compute instances create clovis-tracer-1 \
  --zone europe-west1-c \
  --tags clovis-http-server \
  --scopes cloud-platform \
+ --metadata cloud-storage-path=$CLOUD_STORAGE_PATH
  --metadata auto-start=true \
  --metadata-from-file startup-script=tools/android/loading/gce/startup-script.sh
 ```
@@ -109,7 +84,7 @@ gcloud compute ssh clovis-tracer-1
 
 ## Use the app locally
 
-Setup the local environment:
+Set up the local environment:
 
 ```shell
 virtualenv env
@@ -117,11 +92,27 @@ source env/bin/activate
 pip install -r pip_requirements.txt
 ```
 
-Launch the app, passing the path to the Chrome executable on the host:
+Create a JSON file describing the deployment configuration:
+
+```shell
+# CONFIG_FILE is the output json file.
+# PROJECT_NAME is the Google Cloud project.
+# CLOUD_STORAGE_PATH is the path in Google Storage where generated traces will
+# be stored.
+# CHROME_PATH is the path to the Chrome executable on the host.
+cat >$CONFIG_FILE << EOF
+{
+  "project_name" : "$PROJECT_NAME",
+  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
+  "chrome_path" : "$CHROME_PATH"
+}
+EOF
+```
+
+Launch the app, passing the path to the deployment configuration file:
 
 ```shell
-gunicorn --workers=1 --bind 127.0.0.1:8080 \
-    'main:StartApp("/path/to/chrome")'
+gunicorn --workers=1 --bind 127.0.0.1:8080 'main:StartApp('\"$CONFIG_FILE\"')'
 ```
 
 You can now [use the app][2], which is located at http://localhost:8080.
@@ -137,15 +128,6 @@ deactivate
 This is already setup, no need to do this again.
 Kept here for reference.
 
-### Server configuration file
-
-`main.py` expects to find a `server_config.json` file, which is a dictionary
-with the keys:
-
-*   `project_name`: the name of the Google Compute project,
-*   `bucket_name`: the name of the Google Storage bucket used to store the
-    results.
-
 ### Firewall rule
 
 Firewall rule to allow access to the instance HTTP server from the outside:
diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index 8e937ed..0a3a531 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -5,48 +5,55 @@
 
 # This script copies all dependencies required for trace collection.
 # Usage:
-#   deploy.sh builddir outdir bucket
+#   deploy.sh builddir gcs_path
 #
 # Where:
 #   builddir is the build directory for Chrome
-#   outdir is the directory where files are deployed
-#   bucket is the Google Storage bucket where Chrome is uploaded
+#   gcs_path is the Google Storage bucket under which the deployment is
+#   installed
 
 builddir=$1
-outdir=$2
-bucket=$3
+tmpdir=`mktemp -d`
+deployment_gcs_path=$2/deployment
 
-# Copy files from tools/android/loading
-mkdir -p $outdir/tools/android/loading
-cp tools/android/loading/*.py $outdir/tools/android/loading
-cp -r tools/android/loading/gce $outdir/tools/android/loading
+# Extract needed sources.
+src_suffix=src
+tmp_src_dir=$tmpdir/$src_suffix
 
-# Copy other dependencies
-mkdir $outdir/third_party
-# Use rsync to exclude unwanted files (e.g. the .git directory).
+# Copy files from tools/android/loading.
+mkdir -p $tmp_src_dir/tools/android/loading
+cp tools/android/loading/*.py $tmp_src_dir/tools/android/loading
+cp -r tools/android/loading/gce $tmp_src_dir/tools/android/loading
+
+# Copy other dependencies.
+mkdir $tmp_src_dir/third_party
 rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
-  --delete third_party/catapult $outdir/third_party
-mkdir $outdir/tools/perf
-cp -r tools/perf/chrome_telemetry_build $outdir/tools/perf
-mkdir -p $outdir/build/android
-cp build/android/devil_chromium.py $outdir/build/android/
-cp build/android/video_recorder.py $outdir/build/android/
-cp build/android/devil_chromium.json $outdir/build/android/
-cp -r build/android/pylib $outdir/build/android/
-
-# Copy the chrome executable to Google Cloud Storage
+  --delete third_party/catapult $tmp_src_dir/third_party
+mkdir $tmp_src_dir/tools/perf
+cp -r tools/perf/chrome_telemetry_build $tmp_src_dir/tools/perf
+mkdir -p $tmp_src_dir/build/android
+cp build/android/devil_chromium.py $tmp_src_dir/build/android/
+cp build/android/video_recorder.py $tmp_src_dir/build/android/
+cp build/android/devil_chromium.json $tmp_src_dir/build/android/
+cp -r build/android/pylib $tmp_src_dir/build/android/
+
+# Tar up the source and copy it to Google Cloud Storage.
+source_tarball=$tmpdir/source.tgz
+tar -cvzf $source_tarball -C $tmpdir $src_suffix
+gsutil cp $source_tarball gs://$deployment_gcs_path/source/
+
+# Copy the chrome executable to Google Cloud Storage.
 chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
-  /tmp/linux.zip
-gsutil cp /tmp/linux.zip gs://$bucket/chrome/linux.zip
-rm /tmp/linux.zip
+  $tmpdir/linux.zip
+gsutil cp $tmpdir/linux.zip gs://$deployment_gcs_path/binaries/linux.zip
 
-# Upload Chromium revision
+# Generate and upload metadata about this deployment.
 CHROMIUM_REV=$(git merge-base HEAD origin/master)
-cat >/tmp/build_metadata.json << EOF
+cat >$tmpdir/build_metadata.json << EOF
 {
   "chromium_rev": "$CHROMIUM_REV"
 }
 EOF
-gsutil cp /tmp/build_metadata.json gs://$bucket/chrome/build_metadata.json
-rm /tmp/build_metadata.json
-
+gsutil cp $tmpdir/build_metadata.json \
+  gs://$deployment_gcs_path/deployment_metadata.json
+rm -rf $tmpdir
diff --git a/loading/gce/main.py b/loading/gce/main.py
index cddc903..80f7f50 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -16,25 +16,38 @@ class ServerApp(object):
   Google Cloud Storage.
   """
 
-  def __init__(self, chrome_path):
-    """The chrome_path argument is the path to the Chrome executable as a
-    string.
+  def __init__(self, configuration_file):
+    """|configuration_file| is a path to a file containing JSON as described in
+    README.md.
     """
     self._tasks = []
     self._thread = None
-    self._chrome_path = chrome_path
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
-    print 'Reading server configuration'
-    with open('server_config.json') as configuration_file:
-       self._config = json.load(configuration_file)
+    print 'Reading configuration'
+    with open(configuration_file) as config_json:
+       config = json.load(config_json)
+       self._project_name = config['project_name']
+
+       # Separate the cloud storage path into the bucket and the base path under
+       # the bucket.
+       storage_path_components = config['cloud_storage_path'].split('/')
+       self._bucket_name = storage_path_components[0]
+       self._base_path_in_bucket = ''
+       if len(storage_path_components) > 1:
+         self._base_path_in_bucket = '/'.join(storage_path_components[1:])
+         if not self._base_path_in_bucket.endswith('/'):
+           self._base_path_in_bucket.append('/')
+
+       self._chrome_path = config['chrome_path']
+
 
   def _GetStorageClient(self):
-    return storage.Client(project = self._config['project_name'],
+    return storage.Client(project = self._project_name,
                           credentials = self._credentials)
 
   def _GetStorageBucket(self, storage_client):
-    return storage_client.get_bucket(self._config['bucket_name'])
+    return storage_client.get_bucket(self._bucket_name)
 
   def _UploadFile(self, filename_src, filename_dest):
     """Uploads a file to Google Cloud Storage
@@ -89,8 +102,8 @@ class ServerApp(object):
     """
     ret = subprocess.call(
         ['python', '../analyze.py', 'log_requests', '--clear_cache', '--local',
-         '--headless', '--local_binary', self._chrome_path, '--url', url,
-         '--output', filename])
+         '--headless', '--local_binary', self._chrome_path, '--url',
+         url, '--output', filename])
     return ret == 0
 
   def _ProcessTasks(self):
@@ -100,7 +113,8 @@ class ServerApp(object):
       url = self._tasks.pop()
       filename = pattern.sub('_', url)
       if self._GenerateTrace(url, filename):
-        self._UploadFile(filename, 'traces/' + filename)
+        self._UploadFile(filename, self._base_path_in_bucket + 'traces/'
+                         + filename)
       else:
         # TODO(droger): Upload the list of urls that failed.
         print 'analyze.py failed'
@@ -142,5 +156,5 @@ class ServerApp(object):
     return iter([data])
 
 
-def StartApp(chrome_path):
-  return ServerApp(chrome_path)
+def StartApp(configuration_file):
+  return ServerApp(configuration_file)
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index bb1f94a..74b049a 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -31,37 +31,30 @@ useradd -m -d /home/pythonapp pythonapp
 # pip from apt is out of date, so make it update itself and install virtualenv.
 pip install --upgrade pip virtualenv
 
-# Get the source code from the Google Cloud Repository.
-# It is expected that the contents of this repository have been generated using
+# Download the Clovis deployment from Google Cloud Storage and unzip it.
+# It is expected that the contents of the deployment have been generated using
 # the tools/android/loading/gce/deploy.sh script.
-# git requires $HOME and it's not set during the startup script.
-export HOME=/root
-git config --global credential.helper gcloud.sh
-git clone --depth 1 https://source.developers.google.com/p/$PROJECTID \
-    /opt/app/clovis
+CLOUD_STORAGE_PATH=$(curl -s \
+    "http://metadata/computeMetadata/v1/instance/attributes/cloud-storage-path" \
+    -H "Metadata-Flavor: Google")
+DEPLOYMENT_PATH=$CLOUD_STORAGE_PATH/deployment
+
+mkdir -p /opt/app/clovis
+gsutil cp gs://$DEPLOYMENT_PATH/source/source.tgz /opt/app/clovis/source.tgz
+tar xvf /opt/app/clovis/source.tgz -C /opt/app/clovis
+rm /opt/app/clovis/source.tgz
 
 # Install app dependencies
 virtualenv /opt/app/clovis/env
 /opt/app/clovis/env/bin/pip install \
-    -r /opt/app/clovis/tools/android/loading/gce/pip_requirements.txt
-
-# Download Chrome from Google Cloud Storage and unzip it.
-# It is expected that the contents of the bucket have been generated using the
-# tools/android/loading/gce/deploy.sh script.
-STORAGE_BUCKET=`python - <<EOF
-import json
-config_file = "/opt/app/clovis/tools/android/loading/gce/server_config.json"
-with open(config_file) as config:
-  obj=json.load(config);
-  print obj["bucket_name"]
-EOF`
-
-mkdir /opt/app/clovis/out
-gsutil cp gs://$STORAGE_BUCKET/chrome/* /opt/app/clovis/out/
-unzip /opt/app/clovis/out/linux.zip -d /opt/app/clovis/out/
+    -r /opt/app/clovis/src/tools/android/loading/gce/pip_requirements.txt
+
+mkdir /opt/app/clovis/binaries
+gsutil cp gs://$DEPLOYMENT_PATH/binaries/* /opt/app/clovis/binaries/
+unzip /opt/app/clovis/binaries/linux.zip -d /opt/app/clovis/binaries/
 
 # Install the Chrome sandbox
-cp /opt/app/clovis/out/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
+cp /opt/app/clovis/binaries/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
 chown root:root /usr/local/sbin/chrome-devel-sandbox
 chmod 4755 /usr/local/sbin/chrome-devel-sandbox
 export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
@@ -69,6 +62,16 @@ export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
 # Make sure the pythonapp user owns the application code
 chown -R pythonapp:pythonapp /opt/app
 
+# Create the configuration file for this deployment.
+DEPLOYMENT_CONFIG_PATH=/opt/app/clovis/deployment_config.json
+cat >$DEPLOYMENT_CONFIG_PATH << EOF
+{
+  "project_name" : "$PROJECTID",
+  "cloud_storage_path" : "$CLOUD_STORAGE_PATH",
+  "chrome_path" : "/opt/app/clovis/binaries/chrome"
+}
+EOF
+
 # Check if auto-start is enabled
 AUTO_START=$(curl -s \
     "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
@@ -87,9 +90,9 @@ fi
 # applicaiton.
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
-directory=/opt/app/clovis/tools/android/loading/gce
+directory=/opt/app/clovis/src/tools/android/loading/gce
 command=/opt/app/clovis/env/bin/gunicorn --workers=1 --bind 0.0.0.0:8080 \
-    'main:StartApp("/opt/app/clovis/out/chrome")'
+    'main:StartApp('\"$DEPLOYMENT_CONFIG_PATH\"')'
 autostart=true
 autorestart=true
 user=pythonapp

commit d4219e74cbe5de58e402464e41042631ae2f9097
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 30 06:44:18 2016 -0700

    clovis: Add the timing heuristic to request_dependency_graph.
    
    This heuristic is taken from loading_model, and is a required step to
    complete the refactoring of loading_model. It also provides better
    results for prefetch_view, which uses dependency_graph.
    
    Review URL: https://codereview.chromium.org/1848443002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383968}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4393a4e251c93a2ffe59f37b0a09853c85b32043

diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index be2c5e0..13f5485 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -4,6 +4,9 @@
 
 """Request dependency graph."""
 
+import logging
+import sys
+
 import graph
 import request_track
 
@@ -25,6 +28,12 @@ class _Edge(graph.Edge):
 
 class RequestDependencyGraph(object):
   """Request dependency graph."""
+  # This resource type may induce a timing dependency. See _SplitChildrenByTime
+  # for details.
+  # TODO(lizeb,mattcary): are these right?
+  _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
+  _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
+
   def __init__(self, requests, dependencies_lens):
     """Creates a request dependency graph.
 
@@ -45,6 +54,7 @@ class RequestDependencyGraph(object):
       edges.append(_Edge(parent_node, child_node, reason))
     self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
     self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
+    self._HandleTimingDependencies()
 
   @property
   def graph(self):
@@ -55,9 +65,8 @@ class RequestDependencyGraph(object):
     """Updates the cost of the nodes identified by their request ID.
 
     Args:
-      request_id_to_cost: {request_id: new_cost} Can be a superset of the
+      request_id_to_cost: ({request_id: new_cost}) Can be a superset of the
                           requests actually present in the graph.
-
     """
     for node in self._deps_graph.Nodes():
       request_id = node.request.request_id
@@ -75,3 +84,84 @@ class RequestDependencyGraph(object):
       return self._deps_graph.Cost([self._first_request_node])
     else:
       return self._deps_graph.Cost()
+
+  def _HandleTimingDependencies(self):
+    try:
+      for n in self._deps_graph.TopologicalSort():
+        self._SplitChildrenByTime(n)
+    except AssertionError as exc:
+      sys.stderr.write('Bad topological sort: %s\n'
+                       'Skipping child split\n' % str(exc))
+
+  def _SplitChildrenByTime(self, parent):
+    """Splits children of a node by request times.
+
+    The initiator of a request may not be the true dependency of a request. For
+    example, a script may appear to load several resources independently, but in
+    fact one of them may be a JSON data file, and the remaining resources assets
+    described in the JSON. The assets should be dependent upon the JSON data
+    file, and not the original script.
+
+    This function approximates that by rearranging the children of a node
+    according to their request times. The predecessor of each child is made to
+    be the node with the greatest finishing time, that is before the start time
+    of the child.
+
+    We do this by sorting the nodes twice, once by start time and once by end
+    time. We mark the earliest end time, and then we walk the start time list,
+    advancing the end time mark when it is less than our current start time.
+
+    This is refined by only considering assets which we believe actually create
+    a dependency. We only split if the original parent is a script, and the new
+    parent a data file.
+    We incorporate this heuristic by skipping over any non-script/json resources
+    when moving the end mark.
+
+    TODO(mattcary): More heuristics, like incorporating cachability somehow, and
+    not just picking arbitrarily if there are two nodes with the same end time
+    (does that ever really happen?)
+
+    Args:
+      parent: (_RequestNode) The children of this node are processed by this
+              function.
+    """
+    if parent.request.GetContentType() not in self._CAN_BE_TIMING_PARENT:
+      return
+    edges = self._deps_graph.OutEdges(parent)
+    edges_by_start_time = sorted(
+        edges, key=lambda e: e.to_node.request.start_msec)
+    edges_by_end_time = sorted(
+        edges, key=lambda e: e.to_node.request.end_msec)
+    end_mark = 0
+    for current in edges_by_start_time:
+      assert current.from_node is parent
+      if current.to_node.request.start_msec < parent.request.end_msec - 1e-5:
+        parent_url = parent.request.url
+        child_url = current.to_node.request.url
+        logging.warning('Child loaded before parent finished: %s -> %s',
+                        request_track.ShortName(parent_url),
+                        request_track.ShortName(child_url))
+      go_to_next_child = False
+      while end_mark < len(edges_by_end_time):
+        if edges_by_end_time[end_mark] == current:
+          go_to_next_child = True
+          break
+        elif (edges_by_end_time[end_mark].to_node.request.GetContentType()
+              not in self._CAN_MAKE_TIMING_DEPENDENCE):
+          end_mark += 1
+        elif (end_mark < len(edges_by_end_time) - 1 and
+              edges_by_end_time[end_mark + 1].to_node.request.end_msec
+              < current.to_node.request.start_msec):
+          end_mark += 1
+        else:
+          break
+      if end_mark >= len(edges_by_end_time):
+        break  # It's not possible to rearrange any more children.
+      if go_to_next_child:
+        continue  # We can't rearrange this child, but the next child may be
+                  # eligible.
+      if (edges_by_end_time[end_mark].to_node.request.end_msec
+          <= current.to_node.request.start_msec):
+        self._deps_graph.UpdateEdge(
+            current, edges_by_end_time[end_mark].to_node,
+            current.to_node)
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
index 448a234..4f60995 100644
--- a/loading/dependency_graph_unittest.py
+++ b/loading/dependency_graph_unittest.py
@@ -8,6 +8,7 @@ import dependency_graph
 import request_dependencies_lens
 from request_dependencies_lens_unittest import TestRequests
 import request_track
+import test_utils
 
 
 class RequestDependencyGraphTestCase(unittest.TestCase):
@@ -49,6 +50,94 @@ class RequestDependencyGraphTestCase(unittest.TestCase):
     g.UpdateRequestsCost({TestRequests.SECOND_REDIRECT_REQUEST.request_id: 0})
     self.assertEqual(6990, g.Cost())
 
+  def testHandleTimingDependencies(self):
+    # Timing adds node 1 as a parent to 2 but not 3.
+    requests = [
+        test_utils.MakeRequest(0, 'null', 100, 110, 110,
+                               magic_content_type=True),
+        test_utils.MakeRequest(1, 0, 115, 120, 120,
+                               magic_content_type=True),
+        test_utils.MakeRequest(2, 0, 121, 122, 122,
+                               magic_content_type=True),
+        test_utils.MakeRequest(3, 0, 112, 119, 119,
+                               magic_content_type=True),
+        test_utils.MakeRequest(4, 2, 122, 126, 126),
+        test_utils.MakeRequest(5, 2, 122, 126, 126)]
+
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(
+        self._Successors(g, requests[0]), set([requests[1], requests[3]]))
+    self.assertSetEqual(
+        self._Successors(g, requests[1]), set([requests[2]]))
+    self.assertSetEqual(
+        self._Successors(g, requests[2]), set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set())
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+
+    # Change node 1 so it is a parent of 3, which becomes the parent of 2.
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(self._Successors(g, requests[0]), set([requests[1]]))
+    self.assertSetEqual(self._Successors(g, requests[1]), set([requests[3]]))
+    self.assertSetEqual(self._Successors(g, requests[2]),
+                        set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set([requests[2]]))
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+
+    # Add an initiator dependence to 1 that will become the parent of 3.
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
+    requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
+    g = self._GraphFromRequests(requests)
+    # Check it doesn't change until we change the content type of 6.
+    self.assertEqual(self._Successors(g, requests[6]), set())
+    requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
+                                         magic_content_type=True)
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(self._Successors(g, requests[0]), set([requests[1]]))
+    self.assertSetEqual(self._Successors(g, requests[1]), set([requests[6]]))
+    self.assertSetEqual(self._Successors(g, requests[2]),
+                        set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set([requests[2]]))
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+    self.assertSetEqual(self._Successors(g, requests[6]), set([requests[3]]))
+
+  def testHandleTimingDependenciesImages(self):
+    # If we're all image types, then we shouldn't split by timing.
+    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
+                test_utils.MakeRequest(1, 0, 115, 120, 120),
+                test_utils.MakeRequest(2, 0, 121, 122, 122),
+                test_utils.MakeRequest(3, 0, 112, 119, 119),
+                test_utils.MakeRequest(4, 2, 122, 126, 126),
+                test_utils.MakeRequest(5, 2, 122, 126, 126)]
+    for r in requests:
+      r.response_headers['Content-Type'] = 'image/gif'
+    g = self._GraphFromRequests(requests)
+    self.assertSetEqual(self._Successors(g, requests[0]),
+                        set([requests[1], requests[2], requests[3]]))
+    self.assertSetEqual(self._Successors(g, requests[1]), set())
+    self.assertSetEqual(self._Successors(g, requests[2]),
+                        set([requests[4], requests[5]]))
+    self.assertSetEqual(self._Successors(g, requests[3]), set())
+    self.assertSetEqual(self._Successors(g, requests[4]), set())
+    self.assertSetEqual(self._Successors(g, requests[5]), set())
+
+  @classmethod
+  def _GraphFromRequests(cls, requests):
+    trace = test_utils.LoadingTraceFromEvents(requests)
+    deps_lens = test_utils.SimpleLens(trace)
+    return dependency_graph.RequestDependencyGraph(requests, deps_lens)
+
+  @classmethod
+  def _Successors(cls, g, parent_request):
+    parent_node = g._nodes_by_id[parent_request.request_id]
+    edges = g._deps_graph.OutEdges(parent_node)
+    return set(e.to_node.request for e in edges)
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/loading_model.py b/loading/loading_model.py
index d929ac4..b7f8002 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -17,7 +17,7 @@ ResourceGraph
 
 import logging
 import os
-import urlparse
+
 import sys
 
 import activity_lens
@@ -404,22 +404,7 @@ class ResourceGraph(object):
       """
       if self._shortname:
         return self._shortname
-      parsed = urlparse.urlparse(self._request.url)
-      path = parsed.path
-      hostname = parsed.hostname if parsed.hostname else '?.?.?'
-      if path != '' and path != '/':
-        last_path = parsed.path.split('/')[-1]
-        if len(last_path) < 10:
-          if len(path) < 10:
-            return hostname + '/' + path
-          else:
-            return hostname + '/..' + parsed.path[-10:]
-        elif len(last_path) > 10:
-          return hostname + '/..' + last_path[:5]
-        else:
-          return hostname + '/..' + last_path
-      else:
-        return hostname
+      return request_track.ShortName(self._request.url)
 
     def Url(self):
       return self._request.url
diff --git a/loading/request_track.py b/loading/request_track.py
index 2c083c7..47b874f 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -13,6 +13,7 @@ import copy
 import json
 import logging
 import re
+import urlparse
 
 import devtools_monitor
 
@@ -29,6 +30,24 @@ _TIMING_NAMES_MAPPING = {
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
 
+def ShortName(url):
+  """Returns a shortened version of a URL."""
+  parsed = urlparse.urlparse(url)
+  path = parsed.path
+  hostname = parsed.hostname if parsed.hostname else '?.?.?'
+  if path != '' and path != '/':
+    last_path = parsed.path.split('/')[-1]
+    if len(last_path) < 10:
+      if len(path) < 10:
+        return hostname + '/' + path
+      else:
+        return hostname + '/..' + parsed.path[-10:]
+    else:
+        return hostname + '/..' + last_path[:5]
+  else:
+    return hostname
+
+
 def IntervalBetween(first, second, reason):
   """Returns the start and end of the inteval between two requests, in ms.
 

commit c261f3ca8ef04d92e35abe547f59ccefec612d10
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 30 06:04:47 2016 -0700

    clovis: Remove the deprecated directory.
    
    Review URL: https://codereview.chromium.org/1839373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383958}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7bed61994bd05f0f48d64de84fc4ea23a616c152

diff --git a/loading/deprecated/log_parser.py b/loading/deprecated/log_parser.py
deleted file mode 100644
index a58bd55..0000000
--- a/loading/deprecated/log_parser.py
+++ /dev/null
@@ -1,218 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Parses a JSON request log created by log_requests.py."""
-
-import collections
-import json
-import operator
-import urlparse
-
-Timing = collections.namedtuple(
-    'Timing',
-    ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
-     'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
-     'sslEnd', 'sslStart', 'workerReady', 'workerStart', 'loadingFinished'])
-
-
-class Resource(object):
-  """Describes a resource."""
-
-  def __init__(self, url, content_type):
-    """Creates an instance of Resource.
-
-    Args:
-      url: URL of the resource
-      content_type: Content-Type of the resources.
-    """
-    self.url = url
-    self.content_type = content_type
-
-  def GetShortName(self):
-    """Returns either the hostname of the resource, or the filename,
-    or the end of the path. Tries to include the domain as much as possible.
-    """
-    parsed = urlparse.urlparse(self.url)
-    path = parsed.path
-    if path != '' and path != '/':
-      last_path = parsed.path.split('/')[-1]
-      if len(last_path) < 10:
-        if len(path) < 10:
-          return parsed.hostname + '/' + path
-        else:
-          return parsed.hostname + '/..' + parsed.path[-10:]
-      elif len(last_path) > 10:
-        return parsed.hostname + '/..' + last_path[:5]
-      else:
-        return parsed.hostname + '/..' + last_path
-    else:
-      return parsed.hostname
-
-  def GetContentType(self):
-    mime = self.content_type
-    if 'magic-debug-content' in mime:
-      # A silly hack to make the unittesting easier.
-      return 'magic-debug-content'
-    elif mime == 'text/html':
-      return 'html'
-    elif mime == 'text/css':
-      return 'css'
-    elif mime in ('application/x-javascript', 'text/javascript',
-                  'application/javascript'):
-      return 'script'
-    elif mime == 'application/json':
-      return 'json'
-    elif mime == 'image/gif':
-      return 'gif_image'
-    elif mime.startswith('image/'):
-      return 'image'
-    else:
-      return 'other'
-
-  @classmethod
-  def FromRequest(cls, request):
-    """Creates a Resource from an instance of RequestData."""
-    return Resource(request.url, request.GetContentType())
-
-  def __Fields(self):
-    return (self.url, self.content_type)
-
-  def __eq__(self, o):
-    return  self.__Fields() == o.__Fields()
-
-  def __hash__(self):
-    return hash(self.__Fields())
-
-
-class RequestData(object):
-  """Represents a request, as dumped by log_requests.py."""
-
-  def __init__(self, status, headers, request_headers, timestamp, timing, url,
-               served_from_cache, initiator):
-    self.status = status
-    self.headers = headers
-    self.request_headers = request_headers
-    self.timestamp = timestamp
-    self.timing = Timing(**timing) if timing else None
-    self.url = url
-    self.served_from_cache = served_from_cache
-    self.initiator = initiator
-
-  def IsDataUrl(self):
-    return self.url.startswith('data:')
-
-  def GetContentType(self):
-    content_type = self.headers['Content-Type']
-    if ';' in content_type:
-      return content_type[:content_type.index(';')]
-    else:
-      return content_type
-
-  @classmethod
-  def FromDict(cls, r):
-    """Creates a RequestData object from a dict."""
-    return RequestData(r['status'], r['headers'], r['request_headers'],
-                       r['timestamp'], r['timing'], r['url'],
-                       r['served_from_cache'], r['initiator'])
-
-
-def ParseJsonFile(filename):
-  """Converts a JSON file to a sequence of RequestData."""
-  with open(filename) as f:
-    json_data = json.load(f)
-    return [RequestData.FromDict(r) for r in json_data]
-
-
-def FilterRequests(requests):
-  """Filters a list of requests.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    A list of requests that are not data URL, have a Content-Type, and are
-    not served from the cache.
-  """
-  return [r for r in requests if not r.IsDataUrl()
-          and 'Content-Type' in r.headers and not r.served_from_cache]
-
-
-def ResourceToRequestMap(requests):
-  """Returns a Resource -> Request map.
-
-  A resource can be requested several times in a single page load. Keeps the
-  first request in this case.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    [Resource, ...]
-  """
-  # reversed(requests) because we want the first one to win.
-  return dict([(Resource.FromRequest(r), r) for r in reversed(requests)])
-
-
-def GetResources(requests):
-  """Returns an ordered list of resources from a list of requests.
-
-  The same resource can be requested several time for a single page load. This
-  keeps only the first request.
-
-  Args:
-    requests: [RequestData]
-
-  Returns:
-    [Resource]
-  """
-  resources = []
-  known_resources = set()
-  for r in requests:
-    resource = Resource.FromRequest(r)
-    if r in known_resources:
-      continue
-    known_resources.add(resource)
-    resources.append(resource)
-  return resources
-
-
-def ParseCacheControl(headers):
-  """Parses the "Cache-Control" header and returns a dict representing it.
-
-  Args:
-    headers: (dict) Response headers.
-
-  Returns:
-    {Directive: Value, ...}
-  """
-  # TODO(lizeb): Handle the "Expires" header as well.
-  result = {}
-  cache_control = headers.get('Cache-Control', None)
-  if cache_control is None:
-    return result
-  directives = [s.strip() for s in cache_control.split(',')]
-  for directive in directives:
-    parts = [s.strip() for s in directive.split('=')]
-    if len(parts) == 1:
-      result[parts[0]] = True
-    else:
-      result[parts[0]] = parts[1]
-  return result
-
-
-def MaxAge(request):
-  """Returns the max-age of a resource, or -1."""
-  cache_control = ParseCacheControl(request.headers)
-  if (u'no-store' in cache_control
-      or u'no-cache' in cache_control
-      or len(cache_control) == 0):
-    return -1
-  if 'max-age' in cache_control:
-    return int(cache_control['max-age'])
-  return -1
-
-
-def SortedByCompletion(requests):
-  """Returns the requests, sorted by completion time."""
-  return sorted(requests, key=operator.attrgetter('timestamp'))
diff --git a/loading/deprecated/log_requests.py b/loading/deprecated/log_requests.py
deleted file mode 100755
index 12ebb66..0000000
--- a/loading/deprecated/log_requests.py
+++ /dev/null
@@ -1,239 +0,0 @@
-#! /usr/bin/python
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Loads a URL on an Android device, logging all the requests made to do it
-to a JSON file using DevTools.
-"""
-
-import contextlib
-import httplib
-import json
-import logging
-import optparse
-import os
-import sys
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-import devil_chromium
-
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
-from chrome_telemetry_build import chromium_config
-sys.path.append(chromium_config.GetTelemetryDir())
-from telemetry.internal.backends.chrome_inspector import inspector_websocket
-from telemetry.internal.backends.chrome_inspector import websocket
-
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'chrome_proxy'))
-from common import inspector_network
-
-import device_setup
-
-
-class AndroidRequestsLogger(object):
-  """Logs all the requests made to load a page on a device."""
-
-  def __init__(self, device):
-    """If device is None, we connect to a local chrome session."""
-    self.device = device
-    self._please_stop = False
-    self._main_frame_id = None
-    self._tracing_data = []
-
-  def _PageDataReceived(self, msg):
-    """Called when a Page event is received.
-
-    Records the main frame, and stops the recording once it has finished
-    loading.
-
-    Args:
-      msg: (dict) Message sent by DevTools.
-    """
-    if 'params' not in msg:
-      return
-    params = msg['params']
-    method = msg.get('method', None)
-    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
-      self._main_frame_id = params['frameId']
-    elif (method == 'Page.frameStoppedLoading'
-          and params['frameId'] == self._main_frame_id):
-      self._please_stop = True
-
-  def _TracingDataReceived(self, msg):
-    self._tracing_data.append(msg)
-
-  def _LogPageLoadInternal(self, url, clear_cache):
-    """Returns the collection of requests made to load a given URL.
-
-    Assumes that DevTools is available on http://localhost:DEVTOOLS_PORT.
-
-    Args:
-      url: URL to load.
-      clear_cache: Whether to clear the HTTP cache.
-
-    Returns:
-      [inspector_network.InspectorNetworkResponseData, ...]
-    """
-    self._main_frame_id = None
-    self._please_stop = False
-    r = httplib.HTTPConnection(
-        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      logging.error('Cannot connect to the remote target.')
-      return None
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    inspector = inspector_network.InspectorNetwork(ws)
-    if clear_cache:
-      inspector.ClearCache()
-    ws.SyncRequest({'method': 'Page.enable'})
-    ws.RegisterDomain('Page', self._PageDataReceived)
-    inspector.StartMonitoringNetwork()
-    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
-                              'params': {'url': url}})
-    while not self._please_stop:
-      try:
-        ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException as e:
-        logging.warning('Exception: ' + str(e))
-        break
-    if not self._please_stop:
-      logging.warning('Finished with timeout instead of page load')
-    inspector.StopMonitoringNetwork()
-    return inspector.GetResponseData()
-
-  def _LogTracingInternal(self, url):
-    self._main_frame_id = None
-    self._please_stop = False
-    r = httplib.HTTPConnection('localhost', device_setup.DEVTOOLS_PORT)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      logging.error('Cannot connect to the remote target.')
-      return None
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    ws.RegisterDomain('Tracing', self._TracingDataReceived)
-    logging.warning('Tracing.start: ' +
-                    str(ws.SyncRequest({'method': 'Tracing.start',
-                                        'options': 'zork'})))
-    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
-                              'params': {'url': url}})
-    while not self._please_stop:
-      try:
-        ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException:
-        break
-    if not self._please_stop:
-      logging.warning('Finished with timeout instead of page load')
-    return {'events': self._tracing_data,
-            'end': ws.SyncRequest({'method': 'Tracing.end'})}
-
-
-  def LogPageLoad(self, url, clear_cache, package):
-    """Returns the collection of requests made to load a given URL on a device.
-
-    Args:
-      url: (str) URL to load on the device.
-      clear_cache: (bool) Whether to clear the HTTP cache.
-
-    Returns:
-      See _LogPageLoadInternal().
-    """
-    return device_setup.SetUpAndExecute(
-        self.device, package,
-        lambda: self._LogPageLoadInternal(url, clear_cache))
-
-  def LogTracing(self, url):
-    """Log tracing events from a load of the given URL.
-
-    TODO(mattcary): This doesn't work. It would be best to log tracing
-    simultaneously with network requests, but as that wasn't working the tracing
-    logging was broken out separately. It still doesn't work...
-    """
-    return device_setup.SetUpAndExecute(
-        self.device, 'chrome', lambda: self._LogTracingInternal(url))
-
-
-def _ResponseDataToJson(data):
-  """Converts a list of inspector_network.InspectorNetworkResponseData to JSON.
-
-  Args:
-    data: as returned by _LogPageLoad()
-
-  Returns:
-    A JSON file with the following format:
-    [request1, request2, ...], and a request is:
-    {'status': str, 'headers': dict, 'request_headers': dict,
-     'timestamp': double, 'timing': dict, 'url': str,
-      'served_from_cache': bool, 'initiator': str})
-  """
-  result = []
-  for r in data:
-    result.append({'status': r.status,
-                   'headers': r.headers,
-                   'request_headers': r.request_headers,
-                   'timestamp': r.timestamp,
-                   'timing': r.timing,
-                   'url': r.url,
-                   'served_from_cache': r.served_from_cache,
-                   'initiator': r.initiator})
-  return json.dumps(result)
-
-
-def _CreateOptionParser():
-  """Returns the option parser for this tool."""
-  parser = optparse.OptionParser(description='Starts a browser on an Android '
-                                 'device, gathers the requests made to load a '
-                                 'page and dumps it to a JSON file.')
-  parser.add_option('--url', help='URL to load.',
-                    default='https://www.google.com', metavar='URL')
-  parser.add_option('--output', help='Output file.', default='result.json')
-  parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
-                                              'before loading the URL.'),
-                    default=True, action='store_false', dest='clear_cache')
-  parser.add_option('--package', help='Package info for chrome build. '
-                                      'See build/android/pylib/constants.',
-                    default='chrome')
-  parser.add_option('--local', action='store_true', default=False,
-                    help='Connect to local chrome session rather than android.')
-  return parser
-
-
-def main():
-  logging.basicConfig(level=logging.WARNING)
-  parser = _CreateOptionParser()
-  options, _ = parser.parse_args()
-
-  devil_chromium.Initialize()
-
-  if options.local:
-    device = None
-  else:
-    devices = device_utils.DeviceUtils.HealthyDevices()
-    device = devices[0]
-
-  request_logger = AndroidRequestsLogger(device)
-  response_data = request_logger.LogPageLoad(
-      options.url, options.clear_cache, options.package)
-  json_data = _ResponseDataToJson(response_data)
-  with open(options.output, 'w') as f:
-    f.write(json_data)
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/deprecated/process_request_log.py b/loading/deprecated/process_request_log.py
deleted file mode 100755
index bbec0a8..0000000
--- a/loading/deprecated/process_request_log.py
+++ /dev/null
@@ -1,189 +0,0 @@
-#! /usr/bin/python
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Creates a Graphviz file visualizing the resource dependencies from a JSON
-file dumped by log_requests.py.
-"""
-
-import collections
-import sys
-import urlparse
-
-import log_parser
-from log_parser import Resource
-
-
-def _BuildResourceDependencyGraph(requests):
-  """Builds the graph of resource dependencies.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    A tuple ([Resource], [(resource1, resource2, reason), ...])
-  """
-  resources = log_parser.GetResources(requests)
-  resources_from_url = {resource.url: resource for resource in resources}
-  requests_by_completion = log_parser.SortedByCompletion(requests)
-  deps = []
-  for r in requests:
-    resource = Resource.FromRequest(r)
-    initiator = r.initiator
-    initiator_type = initiator['type']
-    dep = None
-    if initiator_type == 'parser':
-      url = initiator['url']
-      blocking_resource = resources_from_url.get(url, None)
-      if blocking_resource is None:
-        continue
-      dep = (blocking_resource, resource, 'parser')
-    elif initiator_type == 'script' and 'stackTrace' in initiator:
-      for frame in initiator['stackTrace']:
-        url = frame['url']
-        blocking_resource = resources_from_url.get(url, None)
-        if blocking_resource is None:
-          continue
-        dep = (blocking_resource, resource, 'stack')
-        break
-    else:
-      # When the initiator is a script without a stackTrace, infer that it comes
-      # from the most recent script from the same hostname.
-      # TLD+1 might be better, but finding what is a TLD requires a database.
-      request_hostname = urlparse.urlparse(r.url).hostname
-      sorted_script_requests_from_hostname = [
-          r for r in requests_by_completion
-          if (resource.GetContentType() in ('script', 'html', 'json')
-              and urlparse.urlparse(r.url).hostname == request_hostname)]
-      most_recent = None
-      # Linear search is bad, but this shouldn't matter here.
-      for request in sorted_script_requests_from_hostname:
-        if request.timestamp < r.timing.requestTime:
-          most_recent = request
-        else:
-          break
-      if most_recent is not None:
-        blocking = resources_from_url.get(most_recent.url, None)
-        if blocking is not None:
-          dep = (blocking, resource, 'script_inferred')
-    if dep is not None:
-      deps.append(dep)
-  return (resources, deps)
-
-
-def PrefetchableResources(requests):
-  """Returns a list of resources that are discoverable without JS.
-
-  Args:
-    requests: List of requests.
-
-  Returns:
-    List of discoverable resources, with their initial request.
-  """
-  resource_to_request = log_parser.ResourceToRequestMap(requests)
-  (_, all_deps) = _BuildResourceDependencyGraph(requests)
-  # Only keep "parser" arcs
-  deps = [(first, second) for (first, second, reason) in all_deps
-          if reason == 'parser']
-  deps_per_resource = collections.defaultdict(list)
-  for (first, second) in deps:
-    deps_per_resource[first].append(second)
-  result = []
-  visited = set()
-  to_visit = [deps[0][0]]
-  while len(to_visit) != 0:
-    r = to_visit.pop()
-    visited.add(r)
-    to_visit += deps_per_resource[r]
-    result.append(resource_to_request[r])
-  return result
-
-
-_CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
-                          'json': 'purple', 'gif_image': 'grey',
-                          'image': 'orange', 'other': 'white'}
-
-
-def _ResourceGraphvizNode(resource, request, resource_to_index):
-  """Returns the node description for a given resource.
-
-  Args:
-    resource: Resource.
-    request: RequestData associated with the resource.
-    resource_to_index: {Resource: int}.
-
-  Returns:
-    A string describing the resource in graphviz format.
-    The resource is color-coded according to its content type, and its shape is
-    oval if its max-age is less than 300s (or if it's not cacheable).
-  """
-  color = _CONTENT_TYPE_TO_COLOR[resource.GetContentType()]
-  max_age = log_parser.MaxAge(request)
-  shape = 'polygon' if max_age > 300 else 'oval'
-  return ('%d [label = "%s"; style = "filled"; fillcolor = %s; shape = %s];\n'
-          % (resource_to_index[resource], resource.GetShortName(), color,
-             shape))
-
-
-def _GraphvizFileFromDeps(resources, requests, deps, output_filename):
-  """Writes a graphviz file from a set of resource dependencies.
-
-  Args:
-    resources: [Resource, ...]
-    requests: list of requests
-    deps: [(resource1, resource2, reason), ...]
-    output_filename: file to write the graph to.
-  """
-  with open(output_filename, 'w') as f:
-    f.write("""digraph dependencies {
-    rankdir = LR;
-    """)
-    resource_to_request = log_parser.ResourceToRequestMap(requests)
-    resource_to_index = {r: i for (i, r) in enumerate(resources)}
-    resources_with_edges = set()
-    for (first, second, reason) in deps:
-      resources_with_edges.add(first)
-      resources_with_edges.add(second)
-    if len(resources_with_edges) != len(resources):
-      f.write("""subgraph cluster_orphans {
-  color=black;
-  label="Orphans";
-""")
-      for resource in resources:
-        if resource not in resources_with_edges:
-          request = resource_to_request[resource]
-          f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
-      f.write('}\n')
-
-    f.write("""subgraph cluster_nodes {
-  color=invis;
-""")
-    for resource in resources:
-      request = resource_to_request[resource]
-      print resource.url
-      if resource in resources_with_edges:
-        f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
-    for (first, second, reason) in deps:
-      arrow = ''
-      if reason == 'parser':
-        arrow = '[color = red]'
-      elif reason == 'stack':
-        arrow = '[color = blue]'
-      elif reason == 'script_inferred':
-        arrow = '[color = blue; style=dotted]'
-      f.write('%d -> %d %s;\n' % (
-          resource_to_index[first], resource_to_index[second], arrow))
-    f.write('}\n}\n')
-
-
-def main():
-  filename = sys.argv[1]
-  requests = log_parser.ParseJsonFile(filename)
-  requests = log_parser.FilterRequests(requests)
-  (resources, deps) = _BuildResourceDependencyGraph(requests)
-  _GraphvizFileFromDeps(resources, requests, deps, filename + '.dot')
-
-
-if __name__ == '__main__':
-  main()

commit 2385e6fe21b15547360ba6ec24c42ed9a4182462
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 29 07:39:00 2016 -0700

    Clovis: update resource sack to use new dependency graph.
    
    This is a refactoring with (hopefully) no functional change.
    
    Review URL: https://codereview.chromium.org/1837193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383719}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6341256079ecdb84a47165c1fda47ed308ccb87e

diff --git a/loading/core_set.py b/loading/core_set.py
index 8ab9493..3bbb66f 100644
--- a/loading/core_set.py
+++ b/loading/core_set.py
@@ -15,8 +15,9 @@ import multiprocessing
 import os
 import sys
 
-import loading_model
+import dependency_graph
 import loading_trace
+import request_dependencies_lens
 import resource_sack
 
 
@@ -34,8 +35,10 @@ def _PageCore(prefix, graph_set_names, output):
     _Progress('Processing %s' % name)
     for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
       _Progress('Reading %s' % filename)
-      graph = loading_model.ResourceGraph(
-          loading_trace.LoadingTrace.FromJsonFile(filename))
+      trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+      graph = dependency_graph.RequestDependencyGraph(
+          trace.request_track.GetEvents(),
+          request_dependencies_lens.RequestDependencyLens(trace))
       sack.ConsumeGraph(graph)
       name_graphs.append(graph)
     graph_sets.append(name_graphs)
@@ -90,8 +93,10 @@ def _AllCores(prefix, graph_set_names, output, threshold):
     this_set = []
     for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
       _Progress('Reading %s' % filename)
-      graph = loading_model.ResourceGraph(
-          loading_trace.LoadingTrace.FromJsonDict(json.load(open(filename))))
+      trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+      graph = dependency_graph.RequestDependencyGraph(
+          trace.request_track.GetEvents(),
+          request_dependencies_lens.RequestDependencyLens(trace))
       sack.ConsumeGraph(graph)
       big_sack.ConsumeGraph(graph)
       this_set.append(graph)
diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
index 23285cc..be2c5e0 100644
--- a/loading/dependency_graph.py
+++ b/loading/dependency_graph.py
@@ -46,6 +46,11 @@ class RequestDependencyGraph(object):
     self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
     self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
 
+  @property
+  def graph(self):
+    """Return the Graph we're based on."""
+    return self._deps_graph
+
   def UpdateRequestsCost(self, request_id_to_cost):
     """Updates the cost of the nodes identified by their request ID.
 
diff --git a/loading/graph.py b/loading/graph.py
index a665439..789d917 100644
--- a/loading/graph.py
+++ b/loading/graph.py
@@ -43,15 +43,15 @@ class DirectedGraph(object):
     Note that the edges referencing a node not in the provided list are dropped.
 
     Args:
-      nodes: ([Node]) List of nodes.
-      edges: ([Edge]) List of Edges.
+      nodes: ([Node]) Sequence of Nodes.
+      edges: ([Edge]) Sequence of Edges.
     """
-    assert all(isinstance(node, Node) for node in nodes)
-    assert all(isinstance(edge, Edge) for edge in edges)
     self._nodes = set(nodes)
     self._edges = set(filter(
         lambda e: e.from_node in self._nodes and e.to_node in self._nodes,
         edges))
+    assert all(isinstance(node, Node) for node in self._nodes)
+    assert all(isinstance(edge, Edge) for edge in self._edges)
     self._in_edges = {n: [] for n in self._nodes}
     self._out_edges = {n: [] for n in self._nodes}
     for edge in self._edges:
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 90ced38..563bb77 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -17,16 +17,16 @@ import urlparse
 from collections import defaultdict
 
 import content_classification_lens
-import dag
+import graph
 import user_satisfied_lens
 
 class GraphSack(object):
-  """Aggreate of ResourceGraphs.
+  """Aggreate of RequestDependencyGraphs.
 
-  Collects ResourceGraph nodes into bags, where each bag contains the nodes with
-  common urls. Dependency edges are tracked between bags (so that each bag may
-  be considered as a node of a graph). This graph of bags is referred to as a
-  sack.
+  Collects RequestDependencyGraph nodes into bags, where each bag contains the
+  nodes with common urls. Dependency edges are tracked between bags (so that
+  each bag may be considered as a node of a graph). This graph of bags is
+  referred to as a sack.
 
   Each bag is associated with a dag.Node, even though the bag graph may not be a
   DAG. The edges are annotated with list of graphs and nodes that generated
@@ -37,55 +37,56 @@ class GraphSack(object):
 
   _GraphInfo = collections.namedtuple('_GraphInfo', (
       'cost',   # The graph cost (aka critical path length).
-      'total_costs',  # A vector by node index of total cost of each node.
       ))
 
   def __init__(self):
-    # A bag is a node in our combined graph.
-    self._bags = []
-    # Each bag in our sack corresponds to a url, as expressed by this map.
-    self._url_to_bag = {}
+    # Each bag in our sack is named as indicated by this map.
+    self._name_to_bag = {}
+    # List our edges by bag pairs: (from_bag, to_bag) -> graph.Edge.
+    self._edges = {}
     # Maps graph -> _GraphInfo structures for each graph we've consumed.
     self._graph_info = {}
 
-  def ConsumeGraph(self, graph):
+    # Our graph, updated after each ConsumeGraph.
+    self._graph = None
+
+  def ConsumeGraph(self, request_graph):
     """Add a graph and process.
 
     Args:
-      graph: (ResourceGraph) the graph to add. The graph is processed sorted
-        according to its current filter.
+      graph: (RequestDependencyGraph) the graph to add.
     """
     assert graph not in self._graph_info
-    critical_path = []
-    total_costs = []
-    cost = graph.Cost(path_list=critical_path,
-                      costs_out=total_costs)
-    self._graph_info[graph] = self._GraphInfo(
-        cost=cost, total_costs=total_costs)
-    for n in graph.Nodes(sort=True):
-      assert graph._node_filter(n.Node())
-      self.AddNode(graph, n)
-    for node in critical_path:
-      self._url_to_bag[node.Url()].MarkCritical()
-
-  def AddNode(self, graph, node):
+    cost = request_graph.Cost()
+    self._graph_info[request_graph] = self._GraphInfo(cost=cost)
+    for n in request_graph.graph.Nodes():
+      self.AddNode(request_graph, n)
+
+    # TODO(mattcary): this is inefficient but our current API doesn't require an
+    # explicit graph creation from the client.
+    self._graph = graph.DirectedGraph(self.bags, self._edges.itervalues())
+
+  def AddNode(self, request_graph, node):
     """Add a node to our collection.
 
     Args:
-      graph: (ResourceGraph) the graph in which the node lives.
-      node: (NodeInfo) the node to add.
+      graph: (RequestDependencyGraph) the graph in which the node lives.
+      node: (RequestDependencyGraph node) the node to add.
 
     Returns:
       The Bag containing the node.
     """
-    if not graph._node_filter(node):
-      return
-    if node.Url() not in self._url_to_bag:
-      new_index = len(self._bags)
-      self._bags.append(Bag(self, new_index, node.Url()))
-      self._url_to_bag[node.Url()] = self._bags[-1]
-    self._url_to_bag[node.Url()].AddNode(graph, node)
-    return self._url_to_bag[node.Url()]
+    sack_name = self._GetSackName(node)
+    if sack_name not in self._name_to_bag:
+      self._name_to_bag[sack_name] = Bag(self, sack_name)
+    bag = self._name_to_bag[sack_name]
+    bag.AddNode(request_graph, node)
+    return bag
+
+  def AddEdge(self, from_bag, to_bag):
+    """Add an edge between two bags."""
+    if (from_bag, to_bag) not in self._edges:
+      self._edges[(from_bag, to_bag)] = graph.Edge(from_bag, to_bag)
 
   def CoreSet(self, *graph_sets):
     """Compute the core set of this sack.
@@ -141,7 +142,7 @@ class GraphSack(object):
 
   @property
   def bags(self):
-    return self._bags
+    return self._name_to_bag.values()
 
   def _SingleCore(self, graph_set):
     core = set()
@@ -153,31 +154,31 @@ class GraphSack(object):
         core.add(b.label)
     return core
 
+  def _GetSackName(self, node):
+    return self._MakeShortname(node.request.url)
+
+  @classmethod
+  def _MakeShortname(cls, url):
+    # TODO(lizeb): Move this method to a convenient common location.
+    parsed = urlparse.urlparse(url)
+    if parsed.scheme == 'data':
+      if ';' in parsed.path:
+        kind, _ = parsed.path.split(';', 1)
+      else:
+        kind, _ = parsed.path.split(',', 1)
+      return 'data:' + kind
+    path = parsed.path[:10]
+    hostname = parsed.hostname if parsed.hostname else '?.?.?'
+    return hostname + '/' + path
+
 
-class Bag(dag.Node):
-  def __init__(self, sack, index, url):
-    super(Bag, self).__init__(index)
+class Bag(graph.Node):
+  def __init__(self, sack, label):
+    super(Bag, self).__init__()
     self._sack = sack
-    self._url = url
-    self._label = self._MakeShortname(url)
+    self._label = label
     # Maps a ResourceGraph to its Nodes contained in this Bag.
     self._graphs = defaultdict(set)
-    # Maps each successor bag to the set of (graph, node, graph-successor)
-    # tuples that generated it.
-    self._successor_sources = defaultdict(set)
-    # Maps each successor bag to a set of edge costs. This is just used to
-    # track min and max; if we want more statistics we'd have to count the
-    # costs with multiplicity.
-    self._successor_edge_costs = defaultdict(set)
-
-    # Miscellaneous counts and costs used in display.
-    self._total_costs = []
-    self._relative_costs = []
-    self._num_critical = 0
-
-  @property
-  def url(self):
-    return self._url
 
   @property
   def label(self):
@@ -185,62 +186,16 @@ class Bag(dag.Node):
 
   @property
   def graphs(self):
-    return self._graphs
-
-  @property
-  def successor_sources(self):
-    return self._successor_sources
-
-  @property
-  def successor_edge_costs(self):
-    return self._successor_edge_costs
-
-  @property
-  def total_costs(self):
-    return self._total_costs
-
-  @property
-  def relative_costs(self):
-    return self._relative_costs
-
-  @property
-  def num_critical(self):
-    return self._num_critical
+    return self._graphs.iterkeys()
 
   @property
   def num_nodes(self):
-    return len(self._total_costs)
-
-  def MarkCritical(self):
-    self._num_critical += 1
+    return sum(len(g) for g in self._graphs.itervalues())
 
-  def AddNode(self, graph, node):
-    if node in self._graphs[graph]:
+  def AddNode(self, request_graph, node):
+    if node in self._graphs[request_graph]:
       return  # Already added.
-    graph_info = self._sack.graph_info[graph]
-    self._graphs[graph].add(node)
-    node_total_cost = graph_info.total_costs[node.Index()]
-    self._total_costs.append(node_total_cost)
-    self._relative_costs.append(
-        float(node_total_cost) / graph_info.cost)
-    for s in node.Node().Successors():
-      if not graph._node_filter(s):
-        continue
-      node_info = graph.NodeInfo(s)
-      successor_bag = self._sack.AddNode(graph, node_info)
-      self.AddSuccessor(successor_bag)
-      self._successor_sources[successor_bag].add((graph, node, s))
-      self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
-
-  @classmethod
-  def _MakeShortname(cls, url):
-    parsed = urlparse.urlparse(url)
-    if parsed.scheme == 'data':
-      if ';' in parsed.path:
-        kind, _ = parsed.path.split(';', 1)
-      else:
-        kind, _ = parsed.path.split(',', 1)
-      return 'data:' + kind
-    path = parsed.path[:10]
-    hostname = parsed.hostname if parsed.hostname else '?.?.?'
-    return hostname + '/' + path
+    self._graphs[request_graph].add(node)
+    for edge in request_graph.graph.OutEdges(node):
+      out_bag = self._sack.AddNode(request_graph, edge.to_node)
+      self._sack.AddEdge(self, out_bag)
diff --git a/loading/resource_sack_display.py b/loading/resource_sack_display.py
deleted file mode 100644
index 210f4da..0000000
--- a/loading/resource_sack_display.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities for displaying a ResourceSack.
-
-When run standalone, takes traces on the command line and produces a dot file to
-stdout.
-"""
-
-
-def ToDot(sack, output, prune=-1, long_edge_msec=2000):
-  """Output as a dot file.
-
-  Args:
-    sack: (ResourceSack) the sack to convert to dot.
-    output: a file-like output stream.
-    prune: if positive, prune & coalesce nodes under the specified threshold
-      of repeated views, as fraction node views / total graphs. All pruned
-      nodes are represented by a single node, and an edge is connected only if
-      the view count is greater than 1.
-    long_edge_msec: if positive, the definition of a long edge. Long edges are
-      distinguished in graph.
-  """
-  output.write("""digraph dependencies {
-  rankdir = LR;
-  """)
-
-  pruned = set()
-  num_graphs = len(sack.graph_info)
-  for bag in sack.bags:
-    if prune > 0 and float(len(bag.graphs)) / num_graphs < prune:
-      pruned.add(bag)
-      continue
-    output.write('%d [label="%s (%d)\n(%d, %d)\n(%.2f, %.2f)" shape=%s; '
-                 'style=filled; fillcolor=%s];\n' % (
-        bag.Index(), bag.label, len(bag.graphs),
-        min(bag.total_costs), max(bag.total_costs),
-        min(bag.relative_costs), max(bag.relative_costs),
-        _CriticalToShape(bag),
-        _AmountToNodeColor(len(bag.graphs), num_graphs)))
-
-  if pruned:
-    pruned_index = num_graphs
-    output.write('%d [label="Pruned at %.0f%%\n(%d)"; '
-                 'shape=polygon; style=dotted];\n' %
-                 (pruned_index, 100 * prune, len(pruned)))
-
-  for bag in sack.bags:
-    if bag in pruned:
-      for succ in bag.Successors():
-        if succ not in pruned:
-          output.write('%d -> %d [style=dashed];\n' % (
-              pruned_index, succ.Index()))
-    for succ in bag.Successors():
-      if succ in pruned:
-        if len(bag.successor_sources[succ]) > 1:
-          output.write('%d -> %d [label="%d"; style=dashed];\n' % (
-              bag.Index(), pruned_index, len(bag.successor_sources[succ])))
-      else:
-        num_succ = len(bag.successor_sources[succ])
-        num_long = 0
-        for graph, source, target in bag.successor_sources[succ]:
-          if graph.EdgeCost(source, target) > long_edge_msec:
-            num_long += 1
-        if num_long > 0:
-          long_frac = float(num_long) / num_succ
-          long_edge_style = '; penwidth=%f' % (2 + 6.0 * long_frac)
-          if long_frac < 0.75:
-            long_edge_style += '; style=dashed'
-        else:
-          long_edge_style = ''
-        min_edge = min(bag.successor_edge_costs[succ])
-        max_edge = max(bag.successor_edge_costs[succ])
-        output.write('%d -> %d [label="%d\n(%f,%f)"; color=%s %s];\n' % (
-            bag.Index(), succ.Index(), num_succ, min_edge, max_edge,
-            _AmountToEdgeColor(num_succ, len(bag.graphs)),
-            long_edge_style))
-
-  output.write('}')
-
-
-def _CriticalToShape(bag):
-  frac = float(bag.num_critical) / bag.num_nodes
-  if frac < 0.4:
-    return 'oval'
-  elif frac < 0.7:
-    return 'polygon'
-  elif frac < 0.9:
-    return 'trapezium'
-  return 'box'
-
-
-def _AmountToNodeColor(numer, denom):
-  if denom <= 0:
-    return 'grey72'
-  ratio = 1.0 * numer / denom
-  if ratio < .3:
-    return 'white'
-  elif ratio < .6:
-    return 'yellow'
-  elif ratio < .8:
-    return 'orange'
-  return 'green'
-
-
-def _AmountToEdgeColor(numer, denom):
-  color = _AmountToNodeColor(numer, denom)
-  if color == 'white' or color == 'grey72':
-    return 'black'
-  return color
-
-
-def _Main():
-  import json
-  import logging
-  import sys
-
-  import loading_model
-  import loading_trace
-  import resource_sack
-
-  sack = resource_sack.GraphSack()
-  for fname in sys.argv[1:]:
-    trace = loading_trace.LoadingTrace.FromJsonDict(
-      json.load(open(fname)))
-    logging.info('Making graph from %s', fname)
-    model = loading_model.ResourceGraph(trace, content_lens=None)
-    sack.ConsumeGraph(model)
-    logging.info('Finished %s', fname)
-  ToDot(sack, sys.stdout, prune=.1)
-
-
-if __name__ == '__main__':
-  _Main()
diff --git a/loading/resource_sack_display_unittest.py b/loading/resource_sack_display_unittest.py
deleted file mode 100644
index 65b53ff..0000000
--- a/loading/resource_sack_display_unittest.py
+++ /dev/null
@@ -1,42 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import re
-from StringIO import StringIO
-import unittest
-
-import resource_sack
-import resource_sack_display
-from test_utils import (MakeRequest,
-                        TestResourceGraph)
-
-
-class ResourceSackDispayTestCase(unittest.TestCase):
-  def test_SimpleOutput(self):
-    g1 = TestResourceGraph.FromRequestList([
-        MakeRequest(0, 'null'),
-        MakeRequest(1, 0),
-        MakeRequest(2, 0),
-        MakeRequest(3, 1)])
-    g2 = TestResourceGraph.FromRequestList([
-        MakeRequest(0, 'null'),
-        MakeRequest(1, 0),
-        MakeRequest(2, 0),
-        MakeRequest(4, 2)])
-    sack = resource_sack.GraphSack()
-    sack.ConsumeGraph(g1)
-    sack.ConsumeGraph(g2)
-    buf = StringIO()
-    resource_sack_display.ToDot(sack, buf,
-                                long_edge_msec=1000)
-    dot = buf.getvalue()
-    # Short edge.
-    self.assertTrue(re.search(r'0 -> 1[^]]+color=green \]', dot, re.MULTILINE))
-    # Long edge.
-    self.assertTrue(re.search(r'0 -> 3[^]]+penwidth=8', dot))
-
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index a4b1a30..5c3e54d 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -6,7 +6,7 @@ import unittest
 
 import resource_sack
 from test_utils import (MakeRequest,
-                        TestResourceGraph)
+                        TestDependencyGraph)
 
 
 class ResourceSackTestCase(unittest.TestCase):
@@ -15,15 +15,15 @@ class ResourceSackTestCase(unittest.TestCase):
     requests = [MakeRequest(node_names[0], 'null')]
     for n in node_names[1:]:
       requests.append(MakeRequest(n, node_names[0]))
-    return TestResourceGraph.FromRequestList(requests)
+    return TestDependencyGraph(requests)
 
   def test_NodeMerge(self):
-    g1 = TestResourceGraph.FromRequestList([
+    g1 = TestDependencyGraph([
         MakeRequest(0, 'null'),
         MakeRequest(1, 0),
         MakeRequest(2, 0),
         MakeRequest(3, 1)])
-    g2 = TestResourceGraph.FromRequestList([
+    g2 = TestDependencyGraph([
         MakeRequest(0, 'null'),
         MakeRequest(1, 0),
         MakeRequest(2, 0),
@@ -39,10 +39,10 @@ class ResourceSackTestCase(unittest.TestCase):
         self.assertEqual(1, bag.num_nodes)
 
   def test_MultiParents(self):
-    g1 = TestResourceGraph.FromRequestList([
+    g1 = TestDependencyGraph([
         MakeRequest(0, 'null'),
         MakeRequest(2, 0)])
-    g2 = TestResourceGraph.FromRequestList([
+    g2 = TestDependencyGraph([
         MakeRequest(1, 'null'),
         MakeRequest(2, 1)])
     sack = resource_sack.GraphSack()
@@ -50,17 +50,21 @@ class ResourceSackTestCase(unittest.TestCase):
     sack.ConsumeGraph(g2)
     self.assertEqual(3, len(sack.bags))
     labels = {bag.label: bag for bag in sack.bags}
+    def Predecessors(label):
+      bag = labels['%s/' % label]
+      return [e.from_node
+              for e in bag._sack._graph.InEdges(bag)]
     self.assertEqual(
         set(['0/', '1/']),
-        set([bag.label for bag in labels['2/'].Predecessors()]))
-    self.assertFalse(labels['0/'].Predecessors())
-    self.assertFalse(labels['1/'].Predecessors())
+        set([bag.label for bag in Predecessors(2)]))
+    self.assertFalse(Predecessors(0))
+    self.assertFalse(Predecessors(1))
 
   def test_Shortname(self):
     root = MakeRequest(0, 'null')
     shortname = MakeRequest(1, 0)
     shortname.url = 'data:fake/content;' + 'lotsand' * 50 + 'lotsofdata'
-    g1 = TestResourceGraph.FromRequestList([root, shortname])
+    g1 = TestDependencyGraph([root, shortname])
     sack = resource_sack.GraphSack()
     sack.ConsumeGraph(g1)
     self.assertEqual(set(['0/', 'data:fake/content']),
diff --git a/loading/test_utils.py b/loading/test_utils.py
index a221385..8e07826 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -4,6 +4,7 @@
 
 """Common utilities used in unit tests, within this directory."""
 
+import dependency_graph
 import devtools_monitor
 import loading_model
 import loading_trace
@@ -166,6 +167,13 @@ class TestResourceGraph(loading_model.ResourceGraph):
     return cls(LoadingTraceFromEvents(requests, page_events, trace_events))
 
 
+class TestDependencyGraph(dependency_graph.RequestDependencyGraph):
+  """A dependency graph created from requests using a simple lens."""
+  def __init__(self, requests):
+    lens = SimpleLens(LoadingTraceFromEvents(requests))
+    super(TestDependencyGraph, self).__init__(requests, lens)
+
+
 class MockConnection(object):
   """Mock out connection for testing.
 

commit 88ea38781b6ec94423c9ee7fee1885b704436683
Author: lizeb <lizeb@chromium.org>
Date:   Fri Mar 25 11:33:56 2016 -0700

    clovis: Estimate cost of a load, until a given point.
    
    Review URL: https://codereview.chromium.org/1827813005
    
    Cr-Original-Commit-Position: refs/heads/master@{#383318}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4a5dd80de434040fa64d2d2d4b35474f8c9f5f70

diff --git a/loading/dependency_graph.py b/loading/dependency_graph.py
new file mode 100644
index 0000000..23285cc
--- /dev/null
+++ b/loading/dependency_graph.py
@@ -0,0 +1,72 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Request dependency graph."""
+
+import graph
+import request_track
+
+
+class _RequestNode(graph.Node):
+  def __init__(self, request):
+    super(_RequestNode, self).__init__()
+    self.request = request
+    self.cost = request.Cost()
+
+
+class _Edge(graph.Edge):
+  def __init__(self, from_node, to_node, reason):
+    super(_Edge, self).__init__(from_node, to_node)
+    self.reason = reason
+    self.cost = request_track.TimeBetween(
+        self.from_node.request, self.to_node.request, self.reason)
+
+
+class RequestDependencyGraph(object):
+  """Request dependency graph."""
+  def __init__(self, requests, dependencies_lens):
+    """Creates a request dependency graph.
+
+    Args:
+      requests: ([Request]) a list of requests.
+      dependencies_lens: (RequestDependencyLens)
+    """
+    self._requests = requests
+    deps = dependencies_lens.GetRequestDependencies()
+    self._nodes_by_id = {r.request_id : _RequestNode(r) for r in self._requests}
+    edges = []
+    for (parent_request, child_request, reason) in deps:
+      if (parent_request.request_id not in self._nodes_by_id
+          or child_request.request_id not in self._nodes_by_id):
+        continue
+      parent_node = self._nodes_by_id[parent_request.request_id]
+      child_node = self._nodes_by_id[child_request.request_id]
+      edges.append(_Edge(parent_node, child_node, reason))
+    self._first_request_node = self._nodes_by_id[self._requests[0].request_id]
+    self._deps_graph = graph.DirectedGraph(self._nodes_by_id.values(), edges)
+
+  def UpdateRequestsCost(self, request_id_to_cost):
+    """Updates the cost of the nodes identified by their request ID.
+
+    Args:
+      request_id_to_cost: {request_id: new_cost} Can be a superset of the
+                          requests actually present in the graph.
+
+    """
+    for node in self._deps_graph.Nodes():
+      request_id = node.request.request_id
+      if request_id in request_id_to_cost:
+        node.cost = request_id_to_cost[request_id]
+
+  def Cost(self, from_first_request=True):
+    """Returns the cost of the graph, that is the costliest path.
+
+    Args:
+      from_first_request: (boolean) If True, only considers paths that originate
+                          from the first request node.
+    """
+    if from_first_request:
+      return self._deps_graph.Cost([self._first_request_node])
+    else:
+      return self._deps_graph.Cost()
diff --git a/loading/dependency_graph_unittest.py b/loading/dependency_graph_unittest.py
new file mode 100644
index 0000000..448a234
--- /dev/null
+++ b/loading/dependency_graph_unittest.py
@@ -0,0 +1,54 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import dependency_graph
+import request_dependencies_lens
+from request_dependencies_lens_unittest import TestRequests
+import request_track
+
+
+class RequestDependencyGraphTestCase(unittest.TestCase):
+  def setUp(self):
+    super(RequestDependencyGraphTestCase, self).setUp()
+    self.trace = TestRequests.CreateLoadingTrace()
+
+  def testUpdateRequestCost(self):
+    requests = self.trace.request_track.GetEvents()
+    requests[0].timing = request_track.TimingFromDict(
+        {'requestTime': 12, 'loadingFinished': 10})
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        self.trace)
+    g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    self.assertEqual(10, g.Cost())
+    request_id = requests[0].request_id
+    g.UpdateRequestsCost({request_id: 100})
+    self.assertEqual(100, g.Cost())
+    g.UpdateRequestsCost({'unrelated_id': 1000})
+    self.assertEqual(100, g.Cost())
+
+  def testCost(self):
+    requests = self.trace.request_track.GetEvents()
+    for (index, request) in enumerate(requests):
+      request.timing = request_track.TimingFromDict(
+          {'requestTime': index, 'receiveHeadersEnd': 10,
+           'loadingFinished': 10})
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        self.trace)
+    g = dependency_graph.RequestDependencyGraph(requests, dependencies_lens)
+    # First redirect -> Second redirect -> Redirected Request -> Request ->
+    # JS Request 2
+    self.assertEqual(7010, g.Cost())
+    # Not on the critical path
+    g.UpdateRequestsCost({TestRequests.JS_REQUEST.request_id: 0})
+    self.assertEqual(7010, g.Cost())
+    g.UpdateRequestsCost({TestRequests.FIRST_REDIRECT_REQUEST.request_id: 0})
+    self.assertEqual(7000, g.Cost())
+    g.UpdateRequestsCost({TestRequests.SECOND_REDIRECT_REQUEST.request_id: 0})
+    self.assertEqual(6990, g.Cost())
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/graph.py b/loading/graph.py
new file mode 100644
index 0000000..a665439
--- /dev/null
+++ b/loading/graph.py
@@ -0,0 +1,175 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Support for graphs."""
+
+import collections
+
+
+class Node(object):
+  """A node in a Graph.
+
+  Nodes are identified within a graph using object identity.
+  """
+  def __init__(self):
+    """Create a new node."""
+    self.cost = 0
+
+
+class Edge(object):
+  """Represents an edge in a graph."""
+  def __init__(self, from_node, to_node):
+    """Creates an Edge.
+
+    Args:
+      from_node: (Node) Start node.
+      to_node: (Node) End node.
+    """
+    self.from_node = from_node
+    self.to_node = to_node
+    self.cost = 0
+
+
+class DirectedGraph(object):
+  """Directed graph.
+
+  A graph is identified by a list of nodes and a list of edges. It does not need
+  to be acyclic, but then some methods will fail.
+  """
+  def __init__(self, nodes, edges):
+    """Builds a graph from a set of node and edges.
+
+    Note that the edges referencing a node not in the provided list are dropped.
+
+    Args:
+      nodes: ([Node]) List of nodes.
+      edges: ([Edge]) List of Edges.
+    """
+    assert all(isinstance(node, Node) for node in nodes)
+    assert all(isinstance(edge, Edge) for edge in edges)
+    self._nodes = set(nodes)
+    self._edges = set(filter(
+        lambda e: e.from_node in self._nodes and e.to_node in self._nodes,
+        edges))
+    self._in_edges = {n: [] for n in self._nodes}
+    self._out_edges = {n: [] for n in self._nodes}
+    for edge in self._edges:
+      self._out_edges[edge.from_node].append(edge)
+      self._in_edges[edge.to_node].append(edge)
+
+  def OutEdges(self, node):
+    """Returns a list of edges starting from a node.
+    """
+    return self._out_edges[node]
+
+  def InEdges(self, node):
+    """Returns a list of edges ending at a node."""
+    return self._in_edges[node]
+
+  def Nodes(self):
+    """Returns the set of nodes of this graph."""
+    return self._nodes
+
+  def Edges(self):
+    """Returns the set of edges of this graph."""
+    return self._edges
+
+  def UpdateEdge(self, edge, new_from_node, new_to_node):
+    """Updates an edge.
+
+    Args:
+      edge:
+      new_from_node:
+      new_to_node:
+    """
+    assert edge in self._edges
+    assert new_from_node in self._nodes
+    assert new_to_node in self._nodes
+    self._in_edges[edge.to_node].remove(edge)
+    self._out_edges[edge.from_node].remove(edge)
+    edge.from_node = new_from_node
+    edge.to_node = new_to_node
+    # TODO(lizeb): Check for duplicate edges?
+    self._in_edges[edge.to_node].append(edge)
+    self._out_edges[edge.from_node].append(edge)
+
+  def TopologicalSort(self, roots=None):
+    """Returns a list of nodes, in topological order.
+
+      Args:
+        roots: ([Node]) If set, the topological sort will only consider nodes
+                        reachable from this list of sources.
+    """
+    sorted_nodes = []
+    if roots is None:
+      nodes_subset = self._nodes
+    else:
+      nodes_subset = self.ReachableNodes(roots)
+    remaining_in_edges = {n: 0 for n in nodes_subset}
+    for edge in self._edges:
+      if edge.from_node in nodes_subset and edge.to_node in nodes_subset:
+        remaining_in_edges[edge.to_node] += 1
+    sources = [node for (node, count) in remaining_in_edges.items()
+               if count == 0]
+    while sources:
+      node = sources.pop(0)
+      sorted_nodes.append(node)
+      for e in self.OutEdges(node):
+        successor = e.to_node
+        if successor not in nodes_subset:
+          continue
+        assert remaining_in_edges[successor] > 0
+        remaining_in_edges[successor] -= 1
+        if remaining_in_edges[successor] == 0:
+          sources.append(successor)
+    return sorted_nodes
+
+  def ReachableNodes(self, roots):
+    """Returns a list of nodes from a set of root nodes."""
+    visited = set()
+    fifo = collections.deque(roots)
+    while len(fifo) != 0:
+      node = fifo.pop()
+      visited.add(node)
+      for e in self.OutEdges(node):
+        if e.to_node not in visited:
+          visited.add(e.to_node)
+        fifo.appendleft(e.to_node)
+    return list(visited)
+
+  def Cost(self, roots=None, path_list=None, costs_out=None):
+    """Compute the cost of the graph.
+
+    Args:
+      roots: ([Node]) If set, only compute the cost of the paths reachable
+             from this list of nodes.
+      path_list: if not None, gets a list of nodes in the longest path.
+      costs_out: if not None, gets a vector of node costs by node.
+
+    Returns:
+      Cost of the longest path.
+    """
+    costs = {n: 0 for n in self._nodes}
+    for node in self.TopologicalSort(roots):
+      cost = 0
+      if self.InEdges(node):
+        cost = max([costs[e.from_node] + e.cost for e in self.InEdges(node)])
+      costs[node] = cost + node.cost
+    max_cost = max(costs.values())
+    if costs_out is not None:
+      del costs_out[:]
+      costs_out.extend(costs)
+    assert max_cost > 0
+    if path_list is not None:
+      del path_list[:]
+      node = (i for i in self._nodes if costs[i] == max_cost).next()
+      path_list.append(node)
+      while self.InEdges(node):
+        predecessors = [e.from_node for e in self.InEdges(node)]
+        node = reduce(
+            lambda costliest_node, next_node:
+            next_node if costs[next_node] > costs[costliest_node]
+            else costliest_node, predecessors)
+        path_list.insert(0, node)
+    return max_cost
diff --git a/loading/graph_unittest.py b/loading/graph_unittest.py
new file mode 100644
index 0000000..e8ef761
--- /dev/null
+++ b/loading/graph_unittest.py
@@ -0,0 +1,170 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import operator
+import os
+import sys
+import unittest
+
+import graph
+
+
+class _IndexedNode(graph.Node):
+  def __init__(self, index):
+    super(_IndexedNode, self).__init__()
+    self.index = index
+
+
+class GraphTestCase(unittest.TestCase):
+  @classmethod
+  def MakeGraph(cls, count, edge_tuples):
+    """Makes a graph from a list of edges.
+
+    Args:
+      count: Number of nodes.
+      edge_tuples: (from_index, to_index). Both indices must be in [0, count),
+                   and uniquely identify a node.
+    """
+    nodes = [_IndexedNode(i) for i in xrange(count)]
+    edges = [graph.Edge(nodes[from_index], nodes[to_index])
+             for (from_index, to_index) in edge_tuples]
+    return (nodes, edges, graph.DirectedGraph(nodes, edges))
+
+  @classmethod
+  def _NodesIndices(cls, g):
+    return map(operator.attrgetter('index'), g.Nodes())
+
+  def testBuildGraph(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
+    self.assertSetEqual(set(edges), set(g.Edges()))
+
+    self.assertSetEqual(set([edges[0], edges[1]]), set(g.OutEdges(nodes[0])))
+    self.assertFalse(g.InEdges(nodes[0]))
+    self.assertSetEqual(set([edges[2]]), set(g.OutEdges(nodes[1])))
+    self.assertSetEqual(set([edges[0]]), set(g.InEdges(nodes[1])))
+    self.assertFalse(g.OutEdges(nodes[2]))
+    self.assertSetEqual(set([edges[1]]), set(g.InEdges(nodes[2])))
+    self.assertSetEqual(set([edges[3]]), set(g.OutEdges(nodes[3])))
+    self.assertSetEqual(set([edges[2]]), set(g.InEdges(nodes[3])))
+    self.assertFalse(g.OutEdges(nodes[4]))
+    self.assertSetEqual(set([edges[3]]), set(g.InEdges(nodes[4])))
+    self.assertSetEqual(set([edges[4]]), set(g.OutEdges(nodes[5])))
+    self.assertFalse(g.InEdges(nodes[5]))
+    self.assertFalse(g.OutEdges(nodes[6]))
+    self.assertSetEqual(set([edges[4]]), set(g.InEdges(nodes[6])))
+
+  def testIgnoresUnknownEdges(self):
+    nodes = [_IndexedNode(i) for i in xrange(7)]
+    edges = [graph.Edge(nodes[from_index], nodes[to_index])
+             for (from_index, to_index) in [
+                 (0, 1), (0, 2), (1, 3), (3, 4), (5, 6)]]
+    edges.append(graph.Edge(nodes[4], _IndexedNode(42)))
+    edges.append(graph.Edge(_IndexedNode(42), nodes[5]))
+    g = graph.DirectedGraph(nodes, edges)
+    self.assertListEqual(range(7), sorted(self._NodesIndices(g)))
+    self.assertEqual(5, len(g.Edges()))
+
+  def testUpdateEdge(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    edge = edges[1]
+    self.assertTrue(edge in g.OutEdges(nodes[0]))
+    self.assertTrue(edge in g.InEdges(nodes[2]))
+    g.UpdateEdge(edge, nodes[2], nodes[3])
+    self.assertFalse(edge in g.OutEdges(nodes[0]))
+    self.assertFalse(edge in g.InEdges(nodes[2]))
+    self.assertTrue(edge in g.OutEdges(nodes[2]))
+    self.assertTrue(edge in g.InEdges(nodes[3]))
+
+  def testTopologicalSort(self):
+    (_, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    sorted_nodes = g.TopologicalSort()
+    node_to_sorted_index = dict(zip(sorted_nodes, xrange(len(sorted_nodes))))
+    for e in edges:
+      self.assertTrue(
+          node_to_sorted_index[e.from_node] < node_to_sorted_index[e.to_node])
+
+  def testReachableNodes(self):
+    (nodes, _, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    self.assertSetEqual(
+        set([0, 1, 2, 3, 4]),
+        set(n.index for n in g.ReachableNodes([nodes[0]])))
+    self.assertSetEqual(
+        set([0, 1, 2, 3, 4]),
+        set(n.index for n in g.ReachableNodes([nodes[0], nodes[1]])))
+    self.assertSetEqual(
+        set([5, 6]),
+        set(n.index for n in g.ReachableNodes([nodes[5]])))
+    self.assertSetEqual(
+        set([6]),
+        set(n.index for n in g.ReachableNodes([nodes[6]])))
+
+  def testCost(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    for (i, node) in enumerate(nodes):
+      node.cost = i + 1
+    nodes[6].cost = 6
+    for edge in edges:
+      edge.cost = 1
+    self.assertEqual(15, g.Cost())
+    path_list = []
+    g.Cost(path_list=path_list)
+    self.assertListEqual([nodes[i] for i in (0, 1, 3, 4)], path_list)
+    nodes[6].cost = 9
+    self.assertEqual(16, g.Cost())
+    g.Cost(path_list=path_list)
+    self.assertListEqual([nodes[i] for i in (5, 6)], path_list)
+
+  def testCostWithRoots(self):
+    (nodes, edges, g) = self.MakeGraph(
+        7,
+        [(0, 1),
+         (0, 2),
+         (1, 3),
+         (3, 4),
+         (5, 6)])
+    for (i, node) in enumerate(nodes):
+      node.cost = i + 1
+    nodes[6].cost = 9
+    for edge in edges:
+      edge.cost = 1
+    path_list = []
+    self.assertEqual(16, g.Cost(path_list=path_list))
+    self.assertListEqual([nodes[i] for i in (5, 6)], path_list)
+    self.assertEqual(15, g.Cost(roots=[nodes[0]], path_list=path_list))
+    self.assertListEqual([nodes[i] for i in (0, 1, 3, 4)], path_list)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
index 76f7723..8a172bd 100644
--- a/loading/prefetch_view.py
+++ b/loading/prefetch_view.py
@@ -14,20 +14,24 @@ how many requests were prefetched.
 import itertools
 import operator
 
+import dependency_graph
 import loading_trace
+import user_satisfied_lens
 import request_dependencies_lens
+import request_track
 
 
 class PrefetchSimulationView(object):
   """Simulates the effect of prefetching resources discoverable by the preload
   scanner.
   """
-  def __init__(self, trace, dependencies_lens):
+  def __init__(self, trace, dependencies_lens, user_lens):
     """Initializes an instance of PrefetchSimulationView.
 
     Args:
       trace: (LoadingTrace) a loading trace.
       dependencies_lens: (RequestDependencyLens) request dependencies.
+      user_lens: (UserSatisfiedLens) Lens used to compute costs.
     """
     self.trace = trace
     self.dependencies_lens = dependencies_lens
@@ -35,6 +39,13 @@ class PrefetchSimulationView(object):
         categories=set([u'blink.net']))
     assert len(self._resource_events.GetEvents()) > 0,\
             'Was the "blink.net" category enabled at trace collection time?"'
+    self._user_lens = user_lens
+    request_ids = self._user_lens.CriticalRequests()
+    all_requests = self.trace.request_track.GetEvents()
+    self._first_request_node = all_requests[0].request_id
+    requests = [r for r in all_requests if r.request_id in request_ids]
+    self.graph = dependency_graph.RequestDependencyGraph(
+        requests, self.dependencies_lens)
 
   def ParserDiscoverableRequests(self, request, recurse=False):
     """Returns a list of requests discovered by the parser from a given request.
@@ -93,7 +104,7 @@ class PrefetchSimulationView(object):
          for r in preloaded_root_requests]))
 
 
-def _PrintSummary(prefetch_view):
+def _PrintSummary(prefetch_view, user_lens):
   requests = prefetch_view.trace.request_track.GetEvents()
   first_request = prefetch_view.trace.request_track.GetEvents()[0]
   parser_requests = prefetch_view.ExpandRedirectChains(
@@ -102,13 +113,22 @@ def _PrintSummary(prefetch_view):
       prefetch_view.PreloadedRequests(first_request))
   print '%d requests, %d parser from the main request, %d preloaded' % (
       len(requests), len(parser_requests), len(preloaded_requests))
+  print 'Time to user satisfaction: %.02fms' % (
+      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
+
+  print 'With 0-cost prefetched resources...'
+  new_costs = {r.request_id: 0. for r in preloaded_requests}
+  prefetch_view.graph.UpdateRequestsCost(new_costs)
+  print 'Time to user satisfaction: %.02fms' % (
+      prefetch_view.graph.Cost() + user_lens.PostloadTimeMsec())
 
 
 def main(filename):
   trace = loading_trace.LoadingTrace.FromJsonFile(filename)
   dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
-  prefetch_view = PrefetchSimulationView(trace, dependencies_lens)
-  _PrintSummary(prefetch_view)
+  user_lens = user_satisfied_lens.FirstContentfulPaintLens(trace)
+  prefetch_view = PrefetchSimulationView(trace, dependencies_lens, user_lens)
+  _PrintSummary(prefetch_view, user_lens)
 
 
 if __name__ == '__main__':
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
index f7210bc..ca8897d 100644
--- a/loading/prefetch_view_unittest.py
+++ b/loading/prefetch_view_unittest.py
@@ -7,6 +7,8 @@ import unittest
 import prefetch_view
 import request_dependencies_lens
 from request_dependencies_lens_unittest import TestRequests
+import request_track
+import test_utils
 
 
 class PrefetchSimulationViewTestCase(unittest.TestCase):
@@ -54,8 +56,9 @@ class PrefetchSimulationViewTestCase(unittest.TestCase):
     self.trace = TestRequests.CreateLoadingTrace(trace_events)
     dependencies_lens = request_dependencies_lens.RequestDependencyLens(
         self.trace)
+    self.user_satisfied_lens = test_utils.MockUserSatisfiedLens(self.trace)
     self.prefetch_view = prefetch_view.PrefetchSimulationView(
-        self.trace, dependencies_lens)
+        self.trace, dependencies_lens, self.user_satisfied_lens)
 
 
 if __name__ == '__main__':
diff --git a/loading/request_track.py b/loading/request_track.py
index bba015d..2c083c7 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -244,6 +244,14 @@ class Request(object):
       return int(age_match.group(1))
     return -1
 
+  def Cost(self):
+    """Returns the cost of this request in ms, defined as time between
+    request_time and the latest timing event.
+    """
+    # All fields in timing are millis relative to request_time.
+    return max([0] + [t for f, t in self.timing._asdict().iteritems()
+                      if f != 'request_time'])
+
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
 
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 38737c0..a221385 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -10,6 +10,7 @@ import loading_trace
 import page_track
 import request_track
 import tracing
+import user_satisfied_lens
 
 
 class FakeRequestTrack(devtools_monitor.Track):
@@ -207,3 +208,9 @@ class MockConnection(object):
       del self._expected_responses[method]
     self._test_case.assertEqual(expected_params, params)
     return response
+
+
+class MockUserSatisfiedLens(user_satisfied_lens._UserSatisfiedLens):
+  def _CalculateTimes(self, _):
+    self._satisfied_msec = float('inf')
+    self._event_msec = float('inf')

commit 849720169a3b57da2b02bc245cced9fec3d9cb6d
Author: droger <droger@chromium.org>
Date:   Fri Mar 25 07:44:10 2016 -0700

    tools/android/loading Run analyze.py on GCE
    
    Review URL: https://codereview.chromium.org/1831073002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383276}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c0164a8eff21b43ad24d8488424c0ad15d1211c3

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 522be17..453ca86 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -77,7 +77,8 @@ gcloud compute instances list
 
 ## Use the app
 
-Interact with the app on the port 8080 at `http://<instance-ip>:8080`.
+Check that `http://<instance-ip>:8080/test` prints `hello` when opened in a
+browser.
 
 To send a list of URLs to process:
 
@@ -116,13 +117,14 @@ source env/bin/activate
 pip install -r pip_requirements.txt
 ```
 
-Launch the app:
+Launch the app, passing the path to the Chrome executable on the host:
 
 ```shell
-gunicorn --workers=1 main:app --bind 127.0.0.1:8080
+gunicorn --workers=1 --bind 127.0.0.1:8080 \
+    'main:StartApp("/path/to/chrome")'
 ```
 
-In your browser, go to `http://localhost:8080` and use the app.
+You can now [use the app][2], which is located at http://localhost:8080.
 
 Tear down the local environment:
 
@@ -163,3 +165,4 @@ gcloud compute firewall-rules delete default-allow-http-8080
 ```
 
 [1]: https://cloud.google.com/sdk
+[2]: #Use-the-app
diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
index fad1755..8e937ed 100755
--- a/loading/gce/deploy.sh
+++ b/loading/gce/deploy.sh
@@ -24,12 +24,13 @@ cp -r tools/android/loading/gce $outdir/tools/android/loading
 # Copy other dependencies
 mkdir $outdir/third_party
 # Use rsync to exclude unwanted files (e.g. the .git directory).
-rsync -av --exclude=".*" --exclude "*.pyc" --delete \
-  third_party/catapult $outdir/third_party
+rsync -av --exclude=".*" --exclude "*.pyc" --exclude "*.html" --exclude "*.md" \
+  --delete third_party/catapult $outdir/third_party
 mkdir $outdir/tools/perf
 cp -r tools/perf/chrome_telemetry_build $outdir/tools/perf
 mkdir -p $outdir/build/android
 cp build/android/devil_chromium.py $outdir/build/android/
+cp build/android/video_recorder.py $outdir/build/android/
 cp build/android/devil_chromium.json $outdir/build/android/
 cp -r build/android/pylib $outdir/build/android/
 
@@ -38,7 +39,6 @@ chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
   /tmp/linux.zip
 gsutil cp /tmp/linux.zip gs://$bucket/chrome/linux.zip
 rm /tmp/linux.zip
-gsutil cp $builddir/chrome_sandbox gs://$bucket/chrome/chrome_sandbox
 
 # Upload Chromium revision
 CHROMIUM_REV=$(git merge-base HEAD origin/master)
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 2a0fd44..cddc903 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -5,6 +5,7 @@
 import json
 import re
 import threading
+import subprocess
 
 from gcloud import storage
 from gcloud.exceptions import NotFound
@@ -15,9 +16,13 @@ class ServerApp(object):
   Google Cloud Storage.
   """
 
-  def __init__(self):
+  def __init__(self, chrome_path):
+    """The chrome_path argument is the path to the Chrome executable as a
+    string.
+    """
     self._tasks = []
     self._thread = None
+    self._chrome_path = chrome_path
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
     print 'Reading server configuration'
@@ -31,11 +36,21 @@ class ServerApp(object):
   def _GetStorageBucket(self, storage_client):
     return storage_client.get_bucket(self._config['bucket_name'])
 
-  def _UploadFile(self, file_stream, filename):
+  def _UploadFile(self, filename_src, filename_dest):
+    """Uploads a file to Google Cloud Storage
+
+    Args:
+      filename_src: name of the local file
+      filename_dest: name of the file in Google Cloud Storage
+
+    Returns:
+      The URL of the new file in Google Cloud Storage.
+    """
     client = self._GetStorageClient()
     bucket = self._GetStorageBucket(client)
-    blob = bucket.blob(filename)
-    blob.upload_from_string(file_stream)
+    blob = bucket.blob(filename_dest)
+    with open(filename_src) as file_src:
+      blob.upload_from_file(file_src)
     url = blob.public_url
     return url
 
@@ -62,23 +77,36 @@ class ServerApp(object):
     self._tasks = json.loads(task_list)
     return len(self._tasks) != 0
 
+  def _GenerateTrace(self, url, filename):
+    """ Generates a trace using analyze.py
+
+    Args:
+      url: url as a string.
+      filename: name of the file where the output is saved.
+
+    Returns:
+      True if the trace was generated successfully.
+    """
+    ret = subprocess.call(
+        ['python', '../analyze.py', 'log_requests', '--clear_cache', '--local',
+         '--headless', '--local_binary', self._chrome_path, '--url', url,
+         '--output', filename])
+    return ret == 0
+
   def _ProcessTasks(self):
     # Avoid special characters in storage object names
     pattern = re.compile(r"[#\?\[\]\*/]")
     while len(self._tasks) > 0:
       url = self._tasks.pop()
       filename = pattern.sub('_', url)
-      # TODO: compute the actual trace for url.
-      trace = '{}'
-      self._UploadFile(trace, filename)
+      if self._GenerateTrace(url, filename):
+        self._UploadFile(filename, 'traces/' + filename)
+      else:
+        # TODO(droger): Upload the list of urls that failed.
+        print 'analyze.py failed'
 
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
-    if path == '/favicon.ico':
-        start_response('404 NOT FOUND', [('Content-Length', '0')])
-        return iter([''])
-
-    status = '200 OK'
 
     if path == '/set_tasks':
       # Get the tasks from the HTTP body.
@@ -92,21 +120,27 @@ class ServerApp(object):
       else:
         data = 'Something went wrong'
     elif path == '/start':
-      if len(self._tasks) > 0:
-        data = 'Starting...'
+      if len(self._tasks) == 0 :
+        data = 'Nothing to do!'
+      elif self._thread is not None and self._thread.is_alive():
+        data = 'Already running!'
+      else:
+        data = 'Starting generation of ' + str(len(self._tasks)) + 'tasks'
         self._thread = threading.Thread(target = self._ProcessTasks)
         self._thread.start()
-      else:
-        data = 'Nothing to do!'
+    elif path == '/test':
+      data = 'hello'
     else:
-      data = environ['PATH_INFO'] + '\n'
+      start_response('404 NOT FOUND', [('Content-Length', '0')])
+      return iter([''])
 
     response_headers = [
         ('Content-type','text/plain'),
         ('Content-Length', str(len(data)))
     ]
-    start_response(status, response_headers)
+    start_response('200 OK', response_headers)
     return iter([data])
 
 
-app = ServerApp()
+def StartApp(chrome_path):
+  return ServerApp(chrome_path)
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index dcad7f2..bb1f94a 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -74,8 +74,12 @@ AUTO_START=$(curl -s \
     "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
     -H "Metadata-Flavor: Google")
 
+# TODO(droger): Figure out how to correctly restore check for auto-startup
+# as well as auto-startup code.
+exit 1
+
 # Exit early if auto start is not enabled.
-if [-z "$AUTO_START"]; then
+if [ -z "$AUTO_START" ]; then
   exit 1
 fi
 
@@ -84,8 +88,8 @@ fi
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
 directory=/opt/app/clovis/tools/android/loading/gce
-command=/opt/app/clovis/env/bin/gunicorn --workers=1 main:app \
-  --bind 0.0.0.0:8080
+command=/opt/app/clovis/env/bin/gunicorn --workers=1 --bind 0.0.0.0:8080 \
+    'main:StartApp("/opt/app/clovis/out/chrome")'
 autostart=true
 autorestart=true
 user=pythonapp

commit ea403492aa471332438daa1041cf18191c6053c8
Author: mattcary <mattcary@chromium.org>
Date:   Thu Mar 24 06:23:41 2016 -0700

    Clovis: update documentation links. goo.gl short links can't be changed :(
    
    Review URL: https://codereview.chromium.org/1827033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#383056}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f259d70d9f1a7866c9a6c0d4e1192ea7bb9c6e61

diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 08d5d14..90ced38 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -95,7 +95,7 @@ class GraphSack(object):
     appear with frequency at least CORE_THRESHOLD. For a collection of graph
     sets, for instance pulling the same page under different network
     connections, we intersect the core sets to produce a page core set that
-    describes the key resources used by the page. See https://goo.gl/F1BoEB for
+    describes the key resources used by the page. See https://goo.gl/LmqQRS for
     context and discussion.
 
     Args:
@@ -117,7 +117,7 @@ class GraphSack(object):
   def CoreSimilarity(cls, a, b):
     """Compute the similarity of two core sets.
 
-    We use the Jaccard index. See https://goo.gl/F1BoEB for discussion.
+    We use the Jaccard index. See https://goo.gl/LmqQRS for discussion.
 
     Args:
       a: The first core set, as a set of strings.

commit 73a6767a3ad69c7ae2c48d2fab42390e87f9cbe7
Author: agrieve <agrieve@chromium.org>
Date:   Wed Mar 23 12:54:45 2016 -0700

    Replace usages of DEPRECATED_java_in_dir with java_files
    
    BUG=484854
    
    Review URL: https://codereview.chromium.org/1829823002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382905}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 696e6a571b4db1a966ac7b49260eaa47a1ba9bd4

diff --git a/customtabs_benchmark/BUILD.gn b/customtabs_benchmark/BUILD.gn
index 41fb5e2..e1bd05e 100644
--- a/customtabs_benchmark/BUILD.gn
+++ b/customtabs_benchmark/BUILD.gn
@@ -5,7 +5,7 @@
 import("//build/config/android/rules.gni")
 
 android_apk("customtabs_benchmark_apk") {
-  DEPRECATED_java_in_dir = "java/src"
+  java_files = [ "java/src/org/chromium/customtabs/test/MainActivity.java" ]
   android_manifest = "java/AndroidManifest.xml"
   apk_name = "CustomTabsBenchmark"
   deps = [
diff --git a/memconsumer/BUILD.gn b/memconsumer/BUILD.gn
index af7e045..90b7fcf 100644
--- a/memconsumer/BUILD.gn
+++ b/memconsumer/BUILD.gn
@@ -12,7 +12,10 @@ android_resources("memconsumer_apk_resources") {
 android_apk("memconsumer_apk") {
   apk_name = "MemConsumer"
   android_manifest = "java/AndroidManifest.xml"
-  DEPRECATED_java_in_dir = "java/src"
+  java_files = [
+    "java/src/org/chromium/memconsumer/MemConsumer.java",
+    "java/src/org/chromium/memconsumer/ResidentService.java",
+  ]
   native_libs = [ "libmemconsumer.so" ]
 
   deps = [

commit b925141b88c6170fe1dab96a7783518737f0275c
Author: gabadie <gabadie@chromium.org>
Date:   Wed Mar 23 11:05:47 2016 -0700

    tools/android/loading: Remove old chrome API
    
    Currently, all the code has been migrated to the
    ChromeController API. This CL remove the no longer
    used legacy API, becoming dead code.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1825403002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382880}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a9415b6fd69350e6410726008c0843ee690baade

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
deleted file mode 100644
index 245246d..0000000
--- a/loading/chrome_setup.py
+++ /dev/null
@@ -1,187 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Handles Chrome's configuration. DEPRECATED!"""
-
-import contextlib
-import json
-import shutil
-import subprocess
-import tempfile
-import time
-
-import devtools_monitor
-from options import OPTIONS
-
-
-# Copied from
-# WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
-# Units:
-#   download/upload: byte/s
-#   latency: ms
-NETWORK_CONDITIONS = {
-    'GPRS': {
-        'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
-    'Regular 2G': {
-        'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
-    'Good 2G': {
-        'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
-    'Regular 3G': {
-        'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
-    'Good 3G': {
-        'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
-        'latency': 40},
-    'Regular 4G': {
-        'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
-        'latency': 20},
-    'DSL': {
-        'download': 2 * 1024 * 1024 / 8, 'upload': 1 * 1024 * 1024 / 8,
-        'latency': 5},
-    'WiFi': {
-        'download': 30 * 1024 * 1024 / 8, 'upload': 15 * 1024 * 1024 / 8,
-        'latency': 2}
-}
-
-
-def BandwidthToString(bandwidth):
-  """Converts a bandwidth to string.
-
-  Args:
-    bandwidth: The bandwidth to convert in byte/s. Must be a multiple of 1024/8.
-
-  Returns:
-    A string compatible with wpr --{up,down} command line flags.
-  """
-  assert type(bandwidth) == int
-  assert bandwidth % (1024/8) == 0
-  bandwidth_kbps = (bandwidth * 8) / 1024
-  if bandwidth_kbps % 1024:
-    return '{}Kbit/s'.format(bandwidth_kbps)
-  return '{}Mbit/s'.format(bandwidth_kbps / 1024)
-
-
-@contextlib.contextmanager
-def DevToolsConnectionForLocalBinary(flags):
-  """Returns a DevToolsConnection context manager for a local binary.
-
-  Args:
-    flags: ([str]) List of flags to pass to the browser.
-
-  Returns:
-    A DevToolsConnection context manager.
-  """
-  binary_filename = OPTIONS.local_binary
-  profile_dir = OPTIONS.local_profile_dir
-  using_temp_profile_dir = profile_dir is None
-  if using_temp_profile_dir:
-    profile_dir = tempfile.mkdtemp()
-  flags.append('--user-data-dir=%s' % profile_dir)
-  chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-  process = subprocess.Popen(
-      [binary_filename] + flags, shell=False, stderr=chrome_out)
-  try:
-    time.sleep(10)
-    yield devtools_monitor.DevToolsConnection(
-        OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-  finally:
-    process.kill()
-    if using_temp_profile_dir:
-      shutil.rmtree(profile_dir)
-
-
-def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
-                                    emulated_network_name):
-  """Sets up the device and network emulation and returns the trace metadata.
-
-  Args:
-    connection: (DevToolsConnection)
-    emulated_device_name: (str) Key in the dict returned by
-                          _LoadEmulatedDevices().
-    emulated_network_name: (str) Key in NETWORK_CONDITIONS.
-
-  Returns:
-    A metadata dict {'deviceEmulation': params, 'networkEmulation': params}.
-  """
-  result = {'deviceEmulation': {}, 'networkEmulation': {}}
-  if emulated_device_name:
-    devices = _LoadEmulatedDevices(OPTIONS.devices_file)
-    emulated_device = devices[emulated_device_name]
-    emulation_params = _SetUpDeviceEmulationAndReturnMetadata(
-        connection, emulated_device)
-    result['deviceEmulation'] = emulation_params
-  if emulated_network_name:
-    params = NETWORK_CONDITIONS[emulated_network_name]
-    _SetUpNetworkEmulation(
-        connection, params['latency'], params['download'], params['upload'])
-    result['networkEmulation'] = params
-  return result
-
-
-def _LoadEmulatedDevices(filename):
-  """Loads a list of emulated devices from the DevTools JSON registry.
-
-  Args:
-    filename: (str) Path to the JSON file.
-
-  Returns:
-    {'device_name': device}
-  """
-  json_dict = json.load(open(filename, 'r'))
-  devices = {}
-  for device in json_dict['extensions']:
-    device = device['device']
-    devices[device['title']] = device
-  return devices
-
-
-def _GetDeviceEmulationMetadata(device):
-  """Returns the metadata associated with a given device."""
-  return {'width': device['screen']['vertical']['width'],
-          'height': device['screen']['vertical']['height'],
-          'deviceScaleFactor': device['screen']['device-pixel-ratio'],
-          'mobile': 'mobile' in device['capabilities'],
-          'userAgent': device['user-agent']}
-
-
-def _SetUpDeviceEmulationAndReturnMetadata(connection, device):
-  """Configures an instance of Chrome for device emulation.
-
-  Args:
-    connection: (DevToolsConnection)
-    device: (dict) As returned by LoadEmulatedDevices().
-
-  Returns:
-    A dict containing the device emulation metadata.
-  """
-  print device
-  res = connection.SyncRequest('Emulation.canEmulate')
-  assert res['result'], 'Cannot set device emulation.'
-  data = _GetDeviceEmulationMetadata(device)
-  connection.SyncRequestNoResponse(
-      'Emulation.setDeviceMetricsOverride',
-      {'width': data['width'],
-       'height': data['height'],
-       'deviceScaleFactor': data['deviceScaleFactor'],
-       'mobile': data['mobile'],
-       'fitWindow': True})
-  connection.SyncRequestNoResponse('Network.setUserAgentOverride',
-                                   {'userAgent': data['userAgent']})
-  return data
-
-
-def _SetUpNetworkEmulation(connection, latency, download, upload):
-  """Configures an instance of Chrome for network emulation.
-
-  Args:
-    connection: (DevToolsConnection)
-    latency: (float) Latency in ms.
-    download: (float) Download speed (Bytes / s).
-    upload: (float) Upload speed (Bytes / s).
-  """
-  res = connection.SyncRequest('Network.canEmulateNetworkConditions')
-  assert res['result'], 'Cannot set network emulation.'
-  connection.SyncRequestNoResponse(
-      'Network.emulateNetworkConditions',
-      {'offline': False, 'latency': latency, 'downloadThroughput': download,
-       'uploadThroughput': upload})
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 787cd79..d2c466d 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -35,8 +35,8 @@ sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
 import adb_install_cert
 import certutils
 
-import chrome_setup
 import devtools_monitor
+import emulation
 import options
 
 
@@ -127,13 +127,6 @@ def ForwardPort(device, local, remote):
     device.adb.ForwardRemove(local)
 
 
-# Deprecated
-def _SetUpDevice(device, package_info):
-  """Enables root and closes Chrome on a device."""
-  device.EnableRoot()
-  device.KillAll(package_info.package, quiet=True)
-
-
 @contextlib.contextmanager
 def _WprHost(wpr_archive_path, record=False,
              network_condition_name=None,
@@ -148,13 +141,13 @@ def _WprHost(wpr_archive_path, record=False,
   else:
     assert os.path.exists(wpr_archive_path)
   if network_condition_name:
-    condition = chrome_setup.NETWORK_CONDITIONS[network_condition_name]
+    condition = emulation.NETWORK_CONDITIONS[network_condition_name]
     if record:
       logging.warning('WPR network condition is ignored when recording.')
     else:
       wpr_server_args.extend([
-          '--down', chrome_setup.BandwidthToString(condition['download']),
-          '--up', chrome_setup.BandwidthToString(condition['upload']),
+          '--down', emulation.BandwidthToString(condition['download']),
+          '--up', emulation.BandwidthToString(condition['upload']),
           '--delay_ms', str(condition['latency']),
           '--shaping_type', 'proxy'])
 
@@ -207,7 +200,7 @@ def LocalWprHost(wpr_archive_path, record=False,
     wpr_archive_path: host sided WPR archive's path.
     record: Enables or disables WPR archive recording.
     network_condition_name: Network condition name available in
-        chrome_setup.NETWORK_CONDITIONS.
+        emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
 
@@ -243,7 +236,7 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
     wpr_archive_path: host sided WPR archive's path.
     record: Enables or disables WPR archive recording.
     network_condition_name: Network condition name available in
-        chrome_setup.NETWORK_CONDITIONS.
+        emulation.NETWORK_CONDITIONS.
     disable_script_injection: Disable JavaScript file injections that is
       fighting against resources name entropy.
 
@@ -374,57 +367,3 @@ def RemoteSpeedIndexRecorder(device, connection, local_output_path):
       })();
     """)
     yield
-
-
-@contextlib.contextmanager
-def _DevToolsConnectionOnDevice(device, flags):
-  """Returns a DevToolsConnection context manager for a given device.
-
-  Args:
-    device: Device to connect to.
-    flags: ([str]) List of flags.
-
-  Returns:
-    A DevToolsConnection context manager.
-  """
-  package_info = OPTIONS.ChromePackage()
-  command_line_path = '/data/local/chrome-command-line'
-  _SetUpDevice(device, package_info)
-  with FlagReplacer(device, command_line_path, flags):
-    start_intent = intent.Intent(
-        package=package_info.package, activity=package_info.activity,
-        data='about:blank')
-    device.StartActivity(start_intent, blocking=True)
-    time.sleep(2)
-    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
-                     'localabstract:chrome_devtools_remote'):
-      yield devtools_monitor.DevToolsConnection(
-          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-
-
-# Deprecated, use *Controller.
-def DeviceConnection(device, additional_flags=None):
-  """Context for starting recording on a device.
-
-  Sets up and restores any device and tracing appropriately
-
-  Args:
-    device: Android device, or None for a local run (in which case chrome needs
-      to have been started with --remote-debugging-port=XXX).
-    additional_flags: Additional chromium arguments.
-
-  Returns:
-    A context manager type which evaluates to a DevToolsConnection.
-  """
-  new_flags = ['--disable-fre',
-               '--enable-test-events',
-               '--remote-debugging-port=%d' % OPTIONS.devtools_port]
-  if OPTIONS.no_sandbox:
-    new_flags.append('--no-sandbox')
-  if additional_flags != None:
-    new_flags.extend(additional_flags)
-
-  if device:
-    return _DevToolsConnectionOnDevice(device, new_flags)
-  else:
-    return chrome_setup.DevToolsConnectionForLocalBinary(new_flags)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
deleted file mode 100755
index 7c9e822..0000000
--- a/loading/trace_recorder.py
+++ /dev/null
@@ -1,85 +0,0 @@
-#! /usr/bin/python
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Loading trace recorder. DEPRECATED!"""
-
-import argparse
-import datetime
-import json
-import logging
-import os
-import sys
-import time
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-import devil_chromium
-
-import device_setup
-import devtools_monitor
-import loading_trace
-import page_track
-import request_track
-import tracing
-
-
-def MonitorUrl(connection, url, clear_cache=False,
-               categories=tracing.DEFAULT_CATEGORIES,
-               timeout=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
-  """Monitor a URL via a trace recorder.
-
-  DEPRECATED! Use LoadingTrace.FromUrlAndController instead.
-
-  Args:
-    connection: A devtools_monitor.DevToolsConnection instance.
-    url: url to navigate to as string.
-    clear_cache: boolean indicating if cache should be cleared before loading.
-    categories: List of tracing event categories to record.
-    timeout: Websocket timeout.
-
-  Returns:
-    loading_trace.LoadingTrace.
-  """
-  page = page_track.PageTrack(connection)
-  request = request_track.RequestTrack(connection)
-  trace = tracing.TracingTrack(connection, categories=categories)
-  connection.SetUpMonitoring()
-  if clear_cache:
-    connection.ClearCache()
-  connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-  connection.StartMonitoring(timeout=timeout)
-  metadata = {'date': datetime.datetime.utcnow().isoformat(),
-              'seconds_since_epoch': time.time()}
-  return loading_trace.LoadingTrace(url, metadata, page, request, trace)
-
-def RecordAndDumpTrace(device, url, output_filename):
-  with file(output_filename, 'w') as output,\
-        device_setup.DeviceConnection(device) as connection:
-    trace = MonitorUrl(connection, url)
-    json.dump(trace.ToJsonDict(), output)
-
-
-def main():
-  logging.basicConfig(level=logging.INFO)
-  devil_chromium.Initialize()
-
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--url', required=True)
-  parser.add_argument('--output', required=True)
-  args = parser.parse_args()
-  url = args.url
-  if not url.startswith('http'):
-    url = 'http://' + url
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  RecordAndDumpTrace(device, url, args.output)
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index aad36c1..1db5da7 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -44,7 +44,7 @@ sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 import controller
 import loading_trace
 import options
-import trace_recorder
+
 
 OPTIONS = options.OPTIONS
 WEBSERVER = os.path.join(os.path.dirname(__file__), 'test_server.py')

commit 17a094776de09c85980abfcbcb68e3e4730ec035
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 09:40:42 2016 -0700

    Clovis: remove occurrence counting in ResourceSack.
    
    Now that we've settled on CoreSet this is unused.
    
    Review URL: https://codereview.chromium.org/1830523004
    
    Cr-Original-Commit-Position: refs/heads/master@{#382865}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fdd7f55e4218083d927e8de55fb1621c64ab1641

diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 9c174c5..08d5d14 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -131,22 +131,6 @@ class GraphSack(object):
       return 0
     return float(len(a & b)) / len(a | b)
 
-  def FilterOccurrence(self, tag, filter_from_graph):
-    """Accumulate filter occurrences for each bag in the graph.
-
-    This can be retrieved under tag for each Bag in the graph. For example, if
-    FilterContentful marks the nodes of each graph before the first contentful
-    paint, then FilterOccurrence('contentful', FilterContentful) will count, for
-    each bag, the fraction of nodes that were before the first contentful paint.
-
-    Args:
-      tag: the tag to count the filter appearances under.
-      filter_from_graph: a function graph -> node filter, where node filter
-        takes a node to a boolean.
-    """
-    for bag in self.bags:
-      bag.MarkOccurrence(tag, filter_from_graph)
-
   @property
   def num_graphs(self):
     return len(self.graph_info)
@@ -191,12 +175,6 @@ class Bag(dag.Node):
     self._relative_costs = []
     self._num_critical = 0
 
-    # See MarkOccurrence and GetOccurrence, below. This maps an occurrence
-    # tag to a list of nodes matching the occurrence.
-    self._occurence_matches = {}
-    # Number of nodes seen for each occurrence.
-    self._occurence_count = {}
-
   @property
   def url(self):
     return self._url
@@ -254,37 +232,6 @@ class Bag(dag.Node):
       self._successor_sources[successor_bag].add((graph, node, s))
       self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
 
-  def MarkOccurrence(self, tag, filter_from_graph):
-    """Mark occurrences for nodes in this bag according to graph_filters.
-
-    Results can be querried by GetOccurrence().
-
-    Args:
-      tag: a label for this set of occurrences.
-      filter_from_graph: a function graph -> node filter, where node filter
-        takes a node to a boolean.
-    """
-    self._occurence_matches[tag] = 0
-    self._occurence_count[tag] = 0
-    for graph, nodes in self.graphs.iteritems():
-      for n in nodes:
-        self._occurence_count[tag] += 1
-        if filter_from_graph(graph)(n):
-          self._occurence_matches[tag] += 1
-
-  def GetOccurrence(self, tag):
-    """Retrieve the occurrence fraction of a tag.
-
-    Args:
-      tag: the tag under which the occurrence was counted. This must have been
-        previously added at least once via AddOccurrence.
-
-    Returns:
-      A fraction occurrence matches / occurrence node count.
-    """
-    assert self._occurence_count[tag] > 0
-    return float(self._occurence_matches[tag]) / self._occurence_count[tag]
-
   @classmethod
   def _MakeShortname(cls, url):
     parsed = urlparse.urlparse(url)
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index df0fdb7..a4b1a30 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -66,34 +66,6 @@ class ResourceSackTestCase(unittest.TestCase):
     self.assertEqual(set(['0/', 'data:fake/content']),
                      set([bag.label for bag in sack.bags]))
 
-  def test_Occurrence(self):
-    # There are two graph shapes. The first one is added to the sack three
-    # times, and the second once. The second graph has one sibling that doesn't
-    # appear in the first as well as a new child.
-    shape1 = [MakeRequest(0, 'null'), MakeRequest(1, 0), MakeRequest(2, 0)]
-    shape2 = [MakeRequest(0, 'null'), MakeRequest(1, 0),
-              MakeRequest(3, 0), MakeRequest(4, 1)]
-    graphs = [TestResourceGraph.FromRequestList(s)
-              for s in (shape1, shape1,  shape1, shape2)]
-    sack = resource_sack.GraphSack()
-    for g in graphs:
-      sack.ConsumeGraph(g)
-    # Map a graph to a list of nodes that are in its filter.
-    filter_sets = {
-        graphs[0]: set([0, 1, 2]),
-        graphs[1]: set([0, 1, 2]),
-        graphs[2]: set([0, 1]),
-        graphs[3]: set([0, 3])}
-    sack.FilterOccurrence(
-        'test', lambda graph: lambda node:
-            int(node.ShortName()) in filter_sets[graph])
-    labels = {bag.label: bag for bag in sack.bags}
-    self.assertAlmostEqual(1, labels['0/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(0.75, labels['1/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(0.667, labels['2/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(1, labels['3/'].GetOccurrence('test'), 3)
-    self.assertAlmostEqual(0, labels['4/'].GetOccurrence('test'), 3)
-
   def test_Core(self):
     # We will use a core threshold of 0.5 to make it easier to define
     # graphs. Resources 0 and 1 are core and others are not.

commit ffd6a1ef1176bd1ad69104ea861339af47a1e7e8
Author: gabadie <gabadie@chromium.org>
Date:   Wed Mar 23 09:29:17 2016 -0700

    sandwich: Slice up sandwich.py into sandwich_{runner,misc}.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1822163002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382860}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 75fe2ef04b5ad5a3e91e34e20d236a7963da74bb

diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
deleted file mode 100644
index e30232c..0000000
--- a/loading/pull_sandwich_metrics.py
+++ /dev/null
@@ -1,258 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Pull a sandwich run's output directory's metrics from traces into a CSV.
-
-python pull_sandwich_metrics.py -h
-"""
-
-import collections
-import logging
-import os
-import shutil
-import sys
-import tempfile
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
-from chrome_telemetry_build import chromium_config
-
-sys.path.append(chromium_config.GetTelemetryDir())
-from telemetry.internal.image_processing import video
-from telemetry.util import image_util
-from telemetry.util import rgba_color
-
-import loading_trace as loading_trace_module
-import tracing
-
-
-CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
-
-CSV_FIELD_NAMES = [
-    'id',
-    'url',
-    'total_load',
-    'onload',
-    'browser_malloc_avg',
-    'browser_malloc_max',
-    'speed_index']
-
-_TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
-
-# Points of a completeness record.
-#
-# Members:
-#   |time| is in milliseconds,
-#   |frame_completeness| value representing how complete the frame is at a given
-#     |time|. Caution: this completeness might be negative.
-CompletenessPoint = collections.namedtuple('CompletenessPoint',
-    ('time', 'frame_completeness'))
-
-
-def _GetBrowserPID(tracing_track):
-  """Get the browser PID from a trace.
-
-  Args:
-    tracing_track: The tracing.TracingTrack.
-
-  Returns:
-    The browser's PID as an integer.
-  """
-  for event in tracing_track.GetEvents():
-    if event.category != '__metadata' or event.name != 'process_name':
-      continue
-    if event.args['name'] == 'Browser':
-      return event.pid
-  raise ValueError('couldn\'t find browser\'s PID')
-
-
-def _GetBrowserDumpEvents(tracing_track):
-  """Get the browser memory dump events from a tracing track.
-
-  Args:
-    tracing_track: The tracing.TracingTrack.
-
-  Returns:
-    List of memory dump events.
-  """
-  browser_pid = _GetBrowserPID(tracing_track)
-  browser_dumps_events = []
-  for event in tracing_track.GetEvents():
-    if event.category != 'disabled-by-default-memory-infra':
-      continue
-    if event.type != 'v' or event.name != 'periodic_interval':
-      continue
-    # Ignore dump events for processes other than the browser process
-    if event.pid != browser_pid:
-      continue
-    browser_dumps_events.append(event)
-  if len(browser_dumps_events) == 0:
-    raise ValueError('No browser dump events found.')
-  return browser_dumps_events
-
-
-def _GetWebPageTrackedEvents(tracing_track):
-  """Get the web page's tracked events from a tracing track.
-
-  Args:
-    tracing_track: The tracing.TracingTrack.
-
-  Returns:
-    Dictionary all tracked events.
-  """
-  main_frame = None
-  tracked_events = {}
-  for event in tracing_track.GetEvents():
-    if event.category != 'blink.user_timing':
-      continue
-    event_name = event.name
-    # Ignore events until about:blank's unloadEventEnd that give the main
-    # frame id.
-    if not main_frame:
-      if event_name == 'unloadEventEnd':
-        main_frame = event.args['frame']
-        logging.info('found about:blank\'s event \'unloadEventEnd\'')
-      continue
-    # Ignore sub-frames events. requestStart don't have the frame set but it
-    # is fine since tracking the first one after about:blank's unloadEventEnd.
-    if 'frame' in event.args and event.args['frame'] != main_frame:
-      continue
-    if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
-      logging.info('found url\'s event \'%s\'' % event_name)
-      tracked_events[event_name] = event
-  assert len(tracked_events) == len(_TRACKED_EVENT_NAMES)
-  return tracked_events
-
-
-def _PullMetricsFromLoadingTrace(loading_trace):
-  """Pulls all the metrics from a given trace.
-
-  Args:
-    loading_trace: loading_trace_module.LoadingTrace.
-
-  Returns:
-    Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
-  """
-  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
-  web_page_tracked_events = _GetWebPageTrackedEvents(
-      loading_trace.tracing_track)
-
-  browser_malloc_sum = 0
-  browser_malloc_max = 0
-  for dump_event in browser_dump_events:
-    attr = dump_event.args['dumps']['allocators']['malloc']['attrs']['size']
-    assert attr['units'] == 'bytes'
-    size = int(attr['value'], 16)
-    browser_malloc_sum += size
-    browser_malloc_max = max(browser_malloc_max, size)
-
-  return {
-    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
-                   web_page_tracked_events['requestStart'].start_msec),
-    'onload': (web_page_tracked_events['loadEventEnd'].start_msec -
-               web_page_tracked_events['loadEventStart'].start_msec),
-    'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
-    'browser_malloc_max': browser_malloc_max
-  }
-
-
-def _ExtractCompletenessRecordFromVideo(video_path):
-  """Extracts the completeness record from a video.
-
-  The video must start with a filled rectangle of orange (RGB: 222, 100, 13), to
-  give the view-port size/location from where to compute the completeness.
-
-  Args:
-    video_path: Path of the video to extract the completeness list from.
-
-  Returns:
-    list(CompletenessPoint)
-  """
-  video_file = tempfile.NamedTemporaryFile()
-  shutil.copy(video_path, video_file.name)
-  video_capture = video.Video(video_file)
-
-  histograms = [
-      (time, image_util.GetColorHistogram(
-          image, ignore_color=rgba_color.WHITE, tolerance=8))
-      for time, image in video_capture.GetVideoFrameIter()
-  ]
-
-  start_histogram = histograms[1][1]
-  final_histogram = histograms[-1][1]
-  total_distance = start_histogram.Distance(final_histogram)
-
-  def FrameProgress(histogram):
-    if total_distance == 0:
-      if histogram.Distance(final_histogram) == 0:
-        return 1.0
-      else:
-        return 0.0
-    return 1 - histogram.Distance(final_histogram) / total_distance
-
-  return [(time, FrameProgress(hist)) for time, hist in histograms]
-
-
-def ComputeSpeedIndex(completeness_record):
-  """Computes the speed-index from a completeness record.
-
-  Args:
-    completeness_record: list(CompletenessPoint)
-
-  Returns:
-    Speed-index value.
-  """
-  speed_index = 0.0
-  last_time = completeness_record[0][0]
-  last_completness = completeness_record[0][1]
-  for time, completeness in completeness_record:
-    if time < last_time:
-      raise ValueError('Completeness record must be sorted by timestamps.')
-    elapsed = time - last_time
-    speed_index += elapsed * (1.0 - last_completness)
-    last_time = time
-    last_completness = completeness
-  return speed_index
-
-
-def PullMetricsFromOutputDirectory(output_directory_path):
-  """Pulls all the metrics from all the traces of a sandwich run directory.
-
-  Args:
-    output_directory_path: The sandwich run's output directory to pull the
-        metrics from.
-
-  Returns:
-    List of dictionaries with all CSV_FIELD_NAMES's field set.
-  """
-  assert os.path.isdir(output_directory_path)
-  metrics = []
-  for node_name in os.listdir(output_directory_path):
-    if not os.path.isdir(os.path.join(output_directory_path, node_name)):
-      continue
-    try:
-      page_id = int(node_name)
-    except ValueError:
-      continue
-    run_path = os.path.join(output_directory_path, node_name)
-    trace_path = os.path.join(run_path, 'trace.json')
-    if not os.path.isfile(trace_path):
-      continue
-    logging.info('processing \'%s\'' % trace_path)
-    loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
-    row_metrics = {key: 'unavailable' for key in CSV_FIELD_NAMES}
-    row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
-    row_metrics['id'] = page_id
-    row_metrics['url'] = loading_trace.url
-    video_path = os.path.join(run_path, 'video.mp4')
-    if os.path.isfile(video_path):
-      logging.info('processing \'%s\'' % video_path)
-      completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
-      row_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
-    metrics.append(row_metrics)
-  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
-                            'run directory.').format(output_directory_path)
-  return metrics
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
deleted file mode 100644
index 31519af..0000000
--- a/loading/pull_sandwich_metrics_unittest.py
+++ /dev/null
@@ -1,245 +0,0 @@
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-import copy
-import json
-import os
-import shutil
-import subprocess
-import tempfile
-import unittest
-
-import loading_trace
-import page_track
-import pull_sandwich_metrics as puller
-import request_track
-import tracing
-
-
-_BLINK_CAT = 'blink.user_timing'
-_MEM_CAT = 'disabled-by-default-memory-infra'
-_START='requestStart'
-_LOADS='loadEventStart'
-_LOADE='loadEventEnd'
-_UNLOAD='unloadEventEnd'
-
-_MINIMALIST_TRACE_EVENTS = [
-    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10000,
-        'args': {'frame': '0'}},
-    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _START,  'ts': 20000,
-        'args': {}},
-    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
-        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
-            'units': 'bytes', 'value': '1af2', }}}}}}},
-    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35000,
-        'args': {'frame': '0'}},
-    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40000,
-        'args': {'frame': '0'}},
-    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
-        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
-            'units': 'bytes', 'value': 'd704', }}}}}}},
-    {'ph': 'M', 'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'ts': 1,
-        'args': {'name': 'Browser'}}]
-
-
-def TracingTrack(events):
-  return tracing.TracingTrack.FromJsonDict({'events': events})
-
-
-def LoadingTrace(events):
-  return loading_trace.LoadingTrace('http://a.com/', {},
-                                    page_track.PageTrack(None),
-                                    request_track.RequestTrack(None),
-                                    TracingTrack(events))
-
-
-class PageTrackTest(unittest.TestCase):
-  def testGetBrowserPID(self):
-    def RunHelper(expected, events):
-      self.assertEquals(expected, puller._GetBrowserPID(TracingTrack(events)))
-
-    RunHelper(123, [
-        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
-        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'},
-        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
-            'name': 'thread_name'},
-        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
-            'name': 'process_name', 'args': {'name': 'Renderer'}},
-        {'ph': 'M', 'ts': 0, 'pid': 123, 'cat': '__metadata',
-            'name': 'process_name', 'args': {'name': 'Browser'}},
-        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'}])
-
-    with self.assertRaises(ValueError):
-      RunHelper(123, [
-          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
-          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'}])
-
-  def testGetBrowserDumpEvents(self):
-    NAME = 'periodic_interval'
-
-    def RunHelper(trace_events, browser_pid):
-      trace_events = copy.copy(trace_events)
-      trace_events.append({
-          'pid': browser_pid,
-          'cat': '__metadata',
-          'name': 'process_name',
-          'ph': 'M',
-          'ts': 0,
-          'args': {'name': 'Browser'}})
-      return puller._GetBrowserDumpEvents(TracingTrack(trace_events))
-
-    TRACE_EVENTS = [
-        {'pid': 354, 'ts':  1000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts':  2000, 'cat': _MEM_CAT, 'ph': 'V'},
-        {'pid': 672, 'ts':  3000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts':  4000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
-        {'pid': 123, 'ts':  5000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts':  6000, 'cat': _MEM_CAT, 'ph': 'V'},
-        {'pid': 672, 'ts':  7000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts':  8000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
-        {'pid': 123, 'ts':  9000, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts': 10000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
-        {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
-
-    self.assertTrue(_MEM_CAT in puller.CATEGORIES)
-
-    bump_events = RunHelper(TRACE_EVENTS, 123)
-    self.assertEquals(2, len(bump_events))
-    self.assertEquals(5, bump_events[0].start_msec)
-    self.assertEquals(10, bump_events[1].start_msec)
-
-    bump_events = RunHelper(TRACE_EVENTS, 354)
-    self.assertEquals(1, len(bump_events))
-    self.assertEquals(1, bump_events[0].start_msec)
-
-    bump_events = RunHelper(TRACE_EVENTS, 672)
-    self.assertEquals(3, len(bump_events))
-    self.assertEquals(3, bump_events[0].start_msec)
-    self.assertEquals(7, bump_events[1].start_msec)
-    self.assertEquals(12, bump_events[2].start_msec)
-
-    with self.assertRaises(ValueError):
-      RunHelper(TRACE_EVENTS, 895)
-
-  def testGetWebPageTrackedEvents(self):
-    self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
-
-    trace_events = puller._GetWebPageTrackedEvents(TracingTrack([
-        {'ph': 'R', 'ts':  0000, 'args': {},             'cat': 'whatever',
-            'name': _START},
-        {'ph': 'R', 'ts':  1000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _LOADS},
-        {'ph': 'R', 'ts':  2000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _LOADE},
-        {'ph': 'R', 'ts':  3000, 'args': {},             'cat': _BLINK_CAT,
-            'name': _START},
-        {'ph': 'R', 'ts':  4000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADS},
-        {'ph': 'R', 'ts':  5000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADE},
-        {'ph': 'R', 'ts':  6000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _UNLOAD},
-        {'ph': 'R', 'ts':  7000, 'args': {},             'cat': _BLINK_CAT,
-            'name': _START},
-        {'ph': 'R', 'ts':  8000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADS},
-        {'ph': 'R', 'ts':  9000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADE},
-        {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _UNLOAD},
-        {'ph': 'R', 'ts': 11000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _START},
-        {'ph': 'R', 'ts': 12000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _LOADS},
-        {'ph': 'R', 'ts': 13000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _LOADE},
-        {'ph': 'R', 'ts': 14000, 'args': {},             'cat': _BLINK_CAT,
-            'name': _START},
-        {'ph': 'R', 'ts': 15000, 'args': {},             'cat': _BLINK_CAT,
-            'name': _START},
-        {'ph': 'R', 'ts': 16000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
-            'name': _LOADS},
-        {'ph': 'R', 'ts': 17000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADS},
-        {'ph': 'R', 'ts': 18000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
-            'name': _LOADE},
-        {'ph': 'R', 'ts': 19000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADE},
-        {'ph': 'R', 'ts': 20000, 'args': {},             'cat': 'whatever',
-            'name': _START},
-        {'ph': 'R', 'ts': 21000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _LOADS},
-        {'ph': 'R', 'ts': 22000, 'args': {'frame': '0'}, 'cat': 'whatever',
-            'name': _LOADE},
-        {'ph': 'R', 'ts': 23000, 'args': {},             'cat': _BLINK_CAT,
-            'name': _START},
-        {'ph': 'R', 'ts': 24000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADS},
-        {'ph': 'R', 'ts': 25000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
-            'name': _LOADE}]))
-
-    self.assertEquals(3, len(trace_events))
-    self.assertEquals(14, trace_events['requestStart'].start_msec)
-    self.assertEquals(17, trace_events['loadEventStart'].start_msec)
-    self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
-
-  def testPullMetricsFromLoadingTrace(self):
-    metrics = puller._PullMetricsFromLoadingTrace(LoadingTrace(
-        _MINIMALIST_TRACE_EVENTS))
-    self.assertEquals(4, len(metrics))
-    self.assertEquals(20, metrics['total_load'])
-    self.assertEquals(5, metrics['onload'])
-    self.assertEquals(30971, metrics['browser_malloc_avg'])
-    self.assertEquals(55044, metrics['browser_malloc_max'])
-
-  def testComputeSpeedIndex(self):
-    def point(time, frame_completeness):
-      return puller.CompletenessPoint(time=time,
-                                      frame_completeness=frame_completeness)
-    completness_record = [
-      point(0, 0.0),
-      point(120, 0.4),
-      point(190, 0.75),
-      point(280, 1.0),
-      point(400, 1.0),
-    ]
-    self.assertEqual(120 + 70 * 0.6 + 90 * 0.25,
-                     puller.ComputeSpeedIndex(completness_record))
-
-    completness_record = [
-      point(70, 0.0),
-      point(150, 0.3),
-      point(210, 0.6),
-      point(220, 0.9),
-      point(240, 1.0),
-    ]
-    self.assertEqual(80 + 60 * 0.7 + 10 * 0.4 + 20 * 0.1,
-                     puller.ComputeSpeedIndex(completness_record))
-
-    completness_record = [
-      point(90, 0.0),
-      point(200, 0.6),
-      point(150, 0.3),
-      point(230, 1.0),
-    ]
-    with self.assertRaises(ValueError):
-      puller.ComputeSpeedIndex(completness_record)
-
-  def testCommandLine(self):
-    tmp_dir = tempfile.mkdtemp()
-    for dirname in ['1', '2', 'whatever']:
-      os.mkdir(os.path.join(tmp_dir, dirname))
-      LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
-          os.path.join(tmp_dir, dirname, 'trace.json'))
-
-    process = subprocess.Popen(['python', puller.__file__, tmp_dir])
-    process.wait()
-    shutil.rmtree(tmp_dir)
-
-    self.assertEquals(0, process.returncode)
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/loading/sandwich.py b/loading/sandwich.py
index c989b6f..3fac221 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -13,13 +13,9 @@ TODO(pasko): implement cache preparation and WPR.
 
 import argparse
 import csv
-import json
 import logging
 import os
-import shutil
 import sys
-import tempfile
-import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -32,281 +28,16 @@ from pylib import constants
 import devil_chromium
 
 import chrome_cache
-import chrome_setup
-import controller
-import device_setup
-import devtools_monitor
-import frame_load_lens
-import loading_trace
+import emulation
 import options
-import page_track
-import pull_sandwich_metrics
-import request_dependencies_lens
-import trace_recorder
-import tracing
-import wpr_backend
+import sandwich_metrics
+import sandwich_misc
+from sandwich_runner import SandwichRunner
 
 
 # Use options layer to access constants.
 OPTIONS = options.OPTIONS
 
-_JOB_SEARCH_PATH = 'sandwich_jobs'
-
-# An estimate of time to wait for the device to become idle after expensive
-# operations, such as opening the launcher activity.
-_TIME_TO_DEVICE_IDLE_SECONDS = 2
-
-
-# Devtools timeout of 1 minute to avoid websocket timeout on slow
-# network condition.
-_DEVTOOLS_TIMEOUT = 60
-
-
-def _ReadUrlsFromJobDescription(job_name):
-  """Retrieves the list of URLs associated with the job name."""
-  try:
-    # Extra sugar: attempt to load from a relative path.
-    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
-        job_name)
-    with open(json_file_name) as f:
-      json_data = json.load(f)
-  except IOError:
-    # Attempt to read by regular file name.
-    with open(job_name) as f:
-      json_data = json.load(f)
-
-  key = 'urls'
-  if json_data and key in json_data:
-    url_list = json_data[key]
-    if isinstance(url_list, list) and len(url_list) > 0:
-      return url_list
-  raise Exception('Job description does not define a list named "urls"')
-
-
-def _CleanPreviousTraces(output_directories_path):
-  """Cleans previous traces from the output directory.
-
-  Args:
-    output_directories_path: The output directory path where to clean the
-        previous traces.
-  """
-  for dirname in os.listdir(output_directories_path):
-    directory_path = os.path.join(output_directories_path, dirname)
-    if not os.path.isdir(directory_path):
-      continue
-    try:
-      int(dirname)
-    except ValueError:
-      continue
-    shutil.rmtree(directory_path)
-
-
-class SandwichRunner(object):
-  """Sandwich runner.
-
-  This object is meant to be configured first and then run using the Run()
-  method. The runner can configure itself conveniently with parsed arguement
-  using the PullConfigFromArgs() method. The only job is to make sure that the
-  command line flags have `dest` parameter set to existing runner members.
-  """
-
-  def __init__(self, job_name):
-    """Configures a sandwich runner out of the box.
-
-    Public members are meant to be configured as wished before calling Run().
-
-    Args:
-      job_name: The job name to get the associated urls.
-    """
-    # Cache operation to do before doing the chrome navigation.
-    #   Can be: clear,save,push,reload
-    self.cache_operation = 'clear'
-
-    # The cache archive's path to save to or push from. Is str or None.
-    self.cache_archive_path = None
-
-    # Controls whether the WPR server should do script injection.
-    self.disable_wpr_script_injection = False
-
-    # The job name. Is str.
-    self.job_name = job_name
-
-    # Number of times to repeat the job.
-    self.job_repeat = 1
-
-    # Network conditions to emulate. None if no emulation.
-    self.network_condition = None
-
-    # Network condition emulator. Can be: browser,wpr
-    self.network_emulator = 'browser'
-
-    # Output directory where to save the traces. Is str or None.
-    self.trace_output_directory = None
-
-    # List of urls to run.
-    self.urls = _ReadUrlsFromJobDescription(job_name)
-
-    # Configures whether to record speed-index video.
-    self.record_video = False
-
-    # Path to the WPR archive to load or save. Is str or None.
-    self.wpr_archive_path = None
-
-    # Configures whether the WPR archive should be read or generated.
-    self.wpr_record = False
-
-    self._chrome_ctl = None
-    self._local_cache_directory_path = None
-
-  def PullConfigFromArgs(self, args):
-    """Configures the sandwich runner from parsed command line argument.
-
-    Args:
-      args: The command line parsed argument.
-    """
-    for config_name in self.__dict__.keys():
-      if config_name in args.__dict__:
-        self.__dict__[config_name] = args.__dict__[config_name]
-
-  def PrintConfig(self):
-    """Print the current sandwich runner configuration to stdout. """
-    for config_name in sorted(self.__dict__.keys()):
-      if config_name[0] != '_':
-        print '{} = {}'.format(config_name, self.__dict__[config_name])
-
-  def _CleanTraceOutputDirectory(self):
-    assert self.trace_output_directory
-    if not os.path.isdir(self.trace_output_directory):
-      try:
-        os.makedirs(self.trace_output_directory)
-      except OSError:
-        logging.error('Cannot create directory for results: %s',
-            self.trace_output_directory)
-        raise
-    else:
-      _CleanPreviousTraces(self.trace_output_directory)
-
-  def _SaveRunInfos(self, urls):
-    assert self.trace_output_directory
-    run_infos = {
-      'cache-op': self.cache_operation,
-      'job_name': self.job_name,
-      'urls': urls
-    }
-    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
-              'w') as file_output:
-      json.dump(run_infos, file_output, indent=2)
-
-  def _GetEmulatorNetworkCondition(self, emulator):
-    if self.network_emulator == emulator:
-      return self.network_condition
-    return None
-
-  def _RunNavigation(self, url, clear_cache, run_id=None):
-    """Run a page navigation to the given URL.
-
-    Args:
-      url: The URL to navigate to.
-      clear_cache: Whether if the cache should be cleared before navigation.
-      run_id: Id of the run in the output directory. If it is None, then no
-        trace or video will be saved.
-    """
-    run_path = None
-    if self.trace_output_directory is not None and run_id is not None:
-      run_path = os.path.join(self.trace_output_directory, str(run_id))
-      if os.path.isdir(run_path):
-        os.makedirs(run_path)
-    self._chrome_ctl.SetNetworkEmulation(
-        self._GetEmulatorNetworkCondition('browser'))
-    # TODO(gabadie): add a way to avoid recording a trace.
-    with self._chrome_ctl.Open() as connection:
-      if clear_cache:
-        connection.ClearCache()
-      if run_path is not None and self.record_video:
-        device = self._chrome_ctl.GetDevice()
-        assert device, 'Can only record video on a remote device.'
-        video_recording_path = os.path.join(run_path, 'video.mp4')
-        with device_setup.RemoteSpeedIndexRecorder(device, connection,
-                                                   video_recording_path):
-          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-              url=url,
-              connection=connection,
-              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-              categories=pull_sandwich_metrics.CATEGORIES,
-              timeout_seconds=_DEVTOOLS_TIMEOUT)
-      else:
-        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
-            url=url,
-            connection=connection,
-            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
-            categories=pull_sandwich_metrics.CATEGORIES,
-            timeout_seconds=_DEVTOOLS_TIMEOUT)
-    if run_path is not None:
-      trace_path = os.path.join(run_path, 'trace.json')
-      trace.ToJsonFile(trace_path)
-
-  def _RunUrl(self, url, run_id):
-    clear_cache = False
-    if self.cache_operation == 'clear':
-      clear_cache = True
-    elif self.cache_operation == 'push':
-      self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
-    elif self.cache_operation == 'reload':
-      self._RunNavigation(url, clear_cache=True)
-    elif self.cache_operation == 'save':
-      clear_cache = run_id == 0
-    self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
-
-  def _PullCacheFromDevice(self):
-    assert self.cache_operation == 'save'
-    assert self.cache_archive_path, 'Need to specify where to save the cache'
-
-    cache_directory_path = self._chrome_ctl.PullBrowserCache()
-    chrome_cache.ZipDirectoryContent(
-        cache_directory_path, self.cache_archive_path)
-    shutil.rmtree(cache_directory_path)
-
-  def Run(self):
-    """SandwichRunner main entry point meant to be called once configured."""
-    assert self._chrome_ctl == None
-    assert self._local_cache_directory_path == None
-    if self.trace_output_directory:
-      self._CleanTraceOutputDirectory()
-
-    # TODO(gabadie): Make sandwich working on desktop.
-    device = device_utils.DeviceUtils.HealthyDevices()[0]
-    self._chrome_ctl = controller.RemoteChromeController(device)
-    self._chrome_ctl.AddChromeArgument('--disable-infobars')
-    if self.cache_operation == 'save':
-      self._chrome_ctl.SetSlowDeath()
-
-    if self.cache_operation == 'push':
-      assert os.path.isfile(self.cache_archive_path)
-      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-      chrome_cache.UnzipDirectoryContent(
-          self.cache_archive_path, self._local_cache_directory_path)
-
-    ran_urls = []
-    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
-        record=self.wpr_record,
-        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
-        disable_script_injection=self.disable_wpr_script_injection
-        ):
-      for _ in xrange(self.job_repeat):
-        for url in self.urls:
-          self._RunUrl(url, run_id=len(ran_urls))
-          ran_urls.append(url)
-
-    if self._local_cache_directory_path:
-      shutil.rmtree(self._local_cache_directory_path)
-      self._local_cache_directory_path = None
-    if self.cache_operation == 'save':
-      self._PullCacheFromDevice()
-    if self.trace_output_directory:
-      self._SaveRunInfos(ran_urls)
-
-    self._chrome_ctl = None
-
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
@@ -371,7 +102,7 @@ def _ArgumentParser():
                               'overriding javascript\'s Math.random() and ' +
                               'Date() with deterministic implementations.')
   run_parser.add_argument('--network-condition', default=None,
-      choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
+      choices=sorted(emulation.NETWORK_CONDITIONS.keys()),
       help='Set a network profile.')
   run_parser.add_argument('--network-emulator', default='browser',
       choices=['browser', 'wpr'],
@@ -430,36 +161,6 @@ def _RecordWprMain(args):
   return 0
 
 
-def _PatchWprMain(args):
-  # Sets the resources cache max-age to 10 years.
-  MAX_AGE = 10 * 365 * 24 * 60 * 60
-  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
-
-  wpr_archive = wpr_backend.WprArchiveBackend(args.wpr_archive_path)
-  for url_entry in wpr_archive.ListUrlEntries():
-    response_headers = url_entry.GetResponseHeadersDict()
-    if 'cache-control' in response_headers and \
-        response_headers['cache-control'] == CACHE_CONTROL:
-      continue
-    logging.info('patching %s' % url_entry.url)
-    # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
-    # TODO(gabadie): may need to delete ETag.
-    # TODO(gabadie): may need to patch Vary.
-    # TODO(gabadie): may need to take care of x-cache.
-    #
-    # Override the cache-control header to set the resources max age to MAX_AGE.
-    #
-    # Important note: Some resources holding sensitive information might have
-    # cache-control set to no-store which allow the resource to be cached but
-    # not cached in the file system. NoState-Prefetch is going to take care of
-    # this case. But in here, to simulate NoState-Prefetch, we don't have other
-    # choices but save absolutely all cached resources on disk so they survive
-    # after killing chrome for cache save, modification and push.
-    url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
-  wpr_archive.Persist()
-  return 0
-
-
 def _CreateCacheMain(args):
   sandwich_runner = SandwichRunner(args.job)
   sandwich_runner.PullConfigFromArgs(args)
@@ -480,12 +181,12 @@ def _RunJobMain(args):
 
 
 def _ExtractMetricsMain(args):
-  trace_metrics_list = pull_sandwich_metrics.PullMetricsFromOutputDirectory(
+  trace_metrics_list = sandwich_metrics.PullMetricsFromOutputDirectory(
       args.trace_output_directory)
   trace_metrics_list.sort(key=lambda e: e['id'])
   with open(args.metrics_csv_path, 'w') as csv_file:
     writer = csv.DictWriter(csv_file,
-                            fieldnames=pull_sandwich_metrics.CSV_FIELD_NAMES)
+                            fieldnames=sandwich_metrics.CSV_FIELD_NAMES)
     writer.writeheader()
     for trace_metrics in trace_metrics_list:
       writer.writerow(trace_metrics)
@@ -495,31 +196,8 @@ def _ExtractMetricsMain(args):
 def _FilterCacheMain(args):
   whitelisted_urls = set()
   for loading_trace_path in args.loading_trace_paths:
-    logging.info('loading %s' % loading_trace_path)
-    trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
-    requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
-    deps = requests_lens.GetRequestDependencies()
-
-    main_resource_request = deps[0][0]
-    logging.info('white-listing %s' % main_resource_request.url)
-    whitelisted_urls.add(main_resource_request.url)
-    for (first, second, reason) in deps:
-      # Work-around where the protocol may be none for an unclear reason yet.
-      # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
-      #   this work-around.
-      if not second.protocol:
-        logging.info('ignoring %s (no protocol)' % second.url)
-        continue
-      # Ignore data protocols.
-      if not second.protocol.startswith('http'):
-        logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
-            second.url, second.protocol))
-        continue
-      if (first.request_id == main_resource_request.request_id and
-          reason == 'parser' and second.url not in whitelisted_urls):
-        logging.info('white-listing %s' % second.url)
-        whitelisted_urls.add(second.url)
-
+    whitelisted_urls.update(
+        sandwich_misc.ExtractParserDiscoverableResources(loading_trace_path))
   if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
     os.makedirs(os.path.dirname(args.output_cache_archive_path))
   chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
@@ -538,7 +216,8 @@ def main(command_line_args):
   if args.subcommand == 'record-wpr':
     return _RecordWprMain(args)
   if args.subcommand == 'patch-wpr':
-    return _PatchWprMain(args)
+    sandwich_misc.PatchWpr(args.wpr_archive_path)
+    return 0
   if args.subcommand == 'create-cache':
     return _CreateCacheMain(args)
   if args.subcommand == 'run':
diff --git a/loading/sandwich_metrics.py b/loading/sandwich_metrics.py
new file mode 100644
index 0000000..e30232c
--- /dev/null
+++ b/loading/sandwich_metrics.py
@@ -0,0 +1,258 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Pull a sandwich run's output directory's metrics from traces into a CSV.
+
+python pull_sandwich_metrics.py -h
+"""
+
+import collections
+import logging
+import os
+import shutil
+import sys
+import tempfile
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
+from chrome_telemetry_build import chromium_config
+
+sys.path.append(chromium_config.GetTelemetryDir())
+from telemetry.internal.image_processing import video
+from telemetry.util import image_util
+from telemetry.util import rgba_color
+
+import loading_trace as loading_trace_module
+import tracing
+
+
+CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
+
+CSV_FIELD_NAMES = [
+    'id',
+    'url',
+    'total_load',
+    'onload',
+    'browser_malloc_avg',
+    'browser_malloc_max',
+    'speed_index']
+
+_TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
+
+# Points of a completeness record.
+#
+# Members:
+#   |time| is in milliseconds,
+#   |frame_completeness| value representing how complete the frame is at a given
+#     |time|. Caution: this completeness might be negative.
+CompletenessPoint = collections.namedtuple('CompletenessPoint',
+    ('time', 'frame_completeness'))
+
+
+def _GetBrowserPID(tracing_track):
+  """Get the browser PID from a trace.
+
+  Args:
+    tracing_track: The tracing.TracingTrack.
+
+  Returns:
+    The browser's PID as an integer.
+  """
+  for event in tracing_track.GetEvents():
+    if event.category != '__metadata' or event.name != 'process_name':
+      continue
+    if event.args['name'] == 'Browser':
+      return event.pid
+  raise ValueError('couldn\'t find browser\'s PID')
+
+
+def _GetBrowserDumpEvents(tracing_track):
+  """Get the browser memory dump events from a tracing track.
+
+  Args:
+    tracing_track: The tracing.TracingTrack.
+
+  Returns:
+    List of memory dump events.
+  """
+  browser_pid = _GetBrowserPID(tracing_track)
+  browser_dumps_events = []
+  for event in tracing_track.GetEvents():
+    if event.category != 'disabled-by-default-memory-infra':
+      continue
+    if event.type != 'v' or event.name != 'periodic_interval':
+      continue
+    # Ignore dump events for processes other than the browser process
+    if event.pid != browser_pid:
+      continue
+    browser_dumps_events.append(event)
+  if len(browser_dumps_events) == 0:
+    raise ValueError('No browser dump events found.')
+  return browser_dumps_events
+
+
+def _GetWebPageTrackedEvents(tracing_track):
+  """Get the web page's tracked events from a tracing track.
+
+  Args:
+    tracing_track: The tracing.TracingTrack.
+
+  Returns:
+    Dictionary all tracked events.
+  """
+  main_frame = None
+  tracked_events = {}
+  for event in tracing_track.GetEvents():
+    if event.category != 'blink.user_timing':
+      continue
+    event_name = event.name
+    # Ignore events until about:blank's unloadEventEnd that give the main
+    # frame id.
+    if not main_frame:
+      if event_name == 'unloadEventEnd':
+        main_frame = event.args['frame']
+        logging.info('found about:blank\'s event \'unloadEventEnd\'')
+      continue
+    # Ignore sub-frames events. requestStart don't have the frame set but it
+    # is fine since tracking the first one after about:blank's unloadEventEnd.
+    if 'frame' in event.args and event.args['frame'] != main_frame:
+      continue
+    if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
+      logging.info('found url\'s event \'%s\'' % event_name)
+      tracked_events[event_name] = event
+  assert len(tracked_events) == len(_TRACKED_EVENT_NAMES)
+  return tracked_events
+
+
+def _PullMetricsFromLoadingTrace(loading_trace):
+  """Pulls all the metrics from a given trace.
+
+  Args:
+    loading_trace: loading_trace_module.LoadingTrace.
+
+  Returns:
+    Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
+  """
+  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
+  web_page_tracked_events = _GetWebPageTrackedEvents(
+      loading_trace.tracing_track)
+
+  browser_malloc_sum = 0
+  browser_malloc_max = 0
+  for dump_event in browser_dump_events:
+    attr = dump_event.args['dumps']['allocators']['malloc']['attrs']['size']
+    assert attr['units'] == 'bytes'
+    size = int(attr['value'], 16)
+    browser_malloc_sum += size
+    browser_malloc_max = max(browser_malloc_max, size)
+
+  return {
+    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
+                   web_page_tracked_events['requestStart'].start_msec),
+    'onload': (web_page_tracked_events['loadEventEnd'].start_msec -
+               web_page_tracked_events['loadEventStart'].start_msec),
+    'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
+    'browser_malloc_max': browser_malloc_max
+  }
+
+
+def _ExtractCompletenessRecordFromVideo(video_path):
+  """Extracts the completeness record from a video.
+
+  The video must start with a filled rectangle of orange (RGB: 222, 100, 13), to
+  give the view-port size/location from where to compute the completeness.
+
+  Args:
+    video_path: Path of the video to extract the completeness list from.
+
+  Returns:
+    list(CompletenessPoint)
+  """
+  video_file = tempfile.NamedTemporaryFile()
+  shutil.copy(video_path, video_file.name)
+  video_capture = video.Video(video_file)
+
+  histograms = [
+      (time, image_util.GetColorHistogram(
+          image, ignore_color=rgba_color.WHITE, tolerance=8))
+      for time, image in video_capture.GetVideoFrameIter()
+  ]
+
+  start_histogram = histograms[1][1]
+  final_histogram = histograms[-1][1]
+  total_distance = start_histogram.Distance(final_histogram)
+
+  def FrameProgress(histogram):
+    if total_distance == 0:
+      if histogram.Distance(final_histogram) == 0:
+        return 1.0
+      else:
+        return 0.0
+    return 1 - histogram.Distance(final_histogram) / total_distance
+
+  return [(time, FrameProgress(hist)) for time, hist in histograms]
+
+
+def ComputeSpeedIndex(completeness_record):
+  """Computes the speed-index from a completeness record.
+
+  Args:
+    completeness_record: list(CompletenessPoint)
+
+  Returns:
+    Speed-index value.
+  """
+  speed_index = 0.0
+  last_time = completeness_record[0][0]
+  last_completness = completeness_record[0][1]
+  for time, completeness in completeness_record:
+    if time < last_time:
+      raise ValueError('Completeness record must be sorted by timestamps.')
+    elapsed = time - last_time
+    speed_index += elapsed * (1.0 - last_completness)
+    last_time = time
+    last_completness = completeness
+  return speed_index
+
+
+def PullMetricsFromOutputDirectory(output_directory_path):
+  """Pulls all the metrics from all the traces of a sandwich run directory.
+
+  Args:
+    output_directory_path: The sandwich run's output directory to pull the
+        metrics from.
+
+  Returns:
+    List of dictionaries with all CSV_FIELD_NAMES's field set.
+  """
+  assert os.path.isdir(output_directory_path)
+  metrics = []
+  for node_name in os.listdir(output_directory_path):
+    if not os.path.isdir(os.path.join(output_directory_path, node_name)):
+      continue
+    try:
+      page_id = int(node_name)
+    except ValueError:
+      continue
+    run_path = os.path.join(output_directory_path, node_name)
+    trace_path = os.path.join(run_path, 'trace.json')
+    if not os.path.isfile(trace_path):
+      continue
+    logging.info('processing \'%s\'' % trace_path)
+    loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
+    row_metrics = {key: 'unavailable' for key in CSV_FIELD_NAMES}
+    row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
+    row_metrics['id'] = page_id
+    row_metrics['url'] = loading_trace.url
+    video_path = os.path.join(run_path, 'video.mp4')
+    if os.path.isfile(video_path):
+      logging.info('processing \'%s\'' % video_path)
+      completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
+      row_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+    metrics.append(row_metrics)
+  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
+                            'run directory.').format(output_directory_path)
+  return metrics
diff --git a/loading/sandwich_metrics_unittest.py b/loading/sandwich_metrics_unittest.py
new file mode 100644
index 0000000..7cbe795
--- /dev/null
+++ b/loading/sandwich_metrics_unittest.py
@@ -0,0 +1,245 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import json
+import os
+import shutil
+import subprocess
+import tempfile
+import unittest
+
+import loading_trace
+import page_track
+import sandwich_metrics as puller
+import request_track
+import tracing
+
+
+_BLINK_CAT = 'blink.user_timing'
+_MEM_CAT = 'disabled-by-default-memory-infra'
+_START='requestStart'
+_LOADS='loadEventStart'
+_LOADE='loadEventEnd'
+_UNLOAD='unloadEventEnd'
+
+_MINIMALIST_TRACE_EVENTS = [
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10000,
+        'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _START,  'ts': 20000,
+        'args': {}},
+    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
+        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+            'units': 'bytes', 'value': '1af2', }}}}}}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35000,
+        'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40000,
+        'args': {'frame': '0'}},
+    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
+        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+            'units': 'bytes', 'value': 'd704', }}}}}}},
+    {'ph': 'M', 'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'ts': 1,
+        'args': {'name': 'Browser'}}]
+
+
+def TracingTrack(events):
+  return tracing.TracingTrack.FromJsonDict({'events': events})
+
+
+def LoadingTrace(events):
+  return loading_trace.LoadingTrace('http://a.com/', {},
+                                    page_track.PageTrack(None),
+                                    request_track.RequestTrack(None),
+                                    TracingTrack(events))
+
+
+class PageTrackTest(unittest.TestCase):
+  def testGetBrowserPID(self):
+    def RunHelper(expected, events):
+      self.assertEquals(expected, puller._GetBrowserPID(TracingTrack(events)))
+
+    RunHelper(123, [
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
+            'name': 'thread_name'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
+            'name': 'process_name', 'args': {'name': 'Renderer'}},
+        {'ph': 'M', 'ts': 0, 'pid': 123, 'cat': '__metadata',
+            'name': 'process_name', 'args': {'name': 'Browser'}},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'}])
+
+    with self.assertRaises(ValueError):
+      RunHelper(123, [
+          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
+          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'}])
+
+  def testGetBrowserDumpEvents(self):
+    NAME = 'periodic_interval'
+
+    def RunHelper(trace_events, browser_pid):
+      trace_events = copy.copy(trace_events)
+      trace_events.append({
+          'pid': browser_pid,
+          'cat': '__metadata',
+          'name': 'process_name',
+          'ph': 'M',
+          'ts': 0,
+          'args': {'name': 'Browser'}})
+      return puller._GetBrowserDumpEvents(TracingTrack(trace_events))
+
+    TRACE_EVENTS = [
+        {'pid': 354, 'ts':  1000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  2000, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  3000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  4000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  5000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  6000, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  7000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  8000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  9000, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts': 10000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
+        {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
+
+    self.assertTrue(_MEM_CAT in puller.CATEGORIES)
+
+    bump_events = RunHelper(TRACE_EVENTS, 123)
+    self.assertEquals(2, len(bump_events))
+    self.assertEquals(5, bump_events[0].start_msec)
+    self.assertEquals(10, bump_events[1].start_msec)
+
+    bump_events = RunHelper(TRACE_EVENTS, 354)
+    self.assertEquals(1, len(bump_events))
+    self.assertEquals(1, bump_events[0].start_msec)
+
+    bump_events = RunHelper(TRACE_EVENTS, 672)
+    self.assertEquals(3, len(bump_events))
+    self.assertEquals(3, bump_events[0].start_msec)
+    self.assertEquals(7, bump_events[1].start_msec)
+    self.assertEquals(12, bump_events[2].start_msec)
+
+    with self.assertRaises(ValueError):
+      RunHelper(TRACE_EVENTS, 895)
+
+  def testGetWebPageTrackedEvents(self):
+    self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
+
+    trace_events = puller._GetWebPageTrackedEvents(TracingTrack([
+        {'ph': 'R', 'ts':  0000, 'args': {},             'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts':  1000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  2000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts':  3000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts':  4000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  5000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts':  6000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _UNLOAD},
+        {'ph': 'R', 'ts':  7000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts':  8000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  9000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _UNLOAD},
+        {'ph': 'R', 'ts': 11000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts': 12000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 13000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 14000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 15000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 16000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 17000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 18000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 19000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 20000, 'args': {},             'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts': 21000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 22000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 23000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 24000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 25000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE}]))
+
+    self.assertEquals(3, len(trace_events))
+    self.assertEquals(14, trace_events['requestStart'].start_msec)
+    self.assertEquals(17, trace_events['loadEventStart'].start_msec)
+    self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
+
+  def testPullMetricsFromLoadingTrace(self):
+    metrics = puller._PullMetricsFromLoadingTrace(LoadingTrace(
+        _MINIMALIST_TRACE_EVENTS))
+    self.assertEquals(4, len(metrics))
+    self.assertEquals(20, metrics['total_load'])
+    self.assertEquals(5, metrics['onload'])
+    self.assertEquals(30971, metrics['browser_malloc_avg'])
+    self.assertEquals(55044, metrics['browser_malloc_max'])
+
+  def testComputeSpeedIndex(self):
+    def point(time, frame_completeness):
+      return puller.CompletenessPoint(time=time,
+                                      frame_completeness=frame_completeness)
+    completness_record = [
+      point(0, 0.0),
+      point(120, 0.4),
+      point(190, 0.75),
+      point(280, 1.0),
+      point(400, 1.0),
+    ]
+    self.assertEqual(120 + 70 * 0.6 + 90 * 0.25,
+                     puller.ComputeSpeedIndex(completness_record))
+
+    completness_record = [
+      point(70, 0.0),
+      point(150, 0.3),
+      point(210, 0.6),
+      point(220, 0.9),
+      point(240, 1.0),
+    ]
+    self.assertEqual(80 + 60 * 0.7 + 10 * 0.4 + 20 * 0.1,
+                     puller.ComputeSpeedIndex(completness_record))
+
+    completness_record = [
+      point(90, 0.0),
+      point(200, 0.6),
+      point(150, 0.3),
+      point(230, 1.0),
+    ]
+    with self.assertRaises(ValueError):
+      puller.ComputeSpeedIndex(completness_record)
+
+  def testCommandLine(self):
+    tmp_dir = tempfile.mkdtemp()
+    for dirname in ['1', '2', 'whatever']:
+      os.mkdir(os.path.join(tmp_dir, dirname))
+      LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
+          os.path.join(tmp_dir, dirname, 'trace.json'))
+
+    process = subprocess.Popen(['python', puller.__file__, tmp_dir])
+    process.wait()
+    shutil.rmtree(tmp_dir)
+
+    self.assertEquals(0, process.returncode)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/sandwich_misc.py b/loading/sandwich_misc.py
new file mode 100644
index 0000000..a15f5ab
--- /dev/null
+++ b/loading/sandwich_misc.py
@@ -0,0 +1,81 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+
+import wpr_backend
+import loading_trace
+import request_dependencies_lens
+
+
+def PatchWpr(wpr_archive_path):
+  """Patches a WPR archive to get all resources into the HTTP cache and avoid
+  invalidation and revalidations.
+
+  Args:
+    wpr_archive_path: Path of the WPR archive to patch.
+  """
+  # Sets the resources cache max-age to 10 years.
+  MAX_AGE = 10 * 365 * 24 * 60 * 60
+  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
+
+  wpr_archive = wpr_backend.WprArchiveBackend(wpr_archive_path)
+  for url_entry in wpr_archive.ListUrlEntries():
+    response_headers = url_entry.GetResponseHeadersDict()
+    if 'cache-control' in response_headers and \
+        response_headers['cache-control'] == CACHE_CONTROL:
+      continue
+    logging.info('patching %s' % url_entry.url)
+    # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
+    # TODO(gabadie): may need to delete ETag.
+    # TODO(gabadie): may need to patch Vary.
+    # TODO(gabadie): may need to take care of x-cache.
+    #
+    # Override the cache-control header to set the resources max age to MAX_AGE.
+    #
+    # Important note: Some resources holding sensitive information might have
+    # cache-control set to no-store which allow the resource to be cached but
+    # not cached in the file system. NoState-Prefetch is going to take care of
+    # this case. But in here, to simulate NoState-Prefetch, we don't have other
+    # choices but save absolutely all cached resources on disk so they survive
+    # after killing chrome for cache save, modification and push.
+    url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
+  wpr_archive.Persist()
+
+
+def ExtractParserDiscoverableResources(loading_trace_path):
+  """Extracts the parser discoverable resources from a loading trace.
+
+  Args:
+    loading_trace_path: The loading trace's path.
+
+  Returns:
+    A set of urls.
+  """
+  whitelisted_urls = set()
+  logging.info('loading %s' % loading_trace_path)
+  trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
+  requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
+  deps = requests_lens.GetRequestDependencies()
+
+  main_resource_request = deps[0][0]
+  logging.info('white-listing %s' % main_resource_request.url)
+  whitelisted_urls.add(main_resource_request.url)
+  for (first, second, reason) in deps:
+    # Work-around where the protocol may be none for an unclear reason yet.
+    # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
+    #   this work-around.
+    if not second.protocol:
+      logging.info('ignoring %s (no protocol)' % second.url)
+      continue
+    # Ignore data protocols.
+    if not second.protocol.startswith('http'):
+      logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
+          second.url, second.protocol))
+      continue
+    if (first.request_id == main_resource_request.request_id and
+        reason == 'parser' and second.url not in whitelisted_urls):
+      logging.info('white-listing %s' % second.url)
+      whitelisted_urls.add(second.url)
+  return whitelisted_urls
diff --git a/loading/sandwich_runner.py b/loading/sandwich_runner.py
new file mode 100644
index 0000000..5dd7ba2
--- /dev/null
+++ b/loading/sandwich_runner.py
@@ -0,0 +1,276 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+import logging
+import os
+import shutil
+import sys
+import tempfile
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+import chrome_cache
+import controller
+import devtools_monitor
+import device_setup
+import loading_trace
+import sandwich_metrics
+
+
+_JOB_SEARCH_PATH = 'sandwich_jobs'
+
+# Devtools timeout of 1 minute to avoid websocket timeout on slow
+# network condition.
+_DEVTOOLS_TIMEOUT = 60
+
+
+def _ReadUrlsFromJobDescription(job_name):
+  """Retrieves the list of URLs associated with the job name."""
+  try:
+    # Extra sugar: attempt to load from a relative path.
+    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
+        job_name)
+    with open(json_file_name) as f:
+      json_data = json.load(f)
+  except IOError:
+    # Attempt to read by regular file name.
+    with open(job_name) as f:
+      json_data = json.load(f)
+
+  key = 'urls'
+  if json_data and key in json_data:
+    url_list = json_data[key]
+    if isinstance(url_list, list) and len(url_list) > 0:
+      return url_list
+  raise Exception('Job description does not define a list named "urls"')
+
+
+def _CleanPreviousTraces(output_directories_path):
+  """Cleans previous traces from the output directory.
+
+  Args:
+    output_directories_path: The output directory path where to clean the
+        previous traces.
+  """
+  for dirname in os.listdir(output_directories_path):
+    directory_path = os.path.join(output_directories_path, dirname)
+    if not os.path.isdir(directory_path):
+      continue
+    try:
+      int(dirname)
+    except ValueError:
+      continue
+    shutil.rmtree(directory_path)
+
+
+class SandwichRunner(object):
+  """Sandwich runner.
+
+  This object is meant to be configured first and then run using the Run()
+  method. The runner can configure itself conveniently with parsed arguement
+  using the PullConfigFromArgs() method. The only job is to make sure that the
+  command line flags have `dest` parameter set to existing runner members.
+  """
+
+  def __init__(self, job_name):
+    """Configures a sandwich runner out of the box.
+
+    Public members are meant to be configured as wished before calling Run().
+
+    Args:
+      job_name: The job name to get the associated urls.
+    """
+    # Cache operation to do before doing the chrome navigation.
+    #   Can be: clear,save,push,reload
+    self.cache_operation = 'clear'
+
+    # The cache archive's path to save to or push from. Is str or None.
+    self.cache_archive_path = None
+
+    # Controls whether the WPR server should do script injection.
+    self.disable_wpr_script_injection = False
+
+    # The job name. Is str.
+    self.job_name = job_name
+
+    # Number of times to repeat the job.
+    self.job_repeat = 1
+
+    # Network conditions to emulate. None if no emulation.
+    self.network_condition = None
+
+    # Network condition emulator. Can be: browser,wpr
+    self.network_emulator = 'browser'
+
+    # Output directory where to save the traces. Is str or None.
+    self.trace_output_directory = None
+
+    # List of urls to run.
+    self.urls = _ReadUrlsFromJobDescription(job_name)
+
+    # Configures whether to record speed-index video.
+    self.record_video = False
+
+    # Path to the WPR archive to load or save. Is str or None.
+    self.wpr_archive_path = None
+
+    # Configures whether the WPR archive should be read or generated.
+    self.wpr_record = False
+
+    self._chrome_ctl = None
+    self._local_cache_directory_path = None
+
+  def PullConfigFromArgs(self, args):
+    """Configures the sandwich runner from parsed command line argument.
+
+    Args:
+      args: The command line parsed argument.
+    """
+    for config_name in self.__dict__.keys():
+      if config_name in args.__dict__:
+        self.__dict__[config_name] = args.__dict__[config_name]
+
+  def PrintConfig(self):
+    """Print the current sandwich runner configuration to stdout. """
+    for config_name in sorted(self.__dict__.keys()):
+      if config_name[0] != '_':
+        print '{} = {}'.format(config_name, self.__dict__[config_name])
+
+  def _CleanTraceOutputDirectory(self):
+    assert self.trace_output_directory
+    if not os.path.isdir(self.trace_output_directory):
+      try:
+        os.makedirs(self.trace_output_directory)
+      except OSError:
+        logging.error('Cannot create directory for results: %s',
+            self.trace_output_directory)
+        raise
+    else:
+      _CleanPreviousTraces(self.trace_output_directory)
+
+  def _SaveRunInfos(self, urls):
+    assert self.trace_output_directory
+    run_infos = {
+      'cache-op': self.cache_operation,
+      'job_name': self.job_name,
+      'urls': urls
+    }
+    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
+              'w') as file_output:
+      json.dump(run_infos, file_output, indent=2)
+
+  def _GetEmulatorNetworkCondition(self, emulator):
+    if self.network_emulator == emulator:
+      return self.network_condition
+    return None
+
+  def _RunNavigation(self, url, clear_cache, run_id=None):
+    """Run a page navigation to the given URL.
+
+    Args:
+      url: The URL to navigate to.
+      clear_cache: Whether if the cache should be cleared before navigation.
+      run_id: Id of the run in the output directory. If it is None, then no
+        trace or video will be saved.
+    """
+    run_path = None
+    if self.trace_output_directory is not None and run_id is not None:
+      run_path = os.path.join(self.trace_output_directory, str(run_id))
+      if not os.path.isdir(run_path):
+        os.makedirs(run_path)
+    self._chrome_ctl.SetNetworkEmulation(
+        self._GetEmulatorNetworkCondition('browser'))
+    # TODO(gabadie): add a way to avoid recording a trace.
+    with self._chrome_ctl.Open() as connection:
+      if clear_cache:
+        connection.ClearCache()
+      if run_path is not None and self.record_video:
+        device = self._chrome_ctl.GetDevice()
+        assert device, 'Can only record video on a remote device.'
+        video_recording_path = os.path.join(run_path, 'video.mp4')
+        with device_setup.RemoteSpeedIndexRecorder(device, connection,
+                                                   video_recording_path):
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url=url,
+              connection=connection,
+              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+              categories=sandwich_metrics.CATEGORIES,
+              timeout_seconds=_DEVTOOLS_TIMEOUT)
+      else:
+        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+            url=url,
+            connection=connection,
+            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+            categories=sandwich_metrics.CATEGORIES,
+            timeout_seconds=_DEVTOOLS_TIMEOUT)
+    if run_path is not None:
+      trace_path = os.path.join(run_path, 'trace.json')
+      trace.ToJsonFile(trace_path)
+
+  def _RunUrl(self, url, run_id):
+    clear_cache = False
+    if self.cache_operation == 'clear':
+      clear_cache = True
+    elif self.cache_operation == 'push':
+      self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
+    elif self.cache_operation == 'reload':
+      self._RunNavigation(url, clear_cache=True)
+    elif self.cache_operation == 'save':
+      clear_cache = run_id == 0
+    self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
+
+  def _PullCacheFromDevice(self):
+    assert self.cache_operation == 'save'
+    assert self.cache_archive_path, 'Need to specify where to save the cache'
+
+    cache_directory_path = self._chrome_ctl.PullBrowserCache()
+    chrome_cache.ZipDirectoryContent(
+        cache_directory_path, self.cache_archive_path)
+    shutil.rmtree(cache_directory_path)
+
+  def Run(self):
+    """SandwichRunner main entry point meant to be called once configured."""
+    assert self._chrome_ctl == None
+    assert self._local_cache_directory_path == None
+    if self.trace_output_directory:
+      self._CleanTraceOutputDirectory()
+
+    # TODO(gabadie): Make sandwich working on desktop.
+    device = device_utils.DeviceUtils.HealthyDevices()[0]
+    self._chrome_ctl = controller.RemoteChromeController(device)
+    self._chrome_ctl.AddChromeArgument('--disable-infobars')
+    if self.cache_operation == 'save':
+      self._chrome_ctl.SetSlowDeath()
+
+    if self.cache_operation == 'push':
+      assert os.path.isfile(self.cache_archive_path)
+      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+      chrome_cache.UnzipDirectoryContent(
+          self.cache_archive_path, self._local_cache_directory_path)
+
+    ran_urls = []
+    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
+        record=self.wpr_record,
+        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
+        disable_script_injection=self.disable_wpr_script_injection
+        ):
+      for _ in xrange(self.job_repeat):
+        for url in self.urls:
+          self._RunUrl(url, run_id=len(ran_urls))
+          ran_urls.append(url)
+
+    if self._local_cache_directory_path:
+      shutil.rmtree(self._local_cache_directory_path)
+      self._local_cache_directory_path = None
+    if self.cache_operation == 'save':
+      self._PullCacheFromDevice()
+    if self.trace_output_directory:
+      self._SaveRunInfos(ran_urls)
+
+    self._chrome_ctl = None

commit 984e6ba778aedb4eee85d4ccde1c778e66ab3358
Author: gabadie <gabadie@chromium.org>
Date:   Wed Mar 23 08:40:08 2016 -0700

    sandwich: Record a loading video on android and compute speed-index from it.
    
    Before, metrics in the CSV where fetched only from loading traces.
    This CL adds the hability to record a MP4 video of chrome loading
    a web page in sandwich, and then process that recorded video to
    output the loading speed-index into the metrics' CSV.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1782543002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382850}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0c661158cf00b7ea75807aa78787234d80305799

diff --git a/loading/analyze.py b/loading/analyze.py
index 58a4ace..fd1ba4e 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -117,12 +117,15 @@ def _LogRequests(url, clear_cache_override=None):
 
   clear_cache = (clear_cache_override if clear_cache_override is not None
                  else OPTIONS.clear_cache)
-  chrome_ctl.SetClearCache(clear_cache)
   if OPTIONS.emulate_device:
     chrome_ctl.SetDeviceEmulation(OPTIONS.emulate_device)
   if OPTIONS.emulate_network:
     chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_network)
-  trace = loading_trace.LoadingTrace.FromUrlAndController(url, chrome_ctl)
+  with chrome_ctl.Open() as connection:
+    if clear_cache:
+      connection.ClearCache()
+    trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+        url, connection, chrome_ctl.ChromeMetadata())
   return trace.ToJsonDict()
 
 
diff --git a/loading/common_util.py b/loading/common_util.py
new file mode 100644
index 0000000..5b62ce0
--- /dev/null
+++ b/loading/common_util.py
@@ -0,0 +1,26 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+import time
+
+
+def PollFor(condition, condition_name, interval=5):
+  """Polls for a function to return true.
+
+  Args:
+    condition: Function to wait its return to be True.
+    condition_name: The condition's name used for logging.
+    interval: Periods to wait between tries in seconds.
+
+  Returns:
+    What condition has returned to stop waiting.
+  """
+  while True:
+    result = condition()
+    logging.info('Polling condition %s is %s' % (
+        condition_name, 'met' if result else 'not met'))
+    if result:
+      return result
+    time.sleep(interval)
diff --git a/loading/controller.py b/loading/controller.py
index 7aa9bde..053e062 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -66,21 +66,12 @@ class ChromeControllerBase(object):
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
-    self._clear_cache = False
     self._slow_death = False
 
   def AddChromeArgument(self, arg):
     """Add command-line argument to the chrome execution."""
     self._chrome_args.append(arg)
 
-  def SetClearCache(self, clear_cache=True):
-    """Ensure cache is cleared before running.
-
-    Args:
-      clear_cache: true if cache should be cleared.
-    """
-    self._clear_cache = clear_cache
-
   @contextlib.contextmanager
   def Open(self):
     """Context that returns a connection/chrome instance.
@@ -182,11 +173,8 @@ class ChromeControllerBase(object):
     if self._emulated_network:
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
-
     self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
                           seconds_since_epoch=time.time())
-    if self._clear_cache:
-      connection.AddHook(connection.ClearCache)
 
   def _GetChromeArguments(self):
     """Get command-line arguments for the chrome execution."""
@@ -216,6 +204,10 @@ class RemoteChromeController(ChromeControllerBase):
     self._device = device
     self._device.EnableRoot()
 
+  def GetDevice(self):
+    """Overridden android device."""
+    return self._device
+
   @contextlib.contextmanager
   def Open(self):
     """Overridden connection creation."""
diff --git a/loading/device_setup.py b/loading/device_setup.py
index fad2b23..787cd79 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -22,11 +22,13 @@ from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
+from video_recorder import video_recorder
 
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
 from chrome_telemetry_build import chromium_config
 
 sys.path.append(chromium_config.GetTelemetryDir())
+from telemetry.internal.image_processing import video
 from telemetry.internal.util import webpagereplay
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
@@ -40,6 +42,9 @@ import options
 
 OPTIONS = options.OPTIONS
 
+# The speed index's video recording's bit rate in Mb/s.
+_SPEED_INDEX_VIDEO_BITRATE = 4
+
 
 class DeviceSetupException(Exception):
   def __init__(self, msg):
@@ -288,6 +293,90 @@ def RemoteWprHost(device, wpr_archive_path, record=False,
 
 # Deprecated
 @contextlib.contextmanager
+def _RemoteVideoRecorder(device, local_output_path, megabits_per_second):
+  """Record a video on Device.
+
+  Args:
+    device: (device_utils.DeviceUtils) Android device to connect to.
+    local_output_path: Output path were to save the video locally.
+    megabits_per_second: Video recorder Mb/s.
+
+  Yields:
+    None
+  """
+  assert device
+  if megabits_per_second > 100:
+    raise ValueError('Android video capture cannot capture at %dmbps. '
+                     'Max capture rate is 100mbps.' % megabits_per_second)
+  assert local_output_path.endswith('.mp4')
+  recorder = video_recorder.VideoRecorder(device, megabits_per_second)
+  recorder.Start()
+  try:
+    yield
+    recorder.Stop()
+    recorder.Pull(host_file=local_output_path)
+    recorder = None
+  finally:
+    if recorder:
+      recorder.Stop()
+
+
+@contextlib.contextmanager
+def RemoteSpeedIndexRecorder(device, connection, local_output_path):
+  """Records on a device a video compatible for speed-index computation.
+
+  Note:
+    Chrome should be opened with the --disable-infobars command line argument to
+    avoid web page viewport size to be changed, that can change speed-index
+    value.
+
+  Args:
+    device: (device_utils.DeviceUtils) Android device to connect to.
+    connection: devtools connection.
+    local_output_path: Output path were to save the video locally.
+
+  Yields:
+    None
+  """
+  # Paint the current HTML document with the ORANGE that video is detecting with
+  # the view-port position and size.
+  color = video.HIGHLIGHT_ORANGE_FRAME
+  connection.ExecuteJavaScript("""
+    (function() {
+      var screen = document.createElement('div');
+      screen.style.background = 'rgb(%d, %d, %d)';
+      screen.style.position = 'fixed';
+      screen.style.top = '0';
+      screen.style.left = '0';
+      screen.style.width = '100%%';
+      screen.style.height = '100%%';
+      screen.style.zIndex = '2147483638';
+      document.body.appendChild(screen);
+      requestAnimationFrame(function() {
+        requestAnimationFrame(function() {
+          window.__speedindex_screen = screen;
+        });
+      });
+    })();
+  """ % (color.r, color.g, color.b))
+  connection.PollForJavaScriptExpression('!!window.__speedindex_screen', 1)
+
+  with _RemoteVideoRecorder(device, local_output_path,
+                            megabits_per_second=_SPEED_INDEX_VIDEO_BITRATE):
+    # Paint the current HTML document with white so that it is not troubling the
+    # speed index measurement.
+    connection.ExecuteJavaScript("""
+      (function() {
+        requestAnimationFrame(function() {
+          var screen = window.__speedindex_screen;
+          screen.style.background = 'rgb(255, 255, 255)';
+        });
+      })();
+    """)
+    yield
+
+
+@contextlib.contextmanager
 def _DevToolsConnectionOnDevice(device, flags):
   """Returns a DevToolsConnection context manager for a given device.
 
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index b579918..ab78f8e 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -19,6 +19,8 @@ sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
+import common_util
+
 
 DEFAULT_TIMEOUT_SECONDS = 10 # seconds
 
@@ -100,7 +102,6 @@ class DevToolsConnection(object):
     self._domains_to_enable = set()
     self._tearing_down_tracing = False
     self._please_stop = False
-    self._hooks = []
     self._ws = None
     self._target_descriptor = None
 
@@ -212,14 +213,6 @@ class DevToolsConnection(object):
     assert res['result'], 'Cache clearing is not supported by this browser.'
     self.SyncRequest('Network.clearBrowserCache')
 
-  def AddHook(self, hook):
-    """Add hook to be run on monitoring start.
-
-    Args:
-      hook: a function.
-    """
-    self._hooks.append(hook)
-
   def MonitorUrl(self, url, timeout_seconds=DEFAULT_TIMEOUT_SECONDS):
     """Navigate to url and dispatch monitoring loop.
 
@@ -239,8 +232,6 @@ class DevToolsConnection(object):
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][0])
-    for hook in self._hooks:
-      hook()
     self._tearing_down_tracing = False
 
     self.SendAndIgnoreResponse('Page.navigate', {'url': url})
@@ -252,6 +243,37 @@ class DevToolsConnection(object):
     """Stops the monitoring."""
     self._please_stop = True
 
+  def ExecuteJavaScript(self, expression):
+    """Run JavaScript expression.
+
+    Args:
+      expression: JavaScript expression to run.
+
+    Returns:
+      The return value from the JavaScript expression.
+    """
+    response = self.SyncRequest('Runtime.evaluate', {
+        'expression': expression,
+        'returnByValue': True})
+    if 'error' in response:
+      raise Exception(response['error']['message'])
+    if 'wasThrown' in response['result'] and response['result']['wasThrown']:
+      raise Exception(response['error']['result']['description'])
+    if response['result']['result']['type'] == 'undefined':
+      return None
+    return response['result']['result']['value']
+
+  def PollForJavaScriptExpression(self, expression, interval):
+    """Wait until JavaScript expression is true.
+
+    Args:
+      expression: JavaScript expression to run.
+      interval: Period between expression evaluation in seconds.
+    """
+    common_util.PollFor(lambda: bool(self.ExecuteJavaScript(expression)),
+                        'JavaScript: {}'.format(expression),
+                        interval)
+
   def Close(self):
     """Cleanly close chrome by closing the only tab."""
     assert self._ws
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 50dbc23..6dbd13d 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -71,26 +71,26 @@ class LoadingTrace(object):
       return cls.FromJsonDict(json.load(input_file))
 
   @classmethod
-  def FromUrlAndController(
-      cls, url, controller, categories=None,
+  def RecordUrlNavigation(
+      cls, url, connection, chrome_metadata, categories=None,
       timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
     """Create a loading trace by using controller to fetch url.
 
     Args:
       url: (str) url to fetch.
-      controller: (ChromeControllerBase) controller to manage the connection.
+      connection: An opened devtools connection.
+      chrome_metadata: Dictionary of chrome metadata.
       categories: TracingTrack categories to capture.
       timeout_seconds: monitoring connection timeout in seconds.
 
     Returns:
       LoadingTrace instance.
     """
-    with controller.Open() as connection:
-      page = page_track.PageTrack(connection)
-      request = request_track.RequestTrack(connection)
-      trace = tracing.TracingTrack(
-          connection,
-          categories=(tracing.DEFAULT_CATEGORIES if categories is None
-                      else categories))
-      connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
-      return cls(url, controller.ChromeMetadata(), page, request, trace)
+    page = page_track.PageTrack(connection)
+    request = request_track.RequestTrack(connection)
+    trace = tracing.TracingTrack(
+        connection,
+        categories=(tracing.DEFAULT_CATEGORIES if categories is None
+                    else categories))
+    connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
+    return cls(url, chrome_metadata, page, request, trace)
diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
index ee37583..e30232c 100644
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -7,10 +7,23 @@
 python pull_sandwich_metrics.py -h
 """
 
-import json
+import collections
 import logging
 import os
+import shutil
 import sys
+import tempfile
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
+from chrome_telemetry_build import chromium_config
+
+sys.path.append(chromium_config.GetTelemetryDir())
+from telemetry.internal.image_processing import video
+from telemetry.util import image_util
+from telemetry.util import rgba_color
 
 import loading_trace as loading_trace_module
 import tracing
@@ -24,10 +37,20 @@ CSV_FIELD_NAMES = [
     'total_load',
     'onload',
     'browser_malloc_avg',
-    'browser_malloc_max']
+    'browser_malloc_max',
+    'speed_index']
 
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
+# Points of a completeness record.
+#
+# Members:
+#   |time| is in milliseconds,
+#   |frame_completeness| value representing how complete the frame is at a given
+#     |time|. Caution: this completeness might be negative.
+CompletenessPoint = collections.namedtuple('CompletenessPoint',
+    ('time', 'frame_completeness'))
+
 
 def _GetBrowserPID(tracing_track):
   """Get the browser PID from a trace.
@@ -136,6 +159,65 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   }
 
 
+def _ExtractCompletenessRecordFromVideo(video_path):
+  """Extracts the completeness record from a video.
+
+  The video must start with a filled rectangle of orange (RGB: 222, 100, 13), to
+  give the view-port size/location from where to compute the completeness.
+
+  Args:
+    video_path: Path of the video to extract the completeness list from.
+
+  Returns:
+    list(CompletenessPoint)
+  """
+  video_file = tempfile.NamedTemporaryFile()
+  shutil.copy(video_path, video_file.name)
+  video_capture = video.Video(video_file)
+
+  histograms = [
+      (time, image_util.GetColorHistogram(
+          image, ignore_color=rgba_color.WHITE, tolerance=8))
+      for time, image in video_capture.GetVideoFrameIter()
+  ]
+
+  start_histogram = histograms[1][1]
+  final_histogram = histograms[-1][1]
+  total_distance = start_histogram.Distance(final_histogram)
+
+  def FrameProgress(histogram):
+    if total_distance == 0:
+      if histogram.Distance(final_histogram) == 0:
+        return 1.0
+      else:
+        return 0.0
+    return 1 - histogram.Distance(final_histogram) / total_distance
+
+  return [(time, FrameProgress(hist)) for time, hist in histograms]
+
+
+def ComputeSpeedIndex(completeness_record):
+  """Computes the speed-index from a completeness record.
+
+  Args:
+    completeness_record: list(CompletenessPoint)
+
+  Returns:
+    Speed-index value.
+  """
+  speed_index = 0.0
+  last_time = completeness_record[0][0]
+  last_completness = completeness_record[0][1]
+  for time, completeness in completeness_record:
+    if time < last_time:
+      raise ValueError('Completeness record must be sorted by timestamps.')
+    elapsed = time - last_time
+    speed_index += elapsed * (1.0 - last_completness)
+    last_time = time
+    last_completness = completeness
+  return speed_index
+
+
 def PullMetricsFromOutputDirectory(output_directory_path):
   """Pulls all the metrics from all the traces of a sandwich run directory.
 
@@ -147,10 +229,6 @@ def PullMetricsFromOutputDirectory(output_directory_path):
     List of dictionaries with all CSV_FIELD_NAMES's field set.
   """
   assert os.path.isdir(output_directory_path)
-  run_infos = None
-  with open(os.path.join(output_directory_path, 'run_infos.json')) as f:
-    run_infos = json.load(f)
-  assert run_infos
   metrics = []
   for node_name in os.listdir(output_directory_path):
     if not os.path.isdir(os.path.join(output_directory_path, node_name)):
@@ -159,15 +237,22 @@ def PullMetricsFromOutputDirectory(output_directory_path):
       page_id = int(node_name)
     except ValueError:
       continue
-    trace_path = os.path.join(output_directory_path, node_name, 'trace.json')
+    run_path = os.path.join(output_directory_path, node_name)
+    trace_path = os.path.join(run_path, 'trace.json')
     if not os.path.isfile(trace_path):
       continue
     logging.info('processing \'%s\'' % trace_path)
     loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
-    trace_metrics = _PullMetricsFromLoadingTrace(loading_trace)
-    trace_metrics['id'] = page_id
-    trace_metrics['url'] = run_infos['urls'][page_id]
-    metrics.append(trace_metrics)
+    row_metrics = {key: 'unavailable' for key in CSV_FIELD_NAMES}
+    row_metrics.update(_PullMetricsFromLoadingTrace(loading_trace))
+    row_metrics['id'] = page_id
+    row_metrics['url'] = loading_trace.url
+    video_path = os.path.join(run_path, 'video.mp4')
+    if os.path.isfile(video_path):
+      logging.info('processing \'%s\'' % video_path)
+      completeness_record = _ExtractCompletenessRecordFromVideo(video_path)
+      row_metrics['speed_index'] = ComputeSpeedIndex(completeness_record)
+    metrics.append(row_metrics)
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
   return metrics
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
index 518607c..31519af 100644
--- a/loading/pull_sandwich_metrics_unittest.py
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -194,10 +194,41 @@ class PageTrackTest(unittest.TestCase):
     self.assertEquals(30971, metrics['browser_malloc_avg'])
     self.assertEquals(55044, metrics['browser_malloc_max'])
 
+  def testComputeSpeedIndex(self):
+    def point(time, frame_completeness):
+      return puller.CompletenessPoint(time=time,
+                                      frame_completeness=frame_completeness)
+    completness_record = [
+      point(0, 0.0),
+      point(120, 0.4),
+      point(190, 0.75),
+      point(280, 1.0),
+      point(400, 1.0),
+    ]
+    self.assertEqual(120 + 70 * 0.6 + 90 * 0.25,
+                     puller.ComputeSpeedIndex(completness_record))
+
+    completness_record = [
+      point(70, 0.0),
+      point(150, 0.3),
+      point(210, 0.6),
+      point(220, 0.9),
+      point(240, 1.0),
+    ]
+    self.assertEqual(80 + 60 * 0.7 + 10 * 0.4 + 20 * 0.1,
+                     puller.ComputeSpeedIndex(completness_record))
+
+    completness_record = [
+      point(90, 0.0),
+      point(200, 0.6),
+      point(150, 0.3),
+      point(230, 1.0),
+    ]
+    with self.assertRaises(ValueError):
+      puller.ComputeSpeedIndex(completness_record)
+
   def testCommandLine(self):
     tmp_dir = tempfile.mkdtemp()
-    with open(os.path.join(tmp_dir, 'run_infos.json'), 'w') as out_file:
-      json.dump({'urls': ['a.com', 'b.com', 'c.org']}, out_file)
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
       LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 1739578..c989b6f 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -146,6 +146,9 @@ class SandwichRunner(object):
     # List of urls to run.
     self.urls = _ReadUrlsFromJobDescription(job_name)
 
+    # Configures whether to record speed-index video.
+    self.record_video = False
+
     # Path to the WPR archive to load or save. Is str or None.
     self.wpr_archive_path = None
 
@@ -199,23 +202,50 @@ class SandwichRunner(object):
       return self.network_condition
     return None
 
-  def _RunNavigation(self, url, clear_cache, trace_id=None):
-    self._chrome_ctl.SetClearCache(clear_cache)
+  def _RunNavigation(self, url, clear_cache, run_id=None):
+    """Run a page navigation to the given URL.
+
+    Args:
+      url: The URL to navigate to.
+      clear_cache: Whether if the cache should be cleared before navigation.
+      run_id: Id of the run in the output directory. If it is None, then no
+        trace or video will be saved.
+    """
+    run_path = None
+    if self.trace_output_directory is not None and run_id is not None:
+      run_path = os.path.join(self.trace_output_directory, str(run_id))
+      if os.path.isdir(run_path):
+        os.makedirs(run_path)
     self._chrome_ctl.SetNetworkEmulation(
         self._GetEmulatorNetworkCondition('browser'))
     # TODO(gabadie): add a way to avoid recording a trace.
-    trace = loading_trace.LoadingTrace.FromUrlAndController(
-        url=url,
-        controller=self._chrome_ctl,
-        categories=pull_sandwich_metrics.CATEGORIES,
-        timeout_seconds=_DEVTOOLS_TIMEOUT)
-    if trace_id != None and self.trace_output_directory:
-      trace_path = os.path.join(
-          self.trace_output_directory, str(trace_id), 'trace.json')
-      os.makedirs(os.path.dirname(trace_path))
+    with self._chrome_ctl.Open() as connection:
+      if clear_cache:
+        connection.ClearCache()
+      if run_path is not None and self.record_video:
+        device = self._chrome_ctl.GetDevice()
+        assert device, 'Can only record video on a remote device.'
+        video_recording_path = os.path.join(run_path, 'video.mp4')
+        with device_setup.RemoteSpeedIndexRecorder(device, connection,
+                                                   video_recording_path):
+          trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+              url=url,
+              connection=connection,
+              chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+              categories=pull_sandwich_metrics.CATEGORIES,
+              timeout_seconds=_DEVTOOLS_TIMEOUT)
+      else:
+        trace = loading_trace.LoadingTrace.RecordUrlNavigation(
+            url=url,
+            connection=connection,
+            chrome_metadata=self._chrome_ctl.ChromeMetadata(),
+            categories=pull_sandwich_metrics.CATEGORIES,
+            timeout_seconds=_DEVTOOLS_TIMEOUT)
+    if run_path is not None:
+      trace_path = os.path.join(run_path, 'trace.json')
       trace.ToJsonFile(trace_path)
 
-  def _RunUrl(self, url, trace_id=0):
+  def _RunUrl(self, url, run_id):
     clear_cache = False
     if self.cache_operation == 'clear':
       clear_cache = True
@@ -224,8 +254,8 @@ class SandwichRunner(object):
     elif self.cache_operation == 'reload':
       self._RunNavigation(url, clear_cache=True)
     elif self.cache_operation == 'save':
-      clear_cache = trace_id == 0
-    self._RunNavigation(url, clear_cache=clear_cache, trace_id=trace_id)
+      clear_cache = run_id == 0
+    self._RunNavigation(url, clear_cache=clear_cache, run_id=run_id)
 
   def _PullCacheFromDevice(self):
     assert self.cache_operation == 'save'
@@ -264,7 +294,7 @@ class SandwichRunner(object):
         ):
       for _ in xrange(self.job_repeat):
         for url in self.urls:
-          self._RunUrl(url, trace_id=len(ran_urls))
+          self._RunUrl(url, run_id=len(ran_urls))
           ran_urls.append(url)
 
     if self._local_cache_directory_path:
@@ -350,6 +380,9 @@ def _ArgumentParser():
           ' to be set.')
   run_parser.add_argument('--job-repeat', default=1, type=int,
                           help='How many times to run the job.')
+  run_parser.add_argument('--record-video', action='store_true',
+                          help='Configures either to record or not a video of '
+                              +'chrome loading the web pages.')
   run_parser.add_argument('--wpr-archive', default=None, type=str,
                           dest='wpr_archive_path',
                           help='Web page replay archive to load job\'s urls ' +
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index 17c7560..aad36c1 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -229,9 +229,12 @@ def RunTest(webserver, test_page, expected):
   url = 'http://%s/%s' % (webserver.Address(), test_page)
   sys.stdout.write('Testing %s...' % url)
   chrome_controller = controller.LocalChromeController()
-  chrome_controller.SetClearCache()
-  observed_seq = InitiatorSequence(
-      loading_trace.LoadingTrace.FromUrlAndController(url, chrome_controller))
+
+  with chrome_controller.Open() as connection:
+    connection.ClearCache()
+    observed_seq = InitiatorSequence(
+        loading_trace.LoadingTrace.RecordUrlNavigation(
+            url, connection, chrome_controller.ChromeMetadata()))
   if observed_seq == expected:
     sys.stdout.write(' ok\n')
     return True

commit 95080f4fe2de285a508e38fa00d447e8568e55bc
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 07:38:24 2016 -0700

    Clovis: stopgap for webserver test race.
    
    Return test server files in single chunk rather than as individual bytes. This
    affects what seems to be an internal race in chrome or chrome devtools that
    causes different reported initiators.
    
    Review URL: https://codereview.chromium.org/1824343002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382846}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 710720cfe33cf8135aa539e949736cc88357cb89

diff --git a/loading/trace_test/test_server.py b/loading/trace_test/test_server.py
index 5463667..45517f7 100755
--- a/loading/trace_test/test_server.py
+++ b/loading/trace_test/test_server.py
@@ -86,7 +86,7 @@ class ServerApp(object):
       for header in self._response_headers[path]:
         headers.append((str(header[0]), str(header[1])))
     start_response('200 OK', headers)
-    return file(filename).read()
+    return [file(filename).read()]
 
 
 if __name__ == '__main__':

commit 6598f4ed6a8d662d57455d5853d068f6c0bca96f
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 05:21:35 2016 -0700

    Clovis: Update webtest to new controller to make it run.
    
    This incorporates the controller refactor, which incidentally fixes some
    bitrotted code as well as adds --test_filter to make debugging a specific test
    easier. Also updates a test which had missed comments shifting around.
    
    1.html is still flaky, I'm looking into it.
    
    Review URL: https://codereview.chromium.org/1829633002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382832}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b9f18314c220fec5b9b91409db88556234462f9b

diff --git a/loading/analyze.py b/loading/analyze.py
index 2aed605..58a4ace 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -110,6 +110,7 @@ def _LogRequests(url, clear_cache_override=None):
   """
   if OPTIONS.local:
     chrome_ctl = controller.LocalChromeController()
+    chrome_ctl.SetHeadless(OPTIONS.headless)
   else:
     chrome_ctl = controller.RemoteChromeController(
         device_setup.GetFirstDevice())
diff --git a/loading/controller.py b/loading/controller.py
index 1a55085..7aa9bde 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -74,6 +74,11 @@ class ChromeControllerBase(object):
     self._chrome_args.append(arg)
 
   def SetClearCache(self, clear_cache=True):
+    """Ensure cache is cleared before running.
+
+    Args:
+      clear_cache: true if cache should be cleared.
+    """
     self._clear_cache = clear_cache
 
   @contextlib.contextmanager
@@ -287,11 +292,20 @@ class LocalChromeController(ChromeControllerBase):
     self._using_temp_profile_dir = self._profile_dir is None
     if self._using_temp_profile_dir:
       self._profile_dir = tempfile.mkdtemp(suffix='.profile')
+    self._headless = False
 
   def __del__(self):
     if self._using_temp_profile_dir:
       shutil.rmtree(self._profile_dir)
 
+  def SetHeadless(self, headless=True):
+    """Set a headless run.
+
+    Args:
+      headless: true if the chrome instance should be headless.
+    """
+    self._headless = headless
+
   @contextlib.contextmanager
   def Open(self):
     """Override for connection context."""
@@ -305,7 +319,7 @@ class LocalChromeController(ChromeControllerBase):
     chrome_cmd.append('about:blank')
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     environment = os.environ.copy()
-    if OPTIONS.headless:
+    if self._headless:
       environment['DISPLAY'] = 'localhost:99'
       xvfb_process = subprocess.Popen(
           ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
@@ -331,7 +345,7 @@ class LocalChromeController(ChromeControllerBase):
     finally:
       if connection:
         chrome_process.kill()
-      if OPTIONS.headless:
+      if self._headless:
         xvfb_process.kill()
 
   def PushBrowserCache(self, cache_path):
diff --git a/loading/trace_test/results/3.result b/loading/trace_test/results/3.result
index bcebf63..196a88d 100644
--- a/loading/trace_test/results/3.result
+++ b/loading/trace_test/results/3.result
@@ -1,6 +1,6 @@
 parser (no stack) 3a.js
 parser (no stack) 3c.js
-script (3a.js:10/3a.js:14/3.html:23) 3a.jpg
+script (3a.js:10/3a.js:14/3.html:20) 3a.jpg
 script (3a.js:20) 3b.js
 script (3b.js:9) 3b.jpg
-script (3c.js:7/3.html:24) 3c.jpg
+script (3c.js:7/3.html:21) 3c.jpg
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index 800b8d6..17c7560 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -41,7 +41,7 @@ import urlparse
 
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 
-from device_setup import DeviceConnection
+import controller
 import loading_trace
 import options
 import trace_recorder
@@ -211,7 +211,7 @@ class InitiatorSequence(object):
     return short
 
 
-def RunTest(webserver, connection, test_page, expected):
+def RunTest(webserver, test_page, expected):
   """Run an webserver test.
 
   The expected result can be None, in which case --failed_trace_dir can be set
@@ -220,7 +220,6 @@ def RunTest(webserver, connection, test_page, expected):
   Args:
     webserver [WebServer]: the webserver to use for the test. It must be
       started.
-    connection [DevToolsConnection]: the connection to trace against.
     test_page: the name of the page to load.
     expected [InitiatorSequence]: expected initiator sequence.
 
@@ -229,8 +228,10 @@ def RunTest(webserver, connection, test_page, expected):
   """
   url = 'http://%s/%s' % (webserver.Address(), test_page)
   sys.stdout.write('Testing %s...' % url)
-  observed_seq = InitiatorSequence(trace_recorder.MonitorUrl(
-      connection, url, clear_cache=True))
+  chrome_controller = controller.LocalChromeController()
+  chrome_controller.SetClearCache()
+  observed_seq = InitiatorSequence(
+      loading_trace.LoadingTrace.FromUrlAndController(url, chrome_controller))
   if observed_seq == expected:
     sys.stdout.write(' ok\n')
     return True
@@ -251,12 +252,16 @@ def RunAllTests():
   All tests must have a corresponding result in RESULTDIR unless
   --failed_trace_dir is set.
   """
+  test_filter = set(OPTIONS.test_filter.split(',')) \
+      if OPTIONS.test_filter else None
+
   with TemporaryDirectory() as temp_dir, \
-       WebServer.Context(TESTDIR, temp_dir) as webserver, \
-       DeviceConnection(None) as connection:
+       WebServer.Context(TESTDIR, temp_dir) as webserver:
     failure = False
     for test in sorted(os.listdir(TESTDIR)):
       if test.endswith('.html'):
+        if test_filter and test not in test_filter:
+          continue
         result = os.path.join(RESULTDIR, test[:test.rfind('.')] + '.result')
         assert OPTIONS.failed_trace_dir or os.path.exists(result), \
             'No result found for test'
@@ -264,7 +269,7 @@ def RunAllTests():
         if os.path.exists(result):
           with file(result) as result_file:
             expected = InitiatorSequence.ReadFromFile(result_file)
-        if not RunTest(webserver, connection, test, expected):
+        if not RunTest(webserver, test, expected):
           failure = True
   if failure:
     print 'FAILED!'
@@ -276,5 +281,6 @@ if __name__ == '__main__':
   OPTIONS.ParseArgs(sys.argv[1:],
                     description='Run webserver integration test',
                     extra=[('--failed_trace_dir', ''),
-                           ('--noisy', False)])
+                           ('--noisy', False),
+                           ('--test_filter', None)])
   RunAllTests()

commit 104b1dc04ac92a85f172f36435cdff59f738c435
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 23 03:47:37 2016 -0700

    Clovis: Improve --local_binary option.
    
    Make --local_binary smart enough to be relative to code directory no matter where a script is being run from.
    
    Review URL: https://codereview.chromium.org/1825333002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382823}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 23f2d9a9a21c608c5008e8cb0e00d0dcf8d0647c

diff --git a/loading/options.py b/loading/options.py
index 0490ac5..021377c 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -26,7 +26,7 @@ class Options(object):
              'hostname for devtools websocket connection'),
             ('devtools_port', 9222,
              'port for devtools websocket connection'),
-            ('local_binary', 'out/Release/chrome',
+            ('local_binary', os.path.join(_SRC_DIR, 'out/Release/chrome'),
              'chrome binary for local runs'),
             ('local_noisy', False,
              'Enable local chrome console output'),

commit 29d75b6b48431a4f57000ce12cba70fb0a9c3540
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 22 12:15:31 2016 -0700

    tools/android/loading: Launch chrome on devices more reliably.
    
    Currently, we are giving only 2s for chrome to start its devtools
    HTTP server causing flakyness. This CL attempts to connect to
    chrome's devtools every one seconds instead.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1825123002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382628}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a8893e338e26dd9dfcf9c00f2635a025759bd16f

diff --git a/loading/controller.py b/loading/controller.py
index 25fefae..1a55085 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -14,6 +14,7 @@ import datetime
 import logging
 import os
 import shutil
+import socket
 import subprocess
 import sys
 import tempfile
@@ -33,10 +34,6 @@ from devil.android.sdk import intent
 
 OPTIONS = options.OPTIONS
 
-# An estimate of time to wait for the device to become idle after expensive
-# operations, such as opening the launcher activity.
-_TIME_TO_DEVICE_IDLE_SECONDS = 2
-
 
 class ChromeControllerBase(object):
   """Base class for all controllers.
@@ -193,8 +190,15 @@ class ChromeControllerBase(object):
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
-  # Seconds to sleep after starting chrome activity.
-  POST_ACTIVITY_SLEEP_SECONDS = 2
+  # Number of connection attempt to chrome's devtools.
+  DEVTOOLS_CONNECTION_ATTEMPTS = 10
+
+  # Time interval in seconds between chrome's devtools connection attempts.
+  DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS = 1
+
+  # An estimate of time to wait for the device to become idle after expensive
+  # operations, such as opening the launcher activity.
+  TIME_TO_IDLE_SECONDS = 2
 
   def __init__(self, device):
     """Initialize the controller.
@@ -203,38 +207,50 @@ class RemoteChromeController(ChromeControllerBase):
       device: an andriod device.
     """
     assert device is not None, 'Should you be using LocalController instead?'
-    self._device = device
     super(RemoteChromeController, self).__init__()
+    self._device = device
+    self._device.EnableRoot()
 
   @contextlib.contextmanager
   def Open(self):
     """Overridden connection creation."""
     package_info = OPTIONS.ChromePackage()
     command_line_path = '/data/local/chrome-command-line'
-
-    self._device.EnableRoot()
     self._device.KillAll(package_info.package, quiet=True)
-
+    chrome_args = self._GetChromeArguments()
+    logging.info('Launching %s with flags: %s' % (package_info.package,
+        subprocess.list2cmdline(chrome_args)))
     with device_setup.FlagReplacer(
         self._device, command_line_path, self._GetChromeArguments()):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
       self._device.StartActivity(start_intent, blocking=True)
-      time.sleep(self.POST_ACTIVITY_SLEEP_SECONDS)
-      with device_setup.ForwardPort(
-          self._device, 'tcp:%d' % OPTIONS.devtools_port,
-          'localabstract:chrome_devtools_remote'):
-        connection = devtools_monitor.DevToolsConnection(
-            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-        self._StartConnection(connection)
-        yield connection
-    if self._slow_death:
-      self._device.adb.Shell('am start com.google.android.launcher')
-      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    self._device.KillAll(package_info.package, quiet=True)
+      try:
+        for attempt_id in xrange(self.DEVTOOLS_CONNECTION_ATTEMPTS + 1):
+          if attempt_id == self.DEVTOOLS_CONNECTION_ATTEMPTS:
+            raise RuntimeError('Failed to connect to chrome devtools after {} '
+                               'attempts.'.format(attempt_id))
+          logging.info('Devtools connection attempt %d' % attempt_id)
+          with device_setup.ForwardPort(
+              self._device, 'tcp:%d' % OPTIONS.devtools_port,
+              'localabstract:chrome_devtools_remote'):
+            try:
+              connection = devtools_monitor.DevToolsConnection(
+                  OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+              self._StartConnection(connection)
+            except socket.error as e:
+              assert str(e).startswith('[Errno 104] Connection reset by peer')
+              time.sleep(self.DEVTOOLS_CONNECTION_ATTEMPT_INTERVAL_SECONDS)
+              continue
+            logging.info('Devtools connection success')
+            yield connection
+            if self._slow_death:
+              self._device.adb.Shell('am start com.google.android.launcher')
+              time.sleep(self.TIME_TO_IDLE_SECONDS)
+            break
+      finally:
+        self._device.KillAll(package_info.package, quiet=True)
 
   def PushBrowserCache(self, cache_path):
     """Override for chrome cache pushing."""

commit 00e15989d6cb9c5e536902972f8c7400803e9787
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 22 12:07:34 2016 -0700

    tools/android/loading: Implements CacheBackend.GetDecodedContentForKey()
    
    HTTP cache is storing into key's index stream 1 the transport layer
    resource binary. However, the resources might be encoded using a
    compression algorithm specified in the Content-Encoding response
    header. This new method takes care of returning decoded binary
    content of the resource if necessary.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1765033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382625}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f5b1e432220b3d8596912dca53bba2fef17841dd

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 1df3010..9d486e9 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -8,6 +8,7 @@
 from datetime import datetime
 import json
 import os
+import re
 import shutil
 import subprocess
 import sys
@@ -37,6 +38,13 @@ OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
 # Default cachetool binary location.
 CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
 
+# Default content_decoder_tool binary location.
+CONTENT_DECODER_TOOL_BIN_PATH = os.path.join(OUT_DIRECTORY,
+                                             'content_decoder_tool')
+
+# Regex used to parse HTTP headers line by line.
+HEADER_PARSING_REGEX = re.compile(r'^(?P<header>\S+):(?P<value>.*)$')
+
 
 def _EnsureCleanCacheDirectory(directory_dest_path):
   """Ensure that a cache directory is created and clean.
@@ -306,6 +314,42 @@ class CacheBackend(object):
     assert process.returncode == 0
     return stdout_data
 
+  def GetDecodedContentForKey(self, key):
+    """Gets a key's decoded content.
+
+    HTTP cache is storing into key's index stream 1 the transport layer resource
+    binary. However, the resources might be encoded using a compression
+    algorithm specified in the Content-Encoding response header. This method
+    takes care of returning decoded binary content of the resource.
+
+    Args:
+      key: The key to access the decoded content.
+
+    Returns:
+      String holding binary content.
+    """
+    response_headers = self.GetStreamForKey(key, 0)
+    content_encoding = None
+    for response_header_line in response_headers.split('\n'):
+      match = HEADER_PARSING_REGEX.match(response_header_line)
+      if not match:
+        continue
+      if match.group('header').lower() == 'content-encoding':
+        content_encoding = match.group('value')
+        break
+    encoded_content = self.GetStreamForKey(key, 1)
+    if content_encoding == None:
+      return encoded_content
+
+    cmd = [CONTENT_DECODER_TOOL_BIN_PATH]
+    cmd.extend([s.strip() for s in content_encoding.split(',')])
+    process = subprocess.Popen(cmd,
+                               stdin=subprocess.PIPE,
+                               stdout=subprocess.PIPE)
+    decoded_content, _ = process.communicate(input=encoded_content)
+    assert process.returncode == 0
+    return decoded_content
+
 
 def ApplyUrlWhitelistToCacheArchive(cache_archive_path,
                                     whitelisted_urls,
@@ -333,18 +377,33 @@ def ApplyUrlWhitelistToCacheArchive(cache_archive_path,
     shutil.rmtree(cache_temp_directory)
 
 
-if __name__ == '__main__':
+def ManualTestMain():
   import argparse
   parser = argparse.ArgumentParser(description='Tests cache back-end.')
-  parser.add_argument('cache_path', type=str)
+  parser.add_argument('cache_archive_path', type=str)
   parser.add_argument('backend_type', type=str, choices=BACKEND_TYPES)
   command_line_args = parser.parse_args()
 
+  cache_path = tempfile.mkdtemp()
+  UnzipDirectoryContent(command_line_args.cache_archive_path, cache_path)
+
   cache_backend = CacheBackend(
-      cache_directory_path=command_line_args.cache_path,
+      cache_directory_path=cache_path,
       cache_backend_type=command_line_args.backend_type)
-  keys = cache_backend.ListKeys()
-  print '{}\'s HTTP response header:'.format(keys[0])
-  print cache_backend.GetStreamForKey(keys[0], 0)
+  keys = sorted(cache_backend.ListKeys())
+  selected_key = None
+  for key in keys:
+    if key.endswith('.js'):
+      selected_key = key
+      break
+  assert selected_key
+  print '{}\'s HTTP response header:'.format(selected_key)
+  print cache_backend.GetStreamForKey(selected_key, 0)
+  print cache_backend.GetDecodedContentForKey(selected_key)
   cache_backend.DeleteKey(keys[1])
   assert keys[1] not in cache_backend.ListKeys()
+  shutil.rmtree(cache_path)
+
+
+if __name__ == '__main__':
+  ManualTestMain()

commit a963dc7c135a18c0f2a7cc3b6b7efb99cd5961b0
Author: droger <droger@chromium.org>
Date:   Tue Mar 22 08:55:13 2016 -0700

    tools/android/loading Add script to deploy chrome
    
    This CL adds a deploy.sh script that packages the
    analyse.py and Chrome for deployment on GCE.
    
    It also updates the startup script and the pip requirements
    to add required dependencies.
    
    Review URL: https://codereview.chromium.org/1809133002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382578}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c84d04ac9fdf9752bd3ea79180df7ba5cca2a6cb

diff --git a/loading/gce/README.md b/loading/gce/README.md
index 6de07d2..522be17 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -21,10 +21,27 @@ When offered, accept to clone the Google Cloud repo.
 
 ## Update or Change the code
 
-Make changes to the code, or simply copy the latest version from Chromium into
-your local Google Cloud repository. Then commit and push:
+Make changes to the code, or copy the latest version from Chromium into your
+local Google Cloud repository:
 
 ```shell
+# Build Chrome
+BUILD_DIR=out/Release
+ninja -C $BUILD_DIR -j1000 -l60 chrome chrome_sandbox
+
+GCE_DIR=~/dev/clovis/default
+
+# Deploy to GCE
+# CHROME_BUCKET_NAME is the name of the Google Cloud Storage bucket where the
+# Chrome build artifacts will be uploaded, and matches the value of
+# 'bucket_name' in server_config.json.
+./tools/android/loading/gce/deploy.sh $BUILD_DIR $GCE_DIR $CHROME_BUCKET_NAME
+
+cd $GCE_DIR
+
+# git add the relevant files
+
+# commit and push:
 git commit
 git push -u origin master
 ```
@@ -43,9 +60,14 @@ gcloud compute instances create clovis-tracer-1 \
  --zone europe-west1-c \
  --tags clovis-http-server \
  --scopes cloud-platform \
- --metadata-from-file startup-script=default/startup-script.sh
+ --metadata auto-start=true \
+ --metadata-from-file startup-script=tools/android/loading/gce/startup-script.sh
 ```
 
+**Note:** To start an instance without automatically starting the app on it,
+remove the `--metadata auto-start=true` argument. This can be useful when doing
+iterative development on the instance, to be able to restart the app manually.
+
 This should output the IP address of the instance.
 Otherwise the IP address can be retrieved by doing:
 
@@ -53,10 +75,6 @@ Otherwise the IP address can be retrieved by doing:
 gcloud compute instances list
 ```
 
-TODO: allow starting the instance in the cloud without Supervisor. This enables
-iterative development on the instance using SSH, manually starting and stopping
-the app. This can be done using [instance metadata][2].
-
 ## Use the app
 
 Interact with the app on the port 8080 at `http://<instance-ip>:8080`.
@@ -67,7 +85,7 @@ To send a list of URLs to process:
 curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
 ```
 
-where `urls.txt` is a file containing URLs (one per line).
+where `urls.json` is a file containing URLs as a JSON array.
 
 Start the processing by sending a request to `http://<instance-ip>:8080/start`,
 for example:
@@ -101,10 +119,10 @@ pip install -r pip_requirements.txt
 Launch the app:
 
 ```shell
-gunicorn --workers=1 main:app --bind 127.0.0.1:8000
+gunicorn --workers=1 main:app --bind 127.0.0.1:8080
 ```
 
-In your browser, go to `http://localhost:8000` and use the app.
+In your browser, go to `http://localhost:8080` and use the app.
 
 Tear down the local environment:
 
@@ -145,4 +163,3 @@ gcloud compute firewall-rules delete default-allow-http-8080
 ```
 
 [1]: https://cloud.google.com/sdk
-[2]: https://cloud.google.com/compute/docs/startupscript#custom
diff --git a/loading/gce/deploy.sh b/loading/gce/deploy.sh
new file mode 100755
index 0000000..fad1755
--- /dev/null
+++ b/loading/gce/deploy.sh
@@ -0,0 +1,52 @@
+#!/bin/bash
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# This script copies all dependencies required for trace collection.
+# Usage:
+#   deploy.sh builddir outdir bucket
+#
+# Where:
+#   builddir is the build directory for Chrome
+#   outdir is the directory where files are deployed
+#   bucket is the Google Storage bucket where Chrome is uploaded
+
+builddir=$1
+outdir=$2
+bucket=$3
+
+# Copy files from tools/android/loading
+mkdir -p $outdir/tools/android/loading
+cp tools/android/loading/*.py $outdir/tools/android/loading
+cp -r tools/android/loading/gce $outdir/tools/android/loading
+
+# Copy other dependencies
+mkdir $outdir/third_party
+# Use rsync to exclude unwanted files (e.g. the .git directory).
+rsync -av --exclude=".*" --exclude "*.pyc" --delete \
+  third_party/catapult $outdir/third_party
+mkdir $outdir/tools/perf
+cp -r tools/perf/chrome_telemetry_build $outdir/tools/perf
+mkdir -p $outdir/build/android
+cp build/android/devil_chromium.py $outdir/build/android/
+cp build/android/devil_chromium.json $outdir/build/android/
+cp -r build/android/pylib $outdir/build/android/
+
+# Copy the chrome executable to Google Cloud Storage
+chrome/tools/build/make_zip.py $builddir chrome/tools/build/linux/FILES.cfg \
+  /tmp/linux.zip
+gsutil cp /tmp/linux.zip gs://$bucket/chrome/linux.zip
+rm /tmp/linux.zip
+gsutil cp $builddir/chrome_sandbox gs://$bucket/chrome/chrome_sandbox
+
+# Upload Chromium revision
+CHROMIUM_REV=$(git merge-base HEAD origin/master)
+cat >/tmp/build_metadata.json << EOF
+{
+  "chromium_rev": "$CHROMIUM_REV"
+}
+EOF
+gsutil cp /tmp/build_metadata.json gs://$bucket/chrome/build_metadata.json
+rm /tmp/build_metadata.json
+
diff --git a/loading/gce/pip_requirements.txt b/loading/gce/pip_requirements.txt
index e2d68b7..7d97e12 100644
--- a/loading/gce/pip_requirements.txt
+++ b/loading/gce/pip_requirements.txt
@@ -1,2 +1,3 @@
 gunicorn==19.4.5
 gcloud==0.10.1
+psutil==4.1.0
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index b6ddde1..dcad7f2 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -14,7 +14,16 @@ PROJECTID=$(curl -s \
 
 # Install dependencies from apt
 apt-get update
-apt-get install -yq git supervisor python-pip python-dev libffi-dev libssl-dev
+# Basic dependencies
+apt-get install -yq git supervisor python-pip python-dev unzip
+# Web server dependencies
+apt-get install -yq libffi-dev libssl-dev
+# Chrome dependencies
+apt-get install -yq libpangocairo-1.0-0 libXcomposite1 libXcursor1 libXdamage1 \
+    libXi6 libXtst6 libnss3 libcups2 libgconf2-4 libXss1 libXrandr2 \
+    libatk1.0-0 libasound2 libgtk2.0-0
+# Trace collection dependencies
+apt-get install -yq xvfb
 
 # Create a pythonapp user. The application will run as this user.
 useradd -m -d /home/pythonapp pythonapp
@@ -22,24 +31,59 @@ useradd -m -d /home/pythonapp pythonapp
 # pip from apt is out of date, so make it update itself and install virtualenv.
 pip install --upgrade pip virtualenv
 
-# Get the source code from the Google Cloud Repository
+# Get the source code from the Google Cloud Repository.
+# It is expected that the contents of this repository have been generated using
+# the tools/android/loading/gce/deploy.sh script.
 # git requires $HOME and it's not set during the startup script.
 export HOME=/root
 git config --global credential.helper gcloud.sh
-git clone https://source.developers.google.com/p/$PROJECTID /opt/app/clovis
+git clone --depth 1 https://source.developers.google.com/p/$PROJECTID \
+    /opt/app/clovis
 
 # Install app dependencies
 virtualenv /opt/app/clovis/env
-/opt/app/clovis/env/bin/pip install -r /opt/app/clovis/pip_requirements.txt
+/opt/app/clovis/env/bin/pip install \
+    -r /opt/app/clovis/tools/android/loading/gce/pip_requirements.txt
+
+# Download Chrome from Google Cloud Storage and unzip it.
+# It is expected that the contents of the bucket have been generated using the
+# tools/android/loading/gce/deploy.sh script.
+STORAGE_BUCKET=`python - <<EOF
+import json
+config_file = "/opt/app/clovis/tools/android/loading/gce/server_config.json"
+with open(config_file) as config:
+  obj=json.load(config);
+  print obj["bucket_name"]
+EOF`
+
+mkdir /opt/app/clovis/out
+gsutil cp gs://$STORAGE_BUCKET/chrome/* /opt/app/clovis/out/
+unzip /opt/app/clovis/out/linux.zip -d /opt/app/clovis/out/
+
+# Install the Chrome sandbox
+cp /opt/app/clovis/out/chrome_sandbox /usr/local/sbin/chrome-devel-sandbox
+chown root:root /usr/local/sbin/chrome-devel-sandbox
+chmod 4755 /usr/local/sbin/chrome-devel-sandbox
+export CHROME_DEVEL_SANDBOX=/usr/local/sbin/chrome-devel-sandbox
 
 # Make sure the pythonapp user owns the application code
 chown -R pythonapp:pythonapp /opt/app
 
+# Check if auto-start is enabled
+AUTO_START=$(curl -s \
+    "http://metadata/computeMetadata/v1/instance/attributes/auto-start" \
+    -H "Metadata-Flavor: Google")
+
+# Exit early if auto start is not enabled.
+if [-z "$AUTO_START"]; then
+  exit 1
+fi
+
 # Configure supervisor to start gunicorn inside of our virtualenv and run the
 # applicaiton.
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
-directory=/opt/app/clovis
+directory=/opt/app/clovis/tools/android/loading/gce
 command=/opt/app/clovis/env/bin/gunicorn --workers=1 main:app \
   --bind 0.0.0.0:8080
 autostart=true
@@ -47,7 +91,7 @@ autorestart=true
 user=pythonapp
 # Environment variables ensure that the application runs inside of the
 # configured virtualenv.
-environment=VIRTUAL_ENV="/opt/app/env/clovis",PATH="/opt/app/clovis/env/bin",\
+environment=VIRTUAL_ENV="/opt/app/clovis/env",PATH="/opt/app/clovis/env/bin",\
     HOME="/home/pythonapp",USER="pythonapp"
 stdout_logfile=syslog
 stderr_logfile=syslog

commit 9fc49a72b0f12a31992b5170838133fe15fbba35
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 22 04:40:44 2016 -0700

    Clovis: rewrite user satisfaction lens.
    
    This rewrites the existing user satisfaction lens to output a list of request
    ids rather than the old graph filter approach. In addition, we create a
    hierarchy of lenses for all of the various first paint metrics.
    
    Review URL: https://codereview.chromium.org/1814973004
    
    Cr-Original-Commit-Position: refs/heads/master@{#382548}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 49220fb35e2fc3ae4ce364bef2405e0e029e5fa5

diff --git a/loading/tracing.py b/loading/tracing.py
index 8a07a2b..5d0a647 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -366,6 +366,21 @@ class Event(object):
     return ''.join([str(self._tracing_event),
                     '[%s,%s]' % (self.start_msec, self.end_msec)])
 
+  def Matches(self, category, name):
+    """Match tracing events.
+
+    Args:
+      category: a tracing category (event['cat']).
+      name: the tracing event name (event['name']).
+
+    Returns:
+      True if the event matches and False otherwise.
+    """
+    if name != self.name:
+      return False
+    categories = self.category.split(',')
+    return category in categories
+
   def IsIndexable(self):
     """True iff the event can be indexed by time."""
     return self._synthetic or self.type not in [
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index a3bea3f..1bf0273 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -382,6 +382,23 @@ class IntervalTreeTestCase(unittest.TestCase):
     self.assertEquals(3 + 10, len(tree.OverlappingEvents(450, 550)))
     self.assertEquals(8 + 10, len(tree.OverlappingEvents(450, 800)))
 
+  def testEventMatches(self):
+    event = Event({'name': 'foo',
+                   'cat': 'bar',
+                   'ph': 'X',
+                   'ts': 0, 'dur': 0})
+    self.assertTrue(event.Matches('bar', 'foo'))
+    self.assertFalse(event.Matches('bar', 'biz'))
+    self.assertFalse(event.Matches('biz', 'foo'))
+
+    event = Event({'name': 'foo',
+                   'cat': 'bar,baz,bizbiz',
+                   'ph': 'X',
+                   'ts': 0, 'dur': 0})
+    self.assertTrue(event.Matches('bar', 'foo'))
+    self.assertTrue(event.Matches('baz', 'foo'))
+    self.assertFalse(event.Matches('bar', 'biz'))
+    self.assertFalse(event.Matches('biz', 'foo'))
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
index 0b23342..0115f8c 100644
--- a/loading/user_satisfied_lens.py
+++ b/loading/user_satisfied_lens.py
@@ -2,50 +2,136 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Identifies a user satisfied event, and marks all the relationship of all
-model events accordingly.
-"""
+"""Identifies key events related to user satisfaction.
 
+Several lenses are defined, for example FirstTextPaintLens and
+FirstSignificantPaintLens.
+"""
 import logging
+import operator
+
 
+class _UserSatisfiedLens(object):
+  """A base class for all user satisfaction metrics.
 
-class UserSatisfiedLens(object):
-  def __init__(self, trace, graph):
+  All of these work by identifying a user satisfaction event from the trace, and
+  then building a set of request ids whose loading is needed to achieve that
+  event. Subclasses need only provide the time computation. The base class will
+  use that to construct the request ids.
+  """
+  def __init__(self, trace):
     """Initialize the lens.
 
     Args:
       trace: (LoadingTrace) the trace to use in the analysis.
-      graph: (ResourceGraph) the graph to annotate, using the current filter set
-        on the graph.
     """
-    satisfied_time = self._GetFirstContentfulPaintTime(trace.tracing_track)
-    self._satisfied_nodes = set(self._GetNodesAfter(
-        graph.Nodes(), satisfied_time))
+    self._satisfied_msec = None
+    self._event_msec = None
+    self._CalculateTimes(trace.tracing_track)
+    critical_requests = self._RequestsBefore(
+        trace.request_track, self._satisfied_msec)
+    self._critical_request_ids = set(rq.request_id for rq in critical_requests)
+    if critical_requests:
+      last_load = max(rq.end_msec for rq in critical_requests)
+    else:
+      last_load = float('inf')
+    self._postload_msec = self._event_msec - last_load
 
-  def Filter(self, node):
-    """A ResourceGraph filter.
+  def CriticalRequests(self):
+    """Request ids of critical requests.
 
-    Meant to be used in some_graph.Set(node_filter=this_lens.Filter).
+    Returns:
+      A set of request ids (as strings) of an estimate of all requests that are
+      necessary for the user satisfaction defined by this class.
+    """
+    return self._critical_request_ids
 
-    Args:
-      node: a ResourceGraph NodeInfo.
+  def PostloadTimeMsec(self):
+    """Return postload time.
+
+    The postload time is an estimate of the amount of time needed by chrome to
+    transform the critical results into the satisfying event.
 
     Returns:
-      True iff the node occurred before user satisfaction occurred.
+      Postload time in milliseconds.
     """
-    return node not in self._satisfied_nodes
+    return self._postload_msec
+
+  def _CalculateTimes(self, tracing_track):
+    """Subclasses should implement to set _satisfied_msec and _event_msec."""
+    raise NotImplementedError
+
+  @classmethod
+  def _RequestsBefore(cls, request_track, time_ms):
+    return [rq for rq in request_track.GetEvents()
+            if rq.end_msec <= time_ms]
+
+
+class _FirstEventLens(_UserSatisfiedLens):
+  """Helper abstract subclass that defines users first event manipulations."""
+  # pylint can't handle abstract subclasses.
+  # pylint: disable=abstract-method
 
-  # TODO(mattcary): hoist to base class?
   @classmethod
-  def _GetNodesAfter(cls, nodes, time):
-    return (n for n in nodes if n.StartTime() >= time)
+  def _ExtractFirstTiming(cls, times):
+    if not times:
+      return float('inf')
+    if len(times) != 1:
+      # TODO(mattcary): in some cases a trace has two first paint events. Why?
+      logging.error('%d %s with spread of %s', len(times),
+                    str(cls), max(times) - min(times))
+    return float(min(times))
+
+
+class FirstTextPaintLens(_FirstEventLens):
+  """Define satisfaction by the first text paint.
 
-  def _GetFirstContentfulPaintTime(self, tracing_track):
+  This event is taken directly from a trace.
+  """
+  def _CalculateTimes(self, tracing_track):
     first_paints = [e.start_msec for e in tracing_track.GetEvents()
-                    if (e.tracing_event['name'] == 'firstContentfulPaint' and
-                        'blink.user_timing' in e.tracing_event['cat'])]
-    if len(first_paints) != 1:
-      # TODO(mattcary): in some cases a trace has two contentful paints. Why?
-      logging.error('%d first paints with spread of %s', len(first_paints),
-                    max(first_paints) - min(first_paints))
-    return min(first_paints)
+                    if e.Matches('blink.user_timing', 'firstPaint')]
+    self._satisfied_msec = self._event_msec = \
+        self._ExtractFirstTiming(first_paints)
+
+
+class FirstContentfulPaintLens(_FirstEventLens):
+  """Define satisfaction by the first contentful paint.
+
+  This event is taken directly from a trace. Internally to chrome it's computed
+  by filtering out things like background paint from firstPaint.
+  """
+  def _CalculateTimes(self, tracing_track):
+    first_paints = [e.start_msec for e in tracing_track.GetEvents()
+                    if e.Matches('blink.user_timing', 'firstContentfulPaint')]
+    self._satisfied_msec = self._event_msec = \
+       self._ExtractFirstTiming(first_paints)
+
+
+class FirstSignificantPaintLens(_FirstEventLens):
+  """Define satisfaction by the first paint after a big layout change.
+
+  Our satisfaction time is that of the layout change, as all resources must have
+  been loaded to compute the layout. Our event time is that of the next paint as
+  that is the observable event.
+  """
+  FIRST_LAYOUT_COUNTER = 'LayoutObjectsThatHadNeverHadLayout'
+
+  def _CalculateTimes(self, tracing_track):
+    sync_paint_times = []
+    layouts = []  # (layout item count, msec).
+    for e in tracing_track.GetEvents():
+      # TODO(mattcary): is this the right paint event? Check if synchronized
+      # paints appear at the same time as the first*Paint events, above.
+      if e.Matches('blink', 'FrameView::SynchronizedPaint'):
+        sync_paint_times.append(e.start_msec)
+      if ('counters' in e.args and
+          self.FIRST_LAYOUT_COUNTER in e.args['counters']):
+        layouts.append((e.args['counters'][self.FIRST_LAYOUT_COUNTER],
+                        e.start_msec))
+    assert layouts, ('No layout events, was the disabled-by-default-blink'
+                     '.debug.layout category enabled?')
+    layouts.sort(key=operator.itemgetter(0), reverse=True)
+    self._satisfied_msec = layouts[0][1]
+    self._event_msec = self._ExtractFirstTiming([
+        min(t for t in sync_paint_times if t > self._satisfied_msec)])
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
index 7b16f23..29cf3d5 100644
--- a/loading/user_satisfied_lens_unittest.py
+++ b/loading/user_satisfied_lens_unittest.py
@@ -4,12 +4,15 @@
 
 import unittest
 
-import loading_model
 import request_track
 import test_utils
 import user_satisfied_lens
 
 class UserSatisfiedLensTestCase(unittest.TestCase):
+  # We track all times in milliseconds, but raw trace events are in
+  # microseconds.
+  MILLI_TO_MICRO = 1000
+
   def setUp(self):
     super(UserSatisfiedLensTestCase, self).setUp()
     self._request_index = 1
@@ -18,7 +21,7 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     timestamp_sec = float(timestamp_msec) / 1000
     rq = request_track.Request.FromJsonDict({
         'url': 'http://bla-%s-.com' % timestamp_msec,
-        'request_id': '1234.%s' % self._request_index,
+        'request_id': '0.%s' % self._request_index,
         'frame_id': '123.%s' % timestamp_msec,
         'initiator': {'type': 'other'},
         'timestamp': timestamp_sec,
@@ -29,28 +32,86 @@ class UserSatisfiedLensTestCase(unittest.TestCase):
     self._request_index += 1
     return rq
 
-  def testUserSatisfiedLens(self):
-    # We track all times in milliseconds, but raw trace events are in
-    # microseconds.
-    MILLI_TO_MICRO = 1000
+  def testFirstContentfulPaintLens(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
         trace_events=[{'ts': 0, 'ph': 'I',
                        'cat': 'blink.some_other_user_timing',
                        'name': 'firstContentfulPaint'},
-                      {'ts': 9 * MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstDiscontentPaint'},
-                      {'ts': 12 * MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstContentfulPaint'},
-                      {'ts': 22 * MILLI_TO_MICRO, 'ph': 'I',
+                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
                        'cat': 'blink.user_timing',
                        'name': 'firstContentfulPaint'}])
-    graph = loading_model.ResourceGraph(loading_trace)
-    lens = user_satisfied_lens.UserSatisfiedLens(loading_trace, graph)
-    for n in graph.Nodes():
-      if n.Request().frame_id == '123.20':
-        self.assertFalse(lens.Filter(n))
-      else:
-        self.assertTrue(lens.Filter(n))
+    lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(1, lens.PostloadTimeMsec())
+
+  def testCantGetNoSatisfaction(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'not_my_cat',
+                       'name': 'someEvent'}])
+    lens = user_satisfied_lens.FirstContentfulPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2', '0.3']), lens.CriticalRequests())
+    self.assertEqual(float('inf'), lens.PostloadTimeMsec())
+
+  def testFirstTextPaintLens(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink.some_other_user_timing',
+                       'name': 'firstPaint'},
+                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstishPaint'},
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstPaint'},
+                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstPaint'}])
+    lens = user_satisfied_lens.FirstTextPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(1, lens.PostloadTimeMsec())
+
+  def testFirstSignificantPaintLens(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10),
+         self._RequestAt(15), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink',
+                       'name': 'firstPaint'},
+                      {'ts': 9 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'FrameView::SynchronizedPaint'},
+                      {'ts': 18 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink',
+                       'name': 'FrameView::SynchronizedPaint'},
+                      {'ts': 22 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink',
+                       'name': 'FrameView::SynchronizedPaint'},
+
+                      {'ts': 5 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'foobar', 'name': 'biz',
+                       'args': {'counters': {
+                           'LayoutObjectsThatHadNeverHadLayout': 10
+                       } } },
+                      {'ts': 12 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'foobar', 'name': 'biz',
+                       'args': {'counters': {
+                           'LayoutObjectsThatHadNeverHadLayout': 12
+                       } } },
+                      {'ts': 15 * self.MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'foobar', 'name': 'biz',
+                       'args': {'counters': {
+                           'LayoutObjectsThatHadNeverHadLayout': 10
+                       } } } ])
+    lens = user_satisfied_lens.FirstSignificantPaintLens(loading_trace)
+    self.assertEqual(set(['0.1', '0.2']), lens.CriticalRequests())
+    self.assertEqual(7, lens.PostloadTimeMsec())

commit 244714577a0d69d32ca5bdc152c145b81ae8a95d
Author: lizeb <lizeb@chromium.org>
Date:   Tue Mar 22 03:18:19 2016 -0700

    clovis: Identify prefetchable resources from dependencies and tracing.
    
    Resources that are discoverable by the preload scanner are behind a
    "parser" dependency. However, this is a superset of the actual
    discoverable resources. Use tracing to find the actual resources, that
    are then matched to requests.
    
    Review URL: https://codereview.chromium.org/1813723002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382542}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7f2bd1dc11f2e24416a0c0380bf4d908d4e82b62

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index f6165d9..bf6610d 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -29,8 +29,8 @@ class ActivityLens(object):
     self._trace = trace
     events = trace.tracing_track.GetEvents()
     self._renderer_main_pid_tid = self._GetRendererMainThreadId(events)
-    self._tracing = self._trace.tracing_track.TracingTrackForThread(
-        self._renderer_main_pid_tid)
+    self._tracing = self._trace.tracing_track.Filter(
+        *self._renderer_main_pid_tid)
 
   @classmethod
   def _GetRendererMainThreadId(cls, events):
diff --git a/loading/prefetch_view.py b/loading/prefetch_view.py
new file mode 100644
index 0000000..76f7723
--- /dev/null
+++ b/loading/prefetch_view.py
@@ -0,0 +1,116 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Models the effect of prefetching resources from a loading trace.
+
+For example, this can be used to evaluate NoState Prefetch
+(https://goo.gl/B3nRUR).
+
+When executed as a script, takes a trace as a command-line arguments and shows
+how many requests were prefetched.
+"""
+
+import itertools
+import operator
+
+import loading_trace
+import request_dependencies_lens
+
+
+class PrefetchSimulationView(object):
+  """Simulates the effect of prefetching resources discoverable by the preload
+  scanner.
+  """
+  def __init__(self, trace, dependencies_lens):
+    """Initializes an instance of PrefetchSimulationView.
+
+    Args:
+      trace: (LoadingTrace) a loading trace.
+      dependencies_lens: (RequestDependencyLens) request dependencies.
+    """
+    self.trace = trace
+    self.dependencies_lens = dependencies_lens
+    self._resource_events = self.trace.tracing_track.Filter(
+        categories=set([u'blink.net']))
+    assert len(self._resource_events.GetEvents()) > 0,\
+            'Was the "blink.net" category enabled at trace collection time?"'
+
+  def ParserDiscoverableRequests(self, request, recurse=False):
+    """Returns a list of requests discovered by the parser from a given request.
+
+    Args:
+      request: (Request) Root request.
+
+    Returns:
+      [Request]
+    """
+    # TODO(lizeb): handle the recursive case.
+    assert not recurse
+    discoverable_requests = [request]
+    first_request = self.dependencies_lens.GetRedirectChain(request)[-1]
+    deps = self.dependencies_lens.GetRequestDependencies()
+    for (first, second, reason) in deps:
+      if first.request_id == first_request.request_id and reason == 'parser':
+        discoverable_requests.append(second)
+    return discoverable_requests
+
+  def ExpandRedirectChains(self, requests):
+    return list(itertools.chain.from_iterable(
+        [self.dependencies_lens.GetRedirectChain(r) for r in requests]))
+
+  def PreloadedRequests(self, request):
+    """Returns the requests that have been preloaded from a given request.
+
+    This list is the set of request that are:
+    - Discoverable by the parser
+    - Found in the trace log.
+
+    Before looking for dependencies, this follows the redirect chain.
+
+    Args:
+      request: (Request) Root request.
+
+    Returns:
+      A list of Request. Does not include the root request. This list is a
+      subset of the one returned by ParserDiscoverableRequests().
+    """
+    # Preload step events are emitted in ResourceFetcher::preloadStarted().
+    preload_step_events = filter(
+        lambda e:  e.args.get('step') == 'Preload',
+        self._resource_events.GetEvents())
+    preloaded_urls = set()
+    for preload_step_event in preload_step_events:
+      preload_event = self._resource_events.EventFromStep(preload_step_event)
+      if preload_event:
+        preloaded_urls.add(preload_event.args['url'])
+    parser_requests = self.ParserDiscoverableRequests(request)
+    preloaded_root_requests = filter(
+        lambda r: r.url in preloaded_urls, parser_requests)
+    # We can actually fetch the whole redirect chain.
+    return [request] + list(itertools.chain.from_iterable(
+        [self.dependencies_lens.GetRedirectChain(r)
+         for r in preloaded_root_requests]))
+
+
+def _PrintSummary(prefetch_view):
+  requests = prefetch_view.trace.request_track.GetEvents()
+  first_request = prefetch_view.trace.request_track.GetEvents()[0]
+  parser_requests = prefetch_view.ExpandRedirectChains(
+      prefetch_view.ParserDiscoverableRequests(first_request))
+  preloaded_requests = prefetch_view.ExpandRedirectChains(
+      prefetch_view.PreloadedRequests(first_request))
+  print '%d requests, %d parser from the main request, %d preloaded' % (
+      len(requests), len(parser_requests), len(preloaded_requests))
+
+
+def main(filename):
+  trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+  dependencies_lens = request_dependencies_lens.RequestDependencyLens(trace)
+  prefetch_view = PrefetchSimulationView(trace, dependencies_lens)
+  _PrintSummary(prefetch_view)
+
+
+if __name__ == '__main__':
+  import sys
+  main(sys.argv[1])
diff --git a/loading/prefetch_view_unittest.py b/loading/prefetch_view_unittest.py
new file mode 100644
index 0000000..f7210bc
--- /dev/null
+++ b/loading/prefetch_view_unittest.py
@@ -0,0 +1,62 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import prefetch_view
+import request_dependencies_lens
+from request_dependencies_lens_unittest import TestRequests
+
+
+class PrefetchSimulationViewTestCase(unittest.TestCase):
+  def setUp(self):
+    super(PrefetchSimulationViewTestCase, self).setUp()
+    self._SetUp()
+
+  def testExpandRedirectChains(self):
+    self.assertListEqual(
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST],
+        self.prefetch_view.ExpandRedirectChains(
+            [TestRequests.FIRST_REDIRECT_REQUEST]))
+
+  def testParserDiscoverableRequests(self):
+    first_request = TestRequests.FIRST_REDIRECT_REQUEST
+    discovered_requests = self.prefetch_view.ParserDiscoverableRequests(
+        first_request)
+    self.assertListEqual(
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_UNRELATED_FRAME], discovered_requests)
+
+  def testPreloadedRequests(self):
+    first_request = TestRequests.FIRST_REDIRECT_REQUEST
+    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    self.assertListEqual([first_request], preloaded_requests)
+    self._SetUp(
+        [{'args': {'url': 'http://bla.com/nyancat.js'},
+          'cat': 'blink.net', 'id': '0xaf9f14fa9dd6c314', 'name': 'Resource',
+          'ph': 'X', 'ts': 1, 'dur': 120, 'pid': 12, 'tid': 12},
+         {'args': {'step': 'Preload'}, 'cat': 'blink.net',
+          'id': '0xaf9f14fa9dd6c314', 'name': 'Resource', 'ph': 'T',
+          'ts': 12, 'pid': 12, 'tid': 12}])
+    preloaded_requests = self.prefetch_view.PreloadedRequests(first_request)
+    self.assertListEqual([TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_UNRELATED_FRAME], preloaded_requests)
+
+  def _SetUp(self, added_trace_events=None):
+    trace_events = [
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'blink.net'}]
+    if added_trace_events is not None:
+      trace_events += added_trace_events
+    self.trace = TestRequests.CreateLoadingTrace(trace_events)
+    dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+        self.trace)
+    self.prefetch_view = prefetch_view.PrefetchSimulationView(
+        self.trace, dependencies_lens)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 4a72a4f..1689547 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -30,6 +30,7 @@ class RequestDependencyLens(object):
     self._requests = self.loading_trace.request_track.GetEvents()
     self._requests_by_id = {r.request_id: r for r in self._requests}
     self._requests_by_url = collections.defaultdict(list)
+    self._deps = None
     for request in self._requests:
       self._requests_by_url[request.url].append(request)
     self._frame_to_parent = {}
@@ -45,12 +46,36 @@ class RequestDependencyLens(object):
       request_track.Request, and reason is in DEPENDENCIES. The second request
       depends on the first one, with the listed reason.
     """
-    deps = []
+    self._ComputeRequestDependencies()
+    return copy.copy(self._deps)
+
+  def GetRedirectChain(self, request):
+    """Returns the whole redirect chain for a given request.
+
+    Note that this misses some JS-based redirects.
+
+    Returns:
+      A list of request, containing the request passed as a parameter.
+    """
+    self._ComputeRequestDependencies()
+    chain = [request]
+    while True:
+      for (first_request, second_request, why) in self._deps:
+        if first_request == request and why == 'redirect':
+          chain.append(second_request)
+          request = second_request
+          break
+      else:
+        return chain
+
+  def _ComputeRequestDependencies(self):
+    if self._deps is not None:
+      return
+    self._deps = []
     for request in self._requests:
       dependency = self._GetDependency(request)
       if dependency:
-        deps.append(dependency)
-    return deps
+        self._deps.append(dependency)
 
   def _GetDependency(self, request):
     """Returns (first, second, reason), or None.
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 1429a0e..773af44 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -11,77 +11,122 @@ from request_track import (Request, TimingFromDict)
 import test_utils
 
 
-class RequestDependencyLensTestCase(unittest.TestCase):
-  _REDIRECT_REQUEST = Request.FromJsonDict(
+class TestRequests(object):
+  FIRST_REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com', 'request_id': '1234.redirect.1',
        'initiator': {'type': 'other'},
+       'timestamp': 0.5, 'timing': TimingFromDict({})})
+  SECOND_REDIRECT_REQUEST = Request.FromJsonDict(
+      {'url': 'http://bla.com/redirect1', 'request_id': '1234.redirect.2',
+       'initiator': {'type': 'redirect',
+                     'initiating_request': '1234.redirect.1'},
        'timestamp': 1, 'timing': TimingFromDict({})})
-  _REDIRECTED_REQUEST = Request.FromJsonDict({
-      'url': 'http://bla.com',
+  REDIRECTED_REQUEST = Request.FromJsonDict({
+      'url': 'http://bla.com/index.html',
       'request_id': '1234.1',
       'frame_id': '123.1',
       'initiator': {'type': 'redirect',
-                    'initiating_request': '1234.redirect.1'},
+                    'initiating_request': '1234.redirect.2'},
       'timestamp': 2,
       'timing': TimingFromDict({})})
-  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
-                                   'request_id': '1234.1',
-                                   'frame_id': '123.1',
-                                   'initiator': {'type': 'other'},
-                                   'timestamp': 2,
-                                   'timing': TimingFromDict({})})
-  _JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
-                                      'request_id': '1234.12',
-                                      'frame_id': '123.1',
-                                      'initiator': {'type': 'parser',
-                                                    'url': 'http://bla.com'},
-                                      'timestamp': 3,
-                                      'timing': TimingFromDict({})})
-  _JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
+  REQUEST = Request.FromJsonDict({'url': 'http://bla.com/index.html',
+                                  'request_id': '1234.1',
+                                  'frame_id': '123.1',
+                                  'initiator': {'type': 'other'},
+                                  'timestamp': 2,
+                                  'timing': TimingFromDict({})})
+  JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
+                                     'request_id': '1234.12',
+                                     'frame_id': '123.123',
+                                     'initiator': {
+                                         'type': 'parser',
+                                         'url': 'http://bla.com/index.html'},
+                                     'timestamp': 3,
+                                     'timing': TimingFromDict({})})
+  JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
        'request_id': '1234.42',
        'frame_id': '123.13',
        'initiator': {'type': 'parser',
-                     'url': 'http://bla.com'},
+                     'url': 'http://bla.com/index.html'},
        'timestamp': 4, 'timing': TimingFromDict({})})
-  _JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
+  JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
       {'url': 'http://bla.com/nyancat.js',
-       'request_id': '1234.42',
+       'request_id': '1234.56',
        'frame_id': '123.99',
        'initiator': {'type': 'parser',
-                     'url': 'http://bla.com'},
+                     'url': 'http://bla.com/index.html'},
        'timestamp': 5, 'timing': TimingFromDict({})})
-  _JS_REQUEST_2 = Request.FromJsonDict(
+  JS_REQUEST_2 = Request.FromJsonDict(
       {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
-       'frame_id': '123.1',
+       'frame_id': '123.123',
        'initiator': {'type': 'script',
                      'stack': {'callFrames': [
                          {'url': 'unknown'},
                          {'url': 'http://bla.com/nyancat.js'}]}},
        'timestamp': 10, 'timing': TimingFromDict({})})
-  _PAGE_EVENTS = [{'method': 'Page.frameAttached',
-                   'frame_id': '123.13', 'parent_frame_id': '123.1'}]
+  PAGE_EVENTS = [{'method': 'Page.frameAttached',
+                   'frame_id': '123.13', 'parent_frame_id': '123.1'},
+                 {'method': 'Page.frameAttached',
+                  'frame_id': '123.123', 'parent_frame_id': '123.1'}]
+
+  @classmethod
+  def CreateLoadingTrace(cls, trace_events=None):
+    return test_utils.LoadingTraceFromEvents(
+        [cls.FIRST_REDIRECT_REQUEST, cls.SECOND_REDIRECT_REQUEST,
+         cls.REDIRECTED_REQUEST, cls.REQUEST, cls.JS_REQUEST, cls.JS_REQUEST_2,
+         cls.JS_REQUEST_OTHER_FRAME, cls.JS_REQUEST_UNRELATED_FRAME],
+        cls.PAGE_EVENTS, trace_events)
 
+
+class RequestDependencyLensTestCase(unittest.TestCase):
   def testRedirectDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST])
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
-    self.assertEquals(1, len(deps))
+    self.assertEquals(2, len(deps))
     (first, second, reason) = deps[0]
     self.assertEquals('redirect', reason)
-    self.assertEquals(self._REDIRECT_REQUEST.request_id, first.request_id)
-    self.assertEquals(self._REQUEST.request_id, second.request_id)
+    self.assertEquals(TestRequests.FIRST_REDIRECT_REQUEST.request_id,
+                      first.request_id)
+    self.assertEquals(TestRequests.SECOND_REDIRECT_REQUEST.request_id,
+                      second.request_id)
+    (first, second, reason) = deps[1]
+    self.assertEquals('redirect', reason)
+    self.assertEquals(TestRequests.SECOND_REDIRECT_REQUEST.request_id,
+                      first.request_id)
+    self.assertEquals(TestRequests.REQUEST.request_id, second.request_id)
+
+  def testGetRedirectChain(self):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST, TestRequests.REDIRECTED_REQUEST])
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    whole_chain = [TestRequests.FIRST_REDIRECT_REQUEST,
+                   TestRequests.SECOND_REDIRECT_REQUEST,
+                   TestRequests.REDIRECTED_REQUEST]
+    chain = request_dependencies_lens.GetRedirectChain(
+        TestRequests.FIRST_REDIRECT_REQUEST)
+    self.assertListEqual(whole_chain, chain)
+    chain = request_dependencies_lens.GetRedirectChain(
+        TestRequests.SECOND_REDIRECT_REQUEST)
+    self.assertListEqual(whole_chain[1:], chain)
+    chain = request_dependencies_lens.GetRedirectChain(
+        TestRequests.REDIRECTED_REQUEST)
+    self.assertEquals(whole_chain[2:], chain)
 
   def testScriptDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST, self._JS_REQUEST_2])
+        [TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.JS_REQUEST.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def testAsyncScriptDependency(self):
     JS_REQUEST_WITH_ASYNC_STACK = Request.FromJsonDict(
@@ -93,65 +138,76 @@ class RequestDependencyLensTestCase(unittest.TestCase):
                                       {'url': 'http://bla.com/nyancat.js'}]}}},
          'timestamp': 10, 'timing': TimingFromDict({})})
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
+        [TestRequests.JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
-        deps[0], self._JS_REQUEST.request_id,
+        deps[0], TestRequests.JS_REQUEST.request_id,
         JS_REQUEST_WITH_ASYNC_STACK.request_id, 'script')
 
   def testParserDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REQUEST, self._JS_REQUEST])
+        [TestRequests.REQUEST, TestRequests.JS_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+        TestRequests.REQUEST.request_id, TestRequests.JS_REQUEST.request_id,
+        'parser')
 
   def testSeveralDependencies(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST, self._JS_REQUEST,
-         self._JS_REQUEST_2])
+        [TestRequests.FIRST_REDIRECT_REQUEST,
+         TestRequests.SECOND_REDIRECT_REQUEST,
+         TestRequests.REDIRECTED_REQUEST,
+         TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
-    self.assertEquals(3, len(deps))
+    self.assertEquals(4, len(deps))
     self._AssertDependencyIs(
-        deps[0], self._REDIRECT_REQUEST.request_id, self._REQUEST.request_id,
-        'redirect')
+        deps[0], TestRequests.FIRST_REDIRECT_REQUEST.request_id,
+        TestRequests.SECOND_REDIRECT_REQUEST.request_id, 'redirect')
     self._AssertDependencyIs(
-        deps[1],
-        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+        deps[1], TestRequests.SECOND_REDIRECT_REQUEST.request_id,
+        TestRequests.REQUEST.request_id, 'redirect')
     self._AssertDependencyIs(
         deps[2],
-        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.REQUEST.request_id, TestRequests.JS_REQUEST.request_id,
+        'parser')
+    self._AssertDependencyIs(
+        deps[3],
+        TestRequests.JS_REQUEST.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def testDependencyDifferentFrame(self):
     """Checks that a more recent request from another frame is ignored."""
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
+        [TestRequests.JS_REQUEST, TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.JS_REQUEST.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def testDependencySameParentFrame(self):
     """Checks that a more recent request from an unrelated frame is ignored
     if there is one from a related frame."""
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
-         self._JS_REQUEST_2], self._PAGE_EVENTS)
+        [TestRequests.JS_REQUEST_OTHER_FRAME,
+         TestRequests.JS_REQUEST_UNRELATED_FRAME, TestRequests.JS_REQUEST_2],
+        TestRequests.PAGE_EVENTS)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
     self._AssertDependencyIs(
         deps[0],
-        self._JS_REQUEST_OTHER_FRAME.request_id,
-        self._JS_REQUEST_2.request_id, 'script')
+        TestRequests.JS_REQUEST_OTHER_FRAME.request_id,
+        TestRequests.JS_REQUEST_2.request_id, 'script')
 
   def _AssertDependencyIs(
       self, dep, first_request_id, second_request_id, reason):
diff --git a/loading/test_utils.py b/loading/test_utils.py
index f1c80c2..38737c0 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -125,7 +125,7 @@ def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
   request = FakeRequestTrack(requests)
   page_event_track = FakePageTrack(page_events if page_events else [])
-  if trace_events:
+  if trace_events is not None:
     tracing_track = tracing.TracingTrack(None)
     tracing_track.Handle('Tracing.dataCollected',
                          {'params': {'value': [e for e in trace_events]}})
diff --git a/loading/tracing.py b/loading/tracing.py
index 7403a9d..8a07a2b 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -91,19 +91,24 @@ class TracingTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
-  def TracingTrackForThread(self, pid_tid):
-    """Returns a new TracingTrack with only the events from a given thread.
+  def Filter(self, pid=None, tid=None, categories=None):
+    """Returns a new TracingTrack with a subset of the events.
 
     Args:
-      pid_tid: ((int, int) PID and TID.
-
-    Returns:
-      A new instance of TracingTrack.
+      pid: (int or None) Selects events from this PID.
+      tid: (int or None) Selects events from this TID.
+      categories: (set([str]) or None) Selects events belonging to one of the
+                  categories.
     """
-    (pid, tid) = pid_tid
-    events = [e for e in self._events
-              if (e.tracing_event['pid'] == pid
-                  and e.tracing_event['tid'] == tid)]
+    events = self._events
+    if pid is not None:
+      events = filter(lambda e : e.tracing_event['pid'] == pid, events)
+    if tid is not None:
+      events = filter(lambda e : e.tracing_event['tid'] == tid, events)
+    if categories is not None:
+      events = filter(
+          lambda e : set(e.category.split(',')).intersection(categories),
+          events)
     tracing_track = TracingTrack(None)
     tracing_track._events = events
     return tracing_track
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 8d946b7..a3bea3f 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -287,14 +287,35 @@ class TracingTrackTestCase(unittest.TestCase):
     with self.assertRaises(AssertionError):
       self.track.EventFromStep(no_step)
 
-  def testTracingTrackForThread(self):
+  def testFilterPidTid(self):
     self._HandleEvents(self._EVENTS)
-    tracing_track = self.track.TracingTrackForThread((2, 1))
+    tracing_track = self.track.Filter(2, 1)
     self.assertTrue(tracing_track is not self.track)
     self.assertEquals(4, len(tracing_track.GetEvents()))
-    tracing_track = self.track.TracingTrackForThread((2, 42))
+    tracing_track = self.track.Filter(2, 42)
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
+  def testFilterCategories(self):
+    events = [
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'A'},
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'B'},
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'C,D'},
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'cat': 'A,B,C,D'}]
+    self._HandleEvents(events)
+    tracing_events = self.track.GetEvents()
+    self.assertEquals(4, len(tracing_events))
+    filtered_events = self.track.Filter(categories=None).GetEvents()
+    self.assertListEqual(tracing_events, filtered_events)
+    filtered_events = self.track.Filter(categories=set(['A'])).GetEvents()
+    self.assertEquals(2, len(filtered_events))
+    self.assertListEqual([tracing_events[0], tracing_events[3]],
+                         filtered_events)
+    filtered_events = self.track.Filter(categories=set(['Z'])).GetEvents()
+    self.assertEquals(0, len(filtered_events))
+    filtered_events = self.track.Filter(categories=set(['B', 'C'])).GetEvents()
+    self.assertEquals(3, len(filtered_events))
+    self.assertListEqual(tracing_events[1:], filtered_events)
+
   def _HandleEvents(self, events):
     self.track.Handle('Tracing.dataCollected', {'params': {'value': [
         self.EventToMicroseconds(e) for e in events]}})

commit a47e8ad4e144934870395036d13cdff39148c98d
Author: gabadie <gabadie@chromium.org>
Date:   Mon Mar 21 13:19:00 2016 -0700

    sandwich: Make it work on desktop.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1812053002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382368}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f1d603801466887e509cb7c5b3fd2fcdd5da4b61

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index bdbe1e4..1df3010 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -38,6 +38,19 @@ OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
 CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
 
 
+def _EnsureCleanCacheDirectory(directory_dest_path):
+  """Ensure that a cache directory is created and clean.
+
+  Args:
+    directory_dest_path: Path of the cache directory to ensure cleanliness.
+  """
+  if os.path.isdir(directory_dest_path):
+    shutil.rmtree(directory_dest_path)
+  elif not os.path.isdir(os.path.dirname(directory_dest_path)):
+    os.makedirs(os.path.dirname(directory_dest_path))
+  assert not os.path.exists(directory_dest_path)
+
+
 def _RemoteCacheDirectory():
   """Returns the path of the cache directory's on the remote device."""
   return '/data/data/{}/cache/Cache'.format(
@@ -182,9 +195,7 @@ def UnzipDirectoryContent(archive_path, directory_dest_path):
     archive_path: Archive's path to unzip.
     directory_dest_path: Directory destination path.
   """
-  if not os.path.exists(directory_dest_path):
-    os.makedirs(directory_dest_path)
-
+  _EnsureCleanCacheDirectory(directory_dest_path)
   with zipfile.ZipFile(archive_path) as zip_input:
     timestamps = None
     for file_archive_name in zip_input.namelist():
@@ -207,6 +218,19 @@ def UnzipDirectoryContent(archive_path, directory_dest_path):
       os.utime(output_path, (stats['atime'], stats['mtime']))
 
 
+def CopyCacheDirectory(directory_src_path, directory_dest_path):
+  """Copies a cache directory recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    directory_src_path: Path of the cache directory source.
+    directory_dest_path: Path of the cache directory destination.
+  """
+  assert os.path.isdir(directory_src_path)
+  _EnsureCleanCacheDirectory(directory_dest_path)
+  shutil.copytree(directory_src_path, directory_dest_path)
+
+
 class CacheBackend(object):
   """Takes care of reading and deleting cached keys.
   """
diff --git a/loading/chrome_cache_unittest.py b/loading/chrome_cache_unittest.py
new file mode 100644
index 0000000..051a938
--- /dev/null
+++ b/loading/chrome_cache_unittest.py
@@ -0,0 +1,121 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import shutil
+import tempfile
+import unittest
+
+import chrome_cache
+
+
+LOADING_DIR = os.path.dirname(os.path.abspath(__file__))
+THIS_BASEMAME = os.path.basename(__file__)
+
+
+class CacheDirectoryTest(unittest.TestCase):
+  def setUp(self):
+    self._temp_dir = tempfile.mkdtemp()
+
+  def tearDown(self):
+    shutil.rmtree(self._temp_dir)
+
+  def GetTempPath(self, temp_name):
+    return os.path.join(self._temp_dir, temp_name)
+
+  def CreateNewGarbageFile(self, file_path):
+    assert not os.path.exists(file_path)
+    with open(file_path, 'w') as f:
+      f.write('garbage content')
+    assert os.path.isfile(file_path)
+
+  @classmethod
+  def CompareDirectories(cls, reference_path, generated_path):
+    def CompareNode(relative_path):
+      reference_stat = os.stat(os.path.join(reference_path, relative_path))
+      generated_stat = os.stat(os.path.join(generated_path, relative_path))
+      assert int(reference_stat.st_mtime) == int(generated_stat.st_mtime), \
+          "{}: invalid mtime.".format(relative_path)
+    for reference_parent_path, dir_names, file_names in os.walk(reference_path):
+      parent_path = os.path.relpath(reference_parent_path, reference_path)
+      reference_nodes = sorted(dir_names + file_names)
+      generated_nodes = sorted(os.listdir(
+          os.path.join(generated_path, parent_path)))
+      assert reference_nodes == generated_nodes, \
+          '{}: directory entries don\'t match.'.format(parent_path)
+      for node in file_names:
+        CompareNode(os.path.join(parent_path, node))
+      CompareNode(parent_path)
+
+  def testCompareDirectories(self):
+    generated_path = self.GetTempPath('dir0')
+    shutil.copytree(LOADING_DIR, generated_path)
+    self.CompareDirectories(LOADING_DIR, generated_path)
+
+    generated_path = self.GetTempPath('dir1')
+    shutil.copytree(LOADING_DIR, generated_path)
+    self.CreateNewGarbageFile(os.path.join(generated_path, 'garbage'))
+    assert 'garbage' in os.listdir(generated_path)
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+
+    generated_path = self.GetTempPath('dir2')
+    shutil.copytree(LOADING_DIR, generated_path)
+    self.CreateNewGarbageFile(os.path.join(generated_path, 'testdata/garbage'))
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+
+    generated_path = self.GetTempPath('dir3')
+    shutil.copytree(LOADING_DIR, generated_path)
+    os.remove(os.path.join(generated_path, THIS_BASEMAME))
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+    self.CreateNewGarbageFile(os.path.join(generated_path, 'garbage'))
+    with self.assertRaisesRegexp(AssertionError, r'^.* match\.$'):
+      self.CompareDirectories(LOADING_DIR, generated_path)
+
+    def TouchHelper(temp_name, relative_name, timestamps):
+      generated_path = self.GetTempPath(temp_name)
+      shutil.copytree(LOADING_DIR, generated_path)
+      os.utime(os.path.join(generated_path, relative_name), timestamps)
+      with self.assertRaisesRegexp(AssertionError, r'^.* invalid mtime\.$'):
+        self.CompareDirectories(LOADING_DIR, generated_path)
+
+    TouchHelper('dir4', THIS_BASEMAME, (1256925858, 1256463122))
+    TouchHelper('dir5', 'testdata', (1256918318, 1256568641))
+    TouchHelper('dir6', 'trace_test/test_server.py', (1255116211, 1256156632))
+    TouchHelper('dir7', './', (1255115332, 1256251864))
+
+  def testCacheArchive(self):
+    zip_dest = self.GetTempPath('cache.zip')
+    chrome_cache.ZipDirectoryContent(LOADING_DIR, zip_dest)
+
+    unzip_dest = self.GetTempPath('cache')
+    chrome_cache.UnzipDirectoryContent(zip_dest, unzip_dest)
+    self.CompareDirectories(LOADING_DIR, unzip_dest)
+
+    self.CreateNewGarbageFile(os.path.join(unzip_dest, 'garbage'))
+    chrome_cache.UnzipDirectoryContent(zip_dest, unzip_dest)
+    self.CompareDirectories(LOADING_DIR, unzip_dest)
+
+    unzip_dest = self.GetTempPath('foo/bar/cache')
+    chrome_cache.UnzipDirectoryContent(zip_dest, unzip_dest)
+    self.CompareDirectories(LOADING_DIR, unzip_dest)
+
+  def testCopyCacheDirectory(self):
+    copy_dest = self.GetTempPath('cache')
+    chrome_cache.CopyCacheDirectory(LOADING_DIR, copy_dest)
+    self.CompareDirectories(LOADING_DIR, copy_dest)
+
+    self.CreateNewGarbageFile(os.path.join(copy_dest, 'garbage'))
+    chrome_cache.CopyCacheDirectory(LOADING_DIR, copy_dest)
+    self.CompareDirectories(LOADING_DIR, copy_dest)
+
+    copy_dest = self.GetTempPath('foo/bar/cache')
+    chrome_cache.CopyCacheDirectory(LOADING_DIR, copy_dest)
+    self.CompareDirectories(LOADING_DIR, copy_dest)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/controller.py b/loading/controller.py
index 9226ab3..25fefae 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -45,7 +45,23 @@ class ChromeControllerBase(object):
   """
   def __init__(self):
     self._chrome_args = [
+        # Disable backgound network requests that may pollute WPR archive,
+        # pollute HTTP cache generation, and introduce noise in loading
+        # performance.
+        '--disable-background-networking',
+        '--disable-default-apps',
+        '--no-proxy-server',
+        # TODO(gabadie): Remove once crbug.com/354743 done.
+        '--safebrowsing-disable-auto-update',
+
+        # Disables actions that chrome performs only on first run or each
+        # launches, which can interfere with page load performance, or even
+        # block its execution by waiting for user input.
         '--disable-fre',
+        '--no-default-browser-check',
+        '--no-first-run',
+
+        # Tests & dev-tools related stuff.
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
     ]
@@ -54,6 +70,7 @@ class ChromeControllerBase(object):
     self._emulated_device = None
     self._emulated_network = None
     self._clear_cache = False
+    self._slow_death = False
 
   def AddChromeArgument(self, arg):
     """Add command-line argument to the chrome execution."""
@@ -97,13 +114,64 @@ class ChromeControllerBase(object):
     """Set network emulation.
 
     Args:
-      network_name: (str) Key from emulation.NETWORK_CONDITIONS.
+      network_name: (str) Key from emulation.NETWORK_CONDITIONS or None to
+        disable network emulation.
     """
     if network_name:
       self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
     else:
       self._emulated_network = None
 
+  def PushBrowserCache(self, cache_path):
+    """Pushes the HTTP chrome cache to the profile directory.
+
+    Caution:
+      The chrome cache backend type differ according to the platform. On
+      desktop, the cache backend type is `blockfile` versus `simple` on Android.
+      This method assumes that your are pushing a cache with the correct backend
+      type, and will NOT verify for you.
+
+    Args:
+      cache_path: The directory's path containing the cache locally.
+    """
+    raise NotImplementedError
+
+  def PullBrowserCache(self):
+    """Pulls the HTTP chrome cache from the profile directory.
+
+    Returns:
+      Temporary directory containing all the browser cache. Caller will need to
+      remove this directory manually.
+    """
+    raise NotImplementedError
+
+  def SetSlowDeath(self, slow_death=True):
+    """Set to pause before final kill of chrome.
+
+    Gives time for caches to write.
+
+    Args:
+      slow_death: (bool) True if you want that which comes to all who live, to
+        be slow.
+    """
+    self._slow_death = slow_death
+
+  @contextlib.contextmanager
+  def OpenWprHost(self, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+    """Opens a Web Page Replay host context.
+
+    Args:
+      wpr_archive_path: host sided WPR archive's path.
+      record: Enables or disables WPR archive recording.
+      network_condition_name: Network condition name available in
+          emulation.NETWORK_CONDITIONS.
+      disable_script_injection: Disable JavaScript file injections that is
+        fighting against resources name entropy.
+    """
+    raise NotImplementedError
+
   def _StartConnection(self, connection):
     """This should be called after opening an appropriate connection."""
     if self._emulated_device:
@@ -137,7 +205,6 @@ class RemoteChromeController(ChromeControllerBase):
     assert device is not None, 'Should you be using LocalController instead?'
     self._device = device
     super(RemoteChromeController, self).__init__()
-    self._slow_death = False
 
   @contextlib.contextmanager
   def Open(self):
@@ -170,40 +237,21 @@ class RemoteChromeController(ChromeControllerBase):
     self._device.KillAll(package_info.package, quiet=True)
 
   def PushBrowserCache(self, cache_path):
-    """Push a chrome cache.
-
-    Args:
-      cache_path: The directory's path containing the cache locally.
-    """
+    """Override for chrome cache pushing."""
     chrome_cache.PushBrowserCache(self._device, cache_path)
 
   def PullBrowserCache(self):
-    """Pull a chrome cache.
-
-    Returns:
-      Temporary directory containing all the browser cache. Caller will need to
-      remove this directory manually.
-    """
+    """Override for chrome cache pulling."""
+    assert self._slow_death, 'Must do SetSlowDeath() before opening chrome.'
     return chrome_cache.PullBrowserCache(self._device)
 
-  def SetSlowDeath(self, slow_death=True):
-    """Set to pause before final kill of chrome.
-
-    Gives time for caches to write.
-
-    Args:
-      slow_death: (bool) True if you want that which comes to all who live, to
-        be slow.
-    """
-    self._slow_death = slow_death
-
   @contextlib.contextmanager
   def OpenWprHost(self, wpr_archive_path, record=False,
                   network_condition_name=None,
                   disable_script_injection=False):
     """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
     assert not self._chrome_wpr_specific_args, 'WPR is already running.'
-    with device_setup.WprHost(self._device, wpr_archive_path,
+    with device_setup.RemoteWprHost(self._device, wpr_archive_path,
         record=record,
         network_condition_name=network_condition_name,
         disable_script_injection=disable_script_injection) as additional_flags:
@@ -213,25 +261,32 @@ class RemoteChromeController(ChromeControllerBase):
 
 
 class LocalChromeController(ChromeControllerBase):
-  """Controller for a local (desktop) chrome instance.
+  """Controller for a local (desktop) chrome instance."""
 
-  TODO(gabadie): implement cache push/pull and declare up in base class.
-  """
   def __init__(self):
     super(LocalChromeController, self).__init__()
     if OPTIONS.no_sandbox:
       self.AddChromeArgument('--no-sandbox')
+    self._profile_dir = OPTIONS.local_profile_dir
+    self._using_temp_profile_dir = self._profile_dir is None
+    if self._using_temp_profile_dir:
+      self._profile_dir = tempfile.mkdtemp(suffix='.profile')
+
+  def __del__(self):
+    if self._using_temp_profile_dir:
+      shutil.rmtree(self._profile_dir)
 
   @contextlib.contextmanager
   def Open(self):
     """Override for connection context."""
-    binary_filename = OPTIONS.local_binary
-    profile_dir = OPTIONS.local_profile_dir
-    using_temp_profile_dir = profile_dir is None
-    flags = self._GetChromeArguments()
-    if using_temp_profile_dir:
-      profile_dir = tempfile.mkdtemp()
-    flags = ['--user-data-dir=%s' % profile_dir] + flags
+    chrome_cmd = [OPTIONS.local_binary]
+    chrome_cmd.extend(self._GetChromeArguments())
+    chrome_cmd.append('--user-data-dir=%s' % self._profile_dir)
+    chrome_cmd.extend(['--enable-logging=stderr', '--v=1'])
+    # Navigates to about:blank for couples of reasons:
+    #   - To find the correct target descriptor at devtool connection;
+    #   - To avoid cache and WPR pollution by the NTP.
+    chrome_cmd.append('about:blank')
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     environment = os.environ.copy()
     if OPTIONS.headless:
@@ -239,9 +294,10 @@ class LocalChromeController(ChromeControllerBase):
       xvfb_process = subprocess.Popen(
           ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
           stderr=chrome_out)
-    chrome_process = subprocess.Popen(
-        [binary_filename] + flags, shell=False, stderr=chrome_out,
-        env=environment)
+    logging.debug(subprocess.list2cmdline(chrome_cmd))
+    chrome_process = subprocess.Popen(chrome_cmd, shell=False,
+                                      stderr=chrome_out, env=environment)
+    connection = None
     try:
       time.sleep(10)
       process_result = chrome_process.poll()
@@ -252,9 +308,57 @@ class LocalChromeController(ChromeControllerBase):
             OPTIONS.devtools_hostname, OPTIONS.devtools_port)
         self._StartConnection(connection)
         yield connection
+        if self._slow_death:
+          connection.Close()
+          connection = None
+          chrome_process.wait()
     finally:
-      chrome_process.kill()
+      if connection:
+        chrome_process.kill()
       if OPTIONS.headless:
         xvfb_process.kill()
-      if using_temp_profile_dir:
-        shutil.rmtree(profile_dir)
+
+  def PushBrowserCache(self, cache_path):
+    """Override for chrome cache pushing."""
+    self._EnsureProfileDirectory()
+    profile_cache_path = self._GetCacheDirectoryPath()
+    logging.info('Copy cache directory from %s to %s.' % (
+        cache_path, profile_cache_path))
+    chrome_cache.CopyCacheDirectory(cache_path, profile_cache_path)
+
+  def PullBrowserCache(self):
+    """Override for chrome cache pulling."""
+    cache_path = tempfile.mkdtemp()
+    profile_cache_path = self._GetCacheDirectoryPath()
+    logging.info('Copy cache directory from %s to %s.' % (
+        profile_cache_path, cache_path))
+    chrome_cache.CopyCacheDirectory(profile_cache_path, cache_path)
+    return cache_path
+
+  @contextlib.contextmanager
+  def OpenWprHost(self, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+    """Override for WPR context."""
+    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    with device_setup.LocalWprHost(wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection
+        ) as additional_flags:
+      self._chrome_wpr_specific_args = additional_flags
+      yield
+    self._chrome_wpr_specific_args = []
+
+  def _EnsureProfileDirectory(self):
+    if (not os.path.isdir(self._profile_dir) or
+        os.listdir(self._profile_dir) == []):
+      # Launch chrome so that it populates the profile directory.
+      with self.Open():
+        pass
+      print os.listdir(self._profile_dir + '/Default')
+    assert os.path.isdir(self._profile_dir)
+    assert os.path.isdir(os.path.dirname(self._GetCacheDirectoryPath()))
+
+  def _GetCacheDirectoryPath(self):
+    return os.path.join(self._profile_dir, 'Default', 'Cache')
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 4ffbb51..fad2b23 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -130,30 +130,11 @@ def _SetUpDevice(device, package_info):
 
 
 @contextlib.contextmanager
-def WprHost(device, wpr_archive_path, record=False,
-            network_condition_name=None,
-            disable_script_injection=False):
-  """Launches web page replay host.
-
-  Args:
-    device: Android device.
-    wpr_archive_path: host sided WPR archive's path.
-    network_condition_name: Network condition name available in
-        chrome_setup.NETWORK_CONDITIONS.
-    record: Enables or disables WPR archive recording.
-
-  Returns:
-    Additional flags list that may be used for chromium to load web page through
-    the running web page replay host.
-  """
-  assert device
-  if wpr_archive_path == None:
-    assert not record, 'WPR cannot record without a specified archive.'
-    assert not network_condition_name, ('WPR cannot emulate network condition' +
-                                        ' without a specified archive.')
-    yield []
-    return
-
+def _WprHost(wpr_archive_path, record=False,
+             network_condition_name=None,
+             disable_script_injection=False,
+             wpr_ca_cert_path=None):
+  assert wpr_archive_path
   wpr_server_args = ['--use_closest_match']
   if record:
     wpr_server_args.append('--record')
@@ -176,40 +157,130 @@ def WprHost(device, wpr_archive_path, record=False,
     # Remove default WPR injected scripts like deterministic.js which
     # overrides Math.random.
     wpr_server_args.extend(['--inject_scripts', ''])
+  if wpr_ca_cert_path:
+    wpr_server_args.extend(['--should_generate_certs',
+                            '--https_root_ca_cert_path=' + wpr_ca_cert_path])
 
+  # Set up WPR server and device forwarder.
+  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
+      '127.0.0.1', 0, 0, None, wpr_server_args)
+  http_port, https_port = wpr_server.StartServer()[:-1]
+
+  logging.info('WPR server listening on HTTP=%s, HTTPS=%s (options=%s)' % (
+      http_port, https_port, wpr_server_args))
+  try:
+    yield http_port, https_port
+  finally:
+    wpr_server.StopServer()
+
+
+def _VerifySilentWprHost(record, network_condition_name):
+  assert not record, 'WPR cannot record without a specified archive.'
+  assert not network_condition_name, ('WPR cannot emulate network condition' +
+                                      ' without a specified archive.')
+
+
+def _FormatWPRRelatedChromeArgumentFor(http_port, https_port, escape):
+  HOST_RULES='MAP * 127.0.0.1,EXCLUDE localhost'
+  chrome_args = [
+      '--testing-fixed-http-port={}'.format(http_port),
+      '--testing-fixed-https-port={}'.format(https_port)]
+  if escape:
+    chrome_args.append('--host-resolver-rules="{}"'.format(HOST_RULES))
+  else:
+    chrome_args.append('--host-resolver-rules={}'.format(HOST_RULES))
+  return chrome_args
+
+
+@contextlib.contextmanager
+def LocalWprHost(wpr_archive_path, record=False,
+                 network_condition_name=None,
+                 disable_script_injection=False):
+  """Launches web page replay host.
+
+  Args:
+    wpr_archive_path: host sided WPR archive's path.
+    record: Enables or disables WPR archive recording.
+    network_condition_name: Network condition name available in
+        chrome_setup.NETWORK_CONDITIONS.
+    disable_script_injection: Disable JavaScript file injections that is
+      fighting against resources name entropy.
+
+  Returns:
+    Additional flags list that may be used for chromium to load web page through
+    the running web page replay host.
+  """
+  if wpr_archive_path == None:
+    _VerifySilentWprHost(record, network_condition_name)
+    yield []
+    return
+  with _WprHost(
+      wpr_archive_path,
+      record=record,
+      network_condition_name=network_condition_name,
+      disable_script_injection=disable_script_injection
+      ) as (http_port, https_port):
+    chrome_args = _FormatWPRRelatedChromeArgumentFor(http_port, https_port,
+                                                     escape=False)
+    # Certification authority is handled only available on Android.
+    chrome_args.append('--ignore-certificate-errors')
+    yield chrome_args
+
+
+@contextlib.contextmanager
+def RemoteWprHost(device, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+  """Launches web page replay host.
+
+  Args:
+    device: Android device.
+    wpr_archive_path: host sided WPR archive's path.
+    record: Enables or disables WPR archive recording.
+    network_condition_name: Network condition name available in
+        chrome_setup.NETWORK_CONDITIONS.
+    disable_script_injection: Disable JavaScript file injections that is
+      fighting against resources name entropy.
+
+  Returns:
+    Additional flags list that may be used for chromium to load web page through
+    the running web page replay host.
+  """
+  assert device
+  if wpr_archive_path == None:
+    _VerifySilentWprHost(record, network_condition_name)
+    yield []
+    return
   # Deploy certification authority to the device.
   temp_certificate_dir = tempfile.mkdtemp()
   wpr_ca_cert_path = os.path.join(temp_certificate_dir, 'testca.pem')
   certutils.write_dummy_ca_cert(*certutils.generate_dummy_ca_cert(),
                                 cert_path=wpr_ca_cert_path)
-
   device_cert_util = adb_install_cert.AndroidCertInstaller(
       device.adb.GetDeviceSerial(), None, wpr_ca_cert_path)
   device_cert_util.install_cert(overwrite_cert=True)
-  wpr_server_args.extend(['--should_generate_certs',
-                          '--https_root_ca_cert_path=' + wpr_ca_cert_path])
-
-  # Set up WPR server and device forwarder.
-  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
-      '127.0.0.1', 0, 0, None, wpr_server_args)
-  ports = wpr_server.StartServer()[:-1]
-  host_http_port = ports[0]
-  host_https_port = ports[1]
-
-  forwarder.Forwarder.Map([(0, host_http_port), (0, host_https_port)], device)
-  device_http_port = forwarder.Forwarder.DevicePortForHostPort(host_http_port)
-  device_https_port = forwarder.Forwarder.DevicePortForHostPort(host_https_port)
-
   try:
-    yield [
-      '--host-resolver-rules="MAP * 127.0.0.1,EXCLUDE localhost"',
-      '--testing-fixed-http-port={}'.format(device_http_port),
-      '--testing-fixed-https-port={}'.format(device_https_port)]
+    # Set up WPR server
+    with _WprHost(
+        wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection,
+        wpr_ca_cert_path=wpr_ca_cert_path
+        ) as (http_port, https_port):
+      # Set up the forwarder.
+      forwarder.Forwarder.Map([(0, http_port), (0, https_port)], device)
+      device_http_port = forwarder.Forwarder.DevicePortForHostPort(http_port)
+      device_https_port = forwarder.Forwarder.DevicePortForHostPort(https_port)
+      try:
+        yield _FormatWPRRelatedChromeArgumentFor(device_http_port,
+                                                 device_https_port,
+                                                 escape=True)
+      finally:
+        # Tear down the forwarder.
+        forwarder.Forwarder.UnmapDevicePort(device_http_port, device)
+        forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
   finally:
-    forwarder.Forwarder.UnmapDevicePort(device_http_port, device)
-    forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
-    wpr_server.StopServer()
-
     # Remove certification authority from the device.
     device_cert_util.remove_cert()
     shutil.rmtree(temp_certificate_dir)
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 1ea5d91..b579918 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -92,7 +92,8 @@ class DevToolsConnection(object):
       hostname: server hostname.
       port: port number.
     """
-    self._ws = self._Connect(hostname, port)
+    self._http_hostname = hostname
+    self._http_port = port
     self._event_listeners = {}
     self._domain_listeners = {}
     self._scoped_states = {}
@@ -100,6 +101,10 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self._please_stop = False
     self._hooks = []
+    self._ws = None
+    self._target_descriptor = None
+
+    self._Connect()
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -247,6 +252,13 @@ class DevToolsConnection(object):
     """Stops the monitoring."""
     self._please_stop = True
 
+  def Close(self):
+    """Cleanly close chrome by closing the only tab."""
+    assert self._ws
+    response = self._HttpRequest('/close/' + self._target_descriptor['id'])
+    assert response == 'Target is closing'
+    self._ws = None
+
   def _Dispatch(self, timeout, kind='Monitoring'):
     self._please_stop = False
     while not self._please_stop:
@@ -313,25 +325,30 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self.StopMonitoring()
 
-  @classmethod
-  def _GetWebSocketUrl(cls, hostname, port):
-    r = httplib.HTTPConnection(hostname, port)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      raise DevToolsConnectionException(
-          'Cannot connect to DevTools, reponse code %d' % response.status)
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    return websocket_url
-
-  @classmethod
-  def _Connect(cls, hostname, port):
-    websocket_url = cls._GetWebSocketUrl(hostname, port)
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    return ws
+  def _HttpRequest(self, path):
+    assert path[0] == '/'
+    r = httplib.HTTPConnection(self._http_hostname, self._http_port)
+    try:
+      r.request('GET', '/json' + path)
+      response = r.getresponse()
+      if response.status != 200:
+        raise DevToolsConnectionException(
+            'Cannot connect to DevTools, reponse code %d' % response.status)
+      raw_response = response.read()
+    finally:
+      r.close()
+    return raw_response
+
+  def _Connect(self):
+    assert not self._ws
+    assert not self._target_descriptor
+    for target_descriptor in json.loads(self._HttpRequest('/list')):
+      if target_descriptor['type'] == 'page':
+        self._target_descriptor = target_descriptor
+        break
+    assert self._target_descriptor['url'] == 'about:blank'
+    self._ws = inspector_websocket.InspectorWebsocket()
+    self._ws.Connect(self._target_descriptor['webSocketDebuggerUrl'])
 
 
 class Listener(object):
diff --git a/loading/options.py b/loading/options.py
index ec295ac..0490ac5 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -118,7 +118,7 @@ class Options(object):
     for arg, default, help_str in self._ARGS:
       # All global options are named.
       arg = '--' + arg
-      self._AddArg(parser, arg, default, help_str=help_str)
+      self._AddArg(container, arg, default, help_str=help_str)
     if extra is not None:
       if type(extra) is not list:
         extra = [extra]
diff --git a/loading/sandwich.py b/loading/sandwich.py
index 4657eaf..1739578 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -246,6 +246,7 @@ class SandwichRunner(object):
     # TODO(gabadie): Make sandwich working on desktop.
     device = device_utils.DeviceUtils.HealthyDevices()[0]
     self._chrome_ctl = controller.RemoteChromeController(device)
+    self._chrome_ctl.AddChromeArgument('--disable-infobars')
     if self.cache_operation == 'save':
       self._chrome_ctl.SetSlowDeath()
 
@@ -284,8 +285,11 @@ def _ArgumentParser():
   common_job_parser.add_argument('--job', required=True,
                                  help='JSON file with job description.')
 
+  # Plumbing parser to configure OPTIONS.
+  plumbing_parser = OPTIONS.GetParentParser('plumbing options')
+
   # Main parser
-  parser = argparse.ArgumentParser()
+  parser = argparse.ArgumentParser(parents=[plumbing_parser])
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
   # Record WPR subcommand.
@@ -495,11 +499,8 @@ def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
 
-  # Don't give the argument yet. All we are interested in for now is accessing
-  # the default values of OPTIONS.
-  OPTIONS.ParseArgs([])
-
   args = _ArgumentParser().parse_args(command_line_args)
+  OPTIONS.SetParsedArgs(args)
 
   if args.subcommand == 'record-wpr':
     return _RecordWprMain(args)

commit 542cedab0274708011685cd4123226a04bc37f1e
Author: gabadie <gabadie@chromium.org>
Date:   Mon Mar 21 07:43:20 2016 -0700

    tools/android/loading: Lets test_server.py handling custom response headers.
    
    This CL let the test_server.py looking for a RESPONSE_HEADERS.json
    in the source directory to add response headers on resources. This
    will be used for non-regression tests of sandwich's WPR patching.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1759093002
    
    Cr-Original-Commit-Position: refs/heads/master@{#382284}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0869819900ada55ac050c31400665679b01fc8e5

diff --git a/loading/trace_test/test_server.py b/loading/trace_test/test_server.py
index 5898c66..5463667 100755
--- a/loading/trace_test/test_server.py
+++ b/loading/trace_test/test_server.py
@@ -13,6 +13,7 @@ via a named pipe at --fifo. Sources are served from the tree named at
 
 import argparse
 import cgi
+import json
 import os.path
 import logging
 import re
@@ -20,6 +21,27 @@ import time
 import wsgiref.simple_server
 
 
+_CONTENT_TYPE_FOR_SUFFIX = {
+    'css': 'text/css',
+    'html': 'text/html',
+    'jpg': 'image/jpeg',
+    'js': 'text/javascript',
+    'json': 'application/json',
+    'png': 'image/png',
+    'ttf': 'font/ttf',}
+
+# Name of the JSON file containing per file custom response headers located in
+# the --source_dir.
+# This file should structured like:
+#   {
+#     'mydocument.html': [
+#       ['Cache-Control', 'max-age=3600'],
+#       ['Content-Encoding', 'gzip'],
+#     ]
+#   }
+RESPONSE_HEADERS_PATH = 'RESPONSE_HEADERS.json'
+
+
 class ServerApp(object):
   """WSGI App.
 
@@ -27,6 +49,11 @@ class ServerApp(object):
   """
   def __init__(self, source_dir):
     self._source_dir = source_dir
+    self._response_headers = {}
+    response_header_path = os.path.join(source_dir, RESPONSE_HEADERS_PATH)
+    if os.path.exists(response_header_path):
+      with open(response_header_path) as response_headers_file:
+        self._response_headers = json.load(response_headers_file)
 
   def __call__(self, environ, start_response):
     """WSGI dispatch.
@@ -54,14 +81,11 @@ class ServerApp(object):
 
     logging.info('responding with %s', filename)
     suffix = path[path.rfind('.') + 1:]
-    start_response('200 OK', [('Content-Type',
-                               {'css': 'text/css',
-                                'html': 'text/html',
-                                'jpg': 'image/jpeg',
-                                'js': 'text/javascript',
-                                'png': 'image/png',
-                                'ttf': 'font/ttf',
-                              }[suffix])])
+    headers = [('Content-Type', _CONTENT_TYPE_FOR_SUFFIX[suffix])]
+    if path in self._response_headers:
+      for header in self._response_headers[path]:
+        headers.append((str(header[0]), str(header[1])))
+    start_response('200 OK', headers)
     return file(filename).read()
 
 
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index 2daa271..d687b13 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -2,9 +2,12 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import httplib
+import json
 import os
-import socket
+import shutil
 import sys
+import tempfile
 import unittest
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -12,41 +15,101 @@ _SRC_DIR = os.path.abspath(os.path.join(
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'android', 'loading'))
 
 import options
+from trace_test import test_server
 from trace_test import webserver_test
 
 
 OPTIONS = options.OPTIONS
 
 
-class TracingTrackTestCase(unittest.TestCase):
+class WebServerTestCase(unittest.TestCase):
   def setUp(self):
-    OPTIONS.ParseArgs('', extra=[('--noisy', False)])
-
-  def testWebserver(self):
-    with webserver_test.TemporaryDirectory() as temp_dir:
-      test_html = file(os.path.join(temp_dir, 'test.html'), 'w')
-      test_html.write('<!DOCTYPE html><html><head><title>Test</title></head>'
-                      '<body><h1>Test Page</h1></body></html>')
-      test_html.close()
-
-      server = webserver_test.WebServer(temp_dir, temp_dir)
-      server.Start()
-      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-      host, port = server.Address().split(':')
-      sock.connect((host, int(port)))
-      sock.sendall('GET null HTTP/1.1\n\n')
-      data = sock.recv(4096)
-      self.assertTrue(data.startswith('HTTP/1.0 404 Not Found'))
-      sock.close()
-
-      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-      sock.connect((host, int(port)))
-      sock.sendall('GET test.html HTTP/1.1\n\n')
-      data = sock.recv(4096)
-      self.assertTrue('HTTP/1.0 200 OK' in data)
-
-      sock.close()
-      self.assertTrue(server.Stop())
+    if not OPTIONS._parsed_args:
+      OPTIONS.ParseArgs('', extra=[('--noisy', False)])
+    self._temp_dir = tempfile.mkdtemp()
+    self._server = webserver_test.WebServer(self._temp_dir, self._temp_dir)
+
+  def tearDown(self):
+    self.assertTrue(self._server.Stop())
+    shutil.rmtree(self._temp_dir)
+
+  def StartServer(self):
+    self._server.Start()
+
+  def WriteFile(self, path, file_content):
+    with open(os.path.join(self._temp_dir, path), 'w') as file_output:
+      file_output.write(file_content)
+
+  def Request(self, path):
+    host, port = self._server.Address().split(':')
+    connection = httplib.HTTPConnection(host, int(port))
+    connection.request('GET', path)
+    response = connection.getresponse()
+    connection.close()
+    return response
+
+  def testWebserverBasic(self):
+    self.WriteFile('test.html',
+               '<!DOCTYPE html><html><head><title>Test</title></head>'
+               '<body><h1>Test Page</h1></body></html>')
+    self.StartServer()
+
+    response = self.Request('test.html')
+    self.assertEqual(200, response.status)
+
+    response = self.Request('/test.html')
+    self.assertEqual(200, response.status)
+
+    response = self.Request('///test.html')
+    self.assertEqual(200, response.status)
+
+  def testWebserver404(self):
+    self.StartServer()
+
+    response = self.Request('null')
+    self.assertEqual(404, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+
+  def testContentType(self):
+    self.WriteFile('test.html',
+               '<!DOCTYPE html><html><head><title>Test</title></head>'
+               '<body><h1>Test Page</h1></body></html>')
+    self.WriteFile('blobfile',
+               'whatever')
+    self.StartServer()
+
+    response = self.Request('test.html')
+    self.assertEqual(200, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+
+    response = self.Request('blobfile')
+    self.assertEqual(500, response.status)
+
+  def testCustomResponseHeader(self):
+    self.WriteFile('test.html',
+               '<!DOCTYPE html><html><head><title>Test</title></head>'
+               '<body><h1>Test Page</h1></body></html>')
+    self.WriteFile('test2.html',
+               '<!DOCTYPE html><html><head><title>Test 2</title></head>'
+               '<body><h1>Test Page 2</h1></body></html>')
+    self.WriteFile(test_server.RESPONSE_HEADERS_PATH,
+               json.dumps({'test2.html': [['Cache-Control', 'no-store']]}))
+    self.StartServer()
+
+    response = self.Request('test.html')
+    self.assertEqual(200, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+    self.assertEqual(None, response.getheader('cache-control'))
+
+    response = self.Request('test2.html')
+    self.assertEqual(200, response.status)
+    self.assertEqual('text/html', response.getheader('content-type'))
+    self.assertEqual('no-store', response.getheader('cache-control'))
+
+    response = self.Request(test_server.RESPONSE_HEADERS_PATH)
+    self.assertEqual(200, response.status)
+    self.assertEqual('application/json', response.getheader('content-type'))
+    self.assertEqual(None, response.getheader('cache-control'))
 
 
 if __name__ == '__main__':

commit 0a75bdcee248f512eae24f754ebfa963d1ded098
Author: droger <droger@chromium.org>
Date:   Mon Mar 21 03:26:20 2016 -0700

    tools/android/loading Add support for headless mode in analyse.py
    
    Review URL: https://codereview.chromium.org/1814863003
    
    Cr-Original-Commit-Position: refs/heads/master@{#382263}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 63512b75fc78ecb89e3740224ddf902879ee374d

diff --git a/loading/analyze.py b/loading/analyze.py
index 4b72ec6..2aed605 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -314,6 +314,8 @@ def main():
       'prefetch_delay_seconds', 5,
       'delay after requesting load of prefetch page '
       '(only when running full fetch)')
+  OPTIONS.AddGlobalArgument(
+      'headless', False, 'Do not display Chrome UI (only works in local mode).')
 
   parser = argparse.ArgumentParser(description='Analyzes loading')
   parser.add_argument('command', help=' '.join(COMMAND_MAP.keys()))
diff --git a/loading/controller.py b/loading/controller.py
index 2d9c3bd..9226ab3 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -233,11 +233,18 @@ class LocalChromeController(ChromeControllerBase):
       profile_dir = tempfile.mkdtemp()
     flags = ['--user-data-dir=%s' % profile_dir] + flags
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-    process = subprocess.Popen(
-        [binary_filename] + flags, shell=False, stderr=chrome_out)
+    environment = os.environ.copy()
+    if OPTIONS.headless:
+      environment['DISPLAY'] = 'localhost:99'
+      xvfb_process = subprocess.Popen(
+          ['Xvfb', ':99', '-screen', '0', '1600x1200x24'], shell=False,
+          stderr=chrome_out)
+    chrome_process = subprocess.Popen(
+        [binary_filename] + flags, shell=False, stderr=chrome_out,
+        env=environment)
     try:
       time.sleep(10)
-      process_result = process.poll()
+      process_result = chrome_process.poll()
       if process_result is not None:
         logging.error('Unexpected process exit: %s', process_result)
       else:
@@ -246,6 +253,8 @@ class LocalChromeController(ChromeControllerBase):
         self._StartConnection(connection)
         yield connection
     finally:
-      process.kill()
+      chrome_process.kill()
+      if OPTIONS.headless:
+        xvfb_process.kill()
       if using_temp_profile_dir:
         shutil.rmtree(profile_dir)

commit 9b26136d503f4f7ede2bc95432377de85629d214
Author: gabadie <gabadie@chromium.org>
Date:   Fri Mar 18 09:44:35 2016 -0700

    sandwich: Use the ChromeController API.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1814023002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381992}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: dac49a12229b0e2b799ef40f706ca604c30d6700

diff --git a/loading/controller.py b/loading/controller.py
index 09c794b..2d9c3bd 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -49,6 +49,7 @@ class ChromeControllerBase(object):
         '--enable-test-events',
         '--remote-debugging-port=%d' % OPTIONS.devtools_port,
     ]
+    self._chrome_wpr_specific_args = []
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
@@ -98,7 +99,10 @@ class ChromeControllerBase(object):
     Args:
       network_name: (str) Key from emulation.NETWORK_CONDITIONS.
     """
-    self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
+    if network_name:
+      self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
+    else:
+      self._emulated_network = None
 
   def _StartConnection(self, connection):
     """This should be called after opening an appropriate connection."""
@@ -114,6 +118,10 @@ class ChromeControllerBase(object):
     if self._clear_cache:
       connection.AddHook(connection.ClearCache)
 
+  def _GetChromeArguments(self):
+    """Get command-line arguments for the chrome execution."""
+    return self._chrome_args + self._chrome_wpr_specific_args
+
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
@@ -141,7 +149,7 @@ class RemoteChromeController(ChromeControllerBase):
     self._device.KillAll(package_info.package, quiet=True)
 
     with device_setup.FlagReplacer(
-        self._device, command_line_path, self._chrome_args):
+        self._device, command_line_path, self._GetChromeArguments()):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
@@ -189,6 +197,20 @@ class RemoteChromeController(ChromeControllerBase):
     """
     self._slow_death = slow_death
 
+  @contextlib.contextmanager
+  def OpenWprHost(self, wpr_archive_path, record=False,
+                  network_condition_name=None,
+                  disable_script_injection=False):
+    """Starts a WPR host, overrides Chrome flags until contextmanager exit."""
+    assert not self._chrome_wpr_specific_args, 'WPR is already running.'
+    with device_setup.WprHost(self._device, wpr_archive_path,
+        record=record,
+        network_condition_name=network_condition_name,
+        disable_script_injection=disable_script_injection) as additional_flags:
+      self._chrome_wpr_specific_args = additional_flags
+      yield
+    self._chrome_wpr_specific_args = []
+
 
 class LocalChromeController(ChromeControllerBase):
   """Controller for a local (desktop) chrome instance.
@@ -206,7 +228,7 @@ class LocalChromeController(ChromeControllerBase):
     binary_filename = OPTIONS.local_binary
     profile_dir = OPTIONS.local_profile_dir
     using_temp_profile_dir = profile_dir is None
-    flags = self._chrome_args
+    flags = self._GetChromeArguments()
     if using_temp_profile_dir:
       profile_dir = tempfile.mkdtemp()
     flags = ['--user-data-dir=%s' % profile_dir] + flags
diff --git a/loading/sandwich.py b/loading/sandwich.py
index b4e2590..4657eaf 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -33,6 +33,7 @@ import devil_chromium
 
 import chrome_cache
 import chrome_setup
+import controller
 import device_setup
 import devtools_monitor
 import frame_load_lens
@@ -151,8 +152,7 @@ class SandwichRunner(object):
     # Configures whether the WPR archive should be read or generated.
     self.wpr_record = False
 
-    self._device = None
-    self._chrome_additional_flags = []
+    self._chrome_ctl = None
     self._local_cache_directory_path = None
 
   def PullConfigFromArgs(self, args):
@@ -200,35 +200,27 @@ class SandwichRunner(object):
     return None
 
   def _RunNavigation(self, url, clear_cache, trace_id=None):
-    with device_setup.DeviceConnection(
-        device=self._device,
-        additional_flags=self._chrome_additional_flags) as connection:
-      additional_metadata = {}
-      if self._GetEmulatorNetworkCondition('browser'):
-        additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
-            connection=connection,
-            emulated_device_name=None,
-            emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
-      trace = trace_recorder.MonitorUrl(
-          connection, url,
-          clear_cache=clear_cache,
-          categories=pull_sandwich_metrics.CATEGORIES,
-          timeout=_DEVTOOLS_TIMEOUT)
-      trace.metadata.update(additional_metadata)
-      if trace_id != None and self.trace_output_directory:
-        trace_path = os.path.join(
-            self.trace_output_directory, str(trace_id), 'trace.json')
-        os.makedirs(os.path.dirname(trace_path))
-        trace.ToJsonFile(trace_path)
+    self._chrome_ctl.SetClearCache(clear_cache)
+    self._chrome_ctl.SetNetworkEmulation(
+        self._GetEmulatorNetworkCondition('browser'))
+    # TODO(gabadie): add a way to avoid recording a trace.
+    trace = loading_trace.LoadingTrace.FromUrlAndController(
+        url=url,
+        controller=self._chrome_ctl,
+        categories=pull_sandwich_metrics.CATEGORIES,
+        timeout_seconds=_DEVTOOLS_TIMEOUT)
+    if trace_id != None and self.trace_output_directory:
+      trace_path = os.path.join(
+          self.trace_output_directory, str(trace_id), 'trace.json')
+      os.makedirs(os.path.dirname(trace_path))
+      trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, trace_id=0):
     clear_cache = False
     if self.cache_operation == 'clear':
       clear_cache = True
     elif self.cache_operation == 'push':
-      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-      chrome_cache.PushBrowserCache(self._device,
-                                    self._local_cache_directory_path)
+      self._chrome_ctl.PushBrowserCache(self._local_cache_directory_path)
     elif self.cache_operation == 'reload':
       self._RunNavigation(url, clear_cache=True)
     elif self.cache_operation == 'save':
@@ -239,27 +231,24 @@ class SandwichRunner(object):
     assert self.cache_operation == 'save'
     assert self.cache_archive_path, 'Need to specify where to save the cache'
 
-    # Move Chrome to background to allow it to flush the index.
-    self._device.adb.Shell('am start com.google.android.launcher')
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-
-    cache_directory_path = chrome_cache.PullBrowserCache(self._device)
+    cache_directory_path = self._chrome_ctl.PullBrowserCache()
     chrome_cache.ZipDirectoryContent(
         cache_directory_path, self.cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
   def Run(self):
-    """SandwichRunner main entry point meant to be called once configured.
-    """
+    """SandwichRunner main entry point meant to be called once configured."""
+    assert self._chrome_ctl == None
+    assert self._local_cache_directory_path == None
     if self.trace_output_directory:
       self._CleanTraceOutputDirectory()
 
-    self._device = device_utils.DeviceUtils.HealthyDevices()[0]
-    self._chrome_additional_flags = []
+    # TODO(gabadie): Make sandwich working on desktop.
+    device = device_utils.DeviceUtils.HealthyDevices()[0]
+    self._chrome_ctl = controller.RemoteChromeController(device)
+    if self.cache_operation == 'save':
+      self._chrome_ctl.SetSlowDeath()
 
-    assert self._local_cache_directory_path == None
     if self.cache_operation == 'push':
       assert os.path.isfile(self.cache_archive_path)
       self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
@@ -267,12 +256,11 @@ class SandwichRunner(object):
           self.cache_archive_path, self._local_cache_directory_path)
 
     ran_urls = []
-    with device_setup.WprHost(self._device, self.wpr_archive_path,
+    with self._chrome_ctl.OpenWprHost(self.wpr_archive_path,
         record=self.wpr_record,
         network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
         disable_script_injection=self.disable_wpr_script_injection
-        ) as additional_flags:
-      self._chrome_additional_flags.extend(additional_flags)
+        ):
       for _ in xrange(self.job_repeat):
         for url in self.urls:
           self._RunUrl(url, trace_id=len(ran_urls))
@@ -286,6 +274,8 @@ class SandwichRunner(object):
     if self.trace_output_directory:
       self._SaveRunInfos(ran_urls)
 
+    self._chrome_ctl = None
+
 
 def _ArgumentParser():
   """Build a command line argument's parser."""

commit bd92f85e7d36c0058e26ac88daf7b8752d44007b
Author: mattcary <mattcary@chromium.org>
Date:   Fri Mar 18 06:17:08 2016 -0700

    Clovis: add core sets.
    
    This adds the core set computation to resource_sack.py (computing the common resources over a set of runs) as well as a script for computing and comparing the core sets of traces.
    
    Review URL: https://codereview.chromium.org/1804053002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381948}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8e34fbee0755ce47548153ef4fec89d754e78f4a

diff --git a/loading/core_set.py b/loading/core_set.py
new file mode 100644
index 0000000..8ab9493
--- /dev/null
+++ b/loading/core_set.py
@@ -0,0 +1,205 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Compute core set for a page.
+
+This script is a collection of utilities for working with core sets.
+"""
+
+import argparse
+import glob
+import json
+import logging
+import multiprocessing
+import os
+import sys
+
+import loading_model
+import loading_trace
+import resource_sack
+
+
+def _Progress(x):
+  sys.stderr.write(x + '\n')
+
+
+def _PageCore(prefix, graph_set_names, output):
+  """Compute the page core over sets defined by graph_set_names."""
+  assert graph_set_names
+  graph_sets = []
+  sack = resource_sack.GraphSack()
+  for name in graph_set_names:
+    name_graphs = []
+    _Progress('Processing %s' % name)
+    for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
+      _Progress('Reading %s' % filename)
+      graph = loading_model.ResourceGraph(
+          loading_trace.LoadingTrace.FromJsonFile(filename))
+      sack.ConsumeGraph(graph)
+      name_graphs.append(graph)
+    graph_sets.append(name_graphs)
+  json.dump({'page_core': [l for l in sack.CoreSet(*graph_sets)],
+             'threshold': sack.CORE_THRESHOLD},
+            output, sort_keys=True, indent=2)
+  output.write('\n')
+
+
+def _DoSite(site, graph_sets, input_dir, output_dir):
+  """Compute the appropriate page core for a site.
+
+  Used by _Spawn.
+  """
+  _Progress('Doing %s on %s' % (site, '/'.join(graph_sets)))
+  prefix = os.path.join(input_dir, site)
+  with file(os.path.join(output_dir,
+                         '%s-%s.json' % (site, '.'.join(graph_sets))),
+            'w') as output:
+    _PageCore(prefix, graph_sets, output)
+
+
+def _DoSiteRedirect(t):
+  """Unpack arguments for map call.
+
+  Note that multiprocessing.Pool.map cannot use a lambda (as it needs to be
+  serialized into the executing process).
+  """
+  _DoSite(*t)
+
+
+def _Spawn(site_list_file, graph_sets, input_dir, output_dir, workers):
+  """Spool site computation out to a multiprocessing pool."""
+  with file(site_list_file) as site_file:
+    sites = [l.strip() for l in site_file.readlines()]
+  _Progress('Using sites:\n %s' % '\n '.join(sites))
+  pool = multiprocessing.Pool(workers, maxtasksperchild=1)
+  pool.map(_DoSiteRedirect, [(s, graph_sets, input_dir, output_dir)
+                             for s in sites])
+
+
+def _AllCores(prefix, graph_set_names, output, threshold):
+  """Compute all core sets (per-set and overall page core) for a site."""
+  core_sets = []
+  _Progress('Using threshold %s' % threshold)
+  big_sack = resource_sack.GraphSack()
+  graph_sets = []
+  for name in graph_set_names:
+    _Progress('Finding core set for %s' % name)
+    sack = resource_sack.GraphSack()
+    sack.CORE_THRESHOLD = threshold
+    this_set = []
+    for filename in glob.iglob('-'.join([prefix, name, '*.trace'])):
+      _Progress('Reading %s' % filename)
+      graph = loading_model.ResourceGraph(
+          loading_trace.LoadingTrace.FromJsonDict(json.load(open(filename))))
+      sack.ConsumeGraph(graph)
+      big_sack.ConsumeGraph(graph)
+      this_set.append(graph)
+    core_sets.append({
+        'set_name': name,
+        'core_set': [l for l in sack.CoreSet()]
+    })
+    graph_sets.append(this_set)
+  json.dump({'core_sets': core_sets,
+             'page_core': [l for l in big_sack.CoreSet(*graph_sets)]},
+            output, sort_keys=True, indent=2)
+
+
+def _ReadCoreSet(filename):
+  data = json.load(open(filename))
+  return set(data['page_core'])
+
+
+def _Compare(a_name, b_name, csv):
+  """Compare two core sets."""
+  a = _ReadCoreSet(a_name)
+  b = _ReadCoreSet(b_name)
+  result = (resource_sack.GraphSack.CoreSimilarity(a, b),
+            '  Equal' if a == b else 'UnEqual',
+            'a<=b' if a <= b else 'a!<b',
+            'a>=b' if b <= a else 'a!>b')
+  if csv:
+    print '%s,%s,%s,%s' % result
+  else:
+    print '%.2f %s %s %s' % result
+
+
+if __name__ == '__main__':
+  logging.basicConfig(level=logging.ERROR)
+  parser = argparse.ArgumentParser()
+  subparsers = parser.add_subparsers()
+
+  spawn = subparsers.add_parser(
+      'spawn', help=('spawn page core set computation from a sites list.\n'
+                     'A core set will be computed for each site by '
+                     'combining all run indicies from site traces for each '
+                     '--set, then computing the page core over the sets. '
+                     'Assumes trace file names in form {input-dir}/'
+                     '{site}-{set}-{run index}.trace'))
+  spawn.add_argument('--sets', required=True,
+                     help='sets to combine, comma-separated')
+  spawn.add_argument('--sites', required=True, help='file containing sites')
+  spawn.add_argument('--workers', default=8, type=int,
+                     help=('number of parallel workers. Each worker seems to '
+                           'use about 0.5-1G/trace when processing. Total '
+                           'memory usage should be kept less than physical '
+                           'memory for the job to run in a reasonable time'))
+  spawn.add_argument('--input_dir', required=True,
+                     help='trace input directory')
+  spawn.add_argument('--output_dir', required=True,
+                     help=('core set output directory. Each site will have one '
+                           'JSON file generated listing the core set as well '
+                           'as some metadata like the threshold used'))
+  spawn.set_defaults(executor=lambda args:
+                     _Spawn(site_list_file=args.sites,
+                            graph_sets=args.sets.split(','),
+                            input_dir=args.input_dir,
+                            output_dir=args.output_dir,
+                            workers=args.workers))
+
+  page_core = subparsers.add_parser(
+      'page_core',
+      help=('compute page core set for a group of files of form '
+            '{--prefix}{set}*.trace over each set in --sets'))
+  page_core.add_argument('--sets', required=True,
+                       help='sets to combine, comma-separated')
+  page_core.add_argument('--prefix', required=True,
+                           help='trace file prefix')
+  page_core.add_argument('--output', required=True,
+                           help='JSON output file name')
+  page_core.set_defaults(
+      executor=lambda args:
+      _PageCore(args.prefix, args.sets.split(','), file(args.output, 'w')))
+
+  all_cores = subparsers.add_parser(
+      'all_cores',
+      help=('compute core and page core sets. Computes the core for each set '
+            'in --sets and then the overall page core using trace files '
+            'of form {--prefix}{set}*.trace. Outputs all the sets as JSON'))
+  all_cores.add_argument('--sets', required=True,
+                         help='sets to combine, comma-separated')
+  all_cores.add_argument('--prefix', required=True,
+                         help='input file prefix')
+  all_cores.add_argument('--output', required=True,
+                         help='JSON output file name')
+  all_cores.add_argument('--threshold',
+                         default=resource_sack.GraphSack.CORE_THRESHOLD,
+                         type=float, help='core set threshold')
+  all_cores.set_defaults(
+      executor=lambda args:
+      _AllCores(args.prefix, args.sets.split(','), file(args.output, 'w'),
+                args.threshold))
+
+  compare = subparsers.add_parser(
+      'compare',
+      help=('compare two core sets (as output by spawn, page_core or '
+            'all_cores) using Jaccard index. Outputs on stdout'))
+  compare.add_argument('--a', required=True, help='the first core set JSON')
+  compare.add_argument('--b', required=True, help='the second core set JSON')
+  compare.add_argument('--csv', action='store_true', help='output as CSV')
+  compare.set_defaults(
+      executor=lambda args:
+      _Compare(args.a, args.b, args.csv))
+
+  args = parser.parse_args()
+  args.executor(args)
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index f1182e5..9c174c5 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -32,6 +32,9 @@ class GraphSack(object):
   DAG. The edges are annotated with list of graphs and nodes that generated
   them.
   """
+  # See CoreSet().
+  CORE_THRESHOLD = 0.8
+
   _GraphInfo = collections.namedtuple('_GraphInfo', (
       'cost',   # The graph cost (aka critical path length).
       'total_costs',  # A vector by node index of total cost of each node.
@@ -84,6 +87,50 @@ class GraphSack(object):
     self._url_to_bag[node.Url()].AddNode(graph, node)
     return self._url_to_bag[node.Url()]
 
+  def CoreSet(self, *graph_sets):
+    """Compute the core set of this sack.
+
+    The core set of a sack is the set of resource that are common to most of the
+    graphs in the sack. A core set of a set of graphs are the resources that
+    appear with frequency at least CORE_THRESHOLD. For a collection of graph
+    sets, for instance pulling the same page under different network
+    connections, we intersect the core sets to produce a page core set that
+    describes the key resources used by the page. See https://goo.gl/F1BoEB for
+    context and discussion.
+
+    Args:
+      graph_sets: one or more collection of graphs to compute core sets. If one
+        graph set is given, its core set is computed. If more than one set is
+        given, the page core set of all sets is computed (the intersection of
+        core sets). If no graph set is given, the core of all graphs is
+        computed.
+
+    Returns:
+      A set of bag labels (as strings) in the core set.
+    """
+    if not graph_sets:
+      graph_sets = [self._graph_info.keys()]
+    return reduce(lambda a, b: a & b,
+                  (self._SingleCore(s) for s in graph_sets))
+
+  @classmethod
+  def CoreSimilarity(cls, a, b):
+    """Compute the similarity of two core sets.
+
+    We use the Jaccard index. See https://goo.gl/F1BoEB for discussion.
+
+    Args:
+      a: The first core set, as a set of strings.
+      b: The second core set, as a set of strings.
+
+    Returns:
+      A similarity score between zero and one. If both sets are empty the
+      similarity is zero.
+    """
+    if not a and not b:
+      return 0
+    return float(len(a & b)) / len(a | b)
+
   def FilterOccurrence(self, tag, filter_from_graph):
     """Accumulate filter occurrences for each bag in the graph.
 
@@ -101,6 +148,10 @@ class GraphSack(object):
       bag.MarkOccurrence(tag, filter_from_graph)
 
   @property
+  def num_graphs(self):
+    return len(self.graph_info)
+
+  @property
   def graph_info(self):
     return self._graph_info
 
@@ -108,6 +159,17 @@ class GraphSack(object):
   def bags(self):
     return self._bags
 
+  def _SingleCore(self, graph_set):
+    core = set()
+    graph_set = set(graph_set)
+    num_graphs = len(graph_set)
+    for b in self.bags:
+      count = sum([g in graph_set for g in b.graphs])
+      if float(count) / num_graphs > self.CORE_THRESHOLD:
+        core.add(b.label)
+    return core
+
+
 class Bag(dag.Node):
   def __init__(self, sack, index, url):
     super(Bag, self).__init__(index)
@@ -227,7 +289,10 @@ class Bag(dag.Node):
   def _MakeShortname(cls, url):
     parsed = urlparse.urlparse(url)
     if parsed.scheme == 'data':
-      kind, _ = parsed.path.split(';', 1)
+      if ';' in parsed.path:
+        kind, _ = parsed.path.split(';', 1)
+      else:
+        kind, _ = parsed.path.split(',', 1)
       return 'data:' + kind
     path = parsed.path[:10]
     hostname = parsed.hostname if parsed.hostname else '?.?.?'
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index 2034eba..df0fdb7 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -10,6 +10,13 @@ from test_utils import (MakeRequest,
 
 
 class ResourceSackTestCase(unittest.TestCase):
+  def SimpleGraph(self, node_names):
+    """Create a simple graph from a list of nodes."""
+    requests = [MakeRequest(node_names[0], 'null')]
+    for n in node_names[1:]:
+      requests.append(MakeRequest(n, node_names[0]))
+    return TestResourceGraph.FromRequestList(requests)
+
   def test_NodeMerge(self):
     g1 = TestResourceGraph.FromRequestList([
         MakeRequest(0, 'null'),
@@ -66,10 +73,8 @@ class ResourceSackTestCase(unittest.TestCase):
     shape1 = [MakeRequest(0, 'null'), MakeRequest(1, 0), MakeRequest(2, 0)]
     shape2 = [MakeRequest(0, 'null'), MakeRequest(1, 0),
               MakeRequest(3, 0), MakeRequest(4, 1)]
-    graphs = [TestResourceGraph.FromRequestList(shape1),
-              TestResourceGraph.FromRequestList(shape1),
-              TestResourceGraph.FromRequestList(shape1),
-              TestResourceGraph.FromRequestList(shape2)]
+    graphs = [TestResourceGraph.FromRequestList(s)
+              for s in (shape1, shape1,  shape1, shape2)]
     sack = resource_sack.GraphSack()
     for g in graphs:
       sack.ConsumeGraph(g)
@@ -89,6 +94,46 @@ class ResourceSackTestCase(unittest.TestCase):
     self.assertAlmostEqual(1, labels['3/'].GetOccurrence('test'), 3)
     self.assertAlmostEqual(0, labels['4/'].GetOccurrence('test'), 3)
 
+  def test_Core(self):
+    # We will use a core threshold of 0.5 to make it easier to define
+    # graphs. Resources 0 and 1 are core and others are not.
+    graphs = [self.SimpleGraph([0, 1, 2]),
+              self.SimpleGraph([0, 1, 3]),
+              self.SimpleGraph([0, 1, 4]),
+              self.SimpleGraph([0, 5])]
+    sack = resource_sack.GraphSack()
+    sack.CORE_THRESHOLD = 0.5
+    for g in graphs:
+      sack.ConsumeGraph(g)
+    self.assertEqual(set(['0/', '1/']), sack.CoreSet())
+
+  def test_IntersectingCore(self):
+    # Graph set A has core set {0, 1} and B {0, 2} so the final core set should
+    # be {0}. Set C makes sure we restrict core computation to tags A and B.
+    set_A = [self.SimpleGraph([0, 1, 2]),
+             self.SimpleGraph([0, 1, 3])]
+    set_B = [self.SimpleGraph([0, 2, 3]),
+             self.SimpleGraph([0, 2, 1])]
+    set_C = [self.SimpleGraph([2 * i + 4, 2 * i + 5]) for i in xrange(5)]
+    sack = resource_sack.GraphSack()
+    sack.CORE_THRESHOLD = 0.5
+    for g in set_A + set_B + set_C:
+      sack.ConsumeGraph(g)
+    self.assertEqual(set(), sack.CoreSet())
+    self.assertEqual(set(['0/', '1/']), sack.CoreSet(set_A))
+    self.assertEqual(set(['0/', '2/']), sack.CoreSet(set_B))
+    self.assertEqual(set(), sack.CoreSet(set_C))
+    self.assertEqual(set(['0/']), sack.CoreSet(set_A, set_B))
+    self.assertEqual(set(), sack.CoreSet(set_A, set_B, set_C))
+
+  def test_Simililarity(self):
+    self.assertAlmostEqual(
+        0.5,
+        resource_sack.GraphSack.CoreSimilarity(
+            set([1, 2, 3]), set([1, 3, 4])))
+    self.assertEqual(
+        0, resource_sack.GraphSack.CoreSimilarity(set(), set()))
+
 
 if __name__ == '__main__':
   unittest.main()

commit d5706494eeee1285ff5b96ecc5af0065a2f69789
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 16 07:12:33 2016 -0700

    clovis: Add step to event matching to the tracing track.
    
    Steps are used in traces to annotate aync events. One such annotation is
    "Preload", tagging resource fetches. It looks like this in the JSON:
    
            {
                "cat": "blink.net",
                "id": "0xaf9f4a094d99b1a0",
                "name": "Resource",
                "ph": "S",
                "pid": 32197,
                "tid": 1,
                "ts": 2252598472497,
                "tts": 52249
            },
            {
                "args": {
                    "step": "Preload"
                },
                "cat": "blink.net",
                "id": "0xaf9f4a094d99b1a0",
                "name": "Resource",
                "ph": "T",
                "pid": 32197,
                "tid": 1,
                "ts": 2252598472528,
                "tts": 52280
            },
    
    Review URL: https://codereview.chromium.org/1802973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381449}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b852db43b8cbc2f2477b45a344605660a2b7f7be

diff --git a/loading/tracing.py b/loading/tracing.py
index 5798b1e..7403a9d 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -123,6 +123,43 @@ class TracingTrack(devtools_monitor.Track):
         tracing_track._base_msec = e.start_msec
     return tracing_track
 
+  def OverlappingEvents(self, start_msec, end_msec):
+    self._IndexEvents()
+    return self._interval_tree.OverlappingEvents(start_msec, end_msec)
+
+  def EventsEndingBetween(self, start_msec, end_msec):
+    """Gets the list of events ending within an interval.
+
+    Args:
+      start_msec: the start of the range to query, in milliseconds, inclusive.
+      end_msec: the end of the range to query, in milliseconds, inclusive.
+
+    Returns:
+      See OverlappingEvents() above.
+    """
+    overlapping_events = self.OverlappingEvents(start_msec, end_msec)
+    return [e for e in overlapping_events
+            if start_msec <= e.end_msec <= end_msec]
+
+  def EventFromStep(self, step_event):
+    """Returns the Event associated with a step event, or None.
+
+    Args:
+      step_event: (Event) Step event.
+
+    Returns:
+      an Event that matches the step event, or None.
+    """
+    self._IndexEvents()
+    assert 'step' in step_event.args and step_event.tracing_event['ph'] == 'T'
+    candidates = self._interval_tree.EventsAt(step_event.start_msec)
+    for event in candidates:
+      # IDs are only unique within a process (often they are pointers).
+      if (event.pid == step_event.pid and event.tracing_event['ph'] != 'T'
+          and event.name == step_event.name and event.id == step_event.id):
+        return event
+    return None
+
   def _IndexEvents(self, strict=False):
     if self._interval_tree:
       return
@@ -144,24 +181,6 @@ class TracingTrack(devtools_monitor.Track):
           'Pending spanning events: %s' %
           '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
 
-  def OverlappingEvents(self, start_msec, end_msec):
-    self._IndexEvents()
-    return self._interval_tree.OverlappingEvents(start_msec, end_msec)
-
-  def EventsEndingBetween(self, start_msec, end_msec):
-    """Gets the list of events ending within an interval.
-
-    Args:
-      start_msec: the start of the range to query, in milliseconds, inclusive.
-      end_msec: the end of the range to query, in milliseconds, inclusive.
-
-    Returns:
-      See OverlappingEvents() above.
-    """
-    overlapping_events = self.OverlappingEvents(start_msec, end_msec)
-    return [e for e in overlapping_events
-            if start_msec <= e.end_msec <= end_msec]
-
   def _GetEvents(self):
     self._IndexEvents()
     return self._interval_tree.GetEvents()
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index ade5f8d..8d946b7 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -214,10 +214,7 @@ class TracingTrackTestCase(unittest.TestCase):
           event.tracing_event, deserialized_event.tracing_event)
 
   def testTracingTrackSerialization(self):
-    events = self._MIXED_EVENTS
-    self.track.Handle('Tracing.dataCollected',
-                      {'params': {'value': [self.EventToMicroseconds(e)
-                                            for e in events]}})
+    self._HandleEvents(self._MIXED_EVENTS)
     json_dict = self.track.ToJsonDict()
     self.assertTrue('events' in json_dict)
     deserialized_track = TracingTrack.FromJsonDict(json_dict)
@@ -227,9 +224,7 @@ class TracingTrackTestCase(unittest.TestCase):
       self.assertEquals(e1.tracing_event, e2.tracing_event)
 
   def testEventsEndingBetween(self):
-    self.track.Handle(
-        'Tracing.dataCollected', {'params': {'value': [
-            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self._HandleEvents(self._EVENTS)
     self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
                           for e in self.track.EventsEndingBetween(0, 100)]))
@@ -242,9 +237,7 @@ class TracingTrackTestCase(unittest.TestCase):
                           for e in self.track.EventsEndingBetween(3, 6)]))
 
   def testOverlappingEvents(self):
-    self.track.Handle(
-        'Tracing.dataCollected', {'params': {'value': [
-            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self._HandleEvents(self._EVENTS)
     self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
                           for e in self.track.OverlappingEvents(0, 100)]))
@@ -257,16 +250,55 @@ class TracingTrackTestCase(unittest.TestCase):
                      set([e.args['name']
                           for e in self.track.OverlappingEvents(6, 10.1)]))
 
+  def testEventFromStep(self):
+    events = [
+        {'ts': 5, 'ph': 'X', 'dur': 10, 'pid': 2, 'tid': 1, 'id': '0x123',
+         'name': 'B'},
+        {'ts': 5, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 1, 'id': '0x12343',
+        'name': 'A'}]
+    step_events = [{'ts': 6, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 4, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'T', 'pid': 12, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x1234',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'T', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'A', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'n', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {'step': 'Bla'}},
+                   {'ts': 6, 'ph': 'n', 'pid': 2, 'tid': 1, 'id': '0x123',
+                    'name': 'B', 'args': {}}]
+    self._HandleEvents(events + step_events)
+    trace_events = self.track.GetEvents()
+    self.assertEquals(9, len(trace_events))
+    # pylint: disable=unbalanced-tuple-unpacking
+    (event, _, step_event, outside, wrong_pid, wrong_id, wrong_name,
+     wrong_phase, no_step) = trace_events
+    self.assertEquals(event, self.track.EventFromStep(step_event))
+    self.assertIsNone(self.track.EventFromStep(outside))
+    self.assertIsNone(self.track.EventFromStep(wrong_pid))
+    self.assertIsNone(self.track.EventFromStep(wrong_id))
+    self.assertIsNone(self.track.EventFromStep(wrong_name))
+    # Invalid events
+    with self.assertRaises(AssertionError):
+      self.track.EventFromStep(wrong_phase)
+    with self.assertRaises(AssertionError):
+      self.track.EventFromStep(no_step)
+
   def testTracingTrackForThread(self):
-    self.track.Handle(
-        'Tracing.dataCollected', {'params': {'value': [
-            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self._HandleEvents(self._EVENTS)
     tracing_track = self.track.TracingTrackForThread((2, 1))
     self.assertTrue(tracing_track is not self.track)
     self.assertEquals(4, len(tracing_track.GetEvents()))
     tracing_track = self.track.TracingTrackForThread((2, 42))
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
+  def _HandleEvents(self, events):
+    self.track.Handle('Tracing.dataCollected', {'params': {'value': [
+        self.EventToMicroseconds(e) for e in events]}})
+
 
 class IntervalTreeTestCase(unittest.TestCase):
   class FakeEvent(object):

commit 7b500502789727b5cf6fbdbad4de5e65d588636c
Author: lizeb <lizeb@chromium.org>
Date:   Wed Mar 16 06:31:49 2016 -0700

    clovis: Ignore trace context events in tracing.py.
    
    Per https://goo.gl/Qabkqk, the '(' and ')' phases relate to Context
    Events. We don't process them currently, so we need to ignore them to
    prevent tracing.py from failing when indexing a trace.
    
    Review URL: https://codereview.chromium.org/1804413002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381446}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e7736901158ad527b98db7bcf4fc130aa300778e

diff --git a/loading/tracing.py b/loading/tracing.py
index 54ff3d0..5798b1e 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -183,6 +183,8 @@ class TracingTrack(devtools_monitor.Track):
           'M': self._Ignore,
           'X': self._Ignore,
           'R': self._Ignore,
+          '(': self._Ignore, # Context events.
+          ')': self._Ignore, # Ditto.
           None: self._Ignore,
           }
 

commit 8f0715dc597531828509a53cc565b7e6375bed81
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 16 06:06:46 2016 -0700

    tools/android/loading: Remove spaces from network emulation names to make scriptability easier.
    
    Review URL: https://codereview.chromium.org/1802743003
    
    Cr-Original-Commit-Position: refs/heads/master@{#381439}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e5aa4f8b6b4b06e0a163098f31748d048ff50b98

diff --git a/loading/emulation.py b/loading/emulation.py
index fa2fd89..d7f05c6 100644
--- a/loading/emulation.py
+++ b/loading/emulation.py
@@ -14,16 +14,16 @@ import json
 NETWORK_CONDITIONS = {
     'GPRS': {
         'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
-    'Regular 2G': {
+    'Regular2G': {
         'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
-    'Good 2G': {
+    'Good2G': {
         'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
-    'Regular 3G': {
+    'Regular3G': {
         'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
-    'Good 3G': {
+    'Good3G': {
         'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
         'latency': 40},
-    'Regular 4G': {
+    'Regular4G': {
         'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
         'latency': 20},
     'DSL': {

commit fd2ab36728f25059f35d42a7f7aac8385b7ded97
Author: newt <newt@chromium.org>
Date:   Tue Mar 15 16:35:11 2016 -0700

    Discourage use of android.app.AlertDialog and StringBuffer.
    
    android.support.v7.app.AlertDialog and StringBuilder should be used
    instead.
    
    BUG=4562
    
    Review URL: https://codereview.chromium.org/1804293002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381350}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6e35db99271ad9312089a3bc0d3474acff9ceef8

diff --git a/checkstyle/chromium-style-5.0.xml b/checkstyle/chromium-style-5.0.xml
index c8748ad..6557f61 100644
--- a/checkstyle/chromium-style-5.0.xml
+++ b/checkstyle/chromium-style-5.0.xml
@@ -124,7 +124,7 @@
       <property name="tokens" value="ASSIGN, BAND, BAND_ASSIGN, BOR, BOR_ASSIGN, BSR, BSR_ASSIGN, BXOR, BXOR_ASSIGN, COLON, DIV, DIV_ASSIGN, EQUAL, GE, GT, LAND, LE, LITERAL_ASSERT, LITERAL_CATCH, LITERAL_DO, LITERAL_ELSE, LITERAL_FINALLY, LITERAL_FOR, LITERAL_IF, LITERAL_RETURN, LITERAL_SYNCHRONIZED, LITERAL_TRY, LITERAL_WHILE, LOR, LT, MINUS, MINUS_ASSIGN, MOD, MOD_ASSIGN, NOT_EQUAL, PLUS, PLUS_ASSIGN, QUESTION, SL, SLIST, SL_ASSIGN, SR, SR_ASSIGN, STAR, STAR_ASSIGN, TYPE_EXTENSION_AND" />
       <property name="allowEmptyConstructors" value="true"/>
       <property name="allowEmptyMethods" value="true"/>
-   </module>
+    </module>
     <module name="WhitespaceAfter">
       <property name="severity" value="error"/>
       <property name="tokens" value="COMMA, SEMI, TYPECAST"/>
@@ -196,7 +196,21 @@
       <property name="tokens" value="COMMA"/>
       <property name="option" value="EOL"/>
     </module>
+    <module name="RegexpSinglelineJava">
+      <property name="severity" value="error"/>
+      <property name="format" value="StringBuffer"/>
+      <property name="ignoreComments" value="true"/>
+      <property name="message" value="Avoid StringBuffer; use StringBuilder instead, which is faster (it's not thread-safe, but this is almost never needed)"/>
+    </module>
+    <module name="RegexpSinglelineJava">
+      <property name="severity" value="warning"/>
+      <property name="format" value="android\.app\.AlertDialog"/>
+      <property name="ignoreComments" value="true"/>
+      <property name="message" value="Avoid android.app.AlertDialog; if possible, use android.support.v7.app.AlertDialog instead, which has a Material look on all devices. (Some parts of the codebase cant depend on the support library, in which case android.app.AlertDialog is the only option)"/>
+    </module>
   </module>
+
+  <!-- Non-TreeWalker modules -->
   <module name="FileTabCharacter">
     <property name="severity" value="error"/>
   </module>
@@ -204,7 +218,7 @@
     <property name="severity" value="error"/>
     <property name="format" value="[ \t]+$"/>
     <property name="message" value="Trailing whitespace"/>
-    </module>
+  </module>
   <module name="RegexpHeader">
     <property name="severity" value="error"/>
     <property name="header" value="^// Copyright 20\d\d The Chromium Authors. All rights reserved.$\n^// Use of this source code is governed by a BSD-style license that can be$\n^// found in the LICENSE file.$"/>

commit 4f11846d37bd1ca293ab789fd152ceaeddf956a6
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 15 02:48:52 2016 -0700

    tools/android/loading: Optimize chrome_cache.{Pull,Push}BrowserCache()
    
    Some websites have a lot of resources, slowing down sandwich's
    cache pulling and pushing. This CL optimizes these operations
    by pulling the cache directory recursively, but also by
    introducing the commands queue that are written to a temporary
    shell file and then pushed and ran on the device.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1753343002
    
    Cr-Original-Commit-Position: refs/heads/master@{#381200}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 752434c777d2e7b445e37629bf89ecc7c38c7c5c

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 40d15bb..bdbe1e4 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -20,6 +20,7 @@ _SRC_DIR = os.path.abspath(os.path.join(
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 
+import device_setup
 import options
 
 
@@ -43,21 +44,10 @@ def _RemoteCacheDirectory():
       constants.PACKAGE_INFO[OPTIONS.chrome_package_name].package)
 
 
-def _UpdateTimestampFromAdbStat(filename, stat):
-  os.utime(filename, (stat.st_time, stat.st_time))
-
-
 def _AdbShell(adb, cmd):
   adb.Shell(subprocess.list2cmdline(cmd))
 
 
-def _AdbUtime(adb, filename, timestamp):
-  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
-  """
-  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
-  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
-
-
 def PullBrowserCache(device):
   """Pulls the browser cache from the device and saves it locally.
 
@@ -71,8 +61,16 @@ def PullBrowserCache(device):
   _REAL_INDEX_FILE_NAME = 'the-real-index'
 
   remote_cache_directory = _RemoteCacheDirectory()
-  print remote_cache_directory
   save_target = tempfile.mkdtemp(suffix='.cache')
+
+  # Pull the cache recursively.
+  device.adb.Pull(remote_cache_directory, save_target)
+
+  # Update the modification time stamp on the local cache copy.
+  def _UpdateTimestampFromAdbStat(filename, stat):
+    assert os.path.exists(filename)
+    os.utime(filename, (stat.st_time, stat.st_time))
+
   for filename, stat in device.adb.Ls(remote_cache_directory):
     if filename == '..':
       continue
@@ -81,7 +79,6 @@ def PullBrowserCache(device):
       continue
     original_file = os.path.join(remote_cache_directory, filename)
     saved_file = os.path.join(save_target, filename)
-    device.adb.Pull(original_file, saved_file)
     _UpdateTimestampFromAdbStat(saved_file, stat)
     if filename == _INDEX_DIRECTORY_NAME:
       # The directory containing the index was pulled recursively, update the
@@ -119,11 +116,16 @@ def PushBrowserCache(device, local_cache_path):
   # Push cache content.
   device.adb.Push(local_cache_path, remote_cache_directory)
 
+  # Command queue to touch all files with correct timestamp.
+  command_queue = []
+
   # Walk through the local cache to update mtime on the device.
   def MirrorMtime(local_path):
     cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
     remote_path = os.path.join(remote_cache_directory, cache_relative_path)
-    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
+    timestamp = os.stat(local_path).st_mtime
+    touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
+    command_queue.append(['touch', '-t', touch_stamp, remote_path])
 
   for local_directory_path, dirnames, filenames in os.walk(
         local_cache_path, topdown=False):
@@ -133,6 +135,8 @@ def PushBrowserCache(device, local_cache_path):
       MirrorMtime(os.path.join(local_directory_path, dirname))
   MirrorMtime(local_cache_path)
 
+  device_setup.DeviceSubmitShellCommandQueue(device, command_queue)
+
 
 def ZipDirectoryContent(root_directory_path, archive_dest_path):
   """Zip a directory's content recursively with all the directories'
diff --git a/loading/device_setup.py b/loading/device_setup.py
index bb2b4d7..4ffbb51 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -59,6 +59,31 @@ def GetFirstDevice():
   return devices[0]
 
 
+def DeviceSubmitShellCommandQueue(device, command_queue):
+  """Executes on the device a command queue.
+
+  Args:
+    device: The device to execute the shell commands to.
+    command_queue: a list of commands to be executed in that order.
+  """
+  REMOTE_COMMAND_FILE_PATH = '/data/local/tmp/adb_command_file.sh'
+  if not command_queue:
+    return
+  with tempfile.NamedTemporaryFile(prefix='adb_command_file_',
+                                   suffix='.sh') as command_file:
+    command_file.write('#!/bin/sh\n')
+    command_file.write('# Shell file generated by {}\'s {}\n'.format(
+        __file__, DeviceSubmitShellCommandQueue.__name__))
+    command_file.write('set -e\n')
+    for command in command_queue:
+      command_file.write(subprocess.list2cmdline(command) + ' ;\n')
+    command_file.write('exit 0;\n'.format(
+        REMOTE_COMMAND_FILE_PATH))
+    command_file.flush()
+    device.adb.Push(command_file.name, REMOTE_COMMAND_FILE_PATH)
+    device.adb.Shell('sh {p} && rm {p}'.format(p=REMOTE_COMMAND_FILE_PATH))
+
+
 @contextlib.contextmanager
 def FlagReplacer(device, command_line_path, new_flags):
   """Replaces chrome flags in a context, restores them afterwards.

commit 9d1e0288072a2066ed27c26ae42d9f9e983c76aa
Author: droger <droger@chromium.org>
Date:   Mon Mar 14 07:36:33 2016 -0700

    tools/android/loading Send a list of URLs with a POST request
    
    Review URL: https://codereview.chromium.org/1781403002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380973}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 04e1b2e7d54c290c2d8aa5570949cded14e2fd5f

diff --git a/loading/gce/README.md b/loading/gce/README.md
index eda5520..6de07d2 100644
--- a/loading/gce/README.md
+++ b/loading/gce/README.md
@@ -53,13 +53,29 @@ Otherwise the IP address can be retrieved by doing:
 gcloud compute instances list
 ```
 
-Interact with the app on the port 8080 in your browser at
-`http://<instance-ip>:8080`.
-
 TODO: allow starting the instance in the cloud without Supervisor. This enables
 iterative development on the instance using SSH, manually starting and stopping
 the app. This can be done using [instance metadata][2].
 
+## Use the app
+
+Interact with the app on the port 8080 at `http://<instance-ip>:8080`.
+
+To send a list of URLs to process:
+
+```shell
+curl -X POST -d @urls.json http://<instance-ip>:8080/set_tasks
+```
+
+where `urls.txt` is a file containing URLs (one per line).
+
+Start the processing by sending a request to `http://<instance-ip>:8080/start`,
+for example:
+
+```shell
+curl http://<instance-ip>:8080/start
+```
+
 ## Stop the app in the cloud
 
 ```shell
@@ -85,7 +101,7 @@ pip install -r pip_requirements.txt
 Launch the app:
 
 ```shell
-gunicorn --workers=2 main:app --bind 127.0.0.1:8000
+gunicorn --workers=1 main:app --bind 127.0.0.1:8000
 ```
 
 In your browser, go to `http://localhost:8000` and use the app.
diff --git a/loading/gce/main.py b/loading/gce/main.py
index 67fe3d1..2a0fd44 100644
--- a/loading/gce/main.py
+++ b/loading/gce/main.py
@@ -3,6 +3,8 @@
 # found in the LICENSE file.
 
 import json
+import re
+import threading
 
 from gcloud import storage
 from gcloud.exceptions import NotFound
@@ -14,6 +16,8 @@ class ServerApp(object):
   """
 
   def __init__(self):
+    self._tasks = []
+    self._thread = None
     print 'Initializing credentials'
     self._credentials = GoogleCredentials.get_application_default()
     print 'Reading server configuration'
@@ -52,6 +56,22 @@ class ServerApp(object):
       return None
     return blob.download_as_string()
 
+  def _SetTasks(self, task_list):
+    if len(self._tasks) > 0:
+      return False  # There are tasks already.
+    self._tasks = json.loads(task_list)
+    return len(self._tasks) != 0
+
+  def _ProcessTasks(self):
+    # Avoid special characters in storage object names
+    pattern = re.compile(r"[#\?\[\]\*/]")
+    while len(self._tasks) > 0:
+      url = self._tasks.pop()
+      filename = pattern.sub('_', url)
+      # TODO: compute the actual trace for url.
+      trace = '{}'
+      self._UploadFile(trace, filename)
+
   def __call__(self, environ, start_response):
     path = environ['PATH_INFO']
     if path == '/favicon.ico':
@@ -60,19 +80,24 @@ class ServerApp(object):
 
     status = '200 OK'
 
-    if path == '/write':
-      url = self._UploadFile('foo', 'test.txt')
-      data = 'Writing file at\n' + url + '\n'
-    elif path == '/read':
-      data = self._ReadFile('test.txt')
-      if not data:
-        data = ''
-        status = '404 NOT FOUND'
-    elif path == '/delete':
-      if self._DeleteFile('test.txt'):
-        data = 'Success\n'
+    if path == '/set_tasks':
+      # Get the tasks from the HTTP body.
+      try:
+        body_size = int(environ.get('CONTENT_LENGTH', 0))
+      except (ValueError):
+        body_size = 0
+      body = environ['wsgi.input'].read(body_size)
+      if self._SetTasks(body):
+        data = 'Set tasks: ' + str(len(self._tasks))
+      else:
+        data = 'Something went wrong'
+    elif path == '/start':
+      if len(self._tasks) > 0:
+        data = 'Starting...'
+        self._thread = threading.Thread(target = self._ProcessTasks)
+        self._thread.start()
       else:
-        data = 'Failed\n'
+        data = 'Nothing to do!'
     else:
       data = environ['PATH_INFO'] + '\n'
 
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
index 4f2cafa..b6ddde1 100644
--- a/loading/gce/startup-script.sh
+++ b/loading/gce/startup-script.sh
@@ -40,7 +40,7 @@ chown -R pythonapp:pythonapp /opt/app
 cat >/etc/supervisor/conf.d/python-app.conf << EOF
 [program:pythonapp]
 directory=/opt/app/clovis
-command=/opt/app/clovis/env/bin/gunicorn --workers=2 main:app \
+command=/opt/app/clovis/env/bin/gunicorn --workers=1 main:app \
   --bind 0.0.0.0:8080
 autostart=true
 autorestart=true

commit 38e5e580df90a95bb98c3b9944357a4cc65fdfc9
Author: tedchoc <tedchoc@chromium.org>
Date:   Fri Mar 11 11:04:44 2016 -0800

    Update eclipse .classpath to include other android device/ directories.
    
    TBR=newt@chromium.org
    BUG=
    
    Review URL: https://codereview.chromium.org/1783943003
    
    Cr-Original-Commit-Position: refs/heads/master@{#380690}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a0a0219dde77a6b75a57e439d43664e978878ef4

diff --git a/eclipse/.classpath b/eclipse/.classpath
index d9a4516..0495984 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -59,6 +59,9 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="content/shell/android/linker_test_apk/src"/>
     <classpathentry kind="src" path="content/shell/android/shell_apk/src"/>
     <classpathentry kind="src" path="device/battery/android/java/src"/>
+    <classpathentry kind="src" path="device/bluetooth/android/java/src"/>
+    <classpathentry kind="src" path="device/usb/android/java/src"/>
+    <classpathentry kind="src" path="device/vibration/android/java/src"/>
     <classpathentry kind="src" path="media/base/android/java/src"/>
     <classpathentry kind="src" path="mojo/android/system/src"/>
     <classpathentry kind="src" path="mojo/android/javatests/src"/>

commit 43b3c0a84f95e28e873c4174802e907c7b7e5c35
Author: mikecase <mikecase@chromium.org>
Date:   Fri Mar 11 10:38:34 2016 -0800

    Add an Android sample app for Telemetry tests.
    
    The purpose of this app is to be launched and push other apps to
    the background during Android telemetry tests. It is just a
    super simple Android app.
    
    BUG=586148
    
    Review URL: https://codereview.chromium.org/1744423002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380680}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f5e98301e0ee9a7a7388df01a1dfa366a95840ed

diff --git a/BUILD.gn b/BUILD.gn
index bcadd0f..99f9a01 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -79,3 +79,11 @@ group("audio_focus_grabber") {
     "//tools/android/audio_focus_grabber:audio_focus_grabber_apk",
   ]
 }
+
+# GYP: //tools/android/android_tools.gyp:push_apps_to_background
+group("push_apps_to_background") {
+  testonly = true
+  deps = [
+    "//tools/android/push_apps_to_background:push_apps_to_background_apk",
+  ]
+}
diff --git a/android_tools.gyp b/android_tools.gyp
index 065c804..3e1589f 100644
--- a/android_tools.gyp
+++ b/android_tools.gyp
@@ -85,5 +85,13 @@
         'audio_focus_grabber/audio_focus_grabber.gyp:audio_focus_grabber_apk',
       ],
     },
+    {
+      # GN: //tools/android:push_apps_to_background
+      'target_name': 'push_apps_to_background',
+      'type': 'none',
+      'dependencies': [
+        'push_apps_to_background/push_apps_to_background.gyp:push_apps_to_background_apk',
+      ],
+    },
   ],
 }
diff --git a/push_apps_to_background/AndroidManifest.xml b/push_apps_to_background/AndroidManifest.xml
new file mode 100644
index 0000000..92753f3
--- /dev/null
+++ b/push_apps_to_background/AndroidManifest.xml
@@ -0,0 +1,29 @@
+<!--
+ * Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+-->
+
+<manifest
+    xmlns:android="http://schemas.android.com/apk/res/android"
+    package="org.chromium.push_apps_to_background"
+    android:versionCode="1"
+    android:versionName="1.0" >
+
+    <uses-sdk android:minSdkVersion="16" android:targetSdkVersion="23" />
+
+    <application
+        android:icon="@drawable/ic_launcher"
+        android:label="@string/app_name"
+        android:theme="@android:style/Theme.Light" >
+        <activity
+            android:name="org.chromium.push_apps_to_background.PushAppsToBackgroundActivity"
+            android:label="@string/title_activity_push_apps_to_background"
+            android:exported="true">
+        <intent-filter>
+            <action android:name="android.intent.action.MAIN" />
+            <category android:name="android.intent.category.LAUNCHER" />
+        </intent-filter>
+        </activity>
+    </application>
+</manifest>
\ No newline at end of file
diff --git a/push_apps_to_background/BUILD.gn b/push_apps_to_background/BUILD.gn
new file mode 100644
index 0000000..cd90aa3
--- /dev/null
+++ b/push_apps_to_background/BUILD.gn
@@ -0,0 +1,23 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import("//build/config/android/rules.gni")
+import("//testing/test.gni")
+
+# Mark all targets as test only.
+testonly = true
+
+android_apk("push_apps_to_background_apk") {
+  apk_name = "PushAppsToBackground"
+  java_files = [ "src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java" ]
+  android_manifest = "AndroidManifest.xml"
+  deps = [
+    ":push_apps_to_background_apk_resources",
+  ]
+}
+
+android_resources("push_apps_to_background_apk_resources") {
+  resource_dirs = [ "res" ]
+  custom_package = "org.chromium.push_apps_to_background"
+}
diff --git a/push_apps_to_background/OWNERS b/push_apps_to_background/OWNERS
new file mode 100644
index 0000000..56fcbb8
--- /dev/null
+++ b/push_apps_to_background/OWNERS
@@ -0,0 +1,2 @@
+mikecase@chromium.org
+perezju@chromium.org
\ No newline at end of file
diff --git a/push_apps_to_background/push_apps_to_background.gyp b/push_apps_to_background/push_apps_to_background.gyp
new file mode 100644
index 0000000..25fca1f
--- /dev/null
+++ b/push_apps_to_background/push_apps_to_background.gyp
@@ -0,0 +1,21 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+{
+  'targets': [
+    # GN: //tools/android/push_apps_to_background:push_apps_to_background_apk
+    {
+      'target_name': 'push_apps_to_background_apk',
+      'type': 'none',
+      'variables': {
+        'apk_name': 'PushAppsToBackground',
+        'java_in_dir': '.',
+        'resource_dir': 'res',
+        'android_manifest_path': 'AndroidManifest.xml',
+      },
+      'includes': [
+        '../../../build/java_apk.gypi',
+      ],
+    },
+  ],
+}
diff --git a/push_apps_to_background/res/drawable-mdpi/ic_launcher.png b/push_apps_to_background/res/drawable-mdpi/ic_launcher.png
new file mode 100644
index 0000000..96a442e
Binary files /dev/null and b/push_apps_to_background/res/drawable-mdpi/ic_launcher.png differ
diff --git a/push_apps_to_background/res/layout/activity_push_apps_to_background.xml b/push_apps_to_background/res/layout/activity_push_apps_to_background.xml
new file mode 100644
index 0000000..9d292ca
--- /dev/null
+++ b/push_apps_to_background/res/layout/activity_push_apps_to_background.xml
@@ -0,0 +1,19 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!-- Copyright 2016 The Chromium Authors. All rights reserved.
+     Use of this source code is governed by a BSD-style license that can be
+     found in the LICENSE file.
+-->
+
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+    xmlns:tools="http://schemas.android.com/tools"
+    android:id="@+id/container"
+    android:layout_width="match_parent"
+    android:layout_height="match_parent"
+    android:orientation="vertical"
+    android:gravity="center">
+    <TextView
+        android:id="@+id/text_view"
+        android:layout_width="match_parent"
+        android:layout_height="match_parent"
+        android:text="@string/push_apps_to_background_message" />
+</LinearLayout>
\ No newline at end of file
diff --git a/push_apps_to_background/res/values/strings.xml b/push_apps_to_background/res/values/strings.xml
new file mode 100644
index 0000000..99f76a2
--- /dev/null
+++ b/push_apps_to_background/res/values/strings.xml
@@ -0,0 +1,15 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!-- Copyright 2016 The Chromium Authors. All rights reserved.
+     Use of this source code is governed by a BSD-style license that can be
+     found in the LICENSE file.
+-->
+
+<resources>
+    <string name="app_name">Push Apps To Background</string>
+    <string name="push_apps_to_background_message">
+        "<b>What is my purpose?</b>\n
+         <i>You push apps to the background.</i>\n
+         <b>Oh my God</b>."
+    </string>
+    <string name="title_activity_push_apps_to_background">Push Apps To Background</string>
+</resources>
\ No newline at end of file
diff --git a/push_apps_to_background/src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java b/push_apps_to_background/src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java
new file mode 100644
index 0000000..95e9a99
--- /dev/null
+++ b/push_apps_to_background/src/org/chromium/push_apps_to_background/PushAppsToBackgroundActivity.java
@@ -0,0 +1,21 @@
+// Copyright 2016 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.push_apps_to_background;
+
+import android.app.Activity;
+import android.os.Bundle;
+
+/**
+ * This activity is used in performance tests to push other apps
+ * to the background while running automated user stories.
+ */
+public class PushAppsToBackgroundActivity extends Activity {
+
+    @Override
+    public void onCreate(Bundle savedInstanceState) {
+        super.onCreate(savedInstanceState);
+        setContentView(R.layout.activity_push_apps_to_background);
+    }
+}
\ No newline at end of file

commit c6f7be5a1307420cc37d71a8226cbb1aa81f1d79
Author: droger <droger@chromium.org>
Date:   Thu Mar 10 07:00:03 2016 -0800

    tools/android/loading Initial infrastructure for trace collection on GCE
    
    This CL adds some configuration and a http server in prevision to
    running the trace collection on GCE.
    It is far from complete, and does not collect any traces yet.
    
    Review URL: https://codereview.chromium.org/1777523002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380404}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4962c0925d49b909e90ff6b6569b104abbdd9ed8

diff --git a/loading/gce/README.md b/loading/gce/README.md
new file mode 100644
index 0000000..eda5520
--- /dev/null
+++ b/loading/gce/README.md
@@ -0,0 +1,132 @@
+# Clovis in the Cloud: Developer Guide
+
+This document describes how to collect Chromium traces using Google Compute
+Engine.
+
+[TOC]
+
+## Initial setup
+
+Install the [gcloud command line tool][1].
+
+Checkout the source:
+
+```shell
+mkdir clovis
+cd clovis
+gcloud init
+```
+
+When offered, accept to clone the Google Cloud repo.
+
+## Update or Change the code
+
+Make changes to the code, or simply copy the latest version from Chromium into
+your local Google Cloud repository. Then commit and push:
+
+```shell
+git commit
+git push -u origin master
+```
+
+If there are instances already running, they need to be restarted for this to
+take effect.
+
+## Start the app in the cloud
+
+Create an instance using latest ubuntu LTS:
+
+```shell
+gcloud compute instances create clovis-tracer-1 \
+ --machine-type n1-standard-1 \
+ --image ubuntu-14-04 \
+ --zone europe-west1-c \
+ --tags clovis-http-server \
+ --scopes cloud-platform \
+ --metadata-from-file startup-script=default/startup-script.sh
+```
+
+This should output the IP address of the instance.
+Otherwise the IP address can be retrieved by doing:
+
+```shell
+gcloud compute instances list
+```
+
+Interact with the app on the port 8080 in your browser at
+`http://<instance-ip>:8080`.
+
+TODO: allow starting the instance in the cloud without Supervisor. This enables
+iterative development on the instance using SSH, manually starting and stopping
+the app. This can be done using [instance metadata][2].
+
+## Stop the app in the cloud
+
+```shell
+gcloud compute instances delete clovis-tracer-1
+```
+
+## Connect to the instance with SSH
+
+```shell
+gcloud compute ssh clovis-tracer-1
+```
+
+## Use the app locally
+
+Setup the local environment:
+
+```shell
+virtualenv env
+source env/bin/activate
+pip install -r pip_requirements.txt
+```
+
+Launch the app:
+
+```shell
+gunicorn --workers=2 main:app --bind 127.0.0.1:8000
+```
+
+In your browser, go to `http://localhost:8000` and use the app.
+
+Tear down the local environment:
+
+```shell
+deactivate
+```
+
+## Project-wide settings
+
+This is already setup, no need to do this again.
+Kept here for reference.
+
+### Server configuration file
+
+`main.py` expects to find a `server_config.json` file, which is a dictionary
+with the keys:
+
+*   `project_name`: the name of the Google Compute project,
+*   `bucket_name`: the name of the Google Storage bucket used to store the
+    results.
+
+### Firewall rule
+
+Firewall rule to allow access to the instance HTTP server from the outside:
+
+```shell
+gcloud compute firewall-rules create default-allow-http-8080 \
+    --allow tcp:8080 \
+    --source-ranges 0.0.0.0/0 \
+    --target-tags clovis-http-server \
+    --description "Allow port 8080 access to http-server"
+```
+
+The firewall rule can be disabled with:
+
+```shell
+gcloud compute firewall-rules delete default-allow-http-8080
+```
+
+[1]: https://cloud.google.com/sdk
+[2]: https://cloud.google.com/compute/docs/startupscript#custom
diff --git a/loading/gce/main.py b/loading/gce/main.py
new file mode 100644
index 0000000..67fe3d1
--- /dev/null
+++ b/loading/gce/main.py
@@ -0,0 +1,87 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import json
+
+from gcloud import storage
+from gcloud.exceptions import NotFound
+from oauth2client.client import GoogleCredentials
+
+class ServerApp(object):
+  """Simple web server application, collecting traces and writing them in
+  Google Cloud Storage.
+  """
+
+  def __init__(self):
+    print 'Initializing credentials'
+    self._credentials = GoogleCredentials.get_application_default()
+    print 'Reading server configuration'
+    with open('server_config.json') as configuration_file:
+       self._config = json.load(configuration_file)
+
+  def _GetStorageClient(self):
+    return storage.Client(project = self._config['project_name'],
+                          credentials = self._credentials)
+
+  def _GetStorageBucket(self, storage_client):
+    return storage_client.get_bucket(self._config['bucket_name'])
+
+  def _UploadFile(self, file_stream, filename):
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.blob(filename)
+    blob.upload_from_string(file_stream)
+    url = blob.public_url
+    return url
+
+  def _DeleteFile(self, filename):
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    try:
+      bucket.delete_blob(filename)
+      return True
+    except NotFound:
+      return False
+
+  def _ReadFile(self, filename):
+    client = self._GetStorageClient()
+    bucket = self._GetStorageBucket(client)
+    blob = bucket.get_blob(filename)
+    if not blob:
+      return None
+    return blob.download_as_string()
+
+  def __call__(self, environ, start_response):
+    path = environ['PATH_INFO']
+    if path == '/favicon.ico':
+        start_response('404 NOT FOUND', [('Content-Length', '0')])
+        return iter([''])
+
+    status = '200 OK'
+
+    if path == '/write':
+      url = self._UploadFile('foo', 'test.txt')
+      data = 'Writing file at\n' + url + '\n'
+    elif path == '/read':
+      data = self._ReadFile('test.txt')
+      if not data:
+        data = ''
+        status = '404 NOT FOUND'
+    elif path == '/delete':
+      if self._DeleteFile('test.txt'):
+        data = 'Success\n'
+      else:
+        data = 'Failed\n'
+    else:
+      data = environ['PATH_INFO'] + '\n'
+
+    response_headers = [
+        ('Content-type','text/plain'),
+        ('Content-Length', str(len(data)))
+    ]
+    start_response(status, response_headers)
+    return iter([data])
+
+
+app = ServerApp()
diff --git a/loading/gce/pip_requirements.txt b/loading/gce/pip_requirements.txt
new file mode 100644
index 0000000..e2d68b7
--- /dev/null
+++ b/loading/gce/pip_requirements.txt
@@ -0,0 +1,2 @@
+gunicorn==19.4.5
+gcloud==0.10.1
diff --git a/loading/gce/startup-script.sh b/loading/gce/startup-script.sh
new file mode 100644
index 0000000..4f2cafa
--- /dev/null
+++ b/loading/gce/startup-script.sh
@@ -0,0 +1,58 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Script executed at instance startup. It installs the required dependencies,
+# downloads the source code, and starts a web server.
+
+set -v
+
+# Talk to the metadata server to get the project id
+PROJECTID=$(curl -s \
+    "http://metadata.google.internal/computeMetadata/v1/project/project-id" \
+    -H "Metadata-Flavor: Google")
+
+# Install dependencies from apt
+apt-get update
+apt-get install -yq git supervisor python-pip python-dev libffi-dev libssl-dev
+
+# Create a pythonapp user. The application will run as this user.
+useradd -m -d /home/pythonapp pythonapp
+
+# pip from apt is out of date, so make it update itself and install virtualenv.
+pip install --upgrade pip virtualenv
+
+# Get the source code from the Google Cloud Repository
+# git requires $HOME and it's not set during the startup script.
+export HOME=/root
+git config --global credential.helper gcloud.sh
+git clone https://source.developers.google.com/p/$PROJECTID /opt/app/clovis
+
+# Install app dependencies
+virtualenv /opt/app/clovis/env
+/opt/app/clovis/env/bin/pip install -r /opt/app/clovis/pip_requirements.txt
+
+# Make sure the pythonapp user owns the application code
+chown -R pythonapp:pythonapp /opt/app
+
+# Configure supervisor to start gunicorn inside of our virtualenv and run the
+# applicaiton.
+cat >/etc/supervisor/conf.d/python-app.conf << EOF
+[program:pythonapp]
+directory=/opt/app/clovis
+command=/opt/app/clovis/env/bin/gunicorn --workers=2 main:app \
+  --bind 0.0.0.0:8080
+autostart=true
+autorestart=true
+user=pythonapp
+# Environment variables ensure that the application runs inside of the
+# configured virtualenv.
+environment=VIRTUAL_ENV="/opt/app/env/clovis",PATH="/opt/app/clovis/env/bin",\
+    HOME="/home/pythonapp",USER="pythonapp"
+stdout_logfile=syslog
+stderr_logfile=syslog
+EOF
+
+supervisorctl reread
+supervisorctl update
+

commit cf4dfdfa4a1aa8cb1aaec93e68c405198b5b1593
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 07:05:26 2016 -0800

    Clovis: improve method name and typo in analyze.py.
    
    Review URL: https://codereview.chromium.org/1775393002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380139}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d596348857f6d10658e8c4d62414120e8cec501a

diff --git a/loading/analyze.py b/loading/analyze.py
index 7712123..4b72ec6 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -120,8 +120,8 @@ def _LogRequests(url, clear_cache_override=None):
   if OPTIONS.emulate_device:
     chrome_ctl.SetDeviceEmulation(OPTIONS.emulate_device)
   if OPTIONS.emulate_network:
-    chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_device)
-  trace = loading_trace.LoadingTrace.FromUrlController(url, chrome_ctl)
+    chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_network)
+  trace = loading_trace.LoadingTrace.FromUrlAndController(url, chrome_ctl)
   return trace.ToJsonDict()
 
 
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index feb6c38..50dbc23 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -71,7 +71,7 @@ class LoadingTrace(object):
       return cls.FromJsonDict(json.load(input_file))
 
   @classmethod
-  def FromUrlController(
+  def FromUrlAndController(
       cls, url, controller, categories=None,
       timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
     """Create a loading trace by using controller to fetch url.
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 31fee71..7c9e822 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -35,7 +35,7 @@ def MonitorUrl(connection, url, clear_cache=False,
                timeout=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
   """Monitor a URL via a trace recorder.
 
-  DEPRECATED! Use LoadingTrace.FromUrlController instead.
+  DEPRECATED! Use LoadingTrace.FromUrlAndController instead.
 
   Args:
     connection: A devtools_monitor.DevToolsConnection instance.

commit e5125daf176bced4813ac0e1570ef49592ef6954
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 06:06:01 2016 -0800

    Use new chrome controller for analyze.py.
    
    Also updates the controller to track cache clearing.
    
    Review URL: https://codereview.chromium.org/1778543002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380131}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b6e375669d7dadecd6bbb252a56196ecee8843b5

diff --git a/loading/analyze.py b/loading/analyze.py
index 17ab770..7712123 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -25,15 +25,14 @@ import devil_chromium
 from pylib import constants
 
 import activity_lens
-import chrome_setup
 import content_classification_lens
+import controller
 import device_setup
 import frame_load_lens
 import loading_model
 import loading_trace
 import model_graph
 import options
-import trace_recorder
 
 
 # TODO(mattcary): logging.info isn't that useful, as the whole (tools) world
@@ -109,18 +108,21 @@ def _LogRequests(url, clear_cache_override=None):
   Returns:
     JSON dict of logged information (ie, a dict that describes JSON).
   """
-  device = device_setup.GetFirstDevice() if not OPTIONS.local else None
+  if OPTIONS.local:
+    chrome_ctl = controller.LocalChromeController()
+  else:
+    chrome_ctl = controller.RemoteChromeController(
+        device_setup.GetFirstDevice())
+
   clear_cache = (clear_cache_override if clear_cache_override is not None
                  else OPTIONS.clear_cache)
-
-  with device_setup.DeviceConnection(device) as connection:
-    additional_metadata = {}
-    if OPTIONS.local:
-      additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
-          connection, OPTIONS.emulate_device, OPTIONS.emulate_network)
-    trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
-    trace.metadata.update(additional_metadata)
-    return trace.ToJsonDict()
+  chrome_ctl.SetClearCache(clear_cache)
+  if OPTIONS.emulate_device:
+    chrome_ctl.SetDeviceEmulation(OPTIONS.emulate_device)
+  if OPTIONS.emulate_network:
+    chrome_ctl.SetNetworkEmulation(OPTIONS.emulate_device)
+  trace = loading_trace.LoadingTrace.FromUrlController(url, chrome_ctl)
+  return trace.ToJsonDict()
 
 
 def _FullFetch(url, json_output, prefetch):
diff --git a/loading/controller.py b/loading/controller.py
index 28b3e97..09c794b 100644
--- a/loading/controller.py
+++ b/loading/controller.py
@@ -10,6 +10,8 @@ desktop-specific versions.
 """
 
 import contextlib
+import datetime
+import logging
 import os
 import shutil
 import subprocess
@@ -50,14 +52,23 @@ class ChromeControllerBase(object):
     self._metadata = {}
     self._emulated_device = None
     self._emulated_network = None
+    self._clear_cache = False
 
   def AddChromeArgument(self, arg):
     """Add command-line argument to the chrome execution."""
     self._chrome_args.append(arg)
 
+  def SetClearCache(self, clear_cache=True):
+    self._clear_cache = clear_cache
+
   @contextlib.contextmanager
   def Open(self):
-    """Context that returns a connection/chrome instance."""
+    """Context that returns a connection/chrome instance.
+
+    Returns:
+      DevToolsConnection instance for which monitoring has been set up but not
+      started.
+    """
     raise NotImplementedError
 
   def ChromeMetadata(self):
@@ -98,6 +109,11 @@ class ChromeControllerBase(object):
       emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
       self._metadata.update(self._emulated_network)
 
+    self._metadata.update(date=datetime.datetime.utcnow().isoformat(),
+                          seconds_since_epoch=time.time())
+    if self._clear_cache:
+      connection.AddHook(connection.ClearCache)
+
 
 class RemoteChromeController(ChromeControllerBase):
   """A controller for an android device, aka remote chrome instance."""
@@ -125,7 +141,7 @@ class RemoteChromeController(ChromeControllerBase):
     self._device.KillAll(package_info.package, quiet=True)
 
     with device_setup.FlagReplacer(
-        self._device, command_line_path, self._chrome_flags):
+        self._device, command_line_path, self._chrome_args):
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
@@ -190,19 +206,23 @@ class LocalChromeController(ChromeControllerBase):
     binary_filename = OPTIONS.local_binary
     profile_dir = OPTIONS.local_profile_dir
     using_temp_profile_dir = profile_dir is None
-    flags = self._chrome_flags
+    flags = self._chrome_args
     if using_temp_profile_dir:
       profile_dir = tempfile.mkdtemp()
-      flags = '--user-data-dir=%s' + flags
+    flags = ['--user-data-dir=%s' % profile_dir] + flags
     chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
     process = subprocess.Popen(
         [binary_filename] + flags, shell=False, stderr=chrome_out)
     try:
       time.sleep(10)
-      connection =  devtools_monitor.DevToolsConnection(
-          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-      self._StartConnection(connection)
-      yield connection
+      process_result = process.poll()
+      if process_result is not None:
+        logging.error('Unexpected process exit: %s', process_result)
+      else:
+        connection = devtools_monitor.DevToolsConnection(
+            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+        self._StartConnection(connection)
+        yield connection
     finally:
       process.kill()
       if using_temp_profile_dir:
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 110aa79..1ea5d91 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -20,7 +20,7 @@ from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
 
-DEFAULT_TIMEOUT = 10 # seconds
+DEFAULT_TIMEOUT_SECONDS = 10 # seconds
 
 
 class DevToolsConnectionException(Exception):
@@ -98,7 +98,6 @@ class DevToolsConnection(object):
     self._scoped_states = {}
     self._domains_to_enable = set()
     self._tearing_down_tracing = False
-    self._set_up = False
     self._please_stop = False
     self._hooks = []
 
@@ -209,14 +208,23 @@ class DevToolsConnection(object):
     self.SyncRequest('Network.clearBrowserCache')
 
   def AddHook(self, hook):
-    """Add hook to be run on monitoring setup.
+    """Add hook to be run on monitoring start.
 
     Args:
       hook: a function.
     """
     self._hooks.append(hook)
 
-  def SetUpMonitoring(self):
+  def MonitorUrl(self, url, timeout_seconds=DEFAULT_TIMEOUT_SECONDS):
+    """Navigate to url and dispatch monitoring loop.
+
+    Unless you have registered a listener that will call StopMonitoring, this
+    will run until timeout from chrome.
+
+    Args:
+      url: (str) a URL to navigate to before starting monitoring loop.\
+      timeout_seconds: timeout in seconds for monitoring loop.
+    """
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
       if domain != self.TRACING_DOMAIN:
@@ -226,28 +234,20 @@ class DevToolsConnection(object):
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][0])
-
     for hook in self._hooks:
       hook()
-
     self._tearing_down_tracing = False
-    self._set_up = True
 
-  def StartMonitoring(self, timeout=DEFAULT_TIMEOUT):
-    """Starts monitoring.
+    self.SendAndIgnoreResponse('Page.navigate', {'url': url})
 
-    DevToolsConnection.SetUpMonitoring() has to be called first.
-    """
-    assert self._set_up, 'DevToolsConnection.SetUpMonitoring not called.'
-    self._Dispatch(timeout=timeout)
+    self._Dispatch(timeout=timeout_seconds)
     self._TearDownMonitoring()
 
   def StopMonitoring(self):
     """Stops the monitoring."""
     self._please_stop = True
 
-  def _Dispatch(self, kind='Monitoring',
-                timeout=DEFAULT_TIMEOUT):
+  def _Dispatch(self, timeout, kind='Monitoring'):
     self._please_stop = False
     while not self._please_stop:
       try:
@@ -262,7 +262,7 @@ class DevToolsConnection(object):
       logging.info('Fetching tracing')
       self.SyncRequestNoResponse(self.TRACING_END_METHOD)
       self._tearing_down_tracing = True
-      self._Dispatch(kind='Tracing', timeout=self.TRACING_TIMEOUT)
+      self._Dispatch(timeout=self.TRACING_TIMEOUT, kind='Tracing')
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][1])
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 344baeb..feb6c38 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -6,6 +6,7 @@
 
 import json
 
+import devtools_monitor
 import page_track
 import request_track
 import tracing
@@ -68,3 +69,28 @@ class LoadingTrace(object):
     """Returns an instance from a json file saved by ToJsonFile()."""
     with open(json_path) as input_file:
       return cls.FromJsonDict(json.load(input_file))
+
+  @classmethod
+  def FromUrlController(
+      cls, url, controller, categories=None,
+      timeout_seconds=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
+    """Create a loading trace by using controller to fetch url.
+
+    Args:
+      url: (str) url to fetch.
+      controller: (ChromeControllerBase) controller to manage the connection.
+      categories: TracingTrack categories to capture.
+      timeout_seconds: monitoring connection timeout in seconds.
+
+    Returns:
+      LoadingTrace instance.
+    """
+    with controller.Open() as connection:
+      page = page_track.PageTrack(connection)
+      request = request_track.RequestTrack(connection)
+      trace = tracing.TracingTrack(
+          connection,
+          categories=(tracing.DEFAULT_CATEGORIES if categories is None
+                      else categories))
+      connection.MonitorUrl(url, timeout_seconds=timeout_seconds)
+      return cls(url, controller.ChromeMetadata(), page, request, trace)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index de59b35..31fee71 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -3,7 +3,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Loading trace recorder."""
+"""Loading trace recorder. DEPRECATED!"""
 
 import argparse
 import datetime
@@ -32,9 +32,11 @@ import tracing
 
 def MonitorUrl(connection, url, clear_cache=False,
                categories=tracing.DEFAULT_CATEGORIES,
-               timeout=devtools_monitor.DEFAULT_TIMEOUT):
+               timeout=devtools_monitor.DEFAULT_TIMEOUT_SECONDS):
   """Monitor a URL via a trace recorder.
 
+  DEPRECATED! Use LoadingTrace.FromUrlController instead.
+
   Args:
     connection: A devtools_monitor.DevToolsConnection instance.
     url: url to navigate to as string.

commit 9a3cfc8151998ec3a1563691787fd42e70d0eafb
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 02:41:12 2016 -0800

    Add hooks to devtools monitor connection to enable configuration from a controller.
    
    Review URL: https://codereview.chromium.org/1779433002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380115}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e14c5dbb245da627c253528352570adf206de019

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 3ea8eea..110aa79 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -100,6 +100,7 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self._set_up = False
     self._please_stop = False
+    self._hooks = []
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -207,6 +208,14 @@ class DevToolsConnection(object):
     assert res['result'], 'Cache clearing is not supported by this browser.'
     self.SyncRequest('Network.clearBrowserCache')
 
+  def AddHook(self, hook):
+    """Add hook to be run on monitoring setup.
+
+    Args:
+      hook: a function.
+    """
+    self._hooks.append(hook)
+
   def SetUpMonitoring(self):
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
@@ -217,6 +226,10 @@ class DevToolsConnection(object):
     for scoped_state in self._scoped_states:
       self.SyncRequestNoResponse(scoped_state,
                                  self._scoped_states[scoped_state][0])
+
+    for hook in self._hooks:
+      hook()
+
     self._tearing_down_tracing = False
     self._set_up = True
 

commit 271606fbb4039c9a27e73600ec1cab6a5e855044
Author: mattcary <mattcary@chromium.org>
Date:   Wed Mar 9 01:59:33 2016 -0800

    Ignore missing timings for about protocol.
    
    Review URL: https://codereview.chromium.org/1774993002
    
    Cr-Original-Commit-Position: refs/heads/master@{#380111}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: ee641ea1c18485b7b05dc226a9473b4ded2a0b83

diff --git a/loading/request_track.py b/loading/request_track.py
index 685ab0d..bba015d 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -495,7 +495,7 @@ class RequestTrack(devtools_monitor.Track):
     # data URLs don't have a timing dict, and timings for cached requests are
     # stale.
     # TODO(droger): the timestamp is inacurate, get the real timings instead.
-    if r.protocol == 'data' or r.served_from_cache:
+    if r.protocol in ('data', 'about') or r.served_from_cache:
       timing_dict = {'requestTime': r.timestamp}
     else:
       timing_dict = response['timing']

commit d61c96c356da892c6b385c49ddb07ef4e4aee0f7
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 8 03:28:32 2016 -0800

    Device controller refactor.
    
    First step in refactoring connection setup and chrome launching in a way that's
    friendly to sandwich, clovis, pre* and any others.
    
    Review URL: https://codereview.chromium.org/1731973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379807}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d996937e3918d382fddf3f53a862225f49d7d683

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
index 9bf2908..245246d 100644
--- a/loading/chrome_setup.py
+++ b/loading/chrome_setup.py
@@ -2,7 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Handles Chrome's configuration."""
+"""Handles Chrome's configuration. DEPRECATED!"""
 
 import contextlib
 import json
diff --git a/loading/controller.py b/loading/controller.py
new file mode 100644
index 0000000..28b3e97
--- /dev/null
+++ b/loading/controller.py
@@ -0,0 +1,209 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Controller objects that control the context in which chrome runs.
+
+This is responsible for the setup necessary for launching chrome, and for
+creating a DevToolsConnection. There are remote device and local
+desktop-specific versions.
+"""
+
+import contextlib
+import os
+import shutil
+import subprocess
+import sys
+import tempfile
+import time
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+import chrome_cache
+import device_setup
+import devtools_monitor
+import emulation
+import options
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android.sdk import intent
+
+OPTIONS = options.OPTIONS
+
+# An estimate of time to wait for the device to become idle after expensive
+# operations, such as opening the launcher activity.
+_TIME_TO_DEVICE_IDLE_SECONDS = 2
+
+
+class ChromeControllerBase(object):
+  """Base class for all controllers.
+
+  Defines common operations but should not be created directly.
+  """
+  def __init__(self):
+    self._chrome_args = [
+        '--disable-fre',
+        '--enable-test-events',
+        '--remote-debugging-port=%d' % OPTIONS.devtools_port,
+    ]
+    self._metadata = {}
+    self._emulated_device = None
+    self._emulated_network = None
+
+  def AddChromeArgument(self, arg):
+    """Add command-line argument to the chrome execution."""
+    self._chrome_args.append(arg)
+
+  @contextlib.contextmanager
+  def Open(self):
+    """Context that returns a connection/chrome instance."""
+    raise NotImplementedError
+
+  def ChromeMetadata(self):
+    """Return metadata such as emulation information.
+
+    Returns:
+      Metadata as JSON dictionary.
+    """
+    return self._metadata
+
+  def GetDevice(self):
+    """Returns an android device, or None if chrome is local."""
+    return None
+
+  def SetDeviceEmulation(self, device_name):
+    """Set device emulation.
+
+    Args:
+      device_name: (str) Key from --devices_file.
+    """
+    devices = emulation.LoadEmulatedDevices(file(OPTIONS.devices_file))
+    self._emulated_device = devices[device_name]
+
+  def SetNetworkEmulation(self, network_name):
+    """Set network emulation.
+
+    Args:
+      network_name: (str) Key from emulation.NETWORK_CONDITIONS.
+    """
+    self._emulated_network = emulation.NETWORK_CONDITIONS[network_name]
+
+  def _StartConnection(self, connection):
+    """This should be called after opening an appropriate connection."""
+    if self._emulated_device:
+      self._metadata.update(emulation.SetUpDeviceEmulationAndReturnMetadata(
+          connection, self._emulated_device))
+    if self._emulated_network:
+      emulation.SetUpNetworkEmulation(connection, **self._emulated_network)
+      self._metadata.update(self._emulated_network)
+
+
+class RemoteChromeController(ChromeControllerBase):
+  """A controller for an android device, aka remote chrome instance."""
+  # Seconds to sleep after starting chrome activity.
+  POST_ACTIVITY_SLEEP_SECONDS = 2
+
+  def __init__(self, device):
+    """Initialize the controller.
+
+    Args:
+      device: an andriod device.
+    """
+    assert device is not None, 'Should you be using LocalController instead?'
+    self._device = device
+    super(RemoteChromeController, self).__init__()
+    self._slow_death = False
+
+  @contextlib.contextmanager
+  def Open(self):
+    """Overridden connection creation."""
+    package_info = OPTIONS.ChromePackage()
+    command_line_path = '/data/local/chrome-command-line'
+
+    self._device.EnableRoot()
+    self._device.KillAll(package_info.package, quiet=True)
+
+    with device_setup.FlagReplacer(
+        self._device, command_line_path, self._chrome_flags):
+      start_intent = intent.Intent(
+          package=package_info.package, activity=package_info.activity,
+          data='about:blank')
+      self._device.StartActivity(start_intent, blocking=True)
+      time.sleep(self.POST_ACTIVITY_SLEEP_SECONDS)
+      with device_setup.ForwardPort(
+          self._device, 'tcp:%d' % OPTIONS.devtools_port,
+          'localabstract:chrome_devtools_remote'):
+        connection = devtools_monitor.DevToolsConnection(
+            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+        self._StartConnection(connection)
+        yield connection
+    if self._slow_death:
+      self._device.adb.Shell('am start com.google.android.launcher')
+      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+      time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    self._device.KillAll(package_info.package, quiet=True)
+
+  def PushBrowserCache(self, cache_path):
+    """Push a chrome cache.
+
+    Args:
+      cache_path: The directory's path containing the cache locally.
+    """
+    chrome_cache.PushBrowserCache(self._device, cache_path)
+
+  def PullBrowserCache(self):
+    """Pull a chrome cache.
+
+    Returns:
+      Temporary directory containing all the browser cache. Caller will need to
+      remove this directory manually.
+    """
+    return chrome_cache.PullBrowserCache(self._device)
+
+  def SetSlowDeath(self, slow_death=True):
+    """Set to pause before final kill of chrome.
+
+    Gives time for caches to write.
+
+    Args:
+      slow_death: (bool) True if you want that which comes to all who live, to
+        be slow.
+    """
+    self._slow_death = slow_death
+
+
+class LocalChromeController(ChromeControllerBase):
+  """Controller for a local (desktop) chrome instance.
+
+  TODO(gabadie): implement cache push/pull and declare up in base class.
+  """
+  def __init__(self):
+    super(LocalChromeController, self).__init__()
+    if OPTIONS.no_sandbox:
+      self.AddChromeArgument('--no-sandbox')
+
+  @contextlib.contextmanager
+  def Open(self):
+    """Override for connection context."""
+    binary_filename = OPTIONS.local_binary
+    profile_dir = OPTIONS.local_profile_dir
+    using_temp_profile_dir = profile_dir is None
+    flags = self._chrome_flags
+    if using_temp_profile_dir:
+      profile_dir = tempfile.mkdtemp()
+      flags = '--user-data-dir=%s' + flags
+    chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+    process = subprocess.Popen(
+        [binary_filename] + flags, shell=False, stderr=chrome_out)
+    try:
+      time.sleep(10)
+      connection =  devtools_monitor.DevToolsConnection(
+          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+      self._StartConnection(connection)
+      yield connection
+    finally:
+      process.kill()
+      if using_temp_profile_dir:
+        shutil.rmtree(profile_dir)
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 62b99b4..bb2b4d7 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -97,6 +97,7 @@ def ForwardPort(device, local, remote):
     device.adb.ForwardRemove(local)
 
 
+# Deprecated
 def _SetUpDevice(device, package_info):
   """Enables root and closes Chrome on a device."""
   device.EnableRoot()
@@ -189,6 +190,7 @@ def WprHost(device, wpr_archive_path, record=False,
     shutil.rmtree(temp_certificate_dir)
 
 
+# Deprecated
 @contextlib.contextmanager
 def _DevToolsConnectionOnDevice(device, flags):
   """Returns a DevToolsConnection context manager for a given device.
@@ -215,6 +217,7 @@ def _DevToolsConnectionOnDevice(device, flags):
           OPTIONS.devtools_hostname, OPTIONS.devtools_port)
 
 
+# Deprecated, use *Controller.
 def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
 
diff --git a/loading/emulation.py b/loading/emulation.py
new file mode 100644
index 0000000..fa2fd89
--- /dev/null
+++ b/loading/emulation.py
@@ -0,0 +1,124 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Device and network emulation utilities via devtools."""
+
+import json
+
+# Copied from
+# WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
+# Units:
+#   download/upload: byte/s
+#   latency: ms
+NETWORK_CONDITIONS = {
+    'GPRS': {
+        'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
+    'Regular 2G': {
+        'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
+    'Good 2G': {
+        'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
+    'Regular 3G': {
+        'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
+    'Good 3G': {
+        'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
+        'latency': 40},
+    'Regular 4G': {
+        'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
+        'latency': 20},
+    'DSL': {
+        'download': 2 * 1024 * 1024 / 8, 'upload': 1 * 1024 * 1024 / 8,
+        'latency': 5},
+    'WiFi': {
+        'download': 30 * 1024 * 1024 / 8, 'upload': 15 * 1024 * 1024 / 8,
+        'latency': 2}
+}
+
+
+def LoadEmulatedDevices(registry):
+  """Loads a list of emulated devices from the DevTools JSON registry.
+
+  See, for example, third_party/WebKit/Source/devtools/front_end
+  /emulated_devices/module.json.
+
+  Args:
+    registry: A file-like object for the device registry (should be JSON).
+
+  Returns:
+    {'device_name': device}
+  """
+  json_dict = json.load(registry)
+  devices = {}
+  for device in json_dict['extensions']:
+    device = device['device']
+    devices[device['title']] = device
+  return devices
+
+
+def SetUpDeviceEmulationAndReturnMetadata(connection, device):
+  """Configures an instance of Chrome for device emulation.
+
+  Args:
+    connection: (DevToolsConnection)
+    device: (dict) An entry from LoadEmulatedDevices().
+
+  Returns:
+    A dict containing the device emulation metadata.
+  """
+  res = connection.SyncRequest('Emulation.canEmulate')
+  assert res['result'], 'Cannot set device emulation.'
+  data = _GetDeviceEmulationMetadata(device)
+  connection.SyncRequestNoResponse(
+      'Emulation.setDeviceMetricsOverride',
+      {'width': data['width'],
+       'height': data['height'],
+       'deviceScaleFactor': data['deviceScaleFactor'],
+       'mobile': data['mobile'],
+       'fitWindow': True})
+  connection.SyncRequestNoResponse('Network.setUserAgentOverride',
+                                   {'userAgent': data['userAgent']})
+  return data
+
+
+def SetUpNetworkEmulation(connection, latency, download, upload):
+  """Configures an instance of Chrome for network emulation.
+
+  See NETWORK_CONDITIONS for example (or valid?) emulation options.
+
+  Args:
+    connection: (DevToolsConnection)
+    latency: (float) Latency in ms.
+    download: (float) Download speed (Bytes / s).
+    upload: (float) Upload speed (Bytes / s).
+  """
+  res = connection.SyncRequest('Network.canEmulateNetworkConditions')
+  assert res['result'], 'Cannot set network emulation.'
+  connection.SyncRequestNoResponse(
+      'Network.emulateNetworkConditions',
+      {'offline': False, 'latency': latency, 'downloadThroughput': download,
+       'uploadThroughput': upload})
+
+
+def BandwidthToString(bandwidth):
+  """Converts a bandwidth to string.
+
+  Args:
+    bandwidth: The bandwidth to convert in byte/s. Must be a multiple of 1024/8.
+
+  Returns:
+    A string compatible with wpr --{up,down} command line flags.
+  """
+  assert bandwidth % (1024/8) == 0
+  bandwidth_kbps = (int(bandwidth) * 8) / 1024
+  if bandwidth_kbps % 1024:
+    return '{}Kbit/s'.format(bandwidth_kbps)
+  return '{}Mbit/s'.format(bandwidth_kbps / 1024)
+
+
+def _GetDeviceEmulationMetadata(device):
+  """Returns the metadata associated with a given device."""
+  return {'width': device['screen']['vertical']['width'],
+          'height': device['screen']['vertical']['height'],
+          'deviceScaleFactor': device['screen']['device-pixel-ratio'],
+          'mobile': 'mobile' in device['capabilities'],
+          'userAgent': device['user-agent']}
diff --git a/loading/emulation_unittest.py b/loading/emulation_unittest.py
new file mode 100644
index 0000000..9777e58
--- /dev/null
+++ b/loading/emulation_unittest.py
@@ -0,0 +1,81 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+from StringIO import StringIO
+import unittest
+
+import emulation
+import test_utils
+
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+
+class EmulationTestCase(unittest.TestCase):
+  def testLoadDevices(self):
+    devices = emulation.LoadEmulatedDevices(file(os.path.join(
+        _SRC_DIR, 'third_party/WebKit/Source/devtools/front_end',
+        'emulated_devices/module.json')))
+    # Just check we have something. We'll assume that if we were able to read
+    # the file without dying we must be ok.
+    self.assertTrue(devices)
+
+  def testSetUpDevice(self):
+    registry = StringIO("""{
+      "extensions": [
+          {
+              "type": "emulated-device",
+              "device": {
+                  "show-by-default": false,
+                  "title": "mattPhone" ,
+                  "screen": {
+                      "horizontal": {
+                          "width": 480,
+                          "height": 320
+                      },
+                      "device-pixel-ratio": 2,
+                      "vertical": {
+                          "width": 320,
+                          "height": 480
+                      }
+                  },
+                  "capabilities": [
+                      "touch",
+                      "mobile"
+                  ],
+                  "user-agent": "James Bond"
+              }
+          } ]}""")
+    devices = emulation.LoadEmulatedDevices(registry)
+    connection = test_utils.MockConnection(self)
+    connection.ExpectSyncRequest({'result': True}, 'Emulation.canEmulate')
+    metadata = emulation.SetUpDeviceEmulationAndReturnMetadata(
+        connection, devices['mattPhone'])
+    self.assertEqual(320, metadata['width'])
+    self.assertEqual('James Bond', metadata['userAgent'])
+    self.assertTrue(connection.AllExpectationsUsed())
+    self.assertEqual('Emulation.setDeviceMetricsOverride',
+                     connection.no_response_requests_seen[0][0])
+
+  def testSetUpNetwork(self):
+    connection = test_utils.MockConnection(self)
+    connection.ExpectSyncRequest({'result': True},
+                                 'Network.canEmulateNetworkConditions')
+    emulation.SetUpNetworkEmulation(connection, 120, 2048, 1024)
+    self.assertTrue(connection.AllExpectationsUsed())
+    self.assertEqual('Network.emulateNetworkConditions',
+                     connection.no_response_requests_seen[0][0])
+    self.assertEqual(
+        1024,
+        connection.no_response_requests_seen[0][1]['uploadThroughput'])
+
+  def testBandwidthToString(self):
+    self.assertEqual('16Kbit/s', emulation.BandwidthToString(2048))
+    self.assertEqual('8Mbit/s', emulation.BandwidthToString(1024 * 1024))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 5d2b617..f1c80c2 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -163,3 +163,47 @@ class TestResourceGraph(loading_model.ResourceGraph):
   @classmethod
   def FromRequestList(cls, requests, page_events=None, trace_events=None):
     return cls(LoadingTraceFromEvents(requests, page_events, trace_events))
+
+
+class MockConnection(object):
+  """Mock out connection for testing.
+
+  Use Expect* for requests expecting a repsonse. SyncRequestNoResponse puts
+  requests into no_response_requests_seen.
+
+  TODO(mattcary): use a standard mock system (the back-ported python3
+  unittest.mock? devil.utils.mock_calls?)
+
+  """
+  def __init__(self, test_case):
+    # List of (method, params) tuples.
+    self.no_response_requests_seen = []
+
+    self._test_case = test_case
+    self._expected_responses = {}
+
+  def ExpectSyncRequest(self, response, method, params=None):
+    """Test method when the connection is expected to make a SyncRequest.
+
+    Args:
+      response: (dict) the response to generate.
+      method: (str) the expected method in the call.
+      params: (dict) the expected params in the call.
+    """
+    self._expected_responses.setdefault(method, []).append((params, response))
+
+  def AllExpectationsUsed(self):
+    """Returns true when all expectations where used."""
+    return not self._expected_responses
+
+  def SyncRequestNoResponse(self, method, params):
+    """Mocked method."""
+    self.no_response_requests_seen.append((method, params))
+
+  def SyncRequest(self, method, params=None):
+    """Mocked method."""
+    expected_params, response = self._expected_responses[method].pop(0)
+    if not self._expected_responses[method]:
+      del self._expected_responses[method]
+    self._test_case.assertEqual(expected_params, params)
+    return response

commit 28e497bdfd82ad22e30f7d590adea21d1c6311dc
Author: tedchoc <tedchoc@chromium.org>
Date:   Mon Mar 7 16:48:10 2016 -0800

    Update eclipse classpath to include updated enum paths
    
    For the removed targets, I did a git grep and did not
    see any references.
    
    Command I used to generate the list:
    for enumpath in `ls -w1 out/Debug/gen/enums/`; do echo "    <classpathentry kind=\"src\" path=\"out/Debug/gen/enums/$enumpath\"/>"; done
    
    I built the following targets:
    chrome_apk chrome_test_apk custom_tabs_client_example_apk cronet_sample_apk
    
    I suspect the two actual targets that are required are:
    chrome_apk cronet_sample_apk
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1767303002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379689}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5e5d53e4e9e7361a2fd390c9a49b71be4dc27d46

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 62d3c39..d9a4516 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -98,8 +98,8 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/accessibility_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/activity_type_ids_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/android_resource_type_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_java/"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_type_java/"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/ax_enumerations_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_application_state"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_library_load_from_apk_status_codes"/>
@@ -107,46 +107,63 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_memory_pressure_level"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/bitmap_format_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/bookmark_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/browsing_data_time_period_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/browsing_data_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/certificate_mime_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/cert_verify_status_android_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/connection_security_levels_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/chromium_url_request_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/connectivity_check_result_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/console_message_level_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_gamepad_mapping"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_setting_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_settings_type_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/cronet_url_request_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/data_use_ui_message_enum_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/device_sensors_consts_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/gesture_event_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/http_cache_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/infobar_action_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/infobar_delegate_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/invalidate_types_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/investigated_scenario_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/media_android_captureapitype"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/media_android_imageformat"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/model_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/most_visited_tile_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/navigation_controller_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/net_request_priority_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/network_change_notifier_android_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/network_change_notifier_types_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/network_quality_observations_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/offline_page_feature_enums_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/offline_page_model_enums_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/page_info_connection_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/page_transition_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/popup_item_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/private_key_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/profile_account_management_metrics_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/profile_sync_service_model_type_selection_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/readback_response_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/result_codes_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/screen_orientation_values_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/security_state_enums_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/selection_event_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/sensor_manager_android_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/shortcut_source_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/signin_metrics_enum_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/speech_recognition_error_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/stop_source_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/system_ui_resource_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/tab_load_status_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/text_input_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/top_controls_state_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/touch_device_types_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/touch_handle_orientation_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/traffic_stats_error_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/url_request_error_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/url_request_failed_job_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/usb_descriptors_javagen"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_display_mode"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_input_event_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/website_settings_action_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_text_input_type"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/window_open_disposition_java"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/base_build_config_gen"/>

commit d00baca1150220a5e5443c26b48daa0b1be26ce8
Author: lizeb <lizeb@chromium.org>
Date:   Mon Mar 7 09:00:26 2016 -0800

    tools/android/loading: Network Activity lens, and CPU/network activity graphs.
    
    Review URL: https://codereview.chromium.org/1732413002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379565}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d40c3b58207b7bbc0cb7f6379f526d93417101ac

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index c99d47a..f6165d9 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -76,7 +76,7 @@ class ActivityLens(object):
                      - max(start_msec, event.start_msec)))
 
   @classmethod
-  def _ThreadBusiness(cls, events, start_msec, end_msec):
+  def _ThreadBusyness(cls, events, start_msec, end_msec):
     """Amount of time a thread spent executing from the message loop."""
     busy_duration = 0
     message_loop_events = [
@@ -179,7 +179,7 @@ class ActivityLens(object):
     assert end_msec - start_msec >= 0.
     events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
     result = {'edge_cost': end_msec - start_msec,
-              'busy': self._ThreadBusiness(events, start_msec, end_msec),
+              'busy': self._ThreadBusyness(events, start_msec, end_msec),
               'parsing': self._Parsing(events, start_msec, end_msec),
               'script': self._ScriptsExecuting(events, start_msec, end_msec)}
     return result
@@ -219,6 +219,16 @@ class ActivityLens(object):
         breakdown[x] for x in ('script', 'parsing', 'other_url', 'unknown_url'))
     return breakdown
 
+  def MainRendererThreadBusyness(self, start_msec, end_msec):
+    """Returns the amount of time the main renderer thread was busy.
+
+    Args:
+      start_msec: (float) Start of the interval.
+      end_msec: (float) End of the interval.
+    """
+    events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
+    return self._ThreadBusyness(events, start_msec, end_msec)
+
 
 class _EventsTree(object):
   """Builds the hierarchy of events from a list of fully nested events."""
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index 5abc172..f0c1275 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -11,7 +11,7 @@ import test_utils
 import tracing
 
 
-class ActivityLensTestCast(unittest.TestCase):
+class ActivityLensTestCase(unittest.TestCase):
   @classmethod
   def _EventsFromRawEvents(cls, raw_events):
     tracing_track = tracing.TracingTrack(None)
@@ -83,7 +83,7 @@ class ActivityLensTestCast(unittest.TestCase):
     self.assertEquals((1, second_renderer_tid),
                       ActivityLens._GetRendererMainThreadId(events))
 
-  def testThreadBusiness(self):
+  def testThreadBusyness(self):
     raw_events =  [
         {u'args': {},
          u'cat': u'toplevel',
@@ -104,10 +104,10 @@ class ActivityLensTestCast(unittest.TestCase):
          u'ts': 0,
          u'tts': 0}]
     events = self._EventsFromRawEvents(raw_events)
-    self.assertEquals(200, ActivityLens._ThreadBusiness(events, 0, 1000))
+    self.assertEquals(200, ActivityLens._ThreadBusyness(events, 0, 1000))
     # Clamping duration.
-    self.assertEquals(100, ActivityLens._ThreadBusiness(events, 0, 100))
-    self.assertEquals(50, ActivityLens._ThreadBusiness(events, 25, 75))
+    self.assertEquals(100, ActivityLens._ThreadBusyness(events, 0, 100))
+    self.assertEquals(50, ActivityLens._ThreadBusyness(events, 25, 75))
 
   def testScriptExecuting(self):
     url = u'http://example.com/script.js'
@@ -249,6 +249,53 @@ class ActivityLensTestCast(unittest.TestCase):
          'other_url': 6., 'unknown_url': 7.},
         activity.BreakdownEdgeActivityByInitiator(dep))
 
+  def testMainRendererThreadBusyness(self):
+    raw_events =  [
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 1,
+         u'tid': 12,
+         u'ts': 0},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 200 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 12,
+         u'ts': 0,
+         u'tts': 56485},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 8 * 200,
+         u'name': u'MessageLoop::NestedSomething',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 12,
+         u'ts': 0,
+         u'tts': 0},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 500 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 12,
+         u'tid': 12,
+         u'ts': 0,
+         u'tts': 56485}]
+    lens = self._ActivityLens([], raw_events)
+    # Ignore events from another PID.
+    self.assertEquals(200, lens.MainRendererThreadBusyness(0, 1000))
+    # Clamping duration.
+    self.assertEquals(100, lens.MainRendererThreadBusyness(0, 100))
+    self.assertEquals(50, lens.MainRendererThreadBusyness(25, 75))
+    # Other PID.
+    raw_events[0]['pid'] = 12
+    lens = self._ActivityLens([], raw_events)
+    self.assertEquals(500, lens.MainRendererThreadBusyness(0, 1000))
+
   def _ActivityLens(self, requests, raw_events):
     loading_trace = test_utils.LoadingTraceFromEvents(
         requests, None, raw_events)
diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index d383857..344baeb 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -5,6 +5,7 @@
 """Represents the trace of a page load."""
 
 import json
+
 import page_track
 import request_track
 import tracing
diff --git a/loading/network_activity_lens.py b/loading/network_activity_lens.py
new file mode 100644
index 0000000..2040093
--- /dev/null
+++ b/loading/network_activity_lens.py
@@ -0,0 +1,210 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gives a picture of the network activity between timestamps."""
+
+import bisect
+import collections
+import itertools
+import operator
+
+
+class NetworkActivityLens(object):
+  """Reconstructs the network activity during a trace.
+
+  The {uploaded,downloaded}_bytes_timeline timelines are:
+  ([timestamp_msec], [value_at_timestamp]). Bytes are counted when a
+  network event completes.
+
+  The rate timelines are:
+  ([timestamp_msec], [rate]), where the rate is computed over the time
+  period ending at timestamp_msec.
+
+  For all the timelines, the list of timestamps are identical.
+  """
+  def __init__(self, trace):
+    """Initializes a NetworkActivityLens instance.
+
+    Args:
+      trace: (LoadingTrace)
+    """
+    self._trace = trace
+    self._start_end_times = []
+    self._active_events_list = []
+    self._uploaded_bytes_timeline = []
+    self._downloaded_bytes_timeline = []
+    self._upload_rate_timeline = []
+    self._download_rate_timeline = []
+    requests = trace.request_track.GetEvents()
+    self._network_events = list(itertools.chain.from_iterable(
+        NetworkEvent.EventsFromRequest(request) for request in requests))
+    self._IndexEvents()
+    self._CreateTimelines()
+
+  @property
+  def uploaded_bytes_timeline(self): # (timestamps, data)
+    return (self._start_end_times, self._uploaded_bytes_timeline)
+
+  @property
+  def downloaded_bytes_timeline(self):
+    return (self._start_end_times, self._downloaded_bytes_timeline)
+
+  @property
+  def upload_rate_timeline(self):
+    return (self._start_end_times, self._upload_rate_timeline)
+
+  @property
+  def download_rate_timeline(self):
+    return (self._start_end_times, self._download_rate_timeline)
+
+  def _IndexEvents(self):
+    start_end_times_set = set()
+    for event in self._network_events:
+      start_end_times_set.add(event.start_msec)
+      start_end_times_set.add(event.end_msec)
+    self._start_end_times = sorted(list(start_end_times_set))
+    self._active_events_list = [[] for _ in self._start_end_times]
+    for event in self._network_events:
+      start_index = bisect.bisect_right(
+          self._start_end_times, event.start_msec) - 1
+      end_index = bisect.bisect_right(
+          self._start_end_times, event.end_msec)
+      for index in range(start_index, end_index):
+        self._active_events_list[index].append(event)
+
+  def _CreateTimelines(self):
+    for (index, timestamp) in enumerate(self._start_end_times):
+      upload_rate = sum(
+          e.UploadRate() for e in self._active_events_list[index]
+          if timestamp != e.end_msec)
+      download_rate = sum(
+          e.DownloadRate() for e in self._active_events_list[index]
+          if timestamp != e.end_msec)
+      uploaded_bytes = sum(
+          e.UploadedBytes() for e in self._active_events_list[index]
+          if timestamp == e.end_msec)
+      downloaded_bytes = sum(
+          e.DownloadedBytes() for e in self._active_events_list[index]
+          if timestamp == e.end_msec)
+      self._uploaded_bytes_timeline.append(uploaded_bytes)
+      self._downloaded_bytes_timeline.append(downloaded_bytes)
+      self._upload_rate_timeline.append(upload_rate)
+      self._download_rate_timeline.append(download_rate)
+
+
+class NetworkEvent(object):
+  """Represents a network event."""
+  KINDS = set(
+      ('dns', 'connect', 'send', 'receive_headers', 'receive_body'))
+  def __init__(self, request, kind, start_msec, end_msec, chunk_index=None):
+    """Creates a NetworkEvent."""
+    self._request = request
+    self._kind = kind
+    self.start_msec = start_msec
+    self.end_msec = end_msec
+    self._chunk_index = chunk_index
+
+  @classmethod
+  def _GetStartEndOffsetsMsec(cls, request, kind, index=None):
+    start_offset, end_offset = (0, 0)
+    r = request
+    if kind == 'dns':
+      start_offset = r.timing.dns_start
+      end_offset = r.timing.dns_end
+    elif kind == 'connect':
+      start_offset = r.timing.connect_start
+      end_offset = r.timing.connect_end
+    elif kind == 'send':
+      start_offset = r.timing.send_start
+      end_offset = r.timing.send_end
+    elif kind == 'receive_headers': # There is no responseReceived timing.
+      start_offset = r.timing.send_end
+      end_offset = r.timing.receive_headers_end
+    elif kind == 'receive_body':
+      if index is None:
+        start_offset = r.timing.receive_headers_end
+        end_offset = r.timing.loading_finished
+      else:
+        # Some chunks can correspond to no data.
+        i = index - 1
+        while i >= 0:
+          (offset, size) = r.data_chunks[i]
+          if size != 0:
+            previous_chunk_start = offset
+            break
+          i -= 1
+        else:
+          previous_chunk_start = r.timing.receive_headers_end
+        start_offset = previous_chunk_start
+        end_offset = r.data_chunks[index][0]
+    return (start_offset, end_offset)
+
+  @classmethod
+  def EventsFromRequest(cls, request):
+    # TODO(lizeb): This ignore forced revalidations.
+    if (request.from_disk_cache or request.served_from_cache
+        or request.IsDataRequest()):
+      return []
+    events = []
+    for kind in cls.KINDS - set(['receive_body']):
+      event = cls._EventWithKindFromRequest(request, kind)
+      if event:
+        events.append(event)
+    kind = 'receive_body'
+    if request.data_chunks:
+      for (index, chunk) in enumerate(request.data_chunks):
+        if chunk[0] != 0:
+          event = cls._EventWithKindFromRequest(request, kind, index)
+          if event:
+            events.append(event)
+    else:
+      event = cls._EventWithKindFromRequest(request, kind, None)
+      if event:
+        events.append(event)
+    return events
+
+  @classmethod
+  def _EventWithKindFromRequest(cls, request, kind, index=None):
+    (start_offset, end_offset) = cls._GetStartEndOffsetsMsec(
+        request, kind, index)
+    event = cls(request, kind, request.start_msec + start_offset,
+                request.start_msec + end_offset, index)
+    if start_offset == -1 or end_offset == -1:
+      return None
+    return event
+
+  def UploadedBytes(self):
+    """Returns the number of bytes uploaded during this event."""
+    if self._kind not in ('send'):
+      return 0
+    # Headers are not compressed (ignoring SPDY / HTTP/2)
+    if not self._request.request_headers:
+      return 0
+    return sum(len(k) + len(str(v)) for (k, v)
+               in self._request.request_headers.items())
+
+  def DownloadedBytes(self):
+    """Returns the number of bytes downloaded during this event."""
+    if self._kind not in ('receive_headers', 'receive_body'):
+      return 0
+    if self._kind == 'receive_headers':
+      return sum(len(k) + len(str(v)) for (k, v)
+                 in self._request.response_headers.items())
+    else:
+      if self._chunk_index is None:
+        return self._request.encoded_data_length
+      else:
+        return self._request.data_chunks[self._chunk_index][1]
+
+  def UploadRate(self):
+    """Returns the upload rate of this event in Bytes / s."""
+    return 1000 * self.UploadedBytes() / float(self.end_msec - self.start_msec)
+
+  def DownloadRate(self):
+    """Returns the download rate of this event in Bytes / s."""
+    downloaded_bytes = self.DownloadedBytes()
+    value = 1000 * downloaded_bytes / float(self.end_msec - self.start_msec)
+    if value > 1e6:
+      print self._kind, downloaded_bytes, self.end_msec - self.start_msec
+    return value
diff --git a/loading/network_activity_lens_unittest.py b/loading/network_activity_lens_unittest.py
new file mode 100644
index 0000000..e857679
--- /dev/null
+++ b/loading/network_activity_lens_unittest.py
@@ -0,0 +1,111 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import unittest
+
+from network_activity_lens import NetworkActivityLens
+import test_utils
+
+
+class NetworkActivityLensTestCase(unittest.TestCase):
+  def testTimeline(self):
+    timing_dict = {
+        'requestTime': 1.2,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    lens = self._NetworkActivityLens([request])
+    start_end_times = lens.uploaded_bytes_timeline[0]
+    expected_start_times = [
+        1220., 1230., 1250., 1260., 1270., 1280., 1290., 1300.]
+    self.assertListEqual(expected_start_times, start_end_times)
+    timing_dict = copy.copy(timing_dict)
+    timing_dict['requestTime'] += .005
+    second_request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    lens = self._NetworkActivityLens([request, second_request])
+    start_end_times = lens.uploaded_bytes_timeline[0]
+    expected_start_times = sorted(
+        expected_start_times + [x + 5. for x in expected_start_times])
+    for (expected, actual) in zip(expected_start_times, start_end_times):
+      self.assertAlmostEquals(expected, actual)
+
+  def testTransferredBytes(self):
+    timing_dict = {
+        'requestTime': 1.2,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    request.request_headers = {'a': 'b'}
+    request.response_headers = {'c': 'def'}
+    lens = self._NetworkActivityLens([request])
+    # Upload
+    upload_timeline = lens.uploaded_bytes_timeline
+    self.assertEquals(1270, upload_timeline[0][4])
+    self.assertEquals(1280, upload_timeline[0][5])
+    self.assertEquals(0, upload_timeline[1][4])
+    self.assertEquals(2, upload_timeline[1][5])
+    self.assertEquals(0, upload_timeline[1][6])
+    upload_rate = lens.upload_rate_timeline
+    self.assertEquals(2 / 10e-3, upload_rate[1][4])
+    self.assertEquals(0, upload_rate[1][5])
+    # Download
+    download_timeline = lens.downloaded_bytes_timeline
+    download_rate = lens.download_rate_timeline
+    self.assertEquals(1280, download_timeline[0][5])
+    self.assertEquals(1290, download_timeline[0][6])
+    self.assertEquals(0, download_timeline[1][5])
+    self.assertEquals(4, download_timeline[1][6])
+    self.assertEquals(0, download_timeline[1][7])
+    download_rate = lens.download_rate_timeline
+    self.assertEquals(4 / 10e-3, download_rate[1][5])
+    self.assertEquals(0, download_rate[1][6])
+
+  def testLongRequest(self):
+    timing_dict = {
+        'requestTime': 1200,
+        'dnsStart': 20, 'dnsEnd': 30,
+        'connectStart': 50, 'connectEnd': 60,
+        'sendStart': 70, 'sendEnd': 80,
+        'receiveHeadersEnd': 90,
+        'loadingFinished': 100}
+    request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    request.response_headers = {}
+    timing_dict = {
+        'requestTime': 1200,
+        'dnsStart': 2, 'dnsEnd': 3,
+        'connectStart': 5, 'connectEnd': 6,
+        'sendStart': 7, 'sendEnd': 8,
+        'receiveHeadersEnd': 10,
+        'loadingFinished': 1000}
+    long_request = test_utils.MakeRequestWithTiming(1, 2, timing_dict)
+    long_request.response_headers = {}
+    long_request.encoded_data_length = 1000
+    lens = self._NetworkActivityLens([request, long_request])
+    (timestamps, downloaded_bytes) = lens.downloaded_bytes_timeline
+    (_, download_rate) = lens.download_rate_timeline
+    start_receive = (long_request.start_msec
+                     + long_request.timing.receive_headers_end)
+    end_receive = (long_request.start_msec
+                   + long_request.timing.loading_finished)
+    self.assertEquals(1000, downloaded_bytes[-1])
+    for (index, timestamp) in enumerate(timestamps):
+      if start_receive < timestamp < end_receive:
+        self.assertAlmostEqual(1000 / 990e-3, download_rate[index])
+        self.assertEquals(0, downloaded_bytes[index])
+    self.assertEquals(1000, downloaded_bytes[-1])
+
+  def _NetworkActivityLens(self, requests):
+    trace = test_utils.LoadingTraceFromEvents(requests)
+    return NetworkActivityLens(trace)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/network_cpu_activity_view.py b/loading/network_cpu_activity_view.py
new file mode 100755
index 0000000..09450e1
--- /dev/null
+++ b/loading/network_cpu_activity_view.py
@@ -0,0 +1,69 @@
+#!/usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Graphs the CPU and network activity during a load."""
+
+import numpy as np
+import matplotlib
+from matplotlib import pylab as plt
+import sys
+
+import activity_lens
+import loading_trace
+import network_activity_lens
+
+
+def _CpuActivityTimeline(cpu_lens, start_msec, end_msec, granularity):
+  cpu_timestamps = np.arange(start_msec, end_msec, granularity)
+  busy_percentage = []
+  print len(cpu_timestamps)
+  for i in range(len(cpu_timestamps) - 1):
+    (start, end) = (cpu_timestamps[i], cpu_timestamps[i + 1])
+    duration = end - start
+    busy_ms = cpu_lens.MainRendererThreadBusyness(start, end)
+    busy_percentage.append(100 * busy_ms / float(duration))
+  return (cpu_timestamps[:-1], np.array(busy_percentage))
+
+
+def GraphTimelines(trace, output_filename):
+  """Creates and saves a graph of Network and CPU activity for a trace.
+
+  Args:
+    trace: (LoadingTrace)
+    output_filename: (str) Path of the output graph.
+  """
+  cpu_lens = activity_lens.ActivityLens(trace)
+  network_lens = network_activity_lens.NetworkActivityLens(trace)
+  matplotlib.rc('font', size=14)
+  figure, (network, cpu) = plt.subplots(2, sharex = True, figsize=(14, 10))
+  figure.suptitle('Network and CPU Activity - %s' % trace.url)
+  upload_timeline = network_lens.uploaded_bytes_timeline
+  download_timeline = network_lens.downloaded_bytes_timeline
+  start_time = upload_timeline[0][0]
+  end_time = upload_timeline[0][-1]
+  times = np.array(upload_timeline[0]) - start_time
+  network.step(times, np.cumsum(download_timeline[1]) / 1e6, label='Download')
+  network.step(times, np.cumsum(upload_timeline[1]) / 1e6, label='Upload')
+  network.legend(loc='lower right')
+  network.set_xlabel('Time (ms)')
+  network.set_ylabel('Total Data Transferred (MB)')
+
+  (cpu_timestamps, cpu_busyness) = _CpuActivityTimeline(
+      cpu_lens, start_time, end_time, 100)
+  cpu.step(cpu_timestamps - start_time, cpu_busyness)
+  cpu.set_ylim(ymin=0, ymax=100)
+  cpu.set_xlabel('Time (ms)')
+  cpu.set_ylabel('Main Renderer Thread Busyness (%)')
+  figure.savefig(output_filename, dpi=300)
+
+
+def main():
+  filename = sys.argv[1]
+  trace = loading_trace.LoadingTrace.FromJsonFile(filename)
+  GraphTimelines(trace, filename + '.pdf')
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 2d80d1a..5d2b617 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -42,6 +42,41 @@ class FakePageTrack(devtools_monitor.Track):
     return event['frame_id']
 
 
+def MakeRequestWithTiming(
+    url, source_url, timing_dict, magic_content_type=False,
+    initiator_type='other'):
+  """Make a dependent request.
+
+  Args:
+    url: a url, or number which will be used as a url.
+    source_url: a url or number which will be used as the source (initiating)
+      url. If the source url is not present, then url will be a root. The
+      convention in tests is to use a source_url of 'null' in this case.
+    timing_dict: (dict) Suitable to be passed to request_track.TimingFromDict().
+    initiator_type: the initiator type to use.
+
+  Returns:
+    A request_track.Request.
+  """
+  assert initiator_type in ('other', 'parser')
+  timing = request_track.TimingFromDict(timing_dict)
+  rq = request_track.Request.FromJsonDict({
+      'timestamp': timing.request_time,
+      'request_id': str(MakeRequestWithTiming._next_request_id),
+      'url': 'http://' + str(url),
+      'initiator': {'type': initiator_type, 'url': 'http://' + str(source_url)},
+      'response_headers': {'Content-Type':
+                           'null' if not magic_content_type
+                           else 'magic-debug-content' },
+      'timing': request_track.TimingAsList(timing)
+  })
+  MakeRequestWithTiming._next_request_id += 1
+  return rq
+
+
+MakeRequestWithTiming._next_request_id = 0
+
+
 def MakeRequest(
     url, source_url, start_time=None, headers_time=None, end_time=None,
     magic_content_type=False, initiator_type='other'):
@@ -63,7 +98,6 @@ def MakeRequest(
 
   Returns:
     A request_track.Request.
-
   """
   assert ((start_time is None and
            headers_time is None and
@@ -75,29 +109,16 @@ def MakeRequest(
   if start_time is None:
     # Use the request id in seconds for timestamps. This guarantees increasing
     # times which makes request dependencies behave as expected.
-    start_time = headers_time = end_time = MakeRequest._next_request_id * 1000
-  assert initiator_type in ('other', 'parser')
-  timing = request_track.TimingAsList(request_track.TimingFromDict({
+    start_time = headers_time = end_time = (
+        MakeRequestWithTiming._next_request_id * 1000)
+  timing_dict = {
       # connectEnd should be ignored.
       'connectEnd': (end_time - start_time) / 2,
       'receiveHeadersEnd': headers_time - start_time,
       'loadingFinished': end_time - start_time,
-      'requestTime': start_time / 1000.0}))
-  rq = request_track.Request.FromJsonDict({
-      'timestamp': start_time / 1000.0,
-      'request_id': str(MakeRequest._next_request_id),
-      'url': 'http://' + str(url),
-      'initiator': {'type': initiator_type, 'url': 'http://' + str(source_url)},
-      'response_headers': {'Content-Type':
-                           'null' if not magic_content_type
-                           else 'magic-debug-content' },
-      'timing': timing
-  })
-  MakeRequest._next_request_id += 1
-  return rq
-
-
-MakeRequest._next_request_id = 0
+      'requestTime': start_time / 1000.0}
+  return MakeRequestWithTiming(
+      url, source_url, timing_dict, magic_content_type, initiator_type)
 
 
 def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):

commit 45c0613331c70dccf8edd9cf32171175cc769c8d
Author: tfarina <tfarina@chromium.org>
Date:   Mon Mar 7 08:44:20 2016 -0800

    forwarder2: fix documentation typo in Socket::InitUnixSocket() function
    
    This patch fixes, in a documentation comment, the filename to a file
    that does not exist anymore. The code that it is referring to is from
    UnixDomainClientSocket::FillAddress() function from
    net/socket/unix_domain_client_socket_posix.cc.
    
    BUG=None
    R=yfriedman@chromium.org
    
    Review URL: https://codereview.chromium.org/1766913002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379564}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c9a4f315495334f23d2f5e52ac1b3b17f4bd21c3

diff --git a/forwarder2/socket.cc b/forwarder2/socket.cc
index c6073c1..669764f 100644
--- a/forwarder2/socket.cc
+++ b/forwarder2/socket.cc
@@ -136,7 +136,7 @@ bool Socket::InitUnixSocket(const std::string& path) {
   }
   family_ = PF_UNIX;
   addr_.addr_un.sun_family = family_;
-  // Copied from net/socket/unix_domain_socket_posix.cc
+  // Copied from net/socket/unix_domain_client_socket_posix.cc.
   // Convert the path given into abstract socket name. It must start with
   // the '\0' character, so we are adding it. |addr_len| must specify the
   // length of the structure exactly, as potentially the socket name may

commit f94ab6225606e9976077b6bd9718f6be9e4277e4
Author: gabadie <gabadie@chromium.org>
Date:   Mon Mar 7 02:45:08 2016 -0800

    Sandwich: Adds a work-around in the cache filter when a ressource protocol is unknown.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1759183004
    
    Cr-Original-Commit-Position: refs/heads/master@{#379534}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: f9afa9fbba5fa8ba1fad78491f12b178a2eee27b

diff --git a/loading/sandwich.py b/loading/sandwich.py
index 86d730f..b4e2590 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -477,8 +477,16 @@ def _FilterCacheMain(args):
     logging.info('white-listing %s' % main_resource_request.url)
     whitelisted_urls.add(main_resource_request.url)
     for (first, second, reason) in deps:
+      # Work-around where the protocol may be none for an unclear reason yet.
+      # TODO(gabadie): Follow up on this with Clovis guys and possibly remove
+      #   this work-around.
+      if not second.protocol:
+        logging.info('ignoring %s (no protocol)' % second.url)
+        continue
       # Ignore data protocols.
       if not second.protocol.startswith('http'):
+        logging.info('ignoring %s (`%s` is not HTTP{,S} protocol)' % (
+            second.url, second.protocol))
         continue
       if (first.request_id == main_resource_request.request_id and
           reason == 'parser' and second.url not in whitelisted_urls):

commit 14315674915204bf3be56f5f27b3145369e15b38
Author: gabadie <gabadie@chromium.org>
Date:   Thu Mar 3 11:40:07 2016 -0800

    tools/android/loading: Makes WprUrlEntry ready for case in-sensitive headers.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1752793002
    
    Cr-Original-Commit-Position: refs/heads/master@{#379059}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a1437c0f5983ce20e7a7e67b37cd46688f1536c0

diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
index c3a84a1..e983572 100644
--- a/loading/wpr_backend.py
+++ b/loading/wpr_backend.py
@@ -39,7 +39,7 @@ class WprUrlEntry(object):
     """
     headers = collections.defaultdict(list)
     for (key, value) in self._wpr_response.headers:
-      headers[key].append(value)
+      headers[key.lower()].append(value)
     return {k: ','.join(v) for (k, v) in headers.items()}
 
   def SetResponseHeader(self, name, value):
@@ -53,14 +53,15 @@ class WprUrlEntry(object):
       name: The name of the response header to set.
       value: The value of the response header to set.
     """
+    assert name.islower()
     new_headers = []
     new_header_set = False
     for header in self._wpr_response.headers:
-      if header[0] != name:
+      if header[0].lower() != name:
         new_headers.append(header)
       elif not new_header_set:
         new_header_set = True
-        new_headers.append((name, value))
+        new_headers.append((header[0], value))
     if new_header_set:
       self._wpr_response.headers = new_headers
     else:
@@ -76,8 +77,9 @@ class WprUrlEntry(object):
     Args:
       name: The name of the response header field to delete.
     """
+    assert name.islower()
     self._wpr_response.headers = \
-        [x for x in self._wpr_response.headers if x[0] != name]
+        [x for x in self._wpr_response.headers if x[0].lower() != name]
 
   @classmethod
   def _ExtractUrl(cls, request_string):
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
index e38070c..3cdf581 100644
--- a/loading/wpr_backend_unittest.py
+++ b/loading/wpr_backend_unittest.py
@@ -37,12 +37,14 @@ class WprUrlEntryTest(unittest.TestCase):
                                      ('header1', 'value1'),
                                      ('header0', 'value2'),
                                      ('header2', 'value3'),
-                                     ('header0', 'value4')])
+                                     ('header0', 'value4'),
+                                     ('HEadEr3', 'VaLue4')])
     headers = entry.GetResponseHeadersDict()
-    self.assertEquals(3, len(headers))
+    self.assertEquals(4, len(headers))
     self.assertEquals('value0,value2,value4', headers['header0'])
     self.assertEquals('value1', headers['header1'])
     self.assertEquals('value3', headers['header2'])
+    self.assertEquals('VaLue4', headers['header3'])
 
   def test_SetResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
@@ -63,10 +65,20 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('header1', entry._wpr_response.headers[1][0])
 
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('hEADEr1', 'value1'),
+                                     ('header2', 'value1'),])
+    entry.SetResponseHeader('header1', 'new_value1')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('new_value1', headers['header1'])
+    self.assertEquals('hEADEr1', entry._wpr_response.headers[1][0])
+
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
                                      ('header2', 'value2'),
                                      ('header1', 'value3'),
-                                     ('header3', 'value4')])
+                                     ('header3', 'value4'),
+                                     ('heADer1', 'value5')])
     entry.SetResponseHeader('header1', 'new_value2')
     headers = entry.GetResponseHeadersDict()
     self.assertEquals(4, len(headers))
@@ -75,6 +87,20 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertEquals('header3', entry._wpr_response.headers[3][0])
     self.assertEquals('value4', entry._wpr_response.headers[3][1])
 
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('heADer1', 'value1'),
+                                     ('header2', 'value2'),
+                                     ('HEader1', 'value3'),
+                                     ('header3', 'value4'),
+                                     ('header1', 'value5')])
+    entry.SetResponseHeader('header1', 'new_value2')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(4, len(headers))
+    self.assertEquals('new_value2', headers['header1'])
+    self.assertEquals('heADer1', entry._wpr_response.headers[1][0])
+    self.assertEquals('header3', entry._wpr_response.headers[3][0])
+    self.assertEquals('value4', entry._wpr_response.headers[3][1])
+
   def test_DeleteResponseHeader(self):
     entry = self._CreateWprUrlEntry([('header0', 'value0'),
                                      ('header1', 'value1'),
@@ -87,6 +113,14 @@ class WprUrlEntryTest(unittest.TestCase):
     self.assertNotIn('header0', entry.GetResponseHeadersDict())
     self.assertEquals(1, len(entry.GetResponseHeadersDict()))
 
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('hEAder1', 'value1'),
+                                     ('header0', 'value2'),
+                                     ('heaDEr2', 'value3')])
+    entry.DeleteResponseHeader('header1')
+    self.assertNotIn('header1', entry.GetResponseHeadersDict())
+    self.assertEquals(2, len(entry.GetResponseHeadersDict()))
+
 
 if __name__ == '__main__':
   unittest.main()

commit 61a25526e1c912a20edd2c8c5775c56f0ae43dde
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 11:44:05 2016 -0800

    tools/android/loading: Implements loading_trace_analyzer.py's unittest.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1738803003
    
    Cr-Original-Commit-Position: refs/heads/master@{#378531}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 77bd4a8ce7c24b6c8d8ddbc22130f351d6c01b95

diff --git a/loading/loading_trace_analyzer.py b/loading/loading_trace_analyzer.py
index e703360..1821001 100755
--- a/loading/loading_trace_analyzer.py
+++ b/loading/loading_trace_analyzer.py
@@ -21,10 +21,10 @@ def _ArgumentParser():
   # requests listing subcommand.
   requests_parser = subparsers.add_parser('requests',
       help='Lists all request from the loading trace.')
-  requests_parser.add_argument('loading_trace', type=file,
+  requests_parser.add_argument('loading_trace', type=str,
       help='Input loading trace to see the cache usage from.')
   requests_parser.add_argument('--output',
-      type=argparse.FileType(),
+      type=argparse.FileType('w'),
       default=sys.stdout,
       help='Output destination path if different from stdout.')
   requests_parser.add_argument('--output-format', type=str, default='{url}',
@@ -36,9 +36,21 @@ def _ArgumentParser():
   return parser
 
 
-def _RequestsSubcommand(args):
+def ListRequests(loading_trace_path,
+                 output_format='{url}',
+                 where_format='{url}',
+                 where_statement=None):
   """`loading_trace_analyzer.py requests` Command line tool entry point.
 
+  Args:
+    loading_trace_path: Path of the loading trace.
+    output_format: Output format of the generated strings.
+    where_format: String formated to be regex tested with <where_statement>
+    where_statement: Regex for selecting request event.
+
+  Yields:
+    Formated string of the selected request event.
+
   Example:
     Lists all request with timing:
       ... requests --output-format "{timing} {url}"
@@ -46,28 +58,16 @@ def _RequestsSubcommand(args):
     Lists  HTTP/HTTPS requests that have used the cache:
       ... requests --where "{protocol} {from_disk_cache}" "https?\S* True"
   """
-  where_format = None
-  where_statement = None
-  if args.where_statement:
-    where_format = args.where_statement[0]
-    try:
-      where_statement = re.compile(args.where_statement[1])
-    except re.error as e:
-      sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
-          args.where_statement[1], str(e)))
-      return 1
-
-  loading_trace = LoadingTrace.FromJsonDict(json.load(args.loading_trace))
+  if where_statement:
+    where_statement = re.compile(where_statement)
+  loading_trace = LoadingTrace.FromJsonFile(loading_trace_path)
   for request_event in loading_trace.request_track.GetEvents():
     request_event_json = request_event.ToJsonDict()
-
     if where_statement != None:
       where_in = where_format.format(**request_event_json)
       if not where_statement.match(where_in):
         continue
-
-    args.output.write(args.output_format.format(**request_event_json) + '\n')
-  return 0
+    yield output_format.format(**request_event_json)
 
 
 def main(command_line_args):
@@ -75,7 +75,22 @@ def main(command_line_args):
   """
   args = _ArgumentParser().parse_args(command_line_args)
   if args.subcommand == 'requests':
-    return _RequestsSubcommand(args)
+    try:
+      where_format = None
+      where_statement = None
+      if args.where_statement:
+        where_format = args.where_statement[0]
+        where_statement = args.where_statement[1]
+      for output_line in ListRequests(loading_trace_path=args.loading_trace,
+                                      output_format=args.output_format,
+                                      where_format=where_format,
+                                      where_statement=where_statement):
+        args.output.write(output_line + '\n')
+      return 0
+    except re.error as e:
+      sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
+          where_statement[1], str(e)))
+    return 1
   assert False
 
 
diff --git a/loading/loading_trace_analyzer_unittest.py b/loading/loading_trace_analyzer_unittest.py
new file mode 100644
index 0000000..b01aab2
--- /dev/null
+++ b/loading/loading_trace_analyzer_unittest.py
@@ -0,0 +1,59 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import gzip
+import os
+import re
+import shutil
+import subprocess
+import tempfile
+import unittest
+
+import loading_trace_analyzer
+
+LOADING_DIR = os.path.dirname(__file__)
+TEST_DATA_DIR = os.path.join(LOADING_DIR, 'testdata')
+
+
+class LoadingTraceAnalyzerTest(unittest.TestCase):
+  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
+
+  def setUp(self):
+    self._temp_dir = tempfile.mkdtemp()
+    self.trace_path = self._TmpPath('trace.json')
+    with gzip.GzipFile(self._ROLLING_STONE) as f:
+      with open(self.trace_path, 'w') as g:
+        g.write(f.read())
+
+  def tearDown(self):
+    shutil.rmtree(self._temp_dir)
+
+  def _TmpPath(self, name):
+    return os.path.join(self._temp_dir, name)
+
+  def testRequestsCmd(self):
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path)]
+    self.assertNotEqual(0, len(lines))
+
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
+        output_format='hello {protocol} world {url}')]
+    self.assertNotEqual(0, len(lines))
+    for line in lines:
+      self.assertTrue(re.match(r'^hello \S+ world \S+$', line))
+
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
+        where_format='{url}', where_statement=r'^http://.*$')]
+    self.assertNotEqual(0, len(lines))
+    for line in lines:
+      self.assertTrue(line.startswith('http://'))
+
+    lines = [r for r in loading_trace_analyzer.ListRequests(self.trace_path,
+        where_format='{url}', where_statement=r'^https://.*$')]
+    self.assertNotEqual(0, len(lines))
+    for line in lines:
+      self.assertTrue(line.startswith('https://'))
+
+
+if __name__ == '__main__':
+  unittest.main()

commit b19f10e38e8ac2cd173edeb3045e8e7763726ca7
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 10:28:07 2016 -0800

    sandwich: Implements filter-cache sub-command.
    
    NoState-Prefetch won't be able to know all the resources to fetch.
    This new sub-command creates a new cache archive by pruning all
    resources that can't be discovered by the HTML parser of the main
    HTML document.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1737103002
    
    Cr-Original-Commit-Position: refs/heads/master@{#378486}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: aeb69a7ef733515cc168e9cdd4edcb5642c1b3fc

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index d6d65a2..40d15bb 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -8,6 +8,7 @@
 from datetime import datetime
 import json
 import os
+import shutil
 import subprocess
 import sys
 import tempfile
@@ -278,6 +279,32 @@ class CacheBackend(object):
     return stdout_data
 
 
+def ApplyUrlWhitelistToCacheArchive(cache_archive_path,
+                                    whitelisted_urls,
+                                    output_cache_archive_path):
+  """Generate a new cache archive containing only whitelisted urls.
+
+  Args:
+    cache_archive_path: Path of the cache archive to apply the white listing.
+    whitelisted_urls: Set of url to keep in cache.
+    output_cache_archive_path: Destination path of cache archive containing only
+      white-listed urls.
+  """
+  cache_temp_directory = tempfile.mkdtemp(suffix='.cache')
+  try:
+    UnzipDirectoryContent(cache_archive_path, cache_temp_directory)
+    backend = CacheBackend(cache_temp_directory, 'simple')
+    cached_urls = backend.ListKeys()
+    for cached_url in cached_urls:
+      if cached_url not in whitelisted_urls:
+        backend.DeleteKey(cached_url)
+    for cached_url in backend.ListKeys():
+      assert cached_url in whitelisted_urls
+    ZipDirectoryContent(cache_temp_directory, output_cache_archive_path)
+  finally:
+    shutil.rmtree(cache_temp_directory)
+
+
 if __name__ == '__main__':
   import argparse
   parser = argparse.ArgumentParser(description='Tests cache back-end.')
diff --git a/loading/sandwich.py b/loading/sandwich.py
index a76c82b..86d730f 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -35,9 +35,12 @@ import chrome_cache
 import chrome_setup
 import device_setup
 import devtools_monitor
+import frame_load_lens
+import loading_trace
 import options
 import page_track
 import pull_sandwich_metrics
+import request_dependencies_lens
 import trace_recorder
 import tracing
 import wpr_backend
@@ -206,17 +209,17 @@ class SandwichRunner(object):
             connection=connection,
             emulated_device_name=None,
             emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
-      loading_trace = trace_recorder.MonitorUrl(
+      trace = trace_recorder.MonitorUrl(
           connection, url,
           clear_cache=clear_cache,
           categories=pull_sandwich_metrics.CATEGORIES,
           timeout=_DEVTOOLS_TIMEOUT)
-      loading_trace.metadata.update(additional_metadata)
+      trace.metadata.update(additional_metadata)
       if trace_id != None and self.trace_output_directory:
-        loading_trace_path = os.path.join(
+        trace_path = os.path.join(
             self.trace_output_directory, str(trace_id), 'trace.json')
-        os.makedirs(os.path.dirname(loading_trace_path))
-        loading_trace.ToJsonFile(loading_trace_path)
+        os.makedirs(os.path.dirname(trace_path))
+        trace.ToJsonFile(trace_path)
 
   def _RunUrl(self, url, trace_id=0):
     clear_cache = False
@@ -369,6 +372,23 @@ def _ArgumentParser():
                                    help='Path where to save the metrics\'s '+
                                       'CSV.')
 
+  # Filter cache subcommand.
+  filter_cache_parser = subparsers.add_parser('filter-cache',
+      help='Cache filtering that keeps only resources discoverable by the HTML'+
+          ' document parser.')
+  filter_cache_parser.add_argument('--cache-archive', type=str, required=True,
+                                   dest='cache_archive_path',
+                                   help='Path of the cache archive to filter.')
+  filter_cache_parser.add_argument('--output', type=str, required=True,
+                                   dest='output_cache_archive_path',
+                                   help='Path of filtered cache archive.')
+  filter_cache_parser.add_argument('loading_trace_paths', type=str, nargs='+',
+      metavar='LOADING_TRACE',
+      help='A list of loading traces generated by a sandwich run for a given' +
+          ' url. This is used to have a resource dependency graph to white-' +
+          'list the ones discoverable by the HTML pre-scanner for that given ' +
+          'url.')
+
   return parser
 
 
@@ -445,6 +465,34 @@ def _ExtractMetricsMain(args):
   return 0
 
 
+def _FilterCacheMain(args):
+  whitelisted_urls = set()
+  for loading_trace_path in args.loading_trace_paths:
+    logging.info('loading %s' % loading_trace_path)
+    trace = loading_trace.LoadingTrace.FromJsonFile(loading_trace_path)
+    requests_lens = request_dependencies_lens.RequestDependencyLens(trace)
+    deps = requests_lens.GetRequestDependencies()
+
+    main_resource_request = deps[0][0]
+    logging.info('white-listing %s' % main_resource_request.url)
+    whitelisted_urls.add(main_resource_request.url)
+    for (first, second, reason) in deps:
+      # Ignore data protocols.
+      if not second.protocol.startswith('http'):
+        continue
+      if (first.request_id == main_resource_request.request_id and
+          reason == 'parser' and second.url not in whitelisted_urls):
+        logging.info('white-listing %s' % second.url)
+        whitelisted_urls.add(second.url)
+
+  if not os.path.isdir(os.path.dirname(args.output_cache_archive_path)):
+    os.makedirs(os.path.dirname(args.output_cache_archive_path))
+  chrome_cache.ApplyUrlWhitelistToCacheArchive(args.cache_archive_path,
+                                               whitelisted_urls,
+                                               args.output_cache_archive_path)
+  return 0
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -465,6 +513,8 @@ def main(command_line_args):
     return _RunJobMain(args)
   if args.subcommand == 'extract-metrics':
     return _ExtractMetricsMain(args)
+  if args.subcommand == 'filter-cache':
+    return _FilterCacheMain(args)
   assert False
 
 

commit e11859dd8239b140b19fe0d70767e55151f9c034
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 09:29:24 2016 -0800

    sandwich: Makes pull_sandwich_metrics.py a sub-command.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1726403005
    
    Cr-Original-Commit-Position: refs/heads/master@{#378467}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 04c9261d40089b8454379f35df33e908b2ddbbb0

diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
old mode 100755
new mode 100644
index 926a579..ee37583
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -1,4 +1,3 @@
-#! /usr/bin/env python
 # Copyright 2016 The Chromium Authors. All rights reserved.
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -8,8 +7,6 @@
 python pull_sandwich_metrics.py -h
 """
 
-import argparse
-import csv
 import json
 import logging
 import os
@@ -21,7 +18,7 @@ import tracing
 
 CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
 
-_CSV_FIELD_NAMES = [
+CSV_FIELD_NAMES = [
     'id',
     'url',
     'total_load',
@@ -114,7 +111,7 @@ def _PullMetricsFromLoadingTrace(loading_trace):
     loading_trace: loading_trace_module.LoadingTrace.
 
   Returns:
-    Dictionary with all _CSV_FIELD_NAMES's field set (except the 'id').
+    Dictionary with all CSV_FIELD_NAMES's field set (except the 'id').
   """
   browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
   web_page_tracked_events = _GetWebPageTrackedEvents(
@@ -139,7 +136,7 @@ def _PullMetricsFromLoadingTrace(loading_trace):
   }
 
 
-def _PullMetricsFromOutputDirectory(output_directory_path):
+def PullMetricsFromOutputDirectory(output_directory_path):
   """Pulls all the metrics from all the traces of a sandwich run directory.
 
   Args:
@@ -147,7 +144,7 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
         metrics from.
 
   Returns:
-    List of dictionaries with all _CSV_FIELD_NAMES's field set.
+    List of dictionaries with all CSV_FIELD_NAMES's field set.
   """
   assert os.path.isdir(output_directory_path)
   run_infos = None
@@ -174,26 +171,3 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
   return metrics
-
-
-def main():
-  logging.basicConfig(level=logging.INFO)
-
-  parser = argparse.ArgumentParser()
-  parser.add_argument('output', type=str,
-                      help='Output directory of run_sandwich.py command.')
-  args = parser.parse_args()
-
-  trace_metrics_list = _PullMetricsFromOutputDirectory(args.output)
-  trace_metrics_list.sort(key=lambda e: e['id'])
-  cs_file_path = os.path.join(args.output, 'trace_analysis.csv')
-  with open(cs_file_path, 'w') as csv_file:
-    writer = csv.DictWriter(csv_file, fieldnames=_CSV_FIELD_NAMES)
-    writer.writeheader()
-    for trace_metrics in trace_metrics_list:
-      writer.writerow(trace_metrics)
-  return 0
-
-
-if __name__ == '__main__':
-  sys.exit(main())
diff --git a/loading/sandwich.py b/loading/sandwich.py
index aa2f9b5..a76c82b 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -12,6 +12,7 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
+import csv
 import json
 import logging
 import os
@@ -285,13 +286,17 @@ class SandwichRunner(object):
 
 def _ArgumentParser():
   """Build a command line argument's parser."""
+  # Command parser when dealing with jobs.
+  common_job_parser = argparse.ArgumentParser(add_help=False)
+  common_job_parser.add_argument('--job', required=True,
+                                 help='JSON file with job description.')
+
+  # Main parser
   parser = argparse.ArgumentParser()
-  parser.add_argument('--job', required=True,
-                      help='JSON file with job description.')
   subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
 
   # Record WPR subcommand.
-  record_wpr = subparsers.add_parser('record-wpr',
+  record_wpr = subparsers.add_parser('record-wpr', parents=[common_job_parser],
                                      help='Record WPR from sandwich job.')
   record_wpr.add_argument('--wpr-archive', required=True, type=str,
                           dest='wpr_archive_path',
@@ -302,10 +307,11 @@ def _ArgumentParser():
                                      help='Patch WPR response headers.')
   patch_wpr.add_argument('--wpr-archive', required=True, type=str,
                          dest='wpr_archive_path',
-                         help='Web page replay archive to generate.')
+                         help='Web page replay archive to patch.')
 
   # Create cache subcommand.
   create_cache_parser = subparsers.add_parser('create-cache',
+      parents=[common_job_parser],
       help='Create cache from sandwich job.')
   create_cache_parser.add_argument('--cache-archive', required=True, type=str,
                                    dest='cache_archive_path',
@@ -316,7 +322,8 @@ def _ArgumentParser():
                                        'the cache from.')
 
   # Run subcommand.
-  run_parser = subparsers.add_parser('run', help='Run sandwich benchmark.')
+  run_parser = subparsers.add_parser('run', parents=[common_job_parser],
+                                     help='Run sandwich benchmark.')
   run_parser.add_argument('--output', required=True, type=str,
                           dest='trace_output_directory',
                           help='Path of output directory to create.')
@@ -350,6 +357,18 @@ def _ArgumentParser():
                           dest='wpr_archive_path',
                           help='Web page replay archive to load job\'s urls ' +
                               'from.')
+
+  # Pull metrics subcommand.
+  create_cache_parser = subparsers.add_parser('extract-metrics',
+      help='Extracts metrics from a loading trace and saves as CSV.')
+  create_cache_parser.add_argument('--trace-directory', required=True,
+                                   dest='trace_output_directory', type=str,
+                                   help='Path of loading traces directory.')
+  create_cache_parser.add_argument('--out-metrics', default=None, type=str,
+                                   dest='metrics_csv_path',
+                                   help='Path where to save the metrics\'s '+
+                                      'CSV.')
+
   return parser
 
 
@@ -358,6 +377,8 @@ def _RecordWprMain(args):
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.wpr_record = True
   sandwich_runner.PrintConfig()
+  if not os.path.isdir(os.path.dirname(args.wpr_archive_path)):
+    os.makedirs(os.path.dirname(args.wpr_archive_path))
   sandwich_runner.Run()
   return 0
 
@@ -397,6 +418,8 @@ def _CreateCacheMain(args):
   sandwich_runner.PullConfigFromArgs(args)
   sandwich_runner.cache_operation = 'save'
   sandwich_runner.PrintConfig()
+  if not os.path.isdir(os.path.dirname(args.cache_archive_path)):
+    os.makedirs(os.path.dirname(args.cache_archive_path))
   sandwich_runner.Run()
   return 0
 
@@ -409,6 +432,19 @@ def _RunJobMain(args):
   return 0
 
 
+def _ExtractMetricsMain(args):
+  trace_metrics_list = pull_sandwich_metrics.PullMetricsFromOutputDirectory(
+      args.trace_output_directory)
+  trace_metrics_list.sort(key=lambda e: e['id'])
+  with open(args.metrics_csv_path, 'w') as csv_file:
+    writer = csv.DictWriter(csv_file,
+                            fieldnames=pull_sandwich_metrics.CSV_FIELD_NAMES)
+    writer.writeheader()
+    for trace_metrics in trace_metrics_list:
+      writer.writerow(trace_metrics)
+  return 0
+
+
 def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -427,6 +463,8 @@ def main(command_line_args):
     return _CreateCacheMain(args)
   if args.subcommand == 'run':
     return _RunJobMain(args)
+  if args.subcommand == 'extract-metrics':
+    return _ExtractMetricsMain(args)
   assert False
 
 

commit a9447881ef96d2c6ad4579697503ee6d5b4586eb
Author: mattcary <mattcary@chromium.org>
Date:   Tue Mar 1 08:38:26 2016 -0800

    Add occurrence tracking to a multigraph.
    
    The idea is to annotate node bags with things like "75% of urls were after first
    contentful paint".
    
    Review URL: https://codereview.chromium.org/1735093002
    
    Cr-Original-Commit-Position: refs/heads/master@{#378453}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: de1d607e176ce97c3857d2c8231a2fd4a862f625

diff --git a/loading/resource_sack.py b/loading/resource_sack.py
index 493029c..f1182e5 100644
--- a/loading/resource_sack.py
+++ b/loading/resource_sack.py
@@ -84,6 +84,22 @@ class GraphSack(object):
     self._url_to_bag[node.Url()].AddNode(graph, node)
     return self._url_to_bag[node.Url()]
 
+  def FilterOccurrence(self, tag, filter_from_graph):
+    """Accumulate filter occurrences for each bag in the graph.
+
+    This can be retrieved under tag for each Bag in the graph. For example, if
+    FilterContentful marks the nodes of each graph before the first contentful
+    paint, then FilterOccurrence('contentful', FilterContentful) will count, for
+    each bag, the fraction of nodes that were before the first contentful paint.
+
+    Args:
+      tag: the tag to count the filter appearances under.
+      filter_from_graph: a function graph -> node filter, where node filter
+        takes a node to a boolean.
+    """
+    for bag in self.bags:
+      bag.MarkOccurrence(tag, filter_from_graph)
+
   @property
   def graph_info(self):
     return self._graph_info
@@ -113,6 +129,12 @@ class Bag(dag.Node):
     self._relative_costs = []
     self._num_critical = 0
 
+    # See MarkOccurrence and GetOccurrence, below. This maps an occurrence
+    # tag to a list of nodes matching the occurrence.
+    self._occurence_matches = {}
+    # Number of nodes seen for each occurrence.
+    self._occurence_count = {}
+
   @property
   def url(self):
     return self._url
@@ -170,6 +192,37 @@ class Bag(dag.Node):
       self._successor_sources[successor_bag].add((graph, node, s))
       self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
 
+  def MarkOccurrence(self, tag, filter_from_graph):
+    """Mark occurrences for nodes in this bag according to graph_filters.
+
+    Results can be querried by GetOccurrence().
+
+    Args:
+      tag: a label for this set of occurrences.
+      filter_from_graph: a function graph -> node filter, where node filter
+        takes a node to a boolean.
+    """
+    self._occurence_matches[tag] = 0
+    self._occurence_count[tag] = 0
+    for graph, nodes in self.graphs.iteritems():
+      for n in nodes:
+        self._occurence_count[tag] += 1
+        if filter_from_graph(graph)(n):
+          self._occurence_matches[tag] += 1
+
+  def GetOccurrence(self, tag):
+    """Retrieve the occurrence fraction of a tag.
+
+    Args:
+      tag: the tag under which the occurrence was counted. This must have been
+        previously added at least once via AddOccurrence.
+
+    Returns:
+      A fraction occurrence matches / occurrence node count.
+    """
+    assert self._occurence_count[tag] > 0
+    return float(self._occurence_matches[tag]) / self._occurence_count[tag]
+
   @classmethod
   def _MakeShortname(cls, url):
     parsed = urlparse.urlparse(url)
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
index 417918d..2034eba 100644
--- a/loading/resource_sack_unittest.py
+++ b/loading/resource_sack_unittest.py
@@ -59,6 +59,36 @@ class ResourceSackTestCase(unittest.TestCase):
     self.assertEqual(set(['0/', 'data:fake/content']),
                      set([bag.label for bag in sack.bags]))
 
+  def test_Occurrence(self):
+    # There are two graph shapes. The first one is added to the sack three
+    # times, and the second once. The second graph has one sibling that doesn't
+    # appear in the first as well as a new child.
+    shape1 = [MakeRequest(0, 'null'), MakeRequest(1, 0), MakeRequest(2, 0)]
+    shape2 = [MakeRequest(0, 'null'), MakeRequest(1, 0),
+              MakeRequest(3, 0), MakeRequest(4, 1)]
+    graphs = [TestResourceGraph.FromRequestList(shape1),
+              TestResourceGraph.FromRequestList(shape1),
+              TestResourceGraph.FromRequestList(shape1),
+              TestResourceGraph.FromRequestList(shape2)]
+    sack = resource_sack.GraphSack()
+    for g in graphs:
+      sack.ConsumeGraph(g)
+    # Map a graph to a list of nodes that are in its filter.
+    filter_sets = {
+        graphs[0]: set([0, 1, 2]),
+        graphs[1]: set([0, 1, 2]),
+        graphs[2]: set([0, 1]),
+        graphs[3]: set([0, 3])}
+    sack.FilterOccurrence(
+        'test', lambda graph: lambda node:
+            int(node.ShortName()) in filter_sets[graph])
+    labels = {bag.label: bag for bag in sack.bags}
+    self.assertAlmostEqual(1, labels['0/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(0.75, labels['1/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(0.667, labels['2/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(1, labels['3/'].GetOccurrence('test'), 3)
+    self.assertAlmostEqual(0, labels['4/'].GetOccurrence('test'), 3)
+
 
 if __name__ == '__main__':
   unittest.main()

commit ec75308f387bbb121f9d16ded0e7a407055b2db6
Author: gabadie <gabadie@chromium.org>
Date:   Tue Mar 1 08:37:39 2016 -0800

    sandwich: Implements patch-wpr subcommand.
    
    The patch-wpr sub-command patches all resources response headers of a WPR archive, to make sure they all will be going into the chrome cache on disk if not already in, and also takes care of making sure that this cached resources will not be invalidated or re-validated in the next 10 years.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1740653002
    
    Cr-Original-Commit-Position: refs/heads/master@{#378452}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4bfeb32fdb5d16888edca6eeabb694496ccf6fe7

diff --git a/loading/sandwich.py b/loading/sandwich.py
index c337b8b..aa2f9b5 100755
--- a/loading/sandwich.py
+++ b/loading/sandwich.py
@@ -39,6 +39,7 @@ import page_track
 import pull_sandwich_metrics
 import trace_recorder
 import tracing
+import wpr_backend
 
 
 # Use options layer to access constants.
@@ -296,6 +297,13 @@ def _ArgumentParser():
                           dest='wpr_archive_path',
                           help='Web page replay archive to generate.')
 
+  # Patch WPR subcommand.
+  patch_wpr = subparsers.add_parser('patch-wpr',
+                                     help='Patch WPR response headers.')
+  patch_wpr.add_argument('--wpr-archive', required=True, type=str,
+                         dest='wpr_archive_path',
+                         help='Web page replay archive to generate.')
+
   # Create cache subcommand.
   create_cache_parser = subparsers.add_parser('create-cache',
       help='Create cache from sandwich job.')
@@ -354,6 +362,36 @@ def _RecordWprMain(args):
   return 0
 
 
+def _PatchWprMain(args):
+  # Sets the resources cache max-age to 10 years.
+  MAX_AGE = 10 * 365 * 24 * 60 * 60
+  CACHE_CONTROL = 'public, max-age={}'.format(MAX_AGE)
+
+  wpr_archive = wpr_backend.WprArchiveBackend(args.wpr_archive_path)
+  for url_entry in wpr_archive.ListUrlEntries():
+    response_headers = url_entry.GetResponseHeadersDict()
+    if 'cache-control' in response_headers and \
+        response_headers['cache-control'] == CACHE_CONTROL:
+      continue
+    logging.info('patching %s' % url_entry.url)
+    # TODO(gabadie): may need to patch Last-Modified and If-Modified-Since.
+    # TODO(gabadie): may need to delete ETag.
+    # TODO(gabadie): may need to patch Vary.
+    # TODO(gabadie): may need to take care of x-cache.
+    #
+    # Override the cache-control header to set the resources max age to MAX_AGE.
+    #
+    # Important note: Some resources holding sensitive information might have
+    # cache-control set to no-store which allow the resource to be cached but
+    # not cached in the file system. NoState-Prefetch is going to take care of
+    # this case. But in here, to simulate NoState-Prefetch, we don't have other
+    # choices but save absolutely all cached resources on disk so they survive
+    # after killing chrome for cache save, modification and push.
+    url_entry.SetResponseHeader('cache-control', CACHE_CONTROL)
+  wpr_archive.Persist()
+  return 0
+
+
 def _CreateCacheMain(args):
   sandwich_runner = SandwichRunner(args.job)
   sandwich_runner.PullConfigFromArgs(args)
@@ -383,6 +421,8 @@ def main(command_line_args):
 
   if args.subcommand == 'record-wpr':
     return _RecordWprMain(args)
+  if args.subcommand == 'patch-wpr':
+    return _PatchWprMain(args)
   if args.subcommand == 'create-cache':
     return _CreateCacheMain(args)
   if args.subcommand == 'run':

commit 20cababc773ba1d71689c7ce2e27e07cfd89f02a
Author: changwan <changwan@chromium.org>
Date:   Thu Feb 25 16:53:39 2016 -0800

    Introduce ThreadedInputConnection behind a switch
    
    Design doc: https://goo.gl/pcNRA5
    
    AdapterInputConnection is based on a replica model such that it has its
    own implementation and expects renderer to behave the same way. This model
    also required the replica editor to sync when there is a renderer-side
    change. The rationale behind this model was that it does not block the UI
    thread while allowing internal state to change before the next
    inputconnection call.
    
    Now renamed as ReplicaInputConnection, this model caused lots of issues in
    the past. (See design doc for details.)
    
    This CL proposes a whole new approach called ThreadedInputConnection. In
    this model, we create a fake view that proxies the container view such that
    InputConnection can be created on a separate thread (IME thread).
    Subsequent InputConnection method calls will also run on IME thread, so we
    do not block UI thread and can still wait for internal state change before
    returning InputConnection method calls, such as getTextBeforeCursor().
    
    Also note that ThreadedInputConnection does not inherit BaseInputConnection
    because we are not using Editable model.
    
    And there are other changes that followed this model:
    - 'From IME' bit will be set only when there is a blocking method call that
      awaits state update.
    - ImeTest tests behavior and not internal states any more.
    - RecreateInputConnectionTest is replaced by a new test in ImeTest.
    - Move mEditable from ImeAdapter to ReplicaInputConnection.
    - Remove DPAD hacks in RenderWidgetInputHandler.
    
    BUG=551193
    
    Review URL: https://codereview.chromium.org/1278593004
    
    Cr-Original-Commit-Position: refs/heads/master@{#377737}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8c3427482bac96138551530642843126da1b70a7

diff --git a/eclipse/.classpath b/eclipse/.classpath
index db88a17..62d3c39 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -50,6 +50,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="components/web_contents_delegate_android/android/java/src"/>
     <classpathentry kind="src" path="content/public/android/java/src"/>
     <classpathentry kind="src" path="content/public/android/javatests/src"/>
+    <classpathentry kind="src" path="content/public/android/junit/src"/>
     <classpathentry kind="src" path="content/public/test/android/javatests/src"/>
     <classpathentry kind="src" path="content/shell/android/browsertests/src"/>
     <classpathentry kind="src" path="content/shell/android/browsertests_apk/src"/>

commit 9b634f07887a544bb7b4b61008ebc65e2528664a
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 25 11:38:38 2016 -0800

    tools/android/loading: mv run_sandwich.py -> sandwich.py
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1732803002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377645}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9b51e3c81b3eb0033d3511dcfdff2f07032dc9b7

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
deleted file mode 100755
index c337b8b..0000000
--- a/loading/run_sandwich.py
+++ /dev/null
@@ -1,394 +0,0 @@
-#! /usr/bin/env python
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Instructs Chrome to load series of web pages and reports results.
-
-When running Chrome is sandwiched between preprocessed disk caches and
-WepPageReplay serving all connections.
-
-TODO(pasko): implement cache preparation and WPR.
-"""
-
-import argparse
-import json
-import logging
-import os
-import shutil
-import sys
-import tempfile
-import time
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-from pylib import constants
-import devil_chromium
-
-import chrome_cache
-import chrome_setup
-import device_setup
-import devtools_monitor
-import options
-import page_track
-import pull_sandwich_metrics
-import trace_recorder
-import tracing
-
-
-# Use options layer to access constants.
-OPTIONS = options.OPTIONS
-
-_JOB_SEARCH_PATH = 'sandwich_jobs'
-
-# An estimate of time to wait for the device to become idle after expensive
-# operations, such as opening the launcher activity.
-_TIME_TO_DEVICE_IDLE_SECONDS = 2
-
-
-# Devtools timeout of 1 minute to avoid websocket timeout on slow
-# network condition.
-_DEVTOOLS_TIMEOUT = 60
-
-
-def _ReadUrlsFromJobDescription(job_name):
-  """Retrieves the list of URLs associated with the job name."""
-  try:
-    # Extra sugar: attempt to load from a relative path.
-    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
-        job_name)
-    with open(json_file_name) as f:
-      json_data = json.load(f)
-  except IOError:
-    # Attempt to read by regular file name.
-    with open(job_name) as f:
-      json_data = json.load(f)
-
-  key = 'urls'
-  if json_data and key in json_data:
-    url_list = json_data[key]
-    if isinstance(url_list, list) and len(url_list) > 0:
-      return url_list
-  raise Exception('Job description does not define a list named "urls"')
-
-
-def _CleanPreviousTraces(output_directories_path):
-  """Cleans previous traces from the output directory.
-
-  Args:
-    output_directories_path: The output directory path where to clean the
-        previous traces.
-  """
-  for dirname in os.listdir(output_directories_path):
-    directory_path = os.path.join(output_directories_path, dirname)
-    if not os.path.isdir(directory_path):
-      continue
-    try:
-      int(dirname)
-    except ValueError:
-      continue
-    shutil.rmtree(directory_path)
-
-
-class SandwichRunner(object):
-  """Sandwich runner.
-
-  This object is meant to be configured first and then run using the Run()
-  method. The runner can configure itself conveniently with parsed arguement
-  using the PullConfigFromArgs() method. The only job is to make sure that the
-  command line flags have `dest` parameter set to existing runner members.
-  """
-
-  def __init__(self, job_name):
-    """Configures a sandwich runner out of the box.
-
-    Public members are meant to be configured as wished before calling Run().
-
-    Args:
-      job_name: The job name to get the associated urls.
-    """
-    # Cache operation to do before doing the chrome navigation.
-    #   Can be: clear,save,push,reload
-    self.cache_operation = 'clear'
-
-    # The cache archive's path to save to or push from. Is str or None.
-    self.cache_archive_path = None
-
-    # Controls whether the WPR server should do script injection.
-    self.disable_wpr_script_injection = False
-
-    # The job name. Is str.
-    self.job_name = job_name
-
-    # Number of times to repeat the job.
-    self.job_repeat = 1
-
-    # Network conditions to emulate. None if no emulation.
-    self.network_condition = None
-
-    # Network condition emulator. Can be: browser,wpr
-    self.network_emulator = 'browser'
-
-    # Output directory where to save the traces. Is str or None.
-    self.trace_output_directory = None
-
-    # List of urls to run.
-    self.urls = _ReadUrlsFromJobDescription(job_name)
-
-    # Path to the WPR archive to load or save. Is str or None.
-    self.wpr_archive_path = None
-
-    # Configures whether the WPR archive should be read or generated.
-    self.wpr_record = False
-
-    self._device = None
-    self._chrome_additional_flags = []
-    self._local_cache_directory_path = None
-
-  def PullConfigFromArgs(self, args):
-    """Configures the sandwich runner from parsed command line argument.
-
-    Args:
-      args: The command line parsed argument.
-    """
-    for config_name in self.__dict__.keys():
-      if config_name in args.__dict__:
-        self.__dict__[config_name] = args.__dict__[config_name]
-
-  def PrintConfig(self):
-    """Print the current sandwich runner configuration to stdout. """
-    for config_name in sorted(self.__dict__.keys()):
-      if config_name[0] != '_':
-        print '{} = {}'.format(config_name, self.__dict__[config_name])
-
-  def _CleanTraceOutputDirectory(self):
-    assert self.trace_output_directory
-    if not os.path.isdir(self.trace_output_directory):
-      try:
-        os.makedirs(self.trace_output_directory)
-      except OSError:
-        logging.error('Cannot create directory for results: %s',
-            self.trace_output_directory)
-        raise
-    else:
-      _CleanPreviousTraces(self.trace_output_directory)
-
-  def _SaveRunInfos(self, urls):
-    assert self.trace_output_directory
-    run_infos = {
-      'cache-op': self.cache_operation,
-      'job_name': self.job_name,
-      'urls': urls
-    }
-    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
-              'w') as file_output:
-      json.dump(run_infos, file_output, indent=2)
-
-  def _GetEmulatorNetworkCondition(self, emulator):
-    if self.network_emulator == emulator:
-      return self.network_condition
-    return None
-
-  def _RunNavigation(self, url, clear_cache, trace_id=None):
-    with device_setup.DeviceConnection(
-        device=self._device,
-        additional_flags=self._chrome_additional_flags) as connection:
-      additional_metadata = {}
-      if self._GetEmulatorNetworkCondition('browser'):
-        additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
-            connection=connection,
-            emulated_device_name=None,
-            emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
-      loading_trace = trace_recorder.MonitorUrl(
-          connection, url,
-          clear_cache=clear_cache,
-          categories=pull_sandwich_metrics.CATEGORIES,
-          timeout=_DEVTOOLS_TIMEOUT)
-      loading_trace.metadata.update(additional_metadata)
-      if trace_id != None and self.trace_output_directory:
-        loading_trace_path = os.path.join(
-            self.trace_output_directory, str(trace_id), 'trace.json')
-        os.makedirs(os.path.dirname(loading_trace_path))
-        loading_trace.ToJsonFile(loading_trace_path)
-
-  def _RunUrl(self, url, trace_id=0):
-    clear_cache = False
-    if self.cache_operation == 'clear':
-      clear_cache = True
-    elif self.cache_operation == 'push':
-      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-      chrome_cache.PushBrowserCache(self._device,
-                                    self._local_cache_directory_path)
-    elif self.cache_operation == 'reload':
-      self._RunNavigation(url, clear_cache=True)
-    elif self.cache_operation == 'save':
-      clear_cache = trace_id == 0
-    self._RunNavigation(url, clear_cache=clear_cache, trace_id=trace_id)
-
-  def _PullCacheFromDevice(self):
-    assert self.cache_operation == 'save'
-    assert self.cache_archive_path, 'Need to specify where to save the cache'
-
-    # Move Chrome to background to allow it to flush the index.
-    self._device.adb.Shell('am start com.google.android.launcher')
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-
-    cache_directory_path = chrome_cache.PullBrowserCache(self._device)
-    chrome_cache.ZipDirectoryContent(
-        cache_directory_path, self.cache_archive_path)
-    shutil.rmtree(cache_directory_path)
-
-  def Run(self):
-    """SandwichRunner main entry point meant to be called once configured.
-    """
-    if self.trace_output_directory:
-      self._CleanTraceOutputDirectory()
-
-    self._device = device_utils.DeviceUtils.HealthyDevices()[0]
-    self._chrome_additional_flags = []
-
-    assert self._local_cache_directory_path == None
-    if self.cache_operation == 'push':
-      assert os.path.isfile(self.cache_archive_path)
-      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-      chrome_cache.UnzipDirectoryContent(
-          self.cache_archive_path, self._local_cache_directory_path)
-
-    ran_urls = []
-    with device_setup.WprHost(self._device, self.wpr_archive_path,
-        record=self.wpr_record,
-        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
-        disable_script_injection=self.disable_wpr_script_injection
-        ) as additional_flags:
-      self._chrome_additional_flags.extend(additional_flags)
-      for _ in xrange(self.job_repeat):
-        for url in self.urls:
-          self._RunUrl(url, trace_id=len(ran_urls))
-          ran_urls.append(url)
-
-    if self._local_cache_directory_path:
-      shutil.rmtree(self._local_cache_directory_path)
-      self._local_cache_directory_path = None
-    if self.cache_operation == 'save':
-      self._PullCacheFromDevice()
-    if self.trace_output_directory:
-      self._SaveRunInfos(ran_urls)
-
-
-def _ArgumentParser():
-  """Build a command line argument's parser."""
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--job', required=True,
-                      help='JSON file with job description.')
-  subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
-
-  # Record WPR subcommand.
-  record_wpr = subparsers.add_parser('record-wpr',
-                                     help='Record WPR from sandwich job.')
-  record_wpr.add_argument('--wpr-archive', required=True, type=str,
-                          dest='wpr_archive_path',
-                          help='Web page replay archive to generate.')
-
-  # Create cache subcommand.
-  create_cache_parser = subparsers.add_parser('create-cache',
-      help='Create cache from sandwich job.')
-  create_cache_parser.add_argument('--cache-archive', required=True, type=str,
-                                   dest='cache_archive_path',
-                                   help='Cache archive destination path.')
-  create_cache_parser.add_argument('--wpr-archive', default=None, type=str,
-                                   dest='wpr_archive_path',
-                                   help='Web page replay archive to create ' +
-                                       'the cache from.')
-
-  # Run subcommand.
-  run_parser = subparsers.add_parser('run', help='Run sandwich benchmark.')
-  run_parser.add_argument('--output', required=True, type=str,
-                          dest='trace_output_directory',
-                          help='Path of output directory to create.')
-  run_parser.add_argument('--cache-archive', type=str,
-                          dest='cache_archive_path',
-                          help='Cache archive destination path.')
-  run_parser.add_argument('--cache-op',
-                          choices=['clear', 'push', 'reload'],
-                          dest='cache_operation',
-                          default='clear',
-                          help='Configures cache operation to do before '
-                              +'launching Chrome. (Default is clear). The push'
-                              +' cache operation requires --cache-archive to '
-                              +'set.')
-  run_parser.add_argument('--disable-wpr-script-injection',
-                          action='store_true',
-                          help='Disable WPR default script injection such as ' +
-                              'overriding javascript\'s Math.random() and ' +
-                              'Date() with deterministic implementations.')
-  run_parser.add_argument('--network-condition', default=None,
-      choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
-      help='Set a network profile.')
-  run_parser.add_argument('--network-emulator', default='browser',
-      choices=['browser', 'wpr'],
-      help='Set which component is emulating the network condition.' +
-          ' (Default to browser). Wpr network emulator requires --wpr-archive' +
-          ' to be set.')
-  run_parser.add_argument('--job-repeat', default=1, type=int,
-                          help='How many times to run the job.')
-  run_parser.add_argument('--wpr-archive', default=None, type=str,
-                          dest='wpr_archive_path',
-                          help='Web page replay archive to load job\'s urls ' +
-                              'from.')
-  return parser
-
-
-def _RecordWprMain(args):
-  sandwich_runner = SandwichRunner(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
-  sandwich_runner.wpr_record = True
-  sandwich_runner.PrintConfig()
-  sandwich_runner.Run()
-  return 0
-
-
-def _CreateCacheMain(args):
-  sandwich_runner = SandwichRunner(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
-  sandwich_runner.cache_operation = 'save'
-  sandwich_runner.PrintConfig()
-  sandwich_runner.Run()
-  return 0
-
-
-def _RunJobMain(args):
-  sandwich_runner = SandwichRunner(args.job)
-  sandwich_runner.PullConfigFromArgs(args)
-  sandwich_runner.PrintConfig()
-  sandwich_runner.Run()
-  return 0
-
-
-def main(command_line_args):
-  logging.basicConfig(level=logging.INFO)
-  devil_chromium.Initialize()
-
-  # Don't give the argument yet. All we are interested in for now is accessing
-  # the default values of OPTIONS.
-  OPTIONS.ParseArgs([])
-
-  args = _ArgumentParser().parse_args(command_line_args)
-
-  if args.subcommand == 'record-wpr':
-    return _RecordWprMain(args)
-  if args.subcommand == 'create-cache':
-    return _CreateCacheMain(args)
-  if args.subcommand == 'run':
-    return _RunJobMain(args)
-  assert False
-
-
-if __name__ == '__main__':
-  sys.exit(main(sys.argv[1:]))
diff --git a/loading/sandwich.py b/loading/sandwich.py
new file mode 100755
index 0000000..c337b8b
--- /dev/null
+++ b/loading/sandwich.py
@@ -0,0 +1,394 @@
+#! /usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Instructs Chrome to load series of web pages and reports results.
+
+When running Chrome is sandwiched between preprocessed disk caches and
+WepPageReplay serving all connections.
+
+TODO(pasko): implement cache preparation and WPR.
+"""
+
+import argparse
+import json
+import logging
+import os
+import shutil
+import sys
+import tempfile
+import time
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
+import devil_chromium
+
+import chrome_cache
+import chrome_setup
+import device_setup
+import devtools_monitor
+import options
+import page_track
+import pull_sandwich_metrics
+import trace_recorder
+import tracing
+
+
+# Use options layer to access constants.
+OPTIONS = options.OPTIONS
+
+_JOB_SEARCH_PATH = 'sandwich_jobs'
+
+# An estimate of time to wait for the device to become idle after expensive
+# operations, such as opening the launcher activity.
+_TIME_TO_DEVICE_IDLE_SECONDS = 2
+
+
+# Devtools timeout of 1 minute to avoid websocket timeout on slow
+# network condition.
+_DEVTOOLS_TIMEOUT = 60
+
+
+def _ReadUrlsFromJobDescription(job_name):
+  """Retrieves the list of URLs associated with the job name."""
+  try:
+    # Extra sugar: attempt to load from a relative path.
+    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
+        job_name)
+    with open(json_file_name) as f:
+      json_data = json.load(f)
+  except IOError:
+    # Attempt to read by regular file name.
+    with open(job_name) as f:
+      json_data = json.load(f)
+
+  key = 'urls'
+  if json_data and key in json_data:
+    url_list = json_data[key]
+    if isinstance(url_list, list) and len(url_list) > 0:
+      return url_list
+  raise Exception('Job description does not define a list named "urls"')
+
+
+def _CleanPreviousTraces(output_directories_path):
+  """Cleans previous traces from the output directory.
+
+  Args:
+    output_directories_path: The output directory path where to clean the
+        previous traces.
+  """
+  for dirname in os.listdir(output_directories_path):
+    directory_path = os.path.join(output_directories_path, dirname)
+    if not os.path.isdir(directory_path):
+      continue
+    try:
+      int(dirname)
+    except ValueError:
+      continue
+    shutil.rmtree(directory_path)
+
+
+class SandwichRunner(object):
+  """Sandwich runner.
+
+  This object is meant to be configured first and then run using the Run()
+  method. The runner can configure itself conveniently with parsed arguement
+  using the PullConfigFromArgs() method. The only job is to make sure that the
+  command line flags have `dest` parameter set to existing runner members.
+  """
+
+  def __init__(self, job_name):
+    """Configures a sandwich runner out of the box.
+
+    Public members are meant to be configured as wished before calling Run().
+
+    Args:
+      job_name: The job name to get the associated urls.
+    """
+    # Cache operation to do before doing the chrome navigation.
+    #   Can be: clear,save,push,reload
+    self.cache_operation = 'clear'
+
+    # The cache archive's path to save to or push from. Is str or None.
+    self.cache_archive_path = None
+
+    # Controls whether the WPR server should do script injection.
+    self.disable_wpr_script_injection = False
+
+    # The job name. Is str.
+    self.job_name = job_name
+
+    # Number of times to repeat the job.
+    self.job_repeat = 1
+
+    # Network conditions to emulate. None if no emulation.
+    self.network_condition = None
+
+    # Network condition emulator. Can be: browser,wpr
+    self.network_emulator = 'browser'
+
+    # Output directory where to save the traces. Is str or None.
+    self.trace_output_directory = None
+
+    # List of urls to run.
+    self.urls = _ReadUrlsFromJobDescription(job_name)
+
+    # Path to the WPR archive to load or save. Is str or None.
+    self.wpr_archive_path = None
+
+    # Configures whether the WPR archive should be read or generated.
+    self.wpr_record = False
+
+    self._device = None
+    self._chrome_additional_flags = []
+    self._local_cache_directory_path = None
+
+  def PullConfigFromArgs(self, args):
+    """Configures the sandwich runner from parsed command line argument.
+
+    Args:
+      args: The command line parsed argument.
+    """
+    for config_name in self.__dict__.keys():
+      if config_name in args.__dict__:
+        self.__dict__[config_name] = args.__dict__[config_name]
+
+  def PrintConfig(self):
+    """Print the current sandwich runner configuration to stdout. """
+    for config_name in sorted(self.__dict__.keys()):
+      if config_name[0] != '_':
+        print '{} = {}'.format(config_name, self.__dict__[config_name])
+
+  def _CleanTraceOutputDirectory(self):
+    assert self.trace_output_directory
+    if not os.path.isdir(self.trace_output_directory):
+      try:
+        os.makedirs(self.trace_output_directory)
+      except OSError:
+        logging.error('Cannot create directory for results: %s',
+            self.trace_output_directory)
+        raise
+    else:
+      _CleanPreviousTraces(self.trace_output_directory)
+
+  def _SaveRunInfos(self, urls):
+    assert self.trace_output_directory
+    run_infos = {
+      'cache-op': self.cache_operation,
+      'job_name': self.job_name,
+      'urls': urls
+    }
+    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
+              'w') as file_output:
+      json.dump(run_infos, file_output, indent=2)
+
+  def _GetEmulatorNetworkCondition(self, emulator):
+    if self.network_emulator == emulator:
+      return self.network_condition
+    return None
+
+  def _RunNavigation(self, url, clear_cache, trace_id=None):
+    with device_setup.DeviceConnection(
+        device=self._device,
+        additional_flags=self._chrome_additional_flags) as connection:
+      additional_metadata = {}
+      if self._GetEmulatorNetworkCondition('browser'):
+        additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
+            connection=connection,
+            emulated_device_name=None,
+            emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
+      loading_trace = trace_recorder.MonitorUrl(
+          connection, url,
+          clear_cache=clear_cache,
+          categories=pull_sandwich_metrics.CATEGORIES,
+          timeout=_DEVTOOLS_TIMEOUT)
+      loading_trace.metadata.update(additional_metadata)
+      if trace_id != None and self.trace_output_directory:
+        loading_trace_path = os.path.join(
+            self.trace_output_directory, str(trace_id), 'trace.json')
+        os.makedirs(os.path.dirname(loading_trace_path))
+        loading_trace.ToJsonFile(loading_trace_path)
+
+  def _RunUrl(self, url, trace_id=0):
+    clear_cache = False
+    if self.cache_operation == 'clear':
+      clear_cache = True
+    elif self.cache_operation == 'push':
+      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+      chrome_cache.PushBrowserCache(self._device,
+                                    self._local_cache_directory_path)
+    elif self.cache_operation == 'reload':
+      self._RunNavigation(url, clear_cache=True)
+    elif self.cache_operation == 'save':
+      clear_cache = trace_id == 0
+    self._RunNavigation(url, clear_cache=clear_cache, trace_id=trace_id)
+
+  def _PullCacheFromDevice(self):
+    assert self.cache_operation == 'save'
+    assert self.cache_archive_path, 'Need to specify where to save the cache'
+
+    # Move Chrome to background to allow it to flush the index.
+    self._device.adb.Shell('am start com.google.android.launcher')
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+
+    cache_directory_path = chrome_cache.PullBrowserCache(self._device)
+    chrome_cache.ZipDirectoryContent(
+        cache_directory_path, self.cache_archive_path)
+    shutil.rmtree(cache_directory_path)
+
+  def Run(self):
+    """SandwichRunner main entry point meant to be called once configured.
+    """
+    if self.trace_output_directory:
+      self._CleanTraceOutputDirectory()
+
+    self._device = device_utils.DeviceUtils.HealthyDevices()[0]
+    self._chrome_additional_flags = []
+
+    assert self._local_cache_directory_path == None
+    if self.cache_operation == 'push':
+      assert os.path.isfile(self.cache_archive_path)
+      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+      chrome_cache.UnzipDirectoryContent(
+          self.cache_archive_path, self._local_cache_directory_path)
+
+    ran_urls = []
+    with device_setup.WprHost(self._device, self.wpr_archive_path,
+        record=self.wpr_record,
+        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
+        disable_script_injection=self.disable_wpr_script_injection
+        ) as additional_flags:
+      self._chrome_additional_flags.extend(additional_flags)
+      for _ in xrange(self.job_repeat):
+        for url in self.urls:
+          self._RunUrl(url, trace_id=len(ran_urls))
+          ran_urls.append(url)
+
+    if self._local_cache_directory_path:
+      shutil.rmtree(self._local_cache_directory_path)
+      self._local_cache_directory_path = None
+    if self.cache_operation == 'save':
+      self._PullCacheFromDevice()
+    if self.trace_output_directory:
+      self._SaveRunInfos(ran_urls)
+
+
+def _ArgumentParser():
+  """Build a command line argument's parser."""
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--job', required=True,
+                      help='JSON file with job description.')
+  subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
+
+  # Record WPR subcommand.
+  record_wpr = subparsers.add_parser('record-wpr',
+                                     help='Record WPR from sandwich job.')
+  record_wpr.add_argument('--wpr-archive', required=True, type=str,
+                          dest='wpr_archive_path',
+                          help='Web page replay archive to generate.')
+
+  # Create cache subcommand.
+  create_cache_parser = subparsers.add_parser('create-cache',
+      help='Create cache from sandwich job.')
+  create_cache_parser.add_argument('--cache-archive', required=True, type=str,
+                                   dest='cache_archive_path',
+                                   help='Cache archive destination path.')
+  create_cache_parser.add_argument('--wpr-archive', default=None, type=str,
+                                   dest='wpr_archive_path',
+                                   help='Web page replay archive to create ' +
+                                       'the cache from.')
+
+  # Run subcommand.
+  run_parser = subparsers.add_parser('run', help='Run sandwich benchmark.')
+  run_parser.add_argument('--output', required=True, type=str,
+                          dest='trace_output_directory',
+                          help='Path of output directory to create.')
+  run_parser.add_argument('--cache-archive', type=str,
+                          dest='cache_archive_path',
+                          help='Cache archive destination path.')
+  run_parser.add_argument('--cache-op',
+                          choices=['clear', 'push', 'reload'],
+                          dest='cache_operation',
+                          default='clear',
+                          help='Configures cache operation to do before '
+                              +'launching Chrome. (Default is clear). The push'
+                              +' cache operation requires --cache-archive to '
+                              +'set.')
+  run_parser.add_argument('--disable-wpr-script-injection',
+                          action='store_true',
+                          help='Disable WPR default script injection such as ' +
+                              'overriding javascript\'s Math.random() and ' +
+                              'Date() with deterministic implementations.')
+  run_parser.add_argument('--network-condition', default=None,
+      choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
+      help='Set a network profile.')
+  run_parser.add_argument('--network-emulator', default='browser',
+      choices=['browser', 'wpr'],
+      help='Set which component is emulating the network condition.' +
+          ' (Default to browser). Wpr network emulator requires --wpr-archive' +
+          ' to be set.')
+  run_parser.add_argument('--job-repeat', default=1, type=int,
+                          help='How many times to run the job.')
+  run_parser.add_argument('--wpr-archive', default=None, type=str,
+                          dest='wpr_archive_path',
+                          help='Web page replay archive to load job\'s urls ' +
+                              'from.')
+  return parser
+
+
+def _RecordWprMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.wpr_record = True
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def _CreateCacheMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.cache_operation = 'save'
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def _RunJobMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def main(command_line_args):
+  logging.basicConfig(level=logging.INFO)
+  devil_chromium.Initialize()
+
+  # Don't give the argument yet. All we are interested in for now is accessing
+  # the default values of OPTIONS.
+  OPTIONS.ParseArgs([])
+
+  args = _ArgumentParser().parse_args(command_line_args)
+
+  if args.subcommand == 'record-wpr':
+    return _RecordWprMain(args)
+  if args.subcommand == 'create-cache':
+    return _CreateCacheMain(args)
+  if args.subcommand == 'run':
+    return _RunJobMain(args)
+  assert False
+
+
+if __name__ == '__main__':
+  sys.exit(main(sys.argv[1:]))

commit efced8e1630c13b8bc1f09e0dd8f5ad78ae2bcca
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 25 09:57:07 2016 -0800

    sandwich: Refactors main() into SandwichRunner class.
    
    Sandwich's command line tools were getting fatter and
    fatter as the CLs were landing. This CL refactors main()
    into smaller methods of the newly introduced
    SandwichRunner class. It also adds new sub-commands
    using SandwichRunner.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1731113003
    
    Cr-Original-Commit-Position: refs/heads/master@{#377612}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: aa5436dba8ebd27e983596b93f0ab43a10eebe1c

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index ff98acb..c337b8b 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -95,41 +95,283 @@ def _CleanPreviousTraces(output_directories_path):
     shutil.rmtree(directory_path)
 
 
-def _ArgumentParser():
-  """Build a command line argument's parser.
+class SandwichRunner(object):
+  """Sandwich runner.
+
+  This object is meant to be configured first and then run using the Run()
+  method. The runner can configure itself conveniently with parsed arguement
+  using the PullConfigFromArgs() method. The only job is to make sure that the
+  command line flags have `dest` parameter set to existing runner members.
   """
+
+  def __init__(self, job_name):
+    """Configures a sandwich runner out of the box.
+
+    Public members are meant to be configured as wished before calling Run().
+
+    Args:
+      job_name: The job name to get the associated urls.
+    """
+    # Cache operation to do before doing the chrome navigation.
+    #   Can be: clear,save,push,reload
+    self.cache_operation = 'clear'
+
+    # The cache archive's path to save to or push from. Is str or None.
+    self.cache_archive_path = None
+
+    # Controls whether the WPR server should do script injection.
+    self.disable_wpr_script_injection = False
+
+    # The job name. Is str.
+    self.job_name = job_name
+
+    # Number of times to repeat the job.
+    self.job_repeat = 1
+
+    # Network conditions to emulate. None if no emulation.
+    self.network_condition = None
+
+    # Network condition emulator. Can be: browser,wpr
+    self.network_emulator = 'browser'
+
+    # Output directory where to save the traces. Is str or None.
+    self.trace_output_directory = None
+
+    # List of urls to run.
+    self.urls = _ReadUrlsFromJobDescription(job_name)
+
+    # Path to the WPR archive to load or save. Is str or None.
+    self.wpr_archive_path = None
+
+    # Configures whether the WPR archive should be read or generated.
+    self.wpr_record = False
+
+    self._device = None
+    self._chrome_additional_flags = []
+    self._local_cache_directory_path = None
+
+  def PullConfigFromArgs(self, args):
+    """Configures the sandwich runner from parsed command line argument.
+
+    Args:
+      args: The command line parsed argument.
+    """
+    for config_name in self.__dict__.keys():
+      if config_name in args.__dict__:
+        self.__dict__[config_name] = args.__dict__[config_name]
+
+  def PrintConfig(self):
+    """Print the current sandwich runner configuration to stdout. """
+    for config_name in sorted(self.__dict__.keys()):
+      if config_name[0] != '_':
+        print '{} = {}'.format(config_name, self.__dict__[config_name])
+
+  def _CleanTraceOutputDirectory(self):
+    assert self.trace_output_directory
+    if not os.path.isdir(self.trace_output_directory):
+      try:
+        os.makedirs(self.trace_output_directory)
+      except OSError:
+        logging.error('Cannot create directory for results: %s',
+            self.trace_output_directory)
+        raise
+    else:
+      _CleanPreviousTraces(self.trace_output_directory)
+
+  def _SaveRunInfos(self, urls):
+    assert self.trace_output_directory
+    run_infos = {
+      'cache-op': self.cache_operation,
+      'job_name': self.job_name,
+      'urls': urls
+    }
+    with open(os.path.join(self.trace_output_directory, 'run_infos.json'),
+              'w') as file_output:
+      json.dump(run_infos, file_output, indent=2)
+
+  def _GetEmulatorNetworkCondition(self, emulator):
+    if self.network_emulator == emulator:
+      return self.network_condition
+    return None
+
+  def _RunNavigation(self, url, clear_cache, trace_id=None):
+    with device_setup.DeviceConnection(
+        device=self._device,
+        additional_flags=self._chrome_additional_flags) as connection:
+      additional_metadata = {}
+      if self._GetEmulatorNetworkCondition('browser'):
+        additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
+            connection=connection,
+            emulated_device_name=None,
+            emulated_network_name=self._GetEmulatorNetworkCondition('browser'))
+      loading_trace = trace_recorder.MonitorUrl(
+          connection, url,
+          clear_cache=clear_cache,
+          categories=pull_sandwich_metrics.CATEGORIES,
+          timeout=_DEVTOOLS_TIMEOUT)
+      loading_trace.metadata.update(additional_metadata)
+      if trace_id != None and self.trace_output_directory:
+        loading_trace_path = os.path.join(
+            self.trace_output_directory, str(trace_id), 'trace.json')
+        os.makedirs(os.path.dirname(loading_trace_path))
+        loading_trace.ToJsonFile(loading_trace_path)
+
+  def _RunUrl(self, url, trace_id=0):
+    clear_cache = False
+    if self.cache_operation == 'clear':
+      clear_cache = True
+    elif self.cache_operation == 'push':
+      self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+      chrome_cache.PushBrowserCache(self._device,
+                                    self._local_cache_directory_path)
+    elif self.cache_operation == 'reload':
+      self._RunNavigation(url, clear_cache=True)
+    elif self.cache_operation == 'save':
+      clear_cache = trace_id == 0
+    self._RunNavigation(url, clear_cache=clear_cache, trace_id=trace_id)
+
+  def _PullCacheFromDevice(self):
+    assert self.cache_operation == 'save'
+    assert self.cache_archive_path, 'Need to specify where to save the cache'
+
+    # Move Chrome to background to allow it to flush the index.
+    self._device.adb.Shell('am start com.google.android.launcher')
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    self._device.KillAll(OPTIONS.chrome_package_name, quiet=True)
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+
+    cache_directory_path = chrome_cache.PullBrowserCache(self._device)
+    chrome_cache.ZipDirectoryContent(
+        cache_directory_path, self.cache_archive_path)
+    shutil.rmtree(cache_directory_path)
+
+  def Run(self):
+    """SandwichRunner main entry point meant to be called once configured.
+    """
+    if self.trace_output_directory:
+      self._CleanTraceOutputDirectory()
+
+    self._device = device_utils.DeviceUtils.HealthyDevices()[0]
+    self._chrome_additional_flags = []
+
+    assert self._local_cache_directory_path == None
+    if self.cache_operation == 'push':
+      assert os.path.isfile(self.cache_archive_path)
+      self._local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+      chrome_cache.UnzipDirectoryContent(
+          self.cache_archive_path, self._local_cache_directory_path)
+
+    ran_urls = []
+    with device_setup.WprHost(self._device, self.wpr_archive_path,
+        record=self.wpr_record,
+        network_condition_name=self._GetEmulatorNetworkCondition('wpr'),
+        disable_script_injection=self.disable_wpr_script_injection
+        ) as additional_flags:
+      self._chrome_additional_flags.extend(additional_flags)
+      for _ in xrange(self.job_repeat):
+        for url in self.urls:
+          self._RunUrl(url, trace_id=len(ran_urls))
+          ran_urls.append(url)
+
+    if self._local_cache_directory_path:
+      shutil.rmtree(self._local_cache_directory_path)
+      self._local_cache_directory_path = None
+    if self.cache_operation == 'save':
+      self._PullCacheFromDevice()
+    if self.trace_output_directory:
+      self._SaveRunInfos(ran_urls)
+
+
+def _ArgumentParser():
+  """Build a command line argument's parser."""
   parser = argparse.ArgumentParser()
   parser.add_argument('--job', required=True,
                       help='JSON file with job description.')
-  parser.add_argument('--output', required=True,
-                      help='Name of output directory to create.')
-  parser.add_argument('--repeat', default=1, type=int,
-                      help='How many times to run the job')
-  parser.add_argument('--cache-op',
-                      choices=['clear', 'save', 'push', 'reload'],
-                      default='clear',
-                      help='Configures cache operation to do before launching '
-                          +'Chrome. (Default is clear).')
-  parser.add_argument('--wpr-archive', default=None, type=str,
-                      help='Web page replay archive to load job\'s urls from.')
-  parser.add_argument('--wpr-record', default=False, action='store_true',
-                      help='Record web page replay archive.')
-  parser.add_argument('--disable-wpr-script-injection', default=False,
-                      action='store_true',
-                      help='Disable WPR default script injection such as ' +
-                          'overriding javascript\'s Math.random() and Date() ' +
-                          'with deterministic implementations.')
-  parser.add_argument('--network-condition', default=None,
+  subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
+
+  # Record WPR subcommand.
+  record_wpr = subparsers.add_parser('record-wpr',
+                                     help='Record WPR from sandwich job.')
+  record_wpr.add_argument('--wpr-archive', required=True, type=str,
+                          dest='wpr_archive_path',
+                          help='Web page replay archive to generate.')
+
+  # Create cache subcommand.
+  create_cache_parser = subparsers.add_parser('create-cache',
+      help='Create cache from sandwich job.')
+  create_cache_parser.add_argument('--cache-archive', required=True, type=str,
+                                   dest='cache_archive_path',
+                                   help='Cache archive destination path.')
+  create_cache_parser.add_argument('--wpr-archive', default=None, type=str,
+                                   dest='wpr_archive_path',
+                                   help='Web page replay archive to create ' +
+                                       'the cache from.')
+
+  # Run subcommand.
+  run_parser = subparsers.add_parser('run', help='Run sandwich benchmark.')
+  run_parser.add_argument('--output', required=True, type=str,
+                          dest='trace_output_directory',
+                          help='Path of output directory to create.')
+  run_parser.add_argument('--cache-archive', type=str,
+                          dest='cache_archive_path',
+                          help='Cache archive destination path.')
+  run_parser.add_argument('--cache-op',
+                          choices=['clear', 'push', 'reload'],
+                          dest='cache_operation',
+                          default='clear',
+                          help='Configures cache operation to do before '
+                              +'launching Chrome. (Default is clear). The push'
+                              +' cache operation requires --cache-archive to '
+                              +'set.')
+  run_parser.add_argument('--disable-wpr-script-injection',
+                          action='store_true',
+                          help='Disable WPR default script injection such as ' +
+                              'overriding javascript\'s Math.random() and ' +
+                              'Date() with deterministic implementations.')
+  run_parser.add_argument('--network-condition', default=None,
       choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
       help='Set a network profile.')
-  parser.add_argument('--network-emulator', default='browser',
+  run_parser.add_argument('--network-emulator', default='browser',
       choices=['browser', 'wpr'],
       help='Set which component is emulating the network condition.' +
-          ' (Default to browser)')
+          ' (Default to browser). Wpr network emulator requires --wpr-archive' +
+          ' to be set.')
+  run_parser.add_argument('--job-repeat', default=1, type=int,
+                          help='How many times to run the job.')
+  run_parser.add_argument('--wpr-archive', default=None, type=str,
+                          dest='wpr_archive_path',
+                          help='Web page replay archive to load job\'s urls ' +
+                              'from.')
   return parser
 
 
-def main():
+def _RecordWprMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.wpr_record = True
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def _CreateCacheMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.cache_operation = 'save'
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def _RunJobMain(args):
+  sandwich_runner = SandwichRunner(args.job)
+  sandwich_runner.PullConfigFromArgs(args)
+  sandwich_runner.PrintConfig()
+  sandwich_runner.Run()
+  return 0
+
+
+def main(command_line_args):
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
 
@@ -137,102 +379,16 @@ def main():
   # the default values of OPTIONS.
   OPTIONS.ParseArgs([])
 
-  args = _ArgumentParser().parse_args()
-
-  if not os.path.isdir(args.output):
-    try:
-      os.makedirs(args.output)
-    except OSError:
-      logging.error('Cannot create directory for results: %s' % args.output)
-      raise
-  else:
-    _CleanPreviousTraces(args.output)
-
-  run_infos = {
-    'cache-op': args.cache_op,
-    'job': args.job,
-    'urls': []
-  }
-  job_urls = _ReadUrlsFromJobDescription(args.job)
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  local_cache_archive_path = os.path.join(args.output, 'cache.zip')
-  local_cache_directory_path = None
-  wpr_network_condition_name = None
-  browser_network_condition_name = None
-  if args.network_emulator == 'wpr':
-    wpr_network_condition_name = args.network_condition
-  elif args.network_emulator == 'browser':
-    browser_network_condition_name = args.network_condition
-  else:
-    assert False
-
-  if args.cache_op == 'push':
-    assert os.path.isfile(local_cache_archive_path)
-    local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-    chrome_cache.UnzipDirectoryContent(
-        local_cache_archive_path, local_cache_directory_path)
-
-  with device_setup.WprHost(device, args.wpr_archive,
-      record=args.wpr_record,
-      network_condition_name=wpr_network_condition_name,
-      disable_script_injection=args.disable_wpr_script_injection
-      ) as additional_flags:
-    def _RunNavigation(url, clear_cache, trace_id):
-      with device_setup.DeviceConnection(
-          device=device,
-          additional_flags=additional_flags) as connection:
-        additional_metadata = {}
-        if browser_network_condition_name:
-          additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
-              connection=connection,
-              emulated_device_name=None,
-              emulated_network_name=browser_network_condition_name)
-        loading_trace = trace_recorder.MonitorUrl(
-            connection, url,
-            clear_cache=clear_cache,
-            categories=pull_sandwich_metrics.CATEGORIES,
-            timeout=_DEVTOOLS_TIMEOUT)
-        loading_trace.metadata.update(additional_metadata)
-        if trace_id != None:
-          loading_trace_path = os.path.join(
-              args.output, str(trace_id), 'trace.json')
-          os.makedirs(os.path.dirname(loading_trace_path))
-          loading_trace.ToJsonFile(loading_trace_path)
-
-    for _ in xrange(args.repeat):
-      for url in job_urls:
-        clear_cache = False
-        if args.cache_op == 'clear':
-          clear_cache = True
-        elif args.cache_op == 'push':
-          device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-          chrome_cache.PushBrowserCache(device, local_cache_directory_path)
-        elif args.cache_op == 'reload':
-          _RunNavigation(url, clear_cache=True, trace_id=None)
-        elif args.cache_op == 'save':
-          clear_cache = not run_infos['urls']
-        _RunNavigation(url, clear_cache=clear_cache,
-                       trace_id=len(run_infos['urls']))
-        run_infos['urls'].append(url)
-
-  if local_cache_directory_path:
-    shutil.rmtree(local_cache_directory_path)
-
-  if args.cache_op == 'save':
-    # Move Chrome to background to allow it to flush the index.
-    device.adb.Shell('am start com.google.android.launcher')
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-
-    cache_directory_path = chrome_cache.PullBrowserCache(device)
-    chrome_cache.ZipDirectoryContent(
-        cache_directory_path, local_cache_archive_path)
-    shutil.rmtree(cache_directory_path)
+  args = _ArgumentParser().parse_args(command_line_args)
 
-  with open(os.path.join(args.output, 'run_infos.json'), 'w') as file_output:
-    json.dump(run_infos, file_output, indent=2)
+  if args.subcommand == 'record-wpr':
+    return _RecordWprMain(args)
+  if args.subcommand == 'create-cache':
+    return _CreateCacheMain(args)
+  if args.subcommand == 'run':
+    return _RunJobMain(args)
+  assert False
 
 
 if __name__ == '__main__':
-  sys.exit(main())
+  sys.exit(main(sys.argv[1:]))

commit a4b0fc00f331941e69992a5dcd5a7f11edb9c3fe
Author: mattcary <mattcary@chromium.org>
Date:   Thu Feb 25 09:42:15 2016 -0800

    Fix bug in my understanding of python slices.
    
    Review URL: https://codereview.chromium.org/1735953003
    
    Cr-Original-Commit-Position: refs/heads/master@{#377603}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: dc9bee425efc025e9a92a87dae5d29d84fa217b3

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 923183e..d929ac4 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -203,11 +203,11 @@ class ResourceGraph(object):
       costs[n.Index()] = cost
     max_cost = max(costs)
     if costs_out is not None:
-      del costs_out[:-1]
+      del costs_out[:]
       costs_out.extend(costs)
     assert max_cost > 0  # Otherwise probably the filter went awry.
     if path_list is not None:
-      del path_list[:-1]
+      del path_list[:]
       n = (i for i in self._nodes if costs[i.Index()] == max_cost).next()
       path_list.append(self._node_info[n.Index()])
       while n.Predecessors():

commit 3c8a865ae142634931d2d6909ac2c06f80ce073a
Author: mattcary <mattcary@chromium.org>
Date:   Thu Feb 25 08:37:48 2016 -0800

    Add ExtractArgs to options to enable it to play nicely with custom argument parsing.
    
    Review URL: https://codereview.chromium.org/1727263004
    
    Cr-Original-Commit-Position: refs/heads/master@{#377589}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9e0410c9fc70ddc3961678fca476d9de834208f2

diff --git a/loading/options.py b/loading/options.py
index a11b56c..ec295ac 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -62,11 +62,11 @@ class Options(object):
     """
     self._ARGS.append((arg_name, default, help_str))
 
-  def ParseArgs(self, arg_str, description=None, extra=None):
+  def ParseArgs(self, arg_list, description=None, extra=None):
     """Parse command line arguments.
 
     Args:
-      arg_str: command line argument string.
+      arg_list: command line argument list.
       description: description to use in argument parser.
       extra: additional required arguments to add. These will be exposed as
         instance attributes. This is either a list of extra arguments, or a
@@ -75,7 +75,46 @@ class Options(object):
         used as in argparse, ie those beginning with -- are named, and those
         without a dash are positional. Don't use a single dash.
     """
-    parser = argparse.ArgumentParser(description=description)
+    parser = self._MakeParser(description, extra)
+    self._parsed_args = parser.parse_args(arg_list)
+
+  def ExtractArgs(self, arg_list):
+    """Extract arguments from arg_str.
+
+    Args:
+      arg_list: command line argument list. It will be changed so that arguments
+        used by this options instance are removed.
+    """
+    parser = self._MakeParser()
+    (self._parsed_args, unused) = parser.parse_known_args(arg_list)
+    del arg_list[:]
+    arg_list.extend(unused)
+
+  def GetParentParser(self, group_name='Global'):
+    """Returns a parser suitable for passing in as a parent to argparse.
+
+    Args:
+      group_name: A group name for the parser (see argparse's
+        add_argument_group).
+
+    Returns:
+      An argparse parser instance.
+    """
+    return self._MakeParser(group=group_name)
+
+  def SetParsedArgs(self, parsed_args):
+    """Set parsed args. Used with GetParentParser.
+
+    Args:
+      parsed_args: the result of argparse.parse_args or similar.
+    """
+    self._parsed_args = parsed_args
+
+  def _MakeParser(self, description=None, extra=None, group=None):
+    add_help = True if group is None else False
+    parser = argparse.ArgumentParser(
+        description=description, add_help=add_help)
+    container = parser if group is None else parser.add_argument_group(group)
     for arg, default, help_str in self._ARGS:
       # All global options are named.
       arg = '--' + arg
@@ -86,12 +125,12 @@ class Options(object):
       for arg in extra:
         if type(arg) is tuple:
           argname, default = arg
-          self._AddArg(parser, argname, default)
+          self._AddArg(container, argname, default)
         else:
-          self._AddArg(parser, arg, None, required=True)
-    self._parsed_args = parser.parse_args(arg_str)
+          self._AddArg(container, arg, None, required=True)
+    return parser
 
-  def _AddArg(self, parser, arg, default, required=False, help_str=None):
+  def _AddArg(self, container, arg, default, required=False, help_str=None):
     assert not arg.startswith('-') or arg.startswith('--'), \
         "Single dash arguments aren't supported: %s" % arg
     arg_name = arg
@@ -117,7 +156,7 @@ class Options(object):
         kwargs['default'] = default
         kwargs['type'] = type(default)
 
-    parser.add_argument(arg, **kwargs)
+    container.add_argument(arg, **kwargs)
 
   def __getattr__(self, name):
     if name in self._arg_set:
diff --git a/loading/options_unittest.py b/loading/options_unittest.py
new file mode 100644
index 0000000..9ecca66
--- /dev/null
+++ b/loading/options_unittest.py
@@ -0,0 +1,32 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import unittest
+
+import options
+
+
+class OptionsTestCase(unittest.TestCase):
+  def testExtract(self):
+    args = ['--A', 'foo', '--devtools_port', '2000', '--B=20',
+            '--no_sandbox', '--C', '30', 'baz']
+    opts = options.Options()
+    opts.ExtractArgs(args)
+    self.assertEqual(['--A', 'foo', '--B=20', '--C', '30', 'baz'], args)
+    self.assertEqual(2000, opts.devtools_port)
+    self.assertTrue(opts.no_sandbox)
+
+  def testParent(self):
+    opts = options.Options()
+    parser = argparse.ArgumentParser(parents=[opts.GetParentParser()])
+    parser.add_argument('--foo', type=int)
+    parsed_args = parser.parse_args(['--foo=4', '--devtools_port', '2000'])
+    self.assertEqual(4, parsed_args.foo)
+    opts.SetParsedArgs(parsed_args)
+    self.assertEqual(2000, opts.devtools_port)
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 0bc21ed5d6d5d228d02a2d31422c3d3fef55783b
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 25 06:09:43 2016 -0800

    tools/android/loading: Implements WprBackend
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1722243002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377569}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c1adaaec9a61bb544d81d9218433990a1a159311

diff --git a/loading/wpr_backend.py b/loading/wpr_backend.py
new file mode 100644
index 0000000..c3a84a1
--- /dev/null
+++ b/loading/wpr_backend.py
@@ -0,0 +1,124 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Opens and modifies WPR archive.
+"""
+
+import collections
+import os
+import re
+import sys
+
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+_WEBPAGEREPLAY_DIR = os.path.join(_SRC_DIR, 'third_party', 'webpagereplay')
+_WEBPAGEREPLAY_HTTPARCHIVE = os.path.join(_WEBPAGEREPLAY_DIR, 'httparchive.py')
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
+import httparchive
+
+# Regex used to parse httparchive.py stdout's when listing all urls.
+_PARSE_WPR_REQUEST_REGEX = re.compile(r'^\S+\s+(?P<url>\S+)')
+
+
+class WprUrlEntry(object):
+  """Wpr url entry holding request and response infos. """
+
+  def __init__(self, wpr_request, wpr_response):
+    self._wpr_response = wpr_response
+    self.url = self._ExtractUrl(str(wpr_request))
+
+  def GetResponseHeadersDict(self):
+    """Get a copied dictionary of available headers.
+
+    Returns:
+      dict(name -> value)
+    """
+    headers = collections.defaultdict(list)
+    for (key, value) in self._wpr_response.headers:
+      headers[key].append(value)
+    return {k: ','.join(v) for (k, v) in headers.items()}
+
+  def SetResponseHeader(self, name, value):
+    """Set a header value.
+
+    In the case where the <name> response header is present more than once
+    in the response header list, then the given value is set only to the first
+    occurrence of that given headers, and the next ones are removed.
+
+    Args:
+      name: The name of the response header to set.
+      value: The value of the response header to set.
+    """
+    new_headers = []
+    new_header_set = False
+    for header in self._wpr_response.headers:
+      if header[0] != name:
+        new_headers.append(header)
+      elif not new_header_set:
+        new_header_set = True
+        new_headers.append((name, value))
+    if new_header_set:
+      self._wpr_response.headers = new_headers
+    else:
+      self._wpr_response.headers.append((name, value))
+
+  def DeleteResponseHeader(self, name):
+    """Delete a header.
+
+    In the case where the <name> response header is present more than once
+    in the response header list, this method takes care of removing absolutely
+    all them.
+
+    Args:
+      name: The name of the response header field to delete.
+    """
+    self._wpr_response.headers = \
+        [x for x in self._wpr_response.headers if x[0] != name]
+
+  @classmethod
+  def _ExtractUrl(cls, request_string):
+    match = _PARSE_WPR_REQUEST_REGEX.match(request_string)
+    assert match, 'Looks like there is an issue with: {}'.format(request_string)
+    return match.group('url')
+
+
+class WprArchiveBackend(object):
+  """WPR archive back-end able to read and modify. """
+
+  def __init__(self, wpr_archive_path):
+    """Constructor:
+
+    Args:
+      wpr_archive_path: The path of the WPR archive to read/modify.
+    """
+    self._wpr_archive_path = wpr_archive_path
+    self._http_archive = httparchive.HttpArchive.Load(wpr_archive_path)
+
+  def ListUrlEntries(self):
+    """Iterates over all url entries
+
+    Returns:
+      A list of WprUrlEntry.
+    """
+    return [WprUrlEntry(request, self._http_archive[request])
+            for request in self._http_archive.get_requests()]
+
+  def Persist(self):
+    """Persists the archive to disk. """
+    self._http_archive.Persist(self._wpr_archive_path)
+
+
+if __name__ == '__main__':
+  import argparse
+  parser = argparse.ArgumentParser(description='Tests cache back-end.')
+  parser.add_argument('wpr_archive', type=str)
+  command_line_args = parser.parse_args()
+
+  wpr_backend = WprArchiveBackend(command_line_args.wpr_archive)
+  url_entries = wpr_backend.ListUrlEntries()
+  print url_entries[0].url
+  wpr_backend.Persist()
diff --git a/loading/wpr_backend_unittest.py b/loading/wpr_backend_unittest.py
new file mode 100644
index 0000000..e38070c
--- /dev/null
+++ b/loading/wpr_backend_unittest.py
@@ -0,0 +1,92 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from wpr_backend import WprUrlEntry
+
+
+class MockWprResponse(object):
+  def __init__(self, headers):
+    self.headers = headers
+
+class WprUrlEntryTest(unittest.TestCase):
+
+  @classmethod
+  def _CreateWprUrlEntry(cls, headers):
+    wpr_response = MockWprResponse(headers)
+    return WprUrlEntry('GET http://a.com/', wpr_response)
+
+  def test_ExtractUrl(self):
+    self.assertEquals('http://aa.bb/c',
+                      WprUrlEntry._ExtractUrl('GET http://aa.bb/c'))
+    self.assertEquals('http://aa.b/c',
+                      WprUrlEntry._ExtractUrl('POST http://aa.b/c'))
+    self.assertEquals('http://a.bb/c',
+                      WprUrlEntry._ExtractUrl('WHATEVER http://a.bb/c'))
+    self.assertEquals('https://aa.bb/c',
+                      WprUrlEntry._ExtractUrl('GET https://aa.bb/c'))
+    self.assertEquals('http://aa.bb',
+                      WprUrlEntry._ExtractUrl('GET http://aa.bb'))
+    self.assertEquals('http://aa.bb',
+                      WprUrlEntry._ExtractUrl('GET http://aa.bb FOO BAR'))
+
+  def test_GetResponseHeadersDict(self):
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header0', 'value2'),
+                                     ('header2', 'value3'),
+                                     ('header0', 'value4')])
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('value0,value2,value4', headers['header0'])
+    self.assertEquals('value1', headers['header1'])
+    self.assertEquals('value3', headers['header2'])
+
+  def test_SetResponseHeader(self):
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1')])
+    entry.SetResponseHeader('new_header0', 'new_value0')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('new_value0', headers['new_header0'])
+    self.assertEquals('new_header0', entry._wpr_response.headers[2][0])
+
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header2', 'value1'),])
+    entry.SetResponseHeader('header1', 'new_value1')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(3, len(headers))
+    self.assertEquals('new_value1', headers['header1'])
+    self.assertEquals('header1', entry._wpr_response.headers[1][0])
+
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header2', 'value2'),
+                                     ('header1', 'value3'),
+                                     ('header3', 'value4')])
+    entry.SetResponseHeader('header1', 'new_value2')
+    headers = entry.GetResponseHeadersDict()
+    self.assertEquals(4, len(headers))
+    self.assertEquals('new_value2', headers['header1'])
+    self.assertEquals('header1', entry._wpr_response.headers[1][0])
+    self.assertEquals('header3', entry._wpr_response.headers[3][0])
+    self.assertEquals('value4', entry._wpr_response.headers[3][1])
+
+  def test_DeleteResponseHeader(self):
+    entry = self._CreateWprUrlEntry([('header0', 'value0'),
+                                     ('header1', 'value1'),
+                                     ('header0', 'value2'),
+                                     ('header2', 'value3')])
+    entry.DeleteResponseHeader('header1')
+    self.assertNotIn('header1', entry.GetResponseHeadersDict())
+    self.assertEquals(2, len(entry.GetResponseHeadersDict()))
+    entry.DeleteResponseHeader('header0')
+    self.assertNotIn('header0', entry.GetResponseHeadersDict())
+    self.assertEquals(1, len(entry.GetResponseHeadersDict()))
+
+
+if __name__ == '__main__':
+  unittest.main()

commit b53fdc4bfa7d6a7047354d3cf3372ac1423b0c8b
Author: mattcary <mattcary@chromium.org>
Date:   Wed Feb 24 04:22:56 2016 -0800

    DOT output of multi-graph ResourceSack analysis.
    
    Review URL: https://codereview.chromium.org/1726573002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377274}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 311d001586e703ed92e460a0ffafdf00b9006326

diff --git a/loading/resource_sack_display.py b/loading/resource_sack_display.py
new file mode 100644
index 0000000..210f4da
--- /dev/null
+++ b/loading/resource_sack_display.py
@@ -0,0 +1,135 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Utilities for displaying a ResourceSack.
+
+When run standalone, takes traces on the command line and produces a dot file to
+stdout.
+"""
+
+
+def ToDot(sack, output, prune=-1, long_edge_msec=2000):
+  """Output as a dot file.
+
+  Args:
+    sack: (ResourceSack) the sack to convert to dot.
+    output: a file-like output stream.
+    prune: if positive, prune & coalesce nodes under the specified threshold
+      of repeated views, as fraction node views / total graphs. All pruned
+      nodes are represented by a single node, and an edge is connected only if
+      the view count is greater than 1.
+    long_edge_msec: if positive, the definition of a long edge. Long edges are
+      distinguished in graph.
+  """
+  output.write("""digraph dependencies {
+  rankdir = LR;
+  """)
+
+  pruned = set()
+  num_graphs = len(sack.graph_info)
+  for bag in sack.bags:
+    if prune > 0 and float(len(bag.graphs)) / num_graphs < prune:
+      pruned.add(bag)
+      continue
+    output.write('%d [label="%s (%d)\n(%d, %d)\n(%.2f, %.2f)" shape=%s; '
+                 'style=filled; fillcolor=%s];\n' % (
+        bag.Index(), bag.label, len(bag.graphs),
+        min(bag.total_costs), max(bag.total_costs),
+        min(bag.relative_costs), max(bag.relative_costs),
+        _CriticalToShape(bag),
+        _AmountToNodeColor(len(bag.graphs), num_graphs)))
+
+  if pruned:
+    pruned_index = num_graphs
+    output.write('%d [label="Pruned at %.0f%%\n(%d)"; '
+                 'shape=polygon; style=dotted];\n' %
+                 (pruned_index, 100 * prune, len(pruned)))
+
+  for bag in sack.bags:
+    if bag in pruned:
+      for succ in bag.Successors():
+        if succ not in pruned:
+          output.write('%d -> %d [style=dashed];\n' % (
+              pruned_index, succ.Index()))
+    for succ in bag.Successors():
+      if succ in pruned:
+        if len(bag.successor_sources[succ]) > 1:
+          output.write('%d -> %d [label="%d"; style=dashed];\n' % (
+              bag.Index(), pruned_index, len(bag.successor_sources[succ])))
+      else:
+        num_succ = len(bag.successor_sources[succ])
+        num_long = 0
+        for graph, source, target in bag.successor_sources[succ]:
+          if graph.EdgeCost(source, target) > long_edge_msec:
+            num_long += 1
+        if num_long > 0:
+          long_frac = float(num_long) / num_succ
+          long_edge_style = '; penwidth=%f' % (2 + 6.0 * long_frac)
+          if long_frac < 0.75:
+            long_edge_style += '; style=dashed'
+        else:
+          long_edge_style = ''
+        min_edge = min(bag.successor_edge_costs[succ])
+        max_edge = max(bag.successor_edge_costs[succ])
+        output.write('%d -> %d [label="%d\n(%f,%f)"; color=%s %s];\n' % (
+            bag.Index(), succ.Index(), num_succ, min_edge, max_edge,
+            _AmountToEdgeColor(num_succ, len(bag.graphs)),
+            long_edge_style))
+
+  output.write('}')
+
+
+def _CriticalToShape(bag):
+  frac = float(bag.num_critical) / bag.num_nodes
+  if frac < 0.4:
+    return 'oval'
+  elif frac < 0.7:
+    return 'polygon'
+  elif frac < 0.9:
+    return 'trapezium'
+  return 'box'
+
+
+def _AmountToNodeColor(numer, denom):
+  if denom <= 0:
+    return 'grey72'
+  ratio = 1.0 * numer / denom
+  if ratio < .3:
+    return 'white'
+  elif ratio < .6:
+    return 'yellow'
+  elif ratio < .8:
+    return 'orange'
+  return 'green'
+
+
+def _AmountToEdgeColor(numer, denom):
+  color = _AmountToNodeColor(numer, denom)
+  if color == 'white' or color == 'grey72':
+    return 'black'
+  return color
+
+
+def _Main():
+  import json
+  import logging
+  import sys
+
+  import loading_model
+  import loading_trace
+  import resource_sack
+
+  sack = resource_sack.GraphSack()
+  for fname in sys.argv[1:]:
+    trace = loading_trace.LoadingTrace.FromJsonDict(
+      json.load(open(fname)))
+    logging.info('Making graph from %s', fname)
+    model = loading_model.ResourceGraph(trace, content_lens=None)
+    sack.ConsumeGraph(model)
+    logging.info('Finished %s', fname)
+  ToDot(sack, sys.stdout, prune=.1)
+
+
+if __name__ == '__main__':
+  _Main()
diff --git a/loading/resource_sack_display_unittest.py b/loading/resource_sack_display_unittest.py
new file mode 100644
index 0000000..65b53ff
--- /dev/null
+++ b/loading/resource_sack_display_unittest.py
@@ -0,0 +1,42 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import re
+from StringIO import StringIO
+import unittest
+
+import resource_sack
+import resource_sack_display
+from test_utils import (MakeRequest,
+                        TestResourceGraph)
+
+
+class ResourceSackDispayTestCase(unittest.TestCase):
+  def test_SimpleOutput(self):
+    g1 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(3, 1)])
+    g2 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(4, 2)])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    sack.ConsumeGraph(g2)
+    buf = StringIO()
+    resource_sack_display.ToDot(sack, buf,
+                                long_edge_msec=1000)
+    dot = buf.getvalue()
+    # Short edge.
+    self.assertTrue(re.search(r'0 -> 1[^]]+color=green \]', dot, re.MULTILINE))
+    # Long edge.
+    self.assertTrue(re.search(r'0 -> 3[^]]+penwidth=8', dot))
+
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 2549e4b2415e46fe789b490cca240e6a87e43658
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 23 12:45:03 2016 -0800

    tools/android/loading: Moves cache specific code to chrome_cache.py
    
    run_sandwich.py had some cache related code such as pulling/pushing
    code from/to android device and zip/unzip cache directory keeping
    track of all file timestamps.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1712193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377077}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: dca6c239da6ac53adce1d87a01c837a940b25a1b

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
index 9e282b6..d6d65a2 100644
--- a/loading/chrome_cache.py
+++ b/loading/chrome_cache.py
@@ -5,8 +5,24 @@
 """Takes care of manipulating the chrome's HTTP cache.
 """
 
+from datetime import datetime
+import json
 import os
 import subprocess
+import sys
+import tempfile
+import zipfile
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
+
+import options
+
+
+OPTIONS = options.OPTIONS
 
 
 # Cache back-end types supported by cachetool.
@@ -20,6 +36,172 @@ OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
 CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
 
 
+def _RemoteCacheDirectory():
+  """Returns the path of the cache directory's on the remote device."""
+  return '/data/data/{}/cache/Cache'.format(
+      constants.PACKAGE_INFO[OPTIONS.chrome_package_name].package)
+
+
+def _UpdateTimestampFromAdbStat(filename, stat):
+  os.utime(filename, (stat.st_time, stat.st_time))
+
+
+def _AdbShell(adb, cmd):
+  adb.Shell(subprocess.list2cmdline(cmd))
+
+
+def _AdbUtime(adb, filename, timestamp):
+  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
+  """
+  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
+  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
+
+
+def PullBrowserCache(device):
+  """Pulls the browser cache from the device and saves it locally.
+
+  Cache is saved with the same file structure as on the device. Timestamps are
+  important to preserve because indexing and eviction depends on them.
+
+  Returns:
+    Temporary directory containing all the browser cache.
+  """
+  _INDEX_DIRECTORY_NAME = 'index-dir'
+  _REAL_INDEX_FILE_NAME = 'the-real-index'
+
+  remote_cache_directory = _RemoteCacheDirectory()
+  print remote_cache_directory
+  save_target = tempfile.mkdtemp(suffix='.cache')
+  for filename, stat in device.adb.Ls(remote_cache_directory):
+    if filename == '..':
+      continue
+    if filename == '.':
+      cache_directory_stat = stat
+      continue
+    original_file = os.path.join(remote_cache_directory, filename)
+    saved_file = os.path.join(save_target, filename)
+    device.adb.Pull(original_file, saved_file)
+    _UpdateTimestampFromAdbStat(saved_file, stat)
+    if filename == _INDEX_DIRECTORY_NAME:
+      # The directory containing the index was pulled recursively, update the
+      # timestamps for known files. They are ignored by cache backend, but may
+      # be useful for debugging.
+      index_dir_stat = stat
+      saved_index_dir = os.path.join(save_target, _INDEX_DIRECTORY_NAME)
+      saved_index_file = os.path.join(saved_index_dir, _REAL_INDEX_FILE_NAME)
+      for sub_file, sub_stat in device.adb.Ls(original_file):
+        if sub_file == _REAL_INDEX_FILE_NAME:
+          _UpdateTimestampFromAdbStat(saved_index_file, sub_stat)
+          break
+      _UpdateTimestampFromAdbStat(saved_index_dir, index_dir_stat)
+
+  # Store the cache directory modification time. It is important to update it
+  # after all files in it have been written. The timestamp is compared with
+  # the contents of the index file when freshness is determined.
+  _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
+  return save_target
+
+
+def PushBrowserCache(device, local_cache_path):
+  """Pushes the browser cache saved locally to the device.
+
+  Args:
+    device: Android device.
+    local_cache_path: The directory's path containing the cache locally.
+  """
+  remote_cache_directory = _RemoteCacheDirectory()
+
+  # Clear previous cache.
+  _AdbShell(device.adb, ['rm', '-rf', remote_cache_directory])
+  _AdbShell(device.adb, ['mkdir', remote_cache_directory])
+
+  # Push cache content.
+  device.adb.Push(local_cache_path, remote_cache_directory)
+
+  # Walk through the local cache to update mtime on the device.
+  def MirrorMtime(local_path):
+    cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
+    remote_path = os.path.join(remote_cache_directory, cache_relative_path)
+    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
+
+  for local_directory_path, dirnames, filenames in os.walk(
+        local_cache_path, topdown=False):
+    for filename in filenames:
+      MirrorMtime(os.path.join(local_directory_path, filename))
+    for dirname in dirnames:
+      MirrorMtime(os.path.join(local_directory_path, dirname))
+  MirrorMtime(local_cache_path)
+
+
+def ZipDirectoryContent(root_directory_path, archive_dest_path):
+  """Zip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    root_directory_path: The directory's path to archive.
+    archive_dest_path: Archive destination's path.
+  """
+  with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
+    timestamps = {}
+    root_directory_stats = os.stat(root_directory_path)
+    timestamps['.'] = {
+        'atime': root_directory_stats.st_atime,
+        'mtime': root_directory_stats.st_mtime}
+    for directory_path, dirnames, filenames in os.walk(root_directory_path):
+      for dirname in dirnames:
+        subdirectory_path = os.path.join(directory_path, dirname)
+        subdirectory_relative_path = os.path.relpath(subdirectory_path,
+                                                     root_directory_path)
+        subdirectory_stats = os.stat(subdirectory_path)
+        timestamps[subdirectory_relative_path] = {
+            'atime': subdirectory_stats.st_atime,
+            'mtime': subdirectory_stats.st_mtime}
+      for filename in filenames:
+        file_path = os.path.join(directory_path, filename)
+        file_archive_name = os.path.join('content',
+            os.path.relpath(file_path, root_directory_path))
+        file_stats = os.stat(file_path)
+        timestamps[file_archive_name[8:]] = {
+            'atime': file_stats.st_atime,
+            'mtime': file_stats.st_mtime}
+        zip_output.write(file_path, arcname=file_archive_name)
+    zip_output.writestr('timestamps.json',
+                        json.dumps(timestamps, indent=2))
+
+
+def UnzipDirectoryContent(archive_path, directory_dest_path):
+  """Unzip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    archive_path: Archive's path to unzip.
+    directory_dest_path: Directory destination path.
+  """
+  if not os.path.exists(directory_dest_path):
+    os.makedirs(directory_dest_path)
+
+  with zipfile.ZipFile(archive_path) as zip_input:
+    timestamps = None
+    for file_archive_name in zip_input.namelist():
+      if file_archive_name == 'timestamps.json':
+        timestamps = json.loads(zip_input.read(file_archive_name))
+      elif file_archive_name.startswith('content/'):
+        file_relative_path = file_archive_name[8:]
+        file_output_path = os.path.join(directory_dest_path, file_relative_path)
+        file_parent_directory_path = os.path.dirname(file_output_path)
+        if not os.path.exists(file_parent_directory_path):
+          os.makedirs(file_parent_directory_path)
+        with open(file_output_path, 'w') as f:
+          f.write(zip_input.read(file_archive_name))
+
+    assert timestamps
+    for relative_path, stats in timestamps.iteritems():
+      output_path = os.path.join(directory_dest_path, relative_path)
+      if not os.path.exists(output_path):
+        os.makedirs(output_path)
+      os.utime(output_path, (stats['atime'], stats['mtime']))
+
+
 class CacheBackend(object):
   """Takes care of reading and deleting cached keys.
   """
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 0572d5b..ff98acb 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -12,16 +12,13 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
-from datetime import datetime
 import json
 import logging
 import os
 import shutil
-import subprocess
 import sys
 import tempfile
 import time
-import zipfile
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -33,6 +30,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 import devil_chromium
 
+import chrome_cache
 import chrome_setup
 import device_setup
 import devtools_monitor
@@ -48,24 +46,11 @@ OPTIONS = options.OPTIONS
 
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
-# Directory name under --output to save the cache from the device.
-_CACHE_DIRECTORY_NAME = 'cache'
-
-# Name of cache subdirectory on the device where the cache index is stored.
-_INDEX_DIRECTORY_NAME = 'index-dir'
-
-# Name of the file containing the cache index. This file is stored on the device
-# in the cache directory under _INDEX_DIRECTORY_NAME.
-_REAL_INDEX_FILE_NAME = 'the-real-index'
-
 # An estimate of time to wait for the device to become idle after expensive
 # operations, such as opening the launcher activity.
 _TIME_TO_DEVICE_IDLE_SECONDS = 2
 
 
-def _RemoteCacheDirectory():
-  return '/data/data/{}/cache/Cache'.format(OPTIONS.chrome_package_name)
-
 # Devtools timeout of 1 minute to avoid websocket timeout on slow
 # network condition.
 _DEVTOOLS_TIMEOUT = 60
@@ -92,159 +77,6 @@ def _ReadUrlsFromJobDescription(job_name):
   raise Exception('Job description does not define a list named "urls"')
 
 
-def _UpdateTimestampFromAdbStat(filename, stat):
-  os.utime(filename, (stat.st_time, stat.st_time))
-
-
-def _AdbShell(adb, cmd):
-  adb.Shell(subprocess.list2cmdline(cmd))
-
-
-def _AdbUtime(adb, filename, timestamp):
-  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
-  """
-  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
-  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
-
-
-def _PullBrowserCache(device):
-  """Pulls the browser cache from the device and saves it locally.
-
-  Cache is saved with the same file structure as on the device. Timestamps are
-  important to preserve because indexing and eviction depends on them.
-
-  Returns:
-    Temporary directory containing all the browser cache.
-  """
-  save_target = tempfile.mkdtemp(suffix='.cache')
-  for filename, stat in device.adb.Ls(_RemoteCacheDirectory()):
-    if filename == '..':
-      continue
-    if filename == '.':
-      cache_directory_stat = stat
-      continue
-    original_file = os.path.join(_RemoteCacheDirectory(), filename)
-    saved_file = os.path.join(save_target, filename)
-    device.adb.Pull(original_file, saved_file)
-    _UpdateTimestampFromAdbStat(saved_file, stat)
-    if filename == _INDEX_DIRECTORY_NAME:
-      # The directory containing the index was pulled recursively, update the
-      # timestamps for known files. They are ignored by cache backend, but may
-      # be useful for debugging.
-      index_dir_stat = stat
-      saved_index_dir = os.path.join(save_target, _INDEX_DIRECTORY_NAME)
-      saved_index_file = os.path.join(saved_index_dir, _REAL_INDEX_FILE_NAME)
-      for sub_file, sub_stat in device.adb.Ls(original_file):
-        if sub_file == _REAL_INDEX_FILE_NAME:
-          _UpdateTimestampFromAdbStat(saved_index_file, sub_stat)
-          break
-      _UpdateTimestampFromAdbStat(saved_index_dir, index_dir_stat)
-
-  # Store the cache directory modification time. It is important to update it
-  # after all files in it have been written. The timestamp is compared with
-  # the contents of the index file when freshness is determined.
-  _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
-  return save_target
-
-
-def _PushBrowserCache(device, local_cache_path):
-  """Pushes the browser cache saved locally to the device.
-
-  Args:
-    device: Android device.
-    local_cache_path: The directory's path containing the cache locally.
-  """
-  # Clear previous cache.
-  _AdbShell(device.adb, ['rm', '-rf', _RemoteCacheDirectory()])
-  _AdbShell(device.adb, ['mkdir', _RemoteCacheDirectory()])
-
-  # Push cache content.
-  device.adb.Push(local_cache_path, _RemoteCacheDirectory())
-
-  # Walk through the local cache to update mtime on the device.
-  def MirrorMtime(local_path):
-    cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
-    remote_path = os.path.join(_RemoteCacheDirectory(), cache_relative_path)
-    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
-
-  for local_directory_path, dirnames, filenames in os.walk(
-        local_cache_path, topdown=False):
-    for filename in filenames:
-      MirrorMtime(os.path.join(local_directory_path, filename))
-    for dirname in dirnames:
-      MirrorMtime(os.path.join(local_directory_path, dirname))
-  MirrorMtime(local_cache_path)
-
-
-def _ZipDirectoryContent(root_directory_path, archive_dest_path):
-  """Zip a directory's content recursively with all the directories'
-  timestamps preserved.
-
-  Args:
-    root_directory_path: The directory's path to archive.
-    archive_dest_path: Archive destination's path.
-  """
-  with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
-    timestamps = {}
-    root_directory_stats = os.stat(root_directory_path)
-    timestamps['.'] = {
-        'atime': root_directory_stats.st_atime,
-        'mtime': root_directory_stats.st_mtime}
-    for directory_path, dirnames, filenames in os.walk(root_directory_path):
-      for dirname in dirnames:
-        subdirectory_path = os.path.join(directory_path, dirname)
-        subdirectory_relative_path = os.path.relpath(subdirectory_path,
-                                                     root_directory_path)
-        subdirectory_stats = os.stat(subdirectory_path)
-        timestamps[subdirectory_relative_path] = {
-            'atime': subdirectory_stats.st_atime,
-            'mtime': subdirectory_stats.st_mtime}
-      for filename in filenames:
-        file_path = os.path.join(directory_path, filename)
-        file_archive_name = os.path.join('content',
-            os.path.relpath(file_path, root_directory_path))
-        file_stats = os.stat(file_path)
-        timestamps[file_archive_name[8:]] = {
-            'atime': file_stats.st_atime,
-            'mtime': file_stats.st_mtime}
-        zip_output.write(file_path, arcname=file_archive_name)
-    zip_output.writestr('timestamps.json',
-                        json.dumps(timestamps, indent=2))
-
-
-def _UnzipDirectoryContent(archive_path, directory_dest_path):
-  """Unzip a directory's content recursively with all the directories'
-  timestamps preserved.
-
-  Args:
-    archive_path: Archive's path to unzip.
-    directory_dest_path: Directory destination path.
-  """
-  if not os.path.exists(directory_dest_path):
-    os.makedirs(directory_dest_path)
-
-  with zipfile.ZipFile(archive_path) as zip_input:
-    timestamps = None
-    for file_archive_name in zip_input.namelist():
-      if file_archive_name == 'timestamps.json':
-        timestamps = json.loads(zip_input.read(file_archive_name))
-      elif file_archive_name.startswith('content/'):
-        file_relative_path = file_archive_name[8:]
-        file_output_path = os.path.join(directory_dest_path, file_relative_path)
-        file_parent_directory_path = os.path.dirname(file_output_path)
-        if not os.path.exists(file_parent_directory_path):
-          os.makedirs(file_parent_directory_path)
-        with open(file_output_path, 'w') as f:
-          f.write(zip_input.read(file_archive_name))
-
-    assert timestamps
-    for relative_path, stats in timestamps.iteritems():
-      output_path = os.path.join(directory_dest_path, relative_path)
-      if not os.path.exists(output_path):
-        os.makedirs(output_path)
-      os.utime(output_path, (stats['atime'], stats['mtime']))
-
-
 def _CleanPreviousTraces(output_directories_path):
   """Cleans previous traces from the output directory.
 
@@ -337,7 +169,8 @@ def main():
   if args.cache_op == 'push':
     assert os.path.isfile(local_cache_archive_path)
     local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
-    _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
+    chrome_cache.UnzipDirectoryContent(
+        local_cache_archive_path, local_cache_directory_path)
 
   with device_setup.WprHost(device, args.wpr_archive,
       record=args.wpr_record,
@@ -373,7 +206,7 @@ def main():
           clear_cache = True
         elif args.cache_op == 'push':
           device.KillAll(OPTIONS.chrome_package_name, quiet=True)
-          _PushBrowserCache(device, local_cache_directory_path)
+          chrome_cache.PushBrowserCache(device, local_cache_directory_path)
         elif args.cache_op == 'reload':
           _RunNavigation(url, clear_cache=True, trace_id=None)
         elif args.cache_op == 'save':
@@ -392,8 +225,9 @@ def main():
     device.KillAll(OPTIONS.chrome_package_name, quiet=True)
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
 
-    cache_directory_path = _PullBrowserCache(device)
-    _ZipDirectoryContent(cache_directory_path, local_cache_archive_path)
+    cache_directory_path = chrome_cache.PullBrowserCache(device)
+    chrome_cache.ZipDirectoryContent(
+        cache_directory_path, local_cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
   with open(os.path.join(args.output, 'run_infos.json'), 'w') as file_output:

commit 072cbb22690f1bdde2ff0a1f9f35264226256f0d
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 23 11:21:19 2016 -0800

    tools/android/loading: Implements chrome_cache.CacheBackend.
    
    chrome_cache.CacheBackend is a python wraper of the new cachetool
    command line tool that has the following features: list all key,
    access a key's stream, and delete a key.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1713973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377043}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: aa36ddf8dcdc576dc74c87cf014066c2a7c65d24

diff --git a/loading/chrome_cache.py b/loading/chrome_cache.py
new file mode 100644
index 0000000..9e282b6
--- /dev/null
+++ b/loading/chrome_cache.py
@@ -0,0 +1,113 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Takes care of manipulating the chrome's HTTP cache.
+"""
+
+import os
+import subprocess
+
+
+# Cache back-end types supported by cachetool.
+BACKEND_TYPES = ['simple']
+
+# Default build output directory.
+OUT_DIRECTORY = os.getenv('CR_OUT_FULL', os.path.join(
+    os.path.dirname(__file__), '../../../out/Release'))
+
+# Default cachetool binary location.
+CACHETOOL_BIN_PATH = os.path.join(OUT_DIRECTORY, 'cachetool')
+
+
+class CacheBackend(object):
+  """Takes care of reading and deleting cached keys.
+  """
+
+  def __init__(self, cache_directory_path, cache_backend_type,
+               cachetool_bin_path=CACHETOOL_BIN_PATH):
+    """Chrome cache back-end constructor.
+
+    Args:
+      cache_directory_path: The directory path where the cache is locally
+        stored.
+      cache_backend_type: A cache back-end type in BACKEND_TYPES.
+      cachetool_bin_path: Path of the cachetool binary.
+    """
+    assert os.path.isdir(cache_directory_path)
+    assert cache_backend_type in BACKEND_TYPES
+    assert os.path.isfile(cachetool_bin_path), 'invalid ' + cachetool_bin_path
+    self._cache_directory_path = cache_directory_path
+    self._cache_backend_type = cache_backend_type
+    self._cachetool_bin_path = cachetool_bin_path
+    # Make sure cache_directory_path is a valid cache.
+    self._CachetoolCmd('validate')
+
+  def ListKeys(self):
+    """Lists cache's keys.
+
+    Returns:
+      A list of all keys stored in the cache.
+    """
+    return [k.strip() for k in self._CachetoolCmd('list_keys').split('\n')[:-1]]
+
+  def GetStreamForKey(self, key, index):
+    """Gets a key's stream.
+
+    Args:
+      key: The key to access the stream.
+      index: The stream index:
+          index=0 is the HTTP response header;
+          index=1 is the transport encoded content;
+          index=2 is the compiled content.
+
+    Returns:
+      String holding stream binary content.
+    """
+    return self._CachetoolCmd('get_stream', key, str(index))
+
+  def DeleteKey(self, key):
+    """Deletes a key from the cache.
+
+    Args:
+      key: The key delete.
+    """
+    self._CachetoolCmd('delete_key', key)
+
+  def _CachetoolCmd(self, operation, *args):
+    """Runs the cache editor tool and return the stdout.
+
+    Args:
+      operation: Cachetool operation.
+      *args: Additional operation argument to append to the command line.
+
+    Returns:
+      Cachetool's stdout string.
+    """
+    editor_tool_cmd = [
+        self._cachetool_bin_path,
+        self._cache_directory_path,
+        self._cache_backend_type,
+        operation]
+    editor_tool_cmd.extend(args)
+    process = subprocess.Popen(editor_tool_cmd, stdout=subprocess.PIPE)
+    stdout_data, _ = process.communicate()
+    assert process.returncode == 0
+    return stdout_data
+
+
+if __name__ == '__main__':
+  import argparse
+  parser = argparse.ArgumentParser(description='Tests cache back-end.')
+  parser.add_argument('cache_path', type=str)
+  parser.add_argument('backend_type', type=str, choices=BACKEND_TYPES)
+  command_line_args = parser.parse_args()
+
+  cache_backend = CacheBackend(
+      cache_directory_path=command_line_args.cache_path,
+      cache_backend_type=command_line_args.backend_type)
+  keys = cache_backend.ListKeys()
+  print '{}\'s HTTP response header:'.format(keys[0])
+  print cache_backend.GetStreamForKey(keys[0], 0)
+  cache_backend.DeleteKey(keys[1])
+  assert keys[1] not in cache_backend.ListKeys()

commit 3ca5499832089b3a2d4c992eadcfac64b023bddd
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 23 08:11:26 2016 -0800

    Core multi-graph analysis. Adds the ResourceSack object which is a graph of
    graphs. The display code (turning this into a DOT) will follow in a separate
    CL.
    
    This combines multiple loading_model.ResourceGraphs into a summary graph. It's intended to be used on multiple traces from the same web page, to figure out what the core part of a web page is. This CL doesn't accomplish that goal, and is instead the starting point of such analysis.
    
    Review URL: https://codereview.chromium.org/1722583002
    
    Cr-Original-Commit-Position: refs/heads/master@{#377000}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 20de63ca4ca7ea72c7d18c9f299fde1467eeb8f6

diff --git a/loading/loading_model.py b/loading/loading_model.py
index f9905a0..923183e 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -38,6 +38,7 @@ class ResourceGraph(object):
   EDGE_KIND_KEY = 'edge_kind'
   EDGE_KINDS = request_track.Request.INITIATORS + (
       'script_inferred', 'after-load', 'before-load', 'timing')
+
   def __init__(self, trace, content_lens=None, frame_lens=None,
                activity=None):
     """Create from a LoadingTrace (or json of a trace).
@@ -179,11 +180,13 @@ class ResourceGraph(object):
       if self._node_filter(n.Node()) and n.Url() in other_map:
         yield(n, other_map[n.Url()])
 
-  def Cost(self, path_list=None):
+  def Cost(self, path_list=None, costs_out=None):
     """Compute cost of current model.
 
     Args:
       path_list: if not None, gets a list of NodeInfo in the longest path.
+      costs_out: if not None, gets a vector of node costs by node index. Any
+        filtered nodes will have zero cost.
 
     Returns:
       Cost of the longest path.
@@ -199,6 +202,9 @@ class ResourceGraph(object):
         cost += self.NodeCost(n)
       costs[n.Index()] = cost
     max_cost = max(costs)
+    if costs_out is not None:
+      del costs_out[:-1]
+      costs_out.extend(costs)
     assert max_cost > 0  # Otherwise probably the filter went awry.
     if path_list is not None:
       del path_list[:-1]
@@ -385,7 +391,7 @@ class ResourceGraph(object):
 
     def EdgeAnnotations(self, s):
       assert s.Node() in self.Node().Successors()
-      return self._edge_annotations.get(s, [])
+      return self._edge_annotations.get(s, {})
 
     def ContentType(self):
       if self._request is None:
diff --git a/loading/resource_sack.py b/loading/resource_sack.py
new file mode 100644
index 0000000..493029c
--- /dev/null
+++ b/loading/resource_sack.py
@@ -0,0 +1,181 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""A collection of ResourceGraphs.
+
+Processes multiple ResourceGraphs, all presumably from requests to the same
+site. Common urls are collected in Bags and different statistics on the
+relationship between bags are collected.
+"""
+
+import collections
+import json
+import sys
+import urlparse
+
+from collections import defaultdict
+
+import content_classification_lens
+import dag
+import user_satisfied_lens
+
+class GraphSack(object):
+  """Aggreate of ResourceGraphs.
+
+  Collects ResourceGraph nodes into bags, where each bag contains the nodes with
+  common urls. Dependency edges are tracked between bags (so that each bag may
+  be considered as a node of a graph). This graph of bags is referred to as a
+  sack.
+
+  Each bag is associated with a dag.Node, even though the bag graph may not be a
+  DAG. The edges are annotated with list of graphs and nodes that generated
+  them.
+  """
+  _GraphInfo = collections.namedtuple('_GraphInfo', (
+      'cost',   # The graph cost (aka critical path length).
+      'total_costs',  # A vector by node index of total cost of each node.
+      ))
+
+  def __init__(self):
+    # A bag is a node in our combined graph.
+    self._bags = []
+    # Each bag in our sack corresponds to a url, as expressed by this map.
+    self._url_to_bag = {}
+    # Maps graph -> _GraphInfo structures for each graph we've consumed.
+    self._graph_info = {}
+
+  def ConsumeGraph(self, graph):
+    """Add a graph and process.
+
+    Args:
+      graph: (ResourceGraph) the graph to add. The graph is processed sorted
+        according to its current filter.
+    """
+    assert graph not in self._graph_info
+    critical_path = []
+    total_costs = []
+    cost = graph.Cost(path_list=critical_path,
+                      costs_out=total_costs)
+    self._graph_info[graph] = self._GraphInfo(
+        cost=cost, total_costs=total_costs)
+    for n in graph.Nodes(sort=True):
+      assert graph._node_filter(n.Node())
+      self.AddNode(graph, n)
+    for node in critical_path:
+      self._url_to_bag[node.Url()].MarkCritical()
+
+  def AddNode(self, graph, node):
+    """Add a node to our collection.
+
+    Args:
+      graph: (ResourceGraph) the graph in which the node lives.
+      node: (NodeInfo) the node to add.
+
+    Returns:
+      The Bag containing the node.
+    """
+    if not graph._node_filter(node):
+      return
+    if node.Url() not in self._url_to_bag:
+      new_index = len(self._bags)
+      self._bags.append(Bag(self, new_index, node.Url()))
+      self._url_to_bag[node.Url()] = self._bags[-1]
+    self._url_to_bag[node.Url()].AddNode(graph, node)
+    return self._url_to_bag[node.Url()]
+
+  @property
+  def graph_info(self):
+    return self._graph_info
+
+  @property
+  def bags(self):
+    return self._bags
+
+class Bag(dag.Node):
+  def __init__(self, sack, index, url):
+    super(Bag, self).__init__(index)
+    self._sack = sack
+    self._url = url
+    self._label = self._MakeShortname(url)
+    # Maps a ResourceGraph to its Nodes contained in this Bag.
+    self._graphs = defaultdict(set)
+    # Maps each successor bag to the set of (graph, node, graph-successor)
+    # tuples that generated it.
+    self._successor_sources = defaultdict(set)
+    # Maps each successor bag to a set of edge costs. This is just used to
+    # track min and max; if we want more statistics we'd have to count the
+    # costs with multiplicity.
+    self._successor_edge_costs = defaultdict(set)
+
+    # Miscellaneous counts and costs used in display.
+    self._total_costs = []
+    self._relative_costs = []
+    self._num_critical = 0
+
+  @property
+  def url(self):
+    return self._url
+
+  @property
+  def label(self):
+    return self._label
+
+  @property
+  def graphs(self):
+    return self._graphs
+
+  @property
+  def successor_sources(self):
+    return self._successor_sources
+
+  @property
+  def successor_edge_costs(self):
+    return self._successor_edge_costs
+
+  @property
+  def total_costs(self):
+    return self._total_costs
+
+  @property
+  def relative_costs(self):
+    return self._relative_costs
+
+  @property
+  def num_critical(self):
+    return self._num_critical
+
+  @property
+  def num_nodes(self):
+    return len(self._total_costs)
+
+  def MarkCritical(self):
+    self._num_critical += 1
+
+  def AddNode(self, graph, node):
+    if node in self._graphs[graph]:
+      return  # Already added.
+    graph_info = self._sack.graph_info[graph]
+    self._graphs[graph].add(node)
+    node_total_cost = graph_info.total_costs[node.Index()]
+    self._total_costs.append(node_total_cost)
+    self._relative_costs.append(
+        float(node_total_cost) / graph_info.cost)
+    for s in node.Node().Successors():
+      if not graph._node_filter(s):
+        continue
+      node_info = graph.NodeInfo(s)
+      successor_bag = self._sack.AddNode(graph, node_info)
+      self.AddSuccessor(successor_bag)
+      self._successor_sources[successor_bag].add((graph, node, s))
+      self._successor_edge_costs[successor_bag].add(graph.EdgeCost(node, s))
+
+  @classmethod
+  def _MakeShortname(cls, url):
+    parsed = urlparse.urlparse(url)
+    if parsed.scheme == 'data':
+      kind, _ = parsed.path.split(';', 1)
+      return 'data:' + kind
+    path = parsed.path[:10]
+    hostname = parsed.hostname if parsed.hostname else '?.?.?'
+    return hostname + '/' + path
diff --git a/loading/resource_sack_unittest.py b/loading/resource_sack_unittest.py
new file mode 100644
index 0000000..417918d
--- /dev/null
+++ b/loading/resource_sack_unittest.py
@@ -0,0 +1,64 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import resource_sack
+from test_utils import (MakeRequest,
+                        TestResourceGraph)
+
+
+class ResourceSackTestCase(unittest.TestCase):
+  def test_NodeMerge(self):
+    g1 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(3, 1)])
+    g2 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(1, 0),
+        MakeRequest(2, 0),
+        MakeRequest(4, 2)])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    sack.ConsumeGraph(g2)
+    self.assertEqual(5, len(sack.bags))
+    for bag in sack.bags:
+      if bag.label not in ('3/', '4/'):
+        self.assertEqual(2, bag.num_nodes)
+      else:
+        self.assertEqual(1, bag.num_nodes)
+
+  def test_MultiParents(self):
+    g1 = TestResourceGraph.FromRequestList([
+        MakeRequest(0, 'null'),
+        MakeRequest(2, 0)])
+    g2 = TestResourceGraph.FromRequestList([
+        MakeRequest(1, 'null'),
+        MakeRequest(2, 1)])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    sack.ConsumeGraph(g2)
+    self.assertEqual(3, len(sack.bags))
+    labels = {bag.label: bag for bag in sack.bags}
+    self.assertEqual(
+        set(['0/', '1/']),
+        set([bag.label for bag in labels['2/'].Predecessors()]))
+    self.assertFalse(labels['0/'].Predecessors())
+    self.assertFalse(labels['1/'].Predecessors())
+
+  def test_Shortname(self):
+    root = MakeRequest(0, 'null')
+    shortname = MakeRequest(1, 0)
+    shortname.url = 'data:fake/content;' + 'lotsand' * 50 + 'lotsofdata'
+    g1 = TestResourceGraph.FromRequestList([root, shortname])
+    sack = resource_sack.GraphSack()
+    sack.ConsumeGraph(g1)
+    self.assertEqual(set(['0/', 'data:fake/content']),
+                     set([bag.label for bag in sack.bags]))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index aea73a7..2d80d1a 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -43,8 +43,39 @@ class FakePageTrack(devtools_monitor.Track):
 
 
 def MakeRequest(
-    url, source_url, start_time, headers_time, end_time,
+    url, source_url, start_time=None, headers_time=None, end_time=None,
     magic_content_type=False, initiator_type='other'):
+  """Make a dependent request.
+
+  Args:
+    url: a url, or number which will be used as a url.
+    source_url: a url or number which will be used as the source (initiating)
+      url. If the source url is not present, then url will be a root. The
+      convention in tests is to use a source_url of 'null' in this case.
+    start_time: The request start time in milliseconds. If None, this is set to
+      the current request id in seconds. If None, the two other time parameters
+      below must also be None.
+    headers_time: The timestamp when resource headers were received, or None.
+    end_time: The timestamp when the resource was finished, or None.
+    magic_content_type (bool): if true, set a magic content type that makes url
+      always be detected as a valid source and destination request.
+    initiator_type: the initiator type to use.
+
+  Returns:
+    A request_track.Request.
+
+  """
+  assert ((start_time is None and
+           headers_time is None and
+           end_time is None) or
+          (start_time is not None and
+           headers_time is not None and
+           end_time is not None)), \
+      'Need no time specified or all times specified'
+  if start_time is None:
+    # Use the request id in seconds for timestamps. This guarantees increasing
+    # times which makes request dependencies behave as expected.
+    start_time = headers_time = end_time = MakeRequest._next_request_id * 1000
   assert initiator_type in ('other', 'parser')
   timing = request_track.TimingAsList(request_track.TimingFromDict({
       # connectEnd should be ignored.

commit d8911ad8ff299f4dfb8a8865d4f5240ed13f3d1c
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 22 08:04:58 2016 -0800

    tools/android/loading: Implements loading_trace_analyzer.py requests
    
    Command line tool design to quickly analyse a loading trace to
    debug the upcoming cache modification tool.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1708253002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376737}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6ba6f666b3c485e56d4e005d59734c7ab5a64ff2

diff --git a/loading/loading_trace_analyzer.py b/loading/loading_trace_analyzer.py
new file mode 100755
index 0000000..e703360
--- /dev/null
+++ b/loading/loading_trace_analyzer.py
@@ -0,0 +1,83 @@
+#! /usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import json
+import re
+import sys
+
+from loading_trace import LoadingTrace
+import request_track
+
+
+def _ArgumentParser():
+  """Builds a command line argument's parser.
+  """
+  parser = argparse.ArgumentParser()
+  subparsers = parser.add_subparsers(dest='subcommand', help='subcommand line')
+
+  # requests listing subcommand.
+  requests_parser = subparsers.add_parser('requests',
+      help='Lists all request from the loading trace.')
+  requests_parser.add_argument('loading_trace', type=file,
+      help='Input loading trace to see the cache usage from.')
+  requests_parser.add_argument('--output',
+      type=argparse.FileType(),
+      default=sys.stdout,
+      help='Output destination path if different from stdout.')
+  requests_parser.add_argument('--output-format', type=str, default='{url}',
+      help='Output line format (Default to "{url}")')
+  requests_parser.add_argument('--where',
+      dest='where_statement', type=str,
+      nargs=2, metavar=('FORMAT', 'REGEX'), default=[],
+      help='Where statement to filter such as: --where "{protocol}" "https?"')
+  return parser
+
+
+def _RequestsSubcommand(args):
+  """`loading_trace_analyzer.py requests` Command line tool entry point.
+
+  Example:
+    Lists all request with timing:
+      ... requests --output-format "{timing} {url}"
+
+    Lists  HTTP/HTTPS requests that have used the cache:
+      ... requests --where "{protocol} {from_disk_cache}" "https?\S* True"
+  """
+  where_format = None
+  where_statement = None
+  if args.where_statement:
+    where_format = args.where_statement[0]
+    try:
+      where_statement = re.compile(args.where_statement[1])
+    except re.error as e:
+      sys.stderr.write("Invalid where statement REGEX: {}\n{}\n".format(
+          args.where_statement[1], str(e)))
+      return 1
+
+  loading_trace = LoadingTrace.FromJsonDict(json.load(args.loading_trace))
+  for request_event in loading_trace.request_track.GetEvents():
+    request_event_json = request_event.ToJsonDict()
+
+    if where_statement != None:
+      where_in = where_format.format(**request_event_json)
+      if not where_statement.match(where_in):
+        continue
+
+    args.output.write(args.output_format.format(**request_event_json) + '\n')
+  return 0
+
+
+def main(command_line_args):
+  """Command line tool entry point.
+  """
+  args = _ArgumentParser().parse_args(command_line_args)
+  if args.subcommand == 'requests':
+    return _RequestsSubcommand(args)
+  assert False
+
+
+if __name__ == '__main__':
+  sys.exit(main(sys.argv[1:]))

commit 069855b8e84e9c475934ebb855c3e02bcd1ea6fa
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 22 07:36:22 2016 -0800

    sandwich: Implements network condition on WPR server and browser.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1707363002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376731}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 45605a80083f18230d5359890f6b8735310b25af

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
index a1f6849..9bf2908 100644
--- a/loading/chrome_setup.py
+++ b/loading/chrome_setup.py
@@ -17,9 +17,10 @@ from options import OPTIONS
 
 # Copied from
 # WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
-_NETWORK_CONDITIONS = {
-    'Offline': {
-        'download': 0 * 1024 / 8, 'upload': 0 * 1024 / 8, 'latency': 0},
+# Units:
+#   download/upload: byte/s
+#   latency: ms
+NETWORK_CONDITIONS = {
     'GPRS': {
         'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
     'Regular 2G': {
@@ -43,6 +44,23 @@ _NETWORK_CONDITIONS = {
 }
 
 
+def BandwidthToString(bandwidth):
+  """Converts a bandwidth to string.
+
+  Args:
+    bandwidth: The bandwidth to convert in byte/s. Must be a multiple of 1024/8.
+
+  Returns:
+    A string compatible with wpr --{up,down} command line flags.
+  """
+  assert type(bandwidth) == int
+  assert bandwidth % (1024/8) == 0
+  bandwidth_kbps = (bandwidth * 8) / 1024
+  if bandwidth_kbps % 1024:
+    return '{}Kbit/s'.format(bandwidth_kbps)
+  return '{}Mbit/s'.format(bandwidth_kbps / 1024)
+
+
 @contextlib.contextmanager
 def DevToolsConnectionForLocalBinary(flags):
   """Returns a DevToolsConnection context manager for a local binary.
@@ -80,7 +98,7 @@ def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
     connection: (DevToolsConnection)
     emulated_device_name: (str) Key in the dict returned by
                           _LoadEmulatedDevices().
-    emulated_network_name: (str) Key in _NETWORK_CONDITIONS.
+    emulated_network_name: (str) Key in NETWORK_CONDITIONS.
 
   Returns:
     A metadata dict {'deviceEmulation': params, 'networkEmulation': params}.
@@ -93,7 +111,7 @@ def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
         connection, emulated_device)
     result['deviceEmulation'] = emulation_params
   if emulated_network_name:
-    params = _NETWORK_CONDITIONS[emulated_network_name]
+    params = NETWORK_CONDITIONS[emulated_network_name]
     _SetUpNetworkEmulation(
         connection, params['latency'], params['download'], params['upload'])
     result['networkEmulation'] = params
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 0fe95fd..62b99b4 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -105,12 +105,15 @@ def _SetUpDevice(device, package_info):
 
 @contextlib.contextmanager
 def WprHost(device, wpr_archive_path, record=False,
+            network_condition_name=None,
             disable_script_injection=False):
   """Launches web page replay host.
 
   Args:
     device: Android device.
     wpr_archive_path: host sided WPR archive's path.
+    network_condition_name: Network condition name available in
+        chrome_setup.NETWORK_CONDITIONS.
     record: Enables or disables WPR archive recording.
 
   Returns:
@@ -119,6 +122,9 @@ def WprHost(device, wpr_archive_path, record=False,
   """
   assert device
   if wpr_archive_path == None:
+    assert not record, 'WPR cannot record without a specified archive.'
+    assert not network_condition_name, ('WPR cannot emulate network condition' +
+                                        ' without a specified archive.')
     yield []
     return
 
@@ -129,6 +135,16 @@ def WprHost(device, wpr_archive_path, record=False,
       os.remove(wpr_archive_path)
   else:
     assert os.path.exists(wpr_archive_path)
+  if network_condition_name:
+    condition = chrome_setup.NETWORK_CONDITIONS[network_condition_name]
+    if record:
+      logging.warning('WPR network condition is ignored when recording.')
+    else:
+      wpr_server_args.extend([
+          '--down', chrome_setup.BandwidthToString(condition['download']),
+          '--up', chrome_setup.BandwidthToString(condition['upload']),
+          '--delay_ms', str(condition['latency']),
+          '--shaping_type', 'proxy'])
 
   if disable_script_injection:
     # Remove default WPR injected scripts like deterministic.js which
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 68c785f..3ea8eea 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -20,6 +20,9 @@ from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
 
+DEFAULT_TIMEOUT = 10 # seconds
+
+
 class DevToolsConnectionException(Exception):
   def __init__(self, message):
     super(DevToolsConnectionException, self).__init__(message)
@@ -217,20 +220,21 @@ class DevToolsConnection(object):
     self._tearing_down_tracing = False
     self._set_up = True
 
-  def StartMonitoring(self):
+  def StartMonitoring(self, timeout=DEFAULT_TIMEOUT):
     """Starts monitoring.
 
     DevToolsConnection.SetUpMonitoring() has to be called first.
     """
     assert self._set_up, 'DevToolsConnection.SetUpMonitoring not called.'
-    self._Dispatch()
+    self._Dispatch(timeout=timeout)
     self._TearDownMonitoring()
 
   def StopMonitoring(self):
     """Stops the monitoring."""
     self._please_stop = True
 
-  def _Dispatch(self, kind='Monitoring', timeout=10):
+  def _Dispatch(self, kind='Monitoring',
+                timeout=DEFAULT_TIMEOUT):
     self._please_stop = False
     while not self._please_stop:
       try:
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index def4214..0572d5b 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -33,6 +33,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 import devil_chromium
 
+import chrome_setup
 import device_setup
 import devtools_monitor
 import options
@@ -65,6 +66,10 @@ _TIME_TO_DEVICE_IDLE_SECONDS = 2
 def _RemoteCacheDirectory():
   return '/data/data/{}/cache/Cache'.format(OPTIONS.chrome_package_name)
 
+# Devtools timeout of 1 minute to avoid websocket timeout on slow
+# network condition.
+_DEVTOOLS_TIMEOUT = 60
+
 
 def _ReadUrlsFromJobDescription(job_name):
   """Retrieves the list of URLs associated with the job name."""
@@ -258,14 +263,9 @@ def _CleanPreviousTraces(output_directories_path):
     shutil.rmtree(directory_path)
 
 
-def main():
-  logging.basicConfig(level=logging.INFO)
-  devil_chromium.Initialize()
-
-  # Don't give the argument yet. All we are interested in for now is accessing
-  # the default values of OPTIONS.
-  OPTIONS.ParseArgs([])
-
+def _ArgumentParser():
+  """Build a command line argument's parser.
+  """
   parser = argparse.ArgumentParser()
   parser.add_argument('--job', required=True,
                       help='JSON file with job description.')
@@ -287,7 +287,25 @@ def main():
                       help='Disable WPR default script injection such as ' +
                           'overriding javascript\'s Math.random() and Date() ' +
                           'with deterministic implementations.')
-  args = parser.parse_args()
+  parser.add_argument('--network-condition', default=None,
+      choices=sorted(chrome_setup.NETWORK_CONDITIONS.keys()),
+      help='Set a network profile.')
+  parser.add_argument('--network-emulator', default='browser',
+      choices=['browser', 'wpr'],
+      help='Set which component is emulating the network condition.' +
+          ' (Default to browser)')
+  return parser
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+  devil_chromium.Initialize()
+
+  # Don't give the argument yet. All we are interested in for now is accessing
+  # the default values of OPTIONS.
+  OPTIONS.ParseArgs([])
+
+  args = _ArgumentParser().parse_args()
 
   if not os.path.isdir(args.output):
     try:
@@ -307,22 +325,41 @@ def main():
   device = device_utils.DeviceUtils.HealthyDevices()[0]
   local_cache_archive_path = os.path.join(args.output, 'cache.zip')
   local_cache_directory_path = None
+  wpr_network_condition_name = None
+  browser_network_condition_name = None
+  if args.network_emulator == 'wpr':
+    wpr_network_condition_name = args.network_condition
+  elif args.network_emulator == 'browser':
+    browser_network_condition_name = args.network_condition
+  else:
+    assert False
 
   if args.cache_op == 'push':
     assert os.path.isfile(local_cache_archive_path)
     local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
     _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
 
-  with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
-      args.disable_wpr_script_injection) as additional_flags:
+  with device_setup.WprHost(device, args.wpr_archive,
+      record=args.wpr_record,
+      network_condition_name=wpr_network_condition_name,
+      disable_script_injection=args.disable_wpr_script_injection
+      ) as additional_flags:
     def _RunNavigation(url, clear_cache, trace_id):
       with device_setup.DeviceConnection(
           device=device,
           additional_flags=additional_flags) as connection:
+        additional_metadata = {}
+        if browser_network_condition_name:
+          additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
+              connection=connection,
+              emulated_device_name=None,
+              emulated_network_name=browser_network_condition_name)
         loading_trace = trace_recorder.MonitorUrl(
             connection, url,
             clear_cache=clear_cache,
-            categories=pull_sandwich_metrics.CATEGORIES)
+            categories=pull_sandwich_metrics.CATEGORIES,
+            timeout=_DEVTOOLS_TIMEOUT)
+        loading_trace.metadata.update(additional_metadata)
         if trace_id != None:
           loading_trace_path = os.path.join(
               args.output, str(trace_id), 'trace.json')
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 0da057f..de59b35 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -31,14 +31,16 @@ import tracing
 
 
 def MonitorUrl(connection, url, clear_cache=False,
-               categories=tracing.DEFAULT_CATEGORIES):
+               categories=tracing.DEFAULT_CATEGORIES,
+               timeout=devtools_monitor.DEFAULT_TIMEOUT):
   """Monitor a URL via a trace recorder.
 
   Args:
-    connection: A device_monitor.DevToolsConnection instance.
+    connection: A devtools_monitor.DevToolsConnection instance.
     url: url to navigate to as string.
     clear_cache: boolean indicating if cache should be cleared before loading.
     categories: List of tracing event categories to record.
+    timeout: Websocket timeout.
 
   Returns:
     loading_trace.LoadingTrace.
@@ -50,7 +52,7 @@ def MonitorUrl(connection, url, clear_cache=False,
   if clear_cache:
     connection.ClearCache()
   connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-  connection.StartMonitoring()
+  connection.StartMonitoring(timeout=timeout)
   metadata = {'date': datetime.datetime.utcnow().isoformat(),
               'seconds_since_epoch': time.time()}
   return loading_trace.LoadingTrace(url, metadata, page, request, trace)

commit 45d313bd001d00996e20c1b688590a143f7240de
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 22 07:10:49 2016 -0800

    sandwich: Refactor to use more existing code.
    
    Also fixes a failures caused by 23bd22480abd2209f258d1dd8d3a8572ee17954a:
    "Refactor options in analyze.py to global structure"
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1707793002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376728}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 51f679843972a6f6592d871fea887b6435b7fe75

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
index 720fa1d..d383857 100644
--- a/loading/loading_trace.py
+++ b/loading/loading_trace.py
@@ -4,10 +4,12 @@
 
 """Represents the trace of a page load."""
 
+import json
 import page_track
 import request_track
 import tracing
 
+
 class LoadingTrace(object):
   """Represents the trace of a page load."""
   _URL_KEY = 'url'
@@ -40,6 +42,12 @@ class LoadingTrace(object):
               self._TRACING_KEY: self.tracing_track.ToJsonDict()}
     return result
 
+  def ToJsonFile(self, json_path):
+    """Save a json file representing this instance."""
+    json_dict = self.ToJsonDict()
+    with open(json_path, 'w') as output_file:
+       json.dump(json_dict, output_file, indent=2)
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     """Returns an instance from a dictionary returned by ToJsonDict()."""
@@ -53,3 +61,9 @@ class LoadingTrace(object):
         json_dict[cls._TRACING_KEY])
     return LoadingTrace(json_dict[cls._URL_KEY], json_dict[cls._METADATA_KEY],
                         page, request, tracing_track)
+
+  @classmethod
+  def FromJsonFile(cls, json_path):
+    """Returns an instance from a json file saved by ToJsonFile()."""
+    with open(json_path) as input_file:
+      return cls.FromJsonDict(json.load(input_file))
diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
index df24f1c..926a579 100755
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -15,6 +15,9 @@ import logging
 import os
 import sys
 
+import loading_trace as loading_trace_module
+import tracing
+
 
 CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
 
@@ -29,41 +32,41 @@ _CSV_FIELD_NAMES = [
 _TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
 
 
-def _GetBrowserPID(trace):
+def _GetBrowserPID(tracing_track):
   """Get the browser PID from a trace.
 
   Args:
-    trace: The cached trace.
+    tracing_track: The tracing.TracingTrack.
 
   Returns:
     The browser's PID as an integer.
   """
-  for event in trace['traceEvents']:
-    if event['cat'] != '__metadata' or event['name'] != 'process_name':
+  for event in tracing_track.GetEvents():
+    if event.category != '__metadata' or event.name != 'process_name':
       continue
-    if event['args']['name'] == 'Browser':
-      return event['pid']
+    if event.args['name'] == 'Browser':
+      return event.pid
   raise ValueError('couldn\'t find browser\'s PID')
 
 
-def _GetBrowserDumpEvents(trace):
-  """Get the browser memory dump events from a trace.
+def _GetBrowserDumpEvents(tracing_track):
+  """Get the browser memory dump events from a tracing track.
 
   Args:
-    trace: The cached trace.
+    tracing_track: The tracing.TracingTrack.
 
   Returns:
     List of memory dump events.
   """
-  browser_pid = _GetBrowserPID(trace)
+  browser_pid = _GetBrowserPID(tracing_track)
   browser_dumps_events = []
-  for event in trace['traceEvents']:
-    if event['cat'] != 'disabled-by-default-memory-infra':
+  for event in tracing_track.GetEvents():
+    if event.category != 'disabled-by-default-memory-infra':
       continue
-    if event['ph'] != 'v' or event['name'] != 'periodic_interval':
+    if event.type != 'v' or event.name != 'periodic_interval':
       continue
     # Ignore dump events for processes other than the browser process
-    if event['pid'] != browser_pid:
+    if event.pid != browser_pid:
       continue
     browser_dumps_events.append(event)
   if len(browser_dumps_events) == 0:
@@ -71,31 +74,31 @@ def _GetBrowserDumpEvents(trace):
   return browser_dumps_events
 
 
-def _GetWebPageTrackedEvents(trace):
-  """Get the web page's tracked events from a trace.
+def _GetWebPageTrackedEvents(tracing_track):
+  """Get the web page's tracked events from a tracing track.
 
   Args:
-    trace: The cached trace.
+    tracing_track: The tracing.TracingTrack.
 
   Returns:
     Dictionary all tracked events.
   """
   main_frame = None
   tracked_events = {}
-  for event in trace['traceEvents']:
-    if event['cat'] != 'blink.user_timing':
+  for event in tracing_track.GetEvents():
+    if event.category != 'blink.user_timing':
       continue
-    event_name = event['name']
+    event_name = event.name
     # Ignore events until about:blank's unloadEventEnd that give the main
     # frame id.
     if not main_frame:
       if event_name == 'unloadEventEnd':
-        main_frame = event['args']['frame']
+        main_frame = event.args['frame']
         logging.info('found about:blank\'s event \'unloadEventEnd\'')
       continue
     # Ignore sub-frames events. requestStart don't have the frame set but it
     # is fine since tracking the first one after about:blank's unloadEventEnd.
-    if 'frame' in event['args'] and event['args']['frame'] != main_frame:
+    if 'frame' in event.args and event.args['frame'] != main_frame:
       continue
     if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
       logging.info('found url\'s event \'%s\'' % event_name)
@@ -104,32 +107,33 @@ def _GetWebPageTrackedEvents(trace):
   return tracked_events
 
 
-def _PullMetricsFromTrace(trace):
+def _PullMetricsFromLoadingTrace(loading_trace):
   """Pulls all the metrics from a given trace.
 
   Args:
-    trace: The cached trace.
+    loading_trace: loading_trace_module.LoadingTrace.
 
   Returns:
     Dictionary with all _CSV_FIELD_NAMES's field set (except the 'id').
   """
-  browser_dump_events = _GetBrowserDumpEvents(trace)
-  web_page_tracked_events = _GetWebPageTrackedEvents(trace)
+  browser_dump_events = _GetBrowserDumpEvents(loading_trace.tracing_track)
+  web_page_tracked_events = _GetWebPageTrackedEvents(
+      loading_trace.tracing_track)
 
   browser_malloc_sum = 0
   browser_malloc_max = 0
   for dump_event in browser_dump_events:
-    attr = dump_event['args']['dumps']['allocators']['malloc']['attrs']['size']
+    attr = dump_event.args['dumps']['allocators']['malloc']['attrs']['size']
     assert attr['units'] == 'bytes'
     size = int(attr['value'], 16)
     browser_malloc_sum += size
     browser_malloc_max = max(browser_malloc_max, size)
 
   return {
-    'total_load': (web_page_tracked_events['loadEventEnd']['ts'] -
-                   web_page_tracked_events['requestStart']['ts']),
-    'onload': (web_page_tracked_events['loadEventEnd']['ts'] -
-               web_page_tracked_events['loadEventStart']['ts']),
+    'total_load': (web_page_tracked_events['loadEventEnd'].start_msec -
+                   web_page_tracked_events['requestStart'].start_msec),
+    'onload': (web_page_tracked_events['loadEventEnd'].start_msec -
+               web_page_tracked_events['loadEventStart'].start_msec),
     'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
     'browser_malloc_max': browser_malloc_max
   }
@@ -162,12 +166,11 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
     if not os.path.isfile(trace_path):
       continue
     logging.info('processing \'%s\'' % trace_path)
-    with open(trace_path) as trace_file:
-      trace = json.load(trace_file)
-      trace_metrics = _PullMetricsFromTrace(trace)
-      trace_metrics['id'] = page_id
-      trace_metrics['url'] = run_infos['urls'][page_id]
-      metrics.append(trace_metrics)
+    loading_trace = loading_trace_module.LoadingTrace.FromJsonFile(trace_path)
+    trace_metrics = _PullMetricsFromLoadingTrace(loading_trace)
+    trace_metrics['id'] = page_id
+    trace_metrics['url'] = run_infos['urls'][page_id]
+    metrics.append(trace_metrics)
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
   return metrics
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
index 3c98508..518607c 100644
--- a/loading/pull_sandwich_metrics_unittest.py
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -10,7 +10,12 @@ import subprocess
 import tempfile
 import unittest
 
+import loading_trace
+import page_track
 import pull_sandwich_metrics as puller
+import request_track
+import tracing
+
 
 _BLINK_CAT = 'blink.user_timing'
 _MEM_CAT = 'disabled-by-default-memory-infra'
@@ -19,40 +24,56 @@ _LOADS='loadEventStart'
 _LOADE='loadEventEnd'
 _UNLOAD='unloadEventEnd'
 
-_MINIMALIST_TRACE = {'traceEvents': [
-    {'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10, 'args': {'frame': '0'}},
-    {'cat': _BLINK_CAT, 'name': _START,  'ts': 20, 'args': {},           },
+_MINIMALIST_TRACE_EVENTS = [
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10000,
+        'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _START,  'ts': 20000,
+        'args': {}},
     {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
-        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
             'units': 'bytes', 'value': '1af2', }}}}}}},
-    {'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35, 'args': {'frame': '0'}},
-    {'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40, 'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35000,
+        'args': {'frame': '0'}},
+    {'ph': 'R', 'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40000,
+        'args': {'frame': '0'}},
     {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
-        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+        'ts': 1, 'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
             'units': 'bytes', 'value': 'd704', }}}}}}},
-    {'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'args': {
-        'name': 'Browser'}}]}
+    {'ph': 'M', 'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'ts': 1,
+        'args': {'name': 'Browser'}}]
+
+
+def TracingTrack(events):
+  return tracing.TracingTrack.FromJsonDict({'events': events})
+
+
+def LoadingTrace(events):
+  return loading_trace.LoadingTrace('http://a.com/', {},
+                                    page_track.PageTrack(None),
+                                    request_track.RequestTrack(None),
+                                    TracingTrack(events))
 
 
 class PageTrackTest(unittest.TestCase):
   def testGetBrowserPID(self):
-    def RunHelper(expected, trace):
-      self.assertEquals(expected, puller._GetBrowserPID(trace))
-
-    RunHelper(123, {'traceEvents': [
-        {'pid': 354, 'cat': 'whatever0'},
-        {'pid': 354, 'cat': 'whatever1'},
-        {'pid': 354, 'cat': '__metadata', 'name': 'thread_name'},
-        {'pid': 354, 'cat': '__metadata', 'name': 'process_name', 'args': {
-            'name': 'Renderer'}},
-        {'pid': 123, 'cat': '__metadata', 'name': 'process_name', 'args': {
-            'name': 'Browser'}},
-        {'pid': 354, 'cat': 'whatever0'}]})
+    def RunHelper(expected, events):
+      self.assertEquals(expected, puller._GetBrowserPID(TracingTrack(events)))
+
+    RunHelper(123, [
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
+            'name': 'thread_name'},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': '__metadata',
+            'name': 'process_name', 'args': {'name': 'Renderer'}},
+        {'ph': 'M', 'ts': 0, 'pid': 123, 'cat': '__metadata',
+            'name': 'process_name', 'args': {'name': 'Browser'}},
+        {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'}])
 
     with self.assertRaises(ValueError):
-      RunHelper(123, {'traceEvents': [
-          {'pid': 354, 'cat': 'whatever0'},
-          {'pid': 354, 'cat': 'whatever1'}]})
+      RunHelper(123, [
+          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever0'},
+          {'ph': 'M', 'ts': 0, 'pid': 354, 'cat': 'whatever1'}])
 
   def testGetBrowserDumpEvents(self):
     NAME = 'periodic_interval'
@@ -63,39 +84,41 @@ class PageTrackTest(unittest.TestCase):
           'pid': browser_pid,
           'cat': '__metadata',
           'name': 'process_name',
+          'ph': 'M',
+          'ts': 0,
           'args': {'name': 'Browser'}})
-      return puller._GetBrowserDumpEvents({'traceEvents': trace_events})
+      return puller._GetBrowserDumpEvents(TracingTrack(trace_events))
 
     TRACE_EVENTS = [
-        {'pid': 354, 'ts':  1, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts':  2, 'cat': _MEM_CAT, 'ph': 'V'},
-        {'pid': 672, 'ts':  3, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts':  4, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
-        {'pid': 123, 'ts':  5, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts':  6, 'cat': _MEM_CAT, 'ph': 'V'},
-        {'pid': 672, 'ts':  7, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts':  8, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
-        {'pid': 123, 'ts':  9, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
-        {'pid': 123, 'ts': 10, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
-        {'pid': 354, 'ts': 11, 'cat': 'whatever0'},
-        {'pid': 672, 'ts': 12, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
+        {'pid': 354, 'ts':  1000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  2000, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  3000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  4000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  5000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  6000, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  7000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  8000, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  9000, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts': 10000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts': 11000, 'cat': 'whatever0', 'ph': 'R'},
+        {'pid': 672, 'ts': 12000, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
 
     self.assertTrue(_MEM_CAT in puller.CATEGORIES)
 
     bump_events = RunHelper(TRACE_EVENTS, 123)
     self.assertEquals(2, len(bump_events))
-    self.assertEquals(5, bump_events[0]['ts'])
-    self.assertEquals(10, bump_events[1]['ts'])
+    self.assertEquals(5, bump_events[0].start_msec)
+    self.assertEquals(10, bump_events[1].start_msec)
 
     bump_events = RunHelper(TRACE_EVENTS, 354)
     self.assertEquals(1, len(bump_events))
-    self.assertEquals(1, bump_events[0]['ts'])
+    self.assertEquals(1, bump_events[0].start_msec)
 
     bump_events = RunHelper(TRACE_EVENTS, 672)
     self.assertEquals(3, len(bump_events))
-    self.assertEquals(3, bump_events[0]['ts'])
-    self.assertEquals(7, bump_events[1]['ts'])
-    self.assertEquals(12, bump_events[2]['ts'])
+    self.assertEquals(3, bump_events[0].start_msec)
+    self.assertEquals(7, bump_events[1].start_msec)
+    self.assertEquals(12, bump_events[2].start_msec)
 
     with self.assertRaises(ValueError):
       RunHelper(TRACE_EVENTS, 895)
@@ -103,41 +126,68 @@ class PageTrackTest(unittest.TestCase):
   def testGetWebPageTrackedEvents(self):
     self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
 
-    trace_events = puller._GetWebPageTrackedEvents({'traceEvents': [
-        {'ts':  0, 'args': {},             'cat': 'whatever', 'name': _START},
-        {'ts':  1, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
-        {'ts':  2, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
-        {'ts':  3, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts':  4, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts':  5, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts':  6, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _UNLOAD},
-        {'ts':  7, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts':  8, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts':  9, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts': 10, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _UNLOAD},
-        {'ts': 11, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _START},
-        {'ts': 12, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
-        {'ts': 13, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
-        {'ts': 14, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts': 15, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts': 16, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts': 17, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts': 18, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts': 19, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
-        {'ts': 20, 'args': {},             'cat': 'whatever', 'name': _START},
-        {'ts': 21, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
-        {'ts': 22, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
-        {'ts': 23, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
-        {'ts': 24, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
-        {'ts': 25, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE}]})
+    trace_events = puller._GetWebPageTrackedEvents(TracingTrack([
+        {'ph': 'R', 'ts':  0000, 'args': {},             'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts':  1000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  2000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts':  3000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts':  4000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  5000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts':  6000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _UNLOAD},
+        {'ph': 'R', 'ts':  7000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts':  8000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts':  9000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 10000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _UNLOAD},
+        {'ph': 'R', 'ts': 11000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts': 12000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 13000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 14000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 15000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 16000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 17000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 18000, 'args': {'frame': '1'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 19000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 20000, 'args': {},             'cat': 'whatever',
+            'name': _START},
+        {'ph': 'R', 'ts': 21000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 22000, 'args': {'frame': '0'}, 'cat': 'whatever',
+            'name': _LOADE},
+        {'ph': 'R', 'ts': 23000, 'args': {},             'cat': _BLINK_CAT,
+            'name': _START},
+        {'ph': 'R', 'ts': 24000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADS},
+        {'ph': 'R', 'ts': 25000, 'args': {'frame': '0'}, 'cat': _BLINK_CAT,
+            'name': _LOADE}]))
 
     self.assertEquals(3, len(trace_events))
-    self.assertEquals(14, trace_events['requestStart']['ts'])
-    self.assertEquals(17, trace_events['loadEventStart']['ts'])
-    self.assertEquals(19, trace_events['loadEventEnd']['ts'])
+    self.assertEquals(14, trace_events['requestStart'].start_msec)
+    self.assertEquals(17, trace_events['loadEventStart'].start_msec)
+    self.assertEquals(19, trace_events['loadEventEnd'].start_msec)
 
-  def testPullMetricsFromTrace(self):
-    metrics = puller._PullMetricsFromTrace(_MINIMALIST_TRACE)
+  def testPullMetricsFromLoadingTrace(self):
+    metrics = puller._PullMetricsFromLoadingTrace(LoadingTrace(
+        _MINIMALIST_TRACE_EVENTS))
     self.assertEquals(4, len(metrics))
     self.assertEquals(20, metrics['total_load'])
     self.assertEquals(5, metrics['onload'])
@@ -150,8 +200,8 @@ class PageTrackTest(unittest.TestCase):
       json.dump({'urls': ['a.com', 'b.com', 'c.org']}, out_file)
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
-      with open(os.path.join(tmp_dir, dirname, 'trace.json'), 'w') as out_file:
-        json.dump(_MINIMALIST_TRACE, out_file)
+      LoadingTrace(_MINIMALIST_TRACE_EVENTS).ToJsonFile(
+          os.path.join(tmp_dir, dirname, 'trace.json'))
 
     process = subprocess.Popen(['python', puller.__file__, tmp_dir])
     process.wait()
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index ef7fca8..def4214 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -35,12 +35,16 @@ import devil_chromium
 
 import device_setup
 import devtools_monitor
-import json
+import options
 import page_track
 import pull_sandwich_metrics
+import trace_recorder
 import tracing
 
 
+# Use options layer to access constants.
+OPTIONS = options.OPTIONS
+
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
 # Directory name under --output to save the cache from the device.
@@ -53,16 +57,13 @@ _INDEX_DIRECTORY_NAME = 'index-dir'
 # in the cache directory under _INDEX_DIRECTORY_NAME.
 _REAL_INDEX_FILE_NAME = 'the-real-index'
 
-# Name of the chrome package.
-_CHROME_PACKAGE = (
-    constants.PACKAGE_INFO[device_setup.DEFAULT_CHROME_PACKAGE].package)
-
 # An estimate of time to wait for the device to become idle after expensive
 # operations, such as opening the launcher activity.
 _TIME_TO_DEVICE_IDLE_SECONDS = 2
 
-# Cache directory's path on the device.
-_REMOTE_CACHE_DIRECTORY = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
+
+def _RemoteCacheDirectory():
+  return '/data/data/{}/cache/Cache'.format(OPTIONS.chrome_package_name)
 
 
 def _ReadUrlsFromJobDescription(job_name):
@@ -86,23 +87,6 @@ def _ReadUrlsFromJobDescription(job_name):
   raise Exception('Job description does not define a list named "urls"')
 
 
-def _SaveChromeTrace(events, target_directory):
-  """Saves the trace events, ignores IO errors.
-
-  Args:
-    events: a dict as returned by TracingTrack.ToJsonDict()
-    target_directory: Directory path where trace is created.
-  """
-  filename = os.path.join(target_directory, 'trace.json')
-  try:
-    os.makedirs(target_directory)
-    with open(filename, 'w') as f:
-      json.dump({'traceEvents': events['events'], 'metadata': {}}, f, indent=2)
-  except IOError:
-    logging.warning('Could not save a trace: %s' % filename)
-    # Swallow the exception.
-
-
 def _UpdateTimestampFromAdbStat(filename, stat):
   os.utime(filename, (stat.st_time, stat.st_time))
 
@@ -128,13 +112,13 @@ def _PullBrowserCache(device):
     Temporary directory containing all the browser cache.
   """
   save_target = tempfile.mkdtemp(suffix='.cache')
-  for filename, stat in device.adb.Ls(_REMOTE_CACHE_DIRECTORY):
+  for filename, stat in device.adb.Ls(_RemoteCacheDirectory()):
     if filename == '..':
       continue
     if filename == '.':
       cache_directory_stat = stat
       continue
-    original_file = os.path.join(_REMOTE_CACHE_DIRECTORY, filename)
+    original_file = os.path.join(_RemoteCacheDirectory(), filename)
     saved_file = os.path.join(save_target, filename)
     device.adb.Pull(original_file, saved_file)
     _UpdateTimestampFromAdbStat(saved_file, stat)
@@ -166,16 +150,16 @@ def _PushBrowserCache(device, local_cache_path):
     local_cache_path: The directory's path containing the cache locally.
   """
   # Clear previous cache.
-  _AdbShell(device.adb, ['rm', '-rf', _REMOTE_CACHE_DIRECTORY])
-  _AdbShell(device.adb, ['mkdir', _REMOTE_CACHE_DIRECTORY])
+  _AdbShell(device.adb, ['rm', '-rf', _RemoteCacheDirectory()])
+  _AdbShell(device.adb, ['mkdir', _RemoteCacheDirectory()])
 
   # Push cache content.
-  device.adb.Push(local_cache_path, _REMOTE_CACHE_DIRECTORY)
+  device.adb.Push(local_cache_path, _RemoteCacheDirectory())
 
   # Walk through the local cache to update mtime on the device.
   def MirrorMtime(local_path):
     cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
-    remote_path = os.path.join(_REMOTE_CACHE_DIRECTORY, cache_relative_path)
+    remote_path = os.path.join(_RemoteCacheDirectory(), cache_relative_path)
     _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
 
   for local_directory_path, dirnames, filenames in os.walk(
@@ -278,6 +262,10 @@ def main():
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
 
+  # Don't give the argument yet. All we are interested in for now is accessing
+  # the default values of OPTIONS.
+  OPTIONS.ParseArgs([])
+
   parser = argparse.ArgumentParser()
   parser.add_argument('--job', required=True,
                       help='JSON file with job description.')
@@ -331,17 +319,15 @@ def main():
       with device_setup.DeviceConnection(
           device=device,
           additional_flags=additional_flags) as connection:
-        if clear_cache:
-          connection.ClearCache()
-        page_track.PageTrack(connection)
-        tracing_track = tracing.TracingTrack(connection,
+        loading_trace = trace_recorder.MonitorUrl(
+            connection, url,
+            clear_cache=clear_cache,
             categories=pull_sandwich_metrics.CATEGORIES)
-        connection.SetUpMonitoring()
-        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-        connection.StartMonitoring()
         if trace_id != None:
-          trace_target_directory = os.path.join(args.output, str(trace_id))
-          _SaveChromeTrace(tracing_track.ToJsonDict(), trace_target_directory)
+          loading_trace_path = os.path.join(
+              args.output, str(trace_id), 'trace.json')
+          os.makedirs(os.path.dirname(loading_trace_path))
+          loading_trace.ToJsonFile(loading_trace_path)
 
     for _ in xrange(args.repeat):
       for url in job_urls:
@@ -349,7 +335,7 @@ def main():
         if args.cache_op == 'clear':
           clear_cache = True
         elif args.cache_op == 'push':
-          device.KillAll(_CHROME_PACKAGE, quiet=True)
+          device.KillAll(OPTIONS.chrome_package_name, quiet=True)
           _PushBrowserCache(device, local_cache_directory_path)
         elif args.cache_op == 'reload':
           _RunNavigation(url, clear_cache=True, trace_id=None)
@@ -366,7 +352,7 @@ def main():
     # Move Chrome to background to allow it to flush the index.
     device.adb.Shell('am start com.google.android.launcher')
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    device.KillAll(_CHROME_PACKAGE, quiet=True)
+    device.KillAll(OPTIONS.chrome_package_name, quiet=True)
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
 
     cache_directory_path = _PullBrowserCache(device)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 34293f8..0da057f 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -30,20 +30,22 @@ import request_track
 import tracing
 
 
-def MonitorUrl(connection, url, clear_cache=False):
+def MonitorUrl(connection, url, clear_cache=False,
+               categories=tracing.DEFAULT_CATEGORIES):
   """Monitor a URL via a trace recorder.
 
   Args:
     connection: A device_monitor.DevToolsConnection instance.
     url: url to navigate to as string.
     clear_cache: boolean indicating if cache should be cleared before loading.
+    categories: List of tracing event categories to record.
 
   Returns:
     loading_trace.LoadingTrace.
   """
   page = page_track.PageTrack(connection)
   request = request_track.RequestTrack(connection)
-  trace = tracing.TracingTrack(connection)
+  trace = tracing.TracingTrack(connection, categories=categories)
   connection.SetUpMonitoring()
   if clear_cache:
     connection.ClearCache()
diff --git a/loading/tracing.py b/loading/tracing.py
index cf79908..54ff3d0 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -12,12 +12,17 @@ import operator
 import devtools_monitor
 
 
+DEFAULT_CATEGORIES = None
+
+
 class TracingTrack(devtools_monitor.Track):
   """Grabs and processes trace event messages.
 
   See https://goo.gl/Qabkqk for details on the protocol.
   """
-  def __init__(self, connection, categories=None, fetch_stream=False):
+  def __init__(self, connection,
+               categories=DEFAULT_CATEGORIES,
+               fetch_stream=False):
     """Initialize this TracingTrack.
 
     Args:
@@ -304,6 +309,14 @@ class Event(object):
     return self._tracing_event['ph']
 
   @property
+  def category(self):
+    return self._tracing_event['cat']
+
+  @property
+  def pid(self):
+    return self._tracing_event['pid']
+
+  @property
   def args(self):
     return self._tracing_event.get('args', {})
 

commit 7fbfcd6ccf901a3267e0e82a2fa212984ffa633a
Author: droger <droger@chromium.org>
Date:   Mon Feb 22 01:06:14 2016 -0800

    tools/android/loading Add support for async stacks in webserver tests
    
    Review URL: https://codereview.chromium.org/1713063002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376705}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5133c8214f810865c65300ddc6ea2260895cdc2b

diff --git a/loading/trace_test/results/3.result b/loading/trace_test/results/3.result
index f909cae..bcebf63 100644
--- a/loading/trace_test/results/3.result
+++ b/loading/trace_test/results/3.result
@@ -1,6 +1,6 @@
 parser (no stack) 3a.js
 parser (no stack) 3c.js
-script () 3a.jpg
-script () 3b.jpg
-script () 3c.jpg
+script (3a.js:10/3a.js:14/3.html:23) 3a.jpg
 script (3a.js:20) 3b.js
+script (3b.js:9) 3b.jpg
+script (3c.js:7/3.html:24) 3c.jpg
diff --git a/loading/trace_test/tests/3.html b/loading/trace_test/tests/3.html
index 03bbce6..f6850eb 100644
--- a/loading/trace_test/tests/3.html
+++ b/loading/trace_test/tests/3.html
@@ -10,9 +10,6 @@
 
   Note that as 3b.js adds a tag to the body, it is executed only after
   the body has been parsed. No, I don't know how that works either.
-
-  At any rate, only 3c.js has a meaningful stack trace in the
-  initiator. The images have script initiators with empty stacks.
 -->
 <html>
 <head>
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index 2b413a8..800b8d6 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -148,14 +148,24 @@ class InitiatorSequence(object):
       return
     for rq in trace.request_track.GetEvents():
       if rq.initiator['type'] in ('parser', 'script'):
-        stack = 'no stack'
-        if 'stack' in rq.initiator:
-          stack = '/'.join(
+        stack_string = ''
+        stack = rq.initiator.get('stack')
+        # Iteratively walk the stack and its parents.
+        while stack:
+          current_string = '/'.join(
               ['%s:%s' % (self._ShortUrl(frame['url']), frame['lineNumber'])
-               for frame in rq.initiator['stack']['callFrames']])
+               for frame in stack['callFrames']])
+          if len(current_string) and len(stack_string):
+            stack_string += '/'
+          stack_string += current_string
+          stack = stack.get('parent')
+
+        if stack_string == '':
+          stack_string = 'no stack'
+
         self._seq.append('%s (%s) %s' % (
             rq.initiator['type'],
-            stack,
+            stack_string,
             self._ShortUrl(rq.url)))
     self._seq.sort()
 

commit f0f2e8c39bcb1064205d3d621282be7b4b34fbb5
Author: mattcary <mattcary@chromium.org>
Date:   Fri Feb 19 07:49:12 2016 -0800

    Loading model test tweak.
    
    Extract SimpleLens to test_utils and change how it's plugged into ResourceGraph.
    
    This will make it easier to create ResourceGraphs in other tests.
    
    Review URL: https://codereview.chromium.org/1708223005
    
    Cr-Original-Commit-Position: refs/heads/master@{#376442}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 82369343882999c0829645d6559d4f3180c4f29e

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 5deec03..f9905a0 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -31,6 +31,10 @@ class ResourceGraph(object):
 
   See model parameters in Set().
   """
+  # The lens to build request dependencies. Exposed here for subclasses in
+  # unittesting.
+  REQUEST_LENS = request_dependencies_lens.RequestDependencyLens
+
   EDGE_KIND_KEY = 'edge_kind'
   EDGE_KINDS = request_track.Request.INITIATORS + (
       'script_inferred', 'after-load', 'before-load', 'timing')
@@ -477,8 +481,7 @@ class ResourceGraph(object):
       self._nodes.append(node)
       self._node_info.append(node_info)
 
-    dependencies = request_dependencies_lens.RequestDependencyLens(
-        trace).GetRequestDependencies()
+    dependencies = self.REQUEST_LENS(trace).GetRequestDependencies()
     for dep in dependencies:
       (parent_rq, child_rq, reason) = dep
       parent = self._node_info[index_by_request[parent_rq]]
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 44dcbae..ee5e84d 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -13,36 +13,7 @@ import request_dependencies_lens
 import test_utils
 
 
-class SimpleLens(object):
-  def __init__(self, trace):
-    self._trace = trace
-
-  def GetRequestDependencies(self):
-    url_to_rq = {}
-    deps = []
-    for rq in self._trace.request_track.GetEvents():
-      assert rq.url not in url_to_rq
-      url_to_rq[rq.url] = rq
-    for rq in self._trace.request_track.GetEvents():
-      initiating_url = rq.initiator['url']
-      if initiating_url in url_to_rq:
-        deps.append((url_to_rq[initiating_url], rq, rq.initiator['type']))
-    return deps
-
-
 class LoadingModelTestCase(unittest.TestCase):
-
-  def setUp(self):
-    self.old_lens = request_dependencies_lens.RequestDependencyLens
-    request_dependencies_lens.RequestDependencyLens = SimpleLens
-
-  def tearDown(self):
-    request_dependencies_lens.RequestDependencyLens = self.old_lens
-
-  def MakeGraph(self, requests):
-    return loading_model.ResourceGraph(
-        test_utils.LoadingTraceFromEvents(requests))
-
   def SortedIndicies(self, graph):
     return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
 
@@ -50,7 +21,7 @@ class LoadingModelTestCase(unittest.TestCase):
     return [c.Index() for c in node.SortedSuccessors()]
 
   def test_DictConstruction(self):
-    graph = loading_model.ResourceGraph(
+    graph = test_utils.TestResourceGraph(
         {'request_track': {
             'events': [
                 test_utils.MakeRequest(0, 'null', 100, 100.5, 101).ToJsonDict(),
@@ -77,7 +48,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 test_utils.MakeRequest(4, 3, 127, 127.5, 128),
                 test_utils.MakeRequest(5, 'null', 100, 103, 105),
                 test_utils.MakeRequest(6, 5, 105, 107, 110)]
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
@@ -98,7 +69,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 test_utils.MakeRequest(4, 3, 127, 128, 129),
                 test_utils.MakeRequest(5, 'null', 100, 105, 106),
                 test_utils.MakeRequest(6, 5, 105, 110, 111)]
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     path_list = []
     self.assertEqual(29, graph.Cost(path_list))
     self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
@@ -119,7 +90,7 @@ class LoadingModelTestCase(unittest.TestCase):
                                magic_content_type=True),
         test_utils.MakeRequest(4, 2, 122, 126, 126),
         test_utils.MakeRequest(5, 2, 122, 126, 126)]
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -131,7 +102,7 @@ class LoadingModelTestCase(unittest.TestCase):
     # Change node 1 so it is a parent of 3, which becomes the parent of 2.
     requests[1] = test_utils.MakeRequest(
         1, 0, 110, 111, 111, magic_content_type=True)
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -144,12 +115,12 @@ class LoadingModelTestCase(unittest.TestCase):
     requests[1] = test_utils.MakeRequest(
         1, 0, 110, 111, 111, magic_content_type=True)
     requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     # Check it doesn't change until we change the content type of 6.
     self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
     requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
                                          magic_content_type=True)
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -169,7 +140,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 test_utils.MakeRequest(5, 2, 122, 126, 126)]
     for r in requests:
       r.response_headers['Content-Type'] = 'image/gif'
-    graph = self.MakeGraph(requests)
+    graph = test_utils.TestResourceGraph.FromRequestList(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 654e94b..aea73a7 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -5,6 +5,7 @@
 """Common utilities used in unit tests, within this directory."""
 
 import devtools_monitor
+import loading_model
 import loading_trace
 import page_track
 import request_track
@@ -80,3 +81,33 @@ def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
     tracing_track = None
   return loading_trace.LoadingTrace(
       None, None, page_event_track, request, tracing_track)
+
+
+class SimpleLens(object):
+  """A simple replacement for RequestDependencyLens.
+
+  Uses only the initiator url of a request for determining a dependency.
+  """
+  def __init__(self, trace):
+    self._trace = trace
+
+  def GetRequestDependencies(self):
+    url_to_rq = {}
+    deps = []
+    for rq in self._trace.request_track.GetEvents():
+      assert rq.url not in url_to_rq
+      url_to_rq[rq.url] = rq
+    for rq in self._trace.request_track.GetEvents():
+      initiating_url = rq.initiator['url']
+      if initiating_url in url_to_rq:
+        deps.append((url_to_rq[initiating_url], rq, rq.initiator['type']))
+    return deps
+
+
+class TestResourceGraph(loading_model.ResourceGraph):
+  """Replace the default request lens in a ResourceGraph with our SimpleLens."""
+  REQUEST_LENS = SimpleLens
+
+  @classmethod
+  def FromRequestList(cls, requests, page_events=None, trace_events=None):
+    return cls(LoadingTraceFromEvents(requests, page_events, trace_events))

commit f69df4b4519552ac5bb60ba2af99627b4394d4ec
Author: mattcary <mattcary@chromium.org>
Date:   Fri Feb 19 03:41:15 2016 -0800

    Improve trace integration test documentation.
    
    Review URL: https://codereview.chromium.org/1706323003
    
    Cr-Original-Commit-Position: refs/heads/master@{#376432}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: efbeed8cb1c0ec676ae81a3a678d0ffd82d70a09

diff --git a/loading/trace_test/README.md b/loading/trace_test/README.md
new file mode 100644
index 0000000..c16e472
--- /dev/null
+++ b/loading/trace_test/README.md
@@ -0,0 +1,8 @@
+Trace Integration Tests
+
+This directory defines integration tests which verify traces in various corners
+of the HTML/JS/CSS world.
+
+The unittests in this directory are run as part of
+tools/android/loading/run_tests. The integration tests are only run
+manually. See webserver_test.py for details.
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
index f6724c0..2b413a8 100755
--- a/loading/trace_test/webserver_test.py
+++ b/loading/trace_test/webserver_test.py
@@ -10,11 +10,23 @@ operation it can be run with no arguments (or perhaps --no_sandbox depending on
 how you have chrome set up). When debugging or adding tests, setting
 --failed_trace_dir could be useful.
 
-Spawns a local http server to serve web pages. The trace generated by each
-file in tests/*.html will be compared with the corresponding results/*.result.
+The integration test spawns a local http server to serve web pages. The trace
+generated by each file in tests/*.html will be compared with the corresponding
+results/*.result. Each test should have a detailed comment explaining its
+organization and what the important part of the test result is.
 
 By default this will use a release version of chrome built in this same
 code tree (out/Release/chrome), see --local_binary to override.
+
+See InitiatorSequence for what the integration tests verify. The idea is to
+capture a sketch of the initiator and call stack relationship. The output is
+human-readable. To create a new test, first run test_server.py locally with
+--source_dir pointing to tests/, and verify that the test page works as expected
+by pointing a browser to localhost:XXX/your_new_test.html (with XXX the port
+reported in the console output of test_server.py). Then run this
+webserver_test.py with --failed_trace_dir set. Verify that the actual output is
+what you expect it to be, then copy it to results/. If your test is 7.html, you
+should copy to results/7.result.
 """
 
 import argparse
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index 35fcb4d..2daa271 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -43,7 +43,6 @@ class TracingTrackTestCase(unittest.TestCase):
       sock.connect((host, int(port)))
       sock.sendall('GET test.html HTTP/1.1\n\n')
       data = sock.recv(4096)
-      print '%%% ' + data
       self.assertTrue('HTTP/1.0 200 OK' in data)
 
       sock.close()

commit 04a67846fd49a0177b73b2d3c309d5018f276dde
Author: droger <droger@chromium.org>
Date:   Thu Feb 18 09:15:29 2016 -0800

    tools/android/loading Ignore timings for requests coming from cache
    
    The requests coming from Blink cache have stale timings: these
    are the timings corresponding to the request that put the content
    in the cache, rather than of the request that got them from the cache.
    
    Ignoring these timings allows to fallback to the timestamp, which is
    more accurate (although not perfect), and fixes issues where the
    inconsistent timings was breaking initiators.
    
    Review URL: https://codereview.chromium.org/1708233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376187}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0dec7bcceeec4f9b2283fea5040879724b9b04a5

diff --git a/loading/request_track.py b/loading/request_track.py
index 1b3cde2..685ab0d 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -491,12 +491,14 @@ class RequestTrack(devtools_monitor.Track):
                       # network stack.
                       ('requestHeaders', 'request_headers'),
                       ('headers', 'response_headers')))
-    # data URLs don't have a timing dict.
     timing_dict = {}
-    if r.protocol != 'data':
-      timing_dict = response['timing']
-    else:
+    # data URLs don't have a timing dict, and timings for cached requests are
+    # stale.
+    # TODO(droger): the timestamp is inacurate, get the real timings instead.
+    if r.protocol == 'data' or r.served_from_cache:
       timing_dict = {'requestTime': r.timestamp}
+    else:
+      timing_dict = response['timing']
     r.timing = TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
     self._request_id_to_response_received[request_id] = params

commit 9ffe77c6341caeb1acb2322a117b868336daca6c
Author: lizeb <lizeb@chromium.org>
Date:   Thu Feb 18 09:09:37 2016 -0800

    tools/android/loading: Improve nested events handling in ActivityLens.
    
    Review URL: https://codereview.chromium.org/1703863002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376184}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2e161dcde1b6585b12374c90ef458adcfb314dd2

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index 257fdc3..c99d47a 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -17,6 +17,9 @@ import request_track
 
 class ActivityLens(object):
   """Reconstructs the activity of the main renderer thread between requests."""
+  _SCRIPT_EVENT_NAMES = ('EvaluateScript', 'FunctionCall')
+  _PARSING_EVENT_NAMES = ('ParseHTML', 'ParseAuthorStyleSheet')
+
   def __init__(self, trace):
     """Initializes an instance of ActivityLens.
 
@@ -103,8 +106,8 @@ class ActivityLens(object):
     script_to_duration = collections.defaultdict(float)
     script_events = [e for e in events
                      if ('devtools.timeline' in e.tracing_event['cat']
-                         and e.tracing_event['name'] in (
-                             'EvaluateScript', 'FunctionCall'))]
+                         and (e.tracing_event['name']
+                              in cls._SCRIPT_EVENT_NAMES))]
     for event in script_events:
       clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
       script_url = event.args['data'].get('scriptName', None)
@@ -112,6 +115,16 @@ class ActivityLens(object):
     return dict(script_to_duration)
 
   @classmethod
+  def _FullyIncludedEvents(cls, events, event):
+    """Return a list of events wholly included in the |event| span."""
+    (start, end) = (event.start_msec, event.end_msec)
+    result = []
+    for event in events:
+      if start <= event.start_msec < end and start <= event.end_msec < end:
+        result.append(event)
+    return result
+
+  @classmethod
   def _Parsing(cls, events, start_msec, end_msec):
     """Returns the HTML/CSS parsing time within an interval.
 
@@ -127,16 +140,25 @@ class ActivityLens(object):
     url_to_duration = collections.defaultdict(float)
     parsing_events = [e for e in events
                       if ('devtools.timeline' in e.tracing_event['cat']
-                          and e.tracing_event['name'] in (
-                              'ParseHTML', 'ParseAuthorStyleSheet'))]
+                          and (e.tracing_event['name']
+                               in cls._PARSING_EVENT_NAMES))]
     for event in parsing_events:
+      # Parsing events can contain nested script execution events, avoid
+      # double-counting by discounting these.
+      nested_events = cls._FullyIncludedEvents(events, event)
+      events_tree = _EventsTree(event, nested_events)
+      js_events = events_tree.DominatingEventsWithNames(cls._SCRIPT_EVENT_NAMES)
+      duration_to_subtract = sum(
+          cls._ClampedDuration(e, start_msec, end_msec) for e in js_events)
       tracing_event = event.tracing_event
       clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
       if tracing_event['name'] == 'ParseAuthorStyleSheet':
         url = tracing_event['args']['data']['styleSheetUrl']
       else:
         url = tracing_event['args']['beginData']['url']
-      url_to_duration[url] += clamped_duration
+      parsing_duration = clamped_duration - duration_to_subtract
+      assert parsing_duration >= 0
+      url_to_duration[url] += parsing_duration
     return dict(url_to_duration)
 
   def GenerateEdgeActivity(self, dep):
@@ -170,28 +192,69 @@ class ActivityLens(object):
            RequestDependencyLens.GetRequestDependencies().
 
     Returns:
-      {'script': float, 'parsing': float, 'other': float, 'unknown': float}
+      {'script': float, 'parsing': float, 'other_url': float,
+       'unknown_url': float, 'unrelated_work': float}
       where the values are durations in ms:
+      - idle: The renderer main thread was idle.
       - script: The initiating file was executing.
       - parsing: The initiating file was being parsed.
-      - other: Other scripts and/or parsing activities.
-      - unknown: Activity which is not associated with a URL.
+      - other_url: Other scripts and/or parsing activities.
+      - unknown_url: Activity which is not associated with a URL.
+      - unrelated_work: Activity unrelated to scripts or parsing.
     """
     activity = self.GenerateEdgeActivity(dep)
-    related = {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 0}
+    breakdown = {'unrelated_work': activity['busy'],
+                 'idle': activity['edge_cost'] - activity['busy'],
+                 'script': 0, 'parsing': 0,
+                 'other_url': 0, 'unknown_url': 0}
     for kind in ('script', 'parsing'):
       for (script_name, duration_ms) in activity[kind].items():
         if not script_name:
-          related['unknown_url'] += duration_ms
+          breakdown['unknown_url'] += duration_ms
         elif script_name == dep[0].url:
-          related[kind] += duration_ms
+          breakdown[kind] += duration_ms
         else:
-          # A lot of "ParseHTML" tasks are mostly about executing
-          # scripts. Don't double-count.
-          # TODO(lizeb): Better handle TraceEvents nesting.
-          if kind == 'script':
-            related['other_url'] += duration_ms
-    return related
+          breakdown['other_url'] += duration_ms
+    breakdown['unrelated_work'] -= sum(
+        breakdown[x] for x in ('script', 'parsing', 'other_url', 'unknown_url'))
+    return breakdown
+
+
+class _EventsTree(object):
+  """Builds the hierarchy of events from a list of fully nested events."""
+  def __init__(self, root_event, events):
+    """Creates the tree.
+
+    Args:
+      root_event: (Event) Event held by the tree root.
+      events: ([Event]) List of events that are fully included in |root_event|.
+    """
+    self.event = root_event
+    self.start_msec = root_event.start_msec
+    self.end_msec = root_event.end_msec
+    self.children = []
+    events.sort(key=operator.attrgetter('start_msec'))
+    if not events:
+      return
+    current_child = (events[0], [])
+    for event in events[1:]:
+      if event.end_msec < current_child[0].end_msec:
+        current_child[1].append(event)
+      else:
+        self.children.append(_EventsTree(current_child[0], current_child[1]))
+        current_child = (event, [])
+    self.children.append(_EventsTree(current_child[0], current_child[1]))
+
+  def DominatingEventsWithNames(self, names):
+    """Returns a list of the top-most events in the tree with a matching name.
+    """
+    if self.event.name in names:
+      return [self.event]
+    else:
+      result = []
+      for child in self.children:
+        result += child.DominatingEventsWithNames(names)
+      return result
 
 
 if __name__ == '__main__':
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index f96bbd4..5abc172 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -2,9 +2,11 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import collections
+import copy
 import unittest
 
-from activity_lens import ActivityLens
+from activity_lens import (ActivityLens, _EventsTree)
 import test_utils
 import tracing
 
@@ -202,23 +204,33 @@ class ActivityLensTestCast(unittest.TestCase):
          u'ph': u'X',
          u'pid': 1,
          u'tid': 1,
+         u'ts': 0},
+        {u'cat': u'toplevel',
+         u'dur': 100 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 1,
          u'ts': 0}]
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'parser')
     self.assertEquals(
-        {'script': 0, 'parsing': 12, 'other_url': 0, 'unknown_url': 0},
+        {'unrelated_work': 18, 'idle': 0, 'script': 0, 'parsing': 12,
+         'other_url': 0, 'unknown_url': 0},
         activity.BreakdownEdgeActivityByInitiator(dep))
     dep = (requests[0], requests[1], 'other')
-    # Truncating the event from the parent xrequest end.
+    # Truncating the event from the parent request end.
     self.assertEquals(
-        {'script': 0, 'parsing': 7, 'other_url': 0, 'unknown_url': 0},
+        {'unrelated_work': 13, 'idle': 0, 'script': 0, 'parsing': 7,
+         'other_url': 0, 'unknown_url': 0},
         activity.BreakdownEdgeActivityByInitiator(dep))
     # Unknown URL
     raw_events[0]['args']['beginData']['url'] = None
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'parser')
     self.assertEquals(
-        {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 12},
+        {'unrelated_work': 18, 'idle': 0, 'script': 0, 'parsing': 0,
+         'other_url': 0, 'unknown_url': 12},
         activity.BreakdownEdgeActivityByInitiator(dep))
     # Script
     raw_events[1]['ts'] = 40 * 1000
@@ -226,13 +238,15 @@ class ActivityLensTestCast(unittest.TestCase):
     activity = self._ActivityLens(requests, raw_events)
     dep = (requests[0], requests[1], 'script')
     self.assertEquals(
-        {'script': 6, 'parsing': 0, 'other_url': 0, 'unknown_url': 7},
+        {'unrelated_work': 7, 'idle': 0, 'script': 6, 'parsing': 0,
+         'other_url': 0, 'unknown_url': 7},
         activity.BreakdownEdgeActivityByInitiator(dep))
     # Other URL
     raw_events[1]['args']['data']['scriptName'] = 'http://other.com/url'
     activity = self._ActivityLens(requests, raw_events)
     self.assertEquals(
-        {'script': 0., 'parsing': 0., 'other_url': 6., 'unknown_url': 7.},
+        {'unrelated_work': 7, 'idle': 0, 'script': 0., 'parsing': 0.,
+         'other_url': 6., 'unknown_url': 7.},
         activity.BreakdownEdgeActivityByInitiator(dep))
 
   def _ActivityLens(self, requests, raw_events):
@@ -241,5 +255,37 @@ class ActivityLensTestCast(unittest.TestCase):
     return ActivityLens(loading_trace)
 
 
+class EventsTreeTestCase(unittest.TestCase):
+  FakeEvent = collections.namedtuple(
+      'FakeEvent', ('name', 'start_msec', 'end_msec'))
+  _ROOT_EVENT = FakeEvent('-1', 0, 20)
+  _EVENTS = [
+      FakeEvent('0', 2, 4), FakeEvent('1', 1, 5),
+      FakeEvent('2', 6, 9),
+      FakeEvent('3', 13, 14), FakeEvent('4', 14, 17), FakeEvent('5', 12, 18)]
+
+  def setUp(self):
+    self.tree = _EventsTree(self._ROOT_EVENT, copy.deepcopy(self._EVENTS))
+
+  def testEventsTreeConstruction(self):
+    self.assertEquals(self._ROOT_EVENT, self.tree.event)
+    self.assertEquals(3, len(self.tree.children))
+    self.assertEquals(self._EVENTS[1], self.tree.children[0].event)
+    self.assertEquals(self._EVENTS[0], self.tree.children[0].children[0].event)
+    self.assertEquals(self._EVENTS[2], self.tree.children[1].event)
+    self.assertEquals([], self.tree.children[1].children)
+    self.assertEquals(self._EVENTS[5], self.tree.children[2].event)
+    self.assertEquals(2, len(self.tree.children[2].children))
+
+  def testDominatingEventsWithNames(self):
+    self.assertListEqual(
+        [self._ROOT_EVENT], self.tree.DominatingEventsWithNames(('-1')))
+    self.assertListEqual(
+        [self._ROOT_EVENT], self.tree.DominatingEventsWithNames(('-1', '0')))
+    self.assertListEqual(
+        [self._EVENTS[1], self._EVENTS[5]],
+        self.tree.DominatingEventsWithNames(('1', '5')))
+
+
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/model_graph.py b/loading/model_graph.py
index e7fcc93..eaded4a 100644
--- a/loading/model_graph.py
+++ b/loading/model_graph.py
@@ -56,8 +56,8 @@ class GraphVisualization(object):
   }
 
   _ACTIVITY_TYPE_LABEL = (
-      ('script', 'S'), ('parsing', 'P'), ('other_url', 'O'),
-      ('unknown_url', 'U'))
+      ('idle', 'I'), ('unrelated_work', 'W'), ('script', 'S'),
+      ('parsing', 'P'), ('other_url', 'O'), ('unknown_url', 'U'))
 
   def __init__(self, graph):
     """Initialize.
diff --git a/loading/tracing.py b/loading/tracing.py
index 7071011..cf79908 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -312,6 +312,10 @@ class Event(object):
     return self._tracing_event.get('id')
 
   @property
+  def name(self):
+    return self._tracing_event['name']
+
+  @property
   def tracing_event(self):
     return self._tracing_event
 
@@ -413,6 +417,7 @@ class _IntervalTree(object):
     return _IntervalTree(start, end, filtered_events)
 
   def OverlappingEvents(self, start, end):
+    """Returns a set of events overlapping with [start, end)."""
     if min(end, self.end) - max(start, self.start) <= 0:
       return set()
     elif self._IsLeaf():

commit 13c672c69aac1480b3c191d856434c3ef70046f4
Author: droger <droger@chromium.org>
Date:   Thu Feb 18 05:29:57 2016 -0800

    tools/android/loading Fix import issues in webserver unittests
    
    Review URL: https://codereview.chromium.org/1708143002
    
    Cr-Original-Commit-Position: refs/heads/master@{#376158}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b7823f1dab8b7416f1818dfc80c1d322059a5137

diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
index 2716492..35fcb4d 100644
--- a/loading/trace_test/webserver_unittest.py
+++ b/loading/trace_test/webserver_unittest.py
@@ -8,7 +8,7 @@ import sys
 import unittest
 
 _SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
+    os.path.dirname(__file__), '..', '..', '..', '..'))
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'android', 'loading'))
 
 import options
@@ -48,3 +48,7 @@ class TracingTrackTestCase(unittest.TestCase):
 
       sock.close()
       self.assertTrue(server.Stop())
+
+
+if __name__ == '__main__':
+  unittest.main()

commit 66d7f3457728b2553dfe28fe1d701130b67b8efc
Author: droger <droger@chromium.org>
Date:   Thu Feb 18 04:54:03 2016 -0800

    tools/android/loading Add 'redirect' and 'ping' content types.
    
    Review URL: https://codereview.chromium.org/1707173003
    
    Cr-Original-Commit-Position: refs/heads/master@{#376152}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: af41852675231437476a78d79d2a564a44f8b507

diff --git a/loading/model_graph.py b/loading/model_graph.py
index 591bd7f..e7fcc93 100644
--- a/loading/model_graph.py
+++ b/loading/model_graph.py
@@ -37,6 +37,8 @@ class GraphVisualization(object):
       'gif':             'grey',
       'image':           'orange',
       'jpeg':            'orange',
+      'ping':            'cyan',  # Empty response
+      'redirect':        'forestgreen',
       'png':             'orange',
       'plain':           'brown3',
       'octet-stream':    'brown3',
diff --git a/loading/request_track.py b/loading/request_track.py
index 2c94703..1b3cde2 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -179,19 +179,36 @@ class Request(object):
       result.timing = TimingFromDict({'requestTime': result.timestamp})
     return result
 
+  def GetHTTPResponseHeader(self, header_name):
+    """Gets the value of a HTTP response header.
+
+    Does a case-insensitive search for the header name in the HTTP response
+    headers, in order to support servers that use a wrong capitalization.
+    """
+    lower_case_name = header_name.lower()
+    result = None
+    for name, value in self.response_headers.iteritems():
+      if name.lower() == lower_case_name:
+        result = value
+        break
+    return result
+
   def GetContentType(self):
     """Returns the content type, or None."""
+    # Check for redirects. Use the "Location" header, because the HTTP status is
+    # not reliable.
+    if self.GetHTTPResponseHeader('Location') is not None:
+      return 'redirect'
+
+    # Check if the response is empty.
+    if (self.GetHTTPResponseHeader('Content-Length') == '0' or
+        self.status == 204):
+      return 'ping'
+
     if self.mime_type:
       return self.mime_type
 
-    # Case-insensitive search because servers sometimes use a wrong
-    # capitalization.
-    content_type = None
-    for header, value in self.response_headers.iteritems():
-      if header.lower() == 'content-type':
-        content_type = value
-        break
-
+    content_type = self.GetHTTPResponseHeader('Content-Type')
     if not content_type or ';' not in content_type:
       return content_type
     else:
@@ -207,14 +224,7 @@ class Request(object):
     if not self.response_headers:
       return -1
 
-    # Case-insensitive search because servers sometimes use a wrong
-    # capitalization.
-    cache_control_str = None
-    for header, value in self.response_headers.iteritems():
-      if header.lower() == 'cache-control':
-        cache_control_str = value
-        break
-
+    cache_control_str = self.GetHTTPResponseHeader('Cache-Control')
     if cache_control_str is not None:
       directives = [s.strip() for s in cache_control_str.split(',')]
       for directive in directives:
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 7a85f07..6e54208 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -47,10 +47,31 @@ class RequestTestCase(unittest.TestCase):
     # Parameters are filtered out.
     r.response_headers = {'Content-Type': 'application/javascript;bla'}
     self.assertEquals('application/javascript', r.GetContentType())
-    # MIME type takes precedence over headers.
+    # MIME type takes precedence over 'Content-Type' header.
     r.mime_type = 'image/webp'
     self.assertEquals('image/webp', r.GetContentType())
     r.mime_type = None
+    # Test for 'ping' type.
+    r.status = 204
+    self.assertEquals('ping', r.GetContentType())
+    r.status = None
+    r.response_headers = {'Content-Type': 'application/javascript',
+                          'content-length': '0'}
+    self.assertEquals('ping', r.GetContentType())
+    # Test for 'redirect' type.
+    r.response_headers = {'Content-Type': 'application/javascript',
+                          'location': 'http://foo',
+                          'content-length': '0'}
+    self.assertEquals('redirect', r.GetContentType())
+
+  def testGetHTTPResponseHeader(self):
+    r = Request()
+    r.response_headers = {}
+    self.assertEquals(None, r.GetHTTPResponseHeader('Foo'))
+    r.response_headers = {'Foo': 'Bar', 'Baz': 'Foo'}
+    self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
+    r.response_headers = {'foo': 'Bar', 'Baz': 'Foo'}
+    self.assertEquals('Bar', r.GetHTTPResponseHeader('Foo'))
 
 
 class RequestTrackTestCase(unittest.TestCase):

commit 83218c736ace6b097ec68ce90181d84c4218f0a8
Author: mattcary <mattcary@chromium.org>
Date:   Wed Feb 17 08:30:11 2016 -0800

    Local webserver test harness plus 3 tests.
    
    This adds a trace_test directory to tools/android/loading that
    performs an integration test for our tracing, and also serves as a
    respository for initiator corner cases that we are investigating.
    
    Review URL: https://codereview.chromium.org/1696353002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375901}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 07593ac7760c73d9597994c08eeac37f4b01df75

diff --git a/loading/trace_test/__init__.py b/loading/trace_test/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/loading/trace_test/results/1.result b/loading/trace_test/results/1.result
new file mode 100644
index 0000000..0b445dc
--- /dev/null
+++ b/loading/trace_test/results/1.result
@@ -0,0 +1,7 @@
+parser (no stack) 1.css
+parser (no stack) 1a.js
+parser (no stack) 1a.png
+parser (no stack) 1b.png
+script (1a.js:9) 1b.js
+script (1b.js:54) 1.ttf
+script (1b.js:54) application/font-wof...Zk73/mAw==
diff --git a/loading/trace_test/results/2.result b/loading/trace_test/results/2.result
new file mode 100644
index 0000000..6fe8a06
--- /dev/null
+++ b/loading/trace_test/results/2.result
@@ -0,0 +1,5 @@
+parser (no stack) 1.css
+parser (no stack) 1a.png
+parser (no stack) 1b.js
+parser (no stack) 1b.png
+parser (no stack) application/font-wof...Zk73/mAw==
diff --git a/loading/trace_test/results/3.result b/loading/trace_test/results/3.result
new file mode 100644
index 0000000..f909cae
--- /dev/null
+++ b/loading/trace_test/results/3.result
@@ -0,0 +1,6 @@
+parser (no stack) 3a.js
+parser (no stack) 3c.js
+script () 3a.jpg
+script () 3b.jpg
+script () 3c.jpg
+script (3a.js:20) 3b.js
diff --git a/loading/trace_test/test_server.py b/loading/trace_test/test_server.py
new file mode 100755
index 0000000..5898c66
--- /dev/null
+++ b/loading/trace_test/test_server.py
@@ -0,0 +1,86 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""A simple http server for running local integration tests.
+
+This chooses a port dynamically and so can communicate that back to its spawner
+via a named pipe at --fifo. Sources are served from the tree named at
+--source_dir.
+"""
+
+
+import argparse
+import cgi
+import os.path
+import logging
+import re
+import time
+import wsgiref.simple_server
+
+
+class ServerApp(object):
+  """WSGI App.
+
+  Dispatches by matching, in order, against GetPaths.
+  """
+  def __init__(self, source_dir):
+    self._source_dir = source_dir
+
+  def __call__(self, environ, start_response):
+    """WSGI dispatch.
+
+    Args:
+      environ: environment list.
+      start_response: WSGI response start.
+
+    Returns:
+      Iterable server result.
+    """
+    path = environ.get('PATH_INFO', '')
+    while path.startswith('/'):
+      path = path[1:]
+    filename = os.path.join(self._source_dir, path)
+    if not os.path.exists(filename):
+      logging.info('%s not found', filename)
+      start_response('404 Not Found', [('Content-Type', 'text/html')])
+      return ["""<!DOCTYPE html>
+<html>
+<head>
+<title>Not Found</title>
+<body>%s not found</body>
+</html>""" % path]
+
+    logging.info('responding with %s', filename)
+    suffix = path[path.rfind('.') + 1:]
+    start_response('200 OK', [('Content-Type',
+                               {'css': 'text/css',
+                                'html': 'text/html',
+                                'jpg': 'image/jpeg',
+                                'js': 'text/javascript',
+                                'png': 'image/png',
+                                'ttf': 'font/ttf',
+                              }[suffix])])
+    return file(filename).read()
+
+
+if __name__ == '__main__':
+  logging.basicConfig(level=logging.INFO)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--fifo', default=None,
+                      help='Named pipe used to communicate port')
+  parser.add_argument('--source_dir', required=True,
+                      help='Directory holding sources to serve.')
+  args = parser.parse_args()
+  server_app = ServerApp(args.source_dir)
+  server = wsgiref.simple_server.make_server(
+      'localhost', 0, server_app)
+  ip, port = server.server_address
+  logging.info('Listening on port %s at %s', port, args.source_dir)
+  if args.fifo:
+    fifo = file(args.fifo, 'w')
+    fifo.write('%s\n' % port)
+    fifo.flush()
+    fifo.close()
+  server.serve_forever()
diff --git a/loading/trace_test/tests/1.css b/loading/trace_test/tests/1.css
new file mode 100644
index 0000000..5ae3598
--- /dev/null
+++ b/loading/trace_test/tests/1.css
@@ -0,0 +1,9 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+.outside {
+    font-family: inline;
+    color: red;
+}
diff --git a/loading/trace_test/tests/1.html b/loading/trace_test/tests/1.html
new file mode 100644
index 0000000..5dd15ce
--- /dev/null
+++ b/loading/trace_test/tests/1.html
@@ -0,0 +1,43 @@
+<!DOCTYPE html>
+<!--
+  Test Javascript Redirection in <head>
+
+  In <head> we have a CSS, a javascript file and a <style> tag. The javascript
+  file inserts another javascript file into head, which itself inserts a <style>
+  tag containing an inline font. The static <style> tag below has a font
+  resource. We expect the static font resource to have an initiator with a stack
+  trace incorrectly attached from the javascript.
+
+  TODO(mattcary): It also appears that if resources are found in the cache we
+  get different intiators: namely both the fonts have a parser initiator with no
+  stack. This is not exactly the problem, as occasionally the initiator sequence
+  changes, but can become consistent again by switching binaries with each run
+  (eg, out/Debug vs out/Release).
+-->
+<html>
+<head>
+<title>Test Javascript Redirection</title>
+<link rel='stylesheet' type='text/css' href='1.css'>
+<script type='text/javascript' src='1a.js'></script>
+<style>
+/* Custom font */
+@font-face {
+ font-family: 'test1';
+ font-style: normal;
+ font-weight: normal;
+ src: local('test1'), local(test1), url(1.ttf) format('truetype');
+}
+</style>
+<style>
+div {
+ background: url('1a.png')
+}
+</style>
+</head>
+<body>
+<img src='1b.png' alt=''>
+
+<div class="outside">ABCpqrst</div>
+<div class="inside">ABCpqrst</div>
+</body>
+</html>
diff --git a/loading/trace_test/tests/1.ttf b/loading/trace_test/tests/1.ttf
new file mode 100644
index 0000000..d268785
Binary files /dev/null and b/loading/trace_test/tests/1.ttf differ
diff --git a/loading/trace_test/tests/1a.js b/loading/trace_test/tests/1a.js
new file mode 100644
index 0000000..496cebb
--- /dev/null
+++ b/loading/trace_test/tests/1a.js
@@ -0,0 +1,9 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+var scr = document.createElement('script');
+scr.setAttribute('type', 'text/javascript');
+scr.setAttribute('src', '1b.js');
+document.getElementsByTagName('head')[0].appendChild(scr)
diff --git a/loading/trace_test/tests/1a.png b/loading/trace_test/tests/1a.png
new file mode 100644
index 0000000..88a0325
Binary files /dev/null and b/loading/trace_test/tests/1a.png differ
diff --git a/loading/trace_test/tests/1b.js b/loading/trace_test/tests/1b.js
new file mode 100644
index 0000000..eaf905d
--- /dev/null
+++ b/loading/trace_test/tests/1b.js
@@ -0,0 +1,60 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+/* Droid Sans from Google Fonts */
+var font = '@font-face { font-family: "inline"; ' +
+    'src: url(data:application/font-woff2;charset=utf-8;base64,' +
+    'd09GMgABAAAAAAboABEAAAAADcgAAAaGAAEAAAAAAAAAAAAAAAAAAAAAAA' +
+    'AAAAAAP0ZGVE0cGigGVgCCeggUCYRlEQgKiDSIWgE2AiQDWAsuAAQgBYJ+B' +
+    '4FVDHg/d2ViZgYbkQxRlC1Sm+yLArvh2MHKbfiAVIG+Knex+u6x+Pyd0L+n' +
+    '4Cl0n74VfYIZH6AMqEOdEag0hxQqkzJcpeRdedy7DCB9T9LF3Y3l8976Xbg' +
+    'X6AArK4qytKYdx2UW4LK8xGbPr2v+AmhM4aV1UgMv5btaum+17iX0YpGGCG' +
+    'EYLIOf7Zf340t4NJtpeX7PFhBmixQP5C/r1GtZokUUskL2f9fU3r93GZDv8' +
+    '+jM5uzlH7wmKVHaEV07AFCtGtkaPQtEalMT1s5gePQ3sRnV4Ie/BQjAB0te' +
+    '/QV450a0AsBn99o2dz6vCnQQAg6CMAHBq5hchnij85b8j4/nx/4LIH3J2e5' +
+    'XnHWa4BC4kDXZW4H4ypUcLmTqeMADwE+YsRuLDoNQTwOuCFHme+wHNKnjeQ' +
+    '4VQlZxh0I4HB6bOp5lQIUVVdi92f3s9+zLil/yP//x853/zhXWky0SLJ0S5' +
+    '4zrezfa/qbk/3t+wEvL5BhOBEmi7632G4otEyCtC2O/ot+wANdlQyrVGts8' +
+    'YN/SC/C0smwfFwt9QSr1wUnXoLawNbial7VsAvWrAVkfgrAdYtjs6G/3rQ1' +
+    'prtX/7j8bsoFYqqg3bKtO6FyHi5IwOe5DkoPCi688Potvk0Fgih5ZDqp6NR' +
+    '2tSGoKVcR8qEL7C7Ab4UkZ+PwOJggFnUA/cz93Uzq5PGiMDbqKNoiLBbWdd' +
+    'SUHk81sPbrQ01ECBl4Qg1w6qURt3Dq3TkqL8+xIw81VqTxILmtzfUV2mSuX' +
+    '4jxxDKTSs2EtB1oqUXphrTK/5i3bmCC9uSugDMMdBIzsS5gxw7YwvS18KJN' +
+    '2DQUNmFV3mLEd7EpyXcjnRpsqxjkfzhOAwd3NY1rOA3dxgOWS2VOgLH2hnf' +
+    'P/lR3auchORtav1cGLzmsDOUK9VN/Y6HWdO4EFRDgyvioOmZTnCeDGoKywg' +
+    'MUlNKiHoEBT0njIyMNMZAtIl0LryFDQIRkIr/M9BUGyDBuANvmGAaAEfAh8' +
+    'Dxn1wNn1oazEwf00PlI8b3EQVsszOvJSeki/GZNCuSSCHSolHeYacwCKIkV' +
+    'gk0lGdQlFrwAlijFrUPfCPiHBEieVgkVuOoyOOaMxTXcR3AqCGkGfJQCoYX' +
+    'DR0JjAYqMqiuIQszkxdjNRcCh0k26crIa2hwb7S6x64eeF5UQEQuWvZN80m' +
+    'wrN8Xqyl8cyNI2QiZ/ARSYML05ZL/9fbIz/Q15LOjnMbVPpwZQNCuOmwM3L' +
+    'UiDSG5Te4UTpIZyv1JidE620EGKWp6qyYKVa2kGqomYifgQbFl05rNhXdk2' +
+    '39FozuhTZgW7ZxrT0CHrQTGiwxf6RRbMBj8ykW+lgFqPbD7MqhUhzUFOzSI' +
+    'y0Bgv5lRBu4PGKZ4kYGSXtw4jSajk1kHG6FI6ayMYtqVtyIPfmKDtmhsA5s' +
+    'IsBVWRHjmKyii7cJGTPWkAzzVY8Mn5iHJvJtlTehFLHzNU61VhdMNiyC1a7' +
+    '/o0MazQ1udRV1/RSwbgdhHPmTmlfgHUljaZl+YIF21T7wXFURxbqSgaPMXu' +
+    'AKkHFhRQaoCcoQsY5NxVP+7KyQxe8OGLMrp1iuoqu33iNFHQxsQnbG9dkX+' +
+    'mmSC6pbrljMi3Tu7p0zSqlUK3aoeOw827lGNdLWkAuD+wzpiunoecYa+ppN' +
+    'g0uIIfopXHHsrt7Fi0+0zg9123bWyYiwx5W2Asewfq7ckv+qphwrLb4fr4/' +
+    'D/zVWZssC/ATIP8Nc5KAn2R/ECQDG/9xOKzN+ZfVAJzXgmMS8CHxEqHmDhJ' +
+    '3mc9OTpEvQY4D3BOWKkgnsBXYnXT/WbePNtZ/v0kHCURbm/UROYYyz+EiXm' +
+    'G3IQoQks87lP8mIdwuTXrcHm0MuX1CVrsD8px2v0Mbl93vMsIT7veoksL72' +
+    't1Dv3Xp4iOukLFEdgL+7JSKja5Z3qEopSoEbFbnVwz0UEa8/ChDiY5IyMFC' +
+    'IR+TUyC/aWEHS0WxdgAFMY/fmcdC4oqzkDFiW8Qzyrmchn9OxEYbGteVGVs' +
+    'U8eYdv4uJjapb93SE21+g2IOMb1Pj79pAHHFxmcJUpoknvgSSk7wUpCglKU' +
+    'tFqlKTujSkGZxrr7E0c1f7yiDB1UndihFm1SyQURKTMTKbzCFzyTwynyzQV' +
+    'jTEa7U5uvS0VS+ePQ15hk3KbWcPLs8esLd/QzHV/ujFrK/UOR3oVeZfxDPA' +
+    'nXCNktFqJcM1KVF3ohJQDWSpTdTvBwboLiPX7iqwaaZUPuIAt0Zk73/mAw==) ' +
+    'format("woff2"); font-weight: normal; font-style: normal;} ' +
+    '.inside { font-family: test1; color: green;}';
+
+var sty = document.createElement('style');
+sty.appendChild(document.createTextNode(font));
+document.getElementsByTagName('head')[0].appendChild(sty);
+
+var dummyToCheckStackIsnotJustEndOfFile = 0;
+function makingTheStackTraceReallyInteresting(x) {
+  dummyToCheckStackIsnotJustEndOfFile = x + 3;
+}
+makingTheStackTraceReallyInteresting(5);
diff --git a/loading/trace_test/tests/1b.png b/loading/trace_test/tests/1b.png
new file mode 100644
index 0000000..dca89e0
Binary files /dev/null and b/loading/trace_test/tests/1b.png differ
diff --git a/loading/trace_test/tests/2.html b/loading/trace_test/tests/2.html
new file mode 100644
index 0000000..1d79ff3
--- /dev/null
+++ b/loading/trace_test/tests/2.html
@@ -0,0 +1,36 @@
+<!DOCTYPE html>
+<!--
+  Test Less Javascript Redirection in <head>
+
+  Like 1.html, in <head> we have a CSS, a javascript file and a <style> tag. In
+  this case, the javacript file directly inserts a <style> tag into <head>. This
+  causes the subsequently loaded font to not be associated with a stack trace,
+  but also causes the dynamically loaded static font from 1b.js to also not have
+  a stack trace.
+-->
+<html>
+<head>
+<title>As 1.html, but one less redirection</title>
+<link rel='stylesheet' type='text/css' href='1.css'>
+<script type='text/javascript' src='1b.js'></script>
+<style>
+@font-face {
+ font-family: 'indie';
+ font-style: normal;
+ font-weight: normal;
+ src: local('Indie Flower'), local('IndieFlower'), url(1.ttf) format('truetype');
+}
+</style>
+<style>
+div {
+ background: url('1a.png')
+}
+</style>
+</head>
+<body>
+<img src='1b.png' alt=''>
+
+<div class="outside">Outside</div>
+<div class="inside">Inside</div>
+</body>
+</html>
diff --git a/loading/trace_test/tests/3.html b/loading/trace_test/tests/3.html
new file mode 100644
index 0000000..03bbce6
--- /dev/null
+++ b/loading/trace_test/tests/3.html
@@ -0,0 +1,27 @@
+<!DOCTYPE html>
+<!--
+  Javascript indirect image loading.
+
+  3a.js defines fn1(), which adds an <img> tag to the body. 3a.js also
+  inserts a script tag with 3b.js into head (between the scripts for
+  3a and 3c). 3b.js itself creates an <img> tag which directly adds it
+  to the body. Finally, 3c.js defines fn3(), which
+  modifies <img id='img3'>.
+
+  Note that as 3b.js adds a tag to the body, it is executed only after
+  the body has been parsed. No, I don't know how that works either.
+
+  At any rate, only 3c.js has a meaningful stack trace in the
+  initiator. The images have script initiators with empty stacks.
+-->
+<html>
+<head>
+<script type='text/javascript' src='3a.js'></script>
+<script type='text/javascript' src='3c.js'></script>
+<img src='' alt='' id='img3'>
+<script type='text/javascript'>
+ fn1();
+ fn3();
+</script>
+</body>
+</html>
diff --git a/loading/trace_test/tests/3a.jpg b/loading/trace_test/tests/3a.jpg
new file mode 100644
index 0000000..25f3a43
Binary files /dev/null and b/loading/trace_test/tests/3a.jpg differ
diff --git a/loading/trace_test/tests/3a.js b/loading/trace_test/tests/3a.js
new file mode 100644
index 0000000..31f6dca
--- /dev/null
+++ b/loading/trace_test/tests/3a.js
@@ -0,0 +1,21 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+function addImg(img_link) {
+  var img = document.createElement('img');
+  img.setAttribute('src', img_link);
+  img.setAttribute('alt', '');
+  document.body.appendChild(img);
+}
+
+function fn1() {
+  addImg('3a.jpg');
+}
+
+var scr = document.createElement('script');
+scr.setAttribute('src', '3b.js');
+scr.setAttribute('type', 'text/javascript');
+document.getElementsByTagName('head')[0].insertBefore(
+    scr, document.getElementsByTagName('script')[0].nextSibling);
diff --git a/loading/trace_test/tests/3b.jpg b/loading/trace_test/tests/3b.jpg
new file mode 100644
index 0000000..de44b66
Binary files /dev/null and b/loading/trace_test/tests/3b.jpg differ
diff --git a/loading/trace_test/tests/3b.js b/loading/trace_test/tests/3b.js
new file mode 100644
index 0000000..9ccc020
--- /dev/null
+++ b/loading/trace_test/tests/3b.js
@@ -0,0 +1,9 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+var img = document.createElement('img');
+img.setAttribute('src', '3b.jpg');
+img.setAttribute('alt', '');
+document.body.appendChild(img);
diff --git a/loading/trace_test/tests/3c.jpg b/loading/trace_test/tests/3c.jpg
new file mode 100644
index 0000000..688b70b
Binary files /dev/null and b/loading/trace_test/tests/3c.jpg differ
diff --git a/loading/trace_test/tests/3c.js b/loading/trace_test/tests/3c.js
new file mode 100644
index 0000000..b34da79
--- /dev/null
+++ b/loading/trace_test/tests/3c.js
@@ -0,0 +1,8 @@
+/* Copyright 2016 The Chromium Authors. All rights reserved.
+ * Use of this source code is governed by a BSD-style license that can be
+ * found in the LICENSE file.
+ */
+
+function fn3() {
+  document.getElementById('img3').setAttribute('src', '3c.jpg');
+}
diff --git a/loading/trace_test/webserver_test.py b/loading/trace_test/webserver_test.py
new file mode 100755
index 0000000..f6724c0
--- /dev/null
+++ b/loading/trace_test/webserver_test.py
@@ -0,0 +1,258 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""An integration test for tracing.
+
+This is not run as part of unittests and is executed directly. In normal
+operation it can be run with no arguments (or perhaps --no_sandbox depending on
+how you have chrome set up). When debugging or adding tests, setting
+--failed_trace_dir could be useful.
+
+Spawns a local http server to serve web pages. The trace generated by each
+file in tests/*.html will be compared with the corresponding results/*.result.
+
+By default this will use a release version of chrome built in this same
+code tree (out/Release/chrome), see --local_binary to override.
+"""
+
+import argparse
+import contextlib
+import json
+import os
+import shutil
+import subprocess
+import sys
+import tempfile
+import urlparse
+
+sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
+
+from device_setup import DeviceConnection
+import loading_trace
+import options
+import trace_recorder
+
+OPTIONS = options.OPTIONS
+WEBSERVER = os.path.join(os.path.dirname(__file__), 'test_server.py')
+TESTDIR = os.path.join(os.path.dirname(__file__), 'tests')
+RESULTDIR = os.path.join(os.path.dirname(__file__), 'results')
+
+
+@contextlib.contextmanager
+def TemporaryDirectory():
+  """Returns a freshly-created directory that gets automatically deleted after
+  usage.
+  """
+  name = tempfile.mkdtemp()
+  try:
+    yield name
+  finally:
+    shutil.rmtree(name)
+
+
+class WebServer(object):
+  """Wrap the webserver."""
+  def __init__(self, source_dir, communication_dir):
+    """Initialize the server but does not start it.
+
+    Args:
+      source_dir: the directory where source data (html, js, etc) will be found.
+      communication_dir: a directory to use for IPC (eg, discovering the
+        port, which is dynamically allocated). This should probably be a
+        temporary directory.
+    """
+    self._source_dir = source_dir
+    self._communication_dir = communication_dir
+    self._fifo = None
+    self._server_process = None
+    self._port = None
+
+  @classmethod
+  @contextlib.contextmanager
+  def Context(cls, *args, **kwargs):
+    """Creates a webserver as a context manager.
+
+    Args:
+      As in __init__.
+
+    Returns:
+      A context manager for an instance of a WebServer.
+    """
+    try:
+      server = cls(*args, **kwargs)
+      server.Start()
+      yield server
+    finally:
+      server.Stop()
+
+  def Start(self):
+    """Start the server by spawning a process."""
+    fifo_name = os.path.join(self._communication_dir, 'from_server')
+    os.mkfifo(fifo_name)
+    server_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+    self._server_process = subprocess.Popen(
+        [WEBSERVER,
+         '--source_dir=%s' % self._source_dir,
+         '--fifo=%s' % fifo_name],
+        shell=False, stdout=server_out, stderr=server_out)
+    fifo = file(fifo_name)
+    # TODO(mattcary): timeout?
+    self._port = int(fifo.readline())
+    fifo.close()
+
+  def Stop(self):
+    """Stops the server, waiting for it to complete.
+
+    Returns:
+      True if the server stopped correctly.
+    """
+    if self._server_process is None:
+      return False
+    self._server_process.kill()
+    # TODO(mattcary): timeout & error?
+    self._server_process.wait()
+    return True
+
+  def Address(self):
+    """Returns a host:port string suitable for an http request."""
+    assert self._port is not None, \
+        "No port exists until the server is started."
+    return 'localhost:%s' % self._port
+
+
+class InitiatorSequence(object):
+  """The interesting parts of the initiator dependancies that are tested."""
+  def __init__(self, trace):
+    """Create.
+
+    Args:
+      trace: a LoadingTrace.
+    """
+    self._seq = []
+    # ReadFromFile will initialize without a trace.
+    if trace is None:
+      return
+    for rq in trace.request_track.GetEvents():
+      if rq.initiator['type'] in ('parser', 'script'):
+        stack = 'no stack'
+        if 'stack' in rq.initiator:
+          stack = '/'.join(
+              ['%s:%s' % (self._ShortUrl(frame['url']), frame['lineNumber'])
+               for frame in rq.initiator['stack']['callFrames']])
+        self._seq.append('%s (%s) %s' % (
+            rq.initiator['type'],
+            stack,
+            self._ShortUrl(rq.url)))
+    self._seq.sort()
+
+  @classmethod
+  def ReadFromFile(cls, input_file):
+    """Read a file from DumpToFile.
+
+    Args:
+      input_file: a file-like object.
+
+    Returns:
+      An InitiatorSequence instance.
+    """
+    seq = cls(None)
+    seq._seq = sorted([l.strip() for l in input_file.readlines() if l])
+    return seq
+
+  def DumpToFile(self, output):
+    """Write to a file.
+
+    Args:
+      output: a writeable file-like object.
+    """
+    output.write('\n'.join(self._seq) + '\n')
+
+  def __eq__(self, other):
+    if other is None:
+      return False
+    assert type(other) is InitiatorSequence
+    if len(self._seq) != len(other._seq):
+      return False
+    for a, b in zip(self._seq, other._seq):
+      if a != b:
+        return False
+    return True
+
+  def _ShortUrl(self, url):
+    short = urlparse.urlparse(url).path
+    while short.startswith('/'):
+      short = short[1:]
+    if len(short) > 40:
+      short = '...'.join((short[:20], short[-10:]))
+    return short
+
+
+def RunTest(webserver, connection, test_page, expected):
+  """Run an webserver test.
+
+  The expected result can be None, in which case --failed_trace_dir can be set
+  to output the observed trace.
+
+  Args:
+    webserver [WebServer]: the webserver to use for the test. It must be
+      started.
+    connection [DevToolsConnection]: the connection to trace against.
+    test_page: the name of the page to load.
+    expected [InitiatorSequence]: expected initiator sequence.
+
+  Returns:
+    True if the test passed and false otherwise. Status is printed to stdout.
+  """
+  url = 'http://%s/%s' % (webserver.Address(), test_page)
+  sys.stdout.write('Testing %s...' % url)
+  observed_seq = InitiatorSequence(trace_recorder.MonitorUrl(
+      connection, url, clear_cache=True))
+  if observed_seq == expected:
+    sys.stdout.write(' ok\n')
+    return True
+  else:
+    sys.stdout.write(' FAILED!\n')
+    if OPTIONS.failed_trace_dir:
+      outname = os.path.join(OPTIONS.failed_trace_dir,
+                             test_page + '.observed_result')
+      with file(outname, 'w') as output:
+        observed_seq.DumpToFile(output)
+      sys.stdout.write('Wrote observed result to %s\n' % outname)
+  return False
+
+
+def RunAllTests():
+  """Run all tests in TESTDIR.
+
+  All tests must have a corresponding result in RESULTDIR unless
+  --failed_trace_dir is set.
+  """
+  with TemporaryDirectory() as temp_dir, \
+       WebServer.Context(TESTDIR, temp_dir) as webserver, \
+       DeviceConnection(None) as connection:
+    failure = False
+    for test in sorted(os.listdir(TESTDIR)):
+      if test.endswith('.html'):
+        result = os.path.join(RESULTDIR, test[:test.rfind('.')] + '.result')
+        assert OPTIONS.failed_trace_dir or os.path.exists(result), \
+            'No result found for test'
+        expected = None
+        if os.path.exists(result):
+          with file(result) as result_file:
+            expected = InitiatorSequence.ReadFromFile(result_file)
+        if not RunTest(webserver, connection, test, expected):
+          failure = True
+  if failure:
+    print 'FAILED!'
+  else:
+    print 'all tests passed'
+
+
+if __name__ == '__main__':
+  OPTIONS.ParseArgs(sys.argv[1:],
+                    description='Run webserver integration test',
+                    extra=[('--failed_trace_dir', ''),
+                           ('--noisy', False)])
+  RunAllTests()
diff --git a/loading/trace_test/webserver_unittest.py b/loading/trace_test/webserver_unittest.py
new file mode 100644
index 0000000..2716492
--- /dev/null
+++ b/loading/trace_test/webserver_unittest.py
@@ -0,0 +1,50 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import socket
+import sys
+import unittest
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'android', 'loading'))
+
+import options
+from trace_test import webserver_test
+
+
+OPTIONS = options.OPTIONS
+
+
+class TracingTrackTestCase(unittest.TestCase):
+  def setUp(self):
+    OPTIONS.ParseArgs('', extra=[('--noisy', False)])
+
+  def testWebserver(self):
+    with webserver_test.TemporaryDirectory() as temp_dir:
+      test_html = file(os.path.join(temp_dir, 'test.html'), 'w')
+      test_html.write('<!DOCTYPE html><html><head><title>Test</title></head>'
+                      '<body><h1>Test Page</h1></body></html>')
+      test_html.close()
+
+      server = webserver_test.WebServer(temp_dir, temp_dir)
+      server.Start()
+      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+      host, port = server.Address().split(':')
+      sock.connect((host, int(port)))
+      sock.sendall('GET null HTTP/1.1\n\n')
+      data = sock.recv(4096)
+      self.assertTrue(data.startswith('HTTP/1.0 404 Not Found'))
+      sock.close()
+
+      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+      sock.connect((host, int(port)))
+      sock.sendall('GET test.html HTTP/1.1\n\n')
+      data = sock.recv(4096)
+      print '%%% ' + data
+      self.assertTrue('HTTP/1.0 200 OK' in data)
+
+      sock.close()
+      self.assertTrue(server.Stop())

commit 68296502d4944bfc8f8d2a775678792fe4aa9646
Author: blundell <blundell@chromium.org>
Date:   Wed Feb 17 06:44:37 2016 -0800

    tools/android/loading: Fix local profile dir default value
    
    The default value was '' but the code was checking for a default value
    of None.
    
    Review URL: https://codereview.chromium.org/1708553002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375880}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 910318e07e5d98ac95f8e250bf7facd878bc7629

diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
index 8d7948c..a1f6849 100644
--- a/loading/chrome_setup.py
+++ b/loading/chrome_setup.py
@@ -55,8 +55,8 @@ def DevToolsConnectionForLocalBinary(flags):
   """
   binary_filename = OPTIONS.local_binary
   profile_dir = OPTIONS.local_profile_dir
-  temp_profile_dir = profile_dir is None
-  if temp_profile_dir:
+  using_temp_profile_dir = profile_dir is None
+  if using_temp_profile_dir:
     profile_dir = tempfile.mkdtemp()
   flags.append('--user-data-dir=%s' % profile_dir)
   chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
@@ -68,8 +68,8 @@ def DevToolsConnectionForLocalBinary(flags):
         OPTIONS.devtools_hostname, OPTIONS.devtools_port)
   finally:
     process.kill()
-    if temp_profile_dir:
-      shutil.rmtree(temp_profile_dir)
+    if using_temp_profile_dir:
+      shutil.rmtree(profile_dir)
 
 
 def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
diff --git a/loading/options.py b/loading/options.py
index 04b5bb3..a11b56c 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -30,7 +30,7 @@ class Options(object):
              'chrome binary for local runs'),
             ('local_noisy', False,
              'Enable local chrome console output'),
-            ('local_profile_dir', '',
+            ('local_profile_dir', None,
              'profile directory to use for local runs'),
             ('no_sandbox', False,
              'pass --no-sandbox to browser (local run only; see also '

commit 10b4355a469585544cb2986c504c7d8b6abdfdb2
Author: droger <droger@chromium.org>
Date:   Wed Feb 17 05:36:01 2016 -0800

    tools/android/loading Enable async call stacks
    
    This CL enables the "Debugger" domains and sets the async
    call stack depth to 4 for the request track.
    The asynchronous stacks are merged with the basic stacks, which
    provides more information in the initiators and dramatically reduces
    the number of orphan requests.
    
    Review URL: https://codereview.chromium.org/1692983002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375870}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b382e4f33c93cf1e95c6e351f365f72ef8b27f54

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 5279e4f..68c785f 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -92,6 +92,7 @@ class DevToolsConnection(object):
     self._ws = self._Connect(hostname, port)
     self._event_listeners = {}
     self._domain_listeners = {}
+    self._scoped_states = {}
     self._domains_to_enable = set()
     self._tearing_down_tracing = False
     self._set_up = False
@@ -130,6 +131,30 @@ class DevToolsConnection(object):
       if key in self._domain_listeners:
         del(self._domain_listeners[key])
 
+  def SetScopedState(self, method, params, default_params, enable_domain):
+    """Changes state at the beginning the monitoring and resets it at the end.
+
+    |method| is called with |params| at the beginning of the monitoring. After
+    the monitoring completes, the state is reset by calling |method| with
+    |default_params|.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Parameters to set when the monitoring starts.
+      default_params: (dict) Parameters to reset the state at the end.
+      enable_domain: (bool) True if enabling the domain is required.
+    """
+    if enable_domain:
+      if '.' in method:
+        domain = method[:method.index('.')]
+        assert domain, 'No valid domain'
+        self._domains_to_enable.add(domain)
+    scoped_state_value = (params, default_params)
+    if self._scoped_states.has_key(method):
+      assert self._scoped_states[method] == scoped_state_value
+    else:
+      self._scoped_states[method] = scoped_state_value
+
   def SyncRequest(self, method, params=None):
     """Issues a synchronous request to the DevTools server.
 
@@ -186,6 +211,9 @@ class DevToolsConnection(object):
         self.SyncRequestNoResponse('%s.enable' % domain)
         # Tracing setup must be done by the tracing track to control filtering
         # and output.
+    for scoped_state in self._scoped_states:
+      self.SyncRequestNoResponse(scoped_state,
+                                 self._scoped_states[scoped_state][0])
     self._tearing_down_tracing = False
     self._set_up = True
 
@@ -218,6 +246,9 @@ class DevToolsConnection(object):
       self.SyncRequestNoResponse(self.TRACING_END_METHOD)
       self._tearing_down_tracing = True
       self._Dispatch(kind='Tracing', timeout=self.TRACING_TIMEOUT)
+    for scoped_state in self._scoped_states:
+      self.SyncRequestNoResponse(scoped_state,
+                                 self._scoped_states[scoped_state][1])
     for domain in self._domains_to_enable:
       if domain != self.TRACING_DOMAIN:
         self.SyncRequest('%s.disable' % domain)
@@ -225,6 +256,7 @@ class DevToolsConnection(object):
     self._domains_to_enable.clear()
     self._domain_listeners.clear()
     self._event_listeners.clear()
+    self._scoped_states.clear()
 
   def _OnDataReceived(self, msg):
     if 'method' not in msg:
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index b255656..4a72a4f 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -8,6 +8,7 @@ When executed as a script, loads a trace and outputs the dependencies.
 """
 
 import collections
+import copy
 import logging
 import operator
 
@@ -18,6 +19,7 @@ import request_track
 class RequestDependencyLens(object):
   """Analyses and infers request dependencies."""
   DEPENDENCIES = ('redirect', 'parser', 'script', 'inferred', 'other')
+  CALLFRAMES_KEY = 'callFrames'
   def __init__(self, trace):
     """Initializes an instance of RequestDependencyLens.
 
@@ -89,6 +91,30 @@ class RequestDependencyLens(object):
     initiating_request = self._FindBestMatchingInitiator(request, candidates)
     return (initiating_request, request, 'parser')
 
+  def _FlattenScriptStack(self, stack):
+    """Recursively collapses the stack of asynchronous callstacks.
+
+    A stack has a list of call frames and optionnally a "parent" stack.
+    This function recursively folds the parent stacks into the root stack by
+    concatening all the call frames.
+
+    Args:
+      stack: (dict) the stack that must be flattened
+
+    Returns:
+      A stack with no parent, which is a dictionary with a single "callFrames"
+      key, and no "parent" key.
+    """
+    PARENT_KEY = 'parent'
+    if not PARENT_KEY in stack:
+      return stack
+    stack[self.CALLFRAMES_KEY] += stack[PARENT_KEY][self.CALLFRAMES_KEY]
+    if not PARENT_KEY in stack[PARENT_KEY]:
+      stack.pop(PARENT_KEY)
+    else:
+      stack[PARENT_KEY] = stack[PARENT_KEY][PARENT_KEY]
+    return self._FlattenScriptStack(stack)
+
   def _GetInitiatingRequestScript(self, request):
     STACK_KEY = 'stack'
     if not STACK_KEY in request.initiator:
@@ -96,7 +122,10 @@ class RequestDependencyLens(object):
       return None
     initiating_request = None
     timestamp = request.timing.request_time
-    call_frames = request.initiator[STACK_KEY]['callFrames']
+    # Deep copy the initiator's stack to avoid mutating the input request.
+    stack = self._FlattenScriptStack(
+        copy.deepcopy(request.initiator[STACK_KEY]))
+    call_frames = stack[self.CALLFRAMES_KEY]
     for frame in call_frames:
       url = frame['url']
       candidates = self._FindMatchingRequests(url, timestamp)
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 4035abf..1429a0e 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -83,6 +83,24 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         deps[0],
         self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
 
+  def testAsyncScriptDependency(self):
+    JS_REQUEST_WITH_ASYNC_STACK = Request.FromJsonDict(
+        {'url': 'http://bla.com/cat.js', 'request_id': '1234.14',
+         'initiator': {
+             'type': 'script',
+             'stack': {'callFrames': [],
+                       'parent': {'callFrames': [
+                                      {'url': 'http://bla.com/nyancat.js'}]}}},
+         'timestamp': 10, 'timing': TimingFromDict({})})
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._JS_REQUEST, JS_REQUEST_WITH_ASYNC_STACK])
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0], self._JS_REQUEST.request_id,
+        JS_REQUEST_WITH_ASYNC_STACK.request_id, 'script')
+
   def testParserDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
         [self._REQUEST, self._JS_REQUEST])
diff --git a/loading/request_track.py b/loading/request_track.py
index 89e3a5d..2c94703 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -273,6 +273,10 @@ class RequestTrack(devtools_monitor.Track):
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
+      # Enable asynchronous callstacks to get full javascript callstacks in
+      # initiators
+      self._connection.SetScopedState('Debugger.setAsyncCallStackDepth',
+                                      {'maxDepth': 4}, {'maxDepth': 0}, True)
     # responseReceived message are sometimes duplicated. Records the message to
     # detect this.
     self._request_id_to_response_received = {}

commit d56ead08a3df9f92db5cdade6b8ca9d5a1acf19e
Author: wnwen <wnwen@chromium.org>
Date:   Tue Feb 16 13:29:50 2016 -0800

    Also adds debug build flag.
    
    BUG=583690
    
    Review URL: https://codereview.chromium.org/1597273005
    
    Cr-Original-Commit-Position: refs/heads/master@{#375666}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b0d6314305ddd99c0808a0a41e90b7feb89a79ea

diff --git a/eclipse/.classpath b/eclipse/.classpath
index b740bfe..db88a17 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -148,6 +148,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/web_input_event_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/web_text_input_type"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/window_open_disposition_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/templates/base_build_config_gen"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/base_native_libraries_gen"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/chrome_version_java"/>
     <classpathentry kind="src" path="out/Debug/gen/templates/dom_distiller_core_font_family_java"/>

commit dd141c80296d24f4a165807abd0ac3cc59d5d4f7
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 10:04:54 2016 -0800

    tools/android/loading: TIDs are not globally unique on OS X.
    
    BUG=587097
    
    Review URL: https://codereview.chromium.org/1698183004
    
    Cr-Original-Commit-Position: refs/heads/master@{#375606}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b272be9da423be9a2df94eb35fa98f760dafdcfc

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index 92ffd14..257fdc3 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -25,9 +25,9 @@ class ActivityLens(object):
     """
     self._trace = trace
     events = trace.tracing_track.GetEvents()
-    self._renderer_main_tid = self._GetRendererMainThreadId(events)
+    self._renderer_main_pid_tid = self._GetRendererMainThreadId(events)
     self._tracing = self._trace.tracing_track.TracingTrackForThread(
-        self._renderer_main_tid)
+        self._renderer_main_pid_tid)
 
   @classmethod
   def _GetRendererMainThreadId(cls, events):
@@ -41,28 +41,28 @@ class ActivityLens(object):
       events: [tracing.Event] List of trace events.
 
     Returns:
-      The thread ID (int) of the busiest renderer main thread.
-
+      (PID (int), TID (int)) of the busiest renderer main thread.
     """
-    events_count_per_tid = collections.defaultdict(int)
+    events_count_per_pid_tid = collections.defaultdict(int)
     main_renderer_thread_ids = set()
     for event in events:
       tracing_event = event.tracing_event
+      pid = event.tracing_event['pid']
       tid = event.tracing_event['tid']
-      events_count_per_tid[tid] += 1
+      events_count_per_pid_tid[(pid, tid)] += 1
       if (tracing_event['cat'] == '__metadata'
           and tracing_event['name'] == 'thread_name'
           and event.args['name'] == 'CrRendererMain'):
-        main_renderer_thread_ids.add(tid)
-    tid_events_counts = sorted(events_count_per_tid.items(),
-                               key=operator.itemgetter(1), reverse=True)
-    if (len(tid_events_counts) > 1
-        and tid_events_counts[0][1] < 2 * tid_events_counts[1][1]):
+        main_renderer_thread_ids.add((pid, tid))
+    pid_tid_events_counts = sorted(events_count_per_pid_tid.items(),
+                                   key=operator.itemgetter(1), reverse=True)
+    if (len(pid_tid_events_counts) > 1
+        and pid_tid_events_counts[0][1] < 2 * pid_tid_events_counts[1][1]):
       logging.warning(
           'Several active renderers (%d and %d with %d and %d events).'
-          % (tid_events_counts[0][0], tid_events_counts[1][0],
-             tid_events_counts[0][1], tid_events_counts[1][1]))
-    return tid_events_counts[0][0]
+          % (pid_tid_events_counts[0][0][0], pid_tid_events_counts[1][0][0],
+             pid_tid_events_counts[0][1], pid_tid_events_counts[1][1]))
+    return pid_tid_events_counts[0][0]
 
   def _OverlappingMainRendererThreadEvents(self, start_msec, end_msec):
     return self._tracing.OverlappingEvents(start_msec, end_msec)
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index 3b4062f..f96bbd4 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -28,21 +28,21 @@ class ActivityLensTestCast(unittest.TestCase):
          u'cat': u'__metadata',
          u'name': u'thread_name',
          u'ph': u'M',
-         u'pid': 123,
+         u'pid': 1,
          u'tid': 123,
          u'ts': 0},
         {u'args': {u'name': u'CrRendererMain'},
          u'cat': u'__metadata',
          u'name': u'thread_name',
          u'ph': u'M',
-         u'pid': 1234,
+         u'pid': 1,
          u'tid': first_renderer_tid,
          u'ts': 0},
         {u'args': {u'name': u'CrRendererMain'},
          u'cat': u'__metadata',
          u'name': u'thread_name',
          u'ph': u'M',
-         u'pid': 12345,
+         u'pid': 1,
          u'tid': second_renderer_tid,
          u'ts': 0}]
     raw_events += [
@@ -50,7 +50,7 @@ class ActivityLensTestCast(unittest.TestCase):
          u'cat': u'devtools.timeline,v8',
          u'name': u'FunctionCall',
          u'ph': u'X',
-         u'pid': 32723,
+         u'pid': 1,
          u'tdur': 0,
          u'tid': first_renderer_tid,
          u'ts': 251427174674,
@@ -60,13 +60,25 @@ class ActivityLensTestCast(unittest.TestCase):
          u'cat': u'devtools.timeline,v8',
          u'name': u'FunctionCall',
          u'ph': u'X',
-         u'pid': 1234,
+         u'pid': 1,
          u'tdur': 0,
          u'tid': second_renderer_tid,
          u'ts': 251427174674,
          u'tts': 5107725}] * 150
+    # There are more events from first_renderer_tid when (incorrectly) ignoring
+    # the PID.
+    raw_events += [
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 12,
+         u'tdur': 0,
+         u'tid': first_renderer_tid,
+         u'ts': 251427174674,
+         u'tts': 5107725}] * 100
     events = self._EventsFromRawEvents(raw_events)
-    self.assertEquals(second_renderer_tid,
+    self.assertEquals((1, second_renderer_tid),
                       ActivityLens._GetRendererMainThreadId(events))
 
   def testThreadBusiness(self):
diff --git a/loading/tracing.py b/loading/tracing.py
index 35907d3..7071011 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -86,16 +86,19 @@ class TracingTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
-  def TracingTrackForThread(self, tid):
+  def TracingTrackForThread(self, pid_tid):
     """Returns a new TracingTrack with only the events from a given thread.
 
     Args:
-      tid: (int) Thread ID.
+      pid_tid: ((int, int) PID and TID.
 
     Returns:
       A new instance of TracingTrack.
     """
-    events = [e for e in self._events if e.tracing_event['tid'] == tid]
+    (pid, tid) = pid_tid
+    events = [e for e in self._events
+              if (e.tracing_event['pid'] == pid
+                  and e.tracing_event['tid'] == tid)]
     tracing_track = TracingTrack(None)
     tracing_track._events = events
     return tracing_track
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index a2c9bce..ade5f8d 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -28,12 +28,16 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 15, 'ph': 'D', 'id': 1}]
 
   _EVENTS = [
-      {'ts': 5, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'B'}},
-      {'ts': 3, 'ph': 'X', 'dur': 4, 'tid': 1, 'args': {'name': 'A'}},
-      {'ts': 10, 'ph': 'X', 'dur': 1, 'tid': 2, 'args': {'name': 'C'}},
-      {'ts': 10, 'ph': 'X', 'dur': 2, 'tid': 2, 'args': {'name': 'D'}},
-      {'ts': 13, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'F'}},
-      {'ts': 12, 'ph': 'X', 'dur': 3, 'tid': 1, 'args': {'name': 'E'}}]
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'pid': 2, 'tid': 1, 'args': {'name': 'B'}},
+      {'ts': 3, 'ph': 'X', 'dur': 4, 'pid': 2, 'tid': 1, 'args': {'name': 'A'}},
+      {'ts': 10, 'ph': 'X', 'dur': 1, 'pid': 2, 'tid': 2,
+       'args': {'name': 'C'}},
+      {'ts': 10, 'ph': 'X', 'dur': 2, 'pid': 2, 'tid': 2,
+       'args': {'name': 'D'}},
+      {'ts': 13, 'ph': 'X', 'dur': 1, 'pid': 2, 'tid': 1,
+       'args': {'name': 'F'}},
+      {'ts': 12, 'ph': 'X', 'dur': 3, 'pid': 2, 'tid': 1,
+       'args': {'name': 'E'}}]
 
   def setUp(self):
     self.tree_threshold = _IntervalTree._TRESHOLD
@@ -257,10 +261,10 @@ class TracingTrackTestCase(unittest.TestCase):
     self.track.Handle(
         'Tracing.dataCollected', {'params': {'value': [
             self.EventToMicroseconds(e) for e in self._EVENTS]}})
-    tracing_track = self.track.TracingTrackForThread(1)
+    tracing_track = self.track.TracingTrackForThread((2, 1))
     self.assertTrue(tracing_track is not self.track)
     self.assertEquals(4, len(tracing_track.GetEvents()))
-    tracing_track = self.track.TracingTrackForThread(42)
+    tracing_track = self.track.TracingTrackForThread((2, 42))
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
 

commit 9ae51de2957d5f8f618a7c3183d0db17d848cacb
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 08:46:17 2016 -0800

    tools/android/loading: Add support for device and network emulation.
    
    Review URL: https://codereview.chromium.org/1698293002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375597}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 84efb4de22a98e02ecead98c8bdfe95e9d3f1c7a

diff --git a/loading/analyze.py b/loading/analyze.py
index 1c31de0..17ab770 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -25,6 +25,7 @@ import devil_chromium
 from pylib import constants
 
 import activity_lens
+import chrome_setup
 import content_classification_lens
 import device_setup
 import frame_load_lens
@@ -99,7 +100,7 @@ def _GetPrefetchHtml(graph, name=None):
 
 
 def _LogRequests(url, clear_cache_override=None):
-  """Log requests for a web page.
+  """Logs requests for a web page.
 
   Args:
     url: url to log as string.
@@ -111,8 +112,14 @@ def _LogRequests(url, clear_cache_override=None):
   device = device_setup.GetFirstDevice() if not OPTIONS.local else None
   clear_cache = (clear_cache_override if clear_cache_override is not None
                  else OPTIONS.clear_cache)
+
   with device_setup.DeviceConnection(device) as connection:
+    additional_metadata = {}
+    if OPTIONS.local:
+      additional_metadata = chrome_setup.SetUpEmulationAndReturnMetadata(
+          connection, OPTIONS.emulate_device, OPTIONS.emulate_network)
     trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
+    trace.metadata.update(additional_metadata)
     return trace.ToJsonDict()
 
 
diff --git a/loading/chrome_setup.py b/loading/chrome_setup.py
new file mode 100644
index 0000000..8d7948c
--- /dev/null
+++ b/loading/chrome_setup.py
@@ -0,0 +1,169 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Handles Chrome's configuration."""
+
+import contextlib
+import json
+import shutil
+import subprocess
+import tempfile
+import time
+
+import devtools_monitor
+from options import OPTIONS
+
+
+# Copied from
+# WebKit/Source/devtools/front_end/network/NetworkConditionsSelector.js
+_NETWORK_CONDITIONS = {
+    'Offline': {
+        'download': 0 * 1024 / 8, 'upload': 0 * 1024 / 8, 'latency': 0},
+    'GPRS': {
+        'download': 50 * 1024 / 8, 'upload': 20 * 1024 / 8, 'latency': 500},
+    'Regular 2G': {
+        'download': 250 * 1024 / 8, 'upload': 50 * 1024 / 8, 'latency': 300},
+    'Good 2G': {
+        'download': 450 * 1024 / 8, 'upload': 150 * 1024 / 8, 'latency': 150},
+    'Regular 3G': {
+        'download': 750 * 1024 / 8, 'upload': 250 * 1024 / 8, 'latency': 100},
+    'Good 3G': {
+        'download': 1.5 * 1024 * 1024 / 8, 'upload': 750 * 1024 / 8,
+        'latency': 40},
+    'Regular 4G': {
+        'download': 4 * 1024 * 1024 / 8, 'upload': 3 * 1024 * 1024 / 8,
+        'latency': 20},
+    'DSL': {
+        'download': 2 * 1024 * 1024 / 8, 'upload': 1 * 1024 * 1024 / 8,
+        'latency': 5},
+    'WiFi': {
+        'download': 30 * 1024 * 1024 / 8, 'upload': 15 * 1024 * 1024 / 8,
+        'latency': 2}
+}
+
+
+@contextlib.contextmanager
+def DevToolsConnectionForLocalBinary(flags):
+  """Returns a DevToolsConnection context manager for a local binary.
+
+  Args:
+    flags: ([str]) List of flags to pass to the browser.
+
+  Returns:
+    A DevToolsConnection context manager.
+  """
+  binary_filename = OPTIONS.local_binary
+  profile_dir = OPTIONS.local_profile_dir
+  temp_profile_dir = profile_dir is None
+  if temp_profile_dir:
+    profile_dir = tempfile.mkdtemp()
+  flags.append('--user-data-dir=%s' % profile_dir)
+  chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+  process = subprocess.Popen(
+      [binary_filename] + flags, shell=False, stderr=chrome_out)
+  try:
+    time.sleep(10)
+    yield devtools_monitor.DevToolsConnection(
+        OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+  finally:
+    process.kill()
+    if temp_profile_dir:
+      shutil.rmtree(temp_profile_dir)
+
+
+def SetUpEmulationAndReturnMetadata(connection, emulated_device_name,
+                                    emulated_network_name):
+  """Sets up the device and network emulation and returns the trace metadata.
+
+  Args:
+    connection: (DevToolsConnection)
+    emulated_device_name: (str) Key in the dict returned by
+                          _LoadEmulatedDevices().
+    emulated_network_name: (str) Key in _NETWORK_CONDITIONS.
+
+  Returns:
+    A metadata dict {'deviceEmulation': params, 'networkEmulation': params}.
+  """
+  result = {'deviceEmulation': {}, 'networkEmulation': {}}
+  if emulated_device_name:
+    devices = _LoadEmulatedDevices(OPTIONS.devices_file)
+    emulated_device = devices[emulated_device_name]
+    emulation_params = _SetUpDeviceEmulationAndReturnMetadata(
+        connection, emulated_device)
+    result['deviceEmulation'] = emulation_params
+  if emulated_network_name:
+    params = _NETWORK_CONDITIONS[emulated_network_name]
+    _SetUpNetworkEmulation(
+        connection, params['latency'], params['download'], params['upload'])
+    result['networkEmulation'] = params
+  return result
+
+
+def _LoadEmulatedDevices(filename):
+  """Loads a list of emulated devices from the DevTools JSON registry.
+
+  Args:
+    filename: (str) Path to the JSON file.
+
+  Returns:
+    {'device_name': device}
+  """
+  json_dict = json.load(open(filename, 'r'))
+  devices = {}
+  for device in json_dict['extensions']:
+    device = device['device']
+    devices[device['title']] = device
+  return devices
+
+
+def _GetDeviceEmulationMetadata(device):
+  """Returns the metadata associated with a given device."""
+  return {'width': device['screen']['vertical']['width'],
+          'height': device['screen']['vertical']['height'],
+          'deviceScaleFactor': device['screen']['device-pixel-ratio'],
+          'mobile': 'mobile' in device['capabilities'],
+          'userAgent': device['user-agent']}
+
+
+def _SetUpDeviceEmulationAndReturnMetadata(connection, device):
+  """Configures an instance of Chrome for device emulation.
+
+  Args:
+    connection: (DevToolsConnection)
+    device: (dict) As returned by LoadEmulatedDevices().
+
+  Returns:
+    A dict containing the device emulation metadata.
+  """
+  print device
+  res = connection.SyncRequest('Emulation.canEmulate')
+  assert res['result'], 'Cannot set device emulation.'
+  data = _GetDeviceEmulationMetadata(device)
+  connection.SyncRequestNoResponse(
+      'Emulation.setDeviceMetricsOverride',
+      {'width': data['width'],
+       'height': data['height'],
+       'deviceScaleFactor': data['deviceScaleFactor'],
+       'mobile': data['mobile'],
+       'fitWindow': True})
+  connection.SyncRequestNoResponse('Network.setUserAgentOverride',
+                                   {'userAgent': data['userAgent']})
+  return data
+
+
+def _SetUpNetworkEmulation(connection, latency, download, upload):
+  """Configures an instance of Chrome for network emulation.
+
+  Args:
+    connection: (DevToolsConnection)
+    latency: (float) Latency in ms.
+    download: (float) Download speed (Bytes / s).
+    upload: (float) Upload speed (Bytes / s).
+  """
+  res = connection.SyncRequest('Network.canEmulateNetworkConditions')
+  assert res['result'], 'Cannot set network emulation.'
+  connection.SyncRequestNoResponse(
+      'Network.emulateNetworkConditions',
+      {'offline': False, 'latency': latency, 'downloadThroughput': download,
+       'uploadThroughput': upload})
diff --git a/loading/device_setup.py b/loading/device_setup.py
index b758eec..0fe95fd 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -33,6 +33,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
 import adb_install_cert
 import certutils
 
+import chrome_setup
 import devtools_monitor
 import options
 
@@ -40,18 +41,6 @@ import options
 OPTIONS = options.OPTIONS
 
 
-@contextlib.contextmanager
-def TemporaryDirectory():
-  """Returns a freshly-created directory that gets automatically deleted after
-  usage.
-  """
-  name = tempfile.mkdtemp()
-  try:
-    yield name
-  finally:
-    shutil.rmtree(name)
-
-
 class DeviceSetupException(Exception):
   def __init__(self, msg):
     super(DeviceSetupException, self).__init__(msg)
@@ -185,6 +174,31 @@ def WprHost(device, wpr_archive_path, record=False,
 
 
 @contextlib.contextmanager
+def _DevToolsConnectionOnDevice(device, flags):
+  """Returns a DevToolsConnection context manager for a given device.
+
+  Args:
+    device: Device to connect to.
+    flags: ([str]) List of flags.
+
+  Returns:
+    A DevToolsConnection context manager.
+  """
+  package_info = OPTIONS.ChromePackage()
+  command_line_path = '/data/local/chrome-command-line'
+  _SetUpDevice(device, package_info)
+  with FlagReplacer(device, command_line_path, flags):
+    start_intent = intent.Intent(
+        package=package_info.package, activity=package_info.activity,
+        data='about:blank')
+    device.StartActivity(start_intent, blocking=True)
+    time.sleep(2)
+    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
+                     'localabstract:chrome_devtools_remote'):
+      yield devtools_monitor.DevToolsConnection(
+          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+
+
 def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
 
@@ -198,8 +212,6 @@ def DeviceConnection(device, additional_flags=None):
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
   """
-  package_info = OPTIONS.ChromePackage()
-  command_line_path = '/data/local/chrome-command-line'
   new_flags = ['--disable-fre',
                '--enable-test-events',
                '--remote-debugging-port=%d' % OPTIONS.devtools_port]
@@ -207,41 +219,8 @@ def DeviceConnection(device, additional_flags=None):
     new_flags.append('--no-sandbox')
   if additional_flags != None:
     new_flags.extend(additional_flags)
+
   if device:
-    _SetUpDevice(device, package_info)
-  with FlagReplacer(device, command_line_path, new_flags):
-    host_process = None
-    if device:
-      start_intent = intent.Intent(
-          package=package_info.package, activity=package_info.activity,
-          data='about:blank')
-      device.StartActivity(start_intent, blocking=True)
-    else:
-      # Run on the host. We don't care about startup time so will skip the about
-      # page.
-      assert os.path.exists(OPTIONS.local_binary)
-
-      local_profile_dir = OPTIONS.local_profile_dir
-      if not local_profile_dir:
-        local_profile_dir = TemporaryDirectory()
-
-      new_flags.append('--user-data-dir=%s' % local_profile_dir)
-      chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
-      host_process = subprocess.Popen(
-          [OPTIONS.local_binary] + new_flags,
-          shell=False, stdout=chrome_out, stderr=chrome_out)
-    if device:
-      time.sleep(2)
-    else:
-      # TODO(mattcary): This seems to be related to chrome startup. There should
-      # be a way to ping chrome --- maybe keep trying to connect to the devtools
-      # port?
-      time.sleep(10)
-    try:
-      with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
-                       'localabstract:chrome_devtools_remote'):
-        yield devtools_monitor.DevToolsConnection(
-            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-    finally:
-      if host_process:
-        host_process.kill()
+    return _DevToolsConnectionOnDevice(device, new_flags)
+  else:
+    return chrome_setup.DevToolsConnectionForLocalBinary(new_flags)
diff --git a/loading/options.py b/loading/options.py
index 1d9ff72..04b5bb3 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -36,6 +36,13 @@ class Options(object):
              'pass --no-sandbox to browser (local run only; see also '
              'https://chromium.googlesource.com/chromium/src/+/master/'
              'docs/linux_suid_sandbox_development.md)'),
+            ('devices_file', _SRC_DIR + '/third_party/WebKit/Source/devtools'
+             '/front_end/emulated_devices/module.json', 'File containing a'
+             ' list of emulated devices characteristics.'),
+            ('emulate_device', '', 'Name of the device to emulate. Must be '
+             'present in --devices_file, or empty for no emulation.'),
+            ('emulate_network', '', 'Type of network emulation. Empty for no'
+             ' emulation.')
           ]
 
 

commit c5e28a46c31de9e82ae2e36e8c10b01c310be612
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 08:44:29 2016 -0800

    sandwich: Implements reload cache operation to compare with push.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1701973002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375594}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bbcc51966eccb4e6dc7c4ecf2ad8e04add8ac1bc

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index a99a5a3..ef7fca8 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -86,15 +86,13 @@ def _ReadUrlsFromJobDescription(job_name):
   raise Exception('Job description does not define a list named "urls"')
 
 
-def _SaveChromeTrace(events, directory, subdirectory):
+def _SaveChromeTrace(events, target_directory):
   """Saves the trace events, ignores IO errors.
 
   Args:
     events: a dict as returned by TracingTrack.ToJsonDict()
-    directory: directory name contining all traces
-    subdirectory: directory name to create this particular trace in
+    target_directory: Directory path where trace is created.
   """
-  target_directory = os.path.join(directory, subdirectory)
   filename = os.path.join(target_directory, 'trace.json')
   try:
     os.makedirs(target_directory)
@@ -288,7 +286,7 @@ def main():
   parser.add_argument('--repeat', default=1, type=int,
                       help='How many times to run the job')
   parser.add_argument('--cache-op',
-                      choices=['clear', 'save', 'push'],
+                      choices=['clear', 'save', 'push', 'reload'],
                       default='clear',
                       help='Configures cache operation to do before launching '
                           +'Chrome. (Default is clear).')
@@ -329,26 +327,37 @@ def main():
 
   with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
       args.disable_wpr_script_injection) as additional_flags:
+    def _RunNavigation(url, clear_cache, trace_id):
+      with device_setup.DeviceConnection(
+          device=device,
+          additional_flags=additional_flags) as connection:
+        if clear_cache:
+          connection.ClearCache()
+        page_track.PageTrack(connection)
+        tracing_track = tracing.TracingTrack(connection,
+            categories=pull_sandwich_metrics.CATEGORIES)
+        connection.SetUpMonitoring()
+        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+        connection.StartMonitoring()
+        if trace_id != None:
+          trace_target_directory = os.path.join(args.output, str(trace_id))
+          _SaveChromeTrace(tracing_track.ToJsonDict(), trace_target_directory)
+
     for _ in xrange(args.repeat):
       for url in job_urls:
-        if args.cache_op == 'push':
+        clear_cache = False
+        if args.cache_op == 'clear':
+          clear_cache = True
+        elif args.cache_op == 'push':
           device.KillAll(_CHROME_PACKAGE, quiet=True)
           _PushBrowserCache(device, local_cache_directory_path)
-        with device_setup.DeviceConnection(
-            device=device,
-            additional_flags=additional_flags) as connection:
-          if (not run_infos['urls'] and args.cache_op == 'save' or
-              args.cache_op == 'clear'):
-            connection.ClearCache()
-          page_track.PageTrack(connection)
-          tracing_track = tracing.TracingTrack(connection,
-              categories=pull_sandwich_metrics.CATEGORIES)
-          connection.SetUpMonitoring()
-          connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-          connection.StartMonitoring()
-          _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
-              str(len(run_infos['urls'])))
-          run_infos['urls'].append(url)
+        elif args.cache_op == 'reload':
+          _RunNavigation(url, clear_cache=True, trace_id=None)
+        elif args.cache_op == 'save':
+          clear_cache = not run_infos['urls']
+        _RunNavigation(url, clear_cache=clear_cache,
+                       trace_id=len(run_infos['urls']))
+        run_infos['urls'].append(url)
 
   if local_cache_directory_path:
     shutil.rmtree(local_cache_directory_path)

commit 622645e5b637a970247e89b7b33aba4196685181
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 08:19:37 2016 -0800

    tools/android/loading: Add blundell and droger to OWNERS.
    
    Review URL: https://codereview.chromium.org/1698393002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375590}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a0630c858350f2edc8b6dbf3c9d83681bfd3475d

diff --git a/loading/OWNERS b/loading/OWNERS
index 0b168ed..52bebbb 100644
--- a/loading/OWNERS
+++ b/loading/OWNERS
@@ -1,4 +1,6 @@
+blundell@chromium.org
+droger@chromium.org
 lizeb@chromium.org
-pasko@chromium.org
 # Not a committer yet, but OWNER nonetheless:
 # mattcary@chromium.org
+pasko@chromium.org

commit 6413fcce09661de6e9b4dadec177bd95970735e3
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 07:53:03 2016 -0800

    sandwich: Aggregates metrics per URLs
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1694253002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375583}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: eb00ce505fec519a6ca3206e4e3d69e60837abd4

diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
index 1558243..df24f1c 100755
--- a/loading/pull_sandwich_metrics.py
+++ b/loading/pull_sandwich_metrics.py
@@ -20,6 +20,7 @@ CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
 
 _CSV_FIELD_NAMES = [
     'id',
+    'url',
     'total_load',
     'onload',
     'browser_malloc_avg',
@@ -145,6 +146,10 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
     List of dictionaries with all _CSV_FIELD_NAMES's field set.
   """
   assert os.path.isdir(output_directory_path)
+  run_infos = None
+  with open(os.path.join(output_directory_path, 'run_infos.json')) as f:
+    run_infos = json.load(f)
+  assert run_infos
   metrics = []
   for node_name in os.listdir(output_directory_path):
     if not os.path.isdir(os.path.join(output_directory_path, node_name)):
@@ -161,6 +166,7 @@ def _PullMetricsFromOutputDirectory(output_directory_path):
       trace = json.load(trace_file)
       trace_metrics = _PullMetricsFromTrace(trace)
       trace_metrics['id'] = page_id
+      trace_metrics['url'] = run_infos['urls'][page_id]
       metrics.append(trace_metrics)
   assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
                             'run directory.').format(output_directory_path)
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
index ad9fbe1..3c98508 100644
--- a/loading/pull_sandwich_metrics_unittest.py
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -146,6 +146,8 @@ class PageTrackTest(unittest.TestCase):
 
   def testCommandLine(self):
     tmp_dir = tempfile.mkdtemp()
+    with open(os.path.join(tmp_dir, 'run_infos.json'), 'w') as out_file:
+      json.dump({'urls': ['a.com', 'b.com', 'c.org']}, out_file)
     for dirname in ['1', '2', 'whatever']:
       os.mkdir(os.path.join(tmp_dir, dirname))
       with open(os.path.join(tmp_dir, dirname, 'trace.json'), 'w') as out_file:
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index f72ba82..a99a5a3 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -312,6 +312,11 @@ def main():
   else:
     _CleanPreviousTraces(args.output)
 
+  run_infos = {
+    'cache-op': args.cache_op,
+    'job': args.job,
+    'urls': []
+  }
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
   local_cache_archive_path = os.path.join(args.output, 'cache.zip')
@@ -324,7 +329,6 @@ def main():
 
   with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
       args.disable_wpr_script_injection) as additional_flags:
-    pages_loaded = 0
     for _ in xrange(args.repeat):
       for url in job_urls:
         if args.cache_op == 'push':
@@ -333,7 +337,7 @@ def main():
         with device_setup.DeviceConnection(
             device=device,
             additional_flags=additional_flags) as connection:
-          if (pages_loaded == 0 and args.cache_op == 'save' or
+          if (not run_infos['urls'] and args.cache_op == 'save' or
               args.cache_op == 'clear'):
             connection.ClearCache()
           page_track.PageTrack(connection)
@@ -342,9 +346,9 @@ def main():
           connection.SetUpMonitoring()
           connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
           connection.StartMonitoring()
-          pages_loaded += 1
           _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
-              str(pages_loaded))
+              str(len(run_infos['urls'])))
+          run_infos['urls'].append(url)
 
   if local_cache_directory_path:
     shutil.rmtree(local_cache_directory_path)
@@ -360,6 +364,9 @@ def main():
     _ZipDirectoryContent(cache_directory_path, local_cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
+  with open(os.path.join(args.output, 'run_infos.json'), 'w') as file_output:
+    json.dump(run_infos, file_output, indent=2)
+
 
 if __name__ == '__main__':
   sys.exit(main())

commit 376bc675924d93f92310101039462fd3a2447709
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 16 07:46:38 2016 -0800

    Remove some noisy logging and optionally squash local chrome device output.
    
    This will make my upcoming webserver integration tests nicer.
    
    Review URL: https://codereview.chromium.org/1701723003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375580}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0cbea5abf37d765ec043f66edc51daed33ac3f75

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 73b9d85..b758eec 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -226,8 +226,10 @@ def DeviceConnection(device, additional_flags=None):
         local_profile_dir = TemporaryDirectory()
 
       new_flags.append('--user-data-dir=%s' % local_profile_dir)
-      host_process = subprocess.Popen([OPTIONS.local_binary] + new_flags,
-                                      shell=False)
+      chrome_out = None if OPTIONS.local_noisy else file('/dev/null', 'w')
+      host_process = subprocess.Popen(
+          [OPTIONS.local_binary] + new_flags,
+          shell=False, stdout=chrome_out, stderr=chrome_out)
     if device:
       time.sleep(2)
     else:
diff --git a/loading/options.py b/loading/options.py
index f477cd2..1d9ff72 100644
--- a/loading/options.py
+++ b/loading/options.py
@@ -28,6 +28,8 @@ class Options(object):
              'port for devtools websocket connection'),
             ('local_binary', 'out/Release/chrome',
              'chrome binary for local runs'),
+            ('local_noisy', False,
+             'Enable local chrome console output'),
             ('local_profile_dir', '',
              'profile directory to use for local runs'),
             ('no_sandbox', False,
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 61015d2..34293f8 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -41,7 +41,6 @@ def MonitorUrl(connection, url, clear_cache=False):
   Returns:
     loading_trace.LoadingTrace.
   """
-  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
   page = page_track.PageTrack(connection)
   request = request_track.RequestTrack(connection)
   trace = tracing.TracingTrack(connection)

commit c09a73626e42da061de215a88651108ca5fe52ef
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 16 07:00:47 2016 -0800

    tools/android/loading: Speed up trace events processing.
    
    The previous way to index trace events allowed for O(log n) query of
    in-flight events at a given time, and O(n) constrction time, but at the
    cost of replicating events up to p times (p being the maximum number of
    concurrent events). It turns out that p is fairly high.
    
    This replaces this with a tree, which is worse, but in practice faster.
    On a typical loading trace, creating the PNG graph goes from 3m30s to ~20s.
    
    Review URL: https://codereview.chromium.org/1694223002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375575}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 1570bad784b3dd9fa1daf0bf125635a2b77f08cb

diff --git a/loading/tracing.py b/loading/tracing.py
index 37ee7ce..35907d3 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -7,6 +7,7 @@
 import bisect
 import itertools
 import logging
+import operator
 
 import devtools_monitor
 
@@ -40,11 +41,10 @@ class TracingTrack(devtools_monitor.Track):
 
     if connection:
       connection.SyncRequestNoResponse('Tracing.start', params)
-    self._events = []
 
-    self._event_msec_index = None
-    self._event_lists = None
+    self._events = []
     self._base_msec = None
+    self._interval_tree = None
 
   def Handle(self, method, event):
     for e in event['params']['value']:
@@ -52,10 +52,9 @@ class TracingTrack(devtools_monitor.Track):
       self._events.append(event)
       if self._base_msec is None or event.start_msec < self._base_msec:
         self._base_msec = event.start_msec
-    # Just invalidate our indices rather than trying to be fancy and
-    # incrementally update.
-    self._event_msec_index = None
-    self._event_lists = None
+    # Invalidate our index rather than trying to be fancy and incrementally
+    # update.
+    self._interval_tree = None
 
   def GetFirstEventMillis(self):
     """Find the canonical start time for this track.
@@ -80,18 +79,9 @@ class TracingTrack(devtools_monitor.Track):
       sample and counter) events are never included. Event end times are
       exclusive, so that an event ending at the usec parameter will not be
       returned.
-      TODO(mattcary): currently live objects are included. If this is too big we
-      may break that out into a separate index.
     """
     self._IndexEvents()
-    idx = bisect.bisect_right(self._event_msec_index, msec) - 1
-    if idx < 0:
-      return []
-    events = self._event_lists[idx]
-    assert events.start_msec <= msec
-    if not events or events.end_msec < msec:
-      return []
-    return events.event_list
+    return self._interval_tree.EventsAt(msec)
 
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
@@ -125,31 +115,30 @@ class TracingTrack(devtools_monitor.Track):
         tracing_track._base_msec = e.start_msec
     return tracing_track
 
-  def OverlappingEvents(self, start_msec, end_msec):
-    """Gets the list of events overlapping with an interval.
+  def _IndexEvents(self, strict=False):
+    if self._interval_tree:
+      return
+    complete_events = []
+    spanning_events = self._SpanningEvents()
+    for event in self._events:
+      if not event.IsIndexable():
+        continue
+      if event.IsComplete():
+        complete_events.append(event)
+        continue
+      matched_event = spanning_events.Match(event, strict)
+      if matched_event is not None:
+        complete_events.append(matched_event)
+    self._interval_tree = _IntervalTree.FromEvents(complete_events)
 
-    Args:
-      start_msec: the start of the range to query, in milliseconds, inclusive.
-      end_msec: the end of the range to query, in milliseconds, inclusive.
+    if strict and spanning_events.HasPending():
+      raise devtools_monitor.DevToolsConnectionException(
+          'Pending spanning events: %s' %
+          '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
 
-    Returns:
-      List of events overlapping with the range. Events are overlapping only if
-      the overlap is strictly larger than 0.
-    """
+  def OverlappingEvents(self, start_msec, end_msec):
     self._IndexEvents()
-    low_idx = bisect.bisect_left(self._event_msec_index, start_msec) - 1
-    high_idx = bisect.bisect_right(self._event_msec_index, end_msec)
-    matched_events = set()
-    for i in xrange(max(0, low_idx), high_idx):
-      if self._event_lists[i]:
-        for e in self._event_lists[i].event_list:
-          if e.end_msec is None:
-            continue
-          overlap_duration = max(
-              0, min(end_msec, e.end_msec) - max(start_msec, e.start_msec))
-          if overlap_duration > 0:
-            matched_events.add(e)
-    return list(matched_events)
+    return self._interval_tree.OverlappingEvents(start_msec, end_msec)
 
   def EventsEndingBetween(self, start_msec, end_msec):
     """Gets the list of events ending within an interval.
@@ -165,55 +154,9 @@ class TracingTrack(devtools_monitor.Track):
     return [e for e in overlapping_events
             if start_msec <= e.end_msec <= end_msec]
 
-  def _IndexEvents(self, strict=False):
-    """Computes index for in-flight events.
-
-    Creates a list of timestamps where events start or end, and tracks the
-    current set of in-flight events at the instant after each timestamp. To do
-    this we have to synthesize ending events for complete events, as well as
-    join and track the nesting of async, flow and other spanning events.
-
-    Events such as instant and counter events that aren't indexable are skipped.
-    """
-    if self._event_msec_index is not None:
-      return  # Already indexed.
-
-    if not self._events:
-      raise devtools_monitor.DevToolsConnectionException('No events to index')
-
-    self._event_msec_index = []
-    self._event_lists = []
-    synthetic_events = []
-    for e in self._events:
-      synthetic_events.extend(e.Synthesize())
-    synthetic_events.sort(key=lambda e: e.start_msec)
-    current_events = set()
-    next_idx = 0
-    spanning_events = self._SpanningEvents()
-    while next_idx < len(synthetic_events):
-      current_msec = synthetic_events[next_idx].start_msec
-      while next_idx < len(synthetic_events):
-        event = synthetic_events[next_idx]
-        assert event.IsIndexable()
-        if event.start_msec > current_msec:
-          break
-        matched_event = spanning_events.Match(event, strict)
-        if matched_event is not None:
-          event = matched_event
-        if not event.synthetic and (
-            event.end_msec is None or event.end_msec >= current_msec):
-          current_events.add(event)
-        next_idx += 1
-      current_events -= set([
-          e for e in current_events
-          if e.end_msec is not None and e.end_msec <= current_msec])
-      self._event_msec_index.append(current_msec)
-      self._event_lists.append(self._EventList(current_events))
-
-    if strict and spanning_events.HasPending():
-      raise devtools_monitor.DevToolsConnectionException(
-          'Pending spanning events: %s' %
-          '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
+  def _GetEvents(self):
+    self._IndexEvents()
+    return self._interval_tree.GetEvents()
 
   class _SpanningEvents(object):
     def __init__(self):
@@ -315,31 +258,6 @@ class TracingTrack(devtools_monitor.Track):
       start.SetClose(event)
       return start
 
-  class _EventList(object):
-    def __init__(self, events):
-      self._events = [e for e in events]
-      if self._events:
-        self._start_msec = min(e.start_msec for e in self._events)
-        # Event end times may be changed after this list is created so the end
-        # can't be cached.
-      else:
-        self._start_msec = self._end_msec = None
-
-    @property
-    def event_list(self):
-      return self._events
-
-    @property
-    def start_msec(self):
-      return self._start_msec
-
-    @property
-    def end_msec(self):
-      return max(e.end_msec for e in self._events)
-
-    def __nonzero__(self):
-      return bool(self._events)
-
 
 class Event(object):
   """Wraps a tracing event."""
@@ -411,6 +329,9 @@ class Event(object):
         'M'             # Metadata
         ]
 
+  def IsComplete(self):
+    return self.type == 'X'
+
   def Synthesize(self):
     """Expand into synthetic events.
 
@@ -421,7 +342,7 @@ class Event(object):
     """
     if not self.IsIndexable():
       return []
-    if self.type == 'X':
+    if self.IsComplete():
       # Tracing event timestamps are microseconds!
       return [self, Event({'ts': self.end_msec * 1000}, synthetic=True)]
     return [self]
@@ -455,3 +376,81 @@ class Event(object):
   @classmethod
   def FromJsonDict(cls, json_dict):
     return Event(json_dict)
+
+
+class _IntervalTree(object):
+  """Simple interval tree. This is not an optimal one, as the split is done with
+  an equal number of events on each side, according to start time.
+  """
+  _TRESHOLD = 100
+  def __init__(self, start, end, events):
+    """Builds an interval tree.
+
+    Args:
+      start: start timestamp of this node, in ms.
+      end: end timestamp covered by this node, in ms.
+      events: Iterable of objects having start_msec and end_msec fields. Has to
+              be sorted by start_msec.
+    """
+    self.start = start
+    self.end = end
+    self._events = events
+    self._left = self._right = None
+    if len(self._events) > self._TRESHOLD:
+      self._Divide()
+
+  @classmethod
+  def FromEvents(cls, events):
+    """Returns an IntervalTree instance from a list of events."""
+    filtered_events = [e for e in events
+                       if e.start_msec is not None and e.end_msec is not None]
+    filtered_events.sort(key=operator.attrgetter('start_msec'))
+    start = min(event.start_msec for event in filtered_events)
+    end = max(event.end_msec for event in filtered_events)
+    return _IntervalTree(start, end, filtered_events)
+
+  def OverlappingEvents(self, start, end):
+    if min(end, self.end) - max(start, self.start) <= 0:
+      return set()
+    elif self._IsLeaf():
+      result = set()
+      for event in self._events:
+        if self._Overlaps(event, start, end):
+          result.add(event)
+      return result
+    else:
+      return (self._left.OverlappingEvents(start, end)
+              | self._right.OverlappingEvents(start, end))
+
+  def EventsAt(self, timestamp):
+    result = set()
+    if self._IsLeaf():
+      for event in self._events:
+        if event.start_msec <= timestamp < event.end_msec:
+          result.add(event)
+    else:
+      if self._left.start <= timestamp < self._left.end:
+        result |= self._left.EventsAt(timestamp)
+      if self._right.start <= timestamp < self._right.end:
+        result |= self._right.EventsAt(timestamp)
+    return result
+
+  def GetEvents(self):
+    return self._events
+
+  def _Divide(self):
+    middle = len(self._events) / 2
+    left_events = self._events[:middle]
+    right_events = self._events[middle:]
+    left_end = max(e.end_msec for e in left_events)
+    right_start = min(e.start_msec for e in right_events)
+    self._left = _IntervalTree(self.start, left_end, left_events)
+    self._right = _IntervalTree(right_start, self.end, right_events)
+
+  def _IsLeaf(self):
+    return self._left is None
+
+  @classmethod
+  def _Overlaps(cls, event, start, end):
+    return (min(end, event.end_msec) - max(start, event.start_msec) > 0
+            or start <= event.start_msec < end)  # For instant events.
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 9c1ac3a..a2c9bce 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -2,13 +2,15 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import collections
 import copy
 import logging
+import operator
 import unittest
 
 import devtools_monitor
 
-from tracing import (Event, TracingTrack)
+from tracing import (Event, TracingTrack, _IntervalTree)
 
 
 class TracingTrackTestCase(unittest.TestCase):
@@ -34,8 +36,13 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 12, 'ph': 'X', 'dur': 3, 'tid': 1, 'args': {'name': 'E'}}]
 
   def setUp(self):
+    self.tree_threshold = _IntervalTree._TRESHOLD
+    _IntervalTree._TRESHOLD = 2  # Expose more edge cases in the tree.
     self.track = TracingTrack(None)
 
+  def tearDown(self):
+    _IntervalTree._TRESHOLD = self.tree_threshold
+
   def EventToMicroseconds(self, event):
     result = copy.deepcopy(event)
     if 'ts' in result:
@@ -257,5 +264,67 @@ class TracingTrackTestCase(unittest.TestCase):
     self.assertEquals(0, len(tracing_track.GetEvents()))
 
 
+class IntervalTreeTestCase(unittest.TestCase):
+  class FakeEvent(object):
+    def __init__(self, start_msec, end_msec):
+      self.start_msec = start_msec
+      self.end_msec = end_msec
+
+    def __eq__(self, o):
+      return self.start_msec == o.start_msec and self.end_msec == o.end_msec
+
+  _COUNT = 1000
+
+  def testCreateTree(self):
+    events = [self.FakeEvent(100 * i, 100 * (i + 1))
+              for i in range(self._COUNT)]
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(0, tree.start)
+    self.assertEquals(100 * self._COUNT, tree.end)
+    self.assertFalse(tree._IsLeaf())
+
+  def testEventsAt(self):
+    events = ([self.FakeEvent(100 * i, 100 * (i + 1))
+               for i in range(self._COUNT)]
+              + [self.FakeEvent(100 * i + 50, 100 * i + 150)
+                 for i in range(self._COUNT)])
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(0, tree.start)
+    self.assertEquals(100 * self._COUNT + 50, tree.end)
+    self.assertFalse(tree._IsLeaf())
+    for i in range(self._COUNT):
+      self.assertEquals(2, len(tree.EventsAt(100 * i + 75)))
+    # Add instant events, check that they are excluded.
+    events += [self.FakeEvent(100 * i + 75, 100 * i + 75)
+               for i in range(self._COUNT)]
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(3 * self._COUNT, len(tree._events))
+    for i in range(self._COUNT):
+      self.assertEquals(2, len(tree.EventsAt(100 * i + 75)))
+
+  def testOverlappingEvents(self):
+    events = ([self.FakeEvent(100 * i, 100 * (i + 1))
+               for i in range(self._COUNT)]
+              + [self.FakeEvent(100 * i + 50, 100 * i + 150)
+                 for i in range(self._COUNT)])
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(0, tree.start)
+    self.assertEquals(100 * self._COUNT + 50, tree.end)
+    self.assertFalse(tree._IsLeaf())
+    # 400 -> 500, 450 -> 550, 500 -> 600
+    self.assertEquals(3, len(tree.OverlappingEvents(450, 550)))
+    overlapping = sorted(
+        tree.OverlappingEvents(450, 550), key=operator.attrgetter('start_msec'))
+    self.assertEquals(self.FakeEvent(400, 500), overlapping[0])
+    self.assertEquals(self.FakeEvent(450, 550), overlapping[1])
+    self.assertEquals(self.FakeEvent(500, 600), overlapping[2])
+    self.assertEquals(8, len(tree.OverlappingEvents(450, 800)))
+    # Add instant events, check that they are included.
+    events += [self.FakeEvent(500, 500) for i in range(10)]
+    tree = _IntervalTree.FromEvents(events)
+    self.assertEquals(3 + 10, len(tree.OverlappingEvents(450, 550)))
+    self.assertEquals(8 + 10, len(tree.OverlappingEvents(450, 800)))
+
+
 if __name__ == '__main__':
   unittest.main()

commit 1ef48539c8d1598197c5ac3d7310ad60b5ea06c7
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 06:39:07 2016 -0800

    sandwich: Adds command line flag to disable WPR injections
    
    By default, the Web Page Replay server automatically inject a
    javascript (deterministic.js) file that overrides Math.random()
    and Date() to implementations that return a deterministic value.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1698883002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375569}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9d7185f35638545a782d62619d5d73a9ac44552c

diff --git a/loading/device_setup.py b/loading/device_setup.py
index a056ec2..73b9d85 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -115,7 +115,8 @@ def _SetUpDevice(device, package_info):
 
 
 @contextlib.contextmanager
-def WprHost(device, wpr_archive_path, record=False):
+def WprHost(device, wpr_archive_path, record=False,
+            disable_script_injection=False):
   """Launches web page replay host.
 
   Args:
@@ -140,6 +141,11 @@ def WprHost(device, wpr_archive_path, record=False):
   else:
     assert os.path.exists(wpr_archive_path)
 
+  if disable_script_injection:
+    # Remove default WPR injected scripts like deterministic.js which
+    # overrides Math.random.
+    wpr_server_args.extend(['--inject_scripts', ''])
+
   # Deploy certification authority to the device.
   temp_certificate_dir = tempfile.mkdtemp()
   wpr_ca_cert_path = os.path.join(temp_certificate_dir, 'testca.pem')
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 9274819..f72ba82 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -296,6 +296,11 @@ def main():
                       help='Web page replay archive to load job\'s urls from.')
   parser.add_argument('--wpr-record', default=False, action='store_true',
                       help='Record web page replay archive.')
+  parser.add_argument('--disable-wpr-script-injection', default=False,
+                      action='store_true',
+                      help='Disable WPR default script injection such as ' +
+                          'overriding javascript\'s Math.random() and Date() ' +
+                          'with deterministic implementations.')
   args = parser.parse_args()
 
   if not os.path.isdir(args.output):
@@ -317,9 +322,8 @@ def main():
     local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
     _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
 
-  with device_setup.WprHost(device,
-                            args.wpr_archive,
-                            args.wpr_record) as additional_flags:
+  with device_setup.WprHost(device, args.wpr_archive, args.wpr_record,
+      args.disable_wpr_script_injection) as additional_flags:
     pages_loaded = 0
     for _ in xrange(args.repeat):
       for url in job_urls:

commit 4528545fb389017a73627909782f53e88ce0828e
Author: gabadie <gabadie@chromium.org>
Date:   Tue Feb 16 05:40:23 2016 -0800

    sandwich: Pushes locally saved HTTP cache to the device.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1692873003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375559}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c240a2a6b423ed0afcc4c10ad9a03ddf544ede51

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 242cfbf..9274819 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -12,10 +12,12 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
+from datetime import datetime
 import json
 import logging
 import os
 import shutil
+import subprocess
 import sys
 import tempfile
 import time
@@ -59,6 +61,9 @@ _CHROME_PACKAGE = (
 # operations, such as opening the launcher activity.
 _TIME_TO_DEVICE_IDLE_SECONDS = 2
 
+# Cache directory's path on the device.
+_REMOTE_CACHE_DIRECTORY = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
+
 
 def _ReadUrlsFromJobDescription(job_name):
   """Retrieves the list of URLs associated with the job name."""
@@ -104,6 +109,17 @@ def _UpdateTimestampFromAdbStat(filename, stat):
   os.utime(filename, (stat.st_time, stat.st_time))
 
 
+def _AdbShell(adb, cmd):
+  adb.Shell(subprocess.list2cmdline(cmd))
+
+
+def _AdbUtime(adb, filename, timestamp):
+  """Adb equivalent of os.utime(filename, (timestamp, timestamp))
+  """
+  touch_stamp = datetime.fromtimestamp(timestamp).strftime('%Y%m%d.%H%M%S')
+  _AdbShell(adb, ['touch', '-t', touch_stamp, filename])
+
+
 def _PullBrowserCache(device):
   """Pulls the browser cache from the device and saves it locally.
 
@@ -114,14 +130,13 @@ def _PullBrowserCache(device):
     Temporary directory containing all the browser cache.
   """
   save_target = tempfile.mkdtemp(suffix='.cache')
-  cache_directory = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
-  for filename, stat in device.adb.Ls(cache_directory):
+  for filename, stat in device.adb.Ls(_REMOTE_CACHE_DIRECTORY):
     if filename == '..':
       continue
     if filename == '.':
       cache_directory_stat = stat
       continue
-    original_file = os.path.join(cache_directory, filename)
+    original_file = os.path.join(_REMOTE_CACHE_DIRECTORY, filename)
     saved_file = os.path.join(save_target, filename)
     device.adb.Pull(original_file, saved_file)
     _UpdateTimestampFromAdbStat(saved_file, stat)
@@ -145,6 +160,35 @@ def _PullBrowserCache(device):
   return save_target
 
 
+def _PushBrowserCache(device, local_cache_path):
+  """Pushes the browser cache saved locally to the device.
+
+  Args:
+    device: Android device.
+    local_cache_path: The directory's path containing the cache locally.
+  """
+  # Clear previous cache.
+  _AdbShell(device.adb, ['rm', '-rf', _REMOTE_CACHE_DIRECTORY])
+  _AdbShell(device.adb, ['mkdir', _REMOTE_CACHE_DIRECTORY])
+
+  # Push cache content.
+  device.adb.Push(local_cache_path, _REMOTE_CACHE_DIRECTORY)
+
+  # Walk through the local cache to update mtime on the device.
+  def MirrorMtime(local_path):
+    cache_relative_path = os.path.relpath(local_path, start=local_cache_path)
+    remote_path = os.path.join(_REMOTE_CACHE_DIRECTORY, cache_relative_path)
+    _AdbUtime(device.adb, remote_path, os.stat(local_path).st_mtime)
+
+  for local_directory_path, dirnames, filenames in os.walk(
+        local_cache_path, topdown=False):
+    for filename in filenames:
+      MirrorMtime(os.path.join(local_directory_path, filename))
+    for dirname in dirnames:
+      MirrorMtime(os.path.join(local_directory_path, dirname))
+  MirrorMtime(local_cache_path)
+
+
 def _ZipDirectoryContent(root_directory_path, archive_dest_path):
   """Zip a directory's content recursively with all the directories'
   timestamps preserved.
@@ -155,6 +199,10 @@ def _ZipDirectoryContent(root_directory_path, archive_dest_path):
   """
   with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
     timestamps = {}
+    root_directory_stats = os.stat(root_directory_path)
+    timestamps['.'] = {
+        'atime': root_directory_stats.st_atime,
+        'mtime': root_directory_stats.st_mtime}
     for directory_path, dirnames, filenames in os.walk(root_directory_path):
       for dirname in dirnames:
         subdirectory_path = os.path.join(directory_path, dirname)
@@ -210,6 +258,24 @@ def _UnzipDirectoryContent(archive_path, directory_dest_path):
       os.utime(output_path, (stats['atime'], stats['mtime']))
 
 
+def _CleanPreviousTraces(output_directories_path):
+  """Cleans previous traces from the output directory.
+
+  Args:
+    output_directories_path: The output directory path where to clean the
+        previous traces.
+  """
+  for dirname in os.listdir(output_directories_path):
+    directory_path = os.path.join(output_directories_path, dirname)
+    if not os.path.isdir(directory_path):
+      continue
+    try:
+      int(dirname)
+    except ValueError:
+      continue
+    shutil.rmtree(directory_path)
+
+
 def main():
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -221,35 +287,50 @@ def main():
                       help='Name of output directory to create.')
   parser.add_argument('--repeat', default=1, type=int,
                       help='How many times to run the job')
-  parser.add_argument('--save-cache', default=False,
-                      action='store_true',
-                      help='Clear HTTP cache before start,' +
-                      'save cache before exit.')
+  parser.add_argument('--cache-op',
+                      choices=['clear', 'save', 'push'],
+                      default='clear',
+                      help='Configures cache operation to do before launching '
+                          +'Chrome. (Default is clear).')
   parser.add_argument('--wpr-archive', default=None, type=str,
                       help='Web page replay archive to load job\'s urls from.')
   parser.add_argument('--wpr-record', default=False, action='store_true',
                       help='Record web page replay archive.')
   args = parser.parse_args()
 
-  try:
-    os.makedirs(args.output)
-  except OSError:
-    logging.error('Cannot create directory for results: %s' % args.output)
-    raise
+  if not os.path.isdir(args.output):
+    try:
+      os.makedirs(args.output)
+    except OSError:
+      logging.error('Cannot create directory for results: %s' % args.output)
+      raise
+  else:
+    _CleanPreviousTraces(args.output)
 
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
+  local_cache_archive_path = os.path.join(args.output, 'cache.zip')
+  local_cache_directory_path = None
+
+  if args.cache_op == 'push':
+    assert os.path.isfile(local_cache_archive_path)
+    local_cache_directory_path = tempfile.mkdtemp(suffix='.cache')
+    _UnzipDirectoryContent(local_cache_archive_path, local_cache_directory_path)
 
   with device_setup.WprHost(device,
                             args.wpr_archive,
                             args.wpr_record) as additional_flags:
     pages_loaded = 0
-    for iteration in xrange(args.repeat):
+    for _ in xrange(args.repeat):
       for url in job_urls:
+        if args.cache_op == 'push':
+          device.KillAll(_CHROME_PACKAGE, quiet=True)
+          _PushBrowserCache(device, local_cache_directory_path)
         with device_setup.DeviceConnection(
             device=device,
             additional_flags=additional_flags) as connection:
-          if iteration == 0 and pages_loaded == 0 and args.save_cache:
+          if (pages_loaded == 0 and args.cache_op == 'save' or
+              args.cache_op == 'clear'):
             connection.ClearCache()
           page_track.PageTrack(connection)
           tracing_track = tracing.TracingTrack(connection,
@@ -261,7 +342,10 @@ def main():
           _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
               str(pages_loaded))
 
-  if args.save_cache:
+  if local_cache_directory_path:
+    shutil.rmtree(local_cache_directory_path)
+
+  if args.cache_op == 'save':
     # Move Chrome to background to allow it to flush the index.
     device.adb.Shell('am start com.google.android.launcher')
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
@@ -269,8 +353,7 @@ def main():
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
 
     cache_directory_path = _PullBrowserCache(device)
-    _ZipDirectoryContent(cache_directory_path,
-                         os.path.join(args.output, 'cache.zip'))
+    _ZipDirectoryContent(cache_directory_path, local_cache_archive_path)
     shutil.rmtree(cache_directory_path)
 
 

commit 02f7586737a55729a5a341726f82a209c997633c
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 16 01:42:47 2016 -0800

    Tweak device_setup.py so that chrome is killed more reliably.
    
    Review URL: https://codereview.chromium.org/1698193003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375542}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 440f20a7611ff629cf55174439c00cf0d4c976f7

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 728bf75..a056ec2 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -177,6 +177,7 @@ def WprHost(device, wpr_archive_path, record=False):
     device_cert_util.remove_cert()
     shutil.rmtree(temp_certificate_dir)
 
+
 @contextlib.contextmanager
 def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
@@ -210,7 +211,8 @@ def DeviceConnection(device, additional_flags=None):
           data='about:blank')
       device.StartActivity(start_intent, blocking=True)
     else:
-      # Run on the host.
+      # Run on the host. We don't care about startup time so will skip the about
+      # page.
       assert os.path.exists(OPTIONS.local_binary)
 
       local_profile_dir = OPTIONS.local_profile_dir
@@ -223,13 +225,15 @@ def DeviceConnection(device, additional_flags=None):
     if device:
       time.sleep(2)
     else:
-      # TODO(blundell): Figure out why a lower sleep time causes an assertion
-      # in request_track.py to fire.
+      # TODO(mattcary): This seems to be related to chrome startup. There should
+      # be a way to ping chrome --- maybe keep trying to connect to the devtools
+      # port?
       time.sleep(10)
-    # If no device, we don't care about chrome startup so skip the about page.
-    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
-                     'localabstract:chrome_devtools_remote'):
-      yield devtools_monitor.DevToolsConnection(
-          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
-    if host_process:
-      host_process.kill()
+    try:
+      with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
+                       'localabstract:chrome_devtools_remote'):
+        yield devtools_monitor.DevToolsConnection(
+            OPTIONS.devtools_hostname, OPTIONS.devtools_port)
+    finally:
+      if host_process:
+        host_process.kill()

commit d48e82cf18707ea2d0a29fbd887cd1d2698b0d3d
Author: lizeb <lizeb@chromium.org>
Date:   Mon Feb 15 09:07:58 2016 -0800

    tools/android/loading: Add the renderer activity to the PNG output.
    
    Adds edge annotations showing the amount of time spent executing the
    initiating script, parsing the initiating document, and doing "other"
    work.
    
    Review URL: https://codereview.chromium.org/1681103002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375469}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: bce5e88fe35eff0cd975f28ecbbfd2ae3b66ba60

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
index 2973ac4..92ffd14 100644
--- a/loading/activity_lens.py
+++ b/loading/activity_lens.py
@@ -12,6 +12,8 @@ import collections
 import logging
 import operator
 
+import request_track
+
 
 class ActivityLens(object):
   """Reconstructs the activity of the main renderer thread between requests."""
@@ -24,6 +26,8 @@ class ActivityLens(object):
     self._trace = trace
     events = trace.tracing_track.GetEvents()
     self._renderer_main_tid = self._GetRendererMainThreadId(events)
+    self._tracing = self._trace.tracing_track.TracingTrackForThread(
+        self._renderer_main_tid)
 
   @classmethod
   def _GetRendererMainThreadId(cls, events):
@@ -60,9 +64,8 @@ class ActivityLens(object):
              tid_events_counts[0][1], tid_events_counts[1][1]))
     return tid_events_counts[0][0]
 
-  def _OverlappingEventsForTid(self, tid, start_msec, end_msec):
-    events = self._trace.tracing_track.OverlappingEvents(start_msec, end_msec)
-    return [e for e in events if e.tracing_event['tid'] == tid]
+  def _OverlappingMainRendererThreadEvents(self, start_msec, end_msec):
+    return self._tracing.OverlappingEvents(start_msec, end_msec)
 
   @classmethod
   def _ClampedDuration(cls, event, start_msec, end_msec):
@@ -136,7 +139,7 @@ class ActivityLens(object):
       url_to_duration[url] += clamped_duration
     return dict(url_to_duration)
 
-  def ExplainEdgeCost(self, dep):
+  def GenerateEdgeActivity(self, dep):
     """For a dependency between two requests, returns the renderer activity
     breakdown.
 
@@ -148,19 +151,48 @@ class ActivityLens(object):
       {'edge_cost': (float) ms, 'busy': (float) ms,
        'parsing': {'url' -> time_ms}, 'script' -> {'url' -> time_ms}}
     """
-    (first, second, _) = dep
-    # TODO(lizeb): Refactor the edge cost computations.
-    start_msec = first.start_msec
-    end_msec = second.start_msec
+    (first, second, reason) = dep
+    (start_msec, end_msec) = request_track.IntervalBetween(
+        first, second, reason)
     assert end_msec - start_msec >= 0.
-    tid = self._renderer_main_tid
-    events = self._OverlappingEventsForTid(tid, start_msec, end_msec)
+    events = self._OverlappingMainRendererThreadEvents(start_msec, end_msec)
     result = {'edge_cost': end_msec - start_msec,
               'busy': self._ThreadBusiness(events, start_msec, end_msec),
               'parsing': self._Parsing(events, start_msec, end_msec),
               'script': self._ScriptsExecuting(events, start_msec, end_msec)}
     return result
 
+  def BreakdownEdgeActivityByInitiator(self, dep):
+    """For a dependency between two requests, categorizes the renderer activity.
+
+    Args:
+      dep: (Request, Request, str) As returned from
+           RequestDependencyLens.GetRequestDependencies().
+
+    Returns:
+      {'script': float, 'parsing': float, 'other': float, 'unknown': float}
+      where the values are durations in ms:
+      - script: The initiating file was executing.
+      - parsing: The initiating file was being parsed.
+      - other: Other scripts and/or parsing activities.
+      - unknown: Activity which is not associated with a URL.
+    """
+    activity = self.GenerateEdgeActivity(dep)
+    related = {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 0}
+    for kind in ('script', 'parsing'):
+      for (script_name, duration_ms) in activity[kind].items():
+        if not script_name:
+          related['unknown_url'] += duration_ms
+        elif script_name == dep[0].url:
+          related[kind] += duration_ms
+        else:
+          # A lot of "ParseHTML" tasks are mostly about executing
+          # scripts. Don't double-count.
+          # TODO(lizeb): Better handle TraceEvents nesting.
+          if kind == 'script':
+            related['other_url'] += duration_ms
+    return related
+
 
 if __name__ == '__main__':
   import sys
@@ -176,4 +208,4 @@ if __name__ == '__main__':
       loading_trace)
   deps = dependencies_lens.GetRequestDependencies()
   for requests_dep in deps:
-    print activity_lens.ExplainEdgeCost(requests_dep)
+    print activity_lens.GenerateEdgeActivity(requests_dep)
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
index a84c864..3b4062f 100644
--- a/loading/activity_lens_unittest.py
+++ b/loading/activity_lens_unittest.py
@@ -5,6 +5,7 @@
 import unittest
 
 from activity_lens import ActivityLens
+import test_utils
 import tracing
 
 
@@ -170,6 +171,63 @@ class ActivityLensTestCast(unittest.TestCase):
     self.assertTrue(html_url in ActivityLens._Parsing(events, 0, 1000))
     self.assertEquals(42, ActivityLens._Parsing(events, 0, 1000)[html_url])
 
+  def testBreakdownEdgeActivityByInitiator(self):
+    requests = [test_utils.MakeRequest(0, 1, 10, 20, 30),
+                test_utils.MakeRequest(0, 1, 50, 60, 70)]
+    raw_events = [
+        {u'args': {u'beginData': {u'url': requests[0].url}},
+         u'cat': u'devtools.timeline',
+         u'dur': 12 * 1000,
+         u'name': u'ParseHTML',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 1,
+         u'ts': 25 * 1000},
+        {u'args': {u'data': {'scriptName': requests[0].url}},
+         u'cat': u'devtools.timeline,v8',
+         u'dur': 0,
+         u'name': u'EvaluateScript',
+         u'ph': u'X',
+         u'pid': 1,
+         u'tid': 1,
+         u'ts': 0}]
+    activity = self._ActivityLens(requests, raw_events)
+    dep = (requests[0], requests[1], 'parser')
+    self.assertEquals(
+        {'script': 0, 'parsing': 12, 'other_url': 0, 'unknown_url': 0},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    dep = (requests[0], requests[1], 'other')
+    # Truncating the event from the parent xrequest end.
+    self.assertEquals(
+        {'script': 0, 'parsing': 7, 'other_url': 0, 'unknown_url': 0},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    # Unknown URL
+    raw_events[0]['args']['beginData']['url'] = None
+    activity = self._ActivityLens(requests, raw_events)
+    dep = (requests[0], requests[1], 'parser')
+    self.assertEquals(
+        {'script': 0, 'parsing': 0, 'other_url': 0, 'unknown_url': 12},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    # Script
+    raw_events[1]['ts'] = 40 * 1000
+    raw_events[1]['dur'] = 6 * 1000
+    activity = self._ActivityLens(requests, raw_events)
+    dep = (requests[0], requests[1], 'script')
+    self.assertEquals(
+        {'script': 6, 'parsing': 0, 'other_url': 0, 'unknown_url': 7},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+    # Other URL
+    raw_events[1]['args']['data']['scriptName'] = 'http://other.com/url'
+    activity = self._ActivityLens(requests, raw_events)
+    self.assertEquals(
+        {'script': 0., 'parsing': 0., 'other_url': 6., 'unknown_url': 7.},
+        activity.BreakdownEdgeActivityByInitiator(dep))
+
+  def _ActivityLens(self, requests, raw_events):
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        requests, None, raw_events)
+    return ActivityLens(loading_trace)
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/analyze.py b/loading/analyze.py
index fdf18da..1c31de0 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -24,6 +24,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 from pylib import constants
 
+import activity_lens
 import content_classification_lens
 import device_setup
 import frame_load_lens
@@ -158,7 +159,9 @@ def _ProcessRequests(filename):
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
             trace, OPTIONS.ad_rules, OPTIONS.tracking_rules))
     frame_lens = frame_load_lens.FrameLoadLens(trace)
-    graph = loading_model.ResourceGraph(trace, content_lens, frame_lens)
+    activity = activity_lens.ActivityLens(trace)
+    graph = loading_model.ResourceGraph(
+        trace, content_lens, frame_lens, activity)
     if OPTIONS.noads:
       graph.Set(node_filter=graph.FilterAds)
     return graph
diff --git a/loading/loading_model.py b/loading/loading_model.py
index d2889e8..5deec03 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -20,17 +20,22 @@ import os
 import urlparse
 import sys
 
+import activity_lens
 import dag
 import loading_trace
 import request_dependencies_lens
+import request_track
 
 class ResourceGraph(object):
-  """A model of loading by a DAG (tree?) of resource dependancies.
+  """A model of loading by a DAG of resource dependencies.
 
-  Set parameters:
-    cache_all: if true, assume zero loading time for all resources.
+  See model parameters in Set().
   """
-  def __init__(self, trace, content_lens=None, frame_lens=None):
+  EDGE_KIND_KEY = 'edge_kind'
+  EDGE_KINDS = request_track.Request.INITIATORS + (
+      'script_inferred', 'after-load', 'before-load', 'timing')
+  def __init__(self, trace, content_lens=None, frame_lens=None,
+               activity=None):
     """Create from a LoadingTrace (or json of a trace).
 
     Args:
@@ -38,12 +43,15 @@ class ResourceGraph(object):
       content_lens: (ContentClassificationLens) Lens used to annotate the
                     nodes, or None.
       frame_lens: (FrameLoadLens) Lens used to augment graph with load nodes.
+      activity:   (ActivityLens) Lens used to augment the edges with the
+                   activity.
     """
     if type(trace) == dict:
       trace = loading_trace.LoadingTrace.FromJsonDict(trace)
     self._trace = trace
     self._content_lens = content_lens
     self._frame_lens = frame_lens
+    self._activity_lens = activity
     self._BuildDag(trace)
     # Sort before splitting children so that we can correctly dectect if a
     # reparented child is actually a dependency for a child of its new parent.
@@ -58,7 +66,7 @@ class ResourceGraph(object):
 
   @classmethod
   def CheckImageLoadConsistency(cls, g1, g2):
-    """Check that images have the same dependancies between ResourceGraphs.
+    """Check that images have the same dependencies between ResourceGraphs.
 
     Image resources are identified by their short names.
 
@@ -101,10 +109,10 @@ class ResourceGraph(object):
     """Set model parameters.
 
     TODO(mattcary): add parameters for caching certain types of resources (just
-    scripts, just cachable, etc).
+    scripts, just cacheable, etc).
 
     Args:
-      cache_all: boolean that if true ignores emperical resource load times for
+      cache_all: boolean that if true ignores empirical resource load times for
         all resources.
       node_filter: a Node->boolean used to restrict the graph for most
         operations.
@@ -167,7 +175,6 @@ class ResourceGraph(object):
       if self._node_filter(n.Node()) and n.Url() in other_map:
         yield(n, other_map[n.Url()])
 
-
   def Cost(self, path_list=None):
     """Compute cost of current model.
 
@@ -279,9 +286,9 @@ class ResourceGraph(object):
     """Convenience function for redirecting to NodeInfo."""
     return self.NodeInfo(parent).EdgeCost(self.NodeInfo(child))
 
-  def EdgeAnnotation(self, parent, child):
+  def EdgeAnnotations(self, parent, child):
     """Convenience function for redirecting to NodeInfo."""
-    return self.NodeInfo(parent).EdgeAnnotation(self.NodeInfo(child))
+    return self.NodeInfo(parent).EdgeAnnotations(self.NodeInfo(child))
 
   ##
   ## Internal items
@@ -372,7 +379,7 @@ class ResourceGraph(object):
     def EndTime(self):
       return self.StartTime() + self._node_cost
 
-    def EdgeAnnotation(self, s):
+    def EdgeAnnotations(self, s):
       assert s.Node() in self.Node().Successors()
       return self._edge_annotations.get(s, [])
 
@@ -411,9 +418,9 @@ class ResourceGraph(object):
       assert child.Node() in self._node.Successors()
       self._edge_costs[child] = cost
 
-    def AddEdgeAnnotation(self, s, annotation):
+    def AddEdgeAnnotations(self, s, annotations):
       assert s.Node() in self._node.Successors()
-      self._edge_annotations.setdefault(s, []).append(annotation)
+      self._edge_annotations.setdefault(s, {}).update(annotations)
 
     def ReparentTo(self, old_parent, new_parent):
       """Move costs and annotatations from old_parent to new_parent.
@@ -429,13 +436,12 @@ class ResourceGraph(object):
       """
       assert old_parent.Node() in self.Node().Predecessors()
       assert new_parent.Node() not in self.Node().Predecessors()
-      edge_annotations = old_parent._edge_annotations.pop(self, [])
+      edge_annotations = old_parent._edge_annotations.pop(self, {})
       edge_cost =  old_parent._edge_costs.pop(self)
       old_parent.Node().RemoveSuccessor(self.Node())
       new_parent.Node().AddSuccessor(self.Node())
       new_parent.SetEdgeCost(self, edge_cost)
-      for a in edge_annotations:
-        new_parent.AddEdgeAnnotation(self, a)
+      new_parent.AddEdgeAnnotations(self, edge_annotations)
 
     def __eq__(self, o):
       """Note this works whether o is a Node or a NodeInfo."""
@@ -473,10 +479,11 @@ class ResourceGraph(object):
 
     dependencies = request_dependencies_lens.RequestDependencyLens(
         trace).GetRequestDependencies()
-    for parent_rq, child_rq, reason in dependencies:
+    for dep in dependencies:
+      (parent_rq, child_rq, reason) = dep
       parent = self._node_info[index_by_request[parent_rq]]
       child = self._node_info[index_by_request[child_rq]]
-      edge_cost = child.StartTime() - parent.EndTime()
+      edge_cost = request_track.TimeBetween(parent_rq, child_rq, reason)
       if edge_cost < 0:
         edge_cost = 0
         if child.StartTime() < parent.StartTime():
@@ -486,7 +493,10 @@ class ResourceGraph(object):
           # fair amount in practice.
       parent.Node().AddSuccessor(child.Node())
       parent.SetEdgeCost(child, edge_cost)
-      parent.AddEdgeAnnotation(child, reason)
+      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: reason})
+      if self._activity_lens:
+        activity = self._activity_lens.BreakdownEdgeActivityByInitiator(dep)
+        parent.AddEdgeAnnotations(child, {'activity': activity})
 
     self._AugmentFrameLoads(index_by_request)
 
@@ -507,12 +517,12 @@ class ResourceGraph(object):
       parent = self._node_info[load_index_to_node[load_idx]]
       child = self._node_info[index_by_request[rq]]
       parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotation(child, 'after-load')
+      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'after-load'})
     for rq, load_idx in frame_deps[1]:
       child = self._node_info[load_index_to_node[load_idx]]
       parent = self._node_info[index_by_request[rq]]
       parent.Node().AddSuccessor(child.Node())
-      parent.AddEdgeAnnotation(child, 'before-load')
+      parent.AddEdgeAnnotations(child, {self.EDGE_KIND_KEY: 'before-load'})
 
   def _SplitChildrenByTime(self, parent):
     """Split children of a node by request times.
@@ -535,7 +545,7 @@ class ResourceGraph(object):
     This is refined by only considering assets which we believe actually create
     a dependency. We only split if the original parent is a script, and the new
     parent a data file. We confirm these relationships heuristically by loading
-    pages multiple times and ensuring that dependacies do not change; see
+    pages multiple times and ensuring that dependencies do not change; see
     CheckImageLoadConsistency() for details.
 
     We incorporate this heuristic by skipping over any non-script/json resources
@@ -583,7 +593,8 @@ class ResourceGraph(object):
                   # eligible.
       if children_by_end_time[end_mark].EndTime() <= current.StartTime():
         current.ReparentTo(parent, children_by_end_time[end_mark])
-        children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
+        children_by_end_time[end_mark].AddEdgeAnnotations(
+            current, {self.EDGE_KIND_KEY: 'timing'})
 
   def _ExtractImages(self):
     """Return interesting image resources.
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 7afe8ef..44dcbae 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -24,8 +24,9 @@ class SimpleLens(object):
       assert rq.url not in url_to_rq
       url_to_rq[rq.url] = rq
     for rq in self._trace.request_track.GetEvents():
-      if rq.initiator in url_to_rq:
-        deps.append(( url_to_rq[rq.initiator], rq, ''))
+      initiating_url = rq.initiator['url']
+      if initiating_url in url_to_rq:
+        deps.append((url_to_rq[initiating_url], rq, rq.initiator['type']))
     return deps
 
 
@@ -34,31 +35,10 @@ class LoadingModelTestCase(unittest.TestCase):
   def setUp(self):
     self.old_lens = request_dependencies_lens.RequestDependencyLens
     request_dependencies_lens.RequestDependencyLens = SimpleLens
-    self._next_request_id = 0
 
   def tearDown(self):
     request_dependencies_lens.RequestDependencyLens = self.old_lens
 
-  def MakeParserRequest(self, url, source_url, start_time, end_time,
-                        magic_content_type=False):
-    timing = request_track.TimingAsList(request_track.TimingFromDict({
-        # connectEnd should be ignored.
-        'connectEnd': (end_time - start_time) / 2,
-        'receiveHeadersEnd': end_time - start_time,
-        'requestTime': start_time / 1000.0}))
-    rq = request_track.Request.FromJsonDict({
-        'timestamp': start_time / 1000.0,
-        'request_id': self._next_request_id,
-        'url': 'http://' + str(url),
-        'initiator': 'http://' + str(source_url),
-        'response_headers': {'Content-Type':
-                             'null' if not magic_content_type
-                             else 'magic-debug-content' },
-        'timing': timing
-        })
-    self._next_request_id += 1
-    return rq
-
   def MakeGraph(self, requests):
     return loading_model.ResourceGraph(
         test_utils.LoadingTraceFromEvents(requests))
@@ -72,10 +52,11 @@ class LoadingModelTestCase(unittest.TestCase):
   def test_DictConstruction(self):
     graph = loading_model.ResourceGraph(
         {'request_track': {
-            'events': [self.MakeParserRequest(0, 'null', 100, 101).ToJsonDict(),
-                       self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
-                       self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
-                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()],
+            'events': [
+                test_utils.MakeRequest(0, 'null', 100, 100.5, 101).ToJsonDict(),
+                test_utils.MakeRequest(1, 0, 102, 102.5, 103).ToJsonDict(),
+                test_utils.MakeRequest(2, 0, 102, 102.5, 103).ToJsonDict(),
+                test_utils.MakeRequest(3, 2, 104, 114.5, 105).ToJsonDict()],
             'metadata': {
                 request_track.RequestTrack._DUPLICATES_KEY: 0,
                 request_track.RequestTrack._INCONSISTENT_INITIATORS_KEY: 0}},
@@ -89,13 +70,13 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
 
   def test_Costing(self):
-    requests = [self.MakeParserRequest(0, 'null', 100, 110),
-                self.MakeParserRequest(1, 0, 115, 120),
-                self.MakeParserRequest(2, 0, 112, 120),
-                self.MakeParserRequest(3, 1, 122, 126),
-                self.MakeParserRequest(4, 3, 127, 128),
-                self.MakeParserRequest(5, 'null', 100, 105),
-                self.MakeParserRequest(6, 5, 105, 110)]
+    requests = [test_utils.MakeRequest(0, 'null', 100, 105, 110),
+                test_utils.MakeRequest(1, 0, 115, 117, 120),
+                test_utils.MakeRequest(2, 0, 112, 116, 120),
+                test_utils.MakeRequest(3, 1, 122, 124, 126),
+                test_utils.MakeRequest(4, 3, 127, 127.5, 128),
+                test_utils.MakeRequest(5, 'null', 100, 103, 105),
+                test_utils.MakeRequest(6, 5, 105, 107, 110)]
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
@@ -110,16 +91,16 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(8, graph.Cost())
 
   def test_MaxPath(self):
-    requests = [self.MakeParserRequest(0, 'null', 100, 110),
-                self.MakeParserRequest(1, 0, 115, 120),
-                self.MakeParserRequest(2, 0, 112, 120),
-                self.MakeParserRequest(3, 1, 122, 126),
-                self.MakeParserRequest(4, 3, 127, 128),
-                self.MakeParserRequest(5, 'null', 100, 105),
-                self.MakeParserRequest(6, 5, 105, 110)]
+    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 111),
+                test_utils.MakeRequest(1, 0, 115, 120, 121),
+                test_utils.MakeRequest(2, 0, 112, 120, 121),
+                test_utils.MakeRequest(3, 1, 122, 126, 127),
+                test_utils.MakeRequest(4, 3, 127, 128, 129),
+                test_utils.MakeRequest(5, 'null', 100, 105, 106),
+                test_utils.MakeRequest(6, 5, 105, 110, 111)]
     graph = self.MakeGraph(requests)
     path_list = []
-    self.assertEqual(28, graph.Cost(path_list))
+    self.assertEqual(29, graph.Cost(path_list))
     self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
 
     # More interesting would be a test when a node has multiple predecessors,
@@ -127,16 +108,17 @@ class LoadingModelTestCase(unittest.TestCase):
 
   def test_TimingSplit(self):
     # Timing adds node 1 as a parent to 2 but not 3.
-    requests = [self.MakeParserRequest(0, 'null', 100, 110,
-                                       magic_content_type=True),
-                self.MakeParserRequest(1, 0, 115, 120,
-                                       magic_content_type=True),
-                self.MakeParserRequest(2, 0, 121, 122,
-                                       magic_content_type=True),
-                self.MakeParserRequest(3, 0, 112, 119,
-                                       magic_content_type=True),
-                self.MakeParserRequest(4, 2, 122, 126),
-                self.MakeParserRequest(5, 2, 122, 126)]
+    requests = [
+        test_utils.MakeRequest(0, 'null', 100, 110, 110,
+                               magic_content_type=True),
+        test_utils.MakeRequest(1, 0, 115, 120, 120,
+                               magic_content_type=True),
+        test_utils.MakeRequest(2, 0, 121, 122, 122,
+                               magic_content_type=True),
+        test_utils.MakeRequest(3, 0, 112, 119, 119,
+                               magic_content_type=True),
+        test_utils.MakeRequest(4, 2, 122, 126, 126),
+        test_utils.MakeRequest(5, 2, 122, 126, 126)]
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
@@ -147,8 +129,8 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
     # Change node 1 so it is a parent of 3, which becomes the parent of 2.
-    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
-                                         magic_content_type=True)
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
@@ -159,13 +141,13 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
     # Add an initiator dependence to 1 that will become the parent of 3.
-    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
-                                         magic_content_type=True)
-    requests.append(self.MakeParserRequest(6, 1, 111, 112))
+    requests[1] = test_utils.MakeRequest(
+        1, 0, 110, 111, 111, magic_content_type=True)
+    requests.append(test_utils.MakeRequest(6, 1, 111, 112, 112))
     graph = self.MakeGraph(requests)
     # Check it doesn't change until we change the content type of 6.
     self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
-    requests[6] = self.MakeParserRequest(6, 1, 111, 112,
+    requests[6] = test_utils.MakeRequest(6, 1, 111, 112, 112,
                                          magic_content_type=True)
     graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
@@ -179,12 +161,12 @@ class LoadingModelTestCase(unittest.TestCase):
 
   def test_TimingSplitImage(self):
     # If we're all image types, then we shouldn't split by timing.
-    requests = [self.MakeParserRequest(0, 'null', 100, 110),
-                self.MakeParserRequest(1, 0, 115, 120),
-                self.MakeParserRequest(2, 0, 121, 122),
-                self.MakeParserRequest(3, 0, 112, 119),
-                self.MakeParserRequest(4, 2, 122, 126),
-                self.MakeParserRequest(5, 2, 122, 126)]
+    requests = [test_utils.MakeRequest(0, 'null', 100, 110, 110),
+                test_utils.MakeRequest(1, 0, 115, 120, 120),
+                test_utils.MakeRequest(2, 0, 121, 122, 122),
+                test_utils.MakeRequest(3, 0, 112, 119, 119),
+                test_utils.MakeRequest(4, 2, 122, 126, 126),
+                test_utils.MakeRequest(5, 2, 122, 126, 126)]
     for r in requests:
       r.response_headers['Content-Type'] = 'image/gif'
     graph = self.MakeGraph(requests)
diff --git a/loading/model_graph.py b/loading/model_graph.py
index 5c0bf09..591bd7f 100644
--- a/loading/model_graph.py
+++ b/loading/model_graph.py
@@ -7,6 +7,8 @@
 import dag
 import itertools
 
+import loading_model
+
 
 class GraphVisualization(object):
   """Manipulate visual representations of a resource graph.
@@ -23,6 +25,7 @@ class GraphVisualization(object):
       'font':            'grey70',
       'image':           'orange',    # This probably catches gifs?
       'video':           'hotpink1',
+      'audio':           'hotpink2',
       }
 
   _CONTENT_TYPE_TO_COLOR = {
@@ -41,6 +44,19 @@ class GraphVisualization(object):
       'synthetic':       'yellow',
       }
 
+  _EDGE_KIND_TO_COLOR = {
+    'redirect': 'black',
+    'parser': 'red',
+    'script': 'blue',
+    'script_inferred': 'purple',
+    'after-load': 'forestgreen',
+    'before-load': 'forestgreen',
+  }
+
+  _ACTIVITY_TYPE_LABEL = (
+      ('script', 'S'), ('parsing', 'P'), ('other_url', 'O'),
+      ('unknown_url', 'U'))
+
   def __init__(self, graph):
     """Initialize.
 
@@ -98,22 +114,27 @@ class GraphVisualization(object):
         if s not in visited_nodes:
           continue
         style = 'color = orange'
-        annotations = self._graph.EdgeAnnotation(n, s)
-        if 'redirect' in annotations:
-          style = 'color = black'
-        elif 'parser' in annotations:
-          style = 'color = red'
-        elif 'stack' in annotations:
-          style = 'color = blue'
-        elif 'script_inferred' in annotations:
-          style = 'color = purple'
-        if 'after-load' in annotations or 'before-load' in annotations:
-          style = 'color = forestgreen'
-        if 'timing' in annotations:
+        label = '%.02f' % self._graph.EdgeCost(n, s)
+        annotations = self._graph.EdgeAnnotations(n, s)
+        edge_kind = annotations.get(
+            loading_model.ResourceGraph.EDGE_KIND_KEY, None)
+        assert ((edge_kind is None)
+                or (edge_kind in loading_model.ResourceGraph.EDGE_KINDS))
+        style = 'color = %s' % self._EDGE_KIND_TO_COLOR[edge_kind]
+        if edge_kind == 'timing':
           style += '; style=dashed'
         if self._graph.EdgeCost(n, s) > self._LONG_EDGE_THRESHOLD_MS:
           style += '; penwidth=5; weight=2'
-        arrow = '[%s; label="%s"]' % (style, self._graph.EdgeCost(n, s))
+
+        label = '%.02f' % self._graph.EdgeCost(n, s)
+        if 'activity' in annotations:
+          activity = annotations['activity']
+          separator = ' - '
+          for activity_type, activity_label in self._ACTIVITY_TYPE_LABEL:
+            label += '%s%s:%.02f ' % (
+                separator, activity_label, activity[activity_type])
+            separator = ' '
+        arrow = '[%s; label="%s"]' % (style, label)
         output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
     output.write('}\n')
 
diff --git a/loading/model_graph_unittest.py b/loading/model_graph_unittest.py
index 510e945..831b1e1 100644
--- a/loading/model_graph_unittest.py
+++ b/loading/model_graph_unittest.py
@@ -27,3 +27,7 @@ class ModelGraphTestCase(unittest.TestCase):
       graph = loading_model.ResourceGraph(trace=trace, frame_lens=frame_lens)
       visualization = model_graph.GraphVisualization(graph)
       visualization.OutputDot(tmp)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index a19c5e5..89e3a5d 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -29,6 +29,40 @@ _TIMING_NAMES_MAPPING = {
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
 
+def IntervalBetween(first, second, reason):
+  """Returns the start and end of the inteval between two requests, in ms.
+
+  This is defined as:
+  - [first.headers, second.start] if reason is 'parser'. This is to account
+    for incremental parsing.
+  - [first.end, second.start] if reason is 'script', 'redirect' or 'other'.
+
+  Args:
+    first: (Request) First request.
+    second: (Request) Second request.
+    reason: (str) Link between the two requests, in Request.INITIATORS.
+
+  Returns:
+    (start_msec (float), end_msec (float)),
+  """
+  assert reason in Request.INITIATORS
+  second_ms = second.timing.request_time * 1000
+  if reason == 'parser':
+    first_offset_ms = first.timing.receive_headers_end
+  else:
+    first_offset_ms = max(
+        [0] + [t for f, t in first.timing._asdict().iteritems()
+               if f != 'request_time'])
+  return (first.timing.request_time * 1000 + first_offset_ms, second_ms)
+
+
+def TimeBetween(first, second, reason):
+  """(end_msec - start_msec), with the values as returned by IntervalBetween().
+  """
+  (first_ms, second_ms) = IntervalBetween(first, second, reason)
+  return second_ms - first_ms
+
+
 def TimingAsList(timing):
   """Transform Timing to a list, eg as is used in JSON output.
 
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index cad4a11..7a85f07 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -6,7 +6,32 @@ import copy
 import json
 import unittest
 
-from request_track import (Request, RequestTrack, TimingFromDict)
+from request_track import (TimeBetween, Request, RequestTrack, TimingFromDict)
+
+
+class TimeBetweenTestCase(unittest.TestCase):
+  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'request_id': '1234.1',
+                                   'frame_id': '123.1',
+                                   'initiator': {'type': 'other'},
+                                   'timestamp': 2,
+                                   'timing': TimingFromDict({})})
+  def setUp(self):
+    super(TimeBetweenTestCase, self).setUp()
+    self.first = copy.deepcopy(self._REQUEST)
+    self.first.timing = TimingFromDict({'requestTime': 123456,
+                                        'receiveHeadersEnd': 100,
+                                        'loadingFinished': 500})
+    self.second = copy.deepcopy(self._REQUEST)
+    self.second.timing = TimingFromDict({'requestTime': 123456 + 1,
+                                        'receiveHeadersEnd': 200,
+                                        'loadingFinished': 600})
+
+  def testTimeBetweenParser(self):
+    self.assertEquals(900, TimeBetween(self.first, self.second, 'parser'))
+
+  def testTimeBetweenScript(self):
+    self.assertEquals(500, TimeBetween(self.first, self.second, 'script'))
 
 
 class RequestTestCase(unittest.TestCase):
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 2e5bb74..654e94b 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -7,6 +7,7 @@
 import devtools_monitor
 import loading_trace
 import page_track
+import request_track
 import tracing
 
 
@@ -40,9 +41,36 @@ class FakePageTrack(devtools_monitor.Track):
     return event['frame_id']
 
 
+def MakeRequest(
+    url, source_url, start_time, headers_time, end_time,
+    magic_content_type=False, initiator_type='other'):
+  assert initiator_type in ('other', 'parser')
+  timing = request_track.TimingAsList(request_track.TimingFromDict({
+      # connectEnd should be ignored.
+      'connectEnd': (end_time - start_time) / 2,
+      'receiveHeadersEnd': headers_time - start_time,
+      'loadingFinished': end_time - start_time,
+      'requestTime': start_time / 1000.0}))
+  rq = request_track.Request.FromJsonDict({
+      'timestamp': start_time / 1000.0,
+      'request_id': str(MakeRequest._next_request_id),
+      'url': 'http://' + str(url),
+      'initiator': {'type': initiator_type, 'url': 'http://' + str(source_url)},
+      'response_headers': {'Content-Type':
+                           'null' if not magic_content_type
+                           else 'magic-debug-content' },
+      'timing': timing
+  })
+  MakeRequest._next_request_id += 1
+  return rq
+
+
+MakeRequest._next_request_id = 0
+
+
 def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
-  request_track = FakeRequestTrack(requests)
+  request = FakeRequestTrack(requests)
   page_event_track = FakePageTrack(page_events if page_events else [])
   if trace_events:
     tracing_track = tracing.TracingTrack(None)
@@ -51,4 +79,4 @@ def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   else:
     tracing_track = None
   return loading_trace.LoadingTrace(
-      None, None, page_event_track, request_track, tracing_track)
+      None, None, page_event_track, request, tracing_track)
diff --git a/loading/tracing.py b/loading/tracing.py
index c69ffaf..37ee7ce 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -6,6 +6,7 @@
 
 import bisect
 import itertools
+import logging
 
 import devtools_monitor
 
@@ -95,6 +96,20 @@ class TracingTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
+  def TracingTrackForThread(self, tid):
+    """Returns a new TracingTrack with only the events from a given thread.
+
+    Args:
+      tid: (int) Thread ID.
+
+    Returns:
+      A new instance of TracingTrack.
+    """
+    events = [e for e in self._events if e.tracing_event['tid'] == tid]
+    tracing_track = TracingTrack(None)
+    tracing_track._events = events
+    return tracing_track
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     assert 'events' in json_dict
@@ -182,7 +197,7 @@ class TracingTrack(devtools_monitor.Track):
         assert event.IsIndexable()
         if event.start_msec > current_msec:
           break
-        matched_event = spanning_events.Match(event)
+        matched_event = spanning_events.Match(event, strict)
         if matched_event is not None:
           event = matched_event
         if not event.synthetic and (
@@ -220,9 +235,9 @@ class TracingTrack(devtools_monitor.Track):
           None: self._Ignore,
           }
 
-    def Match(self, event):
+    def Match(self, event, strict=False):
       return self._MATCH_HANDLER.get(
-          event.type, self._Unsupported)(event)
+          event.type, self._Unsupported)(event, strict)
 
     def HasPending(self):
       return (self._duration_stack or
@@ -236,21 +251,21 @@ class TracingTrack(devtools_monitor.Track):
           itertools.chain.from_iterable((
               (e for e in s) for s in self._async_stacks.itervalues())))
 
-    def _AsyncKey(self, event):
+    def _AsyncKey(self, event, _):
       return (event.tracing_event['cat'], event.id)
 
-    def _Ignore(self, _event):
+    def _Ignore(self, _event, _):
       return None
 
-    def _Unsupported(self, event):
+    def _Unsupported(self, event, _):
       raise devtools_monitor.DevToolsConnectionException(
           'Unsupported spanning event type: %s' % event)
 
-    def _DurationBegin(self, event):
+    def _DurationBegin(self, event, _):
       self._duration_stack.append(event)
       return None
 
-    def _DurationEnd(self, event):
+    def _DurationEnd(self, event, _):
       if not self._duration_stack:
         raise devtools_monitor.DevToolsConnectionException(
             'Unmatched duration end: %s' % event)
@@ -258,16 +273,20 @@ class TracingTrack(devtools_monitor.Track):
       start.SetClose(event)
       return start
 
-    def _AsyncStart(self, event):
-      key = self._AsyncKey(event)
+    def _AsyncStart(self, event, strict):
+      key = self._AsyncKey(event, strict)
       self._async_stacks.setdefault(key, []).append(event)
       return None
 
-    def _AsyncEnd(self, event):
-      key = self._AsyncKey(event)
+    def _AsyncEnd(self, event, strict):
+      key = self._AsyncKey(event, strict)
       if key not in self._async_stacks:
-        raise devtools_monitor.DevToolsConnectionException(
-            'Unmatched async end %s: %s' % (key, event))
+        message = 'Unmatched async end %s: %s' % (key, event)
+        if strict:
+          raise devtools_monitor.DevToolsConnectionException(message)
+        else:
+          logging.warning(message)
+        return None
       stack = self._async_stacks[key]
       start = stack.pop()
       if not stack:
@@ -275,7 +294,7 @@ class TracingTrack(devtools_monitor.Track):
       start.SetClose(event)
       return start
 
-    def _ObjectCreated(self, event):
+    def _ObjectCreated(self, event, _):
       # The tracing event format has object deletion timestamps being exclusive,
       # that is the timestamp for a deletion my equal that of the next create at
       # the same address. This asserts that does not happen in practice as it is
@@ -287,7 +306,7 @@ class TracingTrack(devtools_monitor.Track):
       self._objects[event.id] = event
       return None
 
-    def _ObjectDestroyed(self, event):
+    def _ObjectDestroyed(self, event, _):
       if event.id not in self._objects:
         raise devtools_monitor.DevToolsConnectionException(
             'Missing object creation for %s' % event)
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 0f30dcc..9c1ac3a 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -26,12 +26,12 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 15, 'ph': 'D', 'id': 1}]
 
   _EVENTS = [
-      {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
-      {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
-      {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
-      {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
-      {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
-      {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'B'}},
+      {'ts': 3, 'ph': 'X', 'dur': 4, 'tid': 1, 'args': {'name': 'A'}},
+      {'ts': 10, 'ph': 'X', 'dur': 1, 'tid': 2, 'args': {'name': 'C'}},
+      {'ts': 10, 'ph': 'X', 'dur': 2, 'tid': 2, 'args': {'name': 'D'}},
+      {'ts': 13, 'ph': 'X', 'dur': 1, 'tid': 1, 'args': {'name': 'F'}},
+      {'ts': 12, 'ph': 'X', 'dur': 3, 'tid': 1, 'args': {'name': 'E'}}]
 
   def setUp(self):
     self.track = TracingTrack(None)
@@ -246,6 +246,16 @@ class TracingTrackTestCase(unittest.TestCase):
                      set([e.args['name']
                           for e in self.track.OverlappingEvents(6, 10.1)]))
 
+  def testTracingTrackForThread(self):
+    self.track.Handle(
+        'Tracing.dataCollected', {'params': {'value': [
+            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    tracing_track = self.track.TracingTrackForThread(1)
+    self.assertTrue(tracing_track is not self.track)
+    self.assertEquals(4, len(tracing_track.GetEvents()))
+    tracing_track = self.track.TracingTrackForThread(42)
+    self.assertEquals(0, len(tracing_track.GetEvents()))
+
 
 if __name__ == '__main__':
   unittest.main()

commit 4e7fb1f2450c1e736d8bd90a8d0fb58fecf62a63
Author: droger <droger@chromium.org>
Date:   Mon Feb 15 08:55:39 2016 -0800

    tools/android/loading Improve content type detection
    
    This CL improves content type detection:
    - uses the |mime_type| field when available, because some requests don't
      have response headers (such as data: URLs).
    - ignore the capitalization for the "Content-Type" header, because some
      servers use the wrong capitatization.
    
    Unittests are updated accordingly.
    
    Review URL: https://codereview.chromium.org/1698153002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375467}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8b9df8cca22c81f04cc95430522d0fc013660453

diff --git a/loading/request_track.py b/loading/request_track.py
index 52e22fb..a19c5e5 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -94,6 +94,7 @@ class Request(object):
     self.url = None
     self.protocol = None
     self.method = None
+    self.mime_type = None
     self.request_headers = None
     self.response_headers = None
     self.initial_priority = None
@@ -146,7 +147,17 @@ class Request(object):
 
   def GetContentType(self):
     """Returns the content type, or None."""
-    content_type = self.response_headers.get('Content-Type', None)
+    if self.mime_type:
+      return self.mime_type
+
+    # Case-insensitive search because servers sometimes use a wrong
+    # capitalization.
+    content_type = None
+    for header, value in self.response_headers.iteritems():
+      if header.lower() == 'content-type':
+        content_type = value
+        break
+
     if not content_type or ';' not in content_type:
       return content_type
     else:
@@ -161,7 +172,15 @@ class Request(object):
     cache_control = {}
     if not self.response_headers:
       return -1
-    cache_control_str = self.response_headers.get('Cache-Control', None)
+
+    # Case-insensitive search because servers sometimes use a wrong
+    # capitalization.
+    cache_control_str = None
+    for header, value in self.response_headers.iteritems():
+      if header.lower() == 'cache-control':
+        cache_control_str = value
+        break
+
     if cache_control_str is not None:
       directives = [s.strip() for s in cache_control_str.split(',')]
       for directive in directives:
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index e742f60..cad4a11 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -16,8 +16,16 @@ class RequestTestCase(unittest.TestCase):
     self.assertEquals(None, r.GetContentType())
     r.response_headers = {'Content-Type': 'application/javascript'}
     self.assertEquals('application/javascript', r.GetContentType())
+    # Case-insensitive match.
+    r.response_headers = {'content-type': 'application/javascript'}
+    self.assertEquals('application/javascript', r.GetContentType())
+    # Parameters are filtered out.
     r.response_headers = {'Content-Type': 'application/javascript;bla'}
     self.assertEquals('application/javascript', r.GetContentType())
+    # MIME type takes precedence over headers.
+    r.mime_type = 'image/webp'
+    self.assertEquals('image/webp', r.GetContentType())
+    r.mime_type = None
 
 
 class RequestTrackTestCase(unittest.TestCase):
@@ -340,6 +348,10 @@ class RequestTrackTestCase(unittest.TestCase):
     rq.response_headers[
         'Cache-Control'] = 'private,s-maxage=0'
     self.assertEqual(-1, rq.MaxAge())
+    # Case-insensitive match.
+    rq.response_headers['cache-control'] = 'max-age=600'
+    self.assertEqual(600, rq.MaxAge())
+
 
   @classmethod
   def _ValidSequence(cls, request_track):

commit 74d33ab0c7d5f0699e5eb279f3d0c6eb8f83e3d2
Author: droger <droger@chromium.org>
Date:   Mon Feb 15 08:47:10 2016 -0800

    tools/android/loading Fix broken initiator format
    
    The initiator format has been changed by CL:
    https://codereview.chromium.org/1666563005/
    
    This CL integrates the new format in //tools/android/loading.
    
    Review URL: https://codereview.chromium.org/1695323002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375465}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5e9d81f956cab1871be97e7f4d1b7fbe9d9fbf7f

diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 8128957..b255656 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -90,12 +90,14 @@ class RequestDependencyLens(object):
     return (initiating_request, request, 'parser')
 
   def _GetInitiatingRequestScript(self, request):
-    if not 'stackTrace' in request.initiator:
+    STACK_KEY = 'stack'
+    if not STACK_KEY in request.initiator:
       logging.warning('Script initiator but no stack trace.')
       return None
     initiating_request = None
     timestamp = request.timing.request_time
-    for frame in request.initiator['stackTrace']:
+    call_frames = request.initiator[STACK_KEY]['callFrames']
+    for frame in call_frames:
       url = frame['url']
       candidates = self._FindMatchingRequests(url, timestamp)
       if candidates:
@@ -104,7 +106,7 @@ class RequestDependencyLens(object):
         if initiating_request:
           break
     else:
-      for frame in request.initiator['stackTrace']:
+      for frame in call_frames:
         if not frame.get('url', None) and frame.get(
             'functionName', None) == 'window.onload':
           logging.warning('Unmatched request for onload handler.')
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 0ab7403..4035abf 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -55,8 +55,9 @@ class RequestDependencyLensTestCase(unittest.TestCase):
       {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
        'frame_id': '123.1',
        'initiator': {'type': 'script',
-                     'stackTrace': [{'url': 'unknown'},
-                                    {'url': 'http://bla.com/nyancat.js'}]},
+                     'stack': {'callFrames': [
+                         {'url': 'unknown'},
+                         {'url': 'http://bla.com/nyancat.js'}]}},
        'timestamp': 10, 'timing': TimingFromDict({})})
   _PAGE_EVENTS = [{'method': 'Page.frameAttached',
                    'frame_id': '123.13', 'parent_frame_id': '123.1'}]

commit 81b01f8d6fcdb0417b2f1333583228784874cf1c
Author: gabadie <gabadie@chromium.org>
Date:   Mon Feb 15 05:39:11 2016 -0800

    sandwich: Save browser cache as zip archive preserving all timestamps
    
    Note that the timestamps are saved in a json in the archive to be
    able to save directories's timestamps as well.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1690233002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375453}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2dc964d1727a9a45c94f411d959b59bcb45a33ec

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 68ca389..242cfbf 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -12,10 +12,14 @@ TODO(pasko): implement cache preparation and WPR.
 """
 
 import argparse
+import json
 import logging
 import os
+import shutil
 import sys
+import tempfile
 import time
+import zipfile
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -100,22 +104,16 @@ def _UpdateTimestampFromAdbStat(filename, stat):
   os.utime(filename, (stat.st_time, stat.st_time))
 
 
-def _SaveBrowserCache(device, output_directory):
+def _PullBrowserCache(device):
   """Pulls the browser cache from the device and saves it locally.
 
   Cache is saved with the same file structure as on the device. Timestamps are
   important to preserve because indexing and eviction depends on them.
 
-  Args:
-    output_directory: name of the directory for saving cache.
+  Returns:
+    Temporary directory containing all the browser cache.
   """
-  save_target = os.path.join(output_directory, _CACHE_DIRECTORY_NAME)
-  try:
-    os.makedirs(save_target)
-  except IOError:
-    logging.warning('Could not create directory: %s' % save_target)
-    raise
-
+  save_target = tempfile.mkdtemp(suffix='.cache')
   cache_directory = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
   for filename, stat in device.adb.Ls(cache_directory):
     if filename == '..':
@@ -144,6 +142,72 @@ def _SaveBrowserCache(device, output_directory):
   # after all files in it have been written. The timestamp is compared with
   # the contents of the index file when freshness is determined.
   _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
+  return save_target
+
+
+def _ZipDirectoryContent(root_directory_path, archive_dest_path):
+  """Zip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    root_directory_path: The directory's path to archive.
+    archive_dest_path: Archive destination's path.
+  """
+  with zipfile.ZipFile(archive_dest_path, 'w') as zip_output:
+    timestamps = {}
+    for directory_path, dirnames, filenames in os.walk(root_directory_path):
+      for dirname in dirnames:
+        subdirectory_path = os.path.join(directory_path, dirname)
+        subdirectory_relative_path = os.path.relpath(subdirectory_path,
+                                                     root_directory_path)
+        subdirectory_stats = os.stat(subdirectory_path)
+        timestamps[subdirectory_relative_path] = {
+            'atime': subdirectory_stats.st_atime,
+            'mtime': subdirectory_stats.st_mtime}
+      for filename in filenames:
+        file_path = os.path.join(directory_path, filename)
+        file_archive_name = os.path.join('content',
+            os.path.relpath(file_path, root_directory_path))
+        file_stats = os.stat(file_path)
+        timestamps[file_archive_name[8:]] = {
+            'atime': file_stats.st_atime,
+            'mtime': file_stats.st_mtime}
+        zip_output.write(file_path, arcname=file_archive_name)
+    zip_output.writestr('timestamps.json',
+                        json.dumps(timestamps, indent=2))
+
+
+def _UnzipDirectoryContent(archive_path, directory_dest_path):
+  """Unzip a directory's content recursively with all the directories'
+  timestamps preserved.
+
+  Args:
+    archive_path: Archive's path to unzip.
+    directory_dest_path: Directory destination path.
+  """
+  if not os.path.exists(directory_dest_path):
+    os.makedirs(directory_dest_path)
+
+  with zipfile.ZipFile(archive_path) as zip_input:
+    timestamps = None
+    for file_archive_name in zip_input.namelist():
+      if file_archive_name == 'timestamps.json':
+        timestamps = json.loads(zip_input.read(file_archive_name))
+      elif file_archive_name.startswith('content/'):
+        file_relative_path = file_archive_name[8:]
+        file_output_path = os.path.join(directory_dest_path, file_relative_path)
+        file_parent_directory_path = os.path.dirname(file_output_path)
+        if not os.path.exists(file_parent_directory_path):
+          os.makedirs(file_parent_directory_path)
+        with open(file_output_path, 'w') as f:
+          f.write(zip_input.read(file_archive_name))
+
+    assert timestamps
+    for relative_path, stats in timestamps.iteritems():
+      output_path = os.path.join(directory_dest_path, relative_path)
+      if not os.path.exists(output_path):
+        os.makedirs(output_path)
+      os.utime(output_path, (stats['atime'], stats['mtime']))
 
 
 def main():
@@ -203,7 +267,11 @@ def main():
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
     device.KillAll(_CHROME_PACKAGE, quiet=True)
     time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
-    _SaveBrowserCache(device, args.output)
+
+    cache_directory_path = _PullBrowserCache(device)
+    _ZipDirectoryContent(cache_directory_path,
+                         os.path.join(args.output, 'cache.zip'))
+    shutil.rmtree(cache_directory_path)
 
 
 if __name__ == '__main__':

commit 5f522e189ad94b4e652fca62a306b64be9ec21c2
Author: gabadie <gabadie@chromium.org>
Date:   Fri Feb 12 06:48:16 2016 -0800

    sandwich: Builds a script to pull the metrics from the traces
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1690813003
    
    Cr-Original-Commit-Position: refs/heads/master@{#375181}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 7c2b4ef2202045b91929d8d9d80ff75b48aa8676

diff --git a/loading/pull_sandwich_metrics.py b/loading/pull_sandwich_metrics.py
new file mode 100755
index 0000000..1558243
--- /dev/null
+++ b/loading/pull_sandwich_metrics.py
@@ -0,0 +1,190 @@
+#! /usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Pull a sandwich run's output directory's metrics from traces into a CSV.
+
+python pull_sandwich_metrics.py -h
+"""
+
+import argparse
+import csv
+import json
+import logging
+import os
+import sys
+
+
+CATEGORIES = ['blink.user_timing', 'disabled-by-default-memory-infra']
+
+_CSV_FIELD_NAMES = [
+    'id',
+    'total_load',
+    'onload',
+    'browser_malloc_avg',
+    'browser_malloc_max']
+
+_TRACKED_EVENT_NAMES = set(['requestStart', 'loadEventStart', 'loadEventEnd'])
+
+
+def _GetBrowserPID(trace):
+  """Get the browser PID from a trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    The browser's PID as an integer.
+  """
+  for event in trace['traceEvents']:
+    if event['cat'] != '__metadata' or event['name'] != 'process_name':
+      continue
+    if event['args']['name'] == 'Browser':
+      return event['pid']
+  raise ValueError('couldn\'t find browser\'s PID')
+
+
+def _GetBrowserDumpEvents(trace):
+  """Get the browser memory dump events from a trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    List of memory dump events.
+  """
+  browser_pid = _GetBrowserPID(trace)
+  browser_dumps_events = []
+  for event in trace['traceEvents']:
+    if event['cat'] != 'disabled-by-default-memory-infra':
+      continue
+    if event['ph'] != 'v' or event['name'] != 'periodic_interval':
+      continue
+    # Ignore dump events for processes other than the browser process
+    if event['pid'] != browser_pid:
+      continue
+    browser_dumps_events.append(event)
+  if len(browser_dumps_events) == 0:
+    raise ValueError('No browser dump events found.')
+  return browser_dumps_events
+
+
+def _GetWebPageTrackedEvents(trace):
+  """Get the web page's tracked events from a trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    Dictionary all tracked events.
+  """
+  main_frame = None
+  tracked_events = {}
+  for event in trace['traceEvents']:
+    if event['cat'] != 'blink.user_timing':
+      continue
+    event_name = event['name']
+    # Ignore events until about:blank's unloadEventEnd that give the main
+    # frame id.
+    if not main_frame:
+      if event_name == 'unloadEventEnd':
+        main_frame = event['args']['frame']
+        logging.info('found about:blank\'s event \'unloadEventEnd\'')
+      continue
+    # Ignore sub-frames events. requestStart don't have the frame set but it
+    # is fine since tracking the first one after about:blank's unloadEventEnd.
+    if 'frame' in event['args'] and event['args']['frame'] != main_frame:
+      continue
+    if event_name in _TRACKED_EVENT_NAMES and event_name not in tracked_events:
+      logging.info('found url\'s event \'%s\'' % event_name)
+      tracked_events[event_name] = event
+  assert len(tracked_events) == len(_TRACKED_EVENT_NAMES)
+  return tracked_events
+
+
+def _PullMetricsFromTrace(trace):
+  """Pulls all the metrics from a given trace.
+
+  Args:
+    trace: The cached trace.
+
+  Returns:
+    Dictionary with all _CSV_FIELD_NAMES's field set (except the 'id').
+  """
+  browser_dump_events = _GetBrowserDumpEvents(trace)
+  web_page_tracked_events = _GetWebPageTrackedEvents(trace)
+
+  browser_malloc_sum = 0
+  browser_malloc_max = 0
+  for dump_event in browser_dump_events:
+    attr = dump_event['args']['dumps']['allocators']['malloc']['attrs']['size']
+    assert attr['units'] == 'bytes'
+    size = int(attr['value'], 16)
+    browser_malloc_sum += size
+    browser_malloc_max = max(browser_malloc_max, size)
+
+  return {
+    'total_load': (web_page_tracked_events['loadEventEnd']['ts'] -
+                   web_page_tracked_events['requestStart']['ts']),
+    'onload': (web_page_tracked_events['loadEventEnd']['ts'] -
+               web_page_tracked_events['loadEventStart']['ts']),
+    'browser_malloc_avg': browser_malloc_sum / float(len(browser_dump_events)),
+    'browser_malloc_max': browser_malloc_max
+  }
+
+
+def _PullMetricsFromOutputDirectory(output_directory_path):
+  """Pulls all the metrics from all the traces of a sandwich run directory.
+
+  Args:
+    output_directory_path: The sandwich run's output directory to pull the
+        metrics from.
+
+  Returns:
+    List of dictionaries with all _CSV_FIELD_NAMES's field set.
+  """
+  assert os.path.isdir(output_directory_path)
+  metrics = []
+  for node_name in os.listdir(output_directory_path):
+    if not os.path.isdir(os.path.join(output_directory_path, node_name)):
+      continue
+    try:
+      page_id = int(node_name)
+    except ValueError:
+      continue
+    trace_path = os.path.join(output_directory_path, node_name, 'trace.json')
+    if not os.path.isfile(trace_path):
+      continue
+    logging.info('processing \'%s\'' % trace_path)
+    with open(trace_path) as trace_file:
+      trace = json.load(trace_file)
+      trace_metrics = _PullMetricsFromTrace(trace)
+      trace_metrics['id'] = page_id
+      metrics.append(trace_metrics)
+  assert len(metrics) > 0, ('Looks like \'{}\' was not a sandwich ' +
+                            'run directory.').format(output_directory_path)
+  return metrics
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument('output', type=str,
+                      help='Output directory of run_sandwich.py command.')
+  args = parser.parse_args()
+
+  trace_metrics_list = _PullMetricsFromOutputDirectory(args.output)
+  trace_metrics_list.sort(key=lambda e: e['id'])
+  cs_file_path = os.path.join(args.output, 'trace_analysis.csv')
+  with open(cs_file_path, 'w') as csv_file:
+    writer = csv.DictWriter(csv_file, fieldnames=_CSV_FIELD_NAMES)
+    writer.writeheader()
+    for trace_metrics in trace_metrics_list:
+      writer.writerow(trace_metrics)
+  return 0
+
+
+if __name__ == '__main__':
+  sys.exit(main())
diff --git a/loading/pull_sandwich_metrics_unittest.py b/loading/pull_sandwich_metrics_unittest.py
new file mode 100644
index 0000000..ad9fbe1
--- /dev/null
+++ b/loading/pull_sandwich_metrics_unittest.py
@@ -0,0 +1,162 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import json
+import os
+import shutil
+import subprocess
+import tempfile
+import unittest
+
+import pull_sandwich_metrics as puller
+
+_BLINK_CAT = 'blink.user_timing'
+_MEM_CAT = 'disabled-by-default-memory-infra'
+_START='requestStart'
+_LOADS='loadEventStart'
+_LOADE='loadEventEnd'
+_UNLOAD='unloadEventEnd'
+
+_MINIMALIST_TRACE = {'traceEvents': [
+    {'cat': _BLINK_CAT, 'name': _UNLOAD, 'ts': 10, 'args': {'frame': '0'}},
+    {'cat': _BLINK_CAT, 'name': _START,  'ts': 20, 'args': {},           },
+    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
+        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+            'units': 'bytes', 'value': '1af2', }}}}}}},
+    {'cat': _BLINK_CAT, 'name': _LOADS,  'ts': 35, 'args': {'frame': '0'}},
+    {'cat': _BLINK_CAT, 'name': _LOADE,  'ts': 40, 'args': {'frame': '0'}},
+    {'cat': _MEM_CAT,   'name': 'periodic_interval', 'pid': 1, 'ph': 'v',
+        'args': {'dumps': {'allocators': {'malloc': {'attrs': {'size':{
+            'units': 'bytes', 'value': 'd704', }}}}}}},
+    {'cat': '__metadata', 'pid': 1, 'name': 'process_name', 'args': {
+        'name': 'Browser'}}]}
+
+
+class PageTrackTest(unittest.TestCase):
+  def testGetBrowserPID(self):
+    def RunHelper(expected, trace):
+      self.assertEquals(expected, puller._GetBrowserPID(trace))
+
+    RunHelper(123, {'traceEvents': [
+        {'pid': 354, 'cat': 'whatever0'},
+        {'pid': 354, 'cat': 'whatever1'},
+        {'pid': 354, 'cat': '__metadata', 'name': 'thread_name'},
+        {'pid': 354, 'cat': '__metadata', 'name': 'process_name', 'args': {
+            'name': 'Renderer'}},
+        {'pid': 123, 'cat': '__metadata', 'name': 'process_name', 'args': {
+            'name': 'Browser'}},
+        {'pid': 354, 'cat': 'whatever0'}]})
+
+    with self.assertRaises(ValueError):
+      RunHelper(123, {'traceEvents': [
+          {'pid': 354, 'cat': 'whatever0'},
+          {'pid': 354, 'cat': 'whatever1'}]})
+
+  def testGetBrowserDumpEvents(self):
+    NAME = 'periodic_interval'
+
+    def RunHelper(trace_events, browser_pid):
+      trace_events = copy.copy(trace_events)
+      trace_events.append({
+          'pid': browser_pid,
+          'cat': '__metadata',
+          'name': 'process_name',
+          'args': {'name': 'Browser'}})
+      return puller._GetBrowserDumpEvents({'traceEvents': trace_events})
+
+    TRACE_EVENTS = [
+        {'pid': 354, 'ts':  1, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  2, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  3, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  4, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  5, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts':  6, 'cat': _MEM_CAT, 'ph': 'V'},
+        {'pid': 672, 'ts':  7, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts':  8, 'cat': _MEM_CAT, 'ph': 'v', 'name': 'foo'},
+        {'pid': 123, 'ts':  9, 'cat': 'whatever1', 'ph': 'v', 'name': NAME},
+        {'pid': 123, 'ts': 10, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME},
+        {'pid': 354, 'ts': 11, 'cat': 'whatever0'},
+        {'pid': 672, 'ts': 12, 'cat': _MEM_CAT, 'ph': 'v', 'name': NAME}]
+
+    self.assertTrue(_MEM_CAT in puller.CATEGORIES)
+
+    bump_events = RunHelper(TRACE_EVENTS, 123)
+    self.assertEquals(2, len(bump_events))
+    self.assertEquals(5, bump_events[0]['ts'])
+    self.assertEquals(10, bump_events[1]['ts'])
+
+    bump_events = RunHelper(TRACE_EVENTS, 354)
+    self.assertEquals(1, len(bump_events))
+    self.assertEquals(1, bump_events[0]['ts'])
+
+    bump_events = RunHelper(TRACE_EVENTS, 672)
+    self.assertEquals(3, len(bump_events))
+    self.assertEquals(3, bump_events[0]['ts'])
+    self.assertEquals(7, bump_events[1]['ts'])
+    self.assertEquals(12, bump_events[2]['ts'])
+
+    with self.assertRaises(ValueError):
+      RunHelper(TRACE_EVENTS, 895)
+
+  def testGetWebPageTrackedEvents(self):
+    self.assertTrue(_BLINK_CAT in puller.CATEGORIES)
+
+    trace_events = puller._GetWebPageTrackedEvents({'traceEvents': [
+        {'ts':  0, 'args': {},             'cat': 'whatever', 'name': _START},
+        {'ts':  1, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
+        {'ts':  2, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
+        {'ts':  3, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts':  4, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts':  5, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts':  6, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _UNLOAD},
+        {'ts':  7, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts':  8, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts':  9, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts': 10, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _UNLOAD},
+        {'ts': 11, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _START},
+        {'ts': 12, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
+        {'ts': 13, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
+        {'ts': 14, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts': 15, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts': 16, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts': 17, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts': 18, 'args': {'frame': '1'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts': 19, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE},
+        {'ts': 20, 'args': {},             'cat': 'whatever', 'name': _START},
+        {'ts': 21, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADS},
+        {'ts': 22, 'args': {'frame': '0'}, 'cat': 'whatever', 'name': _LOADE},
+        {'ts': 23, 'args': {},             'cat': _BLINK_CAT, 'name': _START},
+        {'ts': 24, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADS},
+        {'ts': 25, 'args': {'frame': '0'}, 'cat': _BLINK_CAT, 'name': _LOADE}]})
+
+    self.assertEquals(3, len(trace_events))
+    self.assertEquals(14, trace_events['requestStart']['ts'])
+    self.assertEquals(17, trace_events['loadEventStart']['ts'])
+    self.assertEquals(19, trace_events['loadEventEnd']['ts'])
+
+  def testPullMetricsFromTrace(self):
+    metrics = puller._PullMetricsFromTrace(_MINIMALIST_TRACE)
+    self.assertEquals(4, len(metrics))
+    self.assertEquals(20, metrics['total_load'])
+    self.assertEquals(5, metrics['onload'])
+    self.assertEquals(30971, metrics['browser_malloc_avg'])
+    self.assertEquals(55044, metrics['browser_malloc_max'])
+
+  def testCommandLine(self):
+    tmp_dir = tempfile.mkdtemp()
+    for dirname in ['1', '2', 'whatever']:
+      os.mkdir(os.path.join(tmp_dir, dirname))
+      with open(os.path.join(tmp_dir, dirname, 'trace.json'), 'w') as out_file:
+        json.dump(_MINIMALIST_TRACE, out_file)
+
+    process = subprocess.Popen(['python', puller.__file__, tmp_dir])
+    process.wait()
+    shutil.rmtree(tmp_dir)
+
+    self.assertEquals(0, process.returncode)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 8286c3c..68ca389 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -31,6 +31,7 @@ import device_setup
 import devtools_monitor
 import json
 import page_track
+import pull_sandwich_metrics
 import tracing
 
 
@@ -89,7 +90,7 @@ def _SaveChromeTrace(events, directory, subdirectory):
   try:
     os.makedirs(target_directory)
     with open(filename, 'w') as f:
-      json.dump({'traceEvents': events['events'], 'metadata': {}}, f)
+      json.dump({'traceEvents': events['events'], 'metadata': {}}, f, indent=2)
   except IOError:
     logging.warning('Could not save a trace: %s' % filename)
     # Swallow the exception.
@@ -188,7 +189,7 @@ def main():
             connection.ClearCache()
           page_track.PageTrack(connection)
           tracing_track = tracing.TracingTrack(connection,
-              categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
+              categories=pull_sandwich_metrics.CATEGORIES)
           connection.SetUpMonitoring()
           connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
           connection.StartMonitoring()

commit c216bebf1224291b9e3331afa37c464e9430c11d
Author: mattcary <mattcary@chromium.org>
Date:   Fri Feb 12 05:59:08 2016 -0800

    Refactor options in analyze.py to global structure
    
    Plumbing all sorts of options through arguments was getting unmaintainable,
    especially as many of them should be common across all commands, or affect
    low-level operation that's inappropriate to expose to high-level functions.
    
    Review URL: https://codereview.chromium.org/1694473002
    
    Cr-Original-Commit-Position: refs/heads/master@{#375173}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 23bd22480abd2209f258d1dd8d3a8572ee17954a

diff --git a/loading/analyze.py b/loading/analyze.py
index 5695e7d..fdf18da 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -30,6 +30,7 @@ import frame_load_lens
 import loading_model
 import loading_trace
 import model_graph
+import options
 import trace_recorder
 
 
@@ -38,8 +39,7 @@ import trace_recorder
 # output. For now we just do logging.warning.
 
 
-# TODO(mattcary): probably we want this piped in through a flag.
-CHROME = constants.PACKAGE_INFO['chrome']
+OPTIONS = options.OPTIONS
 
 
 def _LoadPage(device, url):
@@ -50,7 +50,9 @@ def _LoadPage(device, url):
     url: url as a string to load.
   """
   load_intent = intent.Intent(
-      package=CHROME.package, activity=CHROME.activity, data=url)
+      package=OPTIONS.ChromePackage().package,
+      activity=OPTIONS.ChromePackage().activity,
+      data=url)
   logging.warning('Loading ' + url)
   device.StartActivity(load_intent, blocking=True)
 
@@ -95,39 +97,33 @@ def _GetPrefetchHtml(graph, name=None):
   return '\n'.join(output)
 
 
-def _LogRequests(url, clear_cache=True, local=False,
-                 host_exe="out/Release/chrome", host_profile_dir=None):
+def _LogRequests(url, clear_cache_override=None):
   """Log requests for a web page.
 
   Args:
     url: url to log as string.
-    clear_cache: optional flag to clear the cache.
-    local: log from local (desktop) chrome session.
-    host_exe: Binary to execute when running locally.
-    host_profile_dir: Profile dir to use when running locally (if None, a
-      fresh profile dir will be used).
+    clear_cache_override: if not None, set clear_cache different from OPTIONS.
 
   Returns:
     JSON dict of logged information (ie, a dict that describes JSON).
   """
-  device = device_setup.GetFirstDevice() if not local else None
-  with device_setup.DeviceConnection(device, host_exe=host_exe,
-      host_profile_dir=host_profile_dir) as connection:
+  device = device_setup.GetFirstDevice() if not OPTIONS.local else None
+  clear_cache = (clear_cache_override if clear_cache_override is not None
+                 else OPTIONS.clear_cache)
+  with device_setup.DeviceConnection(device) as connection:
     trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
     return trace.ToJsonDict()
 
 
-def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
-               host_exe, host_profile_dir):
+def _FullFetch(url, json_output, prefetch):
   """Do a full fetch with optional prefetching."""
-  if not url.startswith('http'):
+  if not url.startswith('http') and not url.startswith('file'):
     url = 'http://' + url
   logging.warning('Cold fetch')
-  cold_data = _LogRequests(url, local=local,
-                           host_exe=host_exe, host_profile_dir=host_profile_dir)
+  cold_data = _LogRequests(url)
   assert cold_data, 'Cold fetch failed to produce data. Check your phone.'
   if prefetch:
-    assert not local
+    assert not OPTIONS.local
     logging.warning('Generating prefetch')
     prefetch_html = _GetPrefetchHtml(
         loading_model.ResourceGraph(cold_data), name=url)
@@ -140,9 +136,9 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
     device.adb.Push(tmp.name, target)
     logging.warning('Pushed prefetch %s to device at %s' % (tmp.name, target))
     _LoadPage(device, 'file://' + target)
-    time.sleep(prefetch_delay_seconds)
+    time.sleep(OPTIONS.prefetch_delay_seconds)
     logging.warning('Warm fetch')
-    warm_data = _LogRequests(url, clear_cache=False)
+    warm_data = _LogRequests(url, clear_cache_override=False)
     with open(json_output, 'w') as f:
       _WriteJson(f, warm_data)
     logging.warning('Wrote ' + json_output)
@@ -155,17 +151,17 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
     logging.warning('Wrote ' + json_output)
 
 
-# TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
-# with here.
-def _ProcessRequests(filename, ad_rules_filename='',
-                     tracking_rules_filename=''):
+def _ProcessRequests(filename):
   with open(filename) as f:
     trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
     content_lens = (
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
-            trace, ad_rules_filename, tracking_rules_filename))
+            trace, OPTIONS.ad_rules, OPTIONS.tracking_rules))
     frame_lens = frame_load_lens.FrameLoadLens(trace)
-    return loading_model.ResourceGraph(trace, content_lens, frame_lens)
+    graph = loading_model.ResourceGraph(trace, content_lens, frame_lens)
+    if OPTIONS.noads:
+      graph.Set(node_filter=graph.FilterAds)
+    return graph
 
 
 def InvalidCommand(cmd):
@@ -173,63 +169,34 @@ def InvalidCommand(cmd):
            (cmd, ' '.join(COMMAND_MAP.keys())))
 
 
-def DoCost(arg_str):
-  parser = argparse.ArgumentParser(description='Tabulates cost')
-  parser.add_argument('request_json')
-  parser.add_argument('--parameter', nargs='*', default=[])
-  parser.add_argument('--path', action='store_true')
-  parser.add_argument('--noads', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  for p in args.parameter:
-    graph.Set(**{p: True})
-  path_list = []
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
-  print 'Graph cost: ' + str(graph.Cost(path_list))
-  if args.path:
-    for p in path_list:
-      print '  ' + p.ShortName()
-
-
 def DoPng(arg_str):
-  parser = argparse.ArgumentParser(
-      description='Generates a PNG from a trace')
-  parser.add_argument('request_json')
-  parser.add_argument('png_output', nargs='?')
-  parser.add_argument('--eog', action='store_true')
-  parser.add_argument('--noads', action='store_true')
-  parser.add_argument('--ad_rules', default='')
-  parser.add_argument('--tracking_rules', default='')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(
-      args.request_json, args.ad_rules, args.tracking_rules)
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
+  OPTIONS.ParseArgs(arg_str, description='Generates a PNG from a trace',
+                    extra=['request_json', ('--png_output', ''),
+                           ('--eog', False)])
+  graph = _ProcessRequests(OPTIONS.request_json)
   visualization = model_graph.GraphVisualization(graph)
   tmp = tempfile.NamedTemporaryFile()
   visualization.OutputDot(tmp)
   tmp.flush()
-  png_output = args.png_output
+  png_output = OPTIONS.png_output
   if not png_output:
-    if args.request_json.endswith('.json'):
-      png_output = args.request_json[:args.request_json.rfind('.json')] + '.png'
+    if OPTIONS.request_json.endswith('.json'):
+      png_output = OPTIONS.request_json[
+          :OPTIONS.request_json.rfind('.json')] + '.png'
     else:
-      png_output = args.request_json + '.png'
+      png_output = OPTIONS.request_json + '.png'
   subprocess.check_call(['dot', '-Tpng', tmp.name, '-o', png_output])
   logging.warning('Wrote ' + png_output)
-  if args.eog:
+  if OPTIONS.eog:
     subprocess.Popen(['eog', png_output])
   tmp.close()
 
 
 def DoCompare(arg_str):
-  parser = argparse.ArgumentParser(description='Compares two traces')
-  parser.add_argument('g1_json')
-  parser.add_argument('g2_json')
-  args = parser.parse_args(arg_str)
-  g1 = _ProcessRequests(args.g1_json)
-  g2 = _ProcessRequests(args.g2_json)
+  OPTIONS.ParseArgs(arg_str, description='Compares two traces',
+                    extra=['g1_json', 'g2_json'])
+  g1 = _ProcessRequests(OPTIONS.g1_json)
+  g2 = _ProcessRequests(OPTIONS.g2_json)
   discrepancies = loading_model.ResourceGraph.CheckImageLoadConsistency(g1, g2)
   if discrepancies:
     print '%d discrepancies' % len(discrepancies)
@@ -239,104 +206,103 @@ def DoCompare(arg_str):
 
 
 def DoPrefetchSetup(arg_str):
-  parser = argparse.ArgumentParser(description='Sets up prefetch')
-  parser.add_argument('request_json')
-  parser.add_argument('target_html')
-  parser.add_argument('--upload', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  with open(args.target_html, 'w') as html:
+  OPTIONS.ParseArgs(arg_str, description='Sets up prefetch',
+                    extra=['request_json', 'target_html', ('--upload', False)])
+  graph = _ProcessRequests(OPTIONS.request_json)
+  with open(OPTIONS.target_html, 'w') as html:
     html.write(_GetPrefetchHtml(
-        graph, name=os.path.basename(args.request_json)))
-  if args.upload:
+        graph, name=os.path.basename(OPTIONS.request_json)))
+  if OPTIONS.upload:
     device = device_setup.GetFirstDevice()
     destination = os.path.join('/sdcard/Download',
-                               os.path.basename(args.target_html))
-    device.adb.Push(args.target_html, destination)
+                               os.path.basename(OPTIONS.target_html))
+    device.adb.Push(OPTIONS.target_html, destination)
 
     logging.warning(
-        'Pushed %s to device at %s' % (args.target_html, destination))
+        'Pushed %s to device at %s' % (OPTIONS.target_html, destination))
 
 
 def DoLogRequests(arg_str):
-  parser = argparse.ArgumentParser(description='Logs requests of a load')
-  parser.add_argument('--url', required=True)
-  parser.add_argument('--output', required=True)
-  parser.add_argument('--prefetch', action='store_true')
-  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
-  parser.add_argument('--local', action='store_true')
-  parser.add_argument('--host_exe', default='out/Release/chrome')
-  parser.add_argument('--host_profile_dir', default=None)
-  args = parser.parse_args(arg_str)
-  _FullFetch(url=args.url,
-             json_output=args.output,
-             prefetch=args.prefetch,
-             prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=args.local,
-             host_exe=args.host_exe,
-             host_profile_dir=args.host_profile_dir)
+  OPTIONS.ParseArgs(arg_str, description='Logs requests of a load',
+                    extra=['--url', '--output', ('--prefetch', False)])
+  _FullFetch(url=OPTIONS.url,
+             json_output=OPTIONS.output,
+             prefetch=OPTIONS.prefetch)
 
 
 def DoFetch(arg_str):
-  parser = argparse.ArgumentParser(description='Fetches SITE into DIR with '
-                                   'standard naming that can be processed by '
-                                   './cost_to_csv.py.  Both warm and cold '
-                                   'fetches are done.  SITE can be a full url '
-                                   'but the filename may be strange so better '
-                                   'to just use a site (ie, domain).')
-  # Arguments are flags as it's easy to get the wrong order of site vs dir.
-  parser.add_argument('--site', required=True)
-  parser.add_argument('--dir', required=True)
-  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
-  args = parser.parse_args(arg_str)
-  if not os.path.exists(args.dir):
-    os.makedirs(args.dir)
-  _FullFetch(url=args.site,
-             json_output=os.path.join(args.dir, args.site + '.json'),
-             prefetch=True,
-             prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=False,
-             host_exe=None,
-             host_profile_dir=None)
+  OPTIONS.ParseArgs(arg_str,
+                    description=('Fetches SITE into DIR with '
+                                 'standard naming that can be processed by '
+                                 './cost_to_csv.py.  Both warm and cold '
+                                 'fetches are done.  SITE can be a full url '
+                                 'but the filename may be strange so better '
+                                 'to just use a site (ie, domain).'),
+                    extra=['--site', '--dir'])
+  if not os.path.exists(OPTIONS.dir):
+    os.makedirs(OPTIONS.dir)
+  _FullFetch(url=OPTIONS.site,
+             json_output=os.path.join(OPTIONS.dir, OPTIONS.site + '.json'),
+             prefetch=True)
 
 
 def DoLongPole(arg_str):
-  parser = argparse.ArgumentParser(description='Calculates long pole')
-  parser.add_argument('request_json')
-  parser.add_argument('--noads', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
+  OPTIONS.ParseArgs(arg_str, description='Calculates long pole',
+                    extra='request_json')
+  graph = _ProcessRequests(OPTIONS.request_json)
   path_list = []
   cost = graph.Cost(path_list=path_list)
   print '%s (%s)' % (path_list[-1], cost)
 
 
 def DoNodeCost(arg_str):
-  parser = argparse.ArgumentParser(description='Calculates node cost')
-  parser.add_argument('request_json')
-  parser.add_argument('--noads', action='store_true')
-  args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
-  if args.noads:
-    graph.Set(node_filter=graph.FilterAds)
+  OPTIONS.ParseArgs(arg_str,
+                    description='Calculates node cost',
+                    extra='request_json')
+  graph = _ProcessRequests(OPTIONS.request_json)
   print sum((n.NodeCost() for n in graph.Nodes()))
 
 
+def DoCost(arg_str):
+  OPTIONS.ParseArgs(arg_str,
+                    description='Calculates total cost',
+                    extra=['request_json', ('--path', False)])
+  graph = _ProcessRequests(OPTIONS.request_json)
+  path_list = []
+  print 'Graph cost: %s' % graph.Cost(path_list)
+  if OPTIONS.path:
+    for p in path_list:
+      print '  ' + p.ShortName()
+
+
 COMMAND_MAP = {
-    'cost': DoCost,
     'png': DoPng,
     'compare': DoCompare,
     'prefetch_setup': DoPrefetchSetup,
     'log_requests': DoLogRequests,
     'longpole': DoLongPole,
     'nodecost': DoNodeCost,
+    'cost': DoCost,
     'fetch': DoFetch,
 }
 
 def main():
   logging.basicConfig(level=logging.WARNING)
+  OPTIONS.AddGlobalArgument(
+      'local', False,
+      'run against local desktop chrome rather than device '
+      '(see also --local_binary and local_profile_dir)')
+  OPTIONS.AddGlobalArgument(
+      'noads', False, 'ignore ad resources in modeling')
+  OPTIONS.AddGlobalArgument(
+      'ad_rules', '', 'AdBlocker+ ad rules file.')
+  OPTIONS.AddGlobalArgument(
+      'tracking_rules', '', 'AdBlocker+ tracking rules file.')
+  OPTIONS.AddGlobalArgument(
+      'prefetch_delay_seconds', 5,
+      'delay after requesting load of prefetch page '
+      '(only when running full fetch)')
+
   parser = argparse.ArgumentParser(description='Analyzes loading')
   parser.add_argument('command', help=' '.join(COMMAND_MAP.keys()))
   parser.add_argument('rest', nargs=argparse.REMAINDER)
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 1ad0388..728bf75 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -34,10 +34,10 @@ import adb_install_cert
 import certutils
 
 import devtools_monitor
+import options
 
-DEVTOOLS_PORT = 9222
-DEVTOOLS_HOSTNAME = 'localhost'
-DEFAULT_CHROME_PACKAGE = 'chrome'
+
+OPTIONS = options.OPTIONS
 
 
 @contextlib.contextmanager
@@ -178,13 +178,7 @@ def WprHost(device, wpr_archive_path, record=False):
     shutil.rmtree(temp_certificate_dir)
 
 @contextlib.contextmanager
-def DeviceConnection(device,
-                     package=DEFAULT_CHROME_PACKAGE,
-                     hostname=DEVTOOLS_HOSTNAME,
-                     port=DEVTOOLS_PORT,
-                     host_exe='out/Release/chrome',
-                     host_profile_dir=None,
-                     additional_flags=None):
+def DeviceConnection(device, additional_flags=None):
   """Context for starting recording on a device.
 
   Sets up and restores any device and tracing appropriately
@@ -192,21 +186,18 @@ def DeviceConnection(device,
   Args:
     device: Android device, or None for a local run (in which case chrome needs
       to have been started with --remote-debugging-port=XXX).
-    package: The key for chrome package info.
-    port: The port on which to enable remote debugging.
-    host_exe: The binary to execute when running on the host.
-    host_profile_dir: The profile dir to use when running on the host (if None,
-      a fresh profile dir will be used).
     additional_flags: Additional chromium arguments.
 
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
   """
-  package_info = constants.PACKAGE_INFO[package]
+  package_info = OPTIONS.ChromePackage()
   command_line_path = '/data/local/chrome-command-line'
   new_flags = ['--disable-fre',
                '--enable-test-events',
-               '--remote-debugging-port=%d' % port]
+               '--remote-debugging-port=%d' % OPTIONS.devtools_port]
+  if OPTIONS.no_sandbox:
+    new_flags.append('--no-sandbox')
   if additional_flags != None:
     new_flags.extend(additional_flags)
   if device:
@@ -220,14 +211,14 @@ def DeviceConnection(device,
       device.StartActivity(start_intent, blocking=True)
     else:
       # Run on the host.
-      assert os.path.exists(host_exe)
+      assert os.path.exists(OPTIONS.local_binary)
 
-      user_data_dir = host_profile_dir
-      if not user_data_dir:
-        user_data_dir = TemporaryDirectory()
+      local_profile_dir = OPTIONS.local_profile_dir
+      if not local_profile_dir:
+        local_profile_dir = TemporaryDirectory()
 
-      new_flags += ['--user-data-dir=%s' % user_data_dir]
-      host_process = subprocess.Popen([host_exe] + new_flags,
+      new_flags.append('--user-data-dir=%s' % local_profile_dir)
+      host_process = subprocess.Popen([OPTIONS.local_binary] + new_flags,
                                       shell=False)
     if device:
       time.sleep(2)
@@ -236,27 +227,9 @@ def DeviceConnection(device,
       # in request_track.py to fire.
       time.sleep(10)
     # If no device, we don't care about chrome startup so skip the about page.
-    with ForwardPort(device, 'tcp:%d' % port,
+    with ForwardPort(device, 'tcp:%d' % OPTIONS.devtools_port,
                      'localabstract:chrome_devtools_remote'):
-      yield devtools_monitor.DevToolsConnection(hostname, port)
+      yield devtools_monitor.DevToolsConnection(
+          OPTIONS.devtools_hostname, OPTIONS.devtools_port)
     if host_process:
       host_process.kill()
-
-
-def SetUpAndExecute(device, package, fn):
-  """Start logging process.
-
-  Wrapper for DeviceConnection for those functionally inclined.
-
-  Args:
-    device: Android device, or None for a local run.
-    package: the key for chrome package info.
-    fn: the function to execute that launches chrome and performs the
-        appropriate instrumentation. The function will receive a
-        DevToolsConnection as its sole parameter.
-
-  Returns:
-    As fn() returns.
-  """
-  with DeviceConnection(device, package) as connection:
-    return fn(connection)
diff --git a/loading/loading_model.py b/loading/loading_model.py
index b20d3eb..d2889e8 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -220,7 +220,7 @@ class ResourceGraph(object):
     Returns:
       A list of NodeInfo objects that describe the resources fetched.
     """
-    return self._node_info
+    return [n for n in self._node_info if n.Request() is not None]
 
   def DebugString(self):
     """Graph structure for debugging.
@@ -596,8 +596,9 @@ class ResourceGraph(object):
     """
     image_to_info = {}
     for n in self._node_info:
-      if (n.ContentType().startswith('image') and
-          not self._IsAdUrl(n.Url())):
+      if (n.ContentType() is not None and
+          n.ContentType().startswith('image') and
+          self.FilterAds(n)):
         key = str((n.Url(), n.ShortName(), n.StartTime()))
         assert key not in image_to_info, n.Url()
         image_to_info[key] = n
diff --git a/loading/options.py b/loading/options.py
new file mode 100644
index 0000000..f477cd2
--- /dev/null
+++ b/loading/options.py
@@ -0,0 +1,122 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import os.path
+import sys
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
+
+class Options(object):
+  """Global options repository.
+
+  ParseArgs must be called before use. See _ARGS for common members, these will
+  be available as instance attributes (eg, OPTIONS.clear_cache).
+  """
+  # Tuples of (argument name, default value, help string).
+  _ARGS = [ ('clear_cache', True,
+             'clear browser cache before loading'),
+            ('chrome_package_name', 'chrome',
+             'build/android/pylib/constants package description'),
+            ('devtools_hostname', 'localhost',
+             'hostname for devtools websocket connection'),
+            ('devtools_port', 9222,
+             'port for devtools websocket connection'),
+            ('local_binary', 'out/Release/chrome',
+             'chrome binary for local runs'),
+            ('local_profile_dir', '',
+             'profile directory to use for local runs'),
+            ('no_sandbox', False,
+             'pass --no-sandbox to browser (local run only; see also '
+             'https://chromium.googlesource.com/chromium/src/+/master/'
+             'docs/linux_suid_sandbox_development.md)'),
+          ]
+
+
+  def __init__(self):
+    self._arg_set = set()
+    self._parsed_args = None
+
+  def AddGlobalArgument(self, arg_name, default, help_str):
+    """Add a global argument.
+
+    Args:
+      arg_name: the name of the argument. This will be used as an optional --
+        argument.
+      default: the default value for the argument. The type of this default will
+        be used as the type of the argument.
+      help_str: the argument help string.
+    """
+    self._ARGS.append((arg_name, default, help_str))
+
+  def ParseArgs(self, arg_str, description=None, extra=None):
+    """Parse command line arguments.
+
+    Args:
+      arg_str: command line argument string.
+      description: description to use in argument parser.
+      extra: additional required arguments to add. These will be exposed as
+        instance attributes. This is either a list of extra arguments, or a
+        single string or tuple. If a tuple, the first item is the argument and
+        the second a default, otherwise the argument is required. Arguments are
+        used as in argparse, ie those beginning with -- are named, and those
+        without a dash are positional. Don't use a single dash.
+    """
+    parser = argparse.ArgumentParser(description=description)
+    for arg, default, help_str in self._ARGS:
+      # All global options are named.
+      arg = '--' + arg
+      self._AddArg(parser, arg, default, help_str=help_str)
+    if extra is not None:
+      if type(extra) is not list:
+        extra = [extra]
+      for arg in extra:
+        if type(arg) is tuple:
+          argname, default = arg
+          self._AddArg(parser, argname, default)
+        else:
+          self._AddArg(parser, arg, None, required=True)
+    self._parsed_args = parser.parse_args(arg_str)
+
+  def _AddArg(self, parser, arg, default, required=False, help_str=None):
+    assert not arg.startswith('-') or arg.startswith('--'), \
+        "Single dash arguments aren't supported: %s" % arg
+    arg_name = arg
+    if arg.startswith('--'):
+      arg_name = arg[2:]
+    assert arg_name not in self._arg_set, \
+      '%s extra arg is a duplicate' % arg_name
+    self._arg_set.add(arg_name)
+
+    kwargs = {}
+    if required and arg.startswith('--'):
+      kwargs['required'] = required
+    if help_str is not None:
+      kwargs['help'] = help_str
+    if default is not None:
+      if type(default) is bool:
+        # If the default of a switch is true, setting the flag stores false.
+        if default:
+          kwargs['action'] = 'store_false'
+        else:
+          kwargs['action'] = 'store_true'
+      else:
+        kwargs['default'] = default
+        kwargs['type'] = type(default)
+
+    parser.add_argument(arg, **kwargs)
+
+  def __getattr__(self, name):
+    if name in self._arg_set:
+      assert self._parsed_args, 'Option requested before ParseArgs called'
+      return getattr(self._parsed_args, name)
+    raise AttributeError(name)
+
+  def ChromePackage(self):
+    return constants.PACKAGE_INFO[self.chrome_package_name]
+
+OPTIONS = Options()

commit b289e1f886249c475776329f38eb28ab6d97d5a5
Author: agrieve <agrieve@chromium.org>
Date:   Thu Feb 11 12:15:48 2016 -0800

    Create wrapper scripts that set --output-directory
    
    For:
    - build/android/adb_gdb*
    - build/android/asan_symbolize.py
    - build/android/tombstones.py
    - third_party/android_platform/development/scripts/stack
    - tools/perf/run_benchmark
    
    BUG=573345
    
    Review URL: https://codereview.chromium.org/1663103004
    
    Cr-Original-Commit-Position: refs/heads/master@{#374964}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b8d204bc1b7018d3f32f0dda59d4b41655e67349

diff --git a/BUILD.gn b/BUILD.gn
index 4e074bd..bcadd0f 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -8,6 +8,8 @@
 # GYP: //tools/android/android_tools.gyp:android_tools
 group("android_tools") {
   deps = [
+    "//build/android:wrapper_scripts",
+    "//third_party/android_platform:stack_wrapper",
     "//third_party/catapult/telemetry:bitmaptools($host_toolchain)",
     "//tools/android/adb_reboot",
     "//tools/android/file_poller",
@@ -15,6 +17,7 @@ group("android_tools") {
     "//tools/android/md5sum",
     "//tools/android/memtrack_helper:memtrack_helper",
     "//tools/android/purge_ashmem",
+    "//tools/perf:run_benchmark_wrapper",
   ]
 }
 

commit f83b86fa6a2b96b258ac2e329b99013e025ccee4
Author: zhenw <zhenw@chromium.org>
Date:   Thu Feb 11 11:47:47 2016 -0800

    Use flag_changer in catapult devil.android
    
    This CL updates the dependency on pylib.flag_changer to
    Catapult's devil.android.flag_changer in Chromium repo.
    
    - The previous CL added devil.android.flag_changer in
    catapult: https://crrev.com/1675773002/
    - The next CL will remove pylib.flag_changer.
    
    BUG=583785
    
    Review URL: https://codereview.chromium.org/1677313002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374936}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5e903500791f38618b69b8258fc6a92124553f97

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 5782b58..1ad0388 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -16,12 +16,12 @@ _SRC_DIR = os.path.abspath(os.path.join(
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
 from devil.android import device_utils
+from devil.android import flag_changer
 from devil.android import forwarder
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
-from pylib import flag_changer
 
 sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
 from chrome_telemetry_build import chromium_config
diff --git a/mempressure.py b/mempressure.py
index 7d67d9d..ffa7c12 100755
--- a/mempressure.py
+++ b/mempressure.py
@@ -15,12 +15,12 @@ _SRC_PATH = os.path.abspath(os.path.join(
 sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil')
 from devil.android import device_errors
 from devil.android import device_utils
+from devil.android import flag_changer
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
 import devil_chromium
 from pylib import constants
-from pylib import flag_changer
 
 # Browser Constants
 DEFAULT_BROWSER = 'chrome'

commit 2f02f2c1b4300267809164cdf8e03332014e354f
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 11 07:55:35 2016 -0800

    sandwich: Adds web page replay support for HTTPS
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1685133002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374902}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: af2c7d276387b9a0930c22dbc0ba095aeae2c993

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 4986957..5782b58 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -29,6 +29,10 @@ from chrome_telemetry_build import chromium_config
 sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.util import webpagereplay
 
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'webpagereplay'))
+import adb_install_cert
+import certutils
+
 import devtools_monitor
 
 DEVTOOLS_PORT = 9222
@@ -135,6 +139,20 @@ def WprHost(device, wpr_archive_path, record=False):
       os.remove(wpr_archive_path)
   else:
     assert os.path.exists(wpr_archive_path)
+
+  # Deploy certification authority to the device.
+  temp_certificate_dir = tempfile.mkdtemp()
+  wpr_ca_cert_path = os.path.join(temp_certificate_dir, 'testca.pem')
+  certutils.write_dummy_ca_cert(*certutils.generate_dummy_ca_cert(),
+                                cert_path=wpr_ca_cert_path)
+
+  device_cert_util = adb_install_cert.AndroidCertInstaller(
+      device.adb.GetDeviceSerial(), None, wpr_ca_cert_path)
+  device_cert_util.install_cert(overwrite_cert=True)
+  wpr_server_args.extend(['--should_generate_certs',
+                          '--https_root_ca_cert_path=' + wpr_ca_cert_path])
+
+  # Set up WPR server and device forwarder.
   wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
       '127.0.0.1', 0, 0, None, wpr_server_args)
   ports = wpr_server.StartServer()[:-1]
@@ -155,6 +173,10 @@ def WprHost(device, wpr_archive_path, record=False):
     forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
     wpr_server.StopServer()
 
+    # Remove certification authority from the device.
+    device_cert_util.remove_cert()
+    shutil.rmtree(temp_certificate_dir)
+
 @contextlib.contextmanager
 def DeviceConnection(device,
                      package=DEFAULT_CHROME_PACKAGE,

commit 6f4253a4bea52030b61909e9a35f2d4810b3133a
Author: gabadie <gabadie@chromium.org>
Date:   Wed Feb 10 07:38:04 2016 -0800

    sandwich: Adds web page replay support for HTTP
    
    Adds to option --wpr-{archive,record} to respectively run with or
    generate the benchmark's Web Page Replay archive. Https urls
    support are postponed to another CL.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1684653003
    
    Cr-Original-Commit-Position: refs/heads/master@{#374662}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 90f06edabf6cb2880141ad3dfecd775a6559db2d

diff --git a/loading/device_setup.py b/loading/device_setup.py
index c384707..4986957 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -16,12 +16,19 @@ _SRC_DIR = os.path.abspath(os.path.join(
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
 from devil.android import device_utils
+from devil.android import forwarder
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 from pylib import flag_changer
 
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
+from chrome_telemetry_build import chromium_config
+
+sys.path.append(chromium_config.GetTelemetryDir())
+from telemetry.internal.util import webpagereplay
+
 import devtools_monitor
 
 DEVTOOLS_PORT = 9222
@@ -104,12 +111,58 @@ def _SetUpDevice(device, package_info):
 
 
 @contextlib.contextmanager
+def WprHost(device, wpr_archive_path, record=False):
+  """Launches web page replay host.
+
+  Args:
+    device: Android device.
+    wpr_archive_path: host sided WPR archive's path.
+    record: Enables or disables WPR archive recording.
+
+  Returns:
+    Additional flags list that may be used for chromium to load web page through
+    the running web page replay host.
+  """
+  assert device
+  if wpr_archive_path == None:
+    yield []
+    return
+
+  wpr_server_args = ['--use_closest_match']
+  if record:
+    wpr_server_args.append('--record')
+    if os.path.exists(wpr_archive_path):
+      os.remove(wpr_archive_path)
+  else:
+    assert os.path.exists(wpr_archive_path)
+  wpr_server = webpagereplay.ReplayServer(wpr_archive_path,
+      '127.0.0.1', 0, 0, None, wpr_server_args)
+  ports = wpr_server.StartServer()[:-1]
+  host_http_port = ports[0]
+  host_https_port = ports[1]
+
+  forwarder.Forwarder.Map([(0, host_http_port), (0, host_https_port)], device)
+  device_http_port = forwarder.Forwarder.DevicePortForHostPort(host_http_port)
+  device_https_port = forwarder.Forwarder.DevicePortForHostPort(host_https_port)
+
+  try:
+    yield [
+      '--host-resolver-rules="MAP * 127.0.0.1,EXCLUDE localhost"',
+      '--testing-fixed-http-port={}'.format(device_http_port),
+      '--testing-fixed-https-port={}'.format(device_https_port)]
+  finally:
+    forwarder.Forwarder.UnmapDevicePort(device_http_port, device)
+    forwarder.Forwarder.UnmapDevicePort(device_https_port, device)
+    wpr_server.StopServer()
+
+@contextlib.contextmanager
 def DeviceConnection(device,
                      package=DEFAULT_CHROME_PACKAGE,
                      hostname=DEVTOOLS_HOSTNAME,
                      port=DEVTOOLS_PORT,
                      host_exe='out/Release/chrome',
-                     host_profile_dir=None):
+                     host_profile_dir=None,
+                     additional_flags=None):
   """Context for starting recording on a device.
 
   Sets up and restores any device and tracing appropriately
@@ -122,6 +175,7 @@ def DeviceConnection(device,
     host_exe: The binary to execute when running on the host.
     host_profile_dir: The profile dir to use when running on the host (if None,
       a fresh profile dir will be used).
+    additional_flags: Additional chromium arguments.
 
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
@@ -131,6 +185,8 @@ def DeviceConnection(device,
   new_flags = ['--disable-fre',
                '--enable-test-events',
                '--remote-debugging-port=%d' % port]
+  if additional_flags != None:
+    new_flags.extend(additional_flags)
   if device:
     _SetUpDevice(device, package_info)
   with FlagReplacer(device, command_line_path, new_flags):
diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 7b688a7..8286c3c 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -160,6 +160,10 @@ def main():
                       action='store_true',
                       help='Clear HTTP cache before start,' +
                       'save cache before exit.')
+  parser.add_argument('--wpr-archive', default=None, type=str,
+                      help='Web page replay archive to load job\'s urls from.')
+  parser.add_argument('--wpr-record', default=False, action='store_true',
+                      help='Record web page replay archive.')
   args = parser.parse_args()
 
   try:
@@ -171,21 +175,26 @@ def main():
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
 
-  pages_loaded = 0
-  for iteration in xrange(args.repeat):
-    for url in job_urls:
-      with device_setup.DeviceConnection(device) as connection:
-        if iteration == 0 and pages_loaded == 0 and args.save_cache:
-          connection.ClearCache()
-        page_track.PageTrack(connection)
-        tracing_track = tracing.TracingTrack(connection,
-            categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
-        connection.SetUpMonitoring()
-        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-        connection.StartMonitoring()
-        pages_loaded += 1
-        _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
-            str(pages_loaded))
+  with device_setup.WprHost(device,
+                            args.wpr_archive,
+                            args.wpr_record) as additional_flags:
+    pages_loaded = 0
+    for iteration in xrange(args.repeat):
+      for url in job_urls:
+        with device_setup.DeviceConnection(
+            device=device,
+            additional_flags=additional_flags) as connection:
+          if iteration == 0 and pages_loaded == 0 and args.save_cache:
+            connection.ClearCache()
+          page_track.PageTrack(connection)
+          tracing_track = tracing.TracingTrack(connection,
+              categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
+          connection.SetUpMonitoring()
+          connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+          connection.StartMonitoring()
+          pages_loaded += 1
+          _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
+              str(pages_loaded))
 
   if args.save_cache:
     # Move Chrome to background to allow it to flush the index.

commit 027c959b8ab9e70bd7258c1918dd24460d54bbfa
Author: mostynb <mostynb@opera.com>
Date:   Mon Feb 8 15:27:20 2016 -0800

    update obsolete code.google.com documentation links
    
    This is a documentation-only change.
    
    Disabling presubmit checks, due to "noparent" settings for the following files:
    components/policy/resources/policy_templates.json
    content/common/font_config_ipc_linux.h
    
    BUG=567488
    NOPRESUBMIT=true
    TBR=atwilson,dcheng
    
    Review URL: https://codereview.chromium.org/1592403002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374213}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: df175a8c3d1c1060f96d123a5896fdfbe4b3782d

diff --git a/eclipse/.classpath b/eclipse/.classpath
index fb1fd36..b740bfe 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -2,7 +2,7 @@
 <!-- {% block header_message %}
 
 Eclipse classpath. See instructions at
-https://code.google.com/p/chromium/wiki/AndroidEclipseDev for setting up Eclipse
+https://www.chromium.org/developers/android-eclipse-dev for setting up Eclipse
 for Chrome Android development.
 
 Obsolete entries can be found using:
diff --git a/findbugs_plugin/findbugs.xml b/findbugs_plugin/findbugs.xml
index a1c4b5e..002e841 100644
--- a/findbugs_plugin/findbugs.xml
+++ b/findbugs_plugin/findbugs.xml
@@ -9,7 +9,7 @@
         xsi:noNamespaceSchemaLocation="findbugsplugin.xsd"
         pluginid="SynchronizedThisDetector"
         provider="chromium"
-        website="http://code.google.com/p/chromium/wiki/UseFindBugsForAndroid">
+        website="https://chromium.googlesource.com/chromium/src/+/master/docs/use_find_bugs_for_android.md">
         <Detector class="org.chromium.tools.findbugs.plugin.SynchronizedThisDetector" reports="CHROMIUM_SYNCHRONIZED_THIS" />
         <BugPattern type="CHROMIUM_SYNCHRONIZED_THIS" abbrev="CHROMIUM" category="CORRECTNESS"/>
 

commit e0c8b2afb00ba99c968a5d50f6c7b410d2b2df54
Author: newt <newt@chromium.org>
Date:   Mon Feb 8 15:24:51 2016 -0800

    Clean up dead code related to enhanced_bookmarks.
    
    This deletes the last bit of components/enhanced_bookmarks (by moving
    the enum into chrome/android/java, the only place where it's used),
    and cleans up dead code related to enhanced bookmarks.
    
    BUG=474719
    
    Review URL: https://codereview.chromium.org/1664503002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374210}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 407d1de019d5baadbd5eda34cc8f852da1ec41fc

diff --git a/eclipse/.classpath b/eclipse/.classpath
index ab36257..fb1fd36 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -115,7 +115,6 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/content_setting_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/content_settings_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/cronet_url_request_java"/>
-    <classpathentry kind="src" path="out/Debug/gen/enums/enhanced_bookmarks_java_enums_srcjar"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/gesture_event_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/infobar_action_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/invalidate_types_java"/>

commit 5916bb1e485c56d3ac5828a8c3d288c17d591744
Author: blundell <blundell@chromium.org>
Date:   Mon Feb 8 07:36:49 2016 -0800

    [loading] Extend ability to run on host
    
    This extends the functionality of running on the host:
    - Starts the executable before navigation
    - Kills it when done
    - Allows passing the executable
    - Allows running with a specified or fresh profile (the latter is currently
      blocked on a fix to crbug.com/xxx)
    
    Review URL: https://codereview.chromium.org/1669283002
    
    Cr-Original-Commit-Position: refs/heads/master@{#374126}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5ff3c6f038f45615cd53aed19790d6b82710e8e2

diff --git a/loading/analyze.py b/loading/analyze.py
index 1b1a2a3..5695e7d 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -95,29 +95,36 @@ def _GetPrefetchHtml(graph, name=None):
   return '\n'.join(output)
 
 
-def _LogRequests(url, clear_cache=True, local=False):
+def _LogRequests(url, clear_cache=True, local=False,
+                 host_exe="out/Release/chrome", host_profile_dir=None):
   """Log requests for a web page.
 
   Args:
     url: url to log as string.
     clear_cache: optional flag to clear the cache.
     local: log from local (desktop) chrome session.
+    host_exe: Binary to execute when running locally.
+    host_profile_dir: Profile dir to use when running locally (if None, a
+      fresh profile dir will be used).
 
   Returns:
     JSON dict of logged information (ie, a dict that describes JSON).
   """
   device = device_setup.GetFirstDevice() if not local else None
-  with device_setup.DeviceConnection(device) as connection:
+  with device_setup.DeviceConnection(device, host_exe=host_exe,
+      host_profile_dir=host_profile_dir) as connection:
     trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
     return trace.ToJsonDict()
 
 
-def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
+def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds,
+               host_exe, host_profile_dir):
   """Do a full fetch with optional prefetching."""
   if not url.startswith('http'):
     url = 'http://' + url
   logging.warning('Cold fetch')
-  cold_data = _LogRequests(url, local=local)
+  cold_data = _LogRequests(url, local=local,
+                           host_exe=host_exe, host_profile_dir=host_profile_dir)
   assert cold_data, 'Cold fetch failed to produce data. Check your phone.'
   if prefetch:
     assert not local
@@ -258,12 +265,16 @@ def DoLogRequests(arg_str):
   parser.add_argument('--prefetch', action='store_true')
   parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
   parser.add_argument('--local', action='store_true')
+  parser.add_argument('--host_exe', default='out/Release/chrome')
+  parser.add_argument('--host_profile_dir', default=None)
   args = parser.parse_args(arg_str)
   _FullFetch(url=args.url,
              json_output=args.output,
              prefetch=args.prefetch,
              prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=args.local)
+             local=args.local,
+             host_exe=args.host_exe,
+             host_profile_dir=args.host_profile_dir)
 
 
 def DoFetch(arg_str):
@@ -284,7 +295,9 @@ def DoFetch(arg_str):
              json_output=os.path.join(args.dir, args.site + '.json'),
              prefetch=True,
              prefetch_delay_seconds=args.prefetch_delay_seconds,
-             local=False)
+             local=False,
+             host_exe=None,
+             host_profile_dir=None)
 
 
 def DoLongPole(arg_str):
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 0f24668..c384707 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -5,7 +5,10 @@
 import contextlib
 import logging
 import os
+import shutil
+import subprocess
 import sys
+import tempfile
 import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
@@ -26,6 +29,18 @@ DEVTOOLS_HOSTNAME = 'localhost'
 DEFAULT_CHROME_PACKAGE = 'chrome'
 
 
+@contextlib.contextmanager
+def TemporaryDirectory():
+  """Returns a freshly-created directory that gets automatically deleted after
+  usage.
+  """
+  name = tempfile.mkdtemp()
+  try:
+    yield name
+  finally:
+    shutil.rmtree(name)
+
+
 class DeviceSetupException(Exception):
   def __init__(self, msg):
     super(DeviceSetupException, self).__init__(msg)
@@ -92,7 +107,9 @@ def _SetUpDevice(device, package_info):
 def DeviceConnection(device,
                      package=DEFAULT_CHROME_PACKAGE,
                      hostname=DEVTOOLS_HOSTNAME,
-                     port=DEVTOOLS_PORT):
+                     port=DEVTOOLS_PORT,
+                     host_exe='out/Release/chrome',
+                     host_profile_dir=None):
   """Context for starting recording on a device.
 
   Sets up and restores any device and tracing appropriately
@@ -100,7 +117,11 @@ def DeviceConnection(device,
   Args:
     device: Android device, or None for a local run (in which case chrome needs
       to have been started with --remote-debugging-port=XXX).
-    package: the key for chrome package info.
+    package: The key for chrome package info.
+    port: The port on which to enable remote debugging.
+    host_exe: The binary to execute when running on the host.
+    host_profile_dir: The profile dir to use when running on the host (if None,
+      a fresh profile dir will be used).
 
   Returns:
     A context manager type which evaluates to a DevToolsConnection.
@@ -113,16 +134,35 @@ def DeviceConnection(device,
   if device:
     _SetUpDevice(device, package_info)
   with FlagReplacer(device, command_line_path, new_flags):
+    host_process = None
     if device:
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
           data='about:blank')
       device.StartActivity(start_intent, blocking=True)
+    else:
+      # Run on the host.
+      assert os.path.exists(host_exe)
+
+      user_data_dir = host_profile_dir
+      if not user_data_dir:
+        user_data_dir = TemporaryDirectory()
+
+      new_flags += ['--user-data-dir=%s' % user_data_dir]
+      host_process = subprocess.Popen([host_exe] + new_flags,
+                                      shell=False)
+    if device:
       time.sleep(2)
+    else:
+      # TODO(blundell): Figure out why a lower sleep time causes an assertion
+      # in request_track.py to fire.
+      time.sleep(10)
     # If no device, we don't care about chrome startup so skip the about page.
     with ForwardPort(device, 'tcp:%d' % port,
                      'localabstract:chrome_devtools_remote'):
       yield devtools_monitor.DevToolsConnection(hostname, port)
+    if host_process:
+      host_process.kill()
 
 
 def SetUpAndExecute(device, package, fn):

commit c5cb8f90750eb8ac1f516220be4292154b3eefdc
Author: mattcary <mattcary@chromium.org>
Date:   Thu Feb 4 12:43:19 2016 -0800

    Lens for user satisfied metrics.
    
    Currently uses first contentful paint, but could be easily generalized or
    changed if we saw a need.
    
    This is also an experiment with a different sort of lens, that provides a filter
    rather than dependancy information.
    
    Review URL: https://codereview.chromium.org/1658933003
    
    Cr-Original-Commit-Position: refs/heads/master@{#373611}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8ba13fc45df1ba8c588bbe8abe4053d5c0f3145c

diff --git a/loading/test_utils.py b/loading/test_utils.py
index cc24f36..2e5bb74 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -7,6 +7,7 @@
 import devtools_monitor
 import loading_trace
 import page_track
+import tracing
 
 
 class FakeRequestTrack(devtools_monitor.Track):
@@ -39,8 +40,15 @@ class FakePageTrack(devtools_monitor.Track):
     return event['frame_id']
 
 
-def LoadingTraceFromEvents(requests, page_events=None):
+def LoadingTraceFromEvents(requests, page_events=None, trace_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
   request_track = FakeRequestTrack(requests)
-  page = FakePageTrack(page_events if page_events else [])
-  return loading_trace.LoadingTrace(None, None, page, request_track, None)
+  page_event_track = FakePageTrack(page_events if page_events else [])
+  if trace_events:
+    tracing_track = tracing.TracingTrack(None)
+    tracing_track.Handle('Tracing.dataCollected',
+                         {'params': {'value': [e for e in trace_events]}})
+  else:
+    tracing_track = None
+  return loading_trace.LoadingTrace(
+      None, None, page_event_track, request_track, tracing_track)
diff --git a/loading/user_satisfied_lens.py b/loading/user_satisfied_lens.py
new file mode 100644
index 0000000..0b23342
--- /dev/null
+++ b/loading/user_satisfied_lens.py
@@ -0,0 +1,51 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Identifies a user satisfied event, and marks all the relationship of all
+model events accordingly.
+"""
+
+import logging
+
+
+class UserSatisfiedLens(object):
+  def __init__(self, trace, graph):
+    """Initialize the lens.
+
+    Args:
+      trace: (LoadingTrace) the trace to use in the analysis.
+      graph: (ResourceGraph) the graph to annotate, using the current filter set
+        on the graph.
+    """
+    satisfied_time = self._GetFirstContentfulPaintTime(trace.tracing_track)
+    self._satisfied_nodes = set(self._GetNodesAfter(
+        graph.Nodes(), satisfied_time))
+
+  def Filter(self, node):
+    """A ResourceGraph filter.
+
+    Meant to be used in some_graph.Set(node_filter=this_lens.Filter).
+
+    Args:
+      node: a ResourceGraph NodeInfo.
+
+    Returns:
+      True iff the node occurred before user satisfaction occurred.
+    """
+    return node not in self._satisfied_nodes
+
+  # TODO(mattcary): hoist to base class?
+  @classmethod
+  def _GetNodesAfter(cls, nodes, time):
+    return (n for n in nodes if n.StartTime() >= time)
+
+  def _GetFirstContentfulPaintTime(self, tracing_track):
+    first_paints = [e.start_msec for e in tracing_track.GetEvents()
+                    if (e.tracing_event['name'] == 'firstContentfulPaint' and
+                        'blink.user_timing' in e.tracing_event['cat'])]
+    if len(first_paints) != 1:
+      # TODO(mattcary): in some cases a trace has two contentful paints. Why?
+      logging.error('%d first paints with spread of %s', len(first_paints),
+                    max(first_paints) - min(first_paints))
+    return min(first_paints)
diff --git a/loading/user_satisfied_lens_unittest.py b/loading/user_satisfied_lens_unittest.py
new file mode 100644
index 0000000..7b16f23
--- /dev/null
+++ b/loading/user_satisfied_lens_unittest.py
@@ -0,0 +1,56 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import loading_model
+import request_track
+import test_utils
+import user_satisfied_lens
+
+class UserSatisfiedLensTestCase(unittest.TestCase):
+  def setUp(self):
+    super(UserSatisfiedLensTestCase, self).setUp()
+    self._request_index = 1
+
+  def _RequestAt(self, timestamp_msec, duration=1):
+    timestamp_sec = float(timestamp_msec) / 1000
+    rq = request_track.Request.FromJsonDict({
+        'url': 'http://bla-%s-.com' % timestamp_msec,
+        'request_id': '1234.%s' % self._request_index,
+        'frame_id': '123.%s' % timestamp_msec,
+        'initiator': {'type': 'other'},
+        'timestamp': timestamp_sec,
+        'timing': request_track.TimingFromDict({
+            'requestTime': timestamp_sec,
+            'loadingFinished': duration})
+        })
+    self._request_index += 1
+    return rq
+
+  def testUserSatisfiedLens(self):
+    # We track all times in milliseconds, but raw trace events are in
+    # microseconds.
+    MILLI_TO_MICRO = 1000
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._RequestAt(1), self._RequestAt(10), self._RequestAt(20)],
+        trace_events=[{'ts': 0, 'ph': 'I',
+                       'cat': 'blink.some_other_user_timing',
+                       'name': 'firstContentfulPaint'},
+                      {'ts': 9 * MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstDiscontentPaint'},
+                      {'ts': 12 * MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint'},
+                      {'ts': 22 * MILLI_TO_MICRO, 'ph': 'I',
+                       'cat': 'blink.user_timing',
+                       'name': 'firstContentfulPaint'}])
+    graph = loading_model.ResourceGraph(loading_trace)
+    lens = user_satisfied_lens.UserSatisfiedLens(loading_trace, graph)
+    for n in graph.Nodes():
+      if n.Request().frame_id == '123.20':
+        self.assertFalse(lens.Filter(n))
+      else:
+        self.assertTrue(lens.Filter(n))

commit b4dbc20a942f4e7abb95fa8c2b9b79941a0354e4
Author: gogerald <gogerald@chromium.org>
Date:   Thu Feb 4 11:06:01 2016 -0800

    Implementation of newly designed sign in related histograms for Android.
    
    This CL is dedicated to implement newly designed sign in related histograms (Signin.SigninStartedAccessPoint, Signin.SigninCompletedAccessPoint, Signin.SigninReason) for Android. Please refer https://docs.google.com/a/google.com/document/d/1-gXYAMXXgsJhk6jxO55RuYJ00JBGermevJZ0sIlk6ko/edit?usp=sharing for details.
    
    BUG=532557
    
    Review URL: https://codereview.chromium.org/1578433002
    
    Cr-Original-Commit-Position: refs/heads/master@{#373577}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 5a9e2b34d01056dfdc1a60ce418f5e9f884c9967

diff --git a/eclipse/.classpath b/eclipse/.classpath
index 3f65bf4..ab36257 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -136,6 +136,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/result_codes_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/screen_orientation_values_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/selection_event_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/signin_metrics_enum_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/speech_recognition_error_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/system_ui_resource_type_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/tab_load_status_java"/>

commit 5e475ccf6b29ac33b33c03ca89a46aa1a33d8174
Author: lizeb <lizeb@chromium.org>
Date:   Thu Feb 4 08:17:48 2016 -0800

    tools/android/loading: Map the renderer main thread activity during intervals.
    
    Review URL: https://codereview.chromium.org/1670583002
    
    Cr-Original-Commit-Position: refs/heads/master@{#373545}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8a81a4ad08bffced9db75e261827d6dd94595aa8

diff --git a/loading/activity_lens.py b/loading/activity_lens.py
new file mode 100644
index 0000000..2973ac4
--- /dev/null
+++ b/loading/activity_lens.py
@@ -0,0 +1,179 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gives a picture of the CPU activity between timestamps.
+
+When executed as a script, takes a loading trace, and prints the activity
+breakdown for the request dependencies.
+"""
+
+import collections
+import logging
+import operator
+
+
+class ActivityLens(object):
+  """Reconstructs the activity of the main renderer thread between requests."""
+  def __init__(self, trace):
+    """Initializes an instance of ActivityLens.
+
+    Args:
+      trace: (LoadingTrace) loading trace.
+    """
+    self._trace = trace
+    events = trace.tracing_track.GetEvents()
+    self._renderer_main_tid = self._GetRendererMainThreadId(events)
+
+  @classmethod
+  def _GetRendererMainThreadId(cls, events):
+    """Returns the most active main renderer thread.
+
+    Several renderers may be running concurrently, but we assume that only one
+    of them is busy during the time covered by the loading trace.. It can be
+    selected by looking at the number of trace events generated.
+
+    Args:
+      events: [tracing.Event] List of trace events.
+
+    Returns:
+      The thread ID (int) of the busiest renderer main thread.
+
+    """
+    events_count_per_tid = collections.defaultdict(int)
+    main_renderer_thread_ids = set()
+    for event in events:
+      tracing_event = event.tracing_event
+      tid = event.tracing_event['tid']
+      events_count_per_tid[tid] += 1
+      if (tracing_event['cat'] == '__metadata'
+          and tracing_event['name'] == 'thread_name'
+          and event.args['name'] == 'CrRendererMain'):
+        main_renderer_thread_ids.add(tid)
+    tid_events_counts = sorted(events_count_per_tid.items(),
+                               key=operator.itemgetter(1), reverse=True)
+    if (len(tid_events_counts) > 1
+        and tid_events_counts[0][1] < 2 * tid_events_counts[1][1]):
+      logging.warning(
+          'Several active renderers (%d and %d with %d and %d events).'
+          % (tid_events_counts[0][0], tid_events_counts[1][0],
+             tid_events_counts[0][1], tid_events_counts[1][1]))
+    return tid_events_counts[0][0]
+
+  def _OverlappingEventsForTid(self, tid, start_msec, end_msec):
+    events = self._trace.tracing_track.OverlappingEvents(start_msec, end_msec)
+    return [e for e in events if e.tracing_event['tid'] == tid]
+
+  @classmethod
+  def _ClampedDuration(cls, event, start_msec, end_msec):
+      return max(0, (min(end_msec, event.end_msec)
+                     - max(start_msec, event.start_msec)))
+
+  @classmethod
+  def _ThreadBusiness(cls, events, start_msec, end_msec):
+    """Amount of time a thread spent executing from the message loop."""
+    busy_duration = 0
+    message_loop_events = [
+        e for e in events
+        if (e.tracing_event['cat'] == 'toplevel'
+            and e.tracing_event['name'] == 'MessageLoop::RunTask')]
+    for event in message_loop_events:
+      clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
+      busy_duration += clamped_duration
+    interval_msec = end_msec - start_msec
+    assert busy_duration <= interval_msec
+    return busy_duration
+
+  @classmethod
+  def _ScriptsExecuting(cls, events, start_msec, end_msec):
+    """Returns the time during which scripts executed within an interval.
+
+    Args:
+      events: ([tracing.Event]) list of tracing events.
+      start_msec: (float) start time in ms, inclusive.
+      end_msec: (float) end time in ms, inclusive.
+
+    Returns:
+      A dict {URL (str) -> duration_msec (float)}. The dict may have a None key
+      for scripts that aren't associated with a URL.
+    """
+    script_to_duration = collections.defaultdict(float)
+    script_events = [e for e in events
+                     if ('devtools.timeline' in e.tracing_event['cat']
+                         and e.tracing_event['name'] in (
+                             'EvaluateScript', 'FunctionCall'))]
+    for event in script_events:
+      clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
+      script_url = event.args['data'].get('scriptName', None)
+      script_to_duration[script_url] += clamped_duration
+    return dict(script_to_duration)
+
+  @classmethod
+  def _Parsing(cls, events, start_msec, end_msec):
+    """Returns the HTML/CSS parsing time within an interval.
+
+    Args:
+      events: ([tracing.Event]) list of events.
+      start_msec: (float) start time in ms, inclusive.
+      end_msec: (float) end time in ms, inclusive.
+
+    Returns:
+      A dict {URL (str) -> duration_msec (float)}. The dict may have a None key
+      for tasks that aren't associated with a URL.
+    """
+    url_to_duration = collections.defaultdict(float)
+    parsing_events = [e for e in events
+                      if ('devtools.timeline' in e.tracing_event['cat']
+                          and e.tracing_event['name'] in (
+                              'ParseHTML', 'ParseAuthorStyleSheet'))]
+    for event in parsing_events:
+      tracing_event = event.tracing_event
+      clamped_duration = cls._ClampedDuration(event, start_msec, end_msec)
+      if tracing_event['name'] == 'ParseAuthorStyleSheet':
+        url = tracing_event['args']['data']['styleSheetUrl']
+      else:
+        url = tracing_event['args']['beginData']['url']
+      url_to_duration[url] += clamped_duration
+    return dict(url_to_duration)
+
+  def ExplainEdgeCost(self, dep):
+    """For a dependency between two requests, returns the renderer activity
+    breakdown.
+
+    Args:
+      dep: (Request, Request, str) As returned from
+           RequestDependencyLens.GetRequestDependencies().
+
+    Returns:
+      {'edge_cost': (float) ms, 'busy': (float) ms,
+       'parsing': {'url' -> time_ms}, 'script' -> {'url' -> time_ms}}
+    """
+    (first, second, _) = dep
+    # TODO(lizeb): Refactor the edge cost computations.
+    start_msec = first.start_msec
+    end_msec = second.start_msec
+    assert end_msec - start_msec >= 0.
+    tid = self._renderer_main_tid
+    events = self._OverlappingEventsForTid(tid, start_msec, end_msec)
+    result = {'edge_cost': end_msec - start_msec,
+              'busy': self._ThreadBusiness(events, start_msec, end_msec),
+              'parsing': self._Parsing(events, start_msec, end_msec),
+              'script': self._ScriptsExecuting(events, start_msec, end_msec)}
+    return result
+
+
+if __name__ == '__main__':
+  import sys
+  import json
+  import loading_trace
+  import request_dependencies_lens
+
+  filename = sys.argv[1]
+  json_dict = json.load(open(filename))
+  loading_trace = loading_trace.LoadingTrace.FromJsonDict(json_dict)
+  activity_lens = ActivityLens(loading_trace)
+  dependencies_lens = request_dependencies_lens.RequestDependencyLens(
+      loading_trace)
+  deps = dependencies_lens.GetRequestDependencies()
+  for requests_dep in deps:
+    print activity_lens.ExplainEdgeCost(requests_dep)
diff --git a/loading/activity_lens_unittest.py b/loading/activity_lens_unittest.py
new file mode 100644
index 0000000..a84c864
--- /dev/null
+++ b/loading/activity_lens_unittest.py
@@ -0,0 +1,175 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from activity_lens import ActivityLens
+import tracing
+
+
+class ActivityLensTestCast(unittest.TestCase):
+  @classmethod
+  def _EventsFromRawEvents(cls, raw_events):
+    tracing_track = tracing.TracingTrack(None)
+    tracing_track.Handle(
+        'Tracing.dataCollected', {'params': {'value': raw_events}})
+    return tracing_track.GetEvents()
+
+  def setUp(self):
+    self.tracing_track = tracing.TracingTrack(None)
+
+  def testGetRendererMainThread(self):
+    first_renderer_tid = 12345
+    second_renderer_tid = 123456
+    raw_events =  [
+        {u'args': {u'name': u'CrBrowserMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 123,
+         u'tid': 123,
+         u'ts': 0},
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 1234,
+         u'tid': first_renderer_tid,
+         u'ts': 0},
+        {u'args': {u'name': u'CrRendererMain'},
+         u'cat': u'__metadata',
+         u'name': u'thread_name',
+         u'ph': u'M',
+         u'pid': 12345,
+         u'tid': second_renderer_tid,
+         u'ts': 0}]
+    raw_events += [
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 32723,
+         u'tdur': 0,
+         u'tid': first_renderer_tid,
+         u'ts': 251427174674,
+         u'tts': 5107725}] * 100
+    raw_events += [
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 1234,
+         u'tdur': 0,
+         u'tid': second_renderer_tid,
+         u'ts': 251427174674,
+         u'tts': 5107725}] * 150
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(second_renderer_tid,
+                      ActivityLens._GetRendererMainThreadId(events))
+
+  def testThreadBusiness(self):
+    raw_events =  [
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 200 * 1000,
+         u'name': u'MessageLoop::RunTask',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 56485},
+        {u'args': {},
+         u'cat': u'toplevel',
+         u'dur': 8 * 200,
+         u'name': u'MessageLoop::NestedSomething',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 0}]
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(200, ActivityLens._ThreadBusiness(events, 0, 1000))
+    # Clamping duration.
+    self.assertEquals(100, ActivityLens._ThreadBusiness(events, 0, 100))
+    self.assertEquals(50, ActivityLens._ThreadBusiness(events, 25, 75))
+
+  def testScriptExecuting(self):
+    url = u'http://example.com/script.js'
+    raw_events = [
+        {u'args': {u'data': {u'scriptName': url}},
+         u'cat': u'devtools.timeline,v8',
+         u'dur': 250 * 1000,
+         u'name': u'FunctionCall',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tdur': 247,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 0},
+        {u'args': {u'data': {}},
+         u'cat': u'devtools.timeline,v8',
+         u'dur': 350 * 1000,
+         u'name': u'EvaluateScript',
+         u'ph': u'X',
+         u'pid': 123,
+         u'tdur': 247,
+         u'tid': 123,
+         u'ts': 0,
+         u'tts': 0}]
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(2, len(ActivityLens._ScriptsExecuting(events, 0, 1000)))
+    self.assertTrue(None in ActivityLens._ScriptsExecuting(events, 0, 1000))
+    self.assertEquals(
+        350, ActivityLens._ScriptsExecuting(events, 0, 1000)[None])
+    self.assertTrue(url in ActivityLens._ScriptsExecuting(events, 0, 1000))
+    self.assertEquals(250, ActivityLens._ScriptsExecuting(events, 0, 1000)[url])
+    # Aggreagates events.
+    raw_events.append({u'args': {u'data': {}},
+                       u'cat': u'devtools.timeline,v8',
+                       u'dur': 50 * 1000,
+                       u'name': u'EvaluateScript',
+                       u'ph': u'X',
+                       u'pid': 123,
+                       u'tdur': 247,
+                       u'tid': 123,
+                       u'ts': 0,
+                       u'tts': 0})
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(
+        350 + 50, ActivityLens._ScriptsExecuting(events, 0, 1000)[None])
+
+  def testParsing(self):
+    css_url = u'http://example.com/style.css'
+    html_url = u'http://example.com/yeah.htnl'
+    raw_events = [
+        {u'args': {u'data': {u'styleSheetUrl': css_url}},
+         u'cat': u'blink,devtools.timeline',
+         u'dur': 400 * 1000,
+         u'name': u'ParseAuthorStyleSheet',
+         u'ph': u'X',
+         u'pid': 32723,
+         u'tdur': 49721,
+         u'tid': 32738,
+         u'ts': 0,
+         u'tts': 216148},
+        {u'args': {u'beginData': {u'url': html_url}},
+         u'cat': u'devtools.timeline',
+         u'dur': 42 * 1000,
+         u'name': u'ParseHTML',
+         u'ph': u'X',
+         u'pid': 32723,
+         u'tdur': 49721,
+         u'tid': 32738,
+         u'ts': 0,
+         u'tts': 5032310},]
+    events = self._EventsFromRawEvents(raw_events)
+    self.assertEquals(2, len(ActivityLens._Parsing(events, 0, 1000)))
+    self.assertTrue(css_url in ActivityLens._Parsing(events, 0, 1000))
+    self.assertEquals(400, ActivityLens._Parsing(events, 0, 1000)[css_url])
+    self.assertTrue(html_url in ActivityLens._Parsing(events, 0, 1000))
+    self.assertEquals(42, ActivityLens._Parsing(events, 0, 1000)[html_url])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index f4cf0e6..52e22fb 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -28,6 +28,7 @@ _TIMING_NAMES_MAPPING = {
 
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
+
 def TimingAsList(timing):
   """Transform Timing to a list, eg as is used in JSON output.
 
@@ -40,6 +41,7 @@ def TimingAsList(timing):
   """
   return json.loads(json.dumps(timing))
 
+
 class Request(object):
   """Represents a single request.
 
diff --git a/loading/trace_to_chrome_trace.py b/loading/trace_to_chrome_trace.py
index 998614f..382d87d 100755
--- a/loading/trace_to_chrome_trace.py
+++ b/loading/trace_to_chrome_trace.py
@@ -5,8 +5,8 @@
 
 """Convert trace output for Chrome.
 
-Take the tracing track output from tracing_driver.py to a zip'd json that can be
-loading by chrome devtools tracing.
+Takes a loading trace from 'analyze.py log_requests' and outputs a zip'd json
+that can be loaded by chrome's about:tracing..
 """
 
 import argparse
@@ -19,5 +19,5 @@ if __name__ == '__main__':
   parser.add_argument('output')
   args = parser.parse_args()
   with gzip.GzipFile(args.output, 'w') as output_f, file(args.input) as input_f:
-    events = json.load(input_f)
+    events = json.load(input_f)['tracing_track']['events']
     json.dump({'traceEvents': events, 'metadata': {}}, output_f)
diff --git a/loading/tracing.py b/loading/tracing.py
index bc4d45e..c69ffaf 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -110,36 +110,47 @@ class TracingTrack(devtools_monitor.Track):
         tracing_track._base_msec = e.start_msec
     return tracing_track
 
-  def EventsEndingBetween(self, start_msec, end_msec):
-    """Gets the list of events whose end lies in a range.
+  def OverlappingEvents(self, start_msec, end_msec):
+    """Gets the list of events overlapping with an interval.
 
     Args:
       start_msec: the start of the range to query, in milliseconds, inclusive.
       end_msec: the end of the range to query, in milliseconds, inclusive.
 
     Returns:
-      List of events whose end time lies in the range. Note that although the
-      range is inclusive at both ends, an ending timestamp is considered to be
-      exclusive of the actual event. An event ending at 10 msec will be returned
-      for a range [10, 14] as well as [8, 10], though the event is considered to
-      end the instant before 10 msec. In practice this is only important when
-      considering how events overlap; an event ending at 10 msec does not
-      overlap with one starting at 10 msec and so may unambiguously share ids,
-      etc.
+      List of events overlapping with the range. Events are overlapping only if
+      the overlap is strictly larger than 0.
     """
     self._IndexEvents()
     low_idx = bisect.bisect_left(self._event_msec_index, start_msec) - 1
     high_idx = bisect.bisect_right(self._event_msec_index, end_msec)
-    matched_events = []
+    matched_events = set()
     for i in xrange(max(0, low_idx), high_idx):
       if self._event_lists[i]:
         for e in self._event_lists[i].event_list:
-          assert e.end_msec is not None
-          if e.end_msec >= start_msec and e.end_msec <= end_msec:
-            matched_events.append(e)
-    return matched_events
+          if e.end_msec is None:
+            continue
+          overlap_duration = max(
+              0, min(end_msec, e.end_msec) - max(start_msec, e.start_msec))
+          if overlap_duration > 0:
+            matched_events.add(e)
+    return list(matched_events)
+
+  def EventsEndingBetween(self, start_msec, end_msec):
+    """Gets the list of events ending within an interval.
 
-  def _IndexEvents(self):
+    Args:
+      start_msec: the start of the range to query, in milliseconds, inclusive.
+      end_msec: the end of the range to query, in milliseconds, inclusive.
+
+    Returns:
+      See OverlappingEvents() above.
+    """
+    overlapping_events = self.OverlappingEvents(start_msec, end_msec)
+    return [e for e in overlapping_events
+            if start_msec <= e.end_msec <= end_msec]
+
+  def _IndexEvents(self, strict=False):
     """Computes index for in-flight events.
 
     Creates a list of timestamps where events start or end, and tracks the
@@ -183,7 +194,8 @@ class TracingTrack(devtools_monitor.Track):
           if e.end_msec is not None and e.end_msec <= current_msec])
       self._event_msec_index.append(current_msec)
       self._event_lists.append(self._EventList(current_events))
-    if spanning_events.HasPending():
+
+    if strict and spanning_events.HasPending():
       raise devtools_monitor.DevToolsConnectionException(
           'Pending spanning events: %s' %
           '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
@@ -202,8 +214,9 @@ class TracingTrack(devtools_monitor.Track):
           'F': self._AsyncEnd,
           'N': self._ObjectCreated,
           'D': self._ObjectDestroyed,
-          'X': self._Ignore,
           'M': self._Ignore,
+          'X': self._Ignore,
+          'R': self._Ignore,
           None: self._Ignore,
           }
 
@@ -315,6 +328,7 @@ class Event(object):
                     'e': 'b',
                     'F': 'S',
                     'D': 'N'}
+  __slots__ = ('_tracing_event', 'start_msec', 'end_msec', '_synthetic')
   def __init__(self, tracing_event, synthetic=False):
     """Creates Event.
 
@@ -334,22 +348,14 @@ class Event(object):
 
     self._tracing_event = tracing_event
     # Note tracing event times are in microseconds.
-    self._start_msec = tracing_event['ts'] / 1000.0
-    self._end_msec = None
+    self.start_msec = tracing_event['ts'] / 1000.0
+    self.end_msec = None
     self._synthetic = synthetic
     if self.type == 'X':
       # Some events don't have a duration.
       duration = (tracing_event['dur']
                   if 'dur' in tracing_event else tracing_event['tdur'])
-      self._end_msec = self.start_msec + duration / 1000.0
-
-  @property
-  def start_msec(self):
-    return self._start_msec
-
-  @property
-  def end_msec(self):
-    return self._end_msec
+      self.end_msec = self.start_msec + duration / 1000.0
 
   @property
   def type(self):
@@ -383,6 +389,7 @@ class Event(object):
         'I', 'P', 'c', 'C',
         'n', 'T', 'p',  # TODO(mattcary): ?? instant types of async events.
         'O',            # TODO(mattcary): ?? object snapshot
+        'M'             # Metadata
         ]
 
   def Synthesize(self):
@@ -418,7 +425,7 @@ class Event(object):
         self.id != closing.id):
       raise devtools_monitor.DevToolsConnectionException(
         'Bad async closing: %s --> %s' % (self, closing))
-    self._end_msec = closing.start_msec
+    self.end_msec = closing.start_msec
     if 'args' in closing.tracing_event:
       self.tracing_event.setdefault(
           'args', {}).update(closing.tracing_event['args'])
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index d190f8a..0f30dcc 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import copy
 import logging
 import unittest
 
@@ -24,17 +25,27 @@ class TracingTrackTestCase(unittest.TestCase):
       {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
       {'ts': 15, 'ph': 'D', 'id': 1}]
 
+  _EVENTS = [
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+      {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
+      {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
+      {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
+      {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
+      {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]
+
   def setUp(self):
     self.track = TracingTrack(None)
 
   def EventToMicroseconds(self, event):
-    if 'ts' in event:
-      event['ts'] *= 1000
-    if 'dur' in event:
-      event['dur'] *= 1000
-    return event
+    result = copy.deepcopy(event)
+    if 'ts' in result:
+      result['ts'] *= 1000
+    if 'dur' in result:
+      result['dur'] *= 1000
+    return result
 
   def CheckTrack(self, timestamp, names):
+    self.track._IndexEvents(strict=True)
     self.assertEqual(
         set((e.args['name'] for e in self.track.EventsAt(timestamp))),
         set(names))
@@ -206,15 +217,8 @@ class TracingTrackTestCase(unittest.TestCase):
 
   def testEventsEndingBetween(self):
     self.track.Handle(
-        'Tracing.dataCollected',
-        {'params':
-         {'value': [self.EventToMicroseconds(e) for e in
-          [{'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
-           {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
-           {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
-           {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
-           {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
-           {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]]}})
+        'Tracing.dataCollected', {'params': {'value': [
+            self.EventToMicroseconds(e) for e in self._EVENTS]}})
     self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
                           for e in self.track.EventsEndingBetween(0, 100)]))
@@ -225,24 +229,22 @@ class TracingTrackTestCase(unittest.TestCase):
     self.assertEqual(set('B'),
                      set([e.args['name']
                           for e in self.track.EventsEndingBetween(3, 6)]))
-    self.assertEqual(set('AB'),
-                     set([e.args['name']
-                          for e in self.track.EventsEndingBetween(3, 7)]))
-    self.assertEqual(set('AB'),
-                     set([e.args['name']
-                          for e in self.track.EventsEndingBetween(6, 7)]))
-    self.assertEqual(set('A'),
+
+  def testOverlappingEvents(self):
+    self.track.Handle(
+        'Tracing.dataCollected', {'params': {'value': [
+            self.EventToMicroseconds(e) for e in self._EVENTS]}})
+    self.assertEqual(set('ABCDEF'),
                      set([e.args['name']
-                          for e in self.track.EventsEndingBetween(7, 10)]))
-    self.assertEqual(set('AC'),
+                          for e in self.track.OverlappingEvents(0, 100)]))
+    self.assertFalse([e.args['name']
+                      for e in self.track.OverlappingEvents(0, 2)])
+    self.assertEqual(set('BA'),
                      set([e.args['name']
-                          for e in self.track.EventsEndingBetween(7, 11)]))
-    self.assertEqual(set('CD'),
+                          for e in self.track.OverlappingEvents(4, 5.1)]))
+    self.assertEqual(set('ACD'),
                      set([e.args['name']
-                          for e in self.track.EventsEndingBetween(8, 13)]))
-
-
-
+                          for e in self.track.OverlappingEvents(6, 10.1)]))
 
 
 if __name__ == '__main__':

commit 14949a5c7193269c7603f2063b261445b217adb8
Author: gabadie <gabadie@chromium.org>
Date:   Thu Feb 4 06:15:31 2016 -0800

    tools/android/loading: Automatically --disable-fre at device setup.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1664693005
    
    Cr-Original-Commit-Position: refs/heads/master@{#373524}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 78c75fcb2f86c32aa7a1e96f9aa354bf43e141ce

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 8784024..0f24668 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -107,7 +107,8 @@ def DeviceConnection(device,
   """
   package_info = constants.PACKAGE_INFO[package]
   command_line_path = '/data/local/chrome-command-line'
-  new_flags = ['--enable-test-events',
+  new_flags = ['--disable-fre',
+               '--enable-test-events',
                '--remote-debugging-port=%d' % port]
   if device:
     _SetUpDevice(device, package_info)

commit cdd1f5dfca5944d58c533093cd236a2976c0a97f
Author: pkotwicz <pkotwicz@chromium.org>
Date:   Wed Feb 3 19:43:34 2016 -0800

    Add script which extracts defines and include dirs from clang
    
    This CL adds a script which generates an XML file which can be imported into an
    Eclipse CDT project. The combination of the XML file generated by
    generate_cdt_clang_settings.py and the XML file generated by
    "gn gen out/Release --ide=eclipse" is equivalent to the XML file generated by
    gyp/generator/eclipse.py
    
    generate_cdt_clang_settings.py writes an XML file with the include directories
    and the defines that all projects which use the clang compiler inherit.
    
    Using generate_cdt_clang_settings.py makes it no longer necessary to use GYP to
    generate Eclipse CDT settings.
    
    BUG=530676
    
    Review URL: https://codereview.chromium.org/1645383002
    
    Cr-Original-Commit-Position: refs/heads/master@{#373448}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: cfa2836a7fb20d603c33b6c25a6183264e43394c

diff --git a/eclipse/generate_cdt_clang_settings.py b/eclipse/generate_cdt_clang_settings.py
new file mode 100755
index 0000000..903d0f1
--- /dev/null
+++ b/eclipse/generate_cdt_clang_settings.py
@@ -0,0 +1,151 @@
+#!/usr/bin/env python
+#
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Generates XML file which can be imported into an Eclipse CDT project.
+
+The XML file contains the include directories and defines that all applications
+which use the clang compiler inherit. Should be used in conjunction with the
+XML file generated by "gn gen out/Release --ide=eclipse"
+"""
+
+
+from xml.sax.saxutils import escape
+import os
+import subprocess
+import sys
+
+def GetClangIncludeDirectories(compiler_path):
+  """Gets the system include directories as determined by the clang compiler.
+
+  Returns:
+    The list of include directories.
+  """
+
+  includes_set = set()
+
+  command = [compiler_path, '-E', '-xc++', '-v', '-']
+  proc = subprocess.Popen(args=command, stdin=subprocess.PIPE,
+                          stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+  output = proc.communicate()[1]
+  # Extract the list of include dirs from the output, which has this format:
+  #   ...
+  #   #include "..." search starts here:
+  #   #include <...> search starts here:
+  #    /usr/include/c++/4.6
+  #    /usr/local/include
+  #   End of search list.
+  #   ...
+  in_includes_list = False
+  for line in output.splitlines():
+    if line.startswith('#include'):
+      in_includes_list = True
+      continue
+    if line.startswith('End of search list.'):
+      break
+    if in_includes_list:
+      includes_set.add(line.strip())
+
+  return sorted(includes_set)
+
+
+def GetClangDefines(compiler_path):
+  """Gets the system defines as determined by the clang compiler.
+
+  Returns:
+    The dict of defines.
+  """
+
+  all_defines = {}
+  command = [compiler_path, '-E', '-dM', '-']
+  proc = subprocess.Popen(args=command, stdin=subprocess.PIPE,
+                          stdout=subprocess.PIPE)
+
+  # Extract the list of defines from the output, which has this format:
+  # #define __SIZEOF_INT__ 4
+  # ...
+  # #define unix 1
+  output = proc.communicate()[0]
+  for line in output.splitlines():
+    if not line.strip():
+      continue
+    line_parts = line.split(' ', 2)
+    key = line_parts[1]
+    if len(line_parts) >= 3:
+      val = line_parts[2]
+    else:
+      val = '1'
+    all_defines[key] = val
+
+  return all_defines
+
+
+def WriteIncludePaths(out, eclipse_langs, include_dirs):
+  """Write the includes section of a CDT settings export file."""
+
+  out.write('  <section name="org.eclipse.cdt.internal.ui.wizards.' \
+            'settingswizards.IncludePaths">\n')
+  out.write('    <language name="holder for library settings"></language>\n')
+  for lang in eclipse_langs:
+    out.write('    <language name="%s">\n' % lang)
+    for include_dir in include_dirs:
+      out.write('      <includepath workspace_path="false">%s</includepath>\n' %
+                include_dir)
+    out.write('    </language>\n')
+  out.write('  </section>\n')
+
+
+def WriteMacros(out, eclipse_langs, defines):
+  """Write the macros section of a CDT settings export file."""
+
+  out.write('  <section name="org.eclipse.cdt.internal.ui.wizards.' \
+            'settingswizards.Macros">\n')
+  out.write('    <language name="holder for library settings"></language>\n')
+  for lang in eclipse_langs:
+    out.write('    <language name="%s">\n' % lang)
+    for key in sorted(defines.iterkeys()):
+      out.write('      <macro><name>%s</name><value>%s</value></macro>\n' %
+                (escape(key), escape(defines[key])))
+    out.write('    </language>\n')
+  out.write('  </section>\n')
+
+
+def main(argv):
+  if len(argv) != 2:
+    print("Usage: generate_cdt_clang_settings.py destination_file")
+    return
+
+  compiler_path = os.path.abspath(
+      'third_party/llvm-build/Release+Asserts/bin/clang')
+  if not os.path.exists(compiler_path):
+    print('Please run this script from the Chromium src/ directory.')
+    return
+
+  include_dirs = GetClangIncludeDirectories(compiler_path)
+  if not include_dirs:
+    print('ERROR: Could not extract include dirs from %s.' % compiler_path)
+    return
+
+  defines = GetClangDefines(compiler_path)
+  if not defines:
+    print('ERROR: Could not extract defines from %s.' % compiler_path)
+
+  destination_file = os.path.abspath(argv[1])
+  destination_dir = os.path.dirname(destination_file)
+  if not os.path.exists(destination_dir):
+    os.makedirs(destination_dir)
+
+  with open(destination_file, 'w') as out:
+    eclipse_langs = ['C++ Source File', 'C Source File', 'Assembly Source File',
+                     'GNU C++', 'GNU C', 'Assembly']
+
+    out.write('<?xml version="1.0" encoding="UTF-8"?>\n')
+    out.write('<cdtprojectproperties>\n')
+    WriteIncludePaths(out, eclipse_langs, include_dirs)
+    WriteMacros(out, eclipse_langs, defines)
+    out.write('</cdtprojectproperties>\n')
+
+if __name__ == '__main__':
+  sys.exit(main(sys.argv))

commit 8d3613339a3e6bc0f76871ab5ca3da61a507381d
Author: tedchoc <tedchoc@chromium.org>
Date:   Wed Feb 3 12:39:33 2016 -0800

    Share suggestion highlighting behavior in Android omnibox.
    
    TBR=sky@chromium.org
    BUG=568220
    
    Review URL: https://codereview.chromium.org/1630833003
    
    Cr-Original-Commit-Position: refs/heads/master@{#373326}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2d34190c3f29355e6bd2ffea788a0d82fe8d1e7c

diff --git a/eclipse/.classpath b/eclipse/.classpath
index eef7d41..3f65bf4 100644
--- a/eclipse/.classpath
+++ b/eclipse/.classpath
@@ -97,6 +97,7 @@ to the classpath for downstream development. See "additional_entries" below.
     <classpathentry kind="src" path="out/Debug/gen/enums/accessibility_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/activity_type_ids_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/android_resource_type_java"/>
+    <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_java/"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/autocomplete_match_type_java/"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/ax_enumerations_java"/>
     <classpathentry kind="src" path="out/Debug/gen/enums/base_java_application_state"/>

commit fb5e61faa19b62b335dc4ca35f6065deb07c1754
Author: lizeb <lizeb@chromium.org>
Date:   Tue Feb 2 09:29:20 2016 -0800

    tools/android/loading: Properly handle 'third-party' AdBlock rules.
    
    Review URL: https://codereview.chromium.org/1658633003
    
    Cr-Original-Commit-Position: refs/heads/master@{#372972}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c8aa169f3214fc377dd3b1342b8167eeb0e77376

diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
index b12b9af..be05fcb 100644
--- a/loading/content_classification_lens.py
+++ b/loading/content_classification_lens.py
@@ -7,6 +7,7 @@
 import collections
 import logging
 import os
+import urlparse
 
 import loading_trace
 import request_track
@@ -31,6 +32,7 @@ class ContentClassificationLens(object):
     self._tracking_requests = set()
     self._ad_matcher = _RulesMatcher(ad_rules, True)
     self._tracking_matcher = _RulesMatcher(tracking_rules, True)
+    self._document_url = self._GetDocumentUrl()
     self._GroupRequestsByFrameId()
     self._LabelRequests()
 
@@ -73,11 +75,21 @@ class ContentClassificationLens(object):
   def _LabelRequests(self):
     for request in self._requests:
       request_id = request.request_id
-      if self._ad_matcher.Matches(request):
+      if self._ad_matcher.Matches(request, self._document_url):
         self._ad_requests.add(request_id)
-      if self._tracking_matcher.Matches(request):
+      if self._tracking_matcher.Matches(request, self._document_url):
         self._tracking_requests.add(request_id)
 
+  def _GetDocumentUrl(self):
+    main_frame_id = self._trace.page_track.GetMainFrameId()
+    # Take the last one as JS redirects can change the document URL.
+    document_url = None
+    for r in self._requests:
+      # 304: not modified.
+      if r.frame_id == main_frame_id and r.status in (200, 304):
+        document_url = r.document_url
+    return document_url
+
 
 class _RulesMatcher(object):
   """Matches requests with rules in Adblock+ format."""
@@ -106,20 +118,23 @@ class _RulesMatcher(object):
     else:
       self._matcher = None
 
-  def Matches(self, request):
+  def Matches(self, request, document_url):
     """Returns whether a request matches one of the rules."""
     if self._matcher is None:
       return False
     url = request.url
-    return self._matcher.should_block(url, self._GetOptions(request))
+    return self._matcher.should_block(
+        url, self._GetOptions(request, document_url))
 
   @classmethod
-  def _GetOptions(cls, request):
+  def _GetOptions(cls, request, document_url):
     options = {}
     resource_type = request.resource_type
     option = cls._RESOURCE_TYPE_TO_OPTIONS_KEY.get(resource_type)
     if option:
       options[option] = True
+    if cls._IsThirdParty(request.url, document_url):
+      options['third-party'] = True
     return options
 
   @classmethod
@@ -129,3 +144,30 @@ class _RulesMatcher(object):
     else:
       return [rule for rule in rules
               if not rule.startswith(cls._WHITELIST_PREFIX)]
+
+  @classmethod
+  def _IsThirdParty(cls, url, document_url):
+    # Common definition of "third-party" is "not from the same TLD+1".
+    # Unfortunately, knowing what is a TLD is not trivial. To do it without a
+    # database, we use the following simple (and incorrect) rules:
+    # - co.{in,uk,jp,hk} is a TLD
+    # - com.{au,hk} is a TLD
+    # Otherwise, this is the part after the last dot.
+    return cls._GetTldPlusOne(url) != cls._GetTldPlusOne(document_url)
+
+  @classmethod
+  def _GetTldPlusOne(cls, url):
+    hostname = urlparse.urlparse(url).hostname
+    if not hostname:
+      return hostname
+    parts = hostname.split('.')
+    if len(parts) <= 2:
+      return hostname
+    tld_parts_count = 1
+    may_be_tld = parts[-2:]
+    if may_be_tld[0] == 'co' and may_be_tld[1] in ('in', 'uk', 'jp'):
+      tld_parts_count = 2
+    elif may_be_tld[0] == 'com' and may_be_tld[1] in ('au', 'hk'):
+      tld_parts_count = 2
+    tld_plus_one = '.'.join(parts[-(tld_parts_count + 1):])
+    return tld_plus_one
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index b4f5ad1..6a64c86 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -12,19 +12,47 @@ import test_utils
 
 
 class ContentClassificationLensTestCase(unittest.TestCase):
+  _DOCUMENT_URL = 'http://bla.com'
+  _MAIN_FRAME_ID = '123.1'
   _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'document_url': _DOCUMENT_URL,
                                    'request_id': '1234.1',
-                                   'frame_id': '123.1',
+                                   'frame_id': _MAIN_FRAME_ID,
                                    'initiator': {'type': 'other'},
                                    'timestamp': 2,
+                                   'status': 200,
                                    'timing': TimingFromDict({})})
-  _MAIN_FRAME_ID = '123.1'
   _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
                    'frame_id': _MAIN_FRAME_ID},
                   {'method': 'Page.frameAttached',
                    'frame_id': '123.13', 'parent_frame_id': _MAIN_FRAME_ID}]
   _RULES = ['bla.com']
 
+  def testGetDocumentUrl(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertEquals(self._DOCUMENT_URL, lens._GetDocumentUrl())
+    # Don't be fooled by redirects.
+    request = copy.deepcopy(self._REQUEST)
+    request.status = 302
+    request.document_url = 'http://www.bla.com'
+    trace = test_utils.LoadingTraceFromEvents(
+        [request, self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertEquals(self._DOCUMENT_URL, lens._GetDocumentUrl())
+
+  def testGetDocumentUrlSeveralChanges(self):
+    request = copy.deepcopy(self._REQUEST)
+    request.status = 200
+    request.document_url = 'http://www.blabla.com'
+    request2 = copy.deepcopy(request)
+    request2.document_url = 'http://www.blablabla.com'
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST, request, request2], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertEquals(request2.document_url, lens._GetDocumentUrl())
+
   def testNoRules(self):
     trace = test_utils.LoadingTraceFromEvents(
         [self._REQUEST], self._PAGE_EVENTS)
@@ -53,18 +81,18 @@ class ContentClassificationLensTestCase(unittest.TestCase):
     self.assertFalse(lens.IsAdFrame(self._MAIN_FRAME_ID, .5))
 
   def testAdFrame(self):
-    request = self._REQUEST
+    request = copy.deepcopy(self._REQUEST)
     request.frame_id = '123.123'
     trace = test_utils.LoadingTraceFromEvents(
         [request] * 10 + [self._REQUEST] * 5, self._PAGE_EVENTS)
     lens = ContentClassificationLens(trace, self._RULES, [])
     self.assertTrue(lens.IsAdFrame(request.frame_id, .5))
 
-
 class _MatcherTestCase(unittest.TestCase):
   _RULES_WITH_WHITELIST = ['/thisisanad.', '@@myadvertisingdomain.com/*',
                            '@@||www.mydomain.com/ads/$elemhide']
   _SCRIPT_RULE = 'domainwithscripts.com/*$script'
+  _THIRD_PARTY_RULE = 'domainwithscripts.com/*$third-party'
   _SCRIPT_REQUEST = Request.FromJsonDict(
       {'url': 'http://domainwithscripts.com/bla.js',
        'resource_type': 'Script',
@@ -84,8 +112,29 @@ class _MatcherTestCase(unittest.TestCase):
     matcher = _RulesMatcher([self._SCRIPT_RULE], False)
     request = copy.deepcopy(self._SCRIPT_REQUEST)
     request.resource_type = 'Stylesheet'
-    self.assertFalse(matcher.Matches(request))
-    self.assertTrue(matcher.Matches(self._SCRIPT_REQUEST))
+    self.assertFalse(matcher.Matches(
+        request, ContentClassificationLensTestCase._DOCUMENT_URL))
+    self.assertTrue(matcher.Matches(
+        self._SCRIPT_REQUEST, ContentClassificationLensTestCase._DOCUMENT_URL))
+
+  def testGetTldPlusOne(self):
+    self.assertEquals(
+        'easy.com',
+        _RulesMatcher._GetTldPlusOne('http://www.easy.com/hello/you'))
+    self.assertEquals(
+        'not-so-easy.co.uk',
+        _RulesMatcher._GetTldPlusOne('http://www.not-so-easy.co.uk/hello/you'))
+    self.assertEquals(
+        'hard.co.uk',
+        _RulesMatcher._GetTldPlusOne('http://hard.co.uk/'))
+
+  def testThirdPartyRule(self):
+    matcher = _RulesMatcher([self._THIRD_PARTY_RULE], False)
+    request = copy.deepcopy(self._SCRIPT_REQUEST)
+    document_url = 'http://www.domainwithscripts.com/good-morning'
+    self.assertFalse(matcher.Matches(request, document_url))
+    document_url = 'http://anotherdomain.com/good-morning'
+    self.assertTrue(matcher.Matches(request, document_url))
 
 
 if __name__ == '__main__':
diff --git a/loading/page_track.py b/loading/page_track.py
index bb289db..bc904ba 100644
--- a/loading/page_track.py
+++ b/loading/page_track.py
@@ -9,6 +9,7 @@ class PageTrack(devtools_monitor.Track):
   """Records the events from the page track."""
   _METHODS = ('Page.frameStartedLoading', 'Page.frameStoppedLoading',
               'Page.frameAttached')
+  FRAME_STARTED_LOADING = 'Page.frameStartedLoading'
   def __init__(self, connection):
     super(PageTrack, self).__init__(connection)
     self._connection = connection
@@ -26,7 +27,7 @@ class PageTrack(devtools_monitor.Track):
     frame_id = params['frameId']
     should_stop = False
     event = {'method': method, 'frame_id': frame_id}
-    if method == 'Page.frameStartedLoading':
+    if method == self.FRAME_STARTED_LOADING:
       if self._main_frame_id is None:
         self._main_frame_id = params['frameId']
       self._pending_frames.add(frame_id)
@@ -53,6 +54,14 @@ class PageTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     return {'events': [event for event in self._events]}
 
+  def GetMainFrameId(self):
+    """Returns the Id (str) of the main frame, or raises a ValueError."""
+    for event in self._events:
+      if event['method'] == self.FRAME_STARTED_LOADING:
+        return event['frame_id']
+    else:
+      raise ValueError('No frame loads in the track.')
+
   @classmethod
   def FromJsonDict(cls, json_dict):
     assert 'events' in json_dict
diff --git a/loading/page_track_unittest.py b/loading/page_track_unittest.py
index 6757a01..3056d99 100644
--- a/loading/page_track_unittest.py
+++ b/loading/page_track_unittest.py
@@ -53,6 +53,13 @@ class PageTrackTest(unittest.TestCase):
     with self.assertRaises(AssertionError):
       page_track.Handle(msg['method'], msg)
 
+  def testGetMainFrameId(self):
+    devtools_connection = MockDevToolsConnection()
+    page_track = PageTrack(devtools_connection)
+    for msg in PageTrackTest._EVENTS:
+      page_track.Handle(msg['method'], msg)
+    self.assertEquals('1234.1', page_track.GetMainFrameId())
+
 
 if __name__ == '__main__':
   unittest.main()
diff --git a/loading/test_utils.py b/loading/test_utils.py
index 9051062..cc24f36 100644
--- a/loading/test_utils.py
+++ b/loading/test_utils.py
@@ -6,15 +6,7 @@
 
 import devtools_monitor
 import loading_trace
-
-
-class FakeTrack(devtools_monitor.Track):
-  def __init__(self, events):
-    super(FakeTrack, self).__init__(None)
-    self._events = events
-
-  def GetEvents(self):
-    return self._events
+import page_track
 
 
 class FakeRequestTrack(devtools_monitor.Track):
@@ -32,9 +24,23 @@ class FakeRequestTrack(devtools_monitor.Track):
     return event
 
 
+class FakePageTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakePageTrack, self).__init__(None)
+    self._events = events
+
+  def GetEvents(self):
+    return self._events
+
+  def GetMainFrameId(self):
+    event = self._events[0]
+    # Make sure our laziness is not an issue here.
+    assert event['method'] == page_track.PageTrack.FRAME_STARTED_LOADING
+    return event['frame_id']
+
+
 def LoadingTraceFromEvents(requests, page_events=None):
   """Returns a LoadingTrace instance from a list of requests and page events."""
   request_track = FakeRequestTrack(requests)
-  page_track = FakeTrack(page_events if page_events else [])
-  return loading_trace.LoadingTrace(
-      None, None, page_track, request_track, None)
+  page = FakePageTrack(page_events if page_events else [])
+  return loading_trace.LoadingTrace(None, None, page, request_track, None)

commit 4155f7eec5d0f18aa876409606c8f618c0a17e80
Author: mattcary <mattcary@chromium.org>
Date:   Tue Feb 2 06:38:38 2016 -0800

    Loading dependency analysis
    
    This defines a Lens that ads synthetic load events from the trace, and
    correlates them with request events via timestamp. This hasn't been
    sucessful in identifying the cause of long edges.
    
    This CL also does some refactoring to the loading model which I think is
    ultimately useful and should be retained. The DOT drawing has been
    pulled out into its own class and unittested, for a certain very limited
    concept of unittesting.
    
    Review URL: https://codereview.chromium.org/1641203002
    
    Cr-Original-Commit-Position: refs/heads/master@{#372939}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 40a628d4b55ace395c35d733c629ca543651b342

diff --git a/loading/analyze.py b/loading/analyze.py
index 5f15fe2..1b1a2a3 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -26,8 +26,10 @@ from pylib import constants
 
 import content_classification_lens
 import device_setup
+import frame_load_lens
 import loading_model
 import loading_trace
+import model_graph
 import trace_recorder
 
 
@@ -155,7 +157,8 @@ def _ProcessRequests(filename, ad_rules_filename='',
     content_lens = (
         content_classification_lens.ContentClassificationLens.WithRulesFiles(
             trace, ad_rules_filename, tracking_rules_filename))
-    return loading_model.ResourceGraph(trace, content_lens)
+    frame_lens = frame_load_lens.FrameLoadLens(trace)
+    return loading_model.ResourceGraph(trace, content_lens, frame_lens)
 
 
 def InvalidCommand(cmd):
@@ -188,7 +191,6 @@ def DoPng(arg_str):
   parser.add_argument('request_json')
   parser.add_argument('png_output', nargs='?')
   parser.add_argument('--eog', action='store_true')
-  parser.add_argument('--highlight')
   parser.add_argument('--noads', action='store_true')
   parser.add_argument('--ad_rules', default='')
   parser.add_argument('--tracking_rules', default='')
@@ -197,10 +199,9 @@ def DoPng(arg_str):
       args.request_json, args.ad_rules, args.tracking_rules)
   if args.noads:
     graph.Set(node_filter=graph.FilterAds)
+  visualization = model_graph.GraphVisualization(graph)
   tmp = tempfile.NamedTemporaryFile()
-  graph.MakeGraphviz(
-      tmp,
-      highlight=args.highlight.split(',') if args.highlight else None)
+  visualization.OutputDot(tmp)
   tmp.flush()
   png_output = args.png_output
   if not png_output:
diff --git a/loading/frame_load_lens.py b/loading/frame_load_lens.py
new file mode 100644
index 0000000..e0479ab
--- /dev/null
+++ b/loading/frame_load_lens.py
@@ -0,0 +1,109 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gathers and infers dependencies between requests.
+
+When executed as a script, loads a trace and outputs synthetic frame load nodes
+and the new introduced dependencies.
+"""
+
+import bisect
+import collections
+import logging
+import operator
+
+import loading_trace
+
+
+class FrameLoadLens(object):
+  """Analyses and creates request dependencies for inferred frame events."""
+  _FRAME_EVENT = 'RenderFrameImpl::didFinishLoad'
+  _REQUEST_TO_LOAD_GAP_MSEC = 100
+  _LOAD_TO_REQUEST_GAP_MSEC = 100
+  def __init__(self, trace):
+    """Instance initialization.
+
+    Args:
+      trace: (LoadingTrace) Loading trace.
+    """
+    self._frame_load_events = self._GetFrameLoadEvents(trace.tracing_track)
+    self._request_track = trace.request_track
+    self._tracing_track = trace.tracing_track
+    self._load_dependencies = []
+    self._request_dependencies = []
+    for i, load in enumerate(self._frame_load_events):
+      self._load_dependencies.extend(
+          [(i, r) for r in self._GetLoadDependencies(load)])
+      self._request_dependencies.extend(
+          [(r, i) for r in self._GetRequestDependencies(load)])
+
+  def GetFrameLoadInfo(self):
+    """Returns [(index, msec)]."""
+    return [collections.namedtuple('LoadInfo', ['index', 'msec'])._make(
+        (i, self._frame_load_events[i].start_msec))
+            for i in xrange(len(self._frame_load_events))]
+
+  def GetFrameResourceComplete(self, request_track):
+    """Returns [(frame id, msec)]."""
+    frame_to_end_msec = collections.defaultdict(int)
+    for r in request_track.GetEvents():
+      if r.end_msec > frame_to_end_msec[r.frame_id]:
+        frame_to_end_msec[r.frame_id] = r.end_msec
+    loads = []
+    for f in sorted(frame_to_end_msec.keys()):
+      loads.append((f, frame_to_end_msec[f]))
+    return loads
+
+  def GetFrameLoadDependencies(self):
+    """Returns a list of frame load dependencies.
+
+    Returns:
+      ([(frame load index, request), ...],
+       [(request, frame load index), ...]), where request are instances of
+      request_trace.Request, and frame load index is an integer. The first list
+      in the tuple gives the requests that are dependent on the given frame
+      load, and the second list gives the frame loads that are dependent on the
+      given request.
+    """
+    return (self._load_dependencies, self._request_dependencies)
+
+  def _GetFrameLoadEvents(self, tracing_track):
+    events = []
+    for e in tracing_track.GetEvents():
+      if e.tracing_event['name'] == self._FRAME_EVENT:
+        events.append(e)
+    return events
+
+  def _GetLoadDependencies(self, load):
+    for r in self._request_track.GetEventsStartingBetween(
+        load.start_msec, load.start_msec + self._LOAD_TO_REQUEST_GAP_MSEC):
+      yield r
+
+  def _GetRequestDependencies(self, load):
+    for r in self._request_track.GetEventsEndingBetween(
+        load.start_msec - self._REQUEST_TO_LOAD_GAP_MSEC, load.start_msec):
+      yield r
+
+
+if __name__ == '__main__':
+  import loading_trace
+  import json
+  import sys
+  lens = FrameLoadLens(loading_trace.LoadingTrace.FromJsonDict(
+      json.load(open(sys.argv[1]))))
+  load_times = lens.GetFrameLoadInfo()
+  for t in load_times:
+    print t
+  print (lens._request_track.GetFirstRequestMillis(),
+         lens._request_track.GetLastRequestMillis())
+  load_dep, request_dep = lens.GetFrameLoadDependencies()
+  rq_str = lambda r: '%s (%d-%d)' % (
+      r.request_id,
+      r.start_msec - lens._request_track.GetFirstRequestMillis(),
+      r.end_msec - lens._request_track.GetFirstRequestMillis())
+  load_str = lambda i: '%s (%d)' % (i, load_times[i][1])
+  for load_idx, request in load_dep:
+    print '%s -> %s' % (load_str(load_idx), rq_str(request))
+  for request, load_idx in request_dep:
+    print '%s -> %s' % (rq_str(request), load_str(load_idx))
diff --git a/loading/loading_model.py b/loading/loading_model.py
index e46b705..b20d3eb 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -30,19 +30,21 @@ class ResourceGraph(object):
   Set parameters:
     cache_all: if true, assume zero loading time for all resources.
   """
-  def __init__(self, trace, content_lens=None):
+  def __init__(self, trace, content_lens=None, frame_lens=None):
     """Create from a LoadingTrace (or json of a trace).
 
     Args:
       trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
       content_lens: (ContentClassificationLens) Lens used to annotate the
                     nodes, or None.
+      frame_lens: (FrameLoadLens) Lens used to augment graph with load nodes.
     """
     if type(trace) == dict:
       trace = loading_trace.LoadingTrace.FromJsonDict(trace)
+    self._trace = trace
     self._content_lens = content_lens
+    self._frame_lens = frame_lens
     self._BuildDag(trace)
-    self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
     # reparented child is actually a dependency for a child of its new parent.
     try:
@@ -112,15 +114,22 @@ class ResourceGraph(object):
     if node_filter is not None:
       self._node_filter = node_filter
 
-  def Nodes(self):
-    """Return iterable of all nodes via their _NodeInfos.
+  def Nodes(self, sort=False):
+    """Return iterable of all nodes via their NodeInfos.
+
+    Args:
+      sort: if true, return nodes in sorted order. This may prune additional
+        nodes from the unsorted list (eg, non-root, non-ad nodes reachable only
+        by ad nodes)
 
     Returns:
-      Iterable of node infos in arbitrary order.
+      Iterable of node infos.
+
     """
-    for n in self._node_info:
-      if self._node_filter(n.Node()):
-        yield n
+    if sort:
+      return (self._node_info[n.Index()]
+              for n in dag.TopologicalSort(self._nodes, self._node_filter))
+    return (n for n in self._node_info if self._node_filter(n.Node()))
 
   def EdgeCosts(self, node_filter=None):
     """Edge costs.
@@ -140,7 +149,7 @@ class ResourceGraph(object):
         continue
       for s in n.Node().Successors():
         if node_filter(s):
-          total += self._EdgeCost(n.Node(), s)
+          total += self.EdgeCost(n.Node(), s)
     return total
 
   def Intersect(self, other_nodes):
@@ -173,10 +182,10 @@ class ResourceGraph(object):
     for n in dag.TopologicalSort(self._nodes, self._node_filter):
       cost = 0
       if n.Predecessors():
-        cost = max([costs[p.Index()] + self._EdgeCost(p, n)
+        cost = max([costs[p.Index()] + self.EdgeCost(p, n)
                     for p in n.Predecessors()])
       if not self._cache_all:
-        cost += self._NodeCost(n)
+        cost += self.NodeCost(n)
       costs[n.Index()] = cost
     max_cost = max(costs)
     assert max_cost > 0  # Otherwise probably the filter went awry.
@@ -205,68 +214,11 @@ class ResourceGraph(object):
     node_info = self._node_info[node.Index()]
     return not (node_info.IsAd() or node_info.IsTracking())
 
-  def MakeGraphviz(self, output, highlight=None):
-    """Output a graphviz representation of our DAG.
-
-    Args:
-      output: a file-like output stream which recieves a graphviz dot.
-      highlight: a list of node items to emphasize. Any resource url which
-        contains any highlight text will be distinguished in the output.
-    """
-    output.write("""digraph dependencies {
-    rankdir = LR;
-    """)
-    orphans = set()
-    try:
-      sorted_nodes = dag.TopologicalSort(self._nodes,
-                                         node_filter=self._node_filter)
-    except AssertionError as exc:
-      sys.stderr.write('Bad topological sort: %s\n'
-                       'Writing children in order\n' % str(exc))
-      sorted_nodes = self._nodes
-    for n in sorted_nodes:
-      if not n.Successors() and not n.Predecessors():
-        orphans.add(n)
-    if orphans:
-      output.write("""subgraph cluster_orphans {
-  color=black;
-  label="Orphans";
-""")
-      for n in orphans:
-        output.write(self._GraphvizNode(n.Index(), highlight))
-      output.write('}\n')
-
-    output.write("""subgraph cluster_nodes {
-  color=invis;
-""")
-    for n in sorted_nodes:
-      if not n.Successors() and not n.Predecessors():
-        continue
-      output.write(self._GraphvizNode(n.Index(), highlight))
-
-    for n in sorted_nodes:
-      for s in n.Successors():
-        style = 'color = orange'
-        annotations = self._EdgeAnnotation(n, s)
-        if 'redirect' in annotations:
-          style = 'color = black'
-        elif 'parser' in annotations:
-          style = 'color = red'
-        elif 'stack' in annotations:
-          style = 'color = blue'
-        elif 'script_inferred' in annotations:
-          style = 'color = purple'
-        if 'timing' in annotations:
-          style += '; style=dashed'
-        arrow = '[%s; label="%s"]' % (style, self._EdgeCost(n, s))
-        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
-    output.write('}\n}\n')
-
   def ResourceInfo(self):
     """Get resource info.
 
     Returns:
-      A list of _NodeInfo objects that describe the resources fetched.
+      A list of NodeInfo objects that describe the resources fetched.
     """
     return self._node_info
 
@@ -295,32 +247,46 @@ class ResourceGraph(object):
     assert len(visited) == len(self._nodes)
     return '\n'.join(output)
 
+  def NodeInfo(self, node):
+    """Return the node info for a graph node.
+
+    Args:
+      node: (int, dag.Node or NodeInfo) a node representation. An int is taken
+      to be the node's index.
+
+    Returns:
+      The NodeInfo instance corresponding to the node.
+    """
+    if type(node) is self._NodeInfo:
+      return node
+    elif type(node) is int:
+      return self._node_info[node]
+    return self._node_info[node.Index()]
+
+  def ShortName(self, node):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(node).ShortName()
+
+  def Url(self, node):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(node).Url()
+
+  def NodeCost(self, node):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(node).NodeCost()
+
+  def EdgeCost(self, parent, child):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(parent).EdgeCost(self.NodeInfo(child))
+
+  def EdgeAnnotation(self, parent, child):
+    """Convenience function for redirecting to NodeInfo."""
+    return self.NodeInfo(parent).EdgeAnnotation(self.NodeInfo(child))
+
   ##
   ## Internal items
   ##
 
-  _CONTENT_KIND_TO_COLOR = {
-      'application':     'blue',      # Scripts.
-      'font':            'grey70',
-      'image':           'orange',    # This probably catches gifs?
-      'video':           'hotpink1',
-      }
-
-  _CONTENT_TYPE_TO_COLOR = {
-      'html':            'red',
-      'css':             'green',
-      'script':          'blue',
-      'javascript':      'blue',
-      'json':            'purple',
-      'gif':             'grey',
-      'image':           'orange',
-      'jpeg':            'orange',
-      'png':             'orange',
-      'plain':           'brown3',
-      'octet-stream':    'brown3',
-      'other':           'white',
-      }
-
   # This resource type may induce a timing dependency. See _SplitChildrenByTime
   # for details.
   # TODO(mattcary): are these right?
@@ -342,19 +308,28 @@ class ResourceGraph(object):
 
       Args:
         node: The node to augment.
-        request: The request associated with this node.
+        request: The request associated with this node, or an (index, msec)
+          tuple.
       """
-      self._request = request
+      self._node = node
       self._is_ad = False
       self._is_tracking = False
-      self._node = node
       self._edge_costs = {}
       self._edge_annotations = {}
-      # All fields in timing are millis relative to request_time, which is epoch
-      # seconds.
-      self._node_cost = max(
-          [0] + [t for f, t in request.timing._asdict().iteritems()
-                 if f != 'request_time'])
+
+      if type(request) == tuple:
+        self._request = None
+        self._node_cost = 0
+        self._shortname = 'LOAD %s' % request[0]
+        self._start_time = request[1]
+      else:
+        self._shortname = None
+        self._start_time = None
+        self._request = request
+        # All fields in timing are millis relative to request_time.
+        self._node_cost = max(
+            [0] + [t for f, t in request.timing._asdict().iteritems()
+                   if f != 'request_time'])
 
     def __str__(self):
       return self.ShortName()
@@ -387,25 +362,31 @@ class ResourceGraph(object):
       return self._node_cost
 
     def EdgeCost(self, s):
-      return self._edge_costs[s]
+      return self._edge_costs.get(s, 0)
 
     def StartTime(self):
+      if self._start_time:
+        return self._start_time
       return self._request.timing.request_time * 1000
 
     def EndTime(self):
-      return self._request.timing.request_time * 1000 + self._node_cost
+      return self.StartTime() + self._node_cost
 
     def EdgeAnnotation(self, s):
       assert s.Node() in self.Node().Successors()
       return self._edge_annotations.get(s, [])
 
     def ContentType(self):
+      if self._request is None:
+        return 'synthetic'
       return self._request.GetContentType()
 
     def ShortName(self):
       """Returns either the hostname of the resource, or the filename,
       or the end of the path. Tries to include the domain as much as possible.
       """
+      if self._shortname:
+        return self._shortname
       parsed = urlparse.urlparse(self._request.url)
       path = parsed.path
       hostname = parsed.hostname if parsed.hostname else '?.?.?'
@@ -441,9 +422,9 @@ class ResourceGraph(object):
       old_parent.RemoveSuccessor(), etc.
 
       Args:
-        old_parent: the _NodeInfo of a current parent of self. We assert this
+        old_parent: the NodeInfo of a current parent of self. We assert this
           is actually a parent.
-        new_parent: the _NodeInfo of the new parent. We assert it is not already
+        new_parent: the NodeInfo of the new parent. We assert it is not already
           a parent.
       """
       assert old_parent.Node() in self.Node().Predecessors()
@@ -457,37 +438,16 @@ class ResourceGraph(object):
         new_parent.AddEdgeAnnotation(self, a)
 
     def __eq__(self, o):
-      return self.Node().Index() == o.Node().Index()
+      """Note this works whether o is a Node or a NodeInfo."""
+      return self.Index() == o.Index()
 
     def __hash__(self):
       return hash(self.Node().Index())
 
-  def _ShortName(self, node):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[node.Index()].ShortName()
-
-  def _Url(self, node):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[node.Index()].Url()
-
-  def _NodeCost(self, node):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[node.Index()].NodeCost()
-
-  def _EdgeCost(self, parent, child):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[parent.Index()].EdgeCost(
-        self._node_info[child.Index()])
-
-  def _EdgeAnnotation(self, parent, child):
-    """Convenience function for redirecting Nodes to _NodeInfo."""
-    return self._node_info[parent.Index()].EdgeAnnotation(
-        self._node_info[child.Index()])
-
   def _BuildDag(self, trace):
     """Build DAG of resources.
 
-    Build a DAG from our requests and augment with _NodeInfo (see above) in a
+    Build a DAG from our requests and augment with NodeInfo (see above) in a
     parallel array indexed by Node.Index().
 
     Creates self._nodes and self._node_info.
@@ -528,6 +488,32 @@ class ResourceGraph(object):
       parent.SetEdgeCost(child, edge_cost)
       parent.AddEdgeAnnotation(child, reason)
 
+    self._AugmentFrameLoads(index_by_request)
+
+  def _AugmentFrameLoads(self, index_by_request):
+    if not self._frame_lens:
+      return
+    loads = self._frame_lens.GetFrameLoadInfo()
+    load_index_to_node = {}
+    for l in loads:
+      next_index = len(self._nodes)
+      node = dag.Node(next_index)
+      node_info = self._NodeInfo(node, (l.index, l.msec))
+      load_index_to_node[l.index] = next_index
+      self._nodes.append(node)
+      self._node_info.append(node_info)
+    frame_deps = self._frame_lens.GetFrameLoadDependencies()
+    for load_idx, rq in frame_deps[0]:
+      parent = self._node_info[load_index_to_node[load_idx]]
+      child = self._node_info[index_by_request[rq]]
+      parent.Node().AddSuccessor(child.Node())
+      parent.AddEdgeAnnotation(child, 'after-load')
+    for rq, load_idx in frame_deps[1]:
+      child = self._node_info[load_index_to_node[load_idx]]
+      parent = self._node_info[index_by_request[rq]]
+      parent.Node().AddSuccessor(child.Node())
+      parent.AddEdgeAnnotation(child, 'before-load')
+
   def _SplitChildrenByTime(self, parent):
     """Split children of a node by request times.
 
@@ -599,50 +585,6 @@ class ResourceGraph(object):
         current.ReparentTo(parent, children_by_end_time[end_mark])
         children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
 
-  def _ContentTypeToColor(self, content_type):
-    if not content_type:
-      type_str = 'other'
-    elif '/' in content_type:
-      kind, type_str = content_type.split('/', 1)
-      if kind in self._CONTENT_KIND_TO_COLOR:
-        return self._CONTENT_KIND_TO_COLOR[kind]
-    else:
-      type_str = content_type
-    return self._CONTENT_TYPE_TO_COLOR[type_str]
-
-  def _GraphvizNode(self, index, highlight):
-    """Returns a graphviz node description for a given node.
-
-    Args:
-      index: index of the node.
-      highlight: a list of node items to emphasize. Any resource url which
-        contains any highlight text will be distinguished in the output.
-
-    Returns:
-      A string describing the resource in graphviz format.
-      The resource is color-coded according to its content type, and its shape
-      is oval if its max-age is less than 300s (or if it's not cacheable).
-    """
-    node_info = self._node_info[index]
-    color = self._ContentTypeToColor(node_info.ContentType())
-    max_age = node_info.Request().MaxAge()
-    shape = 'polygon' if max_age > 300 else 'oval'
-    styles = ['filled']
-    if highlight:
-      for fragment in highlight:
-        if fragment in node_info.Url():
-          styles.append('dotted')
-          break
-    if node_info.IsAd() or node_info.IsTracking():
-      styles += ['bold', 'diagonals']
-    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
-            'fillcolor = %s; shape = %s];\n'
-            % (index, node_info.ShortName(),
-               node_info.StartTime() - self._global_start,
-               node_info.EndTime() - self._global_start,
-               node_info.EndTime() - node_info.StartTime(),
-               ','.join(styles), color, shape))
-
   def _ExtractImages(self):
     """Return interesting image resources.
 
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 88a62d9..7afe8ef 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -32,9 +32,13 @@ class SimpleLens(object):
 class LoadingModelTestCase(unittest.TestCase):
 
   def setUp(self):
+    self.old_lens = request_dependencies_lens.RequestDependencyLens
     request_dependencies_lens.RequestDependencyLens = SimpleLens
     self._next_request_id = 0
 
+  def tearDown(self):
+    request_dependencies_lens.RequestDependencyLens = self.old_lens
+
   def MakeParserRequest(self, url, source_url, start_time, end_time,
                         magic_content_type=False):
     timing = request_track.TimingAsList(request_track.TimingFromDict({
diff --git a/loading/model_graph.py b/loading/model_graph.py
new file mode 100644
index 0000000..5c0bf09
--- /dev/null
+++ b/loading/model_graph.py
@@ -0,0 +1,161 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Visualize a loading_model.ResourceGraph."""
+
+import dag
+import itertools
+
+
+class GraphVisualization(object):
+  """Manipulate visual representations of a resource graph.
+
+  The output will change as the ResourceGraph is changed, for example by setting
+  filters.
+
+  Currently only DOT output is supported.
+  """
+  _LONG_EDGE_THRESHOLD_MS = 2000  # Time in milliseconds.
+
+  _CONTENT_KIND_TO_COLOR = {
+      'application':     'blue',      # Scripts.
+      'font':            'grey70',
+      'image':           'orange',    # This probably catches gifs?
+      'video':           'hotpink1',
+      }
+
+  _CONTENT_TYPE_TO_COLOR = {
+      'html':            'red',
+      'css':             'green',
+      'script':          'blue',
+      'javascript':      'blue',
+      'json':            'purple',
+      'gif':             'grey',
+      'image':           'orange',
+      'jpeg':            'orange',
+      'png':             'orange',
+      'plain':           'brown3',
+      'octet-stream':    'brown3',
+      'other':           'white',
+      'synthetic':       'yellow',
+      }
+
+  def __init__(self, graph):
+    """Initialize.
+
+    Args:
+      graph: (loading_model.ResourceGraph) the graph to visualize.
+    """
+    self._graph = graph
+    self._global_start = None
+
+  def OutputDot(self, output):
+    """Output DOT (graphviz) representation.
+
+    Args:
+      output: a file-like output stream to receive the dot file.
+    """
+    sorted_nodes = [n for n in self._graph.Nodes(sort=True)]
+    self._global_start = min([n.StartTime() for n in sorted_nodes])
+    visited_nodes = set([n for n in sorted_nodes])
+
+    output.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+
+    orphans = set()
+    for n in sorted_nodes:
+      for s in itertools.chain(n.Node().Successors(),
+                               n.Node().Predecessors()):
+        if s in visited_nodes:
+          break
+      else:
+        orphans.add(n)
+    if orphans:
+      output.write("""subgraph cluster_orphans {
+                        color=black;
+                        label="Orphans";
+                   """)
+      for n in orphans:
+        # Ignore synthetic nodes for orphan display.
+        if not self._graph.NodeInfo(n).Request():
+          continue
+        output.write(self.DotNode(n))
+      output.write('}\n')
+
+    output.write("""subgraph cluster_nodes {
+                      color=invis;
+                 """)
+
+    for n in sorted_nodes:
+      if n in orphans:
+        continue
+      output.write(self.DotNode(n))
+
+    for n in visited_nodes:
+      for s in n.Node().Successors():
+        if s not in visited_nodes:
+          continue
+        style = 'color = orange'
+        annotations = self._graph.EdgeAnnotation(n, s)
+        if 'redirect' in annotations:
+          style = 'color = black'
+        elif 'parser' in annotations:
+          style = 'color = red'
+        elif 'stack' in annotations:
+          style = 'color = blue'
+        elif 'script_inferred' in annotations:
+          style = 'color = purple'
+        if 'after-load' in annotations or 'before-load' in annotations:
+          style = 'color = forestgreen'
+        if 'timing' in annotations:
+          style += '; style=dashed'
+        if self._graph.EdgeCost(n, s) > self._LONG_EDGE_THRESHOLD_MS:
+          style += '; penwidth=5; weight=2'
+        arrow = '[%s; label="%s"]' % (style, self._graph.EdgeCost(n, s))
+        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
+    output.write('}\n')
+
+    output.write('}\n')
+
+  def _ContentTypeToColor(self, content_type):
+    if not content_type:
+      type_str = 'other'
+    elif '/' in content_type:
+      kind, type_str = content_type.split('/', 1)
+      if kind in self._CONTENT_KIND_TO_COLOR:
+        return self._CONTENT_KIND_TO_COLOR[kind]
+    else:
+      type_str = content_type
+    return self._CONTENT_TYPE_TO_COLOR[type_str]
+
+  def DotNode(self, node):
+    """Returns a graphviz node description for a given node.
+
+    Args:
+      node: a dag.Node or ResourceGraph node info.
+
+    Returns:
+      A string describing the resource in graphviz format.
+      The resource is color-coded according to its content type, and its shape
+      is oval if its max-age is less than 300s (or if it's not cacheable).
+    """
+    if type(node) is dag.Node:
+      node = self._graph.NodeInfo(node)
+    color = self._ContentTypeToColor(node.ContentType())
+    if node.Request():
+      max_age = node.Request().MaxAge()
+      shape = 'polygon' if max_age > 300 else 'oval'
+    else:
+      shape = 'doubleoctagon'
+    styles = ['filled']
+    if node.IsAd() or node.IsTracking():
+      styles += ['bold', 'diagonals']
+    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
+            'fillcolor = %s; shape = %s];\n'
+            % (node.Index(), node.ShortName(),
+               node.StartTime() - self._global_start,
+               node.EndTime() - self._global_start,
+               node.EndTime() - node.StartTime(),
+               ','.join(styles), color, shape))
diff --git a/loading/model_graph_unittest.py b/loading/model_graph_unittest.py
new file mode 100644
index 0000000..510e945
--- /dev/null
+++ b/loading/model_graph_unittest.py
@@ -0,0 +1,29 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import gzip
+import json
+import os.path
+import tempfile
+import unittest
+
+import frame_load_lens
+import loading_model
+import loading_trace
+import model_graph
+
+TEST_DATA_DIR = os.path.join(os.path.dirname(__file__), 'testdata')
+
+class ModelGraphTestCase(unittest.TestCase):
+  _ROLLING_STONE = os.path.join(TEST_DATA_DIR, 'rollingstone.trace.gz')
+
+  def test_EndToEnd(self):
+    # Test that we don't crash. This also runs through frame_load_lens.
+    tmp = tempfile.NamedTemporaryFile()
+    with gzip.GzipFile(self._ROLLING_STONE) as f:
+      trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
+      frame_lens = frame_load_lens.FrameLoadLens(trace)
+      graph = loading_model.ResourceGraph(trace=trace, frame_lens=frame_lens)
+      visualization = model_graph.GraphVisualization(graph)
+      visualization.OutputDot(tmp)
diff --git a/loading/request_track.py b/loading/request_track.py
index 566fb3c..f4cf0e6 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -7,6 +7,7 @@
 When executed, parses a JSON dump of DevTools messages.
 """
 
+import bisect
 import collections
 import copy
 import json
@@ -72,6 +73,9 @@ class Request(object):
                  chunks received, with their offset in ms relative to
                  Timing.requestTime.
     failed: (bool) Whether the request failed.
+    start_msec: (float) Request start time, in milliseconds from chrome start.
+    end_msec: (float) Request end time, in milliseconds from chrome start.
+      start_msec.
   """
   REQUEST_PRIORITIES = ('VeryLow', 'Low', 'Medium', 'High', 'VeryHigh')
   RESOURCE_TYPES = ('Document', 'Stylesheet', 'Image', 'Media', 'Font',
@@ -104,6 +108,19 @@ class Request(object):
     self.data_chunks = []
     self.failed = False
 
+  @property
+  def start_msec(self):
+    return self.timing.request_time * 1000
+
+  @property
+  def end_msec(self):
+    if self.start_msec is None:
+      return None
+    return self.start_msec + max(
+        [0] + [t for f, t in self.timing._asdict().iteritems()
+               if f != 'request_time'])
+
+
   def _TimestampOffsetFromStartMs(self, timestamp):
     assert self.timing.request_time != -1
     request_time = self.timing.request_time
@@ -193,6 +210,11 @@ class RequestTrack(devtools_monitor.Track):
     self._requests_in_flight = {}  # requestId -> (request, status)
     self._completed_requests_by_id = {}
     self._redirects_count_by_id = collections.defaultdict(int)
+    self._indexed = False
+    self._request_start_timestamps = None
+    self._request_end_timestamps = None
+    self._requests_by_start = None
+    self._requests_by_end = None
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
@@ -204,6 +226,7 @@ class RequestTrack(devtools_monitor.Track):
 
   def Handle(self, method, msg):
     assert method in RequestTrack._METHOD_TO_HANDLER
+    self._indexed = False
     params = msg['params']
     request_id = params['requestId']
     RequestTrack._METHOD_TO_HANDLER[method](self, request_id, params)
@@ -214,6 +237,56 @@ class RequestTrack(devtools_monitor.Track):
                       % len(self._requests_in_flight))
     return self._requests
 
+  def GetFirstRequestMillis(self):
+    """Find the canonical start time for this track.
+
+    Returns:
+      The millisecond timestamp of the first request.
+    """
+    assert self._requests, "No requests to analyze."
+    self._IndexRequests()
+    return self._request_start_timestamps[0]
+
+  def GetLastRequestMillis(self):
+    """Find the canonical start time for this track.
+
+    Returns:
+      The millisecond timestamp of the first request.
+    """
+    assert self._requests, "No requests to analyze."
+    self._IndexRequests()
+    return self._request_end_timestamps[-1]
+
+  def GetEventsStartingBetween(self, start_ms, end_ms):
+    """Return events that started in a range.
+
+    Args:
+      start_ms: the start time to query, in milliseconds from the first request.
+      end_ms: the end time to query, in milliseconds from the first request.
+
+    Returns:
+      A list of requests whose start time is in [start_ms, end_ms].
+    """
+    self._IndexRequests()
+    low = bisect.bisect_left(self._request_start_timestamps, start_ms)
+    high = bisect.bisect_right(self._request_start_timestamps, end_ms)
+    return self._requests_by_start[low:high]
+
+  def GetEventsEndingBetween(self, start_ms, end_ms):
+    """Return events that ended in a range.
+
+    Args:
+      start_ms: the start time to query, in milliseconds from the first request.
+      end_ms: the end time to query, in milliseconds from the first request.
+
+    Returns:
+      A list of requests whose end time is in [start_ms, end_ms].
+    """
+    self._IndexRequests()
+    low = bisect.bisect_left(self._request_end_timestamps, start_ms)
+    high = bisect.bisect_right(self._request_end_timestamps, end_ms)
+    return self._requests_by_end[low:high]
+
   def ToJsonDict(self):
     if self._requests_in_flight:
       logging.warning('Requests in flight, will be ignored in the dump')
@@ -238,6 +311,24 @@ class RequestTrack(devtools_monitor.Track):
         cls._INCONSISTENT_INITIATORS_KEY, 0)
     return result
 
+  def _IndexRequests(self):
+    # TODO(mattcary): if we ever have requests without timing then we either
+    # need a default, or to make an index that only includes requests with
+    # timings.
+    if self._indexed:
+      return
+    valid_requests = [r for r in self._requests
+                      if r.start_msec is not None]
+    self._requests_by_start = sorted(valid_requests,
+                                     key=lambda r: r.start_msec)
+    self._request_start_timestamps = [r.start_msec
+                                      for r in self._requests_by_start]
+    self._requests_by_end = sorted(valid_requests,
+                                     key=lambda r: r.end_msec)
+    self._request_end_timestamps = [r.end_msec
+                                    for r in self._requests_by_end]
+    self._indexed = True
+
   def _RequestWillBeSent(self, request_id, params):
     # Several "requestWillBeSent" events can be dispatched in a row in the case
     # of redirects.
diff --git a/loading/testdata/rollingstone.trace.gz b/loading/testdata/rollingstone.trace.gz
new file mode 100644
index 0000000..f4a239e
Binary files /dev/null and b/loading/testdata/rollingstone.trace.gz differ
diff --git a/loading/tracing.py b/loading/tracing.py
index 4959ed7..bc4d45e 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -43,15 +43,27 @@ class TracingTrack(devtools_monitor.Track):
 
     self._event_msec_index = None
     self._event_lists = None
+    self._base_msec = None
 
   def Handle(self, method, event):
     for e in event['params']['value']:
-      self._events.append(Event(e))
+      event = Event(e)
+      self._events.append(event)
+      if self._base_msec is None or event.start_msec < self._base_msec:
+        self._base_msec = event.start_msec
     # Just invalidate our indices rather than trying to be fancy and
     # incrementally update.
     self._event_msec_index = None
     self._event_lists = None
 
+  def GetFirstEventMillis(self):
+    """Find the canonical start time for this track.
+
+    Returns:
+      The millisecond timestamp of the first request.
+    """
+    return self._base_msec
+
   def GetEvents(self):
     return self._events
 
@@ -89,6 +101,13 @@ class TracingTrack(devtools_monitor.Track):
     events = [Event(e) for e in json_dict['events']]
     tracing_track = TracingTrack(None)
     tracing_track._events = events
+    tracing_track._base_msec = events[0].start_msec if events else 0
+    for e in events[1:]:
+      if e.type == 'M':
+        continue  # No timestamp for metadata events.
+      assert e.start_msec > 0
+      if e.start_msec < tracing_track._base_msec:
+        tracing_track._base_msec = e.start_msec
     return tracing_track
 
   def EventsEndingBetween(self, start_msec, end_msec):
@@ -129,7 +148,6 @@ class TracingTrack(devtools_monitor.Track):
     join and track the nesting of async, flow and other spanning events.
 
     Events such as instant and counter events that aren't indexable are skipped.
-
     """
     if self._event_msec_index is not None:
       return  # Already indexed.
@@ -185,6 +203,7 @@ class TracingTrack(devtools_monitor.Track):
           'N': self._ObjectCreated,
           'D': self._ObjectDestroyed,
           'X': self._Ignore,
+          'M': self._Ignore,
           None: self._Ignore,
           }
 

commit 90e5144b8ae4e976cff67db733a6b201974c80bb
Author: pasko <pasko@chromium.org>
Date:   Tue Feb 2 05:13:38 2016 -0800

    sandwich: Allow saving cache directory
    
    Adds an option --save-cache to run_sandwich.py to pull the cache directory from
    the device after all page loads completed, preserfing modification times.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1651193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#372931}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3438bda5ae0b01193ea61e88cef435dac3ab0d69

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
index 647e18e..7b688a7 100755
--- a/loading/run_sandwich.py
+++ b/loading/run_sandwich.py
@@ -15,6 +15,7 @@ import argparse
 import logging
 import os
 import sys
+import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -23,6 +24,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
 from devil.android import device_utils
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+from pylib import constants
 import devil_chromium
 
 import device_setup
@@ -34,6 +36,24 @@ import tracing
 
 _JOB_SEARCH_PATH = 'sandwich_jobs'
 
+# Directory name under --output to save the cache from the device.
+_CACHE_DIRECTORY_NAME = 'cache'
+
+# Name of cache subdirectory on the device where the cache index is stored.
+_INDEX_DIRECTORY_NAME = 'index-dir'
+
+# Name of the file containing the cache index. This file is stored on the device
+# in the cache directory under _INDEX_DIRECTORY_NAME.
+_REAL_INDEX_FILE_NAME = 'the-real-index'
+
+# Name of the chrome package.
+_CHROME_PACKAGE = (
+    constants.PACKAGE_INFO[device_setup.DEFAULT_CHROME_PACKAGE].package)
+
+# An estimate of time to wait for the device to become idle after expensive
+# operations, such as opening the launcher activity.
+_TIME_TO_DEVICE_IDLE_SECONDS = 2
+
 
 def _ReadUrlsFromJobDescription(job_name):
   """Retrieves the list of URLs associated with the job name."""
@@ -65,16 +85,66 @@ def _SaveChromeTrace(events, directory, subdirectory):
     subdirectory: directory name to create this particular trace in
   """
   target_directory = os.path.join(directory, subdirectory)
-  file_name = os.path.join(target_directory, 'trace.json')
+  filename = os.path.join(target_directory, 'trace.json')
   try:
     os.makedirs(target_directory)
-    with open(file_name, 'w') as f:
+    with open(filename, 'w') as f:
       json.dump({'traceEvents': events['events'], 'metadata': {}}, f)
   except IOError:
-    logging.warning('Could not save a trace: %s' % file_name)
+    logging.warning('Could not save a trace: %s' % filename)
     # Swallow the exception.
 
 
+def _UpdateTimestampFromAdbStat(filename, stat):
+  os.utime(filename, (stat.st_time, stat.st_time))
+
+
+def _SaveBrowserCache(device, output_directory):
+  """Pulls the browser cache from the device and saves it locally.
+
+  Cache is saved with the same file structure as on the device. Timestamps are
+  important to preserve because indexing and eviction depends on them.
+
+  Args:
+    output_directory: name of the directory for saving cache.
+  """
+  save_target = os.path.join(output_directory, _CACHE_DIRECTORY_NAME)
+  try:
+    os.makedirs(save_target)
+  except IOError:
+    logging.warning('Could not create directory: %s' % save_target)
+    raise
+
+  cache_directory = '/data/data/' + _CHROME_PACKAGE + '/cache/Cache'
+  for filename, stat in device.adb.Ls(cache_directory):
+    if filename == '..':
+      continue
+    if filename == '.':
+      cache_directory_stat = stat
+      continue
+    original_file = os.path.join(cache_directory, filename)
+    saved_file = os.path.join(save_target, filename)
+    device.adb.Pull(original_file, saved_file)
+    _UpdateTimestampFromAdbStat(saved_file, stat)
+    if filename == _INDEX_DIRECTORY_NAME:
+      # The directory containing the index was pulled recursively, update the
+      # timestamps for known files. They are ignored by cache backend, but may
+      # be useful for debugging.
+      index_dir_stat = stat
+      saved_index_dir = os.path.join(save_target, _INDEX_DIRECTORY_NAME)
+      saved_index_file = os.path.join(saved_index_dir, _REAL_INDEX_FILE_NAME)
+      for sub_file, sub_stat in device.adb.Ls(original_file):
+        if sub_file == _REAL_INDEX_FILE_NAME:
+          _UpdateTimestampFromAdbStat(saved_index_file, sub_stat)
+          break
+      _UpdateTimestampFromAdbStat(saved_index_dir, index_dir_stat)
+
+  # Store the cache directory modification time. It is important to update it
+  # after all files in it have been written. The timestamp is compared with
+  # the contents of the index file when freshness is determined.
+  _UpdateTimestampFromAdbStat(save_target, cache_directory_stat)
+
+
 def main():
   logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
@@ -86,6 +156,10 @@ def main():
                       help='Name of output directory to create.')
   parser.add_argument('--repeat', default=1, type=int,
                       help='How many times to run the job')
+  parser.add_argument('--save-cache', default=False,
+                      action='store_true',
+                      help='Clear HTTP cache before start,' +
+                      'save cache before exit.')
   args = parser.parse_args()
 
   try:
@@ -96,10 +170,13 @@ def main():
 
   job_urls = _ReadUrlsFromJobDescription(args.job)
   device = device_utils.DeviceUtils.HealthyDevices()[0]
+
   pages_loaded = 0
-  for _ in xrange(args.repeat):
+  for iteration in xrange(args.repeat):
     for url in job_urls:
       with device_setup.DeviceConnection(device) as connection:
+        if iteration == 0 and pages_loaded == 0 and args.save_cache:
+          connection.ClearCache()
         page_track.PageTrack(connection)
         tracing_track = tracing.TracingTrack(connection,
             categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
@@ -110,6 +187,14 @@ def main():
         _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
             str(pages_loaded))
 
+  if args.save_cache:
+    # Move Chrome to background to allow it to flush the index.
+    device.adb.Shell('am start com.google.android.launcher')
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    device.KillAll(_CHROME_PACKAGE, quiet=True)
+    time.sleep(_TIME_TO_DEVICE_IDLE_SECONDS)
+    _SaveBrowserCache(device, args.output)
+
 
 if __name__ == '__main__':
   sys.exit(main())

commit 61ad3844d5ac9493e425007e3d00bfaed8f06091
Author: pasko <pasko@chromium.org>
Date:   Thu Jan 28 09:04:09 2016 -0800

    Script to load a list of URLs in a loop and save traces.
    
    BUG=582080
    
    Review URL: https://codereview.chromium.org/1645003003
    
    Cr-Original-Commit-Position: refs/heads/master@{#372111}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6f4d9bc54affbe4eb8a26adc5121481093e1e886

diff --git a/loading/run_sandwich.py b/loading/run_sandwich.py
new file mode 100755
index 0000000..647e18e
--- /dev/null
+++ b/loading/run_sandwich.py
@@ -0,0 +1,115 @@
+#! /usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Instructs Chrome to load series of web pages and reports results.
+
+When running Chrome is sandwiched between preprocessed disk caches and
+WepPageReplay serving all connections.
+
+TODO(pasko): implement cache preparation and WPR.
+"""
+
+import argparse
+import logging
+import os
+import sys
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
+
+import device_setup
+import devtools_monitor
+import json
+import page_track
+import tracing
+
+
+_JOB_SEARCH_PATH = 'sandwich_jobs'
+
+
+def _ReadUrlsFromJobDescription(job_name):
+  """Retrieves the list of URLs associated with the job name."""
+  try:
+    # Extra sugar: attempt to load from a relative path.
+    json_file_name = os.path.join(os.path.dirname(__file__), _JOB_SEARCH_PATH,
+        job_name)
+    with open(json_file_name) as f:
+      json_data = json.load(f)
+  except IOError:
+    # Attempt to read by regular file name.
+    with open(job_name) as f:
+      json_data = json.load(f)
+
+  key = 'urls'
+  if json_data and key in json_data:
+    url_list = json_data[key]
+    if isinstance(url_list, list) and len(url_list) > 0:
+      return url_list
+  raise Exception('Job description does not define a list named "urls"')
+
+
+def _SaveChromeTrace(events, directory, subdirectory):
+  """Saves the trace events, ignores IO errors.
+
+  Args:
+    events: a dict as returned by TracingTrack.ToJsonDict()
+    directory: directory name contining all traces
+    subdirectory: directory name to create this particular trace in
+  """
+  target_directory = os.path.join(directory, subdirectory)
+  file_name = os.path.join(target_directory, 'trace.json')
+  try:
+    os.makedirs(target_directory)
+    with open(file_name, 'w') as f:
+      json.dump({'traceEvents': events['events'], 'metadata': {}}, f)
+  except IOError:
+    logging.warning('Could not save a trace: %s' % file_name)
+    # Swallow the exception.
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+  devil_chromium.Initialize()
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--job', required=True,
+                      help='JSON file with job description.')
+  parser.add_argument('--output', required=True,
+                      help='Name of output directory to create.')
+  parser.add_argument('--repeat', default=1, type=int,
+                      help='How many times to run the job')
+  args = parser.parse_args()
+
+  try:
+    os.makedirs(args.output)
+  except OSError:
+    logging.error('Cannot create directory for results: %s' % args.output)
+    raise
+
+  job_urls = _ReadUrlsFromJobDescription(args.job)
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  pages_loaded = 0
+  for _ in xrange(args.repeat):
+    for url in job_urls:
+      with device_setup.DeviceConnection(device) as connection:
+        page_track.PageTrack(connection)
+        tracing_track = tracing.TracingTrack(connection,
+            categories='blink,cc,netlog,renderer.scheduler,toplevel,v8')
+        connection.SetUpMonitoring()
+        connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+        connection.StartMonitoring()
+        pages_loaded += 1
+        _SaveChromeTrace(tracing_track.ToJsonDict(), args.output,
+            str(pages_loaded))
+
+
+if __name__ == '__main__':
+  sys.exit(main())
diff --git a/loading/sandwich_jobs/wikipedia.json b/loading/sandwich_jobs/wikipedia.json
new file mode 100644
index 0000000..84e91d8
--- /dev/null
+++ b/loading/sandwich_jobs/wikipedia.json
@@ -0,0 +1,5 @@
+{
+  "urls": [
+    "https://en.m.wikipedia.org/wiki/Science"
+  ]
+}

commit 9b293438d94b7cfa0a74235edd76bf3122c4b0a6
Author: mattcary <mattcary@chromium.org>
Date:   Thu Jan 28 04:42:25 2016 -0800

    Practical fixes for ad filtering.
    
    Only import adblocker if we actually define any rules. If we define
    rules, and fail to import adblockerparser, give detailed installation
    instructions.
    
    Fix typo that prevented the filtering from working in the model, and have the filtering actually use the new adblocker matcher rather than the old heuristic.
    
    Review URL: https://codereview.chromium.org/1645953002
    
    Cr-Original-Commit-Position: refs/heads/master@{#372079}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 8497c4725e9a01b5c05e8f10af5e523fa77d1580

diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
index 2e8f0bd..b12b9af 100644
--- a/loading/content_classification_lens.py
+++ b/loading/content_classification_lens.py
@@ -4,8 +4,8 @@
 
 """Labels requests according to the type of content they represent."""
 
-import adblockparser # Available on PyPI, through pip.
 import collections
+import logging
 import os
 
 import loading_trace
@@ -93,10 +93,23 @@ class _RulesMatcher(object):
       no_whitelist: (bool) Whether the whitelisting rules should be ignored.
     """
     self._rules = self._FilterRules(rules, no_whitelist)
-    self._matcher = adblockparser.AdblockRules(self._rules)
+    if self._rules:
+      try:
+        import adblockparser
+        self._matcher = adblockparser.AdblockRules(self._rules)
+      except ImportError:
+        logging.critical('Likely you need to install adblockparser. Try:\n'
+                         ' pip install --user adblockparser\n'
+                         'For 10-100x better performance, also try:\n'
+                         " pip install --user 're2 >= 0.2.21'")
+        raise
+    else:
+      self._matcher = None
 
   def Matches(self, request):
     """Returns whether a request matches one of the rules."""
+    if self._matcher is None:
+      return False
     url = request.url
     return self._matcher.should_block(url, self._GetOptions(request))
 
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
index cca7d52..b4f5ad1 100644
--- a/loading/content_classification_lens_unittest.py
+++ b/loading/content_classification_lens_unittest.py
@@ -25,6 +25,13 @@ class ContentClassificationLensTestCase(unittest.TestCase):
                    'frame_id': '123.13', 'parent_frame_id': _MAIN_FRAME_ID}]
   _RULES = ['bla.com']
 
+  def testNoRules(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], [])
+    self.assertFalse(lens.IsAdRequest(self._REQUEST))
+    self.assertFalse(lens.IsTrackingRequest(self._REQUEST))
+
   def testAdRequest(self):
     trace = test_utils.LoadingTraceFromEvents(
         [self._REQUEST], self._PAGE_EVENTS)
diff --git a/loading/loading_model.py b/loading/loading_model.py
index b6aeca6..e46b705 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -202,7 +202,8 @@ class ResourceGraph(object):
     Returns:
       True if the node is not ad-related.
     """
-    return not self._IsAdUrl(self._node_info[node.Index()].Url())
+    node_info = self._node_info[node.Index()]
+    return not (node_info.IsAd() or node_info.IsTracking())
 
   def MakeGraphviz(self, output, highlight=None):
     """Output a graphviz representation of our DAG.
@@ -504,8 +505,9 @@ class ResourceGraph(object):
       node = dag.Node(next_index)
       node_info = self._NodeInfo(node, request)
       if self._content_lens:
-        node.SetRequestContent(self._content_lens.IsAdRequest(request),
-                               self._content_lens.IsTrackingRequest(request))
+        node_info.SetRequestContent(
+            self._content_lens.IsAdRequest(request),
+            self._content_lens.IsTrackingRequest(request))
       self._nodes.append(node)
       self._node_info.append(node_info)
 
@@ -641,82 +643,6 @@ class ResourceGraph(object):
                node_info.EndTime() - node_info.StartTime(),
                ','.join(styles), color, shape))
 
-  @classmethod
-  def _IsAdUrl(cls, url):
-    """Return true if the url is an ad.
-
-    We group content that doesn't seem to be specific to the website along with
-    ads, eg staticxx.facebook.com, as well as analytics like googletagmanager (?
-    is this correct?).
-
-    Args:
-      url: The full string url to examine.
-
-    Returns:
-      True iff the url appears to be an ad.
-
-    """
-    # See below for how these patterns are defined.
-    AD_PATTERNS = ['2mdn.net',
-                   'admarvel.com',
-                   'adnxs.com',
-                   'adobedtm.com',
-                   'adsrvr.org',
-                   'adsafeprotected.com',
-                   'adsymptotic.com',
-                   'adtech.de',
-                   'adtechus.com',
-                   'advertising.com',
-                   'atwola.com',  # brand protection from cscglobal.com?
-                   'bounceexchange.com',
-                   'betrad.com',
-                   'casalemedia.com',
-                   'cloudfront.net//test.png',
-                   'cloudfront.net//atrk.js',
-                   'contextweb.com',
-                   'crwdcntrl.net',
-                   'doubleclick.net',
-                   'dynamicyield.com',
-                   'krxd.net',
-                   'facebook.com//ping',
-                   'fastclick.net',
-                   'google.com//-ads.js',
-                   'cse.google.com',  # Custom search engine.
-                   'googleadservices.com',
-                   'googlesyndication.com',
-                   'googletagmanager.com',
-                   'lightboxcdn.com',
-                   'mediaplex.com',
-                   'meltdsp.com',
-                   'mobile.nytimes.com//ads-success',
-                   'mookie1.com',
-                   'newrelic.com',
-                   'nr-data.net',   # Apparently part of newrelic.
-                   'optnmnstr.com',
-                   'pubmatic.com',
-                   'quantcast.com',
-                   'quantserve.com',
-                   'rubiconproject.com',
-                   'scorecardresearch.com',
-                   'sekindo.com',
-                   'serving-sys.com',
-                   'sharethrough.com',
-                   'staticxx.facebook.com',  # ?
-                   'syndication.twimg.com',
-                   'tapad.com',
-                   'yieldmo.com',
-                ]
-    parts = urlparse.urlparse(url)
-    for pattern in AD_PATTERNS:
-      if '//' in pattern:
-        domain, path = pattern.split('//')
-      else:
-        domain, path = (pattern, None)
-      if parts.netloc.endswith(domain):
-        if not path or path in parts.path:
-          return True
-    return False
-
   def _ExtractImages(self):
     """Return interesting image resources.
 
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index b866136..88a62d9 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -192,18 +192,6 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5])
 
-  def test_AdUrl(self):
-    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/test.png'))
-    self.assertFalse(loading_model.ResourceGraph._IsAdUrl(
-        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/tst.png'))
-
-    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://ums.adtechus.com/mapuser?providerid=1003;'
-        'userid=RUmecco4z3o===='))
-    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'))
-
 
 if __name__ == '__main__':
   unittest.main()

commit 952de4bd9ca163f3c2865041d5bdaf9b7e159e8e
Author: nednguyen <nednguyen@google.com>
Date:   Tue Jan 26 17:39:28 2016 -0800

    Update references to tools/telemetry in .gn, .gyp & .isolate files
    
    Now that tools/telemetry is moved to catapult/ repo
    (https://github.com/catapult-project/catapult/commit/40afc53353daf28b955397c8e27d6f1de7bb10b6), we will soon remove tools/telemetry/.
    
    This patch update the reference to telemetry/ in .gn, .gyp & .isolate files
    
    BUG=478864
    TEST=patch set 1 delete telemetry/telemetry.gyp, telemetry/BUILD.gn & telemetry/telemetry.isolate, CQ shows that all bots are green.
    
    Review URL: https://codereview.chromium.org/1638483002
    
    Cr-Original-Commit-Position: refs/heads/master@{#371681}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c42bf9152ace37475c1149ec57ffe2838671085b

diff --git a/BUILD.gn b/BUILD.gn
index 012dc35..4e074bd 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -8,13 +8,13 @@
 # GYP: //tools/android/android_tools.gyp:android_tools
 group("android_tools") {
   deps = [
+    "//third_party/catapult/telemetry:bitmaptools($host_toolchain)",
     "//tools/android/adb_reboot",
     "//tools/android/file_poller",
     "//tools/android/forwarder2",
     "//tools/android/md5sum",
     "//tools/android/memtrack_helper:memtrack_helper",
     "//tools/android/purge_ashmem",
-    "//tools/telemetry:bitmaptools($host_toolchain)",
   ]
 }
 
diff --git a/android_tools.gyp b/android_tools.gyp
index b963b3f..065c804 100644
--- a/android_tools.gyp
+++ b/android_tools.gyp
@@ -17,7 +17,7 @@
         'md5sum/md5sum.gyp:md5sum',
         'memtrack_helper/memtrack_helper.gyp:memtrack_helper',
         'purge_ashmem/purge_ashmem.gyp:purge_ashmem',
-        '../../tools/telemetry/telemetry.gyp:*#host',
+        '../../third_party/catapult/telemetry/telemetry.gyp:*#host',
       ],
     },
     {

commit b274fe6201a6a50df3c83378ce2915944995fd4a
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 26 08:24:51 2016 -0800

    tools/android/loading: Add support for multiple redirects.
    
    This properly identify redirect chains. Before this CL, each link in the
    redirect chain would seem to be initiated by the eventual request,
    instead of the previous redirect.
    
    Also logs the number of cases where the redirected request doesn't have
    the same initiator as the initial request.
    
    Review URL: https://codereview.chromium.org/1633813005
    
    Cr-Original-Commit-Position: refs/heads/master@{#371522}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 57c2ecb994e01eeb2fc46ecd9ebee8ae9cca8d13

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 90e77a8..b6aeca6 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -247,7 +247,9 @@ class ResourceGraph(object):
       for s in n.Successors():
         style = 'color = orange'
         annotations = self._EdgeAnnotation(n, s)
-        if 'parser' in annotations:
+        if 'redirect' in annotations:
+          style = 'color = black'
+        elif 'parser' in annotations:
           style = 'color = red'
         elif 'stack' in annotations:
           style = 'color = blue'
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 7033cef..b866136 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -72,7 +72,9 @@ class LoadingModelTestCase(unittest.TestCase):
                        self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
                        self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
                        self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()],
-            'metadata': { 'duplicates_count' : 0 }},
+            'metadata': {
+                request_track.RequestTrack._DUPLICATES_KEY: 0,
+                request_track.RequestTrack._INCONSISTENT_INITIATORS_KEY: 0}},
          'url': 'foo.com',
          'tracing_track': {'events': []},
          'page_track': {'events': []},
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 47ea3af..8128957 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -64,8 +64,7 @@ class RequestDependencyLens(object):
     """
     reason = request.initiator['type']
     assert reason in request_track.Request.INITIATORS
-    # Redirect suffixes are added in RequestTrack.
-    if request.request_id.endswith(request_track.RequestTrack.REDIRECT_SUFFIX):
+    if reason == 'redirect':
       return self._GetInitiatingRequestRedirect(request)
     elif reason == 'parser':
       return self._GetInitiatingRequestParser(request)
@@ -76,14 +75,11 @@ class RequestDependencyLens(object):
       return self._GetInitiatingRequestOther(request)
 
   def _GetInitiatingRequestRedirect(self, request):
-    request_id = request.request_id[:request.request_id.index(
-        request_track.RequestTrack.REDIRECT_SUFFIX)]
-    assert request_id in self._requests_by_id
-    dependent_request = self._requests_by_id[request_id]
-    assert request.timing.request_time < \
-        dependent_request.timing.request_time, '.\n'.join(
-            [str(request), str(dependent_request)])
-    return (request, dependent_request, 'redirect')
+    assert request_track.Request.INITIATING_REQUEST in request.initiator
+    initiating_request_id = request.initiator[
+        request_track.Request.INITIATING_REQUEST]
+    assert initiating_request_id in self._requests_by_id
+    return (self._requests_by_id[initiating_request_id], request, 'redirect')
 
   def _GetInitiatingRequestParser(self, request):
     url = request.initiator['url']
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 4fc5d90..0ab7403 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -13,9 +13,17 @@ import test_utils
 
 class RequestDependencyLensTestCase(unittest.TestCase):
   _REDIRECT_REQUEST = Request.FromJsonDict(
-      {'url': 'http://bla.com', 'request_id': '1234.1.redirect',
+      {'url': 'http://bla.com', 'request_id': '1234.redirect.1',
        'initiator': {'type': 'other'},
        'timestamp': 1, 'timing': TimingFromDict({})})
+  _REDIRECTED_REQUEST = Request.FromJsonDict({
+      'url': 'http://bla.com',
+      'request_id': '1234.1',
+      'frame_id': '123.1',
+      'initiator': {'type': 'redirect',
+                    'initiating_request': '1234.redirect.1'},
+      'timestamp': 2,
+      'timing': TimingFromDict({})})
   _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
                                    'request_id': '1234.1',
                                    'frame_id': '123.1',
@@ -55,7 +63,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testRedirectDependency(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REQUEST])
+        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -86,7 +94,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testSeveralDependencies(self):
     loading_trace = test_utils.LoadingTraceFromEvents(
-        [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
+        [self._REDIRECT_REQUEST, self._REDIRECTED_REQUEST, self._JS_REQUEST,
          self._JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
diff --git a/loading/request_track.py b/loading/request_track.py
index 60a54e5..566fb3c 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -46,7 +46,7 @@ class Request(object):
   third_party/WebKit/Source/devtools/protocol.json.
 
   Fields:
-    request_id: (str) unique request ID. Postfixed with REDIRECT_SUFFIX for
+    request_id: (str) unique request ID. Postfixed with _REDIRECT_SUFFIX for
                 redirects.
     frame_id: (str) unique frame identifier.
     loader_id: (str) unique frame identifier.
@@ -77,7 +77,9 @@ class Request(object):
   RESOURCE_TYPES = ('Document', 'Stylesheet', 'Image', 'Media', 'Font',
                     'Script', 'TextTrack', 'XHR', 'Fetch', 'EventSource',
                     'WebSocket', 'Manifest', 'Other')
-  INITIATORS = ('parser', 'script', 'other')
+  INITIATORS = ('parser', 'script', 'other', 'redirect')
+  INITIATING_REQUEST = 'initiating_request'
+  ORIGINAL_INITIATOR = 'original_initiator'
   def __init__(self):
     self.request_id = None
     self.frame_id = None
@@ -172,7 +174,7 @@ class Request(object):
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
-  REDIRECT_SUFFIX = '.redirect'
+  _REDIRECT_SUFFIX = '.redirect'
   # Request status
   _STATUS_SENT = 0
   _STATUS_RESPONSE = 1
@@ -183,12 +185,14 @@ class RequestTrack(devtools_monitor.Track):
   _EVENTS_KEY = 'events'
   _METADATA_KEY = 'metadata'
   _DUPLICATES_KEY = 'duplicates_count'
+  _INCONSISTENT_INITIATORS_KEY = 'inconsistent_initiators'
   def __init__(self, connection):
     super(RequestTrack, self).__init__(connection)
     self._connection = connection
     self._requests = []
     self._requests_in_flight = {}  # requestId -> (request, status)
     self._completed_requests_by_id = {}
+    self._redirects_count_by_id = collections.defaultdict(int)
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
@@ -196,6 +200,7 @@ class RequestTrack(devtools_monitor.Track):
     # detect this.
     self._request_id_to_response_received = {}
     self.duplicates_count = 0
+    self.inconsistent_initiators_count = 0
 
   def Handle(self, method, msg):
     assert method in RequestTrack._METHOD_TO_HANDLER
@@ -214,7 +219,10 @@ class RequestTrack(devtools_monitor.Track):
       logging.warning('Requests in flight, will be ignored in the dump')
     return {self._EVENTS_KEY: [
         request.ToJsonDict() for request in self._requests],
-            self._METADATA_KEY: {self._DUPLICATES_KEY: self.duplicates_count}}
+            self._METADATA_KEY: {
+                self._DUPLICATES_KEY: self.duplicates_count,
+                self._INCONSISTENT_INITIATORS_KEY:
+                self.inconsistent_initiators_count}}
 
   @classmethod
   def FromJsonDict(cls, json_dict):
@@ -224,14 +232,18 @@ class RequestTrack(devtools_monitor.Track):
     requests = [Request.FromJsonDict(request)
                 for request in json_dict[cls._EVENTS_KEY]]
     result._requests = requests
-    result.duplicates_count = json_dict[cls._METADATA_KEY][cls._DUPLICATES_KEY]
+    metadata = json_dict[cls._METADATA_KEY]
+    result.duplicates_count = metadata.get(cls._DUPLICATES_KEY, 0)
+    result.inconsistent_initiators_count = metadata.get(
+        cls._INCONSISTENT_INITIATORS_KEY, 0)
     return result
 
   def _RequestWillBeSent(self, request_id, params):
     # Several "requestWillBeSent" events can be dispatched in a row in the case
     # of redirects.
+    redirect_initiator = None
     if request_id in self._requests_in_flight:
-      self._HandleRedirect(request_id, params)
+      redirect_initiator = self._HandleRedirect(request_id, params)
     assert (request_id not in self._requests_in_flight
             and request_id not in self._completed_requests_by_id)
     r = Request()
@@ -247,6 +259,16 @@ class RequestTrack(devtools_monitor.Track):
                      ('headers', 'headers'),
                      ('initialPriority', 'initial_priority')))
     r.resource_type = params.get('type', 'Other')
+    if redirect_initiator:
+      original_initiator = r.initiator
+      r.initiator = redirect_initiator
+      r.initiator[Request.ORIGINAL_INITIATOR] = original_initiator
+      initiating_request = self._completed_requests_by_id[
+          redirect_initiator[Request.INITIATING_REQUEST]]
+      initiating_initiator = initiating_request.initiator.get(
+          Request.ORIGINAL_INITIATOR, initiating_request.initiator)
+      if initiating_initiator != original_initiator:
+        self.inconsistent_initiators_count += 1
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_SENT)
 
   def _HandleRedirect(self, request_id, params):
@@ -256,15 +278,23 @@ class RequestTrack(devtools_monitor.Track):
     # one. Finalize the first request.
     assert 'redirectResponse' in params
     redirect_response = params['redirectResponse']
+
     _CopyFromDictToObject(redirect_response, r,
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache')))
     r.timing = TimingFromDict(redirect_response['timing'])
-    r.request_id = request_id + self.REDIRECT_SUFFIX
+
+    redirect_index = self._redirects_count_by_id[request_id]
+    self._redirects_count_by_id[request_id] += 1
+    r.request_id = '%s%s.%d' % (request_id, self._REDIRECT_SUFFIX,
+                                 redirect_index + 1)
+    initiator = {
+        'type': 'redirect', Request.INITIATING_REQUEST: r.request_id}
     self._requests_in_flight[r.request_id] = (r, RequestTrack._STATUS_FINISHED)
     del self._requests_in_flight[request_id]
     self._FinalizeRequest(r.request_id)
+    return initiator
 
   def _RequestServedFromCache(self, request_id, _):
     assert request_id in self._requests_in_flight
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index fde7399..e742f60 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -195,7 +195,47 @@ class RequestTrackTestCase(unittest.TestCase):
     self.assertEquals(1, len(self.request_track.GetEvents()))
     redirect_request = self.request_track.GetEvents()[0]
     self.assertTrue(redirect_request.request_id.endswith(
-        RequestTrack.REDIRECT_SUFFIX))
+        RequestTrack._REDIRECT_SUFFIX + '.1'))
+    request = self.request_track._requests_in_flight.values()[0][0]
+    self.assertEquals('redirect', request.initiator['type'])
+    self.assertEquals(
+        redirect_request.request_id,
+        request.initiator[Request.INITIATING_REQUEST])
+    self.assertEquals(0, self.request_track.inconsistent_initiators_count)
+
+  def testMultipleRedirects(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REDIRECT)
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REDIRECT)
+    self.assertEquals(1, len(self.request_track._requests_in_flight))
+    self.assertEquals(2, len(self.request_track.GetEvents()))
+    first_redirect_request = self.request_track.GetEvents()[0]
+    self.assertTrue(first_redirect_request.request_id.endswith(
+        RequestTrack._REDIRECT_SUFFIX + '.1'))
+    second_redirect_request = self.request_track.GetEvents()[1]
+    self.assertTrue(second_redirect_request.request_id.endswith(
+        RequestTrack._REDIRECT_SUFFIX + '.2'))
+    self.assertEquals('redirect', second_redirect_request.initiator['type'])
+    self.assertEquals(
+        first_redirect_request.request_id,
+        second_redirect_request.initiator[Request.INITIATING_REQUEST])
+    request = self.request_track._requests_in_flight.values()[0][0]
+    self.assertEquals('redirect', request.initiator['type'])
+    self.assertEquals(
+        second_redirect_request.request_id,
+        request.initiator[Request.INITIATING_REQUEST])
+    self.assertEquals(0, self.request_track.inconsistent_initiators_count)
+
+  def testInconsistentInitiators(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    request = copy.deepcopy(RequestTrackTestCase._REDIRECT)
+    request['params']['initiator']['type'] = 'script'
+    self.request_track.Handle('Network.requestWillBeSent', request)
+    self.assertEquals(1, self.request_track.inconsistent_initiators_count)
 
   def testRejectDuplicates(self):
     msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
@@ -281,6 +321,7 @@ class RequestTrackTestCase(unittest.TestCase):
   def testCanDeserialize(self):
     self._ValidSequence(self.request_track)
     self.request_track.duplicates_count = 142
+    self.request_track.inconsistent_initiators_count = 123
     json_dict = self.request_track.ToJsonDict()
     request_track = RequestTrack.FromJsonDict(json_dict)
     self.assertEquals(self.request_track, request_track)

commit 3e8df3edfc407047f059165cd6b7285582d05cf4
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 26 05:59:21 2016 -0800

    tools/android/loading: ContentClassificationLens, ads and tracking requests.
    
    Adds a lens leveraging a rules file to tag whether a given request is
    related to Ads and/or tracking/analytics. This CL also displays this in
    the PNG output of the dependency graph.
    
    Review URL: https://codereview.chromium.org/1626393002
    
    Cr-Original-Commit-Position: refs/heads/master@{#371504}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4208a052792bde3b964aa443201ee6c66906f423

diff --git a/loading/analyze.py b/loading/analyze.py
index 27294d3..5f15fe2 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -24,6 +24,7 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 from pylib import constants
 
+import content_classification_lens
 import device_setup
 import loading_model
 import loading_trace
@@ -147,10 +148,14 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
 
 # TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
 # with here.
-def _ProcessRequests(filename):
+def _ProcessRequests(filename, ad_rules_filename='',
+                     tracking_rules_filename=''):
   with open(filename) as f:
-    return loading_model.ResourceGraph(
-        loading_trace.LoadingTrace.FromJsonDict(json.load(f)))
+    trace = loading_trace.LoadingTrace.FromJsonDict(json.load(f))
+    content_lens = (
+        content_classification_lens.ContentClassificationLens.WithRulesFiles(
+            trace, ad_rules_filename, tracking_rules_filename))
+    return loading_model.ResourceGraph(trace, content_lens)
 
 
 def InvalidCommand(cmd):
@@ -185,8 +190,11 @@ def DoPng(arg_str):
   parser.add_argument('--eog', action='store_true')
   parser.add_argument('--highlight')
   parser.add_argument('--noads', action='store_true')
+  parser.add_argument('--ad_rules', default='')
+  parser.add_argument('--tracking_rules', default='')
   args = parser.parse_args(arg_str)
-  graph = _ProcessRequests(args.request_json)
+  graph = _ProcessRequests(
+      args.request_json, args.ad_rules, args.tracking_rules)
   if args.noads:
     graph.Set(node_filter=graph.FilterAds)
   tmp = tempfile.NamedTemporaryFile()
diff --git a/loading/content_classification_lens.py b/loading/content_classification_lens.py
new file mode 100644
index 0000000..2e8f0bd
--- /dev/null
+++ b/loading/content_classification_lens.py
@@ -0,0 +1,118 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Labels requests according to the type of content they represent."""
+
+import adblockparser # Available on PyPI, through pip.
+import collections
+import os
+
+import loading_trace
+import request_track
+
+
+class ContentClassificationLens(object):
+  """Associates requests and frames with the type of content they represent."""
+  def __init__(self, trace, ad_rules, tracking_rules):
+    """Initializes an instance of ContentClassificationLens.
+
+    Args:
+      trace: (LoadingTrace) loading trace.
+      ad_rules: ([str]) List of Adblock+ compatible rules used to classify ads.
+      tracking_rules: ([str]) List of Adblock+ compatible rules used to
+                      classify tracking and analytics.
+    """
+    self._trace = trace
+    self._requests = trace.request_track.GetEvents()
+    self._main_frame_id = trace.page_track.GetEvents()[0]['frame_id']
+    self._frame_to_requests = collections.defaultdict(list)
+    self._ad_requests = set()
+    self._tracking_requests = set()
+    self._ad_matcher = _RulesMatcher(ad_rules, True)
+    self._tracking_matcher = _RulesMatcher(tracking_rules, True)
+    self._GroupRequestsByFrameId()
+    self._LabelRequests()
+
+  def IsAdRequest(self, request):
+    """Returns True iff the request matches one of the ad_rules."""
+    return request.request_id in self._ad_requests
+
+  def IsTrackingRequest(self, request):
+    """Returns True iff the request matches one of the tracking_rules."""
+    return request.request_id in self._tracking_requests
+
+  def IsAdFrame(self, frame_id, ratio):
+    """A Frame is an Ad frame if more than |ratio| of its requests are
+    ad-related, and is not the main frame."""
+    if frame_id == self._main_frame_id:
+      return False
+    ad_requests_count = sum(r in self._ad_requests
+                            for r in self._frame_to_requests[frame_id])
+    frame_requests_count = len(self._frame_to_requests[frame_id])
+    return (float(ad_requests_count) / frame_requests_count) > ratio
+
+  @classmethod
+  def WithRulesFiles(cls, trace, ad_rules_filename, tracking_rules_filename):
+    """Returns an instance of ContentClassificationLens with the rules read
+    from files.
+    """
+    ad_rules = []
+    tracking_rules = []
+    if os.path.exists(ad_rules_filename):
+      ad_rules = open(ad_rules_filename, 'r').readlines()
+    if os.path.exists(tracking_rules_filename):
+      tracking_rules = open(tracking_rules_filename, 'r').readlines()
+    return ContentClassificationLens(trace, ad_rules, tracking_rules)
+
+  def _GroupRequestsByFrameId(self):
+    for request in self._requests:
+      frame_id = request.frame_id
+      self._frame_to_requests[frame_id].append(request.request_id)
+
+  def _LabelRequests(self):
+    for request in self._requests:
+      request_id = request.request_id
+      if self._ad_matcher.Matches(request):
+        self._ad_requests.add(request_id)
+      if self._tracking_matcher.Matches(request):
+        self._tracking_requests.add(request_id)
+
+
+class _RulesMatcher(object):
+  """Matches requests with rules in Adblock+ format."""
+  _WHITELIST_PREFIX = '@@'
+  _RESOURCE_TYPE_TO_OPTIONS_KEY = {
+      'Script': 'script', 'Stylesheet': 'stylesheet', 'Image': 'image',
+      'XHR': 'xmlhttprequest'}
+  def __init__(self, rules, no_whitelist):
+    """Initializes an instance of _RulesMatcher.
+
+    Args:
+      rules: ([str]) list of rules.
+      no_whitelist: (bool) Whether the whitelisting rules should be ignored.
+    """
+    self._rules = self._FilterRules(rules, no_whitelist)
+    self._matcher = adblockparser.AdblockRules(self._rules)
+
+  def Matches(self, request):
+    """Returns whether a request matches one of the rules."""
+    url = request.url
+    return self._matcher.should_block(url, self._GetOptions(request))
+
+  @classmethod
+  def _GetOptions(cls, request):
+    options = {}
+    resource_type = request.resource_type
+    option = cls._RESOURCE_TYPE_TO_OPTIONS_KEY.get(resource_type)
+    if option:
+      options[option] = True
+    return options
+
+  @classmethod
+  def _FilterRules(cls, rules, no_whitelist):
+    if not no_whitelist:
+      return rules
+    else:
+      return [rule for rule in rules
+              if not rule.startswith(cls._WHITELIST_PREFIX)]
diff --git a/loading/content_classification_lens_unittest.py b/loading/content_classification_lens_unittest.py
new file mode 100644
index 0000000..cca7d52
--- /dev/null
+++ b/loading/content_classification_lens_unittest.py
@@ -0,0 +1,85 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import copy
+import unittest
+
+from content_classification_lens import (ContentClassificationLens,
+                                         _RulesMatcher)
+from request_track import (Request, TimingFromDict)
+import test_utils
+
+
+class ContentClassificationLensTestCase(unittest.TestCase):
+  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'request_id': '1234.1',
+                                   'frame_id': '123.1',
+                                   'initiator': {'type': 'other'},
+                                   'timestamp': 2,
+                                   'timing': TimingFromDict({})})
+  _MAIN_FRAME_ID = '123.1'
+  _PAGE_EVENTS = [{'method': 'Page.frameStartedLoading',
+                   'frame_id': _MAIN_FRAME_ID},
+                  {'method': 'Page.frameAttached',
+                   'frame_id': '123.13', 'parent_frame_id': _MAIN_FRAME_ID}]
+  _RULES = ['bla.com']
+
+  def testAdRequest(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertTrue(lens.IsAdRequest(self._REQUEST))
+    self.assertFalse(lens.IsTrackingRequest(self._REQUEST))
+
+  def testTrackingRequest(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST], self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, [], self._RULES)
+    self.assertFalse(lens.IsAdRequest(self._REQUEST))
+    self.assertTrue(lens.IsTrackingRequest(self._REQUEST))
+
+  def testMainFrameIsNotAdFrame(self):
+    trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST] * 10, self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertFalse(lens.IsAdFrame(self._MAIN_FRAME_ID, .5))
+
+  def testAdFrame(self):
+    request = self._REQUEST
+    request.frame_id = '123.123'
+    trace = test_utils.LoadingTraceFromEvents(
+        [request] * 10 + [self._REQUEST] * 5, self._PAGE_EVENTS)
+    lens = ContentClassificationLens(trace, self._RULES, [])
+    self.assertTrue(lens.IsAdFrame(request.frame_id, .5))
+
+
+class _MatcherTestCase(unittest.TestCase):
+  _RULES_WITH_WHITELIST = ['/thisisanad.', '@@myadvertisingdomain.com/*',
+                           '@@||www.mydomain.com/ads/$elemhide']
+  _SCRIPT_RULE = 'domainwithscripts.com/*$script'
+  _SCRIPT_REQUEST = Request.FromJsonDict(
+      {'url': 'http://domainwithscripts.com/bla.js',
+       'resource_type': 'Script',
+       'request_id': '1234.1',
+       'frame_id': '123.1',
+       'initiator': {'type': 'other'},
+       'timestamp': 2,
+       'timing': TimingFromDict({})})
+
+  def testRemovesWhitelistRules(self):
+    matcher = _RulesMatcher(self._RULES_WITH_WHITELIST, False)
+    self.assertEquals(3, len(matcher._rules))
+    matcher = _RulesMatcher(self._RULES_WITH_WHITELIST, True)
+    self.assertEquals(1, len(matcher._rules))
+
+  def testScriptRule(self):
+    matcher = _RulesMatcher([self._SCRIPT_RULE], False)
+    request = copy.deepcopy(self._SCRIPT_REQUEST)
+    request.resource_type = 'Stylesheet'
+    self.assertFalse(matcher.Matches(request))
+    self.assertTrue(matcher.Matches(self._SCRIPT_REQUEST))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/loading_model.py b/loading/loading_model.py
index 155da1d..90e77a8 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -30,14 +30,17 @@ class ResourceGraph(object):
   Set parameters:
     cache_all: if true, assume zero loading time for all resources.
   """
-  def __init__(self, trace):
+  def __init__(self, trace, content_lens=None):
     """Create from a LoadingTrace (or json of a trace).
 
     Args:
       trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
+      content_lens: (ContentClassificationLens) Lens used to annotate the
+                    nodes, or None.
     """
     if type(trace) == dict:
       trace = loading_trace.LoadingTrace.FromJsonDict(trace)
+    self._content_lens = content_lens
     self._BuildDag(trace)
     self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
@@ -339,6 +342,8 @@ class ResourceGraph(object):
         request: The request associated with this node.
       """
       self._request = request
+      self._is_ad = False
+      self._is_tracking = False
       self._node = node
       self._edge_costs = {}
       self._edge_annotations = {}
@@ -357,6 +362,21 @@ class ResourceGraph(object):
     def Index(self):
       return self._node.Index()
 
+    def SetRequestContent(self, is_ad, is_tracking):
+      """Sets the kind of content the request relates to.
+
+      Args:
+        is_ad: (bool) Whether the request is an Ad.
+        is_tracking: (bool) Whether the request is related to tracking.
+      """
+      (self._is_ad, self._is_tracking) = (is_ad, is_tracking)
+
+    def IsAd(self):
+      return self._is_ad
+
+    def IsTracking(self):
+      return self._is_tracking
+
     def Request(self):
       return self._request
 
@@ -481,6 +501,9 @@ class ResourceGraph(object):
       index_by_request[request] = next_index
       node = dag.Node(next_index)
       node_info = self._NodeInfo(node, request)
+      if self._content_lens:
+        node.SetRequestContent(self._content_lens.IsAdRequest(request),
+                               self._content_lens.IsTrackingRequest(request))
       self._nodes.append(node)
       self._node_info.append(node_info)
 
@@ -606,6 +629,8 @@ class ResourceGraph(object):
         if fragment in node_info.Url():
           styles.append('dotted')
           break
+    if node_info.IsAd() or node_info.IsTracking():
+      styles += ['bold', 'diagonals']
     return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
             'fillcolor = %s; shape = %s];\n'
             % (index, node_info.ShortName(),

commit efc254ebca7e8ed57f8d278b48e3adb2384bf49a
Author: lizeb <lizeb@chromium.org>
Date:   Mon Jan 25 08:09:15 2016 -0800

    tools/android/loading: Move testing boilerplate to a separate file.
    
    Review URL: https://codereview.chromium.org/1626353002
    
    Cr-Original-Commit-Position: refs/heads/master@{#371249}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 09f101135900007dec6edd54bc551d83aac140c8

diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index c65eb95..7033cef 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -8,9 +8,9 @@ import unittest
 
 import dag
 import loading_model
-import loading_trace
 import request_track
 import request_dependencies_lens
+import test_utils
 
 
 class SimpleLens(object):
@@ -29,14 +29,6 @@ class SimpleLens(object):
     return deps
 
 
-class MockRequestTrack(object):
-  def __init__(self, requests):
-    self._requests = requests
-
-  def GetEvents(self):
-    return self._requests
-
-
 class LoadingModelTestCase(unittest.TestCase):
 
   def setUp(self):
@@ -51,6 +43,7 @@ class LoadingModelTestCase(unittest.TestCase):
         'receiveHeadersEnd': end_time - start_time,
         'requestTime': start_time / 1000.0}))
     rq = request_track.Request.FromJsonDict({
+        'timestamp': start_time / 1000.0,
         'request_id': self._next_request_id,
         'url': 'http://' + str(url),
         'initiator': 'http://' + str(source_url),
@@ -63,8 +56,8 @@ class LoadingModelTestCase(unittest.TestCase):
     return rq
 
   def MakeGraph(self, requests):
-    return loading_model.ResourceGraph(loading_trace.LoadingTrace(
-        None, None, None, MockRequestTrack(requests), None))
+    return loading_model.ResourceGraph(
+        test_utils.LoadingTraceFromEvents(requests))
 
   def SortedIndicies(self, graph):
     return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index 9549d04..4fc5d90 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -8,31 +8,7 @@ import devtools_monitor
 from loading_trace import LoadingTrace
 from request_dependencies_lens import RequestDependencyLens
 from request_track import (Request, TimingFromDict)
-from page_track import PageTrack
-
-
-class FakeTrack(devtools_monitor.Track):
-  def __init__(self, events):
-    super(FakeTrack, self).__init__(None)
-    self._events = events
-
-  def GetEvents(self):
-    return self._events
-
-
-class FakeRequestTrack(devtools_monitor.Track):
-  def __init__(self, events):
-    super(FakeRequestTrack, self).__init__(None)
-    self._events = [self._RewriteEvent(e) for e in events]
-
-  def GetEvents(self):
-    return self._events
-
-  def _RewriteEvent(self, event):
-    # This modifies the instance used across tests, so this method
-    # must be idempotent.
-    event.timing = event.timing._replace(request_time=event.timestamp)
-    return event
+import test_utils
 
 
 class RequestDependencyLensTestCase(unittest.TestCase):
@@ -74,14 +50,12 @@ class RequestDependencyLensTestCase(unittest.TestCase):
                      'stackTrace': [{'url': 'unknown'},
                                     {'url': 'http://bla.com/nyancat.js'}]},
        'timestamp': 10, 'timing': TimingFromDict({})})
-  _PAGE_TRACK = FakeTrack(
-      [{'method': 'Page.frameAttached',
-        'frame_id': '123.13', 'parent_frame_id': '123.1'}])
+  _PAGE_EVENTS = [{'method': 'Page.frameAttached',
+                   'frame_id': '123.13', 'parent_frame_id': '123.1'}]
 
   def testRedirectDependency(self):
-    request_track = FakeRequestTrack([self._REDIRECT_REQUEST, self._REQUEST])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._REDIRECT_REQUEST, self._REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -91,9 +65,8 @@ class RequestDependencyLensTestCase(unittest.TestCase):
     self.assertEquals(self._REQUEST.request_id, second.request_id)
 
   def testScriptDependency(self):
-    request_track = FakeRequestTrack([self._JS_REQUEST, self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._JS_REQUEST, self._JS_REQUEST_2])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -102,9 +75,8 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
 
   def testParserDependency(self):
-    request_track = FakeRequestTrack([self._REQUEST, self._JS_REQUEST])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
+    loading_trace = test_utils.LoadingTraceFromEvents(
+        [self._REQUEST, self._JS_REQUEST])
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -113,11 +85,9 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
 
   def testSeveralDependencies(self):
-    request_track = FakeRequestTrack(
+    loading_trace = test_utils.LoadingTraceFromEvents(
         [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
          self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(3, len(deps))
@@ -133,10 +103,8 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testDependencyDifferentFrame(self):
     """Checks that a more recent request from another frame is ignored."""
-    request_track = FakeRequestTrack(
+    loading_trace = test_utils.LoadingTraceFromEvents(
         [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, PageTrack(None),
-                                 request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
@@ -147,11 +115,9 @@ class RequestDependencyLensTestCase(unittest.TestCase):
   def testDependencySameParentFrame(self):
     """Checks that a more recent request from an unrelated frame is ignored
     if there is one from a related frame."""
-    request_track = FakeRequestTrack(
+    loading_trace = test_utils.LoadingTraceFromEvents(
         [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
-         self._JS_REQUEST_2])
-    loading_trace = LoadingTrace(None, None, self._PAGE_TRACK,
-                                 request_track, None)
+         self._JS_REQUEST_2], self._PAGE_EVENTS)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
     deps = request_dependencies_lens.GetRequestDependencies()
     self.assertEquals(1, len(deps))
diff --git a/loading/test_utils.py b/loading/test_utils.py
new file mode 100644
index 0000000..9051062
--- /dev/null
+++ b/loading/test_utils.py
@@ -0,0 +1,40 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Common utilities used in unit tests, within this directory."""
+
+import devtools_monitor
+import loading_trace
+
+
+class FakeTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeTrack, self).__init__(None)
+    self._events = events
+
+  def GetEvents(self):
+    return self._events
+
+
+class FakeRequestTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeRequestTrack, self).__init__(None)
+    self._events = [self._RewriteEvent(e) for e in events]
+
+  def GetEvents(self):
+    return self._events
+
+  def _RewriteEvent(self, event):
+    # This modifies the instance used across tests, so this method
+    # must be idempotent.
+    event.timing = event.timing._replace(request_time=event.timestamp)
+    return event
+
+
+def LoadingTraceFromEvents(requests, page_events=None):
+  """Returns a LoadingTrace instance from a list of requests and page events."""
+  request_track = FakeRequestTrack(requests)
+  page_track = FakeTrack(page_events if page_events else [])
+  return loading_trace.LoadingTrace(
+      None, None, page_track, request_track, None)

commit 21651a72b641e1a9666f0b4099031ddfe2ac1bd2
Author: eakuefner <eakuefner@chromium.org>
Date:   Fri Jan 22 11:48:33 2016 -0800

    [Telemetry] Update all clients to use chromium_config.GetTelemetryDir()
    
    Telemetry is soon to move to Catapult. The effect will be that Telemetry will
    begin to be located at src/third_party/catapult/telemetry, rather than
    tools/telemetry.
    
    This CL updates all src-side Telemetry clients to use
    chromium_config.GetTelemetryDir(), which is a helper that Telemetry team will
    update when the move is complete.
    
    BUG=576374
    
    Review URL: https://codereview.chromium.org/1582793006
    
    Cr-Original-Commit-Position: refs/heads/master@{#371013}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: a5d5d3d6b40905cf3dd3ead516b9c42bf9b94aff

diff --git a/loading/deprecated/log_requests.py b/loading/deprecated/log_requests.py
index ee5f091..12ebb66 100755
--- a/loading/deprecated/log_requests.py
+++ b/loading/deprecated/log_requests.py
@@ -24,7 +24,9 @@ from devil.android import device_utils
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'telemetry'))
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'perf'))
+from chrome_telemetry_build import chromium_config
+sys.path.append(chromium_config.GetTelemetryDir())
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index f1aab2e..5279e4f 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -12,7 +12,9 @@ import os
 import sys
 
 file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
+sys.path.append(os.path.join(file_dir, '..', '..', 'perf'))
+from chrome_telemetry_build import chromium_config
+sys.path.append(chromium_config.GetTelemetryDir())
 
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket

commit 6e767d1fb23e3daeea27a8c363a7dfbc9aa71113
Author: mattcary <mattcary@chromium.org>
Date:   Fri Jan 22 08:38:02 2016 -0800

    Fixes request_dependencies_lens to use timing.request_time in favor of timestamp. The request timestamp turns out to be based on when devtools processed the request, and not when the request actually came. This was causing us to incorrectly infer initiator when looking through stack frame.
    
    Also improves loading_model content-type handling.
    
    Review URL: https://codereview.chromium.org/1619373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370981}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0788ff3eb5eb2073705981eb6b0b0d1a6d59ea9d

diff --git a/loading/loading_model.py b/loading/loading_model.py
index 91aedcd..155da1d 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -576,7 +576,7 @@ class ResourceGraph(object):
     if not content_type:
       type_str = 'other'
     elif '/' in content_type:
-      kind, type_str = content_type.split('/')
+      kind, type_str = content_type.split('/', 1)
       if kind in self._CONTENT_KIND_TO_COLOR:
         return self._CONTENT_KIND_TO_COLOR[kind]
     else:
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 81a029a..47ea3af 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -80,12 +80,14 @@ class RequestDependencyLens(object):
         request_track.RequestTrack.REDIRECT_SUFFIX)]
     assert request_id in self._requests_by_id
     dependent_request = self._requests_by_id[request_id]
-    assert request.timestamp < dependent_request.timestamp
+    assert request.timing.request_time < \
+        dependent_request.timing.request_time, '.\n'.join(
+            [str(request), str(dependent_request)])
     return (request, dependent_request, 'redirect')
 
   def _GetInitiatingRequestParser(self, request):
     url = request.initiator['url']
-    candidates = self._FindMatchingRequests(url, request.timestamp)
+    candidates = self._FindMatchingRequests(url, request.timing.request_time)
     if not candidates:
       return None
     initiating_request = self._FindBestMatchingInitiator(request, candidates)
@@ -96,16 +98,23 @@ class RequestDependencyLens(object):
       logging.warning('Script initiator but no stack trace.')
       return None
     initiating_request = None
-    timestamp = request.timestamp
+    timestamp = request.timing.request_time
     for frame in request.initiator['stackTrace']:
       url = frame['url']
       candidates = self._FindMatchingRequests(url, timestamp)
       if candidates:
         initiating_request = self._FindBestMatchingInitiator(
             request, candidates)
-        break
+        if initiating_request:
+          break
     else:
-      logging.warning('Unmatched request')
+      for frame in request.initiator['stackTrace']:
+        if not frame.get('url', None) and frame.get(
+            'functionName', None) == 'window.onload':
+          logging.warning('Unmatched request for onload handler.')
+          break
+      else:
+        logging.warning('Unmatched request.')
       return None
     return (initiating_request, request, 'script')
 
@@ -126,9 +135,9 @@ class RequestDependencyLens(object):
     """
     candidates = self._requests_by_url.get(url, [])
     candidates = [r for r in candidates if (
-        r.timestamp + max(
+        r.timing.request_time + max(
             0, r.timing.receive_headers_end / 1000) <= before_timestamp)]
-    candidates.sort(key=operator.attrgetter('timestamp'))
+    candidates.sort(key=lambda r: r.timing.request_time)
     return candidates
 
   def _FindBestMatchingInitiator(self, request, matches):
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
index e5aed37..9549d04 100644
--- a/loading/request_dependencies_lens_unittest.py
+++ b/loading/request_dependencies_lens_unittest.py
@@ -20,6 +20,21 @@ class FakeTrack(devtools_monitor.Track):
     return self._events
 
 
+class FakeRequestTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeRequestTrack, self).__init__(None)
+    self._events = [self._RewriteEvent(e) for e in events]
+
+  def GetEvents(self):
+    return self._events
+
+  def _RewriteEvent(self, event):
+    # This modifies the instance used across tests, so this method
+    # must be idempotent.
+    event.timing = event.timing._replace(request_time=event.timestamp)
+    return event
+
+
 class RequestDependencyLensTestCase(unittest.TestCase):
   _REDIRECT_REQUEST = Request.FromJsonDict(
       {'url': 'http://bla.com', 'request_id': '1234.1.redirect',
@@ -64,7 +79,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         'frame_id': '123.13', 'parent_frame_id': '123.1'}])
 
   def testRedirectDependency(self):
-    request_track = FakeTrack([self._REDIRECT_REQUEST, self._REQUEST])
+    request_track = FakeRequestTrack([self._REDIRECT_REQUEST, self._REQUEST])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
@@ -76,7 +91,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
     self.assertEquals(self._REQUEST.request_id, second.request_id)
 
   def testScriptDependency(self):
-    request_track = FakeTrack([self._JS_REQUEST, self._JS_REQUEST_2])
+    request_track = FakeRequestTrack([self._JS_REQUEST, self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
@@ -87,7 +102,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
 
   def testParserDependency(self):
-    request_track = FakeTrack([self._REQUEST, self._JS_REQUEST])
+    request_track = FakeRequestTrack([self._REQUEST, self._JS_REQUEST])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
     request_dependencies_lens = RequestDependencyLens(loading_trace)
@@ -98,7 +113,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
         self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
 
   def testSeveralDependencies(self):
-    request_track = FakeTrack(
+    request_track = FakeRequestTrack(
         [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
          self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
@@ -118,7 +133,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
 
   def testDependencyDifferentFrame(self):
     """Checks that a more recent request from another frame is ignored."""
-    request_track = FakeTrack(
+    request_track = FakeRequestTrack(
         [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, PageTrack(None),
                                  request_track, None)
@@ -132,7 +147,7 @@ class RequestDependencyLensTestCase(unittest.TestCase):
   def testDependencySameParentFrame(self):
     """Checks that a more recent request from an unrelated frame is ignored
     if there is one from a related frame."""
-    request_track = FakeTrack(
+    request_track = FakeRequestTrack(
         [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
          self._JS_REQUEST_2])
     loading_trace = LoadingTrace(None, None, self._PAGE_TRACK,
diff --git a/loading/request_track.py b/loading/request_track.py
index 5e73733..60a54e5 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -166,6 +166,9 @@ class Request(object):
   def __hash__(self):
     return hash(self.request_id)
 
+  def __str__(self):
+    return json.dumps(self.ToJsonDict(), sort_keys=True, indent=2)
+
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""

commit b56bbf98ac6058e2049a6a1efab1db729e528927
Author: blundell <blundell@chromium.org>
Date:   Fri Jan 22 07:08:05 2016 -0800

    [loading] Fix loading_model_unittest
    
    The test request_tracks dict didn't have the expected structure.
    
    Review URL: https://codereview.chromium.org/1614333003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370966}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b9d3191c645c5d349ff5cbe8267f9bd37b912997

diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index bb6495f..c65eb95 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -78,7 +78,8 @@ class LoadingModelTestCase(unittest.TestCase):
             'events': [self.MakeParserRequest(0, 'null', 100, 101).ToJsonDict(),
                        self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
                        self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
-                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()]},
+                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()],
+            'metadata': { 'duplicates_count' : 0 }},
          'url': 'foo.com',
          'tracing_track': {'events': []},
          'page_track': {'events': []},

commit 604f0bf5d379909a6af2ee76540aade24d884ed9
Author: blundell <blundell@chromium.org>
Date:   Fri Jan 22 06:36:55 2016 -0800

    [loading] Towards more useful help messages in analyze.py
    
    This CL changes the ArgParsers in analyze.py to be given 'description' instead
    of 'usage'. This way, they auto-generate usage messages, which are guaranteed
    to be consistent with the current arguments and options that they contain.
    
    Review URL: https://codereview.chromium.org/1620283002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370962}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 2679052c78ff06908573c47b9f7021c9350407e0

diff --git a/loading/analyze.py b/loading/analyze.py
index cb2e837..27294d3 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -159,7 +159,7 @@ def InvalidCommand(cmd):
 
 
 def DoCost(arg_str):
-  parser = argparse.ArgumentParser(usage='cost [--parameter ...] REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Tabulates cost')
   parser.add_argument('request_json')
   parser.add_argument('--parameter', nargs='*', default=[])
   parser.add_argument('--path', action='store_true')
@@ -179,7 +179,7 @@ def DoCost(arg_str):
 
 def DoPng(arg_str):
   parser = argparse.ArgumentParser(
-      usage='png [--eog] [--highlight X[,...] REQUEST_JSON [PNG_OUTPUT]')
+      description='Generates a PNG from a trace')
   parser.add_argument('request_json')
   parser.add_argument('png_output', nargs='?')
   parser.add_argument('--eog', action='store_true')
@@ -208,7 +208,7 @@ def DoPng(arg_str):
 
 
 def DoCompare(arg_str):
-  parser = argparse.ArgumentParser(usage='compare REQUEST_JSON REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Compares two traces')
   parser.add_argument('g1_json')
   parser.add_argument('g2_json')
   args = parser.parse_args(arg_str)
@@ -223,8 +223,7 @@ def DoCompare(arg_str):
 
 
 def DoPrefetchSetup(arg_str):
-  parser = argparse.ArgumentParser(
-      usage='prefetch_setup [--upload] REQUEST_JSON TARGET_HTML')
+  parser = argparse.ArgumentParser(description='Sets up prefetch')
   parser.add_argument('request_json')
   parser.add_argument('target_html')
   parser.add_argument('--upload', action='store_true')
@@ -244,8 +243,7 @@ def DoPrefetchSetup(arg_str):
 
 
 def DoLogRequests(arg_str):
-  parser = argparse.ArgumentParser(
-      usage='log_requests [--prefetch] --site URL --output JSON_OUTPUT')
+  parser = argparse.ArgumentParser(description='Logs requests of a load')
   parser.add_argument('--url', required=True)
   parser.add_argument('--output', required=True)
   parser.add_argument('--prefetch', action='store_true')
@@ -260,13 +258,12 @@ def DoLogRequests(arg_str):
 
 
 def DoFetch(arg_str):
-  parser = argparse.ArgumentParser(usage='fetch --site SITE --dir DIR\n'
-                                   'Fetches SITE into DIR with standard naming '
-                                   'that can be processed by ./cost_to_csv.py. '
-                                   'Both warm and cold fetches are done. '
-                                   'SITE can be a full url but the filename '
-                                   'may be strange so better to just use a '
-                                   'site (ie, domain).')
+  parser = argparse.ArgumentParser(description='Fetches SITE into DIR with '
+                                   'standard naming that can be processed by '
+                                   './cost_to_csv.py.  Both warm and cold '
+                                   'fetches are done.  SITE can be a full url '
+                                   'but the filename may be strange so better '
+                                   'to just use a site (ie, domain).')
   # Arguments are flags as it's easy to get the wrong order of site vs dir.
   parser.add_argument('--site', required=True)
   parser.add_argument('--dir', required=True)
@@ -282,7 +279,7 @@ def DoFetch(arg_str):
 
 
 def DoLongPole(arg_str):
-  parser = argparse.ArgumentParser(usage='longpole [--noads] REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Calculates long pole')
   parser.add_argument('request_json')
   parser.add_argument('--noads', action='store_true')
   args = parser.parse_args(arg_str)
@@ -295,7 +292,7 @@ def DoLongPole(arg_str):
 
 
 def DoNodeCost(arg_str):
-  parser = argparse.ArgumentParser(usage='nodecost [--noads] REQUEST_JSON')
+  parser = argparse.ArgumentParser(description='Calculates node cost')
   parser.add_argument('request_json')
   parser.add_argument('--noads', action='store_true')
   args = parser.parse_args(arg_str)
@@ -318,8 +315,8 @@ COMMAND_MAP = {
 
 def main():
   logging.basicConfig(level=logging.WARNING)
-  parser = argparse.ArgumentParser(usage=' '.join(COMMAND_MAP.keys()))
-  parser.add_argument('command')
+  parser = argparse.ArgumentParser(description='Analyzes loading')
+  parser.add_argument('command', help=' '.join(COMMAND_MAP.keys()))
   parser.add_argument('rest', nargs=argparse.REMAINDER)
   args = parser.parse_args()
   devil_chromium.Initialize()

commit f125fab8afc77554cf12dd555aa82dbafe76457f
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jan 22 02:03:36 2016 -0800

    tools/android/loading: Cope with duplicated messages.
    
    TEST=record a trace on mobile for nytimes.com.
    
    Review URL: https://codereview.chromium.org/1619723003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370934}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 489a998950c75693eaa5bd96eacd7e034fab3fec

diff --git a/loading/request_track.py b/loading/request_track.py
index e83dc30..5e73733 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -176,6 +176,10 @@ class RequestTrack(devtools_monitor.Track):
   _STATUS_DATA = 2
   _STATUS_FINISHED = 3
   _STATUS_FAILED = 4
+  # Serialization KEYS
+  _EVENTS_KEY = 'events'
+  _METADATA_KEY = 'metadata'
+  _DUPLICATES_KEY = 'duplicates_count'
   def __init__(self, connection):
     super(RequestTrack, self).__init__(connection)
     self._connection = connection
@@ -185,6 +189,10 @@ class RequestTrack(devtools_monitor.Track):
     if connection:  # Optional for testing.
       for method in RequestTrack._METHOD_TO_HANDLER:
         self._connection.RegisterListener(method, self)
+    # responseReceived message are sometimes duplicated. Records the message to
+    # detect this.
+    self._request_id_to_response_received = {}
+    self.duplicates_count = 0
 
   def Handle(self, method, msg):
     assert method in RequestTrack._METHOD_TO_HANDLER
@@ -201,15 +209,19 @@ class RequestTrack(devtools_monitor.Track):
   def ToJsonDict(self):
     if self._requests_in_flight:
       logging.warning('Requests in flight, will be ignored in the dump')
-    return {'events': [request.ToJsonDict() for request in self._requests]}
+    return {self._EVENTS_KEY: [
+        request.ToJsonDict() for request in self._requests],
+            self._METADATA_KEY: {self._DUPLICATES_KEY: self.duplicates_count}}
 
   @classmethod
   def FromJsonDict(cls, json_dict):
-    assert 'events' in json_dict
+    assert cls._EVENTS_KEY in json_dict
+    assert cls._METADATA_KEY in json_dict
     result = RequestTrack(None)
     requests = [Request.FromJsonDict(request)
-                for request in json_dict['events']]
+                for request in json_dict[cls._EVENTS_KEY]]
     result._requests = requests
+    result.duplicates_count = json_dict[cls._METADATA_KEY][cls._DUPLICATES_KEY]
     return result
 
   def _RequestWillBeSent(self, request_id, params):
@@ -260,8 +272,16 @@ class RequestTrack(devtools_monitor.Track):
   def _ResponseReceived(self, request_id, params):
     assert request_id in self._requests_in_flight
     (r, status) = self._requests_in_flight[request_id]
+    if status == RequestTrack._STATUS_RESPONSE:
+      # Duplicated messages (apart from the timestamp) are OK.
+      old_params = self._request_id_to_response_received[request_id]
+      params_copy = copy.deepcopy(params)
+      params_copy['timestamp'] = None
+      old_params['timestamp'] = None
+      assert params_copy == old_params
+      self.duplicates_count += 1
+      return
     assert status == RequestTrack._STATUS_SENT
-    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
     assert r.frame_id == params['frameId']
     assert r.timestamp <= params['timestamp']
     if r.resource_type == 'Other':
@@ -286,6 +306,7 @@ class RequestTrack(devtools_monitor.Track):
       timing_dict = {'requestTime': r.timestamp}
     r.timing = TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
+    self._request_id_to_response_received[request_id] = params
 
   def _DataReceived(self, request_id, params):
     (r, status) = self._requests_in_flight[request_id]
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 97ea57f..fde7399 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import copy
 import json
 import unittest
 
@@ -257,6 +258,21 @@ class RequestTrackTestCase(unittest.TestCase):
         RequestTrackTestCase._DATA_RECEIVED_2['params']['encodedDataLength'],
         r.data_chunks[1][1])
 
+  def testDuplicatedResponseReceived(self):
+    msg1 = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    msg2 = copy.deepcopy(RequestTrackTestCase._RESPONSE_RECEIVED)
+    msg2_other_timestamp = copy.deepcopy(msg2)
+    msg2_other_timestamp['params']['timestamp'] += 12
+    msg2_different = copy.deepcopy(msg2)
+    msg2_different['params']['response']['encodedDataLength'] += 1
+    self.request_track.Handle('Network.requestWillBeSent', msg1)
+    self.request_track.Handle('Network.responseReceived', msg2)
+    # Should not raise an AssertionError.
+    self.request_track.Handle('Network.responseReceived', msg2)
+    self.assertEquals(1, self.request_track.duplicates_count)
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle('Network.responseReceived', msg2_different)
+
   def testCanSerialize(self):
     self._ValidSequence(self.request_track)
     json_dict = self.request_track.ToJsonDict()
@@ -264,6 +280,7 @@ class RequestTrackTestCase(unittest.TestCase):
 
   def testCanDeserialize(self):
     self._ValidSequence(self.request_track)
+    self.request_track.duplicates_count = 142
     json_dict = self.request_track.ToJsonDict()
     request_track = RequestTrack.FromJsonDict(json_dict)
     self.assertEquals(self.request_track, request_track)

commit f45d8aa2bec8ed78340507121704e863c496d06d
Author: mattcary <mattcary@chromium.org>
Date:   Thu Jan 21 08:55:20 2016 -0800

    Upgrade analyze.py and related scripts to new world order.
    
    Changes analyze.py to use loading_trace methods. Moved old files to
    deprecated/, I think we're all done with them but we may need them for
    reference.
    
    Updated loading_model graph drawing to account for new content-types;
    what I've done seems sub-optimal & suggestions are welcome.
    
    Fixed at least one minor bug (the request end units problem Benoit
    found).
    
    Review URL: https://codereview.chromium.org/1619713002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370715}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 76303eb216a17c2f93f19593f8ba8b81472c29e4

diff --git a/loading/analyze.py b/loading/analyze.py
index 3bb5890..cb2e837 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -24,31 +24,21 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import devil_chromium
 from pylib import constants
 
-import log_parser
-import log_requests
+import device_setup
 import loading_model
+import loading_trace
+import trace_recorder
 
 
-# TODO(mattcary): logging.info isn't that useful; we need something finer
-# grained. For now we just do logging.warning.
+# TODO(mattcary): logging.info isn't that useful, as the whole (tools) world
+# uses logging info; we need to introduce logging modules to get finer-grained
+# output. For now we just do logging.warning.
 
 
 # TODO(mattcary): probably we want this piped in through a flag.
 CHROME = constants.PACKAGE_INFO['chrome']
 
 
-def _SetupAndGetDevice():
-  """Gets an android device, set up the way we like it.
-
-  Returns:
-    An instance of DeviceUtils for the first device found.
-  """
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  device.EnableRoot()
-  device.KillAll(CHROME.package, quiet=True)
-  return device
-
-
 def _LoadPage(device, url):
   """Load a page on chrome on our device.
 
@@ -99,31 +89,24 @@ def _GetPrefetchHtml(graph, name=None):
 <body>%s</body>
 </html>
   """ % title)
-
   return '\n'.join(output)
 
 
 def _LogRequests(url, clear_cache=True, local=False):
   """Log requests for a web page.
 
-  TODO(mattcary): loading.log_requests probably needs to be refactored as we're
-  using private methods, also there's ugliness like _ResponseDataToJson return a
-  json.dumps that we immediately json.loads.
-
   Args:
     url: url to log as string.
     clear_cache: optional flag to clear the cache.
     local: log from local (desktop) chrome session.
 
   Returns:
-    JSON of logged information (ie, a dict that describes JSON).
+    JSON dict of logged information (ie, a dict that describes JSON).
   """
-  device = _SetupAndGetDevice() if not local else None
-  request_logger = log_requests.AndroidRequestsLogger(device)
-  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
-  response_data = request_logger.LogPageLoad(
-      url, clear_cache, 'chrome')
-  return json.loads(log_requests._ResponseDataToJson(response_data))
+  device = device_setup.GetFirstDevice() if not local else None
+  with device_setup.DeviceConnection(device) as connection:
+    trace = trace_recorder.MonitorUrl(connection, url, clear_cache=clear_cache)
+    return trace.ToJsonDict()
 
 
 def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
@@ -136,13 +119,14 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
   if prefetch:
     assert not local
     logging.warning('Generating prefetch')
-    prefetch_html = _GetPrefetchHtml(_ProcessJson(cold_data), name=url)
+    prefetch_html = _GetPrefetchHtml(
+        loading_model.ResourceGraph(cold_data), name=url)
     tmp = tempfile.NamedTemporaryFile()
     tmp.write(prefetch_html)
     tmp.flush()
     # We hope that the tmpfile name is unique enough for the device.
     target = os.path.join('/sdcard/Download', os.path.basename(tmp.name))
-    device = _SetupAndGetDevice()
+    device = device_setup.GetFirstDevice()
     device.adb.Push(tmp.name, target)
     logging.warning('Pushed prefetch %s to device at %s' % (tmp.name, target))
     _LoadPage(device, 'file://' + target)
@@ -164,14 +148,9 @@ def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
 # TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
 # with here.
 def _ProcessRequests(filename):
-  requests = log_parser.FilterRequests(log_parser.ParseJsonFile(filename))
-  return loading_model.ResourceGraph(requests)
-
-
-def _ProcessJson(json_data):
-  assert json_data
-  return loading_model.ResourceGraph(log_parser.FilterRequests(
-      [log_parser.RequestData.FromDict(r) for r in json_data]))
+  with open(filename) as f:
+    return loading_model.ResourceGraph(
+        loading_trace.LoadingTrace.FromJsonDict(json.load(f)))
 
 
 def InvalidCommand(cmd):
@@ -255,7 +234,7 @@ def DoPrefetchSetup(arg_str):
     html.write(_GetPrefetchHtml(
         graph, name=os.path.basename(args.request_json)))
   if args.upload:
-    device = _SetupAndGetDevice()
+    device = device_setup.GetFirstDevice()
     destination = os.path.join('/sdcard/Download',
                                os.path.basename(args.target_html))
     device.adb.Push(args.target_html, destination)
@@ -302,20 +281,6 @@ def DoFetch(arg_str):
              local=False)
 
 
-def DoTracing(arg_str):
-  parser = argparse.ArgumentParser(
-      usage='tracing URL JSON_OUTPUT')
-  parser.add_argument('url')
-  parser.add_argument('json_output')
-  args = parser.parse_args(arg_str)
-  device = _SetupAndGetDevice()
-  request_logger = log_requests.AndroidRequestsLogger(device)
-  tracing = request_logger.LogTracing(args.url)
-  with open(args.json_output, 'w') as f:
-    _WriteJson(f, tracing)
-  logging.warning('Wrote ' + args.json_output)
-
-
 def DoLongPole(arg_str):
   parser = argparse.ArgumentParser(usage='longpole [--noads] REQUEST_JSON')
   parser.add_argument('request_json')
@@ -346,7 +311,6 @@ COMMAND_MAP = {
     'compare': DoCompare,
     'prefetch_setup': DoPrefetchSetup,
     'log_requests': DoLogRequests,
-    'tracing': DoTracing,
     'longpole': DoLongPole,
     'nodecost': DoNodeCost,
     'fetch': DoFetch,
diff --git a/loading/deprecated/log_parser.py b/loading/deprecated/log_parser.py
new file mode 100644
index 0000000..a58bd55
--- /dev/null
+++ b/loading/deprecated/log_parser.py
@@ -0,0 +1,218 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Parses a JSON request log created by log_requests.py."""
+
+import collections
+import json
+import operator
+import urlparse
+
+Timing = collections.namedtuple(
+    'Timing',
+    ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
+     'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
+     'sslEnd', 'sslStart', 'workerReady', 'workerStart', 'loadingFinished'])
+
+
+class Resource(object):
+  """Describes a resource."""
+
+  def __init__(self, url, content_type):
+    """Creates an instance of Resource.
+
+    Args:
+      url: URL of the resource
+      content_type: Content-Type of the resources.
+    """
+    self.url = url
+    self.content_type = content_type
+
+  def GetShortName(self):
+    """Returns either the hostname of the resource, or the filename,
+    or the end of the path. Tries to include the domain as much as possible.
+    """
+    parsed = urlparse.urlparse(self.url)
+    path = parsed.path
+    if path != '' and path != '/':
+      last_path = parsed.path.split('/')[-1]
+      if len(last_path) < 10:
+        if len(path) < 10:
+          return parsed.hostname + '/' + path
+        else:
+          return parsed.hostname + '/..' + parsed.path[-10:]
+      elif len(last_path) > 10:
+        return parsed.hostname + '/..' + last_path[:5]
+      else:
+        return parsed.hostname + '/..' + last_path
+    else:
+      return parsed.hostname
+
+  def GetContentType(self):
+    mime = self.content_type
+    if 'magic-debug-content' in mime:
+      # A silly hack to make the unittesting easier.
+      return 'magic-debug-content'
+    elif mime == 'text/html':
+      return 'html'
+    elif mime == 'text/css':
+      return 'css'
+    elif mime in ('application/x-javascript', 'text/javascript',
+                  'application/javascript'):
+      return 'script'
+    elif mime == 'application/json':
+      return 'json'
+    elif mime == 'image/gif':
+      return 'gif_image'
+    elif mime.startswith('image/'):
+      return 'image'
+    else:
+      return 'other'
+
+  @classmethod
+  def FromRequest(cls, request):
+    """Creates a Resource from an instance of RequestData."""
+    return Resource(request.url, request.GetContentType())
+
+  def __Fields(self):
+    return (self.url, self.content_type)
+
+  def __eq__(self, o):
+    return  self.__Fields() == o.__Fields()
+
+  def __hash__(self):
+    return hash(self.__Fields())
+
+
+class RequestData(object):
+  """Represents a request, as dumped by log_requests.py."""
+
+  def __init__(self, status, headers, request_headers, timestamp, timing, url,
+               served_from_cache, initiator):
+    self.status = status
+    self.headers = headers
+    self.request_headers = request_headers
+    self.timestamp = timestamp
+    self.timing = Timing(**timing) if timing else None
+    self.url = url
+    self.served_from_cache = served_from_cache
+    self.initiator = initiator
+
+  def IsDataUrl(self):
+    return self.url.startswith('data:')
+
+  def GetContentType(self):
+    content_type = self.headers['Content-Type']
+    if ';' in content_type:
+      return content_type[:content_type.index(';')]
+    else:
+      return content_type
+
+  @classmethod
+  def FromDict(cls, r):
+    """Creates a RequestData object from a dict."""
+    return RequestData(r['status'], r['headers'], r['request_headers'],
+                       r['timestamp'], r['timing'], r['url'],
+                       r['served_from_cache'], r['initiator'])
+
+
+def ParseJsonFile(filename):
+  """Converts a JSON file to a sequence of RequestData."""
+  with open(filename) as f:
+    json_data = json.load(f)
+    return [RequestData.FromDict(r) for r in json_data]
+
+
+def FilterRequests(requests):
+  """Filters a list of requests.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    A list of requests that are not data URL, have a Content-Type, and are
+    not served from the cache.
+  """
+  return [r for r in requests if not r.IsDataUrl()
+          and 'Content-Type' in r.headers and not r.served_from_cache]
+
+
+def ResourceToRequestMap(requests):
+  """Returns a Resource -> Request map.
+
+  A resource can be requested several times in a single page load. Keeps the
+  first request in this case.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    [Resource, ...]
+  """
+  # reversed(requests) because we want the first one to win.
+  return dict([(Resource.FromRequest(r), r) for r in reversed(requests)])
+
+
+def GetResources(requests):
+  """Returns an ordered list of resources from a list of requests.
+
+  The same resource can be requested several time for a single page load. This
+  keeps only the first request.
+
+  Args:
+    requests: [RequestData]
+
+  Returns:
+    [Resource]
+  """
+  resources = []
+  known_resources = set()
+  for r in requests:
+    resource = Resource.FromRequest(r)
+    if r in known_resources:
+      continue
+    known_resources.add(resource)
+    resources.append(resource)
+  return resources
+
+
+def ParseCacheControl(headers):
+  """Parses the "Cache-Control" header and returns a dict representing it.
+
+  Args:
+    headers: (dict) Response headers.
+
+  Returns:
+    {Directive: Value, ...}
+  """
+  # TODO(lizeb): Handle the "Expires" header as well.
+  result = {}
+  cache_control = headers.get('Cache-Control', None)
+  if cache_control is None:
+    return result
+  directives = [s.strip() for s in cache_control.split(',')]
+  for directive in directives:
+    parts = [s.strip() for s in directive.split('=')]
+    if len(parts) == 1:
+      result[parts[0]] = True
+    else:
+      result[parts[0]] = parts[1]
+  return result
+
+
+def MaxAge(request):
+  """Returns the max-age of a resource, or -1."""
+  cache_control = ParseCacheControl(request.headers)
+  if (u'no-store' in cache_control
+      or u'no-cache' in cache_control
+      or len(cache_control) == 0):
+    return -1
+  if 'max-age' in cache_control:
+    return int(cache_control['max-age'])
+  return -1
+
+
+def SortedByCompletion(requests):
+  """Returns the requests, sorted by completion time."""
+  return sorted(requests, key=operator.attrgetter('timestamp'))
diff --git a/loading/deprecated/log_requests.py b/loading/deprecated/log_requests.py
new file mode 100755
index 0000000..ee5f091
--- /dev/null
+++ b/loading/deprecated/log_requests.py
@@ -0,0 +1,237 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loads a URL on an Android device, logging all the requests made to do it
+to a JSON file using DevTools.
+"""
+
+import contextlib
+import httplib
+import json
+import logging
+import optparse
+import os
+import sys
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
+
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'telemetry'))
+from telemetry.internal.backends.chrome_inspector import inspector_websocket
+from telemetry.internal.backends.chrome_inspector import websocket
+
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'chrome_proxy'))
+from common import inspector_network
+
+import device_setup
+
+
+class AndroidRequestsLogger(object):
+  """Logs all the requests made to load a page on a device."""
+
+  def __init__(self, device):
+    """If device is None, we connect to a local chrome session."""
+    self.device = device
+    self._please_stop = False
+    self._main_frame_id = None
+    self._tracing_data = []
+
+  def _PageDataReceived(self, msg):
+    """Called when a Page event is received.
+
+    Records the main frame, and stops the recording once it has finished
+    loading.
+
+    Args:
+      msg: (dict) Message sent by DevTools.
+    """
+    if 'params' not in msg:
+      return
+    params = msg['params']
+    method = msg.get('method', None)
+    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
+      self._main_frame_id = params['frameId']
+    elif (method == 'Page.frameStoppedLoading'
+          and params['frameId'] == self._main_frame_id):
+      self._please_stop = True
+
+  def _TracingDataReceived(self, msg):
+    self._tracing_data.append(msg)
+
+  def _LogPageLoadInternal(self, url, clear_cache):
+    """Returns the collection of requests made to load a given URL.
+
+    Assumes that DevTools is available on http://localhost:DEVTOOLS_PORT.
+
+    Args:
+      url: URL to load.
+      clear_cache: Whether to clear the HTTP cache.
+
+    Returns:
+      [inspector_network.InspectorNetworkResponseData, ...]
+    """
+    self._main_frame_id = None
+    self._please_stop = False
+    r = httplib.HTTPConnection(
+        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      logging.error('Cannot connect to the remote target.')
+      return None
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    inspector = inspector_network.InspectorNetwork(ws)
+    if clear_cache:
+      inspector.ClearCache()
+    ws.SyncRequest({'method': 'Page.enable'})
+    ws.RegisterDomain('Page', self._PageDataReceived)
+    inspector.StartMonitoringNetwork()
+    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
+                              'params': {'url': url}})
+    while not self._please_stop:
+      try:
+        ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException as e:
+        logging.warning('Exception: ' + str(e))
+        break
+    if not self._please_stop:
+      logging.warning('Finished with timeout instead of page load')
+    inspector.StopMonitoringNetwork()
+    return inspector.GetResponseData()
+
+  def _LogTracingInternal(self, url):
+    self._main_frame_id = None
+    self._please_stop = False
+    r = httplib.HTTPConnection('localhost', device_setup.DEVTOOLS_PORT)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      logging.error('Cannot connect to the remote target.')
+      return None
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    ws.RegisterDomain('Tracing', self._TracingDataReceived)
+    logging.warning('Tracing.start: ' +
+                    str(ws.SyncRequest({'method': 'Tracing.start',
+                                        'options': 'zork'})))
+    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
+                              'params': {'url': url}})
+    while not self._please_stop:
+      try:
+        ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException:
+        break
+    if not self._please_stop:
+      logging.warning('Finished with timeout instead of page load')
+    return {'events': self._tracing_data,
+            'end': ws.SyncRequest({'method': 'Tracing.end'})}
+
+
+  def LogPageLoad(self, url, clear_cache, package):
+    """Returns the collection of requests made to load a given URL on a device.
+
+    Args:
+      url: (str) URL to load on the device.
+      clear_cache: (bool) Whether to clear the HTTP cache.
+
+    Returns:
+      See _LogPageLoadInternal().
+    """
+    return device_setup.SetUpAndExecute(
+        self.device, package,
+        lambda: self._LogPageLoadInternal(url, clear_cache))
+
+  def LogTracing(self, url):
+    """Log tracing events from a load of the given URL.
+
+    TODO(mattcary): This doesn't work. It would be best to log tracing
+    simultaneously with network requests, but as that wasn't working the tracing
+    logging was broken out separately. It still doesn't work...
+    """
+    return device_setup.SetUpAndExecute(
+        self.device, 'chrome', lambda: self._LogTracingInternal(url))
+
+
+def _ResponseDataToJson(data):
+  """Converts a list of inspector_network.InspectorNetworkResponseData to JSON.
+
+  Args:
+    data: as returned by _LogPageLoad()
+
+  Returns:
+    A JSON file with the following format:
+    [request1, request2, ...], and a request is:
+    {'status': str, 'headers': dict, 'request_headers': dict,
+     'timestamp': double, 'timing': dict, 'url': str,
+      'served_from_cache': bool, 'initiator': str})
+  """
+  result = []
+  for r in data:
+    result.append({'status': r.status,
+                   'headers': r.headers,
+                   'request_headers': r.request_headers,
+                   'timestamp': r.timestamp,
+                   'timing': r.timing,
+                   'url': r.url,
+                   'served_from_cache': r.served_from_cache,
+                   'initiator': r.initiator})
+  return json.dumps(result)
+
+
+def _CreateOptionParser():
+  """Returns the option parser for this tool."""
+  parser = optparse.OptionParser(description='Starts a browser on an Android '
+                                 'device, gathers the requests made to load a '
+                                 'page and dumps it to a JSON file.')
+  parser.add_option('--url', help='URL to load.',
+                    default='https://www.google.com', metavar='URL')
+  parser.add_option('--output', help='Output file.', default='result.json')
+  parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
+                                              'before loading the URL.'),
+                    default=True, action='store_false', dest='clear_cache')
+  parser.add_option('--package', help='Package info for chrome build. '
+                                      'See build/android/pylib/constants.',
+                    default='chrome')
+  parser.add_option('--local', action='store_true', default=False,
+                    help='Connect to local chrome session rather than android.')
+  return parser
+
+
+def main():
+  logging.basicConfig(level=logging.WARNING)
+  parser = _CreateOptionParser()
+  options, _ = parser.parse_args()
+
+  devil_chromium.Initialize()
+
+  if options.local:
+    device = None
+  else:
+    devices = device_utils.DeviceUtils.HealthyDevices()
+    device = devices[0]
+
+  request_logger = AndroidRequestsLogger(device)
+  response_data = request_logger.LogPageLoad(
+      options.url, options.clear_cache, options.package)
+  json_data = _ResponseDataToJson(response_data)
+  with open(options.output, 'w') as f:
+    f.write(json_data)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/deprecated/process_request_log.py b/loading/deprecated/process_request_log.py
new file mode 100755
index 0000000..bbec0a8
--- /dev/null
+++ b/loading/deprecated/process_request_log.py
@@ -0,0 +1,189 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Creates a Graphviz file visualizing the resource dependencies from a JSON
+file dumped by log_requests.py.
+"""
+
+import collections
+import sys
+import urlparse
+
+import log_parser
+from log_parser import Resource
+
+
+def _BuildResourceDependencyGraph(requests):
+  """Builds the graph of resource dependencies.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    A tuple ([Resource], [(resource1, resource2, reason), ...])
+  """
+  resources = log_parser.GetResources(requests)
+  resources_from_url = {resource.url: resource for resource in resources}
+  requests_by_completion = log_parser.SortedByCompletion(requests)
+  deps = []
+  for r in requests:
+    resource = Resource.FromRequest(r)
+    initiator = r.initiator
+    initiator_type = initiator['type']
+    dep = None
+    if initiator_type == 'parser':
+      url = initiator['url']
+      blocking_resource = resources_from_url.get(url, None)
+      if blocking_resource is None:
+        continue
+      dep = (blocking_resource, resource, 'parser')
+    elif initiator_type == 'script' and 'stackTrace' in initiator:
+      for frame in initiator['stackTrace']:
+        url = frame['url']
+        blocking_resource = resources_from_url.get(url, None)
+        if blocking_resource is None:
+          continue
+        dep = (blocking_resource, resource, 'stack')
+        break
+    else:
+      # When the initiator is a script without a stackTrace, infer that it comes
+      # from the most recent script from the same hostname.
+      # TLD+1 might be better, but finding what is a TLD requires a database.
+      request_hostname = urlparse.urlparse(r.url).hostname
+      sorted_script_requests_from_hostname = [
+          r for r in requests_by_completion
+          if (resource.GetContentType() in ('script', 'html', 'json')
+              and urlparse.urlparse(r.url).hostname == request_hostname)]
+      most_recent = None
+      # Linear search is bad, but this shouldn't matter here.
+      for request in sorted_script_requests_from_hostname:
+        if request.timestamp < r.timing.requestTime:
+          most_recent = request
+        else:
+          break
+      if most_recent is not None:
+        blocking = resources_from_url.get(most_recent.url, None)
+        if blocking is not None:
+          dep = (blocking, resource, 'script_inferred')
+    if dep is not None:
+      deps.append(dep)
+  return (resources, deps)
+
+
+def PrefetchableResources(requests):
+  """Returns a list of resources that are discoverable without JS.
+
+  Args:
+    requests: List of requests.
+
+  Returns:
+    List of discoverable resources, with their initial request.
+  """
+  resource_to_request = log_parser.ResourceToRequestMap(requests)
+  (_, all_deps) = _BuildResourceDependencyGraph(requests)
+  # Only keep "parser" arcs
+  deps = [(first, second) for (first, second, reason) in all_deps
+          if reason == 'parser']
+  deps_per_resource = collections.defaultdict(list)
+  for (first, second) in deps:
+    deps_per_resource[first].append(second)
+  result = []
+  visited = set()
+  to_visit = [deps[0][0]]
+  while len(to_visit) != 0:
+    r = to_visit.pop()
+    visited.add(r)
+    to_visit += deps_per_resource[r]
+    result.append(resource_to_request[r])
+  return result
+
+
+_CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
+                          'json': 'purple', 'gif_image': 'grey',
+                          'image': 'orange', 'other': 'white'}
+
+
+def _ResourceGraphvizNode(resource, request, resource_to_index):
+  """Returns the node description for a given resource.
+
+  Args:
+    resource: Resource.
+    request: RequestData associated with the resource.
+    resource_to_index: {Resource: int}.
+
+  Returns:
+    A string describing the resource in graphviz format.
+    The resource is color-coded according to its content type, and its shape is
+    oval if its max-age is less than 300s (or if it's not cacheable).
+  """
+  color = _CONTENT_TYPE_TO_COLOR[resource.GetContentType()]
+  max_age = log_parser.MaxAge(request)
+  shape = 'polygon' if max_age > 300 else 'oval'
+  return ('%d [label = "%s"; style = "filled"; fillcolor = %s; shape = %s];\n'
+          % (resource_to_index[resource], resource.GetShortName(), color,
+             shape))
+
+
+def _GraphvizFileFromDeps(resources, requests, deps, output_filename):
+  """Writes a graphviz file from a set of resource dependencies.
+
+  Args:
+    resources: [Resource, ...]
+    requests: list of requests
+    deps: [(resource1, resource2, reason), ...]
+    output_filename: file to write the graph to.
+  """
+  with open(output_filename, 'w') as f:
+    f.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+    resource_to_request = log_parser.ResourceToRequestMap(requests)
+    resource_to_index = {r: i for (i, r) in enumerate(resources)}
+    resources_with_edges = set()
+    for (first, second, reason) in deps:
+      resources_with_edges.add(first)
+      resources_with_edges.add(second)
+    if len(resources_with_edges) != len(resources):
+      f.write("""subgraph cluster_orphans {
+  color=black;
+  label="Orphans";
+""")
+      for resource in resources:
+        if resource not in resources_with_edges:
+          request = resource_to_request[resource]
+          f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
+      f.write('}\n')
+
+    f.write("""subgraph cluster_nodes {
+  color=invis;
+""")
+    for resource in resources:
+      request = resource_to_request[resource]
+      print resource.url
+      if resource in resources_with_edges:
+        f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
+    for (first, second, reason) in deps:
+      arrow = ''
+      if reason == 'parser':
+        arrow = '[color = red]'
+      elif reason == 'stack':
+        arrow = '[color = blue]'
+      elif reason == 'script_inferred':
+        arrow = '[color = blue; style=dotted]'
+      f.write('%d -> %d %s;\n' % (
+          resource_to_index[first], resource_to_index[second], arrow))
+    f.write('}\n}\n')
+
+
+def main():
+  filename = sys.argv[1]
+  requests = log_parser.ParseJsonFile(filename)
+  requests = log_parser.FilterRequests(requests)
+  (resources, deps) = _BuildResourceDependencyGraph(requests)
+  _GraphvizFileFromDeps(resources, requests, deps, filename + '.dot')
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/device_setup.py b/loading/device_setup.py
index 4d5d40b..8784024 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -3,6 +3,7 @@
 # found in the LICENSE file.
 
 import contextlib
+import logging
 import os
 import sys
 import time
@@ -11,6 +12,7 @@ _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
 
 sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
 from devil.android.sdk import intent
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
@@ -23,6 +25,25 @@ DEVTOOLS_PORT = 9222
 DEVTOOLS_HOSTNAME = 'localhost'
 DEFAULT_CHROME_PACKAGE = 'chrome'
 
+
+class DeviceSetupException(Exception):
+  def __init__(self, msg):
+    super(DeviceSetupException, self).__init__(msg)
+    logging.error(msg)
+
+
+def GetFirstDevice():
+  """Returns the first connected device.
+
+  Raises:
+    DeviceSetupException if there is no such device.
+  """
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  if not devices:
+    raise DeviceSetupException('No devices found')
+  return devices[0]
+
+
 @contextlib.contextmanager
 def FlagReplacer(device, command_line_path, new_flags):
   """Replaces chrome flags in a context, restores them afterwards.
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 5e8470b..f1aab2e 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -168,6 +168,15 @@ class DevToolsConnection(object):
       raise DevToolsConnectionException(
           'Unexpected response for %s: %s' % (method, result))
 
+  def ClearCache(self):
+    """Clears buffer cache.
+
+    Will assert that the browser supports cache clearing.
+    """
+    res = self.SyncRequest('Network.canClearBrowserCache')
+    assert res['result'], 'Cache clearing is not supported by this browser.'
+    self.SyncRequest('Network.clearBrowserCache')
+
   def SetUpMonitoring(self):
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
diff --git a/loading/loading_model.py b/loading/loading_model.py
index 7ba75fc..91aedcd 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -21,6 +21,7 @@ import urlparse
 import sys
 
 import dag
+import loading_trace
 import request_dependencies_lens
 
 class ResourceGraph(object):
@@ -30,11 +31,13 @@ class ResourceGraph(object):
     cache_all: if true, assume zero loading time for all resources.
   """
   def __init__(self, trace):
-    """Create from a LoadingTrace.
+    """Create from a LoadingTrace (or json of a trace).
 
     Args:
-      trace: (LoadingTrace) Loading trace.
+      trace: (LoadingTrace/JSON) Loading trace or JSON of a trace.
     """
+    if type(trace) == dict:
+      trace = loading_trace.LoadingTrace.FromJsonDict(trace)
     self._BuildDag(trace)
     self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
@@ -290,9 +293,27 @@ class ResourceGraph(object):
   ## Internal items
   ##
 
-  _CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
-                            'json': 'purple', 'gif_image': 'grey',
-                            'image': 'orange', 'other': 'white'}
+  _CONTENT_KIND_TO_COLOR = {
+      'application':     'blue',      # Scripts.
+      'font':            'grey70',
+      'image':           'orange',    # This probably catches gifs?
+      'video':           'hotpink1',
+      }
+
+  _CONTENT_TYPE_TO_COLOR = {
+      'html':            'red',
+      'css':             'green',
+      'script':          'blue',
+      'javascript':      'blue',
+      'json':            'purple',
+      'gif':             'grey',
+      'image':           'orange',
+      'jpeg':            'orange',
+      'png':             'orange',
+      'plain':           'brown3',
+      'octet-stream':    'brown3',
+      'other':           'white',
+      }
 
   # This resource type may induce a timing dependency. See _SplitChildrenByTime
   # for details.
@@ -323,8 +344,9 @@ class ResourceGraph(object):
       self._edge_annotations = {}
       # All fields in timing are millis relative to request_time, which is epoch
       # seconds.
-      self._node_cost = max([t for f, t in request.timing._asdict().iteritems()
-                             if f != 'request_time'])
+      self._node_cost = max(
+          [0] + [t for f, t in request.timing._asdict().iteritems()
+                 if f != 'request_time'])
 
     def __str__(self):
       return self.ShortName()
@@ -363,19 +385,20 @@ class ResourceGraph(object):
       """
       parsed = urlparse.urlparse(self._request.url)
       path = parsed.path
+      hostname = parsed.hostname if parsed.hostname else '?.?.?'
       if path != '' and path != '/':
         last_path = parsed.path.split('/')[-1]
         if len(last_path) < 10:
           if len(path) < 10:
-            return parsed.hostname + '/' + path
+            return hostname + '/' + path
           else:
-            return parsed.hostname + '/..' + parsed.path[-10:]
+            return hostname + '/..' + parsed.path[-10:]
         elif len(last_path) > 10:
-          return parsed.hostname + '/..' + last_path[:5]
+          return hostname + '/..' + last_path[:5]
         else:
-          return parsed.hostname + '/..' + last_path
+          return hostname + '/..' + last_path
       else:
-        return parsed.hostname
+        return hostname
 
     def Url(self):
       return self._request.url
@@ -463,7 +486,7 @@ class ResourceGraph(object):
 
     dependencies = request_dependencies_lens.RequestDependencyLens(
         trace).GetRequestDependencies()
-    for child_rq, parent_rq, reason in dependencies:
+    for parent_rq, child_rq, reason in dependencies:
       parent = self._node_info[index_by_request[parent_rq]]
       child = self._node_info[index_by_request[child_rq]]
       edge_cost = child.StartTime() - parent.EndTime()
@@ -549,6 +572,17 @@ class ResourceGraph(object):
         current.ReparentTo(parent, children_by_end_time[end_mark])
         children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
 
+  def _ContentTypeToColor(self, content_type):
+    if not content_type:
+      type_str = 'other'
+    elif '/' in content_type:
+      kind, type_str = content_type.split('/')
+      if kind in self._CONTENT_KIND_TO_COLOR:
+        return self._CONTENT_KIND_TO_COLOR[kind]
+    else:
+      type_str = content_type
+    return self._CONTENT_TYPE_TO_COLOR[type_str]
+
   def _GraphvizNode(self, index, highlight):
     """Returns a graphviz node description for a given node.
 
@@ -563,7 +597,7 @@ class ResourceGraph(object):
       is oval if its max-age is less than 300s (or if it's not cacheable).
     """
     node_info = self._node_info[index]
-    color = self._CONTENT_TYPE_TO_COLOR[node_info.ContentType()]
+    color = self._ContentTypeToColor(node_info.ContentType())
     max_age = node_info.Request().MaxAge()
     shape = 'polygon' if max_age > 300 else 'oval'
     styles = ['filled']
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index d76f5ac..bb6495f 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -25,7 +25,7 @@ class SimpleLens(object):
       url_to_rq[rq.url] = rq
     for rq in self._trace.request_track.GetEvents():
       if rq.initiator in url_to_rq:
-        deps.append((rq, url_to_rq[rq.initiator], ''))
+        deps.append(( url_to_rq[rq.initiator], rq, ''))
     return deps
 
 
@@ -45,6 +45,11 @@ class LoadingModelTestCase(unittest.TestCase):
 
   def MakeParserRequest(self, url, source_url, start_time, end_time,
                         magic_content_type=False):
+    timing = request_track.TimingAsList(request_track.TimingFromDict({
+        # connectEnd should be ignored.
+        'connectEnd': (end_time - start_time) / 2,
+        'receiveHeadersEnd': end_time - start_time,
+        'requestTime': start_time / 1000.0}))
     rq = request_track.Request.FromJsonDict({
         'request_id': self._next_request_id,
         'url': 'http://' + str(url),
@@ -52,11 +57,7 @@ class LoadingModelTestCase(unittest.TestCase):
         'response_headers': {'Content-Type':
                              'null' if not magic_content_type
                              else 'magic-debug-content' },
-        'timing': request_track.TimingFromDict({
-            # connectEnd should be ignored.
-            'connectEnd': (end_time - start_time) / 2,
-            'receiveHeadersEnd': end_time - start_time,
-            'requestTime': start_time / 1000.0})
+        'timing': timing
         })
     self._next_request_id += 1
     return rq
@@ -71,6 +72,22 @@ class LoadingModelTestCase(unittest.TestCase):
   def SuccessorIndicies(self, node):
     return [c.Index() for c in node.SortedSuccessors()]
 
+  def test_DictConstruction(self):
+    graph = loading_model.ResourceGraph(
+        {'request_track': {
+            'events': [self.MakeParserRequest(0, 'null', 100, 101).ToJsonDict(),
+                       self.MakeParserRequest(1, 0, 102, 103).ToJsonDict(),
+                       self.MakeParserRequest(2, 0, 102, 103).ToJsonDict(),
+                       self.MakeParserRequest(3, 2, 104, 105).ToJsonDict()]},
+         'url': 'foo.com',
+         'tracing_track': {'events': []},
+         'page_track': {'events': []},
+         'metadata': {}})
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
+
   def test_Costing(self):
     requests = [self.MakeParserRequest(0, 'null', 100, 110),
                 self.MakeParserRequest(1, 0, 115, 120),
diff --git a/loading/log_parser.py b/loading/log_parser.py
deleted file mode 100644
index a58bd55..0000000
--- a/loading/log_parser.py
+++ /dev/null
@@ -1,218 +0,0 @@
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Parses a JSON request log created by log_requests.py."""
-
-import collections
-import json
-import operator
-import urlparse
-
-Timing = collections.namedtuple(
-    'Timing',
-    ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
-     'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
-     'sslEnd', 'sslStart', 'workerReady', 'workerStart', 'loadingFinished'])
-
-
-class Resource(object):
-  """Describes a resource."""
-
-  def __init__(self, url, content_type):
-    """Creates an instance of Resource.
-
-    Args:
-      url: URL of the resource
-      content_type: Content-Type of the resources.
-    """
-    self.url = url
-    self.content_type = content_type
-
-  def GetShortName(self):
-    """Returns either the hostname of the resource, or the filename,
-    or the end of the path. Tries to include the domain as much as possible.
-    """
-    parsed = urlparse.urlparse(self.url)
-    path = parsed.path
-    if path != '' and path != '/':
-      last_path = parsed.path.split('/')[-1]
-      if len(last_path) < 10:
-        if len(path) < 10:
-          return parsed.hostname + '/' + path
-        else:
-          return parsed.hostname + '/..' + parsed.path[-10:]
-      elif len(last_path) > 10:
-        return parsed.hostname + '/..' + last_path[:5]
-      else:
-        return parsed.hostname + '/..' + last_path
-    else:
-      return parsed.hostname
-
-  def GetContentType(self):
-    mime = self.content_type
-    if 'magic-debug-content' in mime:
-      # A silly hack to make the unittesting easier.
-      return 'magic-debug-content'
-    elif mime == 'text/html':
-      return 'html'
-    elif mime == 'text/css':
-      return 'css'
-    elif mime in ('application/x-javascript', 'text/javascript',
-                  'application/javascript'):
-      return 'script'
-    elif mime == 'application/json':
-      return 'json'
-    elif mime == 'image/gif':
-      return 'gif_image'
-    elif mime.startswith('image/'):
-      return 'image'
-    else:
-      return 'other'
-
-  @classmethod
-  def FromRequest(cls, request):
-    """Creates a Resource from an instance of RequestData."""
-    return Resource(request.url, request.GetContentType())
-
-  def __Fields(self):
-    return (self.url, self.content_type)
-
-  def __eq__(self, o):
-    return  self.__Fields() == o.__Fields()
-
-  def __hash__(self):
-    return hash(self.__Fields())
-
-
-class RequestData(object):
-  """Represents a request, as dumped by log_requests.py."""
-
-  def __init__(self, status, headers, request_headers, timestamp, timing, url,
-               served_from_cache, initiator):
-    self.status = status
-    self.headers = headers
-    self.request_headers = request_headers
-    self.timestamp = timestamp
-    self.timing = Timing(**timing) if timing else None
-    self.url = url
-    self.served_from_cache = served_from_cache
-    self.initiator = initiator
-
-  def IsDataUrl(self):
-    return self.url.startswith('data:')
-
-  def GetContentType(self):
-    content_type = self.headers['Content-Type']
-    if ';' in content_type:
-      return content_type[:content_type.index(';')]
-    else:
-      return content_type
-
-  @classmethod
-  def FromDict(cls, r):
-    """Creates a RequestData object from a dict."""
-    return RequestData(r['status'], r['headers'], r['request_headers'],
-                       r['timestamp'], r['timing'], r['url'],
-                       r['served_from_cache'], r['initiator'])
-
-
-def ParseJsonFile(filename):
-  """Converts a JSON file to a sequence of RequestData."""
-  with open(filename) as f:
-    json_data = json.load(f)
-    return [RequestData.FromDict(r) for r in json_data]
-
-
-def FilterRequests(requests):
-  """Filters a list of requests.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    A list of requests that are not data URL, have a Content-Type, and are
-    not served from the cache.
-  """
-  return [r for r in requests if not r.IsDataUrl()
-          and 'Content-Type' in r.headers and not r.served_from_cache]
-
-
-def ResourceToRequestMap(requests):
-  """Returns a Resource -> Request map.
-
-  A resource can be requested several times in a single page load. Keeps the
-  first request in this case.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    [Resource, ...]
-  """
-  # reversed(requests) because we want the first one to win.
-  return dict([(Resource.FromRequest(r), r) for r in reversed(requests)])
-
-
-def GetResources(requests):
-  """Returns an ordered list of resources from a list of requests.
-
-  The same resource can be requested several time for a single page load. This
-  keeps only the first request.
-
-  Args:
-    requests: [RequestData]
-
-  Returns:
-    [Resource]
-  """
-  resources = []
-  known_resources = set()
-  for r in requests:
-    resource = Resource.FromRequest(r)
-    if r in known_resources:
-      continue
-    known_resources.add(resource)
-    resources.append(resource)
-  return resources
-
-
-def ParseCacheControl(headers):
-  """Parses the "Cache-Control" header and returns a dict representing it.
-
-  Args:
-    headers: (dict) Response headers.
-
-  Returns:
-    {Directive: Value, ...}
-  """
-  # TODO(lizeb): Handle the "Expires" header as well.
-  result = {}
-  cache_control = headers.get('Cache-Control', None)
-  if cache_control is None:
-    return result
-  directives = [s.strip() for s in cache_control.split(',')]
-  for directive in directives:
-    parts = [s.strip() for s in directive.split('=')]
-    if len(parts) == 1:
-      result[parts[0]] = True
-    else:
-      result[parts[0]] = parts[1]
-  return result
-
-
-def MaxAge(request):
-  """Returns the max-age of a resource, or -1."""
-  cache_control = ParseCacheControl(request.headers)
-  if (u'no-store' in cache_control
-      or u'no-cache' in cache_control
-      or len(cache_control) == 0):
-    return -1
-  if 'max-age' in cache_control:
-    return int(cache_control['max-age'])
-  return -1
-
-
-def SortedByCompletion(requests):
-  """Returns the requests, sorted by completion time."""
-  return sorted(requests, key=operator.attrgetter('timestamp'))
diff --git a/loading/log_requests.py b/loading/log_requests.py
deleted file mode 100755
index ee5f091..0000000
--- a/loading/log_requests.py
+++ /dev/null
@@ -1,237 +0,0 @@
-#! /usr/bin/python
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Loads a URL on an Android device, logging all the requests made to do it
-to a JSON file using DevTools.
-"""
-
-import contextlib
-import httplib
-import json
-import logging
-import optparse
-import os
-import sys
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-import devil_chromium
-
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'telemetry'))
-from telemetry.internal.backends.chrome_inspector import inspector_websocket
-from telemetry.internal.backends.chrome_inspector import websocket
-
-sys.path.append(os.path.join(_SRC_DIR, 'tools', 'chrome_proxy'))
-from common import inspector_network
-
-import device_setup
-
-
-class AndroidRequestsLogger(object):
-  """Logs all the requests made to load a page on a device."""
-
-  def __init__(self, device):
-    """If device is None, we connect to a local chrome session."""
-    self.device = device
-    self._please_stop = False
-    self._main_frame_id = None
-    self._tracing_data = []
-
-  def _PageDataReceived(self, msg):
-    """Called when a Page event is received.
-
-    Records the main frame, and stops the recording once it has finished
-    loading.
-
-    Args:
-      msg: (dict) Message sent by DevTools.
-    """
-    if 'params' not in msg:
-      return
-    params = msg['params']
-    method = msg.get('method', None)
-    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
-      self._main_frame_id = params['frameId']
-    elif (method == 'Page.frameStoppedLoading'
-          and params['frameId'] == self._main_frame_id):
-      self._please_stop = True
-
-  def _TracingDataReceived(self, msg):
-    self._tracing_data.append(msg)
-
-  def _LogPageLoadInternal(self, url, clear_cache):
-    """Returns the collection of requests made to load a given URL.
-
-    Assumes that DevTools is available on http://localhost:DEVTOOLS_PORT.
-
-    Args:
-      url: URL to load.
-      clear_cache: Whether to clear the HTTP cache.
-
-    Returns:
-      [inspector_network.InspectorNetworkResponseData, ...]
-    """
-    self._main_frame_id = None
-    self._please_stop = False
-    r = httplib.HTTPConnection(
-        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      logging.error('Cannot connect to the remote target.')
-      return None
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    inspector = inspector_network.InspectorNetwork(ws)
-    if clear_cache:
-      inspector.ClearCache()
-    ws.SyncRequest({'method': 'Page.enable'})
-    ws.RegisterDomain('Page', self._PageDataReceived)
-    inspector.StartMonitoringNetwork()
-    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
-                              'params': {'url': url}})
-    while not self._please_stop:
-      try:
-        ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException as e:
-        logging.warning('Exception: ' + str(e))
-        break
-    if not self._please_stop:
-      logging.warning('Finished with timeout instead of page load')
-    inspector.StopMonitoringNetwork()
-    return inspector.GetResponseData()
-
-  def _LogTracingInternal(self, url):
-    self._main_frame_id = None
-    self._please_stop = False
-    r = httplib.HTTPConnection('localhost', device_setup.DEVTOOLS_PORT)
-    r.request('GET', '/json')
-    response = r.getresponse()
-    if response.status != 200:
-      logging.error('Cannot connect to the remote target.')
-      return None
-    json_response = json.loads(response.read())
-    r.close()
-    websocket_url = json_response[0]['webSocketDebuggerUrl']
-    ws = inspector_websocket.InspectorWebsocket()
-    ws.Connect(websocket_url)
-    ws.RegisterDomain('Tracing', self._TracingDataReceived)
-    logging.warning('Tracing.start: ' +
-                    str(ws.SyncRequest({'method': 'Tracing.start',
-                                        'options': 'zork'})))
-    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
-                              'params': {'url': url}})
-    while not self._please_stop:
-      try:
-        ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException:
-        break
-    if not self._please_stop:
-      logging.warning('Finished with timeout instead of page load')
-    return {'events': self._tracing_data,
-            'end': ws.SyncRequest({'method': 'Tracing.end'})}
-
-
-  def LogPageLoad(self, url, clear_cache, package):
-    """Returns the collection of requests made to load a given URL on a device.
-
-    Args:
-      url: (str) URL to load on the device.
-      clear_cache: (bool) Whether to clear the HTTP cache.
-
-    Returns:
-      See _LogPageLoadInternal().
-    """
-    return device_setup.SetUpAndExecute(
-        self.device, package,
-        lambda: self._LogPageLoadInternal(url, clear_cache))
-
-  def LogTracing(self, url):
-    """Log tracing events from a load of the given URL.
-
-    TODO(mattcary): This doesn't work. It would be best to log tracing
-    simultaneously with network requests, but as that wasn't working the tracing
-    logging was broken out separately. It still doesn't work...
-    """
-    return device_setup.SetUpAndExecute(
-        self.device, 'chrome', lambda: self._LogTracingInternal(url))
-
-
-def _ResponseDataToJson(data):
-  """Converts a list of inspector_network.InspectorNetworkResponseData to JSON.
-
-  Args:
-    data: as returned by _LogPageLoad()
-
-  Returns:
-    A JSON file with the following format:
-    [request1, request2, ...], and a request is:
-    {'status': str, 'headers': dict, 'request_headers': dict,
-     'timestamp': double, 'timing': dict, 'url': str,
-      'served_from_cache': bool, 'initiator': str})
-  """
-  result = []
-  for r in data:
-    result.append({'status': r.status,
-                   'headers': r.headers,
-                   'request_headers': r.request_headers,
-                   'timestamp': r.timestamp,
-                   'timing': r.timing,
-                   'url': r.url,
-                   'served_from_cache': r.served_from_cache,
-                   'initiator': r.initiator})
-  return json.dumps(result)
-
-
-def _CreateOptionParser():
-  """Returns the option parser for this tool."""
-  parser = optparse.OptionParser(description='Starts a browser on an Android '
-                                 'device, gathers the requests made to load a '
-                                 'page and dumps it to a JSON file.')
-  parser.add_option('--url', help='URL to load.',
-                    default='https://www.google.com', metavar='URL')
-  parser.add_option('--output', help='Output file.', default='result.json')
-  parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
-                                              'before loading the URL.'),
-                    default=True, action='store_false', dest='clear_cache')
-  parser.add_option('--package', help='Package info for chrome build. '
-                                      'See build/android/pylib/constants.',
-                    default='chrome')
-  parser.add_option('--local', action='store_true', default=False,
-                    help='Connect to local chrome session rather than android.')
-  return parser
-
-
-def main():
-  logging.basicConfig(level=logging.WARNING)
-  parser = _CreateOptionParser()
-  options, _ = parser.parse_args()
-
-  devil_chromium.Initialize()
-
-  if options.local:
-    device = None
-  else:
-    devices = device_utils.DeviceUtils.HealthyDevices()
-    device = devices[0]
-
-  request_logger = AndroidRequestsLogger(device)
-  response_data = request_logger.LogPageLoad(
-      options.url, options.clear_cache, options.package)
-  json_data = _ResponseDataToJson(response_data)
-  with open(options.output, 'w') as f:
-    f.write(json_data)
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/process_request_log.py b/loading/process_request_log.py
deleted file mode 100755
index bbec0a8..0000000
--- a/loading/process_request_log.py
+++ /dev/null
@@ -1,189 +0,0 @@
-#! /usr/bin/python
-# Copyright 2015 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Creates a Graphviz file visualizing the resource dependencies from a JSON
-file dumped by log_requests.py.
-"""
-
-import collections
-import sys
-import urlparse
-
-import log_parser
-from log_parser import Resource
-
-
-def _BuildResourceDependencyGraph(requests):
-  """Builds the graph of resource dependencies.
-
-  Args:
-    requests: [RequestData, ...]
-
-  Returns:
-    A tuple ([Resource], [(resource1, resource2, reason), ...])
-  """
-  resources = log_parser.GetResources(requests)
-  resources_from_url = {resource.url: resource for resource in resources}
-  requests_by_completion = log_parser.SortedByCompletion(requests)
-  deps = []
-  for r in requests:
-    resource = Resource.FromRequest(r)
-    initiator = r.initiator
-    initiator_type = initiator['type']
-    dep = None
-    if initiator_type == 'parser':
-      url = initiator['url']
-      blocking_resource = resources_from_url.get(url, None)
-      if blocking_resource is None:
-        continue
-      dep = (blocking_resource, resource, 'parser')
-    elif initiator_type == 'script' and 'stackTrace' in initiator:
-      for frame in initiator['stackTrace']:
-        url = frame['url']
-        blocking_resource = resources_from_url.get(url, None)
-        if blocking_resource is None:
-          continue
-        dep = (blocking_resource, resource, 'stack')
-        break
-    else:
-      # When the initiator is a script without a stackTrace, infer that it comes
-      # from the most recent script from the same hostname.
-      # TLD+1 might be better, but finding what is a TLD requires a database.
-      request_hostname = urlparse.urlparse(r.url).hostname
-      sorted_script_requests_from_hostname = [
-          r for r in requests_by_completion
-          if (resource.GetContentType() in ('script', 'html', 'json')
-              and urlparse.urlparse(r.url).hostname == request_hostname)]
-      most_recent = None
-      # Linear search is bad, but this shouldn't matter here.
-      for request in sorted_script_requests_from_hostname:
-        if request.timestamp < r.timing.requestTime:
-          most_recent = request
-        else:
-          break
-      if most_recent is not None:
-        blocking = resources_from_url.get(most_recent.url, None)
-        if blocking is not None:
-          dep = (blocking, resource, 'script_inferred')
-    if dep is not None:
-      deps.append(dep)
-  return (resources, deps)
-
-
-def PrefetchableResources(requests):
-  """Returns a list of resources that are discoverable without JS.
-
-  Args:
-    requests: List of requests.
-
-  Returns:
-    List of discoverable resources, with their initial request.
-  """
-  resource_to_request = log_parser.ResourceToRequestMap(requests)
-  (_, all_deps) = _BuildResourceDependencyGraph(requests)
-  # Only keep "parser" arcs
-  deps = [(first, second) for (first, second, reason) in all_deps
-          if reason == 'parser']
-  deps_per_resource = collections.defaultdict(list)
-  for (first, second) in deps:
-    deps_per_resource[first].append(second)
-  result = []
-  visited = set()
-  to_visit = [deps[0][0]]
-  while len(to_visit) != 0:
-    r = to_visit.pop()
-    visited.add(r)
-    to_visit += deps_per_resource[r]
-    result.append(resource_to_request[r])
-  return result
-
-
-_CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
-                          'json': 'purple', 'gif_image': 'grey',
-                          'image': 'orange', 'other': 'white'}
-
-
-def _ResourceGraphvizNode(resource, request, resource_to_index):
-  """Returns the node description for a given resource.
-
-  Args:
-    resource: Resource.
-    request: RequestData associated with the resource.
-    resource_to_index: {Resource: int}.
-
-  Returns:
-    A string describing the resource in graphviz format.
-    The resource is color-coded according to its content type, and its shape is
-    oval if its max-age is less than 300s (or if it's not cacheable).
-  """
-  color = _CONTENT_TYPE_TO_COLOR[resource.GetContentType()]
-  max_age = log_parser.MaxAge(request)
-  shape = 'polygon' if max_age > 300 else 'oval'
-  return ('%d [label = "%s"; style = "filled"; fillcolor = %s; shape = %s];\n'
-          % (resource_to_index[resource], resource.GetShortName(), color,
-             shape))
-
-
-def _GraphvizFileFromDeps(resources, requests, deps, output_filename):
-  """Writes a graphviz file from a set of resource dependencies.
-
-  Args:
-    resources: [Resource, ...]
-    requests: list of requests
-    deps: [(resource1, resource2, reason), ...]
-    output_filename: file to write the graph to.
-  """
-  with open(output_filename, 'w') as f:
-    f.write("""digraph dependencies {
-    rankdir = LR;
-    """)
-    resource_to_request = log_parser.ResourceToRequestMap(requests)
-    resource_to_index = {r: i for (i, r) in enumerate(resources)}
-    resources_with_edges = set()
-    for (first, second, reason) in deps:
-      resources_with_edges.add(first)
-      resources_with_edges.add(second)
-    if len(resources_with_edges) != len(resources):
-      f.write("""subgraph cluster_orphans {
-  color=black;
-  label="Orphans";
-""")
-      for resource in resources:
-        if resource not in resources_with_edges:
-          request = resource_to_request[resource]
-          f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
-      f.write('}\n')
-
-    f.write("""subgraph cluster_nodes {
-  color=invis;
-""")
-    for resource in resources:
-      request = resource_to_request[resource]
-      print resource.url
-      if resource in resources_with_edges:
-        f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
-    for (first, second, reason) in deps:
-      arrow = ''
-      if reason == 'parser':
-        arrow = '[color = red]'
-      elif reason == 'stack':
-        arrow = '[color = blue]'
-      elif reason == 'script_inferred':
-        arrow = '[color = blue; style=dotted]'
-      f.write('%d -> %d %s;\n' % (
-          resource_to_index[first], resource_to_index[second], arrow))
-    f.write('}\n}\n')
-
-
-def main():
-  filename = sys.argv[1]
-  requests = log_parser.ParseJsonFile(filename)
-  requests = log_parser.FilterRequests(requests)
-  (resources, deps) = _BuildResourceDependencyGraph(requests)
-  _GraphvizFileFromDeps(resources, requests, deps, filename + '.dot')
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/processing.py b/loading/processing.py
index 1f48939..7c00951 100644
--- a/loading/processing.py
+++ b/loading/processing.py
@@ -2,12 +2,13 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import json
 import os
 import os.path
 import sys
 
-import log_parser
 import loading_model
+import loading_trace
 
 
 def SitesFromDir(directory):
@@ -52,8 +53,9 @@ def WarmGraph(datadir, site):
   Returns:
     A loading model object.
   """
-  return loading_model.ResourceGraph(log_parser.FilterRequests(
-      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json'))))
+  with file(os.path.join(datadir, site + '.json')) as f:
+    return loading_model.ResourceGraph(loading_trace.LoadingTrace.FromJsonDict(
+        json.load(f)))
 
 
 def ColdGraph(datadir, site):
@@ -68,5 +70,6 @@ def ColdGraph(datadir, site):
   Returns:
     A loading model object.
   """
-  return loading_model.ResourceGraph(log_parser.FilterRequests(
-      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json.cold'))))
+  with file(os.path.join(datadir, site + '.json.cold')) as f:
+    return loading_model.ResourceGraph(loading_trace.LoadingTrace.FromJsonDict(
+        json.load(f)))
diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
index 9aacd81..81a029a 100644
--- a/loading/request_dependencies_lens.py
+++ b/loading/request_dependencies_lens.py
@@ -126,7 +126,8 @@ class RequestDependencyLens(object):
     """
     candidates = self._requests_by_url.get(url, [])
     candidates = [r for r in candidates if (
-        r.timestamp + max(0, r.timing.receive_headers_end) <= before_timestamp)]
+        r.timestamp + max(
+            0, r.timing.receive_headers_end / 1000) <= before_timestamp)]
     candidates.sort(key=operator.attrgetter('timestamp'))
     return candidates
 
@@ -157,7 +158,8 @@ class RequestDependencyLens(object):
       parent_frame_id = self._frame_to_parent[request.frame_id]
       same_parent_matches = [
           r for r in matches
-          if self._frame_to_parent[r.frame_id] == parent_frame_id]
+          if r.frame_id in self._frame_to_parent and
+          self._frame_to_parent[r.frame_id] == parent_frame_id]
       if not same_parent_matches:
         logging.warning('All matches are from non-sibling frames.')
         return matches[-1]
diff --git a/loading/request_track.py b/loading/request_track.py
index e7338ab..e83dc30 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -9,7 +9,9 @@ When executed, parses a JSON dump of DevTools messages.
 
 import collections
 import copy
+import json
 import logging
+import re
 
 import devtools_monitor
 
@@ -25,6 +27,17 @@ _TIMING_NAMES_MAPPING = {
 
 Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
 
+def TimingAsList(timing):
+  """Transform Timing to a list, eg as is used in JSON output.
+
+  Args:
+    timing: a Timing.
+
+  Returns:
+    A list identical to what the eventual JSON output will be (eg,
+    Request.ToJsonDict).
+  """
+  return json.loads(json.dumps(timing))
 
 class Request(object):
   """Represents a single request.
@@ -102,8 +115,12 @@ class Request(object):
     result = Request()
     for (k, v) in data_dict.items():
       setattr(result, k, v)
+    if not result.response_headers:
+      result.response_headers = {}
     if result.timing:
       result.timing = Timing(*result.timing)
+    else:
+      result.timing = TimingFromDict({'requestTime': result.timestamp})
     return result
 
   def GetContentType(self):
@@ -137,7 +154,10 @@ class Request(object):
         or len(cache_control) == 0):
       return -1
     if 'max-age' in cache_control:
-      return int(cache_control['max-age'])
+      age_match = re.match(r'\s*(\d+)+', cache_control['max-age'])
+      if not age_match:
+        return -1
+      return int(age_match.group(1))
     return -1
 
   def __eq__(self, o):
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index fdf6cc6..61015d2 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -30,20 +30,34 @@ import request_track
 import tracing
 
 
+def MonitorUrl(connection, url, clear_cache=False):
+  """Monitor a URL via a trace recorder.
+
+  Args:
+    connection: A device_monitor.DevToolsConnection instance.
+    url: url to navigate to as string.
+    clear_cache: boolean indicating if cache should be cleared before loading.
+
+  Returns:
+    loading_trace.LoadingTrace.
+  """
+  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
+  page = page_track.PageTrack(connection)
+  request = request_track.RequestTrack(connection)
+  trace = tracing.TracingTrack(connection)
+  connection.SetUpMonitoring()
+  if clear_cache:
+    connection.ClearCache()
+  connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+  connection.StartMonitoring()
+  metadata = {'date': datetime.datetime.utcnow().isoformat(),
+              'seconds_since_epoch': time.time()}
+  return loading_trace.LoadingTrace(url, metadata, page, request, trace)
+
 def RecordAndDumpTrace(device, url, output_filename):
   with file(output_filename, 'w') as output,\
         device_setup.DeviceConnection(device) as connection:
-    page = page_track.PageTrack(connection)
-    request = request_track.RequestTrack(connection)
-    tracing_track = tracing.TracingTrack(connection)
-    connection.SetUpMonitoring()
-    connection.SendAndIgnoreResponse('Network.clearBrowserCache', {})
-    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-    connection.StartMonitoring()
-    metadata = {'date': datetime.datetime.utcnow().isoformat(),
-                'seconds_since_epoch': time.time()}
-    trace = loading_trace.LoadingTrace(url, metadata, page, request,
-                                       tracing_track)
+    trace = MonitorUrl(connection, url)
     json.dump(trace.ToJsonDict(), output)
 
 

commit 49ef39909ac5a58a6df65fcb1d956ac762c08964
Author: mattcary <mattcary@chromium.org>
Date:   Thu Jan 21 04:18:18 2016 -0800

    Upgrade loading_model to use the new request_track and loading_trace.
    
    Review URL: https://codereview.chromium.org/1610273002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370673}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6a14e6fe7c6bc7a0b355c2a3812179fa5cde4534

diff --git a/loading/loading_model.py b/loading/loading_model.py
index fed48e4..7ba75fc 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -6,11 +6,11 @@
 
 (Redirect the following to the general model module once we have one)
 A model is an object with the following methods.
-  CostMs(): return the cost of the cost in milliseconds.
-  Set(): set model-specifical parameters.
+  CostMs(): return the cost of the model in milliseconds.
+  Set(): set model-specific parameters.
 
 ResourceGraph
-  This creates a DAG of resource dependancies from loading.log_requests to model
+  This creates a DAG of resource dependencies from loading.log_requests to model
   loading time. The model may be parameterized by changing the loading time of
   a particular or all resources.
 """
@@ -21,7 +21,7 @@ import urlparse
 import sys
 
 import dag
-import log_parser
+import request_dependencies_lens
 
 class ResourceGraph(object):
   """A model of loading by a DAG (tree?) of resource dependancies.
@@ -29,14 +29,13 @@ class ResourceGraph(object):
   Set parameters:
     cache_all: if true, assume zero loading time for all resources.
   """
-
-  def __init__(self, requests):
-    """Create from a parsed request set.
+  def __init__(self, trace):
+    """Create from a LoadingTrace.
 
     Args:
-      requests: [RequestData, ...] filtered RequestData from loading.log_parser.
+      trace: (LoadingTrace) Loading trace.
     """
-    self._BuildDag(requests)
+    self._BuildDag(trace)
     self._global_start = min([n.StartTime() for n in self._node_info])
     # Sort before splitting children so that we can correctly dectect if a
     # reparented child is actually a dependency for a child of its new parent.
@@ -182,7 +181,7 @@ class ResourceGraph(object):
       while n.Predecessors():
         n = reduce(lambda costliest, next:
                    next if (self._node_filter(next) and
-                            cost[next.Index()] > cost[costliest.Index()])
+                            costs[next.Index()] > costs[costliest.Index()])
                         else costliest,
                    n.Predecessors())
         path_list.insert(0, self._node_info[n.Index()])
@@ -322,10 +321,10 @@ class ResourceGraph(object):
       self._node = node
       self._edge_costs = {}
       self._edge_annotations = {}
-      # All fields in timing are millis relative to requestTime, which is epoch
+      # All fields in timing are millis relative to request_time, which is epoch
       # seconds.
       self._node_cost = max([t for f, t in request.timing._asdict().iteritems()
-                             if f != 'requestTime'])
+                             if f != 'request_time'])
 
     def __str__(self):
       return self.ShortName()
@@ -346,20 +345,37 @@ class ResourceGraph(object):
       return self._edge_costs[s]
 
     def StartTime(self):
-      return self._request.timing.requestTime * 1000
+      return self._request.timing.request_time * 1000
 
     def EndTime(self):
-      return self._request.timing.requestTime * 1000 + self._node_cost
+      return self._request.timing.request_time * 1000 + self._node_cost
 
     def EdgeAnnotation(self, s):
       assert s.Node() in self.Node().Successors()
       return self._edge_annotations.get(s, [])
 
     def ContentType(self):
-      return log_parser.Resource.FromRequest(self._request).GetContentType()
+      return self._request.GetContentType()
 
     def ShortName(self):
-      return log_parser.Resource.FromRequest(self._request).GetShortName()
+      """Returns either the hostname of the resource, or the filename,
+      or the end of the path. Tries to include the domain as much as possible.
+      """
+      parsed = urlparse.urlparse(self._request.url)
+      path = parsed.path
+      if path != '' and path != '/':
+        last_path = parsed.path.split('/')[-1]
+        if len(last_path) < 10:
+          if len(path) < 10:
+            return parsed.hostname + '/' + path
+          else:
+            return parsed.hostname + '/..' + parsed.path[-10:]
+        elif len(last_path) > 10:
+          return parsed.hostname + '/..' + last_path[:5]
+        else:
+          return parsed.hostname + '/..' + last_path
+      else:
+        return parsed.hostname
 
     def Url(self):
       return self._request.url
@@ -422,7 +438,7 @@ class ResourceGraph(object):
     return self._node_info[parent.Index()].EdgeAnnotation(
         self._node_info[child.Index()])
 
-  def _BuildDag(self, requests):
+  def _BuildDag(self, trace):
     """Build DAG of resources.
 
     Build a DAG from our requests and augment with _NodeInfo (see above) in a
@@ -431,112 +447,36 @@ class ResourceGraph(object):
     Creates self._nodes and self._node_info.
 
     Args:
-      requests: [Request, ...] Requests from loading.log_parser.
+      trace: A LoadingTrace.
     """
     self._nodes = []
     self._node_info = []
-    indicies_by_url = {}
-    requests_by_completion = log_parser.SortedByCompletion(requests)
-    for request in requests:
+    index_by_request = {}
+    for request in trace.request_track.GetEvents():
       next_index = len(self._nodes)
-      indicies_by_url.setdefault(request.url, []).append(next_index)
+      assert request not in index_by_request
+      index_by_request[request] = next_index
       node = dag.Node(next_index)
       node_info = self._NodeInfo(node, request)
       self._nodes.append(node)
       self._node_info.append(node_info)
-    for url, indicies in indicies_by_url.iteritems():
-      if len(indicies) > 1:
-        logging.warning('Multiple loads (%d) for url: %s' %
-                        (len(indicies), url))
-    for i in xrange(len(requests)):
-      request = requests[i]
-      current_node_info = self._node_info[i]
-      resource = log_parser.Resource.FromRequest(current_node_info.Request())
-      initiator = request.initiator
-      initiator_type = initiator['type']
-      predecessor_url = None
-      predecessor_type = None
-      # Classify & infer the predecessor. If a candidate url we identify as the
-      # predecessor is not in index_by_url, then we haven't seen it in our
-      # requests and we will try to find a better predecessor.
-      if initiator_type == 'parser':
-        url = initiator['url']
-        if url in indicies_by_url:
-          predecessor_url = url
-          predecessor_type = 'parser'
-      elif initiator_type == 'script' and 'stackTrace' in initiator:
-        for frame in initiator['stackTrace']:
-          url = frame['url']
-          if url in indicies_by_url:
-            predecessor_url = url
-            predecessor_type = 'stack'
-            break
-      elif initiator_type == 'script':
-        # When the initiator is a script without a stackTrace, infer that it
-        # comes from the most recent script from the same hostname.  TLD+1 might
-        # be better, but finding what is a TLD requires a database.
-        request_hostname = urlparse.urlparse(request.url).hostname
-        sorted_script_requests_from_hostname = [
-            r for r in requests_by_completion
-            if (resource.GetContentType() in ('script', 'html', 'json')
-                and urlparse.urlparse(r.url).hostname == request_hostname)]
-        most_recent = None
-        # Linear search is bad, but this shouldn't matter here.
-        for r in sorted_script_requests_from_hostname:
-          if r.timestamp < request.timing.requestTime:
-            most_recent = r
-          else:
-            break
-        if most_recent is not None:
-          url = most_recent.url
-          if url in indicies_by_url:
-            predecessor_url = url
-            predecessor_type = 'script_inferred'
-      # TODO(mattcary): we skip initiator type other, is that correct?
-      if predecessor_url is not None:
-        predecessor = self._FindBestPredecessor(
-            current_node_info, indicies_by_url[predecessor_url])
-        edge_cost = current_node_info.StartTime() - predecessor.EndTime()
-        if edge_cost < 0:
-          edge_cost = 0
-        if current_node_info.StartTime() < predecessor.StartTime():
-          logging.error('Inverted dependency: %s->%s',
-                        predecessor.ShortName(), current_node_info.ShortName())
-          # Note that current.StartTime() < predecessor.EndTime() appears to
-          # happen a fair amount in practice.
-        predecessor.Node().AddSuccessor(current_node_info.Node())
-        predecessor.SetEdgeCost(current_node_info, edge_cost)
-        predecessor.AddEdgeAnnotation(current_node_info, predecessor_type)
-
-  def _FindBestPredecessor(self, node_info, candidate_indicies):
-    """Find best predecessor for node_info
-
-    If there is only one candidate, we use it regardless of timings. We will
-    later warn about inverted dependencies. If there are more than one, we use
-    the latest whose end time is before node_info's start time. If there is no
-    such candidate, we throw up our hands and return an arbitrary one.
-
-    Args:
-      node_info: _NodeInfo of interest.
-      candidate_indicies: indicies of candidate predecessors.
-
-    Returns:
-      _NodeInfo of best predecessor.
-    """
-    assert candidate_indicies
-    if len(candidate_indicies) == 1:
-      return self._node_info[candidate_indicies[0]]
-    candidate = self._node_info[candidate_indicies[0]]
-    for i in xrange(1, len(candidate_indicies)):
-      next_candidate = self._node_info[candidate_indicies[i]]
-      if (next_candidate.EndTime() < node_info.StartTime() and
-          next_candidate.StartTime() > candidate.StartTime()):
-        candidate = next_candidate
-    if candidate.EndTime() > node_info.StartTime():
-      logging.warning('Multiple candidates but all inverted for ' +
-                      node_info.ShortName())
-    return candidate
 
+    dependencies = request_dependencies_lens.RequestDependencyLens(
+        trace).GetRequestDependencies()
+    for child_rq, parent_rq, reason in dependencies:
+      parent = self._node_info[index_by_request[parent_rq]]
+      child = self._node_info[index_by_request[child_rq]]
+      edge_cost = child.StartTime() - parent.EndTime()
+      if edge_cost < 0:
+        edge_cost = 0
+        if child.StartTime() < parent.StartTime():
+          logging.error('Inverted dependency: %s->%s',
+                        parent.ShortName(), child.ShortName())
+          # Note that child.StartTime() < parent.EndTime() appears to happen a
+          # fair amount in practice.
+      parent.Node().AddSuccessor(child.Node())
+      parent.SetEdgeCost(child, edge_cost)
+      parent.AddEdgeAnnotation(child, reason)
 
   def _SplitChildrenByTime(self, parent):
     """Split children of a node by request times.
@@ -624,7 +564,7 @@ class ResourceGraph(object):
     """
     node_info = self._node_info[index]
     color = self._CONTENT_TYPE_TO_COLOR[node_info.ContentType()]
-    max_age = log_parser.MaxAge(node_info.Request())
+    max_age = node_info.Request().MaxAge()
     shape = 'polygon' if max_age > 300 else 'oval'
     styles = ['filled']
     if highlight:
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index 970f00a..d76f5ac 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -8,22 +8,62 @@ import unittest
 
 import dag
 import loading_model
-import log_parser
+import loading_trace
+import request_track
+import request_dependencies_lens
+
+
+class SimpleLens(object):
+  def __init__(self, trace):
+    self._trace = trace
+
+  def GetRequestDependencies(self):
+    url_to_rq = {}
+    deps = []
+    for rq in self._trace.request_track.GetEvents():
+      assert rq.url not in url_to_rq
+      url_to_rq[rq.url] = rq
+    for rq in self._trace.request_track.GetEvents():
+      if rq.initiator in url_to_rq:
+        deps.append((rq, url_to_rq[rq.initiator], ''))
+    return deps
+
+
+class MockRequestTrack(object):
+  def __init__(self, requests):
+    self._requests = requests
+
+  def GetEvents(self):
+    return self._requests
+
 
 class LoadingModelTestCase(unittest.TestCase):
 
+  def setUp(self):
+    request_dependencies_lens.RequestDependencyLens = SimpleLens
+    self._next_request_id = 0
+
   def MakeParserRequest(self, url, source_url, start_time, end_time,
                         magic_content_type=False):
-    timing_data = {f: -1 for f in log_parser.Timing._fields}
-    # We should ignore connectEnd.
-    timing_data['connectEnd'] = (end_time - start_time) / 2
-    timing_data['receiveHeadersEnd'] = end_time - start_time
-    timing_data['requestTime'] = start_time / 1000.0
-    return log_parser.RequestData(
-        None, {'Content-Type': 'null' if not magic_content_type
-                                      else 'magic-debug-content' },
-        None, start_time, timing_data, 'http://' + str(url), False,
-        {'type': 'parser', 'url': 'http://' + str(source_url)})
+    rq = request_track.Request.FromJsonDict({
+        'request_id': self._next_request_id,
+        'url': 'http://' + str(url),
+        'initiator': 'http://' + str(source_url),
+        'response_headers': {'Content-Type':
+                             'null' if not magic_content_type
+                             else 'magic-debug-content' },
+        'timing': request_track.TimingFromDict({
+            # connectEnd should be ignored.
+            'connectEnd': (end_time - start_time) / 2,
+            'receiveHeadersEnd': end_time - start_time,
+            'requestTime': start_time / 1000.0})
+        })
+    self._next_request_id += 1
+    return rq
+
+  def MakeGraph(self, requests):
+    return loading_model.ResourceGraph(loading_trace.LoadingTrace(
+        None, None, None, MockRequestTrack(requests), None))
 
   def SortedIndicies(self, graph):
     return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
@@ -39,7 +79,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 self.MakeParserRequest(4, 3, 127, 128),
                 self.MakeParserRequest(5, 'null', 100, 105),
                 self.MakeParserRequest(6, 5, 105, 110)]
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
@@ -60,7 +100,7 @@ class LoadingModelTestCase(unittest.TestCase):
                 self.MakeParserRequest(4, 3, 127, 128),
                 self.MakeParserRequest(5, 'null', 100, 105),
                 self.MakeParserRequest(6, 5, 105, 110)]
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     path_list = []
     self.assertEqual(28, graph.Cost(path_list))
     self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
@@ -76,10 +116,11 @@ class LoadingModelTestCase(unittest.TestCase):
                                        magic_content_type=True),
                 self.MakeParserRequest(2, 0, 121, 122,
                                        magic_content_type=True),
-                self.MakeParserRequest(3, 0, 112, 119),
+                self.MakeParserRequest(3, 0, 112, 119,
+                                       magic_content_type=True),
                 self.MakeParserRequest(4, 2, 122, 126),
                 self.MakeParserRequest(5, 2, 122, 126)]
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -88,10 +129,10 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
-    # Change node 1 so it is a parent of 3, which become parent of 2.
+    # Change node 1 so it is a parent of 3, which becomes the parent of 2.
     requests[1] = self.MakeParserRequest(1, 0, 110, 111,
                                          magic_content_type=True)
-    graph = loading_model.ResourceGraph(requests)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -101,14 +142,15 @@ class LoadingModelTestCase(unittest.TestCase):
     self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
 
     # Add an initiator dependence to 1 that will become the parent of 3.
-    requests[1] = self.MakeParserRequest(1, 0, 110, 111)
-    requests.append(self.MakeParserRequest(6, 1, 111, 112))
-    graph = loading_model.ResourceGraph(requests)
-    # Check it doesn't change until we change the content type of 1.
-    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3, 6])
     requests[1] = self.MakeParserRequest(1, 0, 110, 111,
                                          magic_content_type=True)
-    graph = loading_model.ResourceGraph(requests)
+    requests.append(self.MakeParserRequest(6, 1, 111, 112))
+    graph = self.MakeGraph(requests)
+    # Check it doesn't change until we change the content type of 6.
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
+    requests[6] = self.MakeParserRequest(6, 1, 111, 112,
+                                         magic_content_type=True)
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
@@ -127,8 +169,8 @@ class LoadingModelTestCase(unittest.TestCase):
                 self.MakeParserRequest(4, 2, 122, 126),
                 self.MakeParserRequest(5, 2, 122, 126)]
     for r in requests:
-      r.headers['Content-Type'] = 'image/gif'
-    graph = loading_model.ResourceGraph(requests)
+      r.response_headers['Content-Type'] = 'image/gif'
+    graph = self.MakeGraph(requests)
     self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
     self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
diff --git a/loading/request_track.py b/loading/request_track.py
index f3b5c9c..e7338ab 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -117,10 +117,35 @@ class Request(object):
   def IsDataRequest(self):
     return self.protocol == 'data'
 
-  # For testing.
+  def MaxAge(self):
+    """Returns the max-age of a resource, or -1."""
+    # TODO(lizeb): Handle the "Expires" header as well.
+    cache_control = {}
+    if not self.response_headers:
+      return -1
+    cache_control_str = self.response_headers.get('Cache-Control', None)
+    if cache_control_str is not None:
+      directives = [s.strip() for s in cache_control_str.split(',')]
+      for directive in directives:
+        parts = [s.strip() for s in directive.split('=')]
+        if len(parts) == 1:
+          cache_control[parts[0]] = True
+        else:
+          cache_control[parts[0]] = parts[1]
+    if (u'no-store' in cache_control
+        or u'no-cache' in cache_control
+        or len(cache_control) == 0):
+      return -1
+    if 'max-age' in cache_control:
+      return int(cache_control['max-age'])
+    return -1
+
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
 
+  def __hash__(self):
+    return hash(self.request_id)
+
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index ac08b30..97ea57f 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -268,6 +268,21 @@ class RequestTrackTestCase(unittest.TestCase):
     request_track = RequestTrack.FromJsonDict(json_dict)
     self.assertEquals(self.request_track, request_track)
 
+  def testMaxAge(self):
+    rq = Request()
+    self.assertEqual(-1, rq.MaxAge())
+    rq.response_headers = {}
+    self.assertEqual(-1, rq.MaxAge())
+    rq.response_headers[
+        'Cache-Control'] = 'private,s-maxage=0,max-age=0,must-revalidate'
+    self.assertEqual(0, rq.MaxAge())
+    rq.response_headers[
+        'Cache-Control'] = 'private,s-maxage=0,no-store,max-age=100'
+    self.assertEqual(-1, rq.MaxAge())
+    rq.response_headers[
+        'Cache-Control'] = 'private,s-maxage=0'
+    self.assertEqual(-1, rq.MaxAge())
+
   @classmethod
   def _ValidSequence(cls, request_track):
     request_track.Handle(

commit 9069d98e5b3b61f0a42be2aa1125d556e32393d0
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jan 20 08:45:54 2016 -0800

    tools/android/loading: Fix RequestTrack deserialization.
    
    Review URL: https://codereview.chromium.org/1604133004
    
    Cr-Original-Commit-Position: refs/heads/master@{#370414}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d10b4905ef6365b1488b18fecefcc70b67fc190a

diff --git a/loading/request_track.py b/loading/request_track.py
index 77b3d4a..f3b5c9c 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -102,6 +102,8 @@ class Request(object):
     result = Request()
     for (k, v) in data_dict.items():
       setattr(result, k, v)
+    if result.timing:
+      result.timing = Timing(*result.timing)
     return result
 
   def GetContentType(self):

commit 5843c7c34b244d188033371336b6307adb283bb9
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jan 20 07:22:55 2016 -0800

    tools/android/loading: Add RequestDependencyLens.
    
    Represents and infers the dependencies between two requests, as surfaced
    by the various tracks.
    
    Review URL: https://codereview.chromium.org/1603883002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370397}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: e376bba4c0d6ce5765b28d003dd2bbc74c1b7a84

diff --git a/loading/request_dependencies_lens.py b/loading/request_dependencies_lens.py
new file mode 100644
index 0000000..9aacd81
--- /dev/null
+++ b/loading/request_dependencies_lens.py
@@ -0,0 +1,186 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Gathers and infers dependencies between requests.
+
+When executed as a script, loads a trace and outputs the dependencies.
+"""
+
+import collections
+import logging
+import operator
+
+import loading_trace
+import request_track
+
+
+class RequestDependencyLens(object):
+  """Analyses and infers request dependencies."""
+  DEPENDENCIES = ('redirect', 'parser', 'script', 'inferred', 'other')
+  def __init__(self, trace):
+    """Initializes an instance of RequestDependencyLens.
+
+    Args:
+      trace: (LoadingTrace) Loading trace.
+    """
+    self.loading_trace = trace
+    self._requests = self.loading_trace.request_track.GetEvents()
+    self._requests_by_id = {r.request_id: r for r in self._requests}
+    self._requests_by_url = collections.defaultdict(list)
+    for request in self._requests:
+      self._requests_by_url[request.url].append(request)
+    self._frame_to_parent = {}
+    for event in self.loading_trace.page_track.GetEvents():
+      if event['method'] == 'Page.frameAttached':
+        self._frame_to_parent[event['frame_id']] = event['parent_frame_id']
+
+  def GetRequestDependencies(self):
+    """Returns a list of request dependencies.
+
+    Returns:
+      [(first, second, reason), ...] where first and second are instances of
+      request_track.Request, and reason is in DEPENDENCIES. The second request
+      depends on the first one, with the listed reason.
+    """
+    deps = []
+    for request in self._requests:
+      dependency = self._GetDependency(request)
+      if dependency:
+        deps.append(dependency)
+    return deps
+
+  def _GetDependency(self, request):
+    """Returns (first, second, reason), or None.
+
+    |second| depends on |first|.
+
+    Args:
+      request: (Request) the request we wish to get the initiator of.
+
+    Returns:
+      None if no dependency is found from this request, or
+      (initiator (Request), blocked_request (Request), reason (str)).
+    """
+    reason = request.initiator['type']
+    assert reason in request_track.Request.INITIATORS
+    # Redirect suffixes are added in RequestTrack.
+    if request.request_id.endswith(request_track.RequestTrack.REDIRECT_SUFFIX):
+      return self._GetInitiatingRequestRedirect(request)
+    elif reason == 'parser':
+      return self._GetInitiatingRequestParser(request)
+    elif reason == 'script':
+      return self._GetInitiatingRequestScript(request)
+    else:
+      assert reason == 'other'
+      return self._GetInitiatingRequestOther(request)
+
+  def _GetInitiatingRequestRedirect(self, request):
+    request_id = request.request_id[:request.request_id.index(
+        request_track.RequestTrack.REDIRECT_SUFFIX)]
+    assert request_id in self._requests_by_id
+    dependent_request = self._requests_by_id[request_id]
+    assert request.timestamp < dependent_request.timestamp
+    return (request, dependent_request, 'redirect')
+
+  def _GetInitiatingRequestParser(self, request):
+    url = request.initiator['url']
+    candidates = self._FindMatchingRequests(url, request.timestamp)
+    if not candidates:
+      return None
+    initiating_request = self._FindBestMatchingInitiator(request, candidates)
+    return (initiating_request, request, 'parser')
+
+  def _GetInitiatingRequestScript(self, request):
+    if not 'stackTrace' in request.initiator:
+      logging.warning('Script initiator but no stack trace.')
+      return None
+    initiating_request = None
+    timestamp = request.timestamp
+    for frame in request.initiator['stackTrace']:
+      url = frame['url']
+      candidates = self._FindMatchingRequests(url, timestamp)
+      if candidates:
+        initiating_request = self._FindBestMatchingInitiator(
+            request, candidates)
+        break
+    else:
+      logging.warning('Unmatched request')
+      return None
+    return (initiating_request, request, 'script')
+
+  def _GetInitiatingRequestOther(self, _):
+    # TODO(lizeb): Infer "other" initiator types.
+    return None
+
+  def _FindMatchingRequests(self, url, before_timestamp):
+    """Returns a list of requests matching a URL, before a timestamp.
+
+    Args:
+      url: (str) URL to match in requests.
+      before_timestamp: (int) Only keep requests submitted before a given
+                        timestamp.
+
+    Returns:
+      A list of candidates, ordered by timestamp.
+    """
+    candidates = self._requests_by_url.get(url, [])
+    candidates = [r for r in candidates if (
+        r.timestamp + max(0, r.timing.receive_headers_end) <= before_timestamp)]
+    candidates.sort(key=operator.attrgetter('timestamp'))
+    return candidates
+
+  def _FindBestMatchingInitiator(self, request, matches):
+    """Returns the best matching request within a list of matches.
+
+    Iteratively removes candidates until one is left:
+    - With the same parent frame.
+    - From the same frame.
+
+    If this is not successful, takes the most recent request.
+
+    Args:
+      request: (Request) Request.
+      matches: [Request] As returned by _FindMatchingRequests(), that is
+               sorted by timestamp.
+
+    Returns:
+      The best matching initiating request, or None.
+    """
+    if not matches:
+      return None
+    if len(matches) == 1:
+      return matches[0]
+    # Several matches, try to reduce this number to 1. Otherwise, return the
+    # most recent one.
+    if request.frame_id in self._frame_to_parent: # Main frame has no parent.
+      parent_frame_id = self._frame_to_parent[request.frame_id]
+      same_parent_matches = [
+          r for r in matches
+          if self._frame_to_parent[r.frame_id] == parent_frame_id]
+      if not same_parent_matches:
+        logging.warning('All matches are from non-sibling frames.')
+        return matches[-1]
+      if len(same_parent_matches) == 1:
+        return same_parent_matches[0]
+    same_frame_matches = [r for r in matches if r.frame_id == request.frame_id]
+    if not same_frame_matches:
+      logging.warning('All matches are from non-sibling frames.')
+      return matches[-1]
+    if len(same_frame_matches) == 1:
+      return same_frame_matches[0]
+    else:
+      logging.warning('Several matches')
+      return same_frame_matches[-1]
+
+
+if __name__ == '__main__':
+  import json
+  import sys
+  trace_filename = sys.argv[1]
+  json_dict = json.load(open(trace_filename, 'r'))
+  lens = RequestDependencyLens(
+      loading_trace.LoadingTrace.FromJsonDict(json_dict))
+  depedencies = lens.GetRequestDependencies()
+  for (first, second, dep_reason) in depedencies:
+    print '%s -> %s\t(%s)' % (first.request_id, second.request_id, dep_reason)
diff --git a/loading/request_dependencies_lens_unittest.py b/loading/request_dependencies_lens_unittest.py
new file mode 100644
index 0000000..e5aed37
--- /dev/null
+++ b/loading/request_dependencies_lens_unittest.py
@@ -0,0 +1,157 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import devtools_monitor
+from loading_trace import LoadingTrace
+from request_dependencies_lens import RequestDependencyLens
+from request_track import (Request, TimingFromDict)
+from page_track import PageTrack
+
+
+class FakeTrack(devtools_monitor.Track):
+  def __init__(self, events):
+    super(FakeTrack, self).__init__(None)
+    self._events = events
+
+  def GetEvents(self):
+    return self._events
+
+
+class RequestDependencyLensTestCase(unittest.TestCase):
+  _REDIRECT_REQUEST = Request.FromJsonDict(
+      {'url': 'http://bla.com', 'request_id': '1234.1.redirect',
+       'initiator': {'type': 'other'},
+       'timestamp': 1, 'timing': TimingFromDict({})})
+  _REQUEST = Request.FromJsonDict({'url': 'http://bla.com',
+                                   'request_id': '1234.1',
+                                   'frame_id': '123.1',
+                                   'initiator': {'type': 'other'},
+                                   'timestamp': 2,
+                                   'timing': TimingFromDict({})})
+  _JS_REQUEST = Request.FromJsonDict({'url': 'http://bla.com/nyancat.js',
+                                      'request_id': '1234.12',
+                                      'frame_id': '123.1',
+                                      'initiator': {'type': 'parser',
+                                                    'url': 'http://bla.com'},
+                                      'timestamp': 3,
+                                      'timing': TimingFromDict({})})
+  _JS_REQUEST_OTHER_FRAME = Request.FromJsonDict(
+      {'url': 'http://bla.com/nyancat.js',
+       'request_id': '1234.42',
+       'frame_id': '123.13',
+       'initiator': {'type': 'parser',
+                     'url': 'http://bla.com'},
+       'timestamp': 4, 'timing': TimingFromDict({})})
+  _JS_REQUEST_UNRELATED_FRAME = Request.FromJsonDict(
+      {'url': 'http://bla.com/nyancat.js',
+       'request_id': '1234.42',
+       'frame_id': '123.99',
+       'initiator': {'type': 'parser',
+                     'url': 'http://bla.com'},
+       'timestamp': 5, 'timing': TimingFromDict({})})
+  _JS_REQUEST_2 = Request.FromJsonDict(
+      {'url': 'http://bla.com/cat.js', 'request_id': '1234.13',
+       'frame_id': '123.1',
+       'initiator': {'type': 'script',
+                     'stackTrace': [{'url': 'unknown'},
+                                    {'url': 'http://bla.com/nyancat.js'}]},
+       'timestamp': 10, 'timing': TimingFromDict({})})
+  _PAGE_TRACK = FakeTrack(
+      [{'method': 'Page.frameAttached',
+        'frame_id': '123.13', 'parent_frame_id': '123.1'}])
+
+  def testRedirectDependency(self):
+    request_track = FakeTrack([self._REDIRECT_REQUEST, self._REQUEST])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    (first, second, reason) = deps[0]
+    self.assertEquals('redirect', reason)
+    self.assertEquals(self._REDIRECT_REQUEST.request_id, first.request_id)
+    self.assertEquals(self._REQUEST.request_id, second.request_id)
+
+  def testScriptDependency(self):
+    request_track = FakeTrack([self._JS_REQUEST, self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+
+  def testParserDependency(self):
+    request_track = FakeTrack([self._REQUEST, self._JS_REQUEST])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+
+  def testSeveralDependencies(self):
+    request_track = FakeTrack(
+        [self._REDIRECT_REQUEST, self._REQUEST, self._JS_REQUEST,
+         self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(3, len(deps))
+    self._AssertDependencyIs(
+        deps[0], self._REDIRECT_REQUEST.request_id, self._REQUEST.request_id,
+        'redirect')
+    self._AssertDependencyIs(
+        deps[1],
+        self._REQUEST.request_id, self._JS_REQUEST.request_id, 'parser')
+    self._AssertDependencyIs(
+        deps[2],
+        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+
+  def testDependencyDifferentFrame(self):
+    """Checks that a more recent request from another frame is ignored."""
+    request_track = FakeTrack(
+        [self._JS_REQUEST, self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, PageTrack(None),
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._JS_REQUEST.request_id, self._JS_REQUEST_2.request_id, 'script')
+
+  def testDependencySameParentFrame(self):
+    """Checks that a more recent request from an unrelated frame is ignored
+    if there is one from a related frame."""
+    request_track = FakeTrack(
+        [self._JS_REQUEST_OTHER_FRAME, self._JS_REQUEST_UNRELATED_FRAME,
+         self._JS_REQUEST_2])
+    loading_trace = LoadingTrace(None, None, self._PAGE_TRACK,
+                                 request_track, None)
+    request_dependencies_lens = RequestDependencyLens(loading_trace)
+    deps = request_dependencies_lens.GetRequestDependencies()
+    self.assertEquals(1, len(deps))
+    self._AssertDependencyIs(
+        deps[0],
+        self._JS_REQUEST_OTHER_FRAME.request_id,
+        self._JS_REQUEST_2.request_id, 'script')
+
+  def _AssertDependencyIs(
+      self, dep, first_request_id, second_request_id, reason):
+    (first, second, dependency_reason) = dep
+    self.assertEquals(reason, dependency_reason)
+    self.assertEquals(first_request_id, first.request_id)
+    self.assertEquals(second_request_id, second.request_id)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index 7130965..77b3d4a 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -33,7 +33,7 @@ class Request(object):
   third_party/WebKit/Source/devtools/protocol.json.
 
   Fields:
-    request_id: (str) unique request ID. Postfixed with ".redirect" for
+    request_id: (str) unique request ID. Postfixed with REDIRECT_SUFFIX for
                 redirects.
     frame_id: (str) unique frame identifier.
     loader_id: (str) unique frame identifier.
@@ -122,6 +122,7 @@ class Request(object):
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
+  REDIRECT_SUFFIX = '.redirect'
   # Request status
   _STATUS_SENT = 0
   _STATUS_RESPONSE = 1
@@ -197,8 +198,8 @@ class RequestTrack(devtools_monitor.Track):
                           (('headers', 'response_headers'),
                            ('encodedDataLength', 'encoded_data_length'),
                            ('fromDiskCache', 'from_disk_cache')))
-    r.timing = _TimingFromDict(redirect_response['timing'])
-    r.request_id = request_id + '.redirect'
+    r.timing = TimingFromDict(redirect_response['timing'])
+    r.request_id = request_id + self.REDIRECT_SUFFIX
     self._requests_in_flight[r.request_id] = (r, RequestTrack._STATUS_FINISHED)
     del self._requests_in_flight[request_id]
     self._FinalizeRequest(r.request_id)
@@ -236,7 +237,7 @@ class RequestTrack(devtools_monitor.Track):
       timing_dict = response['timing']
     else:
       timing_dict = {'requestTime': r.timestamp}
-    r.timing = _TimingFromDict(timing_dict)
+    r.timing = TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
 
   def _DataReceived(self, request_id, params):
@@ -286,7 +287,8 @@ RequestTrack._METHOD_TO_HANDLER = {
     'Network.loadingFailed': RequestTrack._LoadingFailed}
 
 
-def _TimingFromDict(timing_dict):
+def TimingFromDict(timing_dict):
+  """Returns an instance of Timing from an () dict."""
   complete_timing_dict = {field: -1 for field in Timing._fields}
   timing_dict_mapped = {
       _TIMING_NAMES_MAPPING[k]: v for (k, v) in timing_dict.items()}
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 1e9403b..ac08b30 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -5,7 +5,7 @@
 import json
 import unittest
 
-from request_track import (Request, RequestTrack, _TimingFromDict)
+from request_track import (Request, RequestTrack, TimingFromDict)
 
 
 class RequestTestCase(unittest.TestCase):
@@ -192,6 +192,9 @@ class RequestTrackTestCase(unittest.TestCase):
                               RequestTrackTestCase._REDIRECT)
     self.assertEquals(1, len(self.request_track._requests_in_flight))
     self.assertEquals(1, len(self.request_track.GetEvents()))
+    redirect_request = self.request_track.GetEvents()[0]
+    self.assertTrue(redirect_request.request_id.endswith(
+        RequestTrack.REDIRECT_SUFFIX))
 
   def testRejectDuplicates(self):
     msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
@@ -231,7 +234,7 @@ class RequestTrackTestCase(unittest.TestCase):
     self.assertEquals(False, r.served_from_cache)
     self.assertEquals(False, r.from_disk_cache)
     self.assertEquals(False, r.from_service_worker)
-    timing = _TimingFromDict(response['timing'])
+    timing = TimingFromDict(response['timing'])
     loading_finished = RequestTrackTestCase._LOADING_FINISHED['params']
     loading_finished_offset = r._TimestampOffsetFromStartMs(
         loading_finished['timestamp'])

commit 56dd5cc56899f8931a6336ed926ced98d93416f5
Author: mattcary <mattcary@chromium.org>
Date:   Wed Jan 20 06:48:38 2016 -0800

    Lookup events which end in the given time range. This is a straightforward application of the index already computed.
    
    Review URL: https://codereview.chromium.org/1602963002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370389}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9619968005e1f8e2fbefbaa16499aee4ec3bffff

diff --git a/loading/tracing.py b/loading/tracing.py
index 47a0ae3..4959ed7 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -64,7 +64,9 @@ class TracingTrack(devtools_monitor.Track):
 
     Returns:
       List of events active at that timestamp. Instantaneous (ie, instant,
-      sample and counter) events are never included.
+      sample and counter) events are never included. Event end times are
+      exclusive, so that an event ending at the usec parameter will not be
+      returned.
       TODO(mattcary): currently live objects are included. If this is too big we
       may break that out into a separate index.
     """
@@ -78,7 +80,6 @@ class TracingTrack(devtools_monitor.Track):
       return []
     return events.event_list
 
-
   def ToJsonDict(self):
     return {'events': [e.ToJsonDict() for e in self._events]}
 
@@ -90,6 +91,35 @@ class TracingTrack(devtools_monitor.Track):
     tracing_track._events = events
     return tracing_track
 
+  def EventsEndingBetween(self, start_msec, end_msec):
+    """Gets the list of events whose end lies in a range.
+
+    Args:
+      start_msec: the start of the range to query, in milliseconds, inclusive.
+      end_msec: the end of the range to query, in milliseconds, inclusive.
+
+    Returns:
+      List of events whose end time lies in the range. Note that although the
+      range is inclusive at both ends, an ending timestamp is considered to be
+      exclusive of the actual event. An event ending at 10 msec will be returned
+      for a range [10, 14] as well as [8, 10], though the event is considered to
+      end the instant before 10 msec. In practice this is only important when
+      considering how events overlap; an event ending at 10 msec does not
+      overlap with one starting at 10 msec and so may unambiguously share ids,
+      etc.
+    """
+    self._IndexEvents()
+    low_idx = bisect.bisect_left(self._event_msec_index, start_msec) - 1
+    high_idx = bisect.bisect_right(self._event_msec_index, end_msec)
+    matched_events = []
+    for i in xrange(max(0, low_idx), high_idx):
+      if self._event_lists[i]:
+        for e in self._event_lists[i].event_list:
+          assert e.end_msec is not None
+          if e.end_msec >= start_msec and e.end_msec <= end_msec:
+            matched_events.append(e)
+    return matched_events
+
   def _IndexEvents(self):
     """Computes index for in-flight events.
 
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 77116fe..d190f8a 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -204,6 +204,46 @@ class TracingTrackTestCase(unittest.TestCase):
     for (e1, e2) in zip(self.track._events, deserialized_track._events):
       self.assertEquals(e1.tracing_event, e2.tracing_event)
 
+  def testEventsEndingBetween(self):
+    self.track.Handle(
+        'Tracing.dataCollected',
+        {'params':
+         {'value': [self.EventToMicroseconds(e) for e in
+          [{'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+           {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
+           {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
+           {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
+           {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
+           {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}]]}})
+    self.assertEqual(set('ABCDEF'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(0, 100)]))
+    self.assertFalse([e.args['name']
+                      for e in self.track.EventsEndingBetween(3, 5)])
+    self.assertTrue('B' in set([e.args['name']
+                          for e in self.track.EventsEndingBetween(3, 6)]))
+    self.assertEqual(set('B'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(3, 6)]))
+    self.assertEqual(set('AB'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(3, 7)]))
+    self.assertEqual(set('AB'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(6, 7)]))
+    self.assertEqual(set('A'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(7, 10)]))
+    self.assertEqual(set('AC'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(7, 11)]))
+    self.assertEqual(set('CD'),
+                     set([e.args['name']
+                          for e in self.track.EventsEndingBetween(8, 13)]))
+
+
+
+
 
 if __name__ == '__main__':
   unittest.main()

commit ecbb79355d6ff0346fec9067416ca8a95cf27aad
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 19 08:13:21 2016 -0800

    tools/android/loading: Archive tracks in LoadingTrace.
    
    Review URL: https://codereview.chromium.org/1606903002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370121}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6cf3e7e1f3cb4bd595f3ddabcc126ead2e10ddf7

diff --git a/loading/loading_trace.py b/loading/loading_trace.py
new file mode 100644
index 0000000..720fa1d
--- /dev/null
+++ b/loading/loading_trace.py
@@ -0,0 +1,55 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Represents the trace of a page load."""
+
+import page_track
+import request_track
+import tracing
+
+class LoadingTrace(object):
+  """Represents the trace of a page load."""
+  _URL_KEY = 'url'
+  _METADATA_KEY = 'metadata'
+  _PAGE_KEY = 'page_track'
+  _REQUEST_KEY = 'request_track'
+  _TRACING_KEY = 'tracing_track'
+
+  def __init__(self, url, metadata, page, request, tracing_track):
+    """Initializes a loading trace instance.
+
+    Args:
+      url: (str) URL that has been loaded
+      metadata: (dict) Metadata associated with the load.
+      page: (PageTrack) instance of PageTrack.
+      request: (RequestTrack) instance of RequestTrack.
+      tracing_track: (TracingTrack) instance of TracingTrack.
+    """
+    self.url = url
+    self.metadata = metadata
+    self.page_track = page
+    self.request_track = request
+    self.tracing_track = tracing_track
+
+  def ToJsonDict(self):
+    """Returns a dictionary representing this instance."""
+    result = {self._URL_KEY: self.url, self._METADATA_KEY: self.metadata,
+              self._PAGE_KEY: self.page_track.ToJsonDict(),
+              self._REQUEST_KEY: self.request_track.ToJsonDict(),
+              self._TRACING_KEY: self.tracing_track.ToJsonDict()}
+    return result
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns an instance from a dictionary returned by ToJsonDict()."""
+    keys = (cls._URL_KEY, cls._METADATA_KEY, cls._PAGE_KEY, cls._REQUEST_KEY,
+            cls._TRACING_KEY)
+    assert all(key in json_dict for key in keys)
+    page = page_track.PageTrack.FromJsonDict(json_dict[cls._PAGE_KEY])
+    request = request_track.RequestTrack.FromJsonDict(
+        json_dict[cls._REQUEST_KEY])
+    tracing_track = tracing.TracingTrack.FromJsonDict(
+        json_dict[cls._TRACING_KEY])
+    return LoadingTrace(json_dict[cls._URL_KEY], json_dict[cls._METADATA_KEY],
+                        page, request, tracing_track)
diff --git a/loading/request_track.py b/loading/request_track.py
index 08e1588..7130965 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -230,7 +230,12 @@ class RequestTrack(devtools_monitor.Track):
                       # network stack.
                       ('requestHeaders', 'request_headers'),
                       ('headers', 'response_headers')))
-    timing_dict = response['timing'] if r.protocol != 'data' else {}
+    # data URLs don't have a timing dict.
+    timing_dict = {}
+    if r.protocol != 'data':
+      timing_dict = response['timing']
+    else:
+      timing_dict = {'requestTime': r.timestamp}
     r.timing = _TimingFromDict(timing_dict)
     self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
 
@@ -291,7 +296,8 @@ def _TimingFromDict(timing_dict):
 
 def _CopyFromDictToObject(d, o, key_attrs):
   for (key, attr) in key_attrs:
-    setattr(o, attr, d[key])
+    if key in d:
+      setattr(o, attr, d[key])
 
 
 if __name__ == '__main__':
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 0b96225..fdf6cc6 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -5,8 +5,13 @@
 
 """Loading trace recorder."""
 
+import argparse
+import datetime
+import json
+import logging
 import os
 import sys
+import time
 
 _SRC_DIR = os.path.abspath(os.path.join(
     os.path.dirname(__file__), '..', '..', '..'))
@@ -19,32 +24,43 @@ import devil_chromium
 
 import device_setup
 import devtools_monitor
+import loading_trace
 import page_track
+import request_track
+import tracing
 
-class AndroidTraceRecorder(object):
-  """Records a loading trace."""
-  def __init__(self, url):
-    self.url = url
-    self.devtools_connection = None
-    self.page_track = None
 
-  def Go(self, connection):
-    self.devtools_connection = connection
-    self.page_track = page_track.PageTrack(self.devtools_connection)
-    self.devtools_connection.SetUpMonitoring()
-    self.devtools_connection.SendAndIgnoreResponse(
-        'Page.navigate', {'url': self.url})
-    self.devtools_connection.StartMonitoring()
-    print self.page_track.GetEvents()
+def RecordAndDumpTrace(device, url, output_filename):
+  with file(output_filename, 'w') as output,\
+        device_setup.DeviceConnection(device) as connection:
+    page = page_track.PageTrack(connection)
+    request = request_track.RequestTrack(connection)
+    tracing_track = tracing.TracingTrack(connection)
+    connection.SetUpMonitoring()
+    connection.SendAndIgnoreResponse('Network.clearBrowserCache', {})
+    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+    connection.StartMonitoring()
+    metadata = {'date': datetime.datetime.utcnow().isoformat(),
+                'seconds_since_epoch': time.time()}
+    trace = loading_trace.LoadingTrace(url, metadata, page, request,
+                                       tracing_track)
+    json.dump(trace.ToJsonDict(), output)
 
 
-def DoIt(url):
+def main():
+  logging.basicConfig(level=logging.INFO)
   devil_chromium.Initialize()
-  devices = device_utils.DeviceUtils.HealthyDevices()
-  device = devices[0]
-  trace_recorder = AndroidTraceRecorder(url)
-  device_setup.SetUpAndExecute(device, 'chrome', trace_recorder.Go)
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--url', required=True)
+  parser.add_argument('--output', required=True)
+  args = parser.parse_args()
+  url = args.url
+  if not url.startswith('http'):
+    url = 'http://' + url
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  RecordAndDumpTrace(device, url, args.output)
 
 
 if __name__ == '__main__':
-  DoIt(sys.argv[1])
+  main()
diff --git a/loading/tracing.py b/loading/tracing.py
index 318baf0..47a0ae3 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -28,7 +28,8 @@ class TracingTrack(devtools_monitor.Track):
         a stream is slower than the default reporting as dataCollected events.
     """
     super(TracingTrack, self).__init__(connection)
-    connection.RegisterListener('Tracing.dataCollected', self)
+    if connection:
+      connection.RegisterListener('Tracing.dataCollected', self)
     params = {}
     if categories:
       params['categories'] = (categories if type(categories) is str
@@ -36,7 +37,8 @@ class TracingTrack(devtools_monitor.Track):
     if fetch_stream:
       params['transferMode'] = 'ReturnAsStream'
 
-    connection.SyncRequestNoResponse('Tracing.start', params)
+    if connection:
+      connection.SyncRequestNoResponse('Tracing.start', params)
     self._events = []
 
     self._event_msec_index = None
@@ -76,6 +78,18 @@ class TracingTrack(devtools_monitor.Track):
       return []
     return events.event_list
 
+
+  def ToJsonDict(self):
+    return {'events': [e.ToJsonDict() for e in self._events]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    events = [Event(e) for e in json_dict['events']]
+    tracing_track = TracingTrack(None)
+    tracing_track._events = events
+    return tracing_track
+
   def _IndexEvents(self):
     """Computes index for in-flight events.
 
@@ -275,7 +289,10 @@ class Event(object):
     self._end_msec = None
     self._synthetic = synthetic
     if self.type == 'X':
-      self._end_msec = self.start_msec + tracing_event['dur'] / 1000.0
+      # Some events don't have a duration.
+      duration = (tracing_event['dur']
+                  if 'dur' in tracing_event else tracing_event['tdur'])
+      self._end_msec = self.start_msec + duration / 1000.0
 
   @property
   def start_msec(self):
@@ -356,3 +373,10 @@ class Event(object):
     if 'args' in closing.tracing_event:
       self.tracing_event.setdefault(
           'args', {}).update(closing.tracing_event['args'])
+
+  def ToJsonDict(self):
+    return self._tracing_event
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    return Event(json_dict)
diff --git a/loading/tracing_driver.py b/loading/tracing_driver.py
deleted file mode 100755
index c62e870..0000000
--- a/loading/tracing_driver.py
+++ /dev/null
@@ -1,49 +0,0 @@
-#! /usr/bin/python
-# Copyright 2016 The Chromium Authors. All rights reserved.
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Drive TracingConnection"""
-
-import argparse
-import json
-import logging
-import os.path
-import sys
-
-_SRC_DIR = os.path.abspath(os.path.join(
-    os.path.dirname(__file__), '..', '..', '..'))
-
-sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
-from devil.android import device_utils
-
-sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
-import device_setup
-import page_track
-import tracing
-
-
-def main():
-  logging.basicConfig(level=logging.INFO)
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--url', required=True)
-  parser.add_argument('--output', required=True)
-  args = parser.parse_args()
-  url = args.url
-  if not url.startswith('http'):
-    url = 'http://' + url
-  device = device_utils.DeviceUtils.HealthyDevices()[0]
-  with file(args.output, 'w') as output, \
-       file(args.output + '.page', 'w') as page_output, \
-       device_setup.DeviceConnection(device) as connection:
-    track = tracing.TracingTrack(connection, fetch_stream=False)
-    page = page_track.PageTrack(connection)
-    connection.SetUpMonitoring()
-    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
-    connection.StartMonitoring()
-    json.dump(page.GetEvents(), page_output, sort_keys=True, indent=2)
-    json.dump(track.GetEvents(), output, sort_keys=True, indent=2)
-
-
-if __name__ == '__main__':
-  main()
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
index 9a4bbb9..77116fe 100644
--- a/loading/tracing_unittest.py
+++ b/loading/tracing_unittest.py
@@ -7,20 +7,25 @@ import unittest
 
 import devtools_monitor
 
-from tracing import TracingTrack
-
-
-class StubConnection(object):
-  def RegisterListener(self, name, obj):
-    pass
-
-  def SyncRequestNoResponse(self, method, params):
-    pass
+from tracing import (Event, TracingTrack)
 
 
 class TracingTrackTestCase(unittest.TestCase):
+  _MIXED_EVENTS = [
+      {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
+      {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+      {'ts': 7, 'ph': 'D', 'id': 1},
+      {'ts': 10, 'ph': 'B', 'args': {'name': 'D'}},
+      {'ts': 10, 'ph': 'b', 'cat': 'X', 'id': 1, 'args': {'name': 'C'}},
+      {'ts': 11, 'ph': 'e', 'cat': 'X', 'id': 1},
+      {'ts': 12, 'ph': 'E'},
+      {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
+      {'ts': 13, 'ph': 'b', 'cat': 'X', 'id': 2, 'args': {'name': 'F'}},
+      {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
+      {'ts': 15, 'ph': 'D', 'id': 1}]
+
   def setUp(self):
-    self.track = TracingTrack(StubConnection())
+    self.track = TracingTrack(None)
 
   def EventToMicroseconds(self, event):
     if 'ts' in event:
@@ -176,18 +181,28 @@ class TracingTrackTestCase(unittest.TestCase):
 
   def testMixed(self):
     # A and E are objects, B complete, D a duration, and C and F async.
-    self.CheckIntervals([
-        {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
-        {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
-        {'ts': 7, 'ph': 'D', 'id': 1},
-        {'ts': 10, 'ph': 'B', 'args': {'name': 'D'}},
-        {'ts': 10, 'ph': 'b', 'cat': 'X', 'id': 1, 'args': {'name': 'C'}},
-        {'ts': 11, 'ph': 'e', 'cat': 'X', 'id': 1},
-        {'ts': 12, 'ph': 'E'},
-        {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
-        {'ts': 13, 'ph': 'b', 'cat': 'X', 'id': 2, 'args': {'name': 'F'}},
-        {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
-        {'ts': 15, 'ph': 'D', 'id': 1}])
+    self.CheckIntervals(self._MIXED_EVENTS)
+
+  def testEventSerialization(self):
+    for e in self._MIXED_EVENTS:
+      event = Event(e)
+      json_dict = event.ToJsonDict()
+      deserialized_event = Event.FromJsonDict(json_dict)
+      self.assertEquals(
+          event.tracing_event, deserialized_event.tracing_event)
+
+  def testTracingTrackSerialization(self):
+    events = self._MIXED_EVENTS
+    self.track.Handle('Tracing.dataCollected',
+                      {'params': {'value': [self.EventToMicroseconds(e)
+                                            for e in events]}})
+    json_dict = self.track.ToJsonDict()
+    self.assertTrue('events' in json_dict)
+    deserialized_track = TracingTrack.FromJsonDict(json_dict)
+    self.assertEquals(
+        len(self.track._events), len(deserialized_track._events))
+    for (e1, e2) in zip(self.track._events, deserialized_track._events):
+      self.assertEquals(e1.tracing_event, e2.tracing_event)
 
 
 if __name__ == '__main__':

commit c4666ec5fee4a5408902341bf5b28f9b6e5fe4c1
Author: mattcary <mattcary@chromium.org>
Date:   Tue Jan 19 03:21:05 2016 -0800

    Processing for tracing: add EventsAt method which gives the in-flight events
    present at a timestamp.
    
    Review URL: https://codereview.chromium.org/1607503003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370095}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 47ad0a66f5966f783dee80b0677b5b68c29010b4

diff --git a/loading/tracing.py b/loading/tracing.py
index 83ad38c..318baf0 100644
--- a/loading/tracing.py
+++ b/loading/tracing.py
@@ -4,9 +4,17 @@
 
 """Monitor tracing events on chrome via chrome remote debugging."""
 
+import bisect
+import itertools
+
 import devtools_monitor
 
+
 class TracingTrack(devtools_monitor.Track):
+  """Grabs and processes trace event messages.
+
+  See https://goo.gl/Qabkqk for details on the protocol.
+  """
   def __init__(self, connection, categories=None, fetch_stream=False):
     """Initialize this TracingTrack.
 
@@ -31,8 +39,320 @@ class TracingTrack(devtools_monitor.Track):
     connection.SyncRequestNoResponse('Tracing.start', params)
     self._events = []
 
+    self._event_msec_index = None
+    self._event_lists = None
+
   def Handle(self, method, event):
-    self._events.append(event)
+    for e in event['params']['value']:
+      self._events.append(Event(e))
+    # Just invalidate our indices rather than trying to be fancy and
+    # incrementally update.
+    self._event_msec_index = None
+    self._event_lists = None
 
   def GetEvents(self):
     return self._events
+
+  def EventsAt(self, msec):
+    """Gets events active at a timestamp.
+
+    Args:
+      msec: tracing milliseconds to query. Tracing milliseconds appears to be
+        since chrome startup (ie, arbitrary epoch).
+
+    Returns:
+      List of events active at that timestamp. Instantaneous (ie, instant,
+      sample and counter) events are never included.
+      TODO(mattcary): currently live objects are included. If this is too big we
+      may break that out into a separate index.
+    """
+    self._IndexEvents()
+    idx = bisect.bisect_right(self._event_msec_index, msec) - 1
+    if idx < 0:
+      return []
+    events = self._event_lists[idx]
+    assert events.start_msec <= msec
+    if not events or events.end_msec < msec:
+      return []
+    return events.event_list
+
+  def _IndexEvents(self):
+    """Computes index for in-flight events.
+
+    Creates a list of timestamps where events start or end, and tracks the
+    current set of in-flight events at the instant after each timestamp. To do
+    this we have to synthesize ending events for complete events, as well as
+    join and track the nesting of async, flow and other spanning events.
+
+    Events such as instant and counter events that aren't indexable are skipped.
+
+    """
+    if self._event_msec_index is not None:
+      return  # Already indexed.
+
+    if not self._events:
+      raise devtools_monitor.DevToolsConnectionException('No events to index')
+
+    self._event_msec_index = []
+    self._event_lists = []
+    synthetic_events = []
+    for e in self._events:
+      synthetic_events.extend(e.Synthesize())
+    synthetic_events.sort(key=lambda e: e.start_msec)
+    current_events = set()
+    next_idx = 0
+    spanning_events = self._SpanningEvents()
+    while next_idx < len(synthetic_events):
+      current_msec = synthetic_events[next_idx].start_msec
+      while next_idx < len(synthetic_events):
+        event = synthetic_events[next_idx]
+        assert event.IsIndexable()
+        if event.start_msec > current_msec:
+          break
+        matched_event = spanning_events.Match(event)
+        if matched_event is not None:
+          event = matched_event
+        if not event.synthetic and (
+            event.end_msec is None or event.end_msec >= current_msec):
+          current_events.add(event)
+        next_idx += 1
+      current_events -= set([
+          e for e in current_events
+          if e.end_msec is not None and e.end_msec <= current_msec])
+      self._event_msec_index.append(current_msec)
+      self._event_lists.append(self._EventList(current_events))
+    if spanning_events.HasPending():
+      raise devtools_monitor.DevToolsConnectionException(
+          'Pending spanning events: %s' %
+          '\n'.join([str(e) for e in spanning_events.PendingEvents()]))
+
+  class _SpanningEvents(object):
+    def __init__(self):
+      self._duration_stack = []
+      self._async_stacks = {}
+      self._objects = {}
+      self._MATCH_HANDLER = {
+          'B': self._DurationBegin,
+          'E': self._DurationEnd,
+          'b': self._AsyncStart,
+          'e': self._AsyncEnd,
+          'S': self._AsyncStart,
+          'F': self._AsyncEnd,
+          'N': self._ObjectCreated,
+          'D': self._ObjectDestroyed,
+          'X': self._Ignore,
+          None: self._Ignore,
+          }
+
+    def Match(self, event):
+      return self._MATCH_HANDLER.get(
+          event.type, self._Unsupported)(event)
+
+    def HasPending(self):
+      return (self._duration_stack or
+              self._async_stacks or
+              self._objects)
+
+    def PendingEvents(self):
+      return itertools.chain(
+          (e for e in self._duration_stack),
+          (o for o in self._objects),
+          itertools.chain.from_iterable((
+              (e for e in s) for s in self._async_stacks.itervalues())))
+
+    def _AsyncKey(self, event):
+      return (event.tracing_event['cat'], event.id)
+
+    def _Ignore(self, _event):
+      return None
+
+    def _Unsupported(self, event):
+      raise devtools_monitor.DevToolsConnectionException(
+          'Unsupported spanning event type: %s' % event)
+
+    def _DurationBegin(self, event):
+      self._duration_stack.append(event)
+      return None
+
+    def _DurationEnd(self, event):
+      if not self._duration_stack:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Unmatched duration end: %s' % event)
+      start = self._duration_stack.pop()
+      start.SetClose(event)
+      return start
+
+    def _AsyncStart(self, event):
+      key = self._AsyncKey(event)
+      self._async_stacks.setdefault(key, []).append(event)
+      return None
+
+    def _AsyncEnd(self, event):
+      key = self._AsyncKey(event)
+      if key not in self._async_stacks:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Unmatched async end %s: %s' % (key, event))
+      stack = self._async_stacks[key]
+      start = stack.pop()
+      if not stack:
+        del self._async_stacks[key]
+      start.SetClose(event)
+      return start
+
+    def _ObjectCreated(self, event):
+      # The tracing event format has object deletion timestamps being exclusive,
+      # that is the timestamp for a deletion my equal that of the next create at
+      # the same address. This asserts that does not happen in practice as it is
+      # inconvenient to handle that correctly here.
+      if event.id in self._objects:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Multiple objects at same address: %s, %s' %
+            (event, self._objects[event.id]))
+      self._objects[event.id] = event
+      return None
+
+    def _ObjectDestroyed(self, event):
+      if event.id not in self._objects:
+        raise devtools_monitor.DevToolsConnectionException(
+            'Missing object creation for %s' % event)
+      start = self._objects[event.id]
+      del self._objects[event.id]
+      start.SetClose(event)
+      return start
+
+  class _EventList(object):
+    def __init__(self, events):
+      self._events = [e for e in events]
+      if self._events:
+        self._start_msec = min(e.start_msec for e in self._events)
+        # Event end times may be changed after this list is created so the end
+        # can't be cached.
+      else:
+        self._start_msec = self._end_msec = None
+
+    @property
+    def event_list(self):
+      return self._events
+
+    @property
+    def start_msec(self):
+      return self._start_msec
+
+    @property
+    def end_msec(self):
+      return max(e.end_msec for e in self._events)
+
+    def __nonzero__(self):
+      return bool(self._events)
+
+
+class Event(object):
+  """Wraps a tracing event."""
+  CLOSING_EVENTS = {'E': 'B',
+                    'e': 'b',
+                    'F': 'S',
+                    'D': 'N'}
+  def __init__(self, tracing_event, synthetic=False):
+    """Creates Event.
+
+    Intended to be created only by TracingTrack.
+
+    Args:
+      tracing_event: JSON tracing event, as defined in https://goo.gl/Qabkqk.
+      synthetic: True if the event is synthetic. This is only used for indexing
+        internal to TracingTrack.
+    """
+    if not synthetic and tracing_event['ph'] in ['s', 't', 'f']:
+      raise devtools_monitor.DevToolsConnectionException(
+          'Unsupported event: %s' % tracing_event)
+    if not synthetic and tracing_event['ph'] in ['p']:
+      raise devtools_monitor.DevToolsConnectionException(
+          'Deprecated event: %s' % tracing_event)
+
+    self._tracing_event = tracing_event
+    # Note tracing event times are in microseconds.
+    self._start_msec = tracing_event['ts'] / 1000.0
+    self._end_msec = None
+    self._synthetic = synthetic
+    if self.type == 'X':
+      self._end_msec = self.start_msec + tracing_event['dur'] / 1000.0
+
+  @property
+  def start_msec(self):
+    return self._start_msec
+
+  @property
+  def end_msec(self):
+    return self._end_msec
+
+  @property
+  def type(self):
+    if self._synthetic:
+      return None
+    return self._tracing_event['ph']
+
+  @property
+  def args(self):
+    return self._tracing_event.get('args', {})
+
+  @property
+  def id(self):
+    return self._tracing_event.get('id')
+
+  @property
+  def tracing_event(self):
+    return self._tracing_event
+
+  @property
+  def synthetic(self):
+    return self._synthetic
+
+  def __str__(self):
+    return ''.join([str(self._tracing_event),
+                    '[%s,%s]' % (self.start_msec, self.end_msec)])
+
+  def IsIndexable(self):
+    """True iff the event can be indexed by time."""
+    return self._synthetic or self.type not in [
+        'I', 'P', 'c', 'C',
+        'n', 'T', 'p',  # TODO(mattcary): ?? instant types of async events.
+        'O',            # TODO(mattcary): ?? object snapshot
+        ]
+
+  def Synthesize(self):
+    """Expand into synthetic events.
+
+    Returns:
+      A list of events, possibly some synthetic, whose start times are all
+      interesting for purposes of indexing. If the event is not indexable the
+      set may be empty.
+    """
+    if not self.IsIndexable():
+      return []
+    if self.type == 'X':
+      # Tracing event timestamps are microseconds!
+      return [self, Event({'ts': self.end_msec * 1000}, synthetic=True)]
+    return [self]
+
+  def SetClose(self, closing):
+    """Close a spanning event.
+
+    Args:
+      closing: The closing event.
+
+    Raises:
+      devtools_monitor.DevToolsConnectionException if closing can't property
+      close this event.
+    """
+    if self.type != self.CLOSING_EVENTS.get(closing.type):
+      raise devtools_monitor.DevToolsConnectionException(
+        'Bad closing: %s --> %s' % (self, closing))
+    if self.type in ['b', 'S'] and (
+        self.tracing_event['cat'] != closing.tracing_event['cat'] or
+        self.id != closing.id):
+      raise devtools_monitor.DevToolsConnectionException(
+        'Bad async closing: %s --> %s' % (self, closing))
+    self._end_msec = closing.start_msec
+    if 'args' in closing.tracing_event:
+      self.tracing_event.setdefault(
+          'args', {}).update(closing.tracing_event['args'])
diff --git a/loading/tracing_unittest.py b/loading/tracing_unittest.py
new file mode 100644
index 0000000..9a4bbb9
--- /dev/null
+++ b/loading/tracing_unittest.py
@@ -0,0 +1,194 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+import unittest
+
+import devtools_monitor
+
+from tracing import TracingTrack
+
+
+class StubConnection(object):
+  def RegisterListener(self, name, obj):
+    pass
+
+  def SyncRequestNoResponse(self, method, params):
+    pass
+
+
+class TracingTrackTestCase(unittest.TestCase):
+  def setUp(self):
+    self.track = TracingTrack(StubConnection())
+
+  def EventToMicroseconds(self, event):
+    if 'ts' in event:
+      event['ts'] *= 1000
+    if 'dur' in event:
+      event['dur'] *= 1000
+    return event
+
+  def CheckTrack(self, timestamp, names):
+    self.assertEqual(
+        set((e.args['name'] for e in self.track.EventsAt(timestamp))),
+        set(names))
+
+  def CheckIntervals(self, events):
+    """All tests should produce the following sequence of intervals, each
+    identified by a 'name' in the event args.
+
+    Timestamp
+    3    |      A
+    4    |
+    5    | |    B
+    6    |
+    7
+    ..
+    10   | |    C, D
+    11     |
+    12   |      E
+    13   | |    F
+    14   |
+    """
+    self.track.Handle('Tracing.dataCollected',
+                      {'params': {'value': [self.EventToMicroseconds(e)
+                                            for e in events]}})
+    self.CheckTrack(0, '')
+    self.CheckTrack(2, '')
+    self.CheckTrack(3, 'A')
+    self.CheckTrack(4, 'A')
+    self.CheckTrack(5, 'AB')
+    self.CheckTrack(6, 'A')
+    self.CheckTrack(7, '')
+    self.CheckTrack(9, '')
+    self.CheckTrack(10, 'CD')
+    self.CheckTrack(11, 'D')
+    self.CheckTrack(12, 'E')
+    self.CheckTrack(13, 'EF')
+    self.CheckTrack(14, 'E')
+    self.CheckTrack(15, '')
+    self.CheckTrack(100, '')
+
+  def testComplete(self):
+    # These are deliberately out of order.
+    self.CheckIntervals([
+        {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+        {'ts': 3, 'ph': 'X', 'dur': 4, 'args': {'name': 'A'}},
+        {'ts': 10, 'ph': 'X', 'dur': 1, 'args': {'name': 'C'}},
+        {'ts': 10, 'ph': 'X', 'dur': 2, 'args': {'name': 'D'}},
+        {'ts': 13, 'ph': 'X', 'dur': 1, 'args': {'name': 'F'}},
+        {'ts': 12, 'ph': 'X', 'dur': 3, 'args': {'name': 'E'}}])
+
+  def testDuration(self):
+    self.CheckIntervals([
+        {'ts': 3, 'ph': 'B', 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'B', 'args': {'name': 'B'}},
+        {'ts': 6, 'ph': 'E'},
+        {'ts': 7, 'ph': 'E'},
+        # Since async intervals aren't named and must be nested, we fudge the
+        # beginning of D by a tenth to ensure it's consistently detected as the
+        # outermost event.
+        {'ts': 9.9, 'ph': 'B', 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'B', 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'E'},
+        # End of D. As end times are exclusive this should not conflict with the
+        # start of E.
+        {'ts': 12, 'ph': 'E'},
+        {'ts': 12, 'ph': 'B', 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'B', 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'E'},
+        {'ts': 15, 'ph': 'E'}])
+
+  def testBadDurationExtraBegin(self):
+    self.assertRaises(devtools_monitor.DevToolsConnectionException,
+                      self.CheckIntervals,
+                      [{'ts': 3, 'ph': 'B'},
+                       {'ts': 4, 'ph': 'B'},
+                       {'ts': 5, 'ph': 'E'}])
+
+  def testBadDurationExtraEnd(self):
+    self.assertRaises(devtools_monitor.DevToolsConnectionException,
+                      self.CheckIntervals,
+                      [{'ts': 3, 'ph': 'B'},
+                       {'ts': 4, 'ph': 'E'},
+                       {'ts': 5, 'ph': 'E'}])
+
+  def testAsync(self):
+    self.CheckIntervals([
+        # A, B and F have the same category/id (so that A & B nest); C-E do not.
+        {'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'B'}},
+        # Not indexable.
+        {'ts': 4, 'ph': 'n', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 1},
+        {'ts': 7, 'ph': 'e', 'cat': 'A', 'id': 1},
+        {'ts': 10, 'ph': 'b', 'cat': 'B', 'id': 2, 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'b', 'cat': 'B', 'id': 3, 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'e', 'cat': 'B', 'id': 3},
+        {'ts': 12, 'ph': 'e', 'cat': 'B', 'id': 2},
+        {'ts': 12, 'ph': 'b', 'cat': 'A', 'id': 2, 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'e', 'cat': 'A', 'id': 1},
+        {'ts': 15, 'ph': 'e', 'cat': 'A', 'id': 2}])
+
+  def testBadAsyncIdMismatch(self):
+    self.assertRaises(
+        devtools_monitor.DevToolsConnectionException,
+        self.CheckIntervals,
+        [{'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+         {'ts': 5, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'B'}},
+         {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 2},
+         {'ts': 7, 'ph': 'e', 'cat': 'A', 'id': 1}])
+
+  def testBadAsyncExtraBegin(self):
+    self.assertRaises(
+        devtools_monitor.DevToolsConnectionException,
+        self.CheckIntervals,
+        [{'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+         {'ts': 5, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'B'}},
+         {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 1}])
+
+  def testBadAsyncExtraEnd(self):
+    self.assertRaises(
+        devtools_monitor.DevToolsConnectionException,
+        self.CheckIntervals,
+        [{'ts': 3, 'ph': 'b', 'cat': 'A', 'id': 1, 'args': {'name': 'A'}},
+         {'ts': 5, 'ph': 'e', 'cat': 'A', 'id': 1},
+         {'ts': 6, 'ph': 'e', 'cat': 'A', 'id': 1}])
+
+  def testObject(self):
+    # A and E share ids, which is okay as their scopes are disjoint.
+    self.CheckIntervals([
+        {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'N', 'id': 2, 'args': {'name': 'B'}},
+        {'ts': 6, 'ph': 'D', 'id': 2},
+        {'ts': 6, 'ph': 'O', 'id': 2},  #  Ignored.
+        {'ts': 7, 'ph': 'D', 'id': 1},
+        {'ts': 10, 'ph': 'N', 'id': 3, 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'N', 'id': 4, 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'D', 'id': 4},
+        {'ts': 12, 'ph': 'D', 'id': 3},
+        {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'N', 'id': 5, 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'D', 'id': 5},
+        {'ts': 15, 'ph': 'D', 'id': 1}])
+
+  def testMixed(self):
+    # A and E are objects, B complete, D a duration, and C and F async.
+    self.CheckIntervals([
+        {'ts': 3, 'ph': 'N', 'id': 1, 'args': {'name': 'A'}},
+        {'ts': 5, 'ph': 'X', 'dur': 1, 'args': {'name': 'B'}},
+        {'ts': 7, 'ph': 'D', 'id': 1},
+        {'ts': 10, 'ph': 'B', 'args': {'name': 'D'}},
+        {'ts': 10, 'ph': 'b', 'cat': 'X', 'id': 1, 'args': {'name': 'C'}},
+        {'ts': 11, 'ph': 'e', 'cat': 'X', 'id': 1},
+        {'ts': 12, 'ph': 'E'},
+        {'ts': 12, 'ph': 'N', 'id': 1, 'args': {'name': 'E'}},
+        {'ts': 13, 'ph': 'b', 'cat': 'X', 'id': 2, 'args': {'name': 'F'}},
+        {'ts': 14, 'ph': 'e', 'cat': 'X', 'id': 2},
+        {'ts': 15, 'ph': 'D', 'id': 1}])
+
+
+if __name__ == '__main__':
+  unittest.main()

commit a3f2f68229bbc4266c650b348741b57a35e54e0f
Author: lizeb <lizeb@chromium.org>
Date:   Mon Jan 18 09:41:36 2016 -0800

    tools/android/loading: Separate and improve PageTrack.
    
    Also add GetContentType() and IsDataRequest() to Request.
    
    Review URL: https://codereview.chromium.org/1606523003
    
    Cr-Original-Commit-Position: refs/heads/master@{#370035}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 36937af36f5423184a35e07f89daef5a3e974609

diff --git a/loading/page_track.py b/loading/page_track.py
new file mode 100644
index 0000000..bb289db
--- /dev/null
+++ b/loading/page_track.py
@@ -0,0 +1,62 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import devtools_monitor
+
+
+class PageTrack(devtools_monitor.Track):
+  """Records the events from the page track."""
+  _METHODS = ('Page.frameStartedLoading', 'Page.frameStoppedLoading',
+              'Page.frameAttached')
+  def __init__(self, connection):
+    super(PageTrack, self).__init__(connection)
+    self._connection = connection
+    self._events = []
+    self._pending_frames = set()
+    self._known_frames = set()
+    self._main_frame_id = None
+    if self._connection:
+      for method in PageTrack._METHODS:
+        self._connection.RegisterListener(method, self)
+
+  def Handle(self, method, msg):
+    assert method in PageTrack._METHODS
+    params = msg['params']
+    frame_id = params['frameId']
+    should_stop = False
+    event = {'method': method, 'frame_id': frame_id}
+    if method == 'Page.frameStartedLoading':
+      if self._main_frame_id is None:
+        self._main_frame_id = params['frameId']
+      self._pending_frames.add(frame_id)
+      self._known_frames.add(frame_id)
+    elif method == 'Page.frameStoppedLoading':
+      assert frame_id in self._pending_frames
+      self._pending_frames.remove(frame_id)
+      if frame_id == self._main_frame_id:
+        should_stop = True
+    elif method == 'Page.frameAttached':
+      self._known_frames.add(frame_id)
+      parent_frame = params['parentFrameId']
+      assert parent_frame in self._known_frames
+      event['parent_frame_id'] = parent_frame
+    self._events.append(event)
+    if should_stop and self._connection:
+      self._connection.StopMonitoring()
+
+  def GetEvents(self):
+    #TODO(lizeb): Add more checks here (child frame stops loading before parent,
+    #for instance).
+    return self._events
+
+  def ToJsonDict(self):
+    return {'events': [event for event in self._events]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    result = PageTrack(None)
+    events = [event for event in json_dict['events']]
+    result._events = events
+    return result
diff --git a/loading/page_track_unittest.py b/loading/page_track_unittest.py
new file mode 100644
index 0000000..6757a01
--- /dev/null
+++ b/loading/page_track_unittest.py
@@ -0,0 +1,58 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+import devtools_monitor
+from page_track import PageTrack
+
+class MockDevToolsConnection(object):
+  def __init__(self):
+    self.stop_has_been_called = False
+
+  def RegisterListener(self, name, listener):
+    pass
+
+  def StopMonitoring(self):
+    self.stop_has_been_called = True
+
+
+class PageTrackTest(unittest.TestCase):
+  _EVENTS = [{'method': 'Page.frameStartedLoading',
+              'params': {'frameId': '1234.1'}},
+             {'method': 'Page.frameAttached',
+              'params': {'frameId': '1234.12', 'parentFrameId': '1234.1'}},
+             {'method': 'Page.frameStartedLoading',
+              'params': {'frameId': '1234.12'}},
+             {'method': 'Page.frameStoppedLoading',
+              'params': {'frameId': '1234.12'}},
+             {'method': 'Page.frameStoppedLoading',
+              'params': {'frameId': '1234.1'}}]
+  def testAsksMonitoringToStop(self):
+    devtools_connection = MockDevToolsConnection()
+    page_track = PageTrack(devtools_connection)
+    for msg in PageTrackTest._EVENTS[:-1]:
+      page_track.Handle(msg['method'], msg)
+      self.assertFalse(devtools_connection.stop_has_been_called)
+    msg = PageTrackTest._EVENTS[-1]
+    page_track.Handle(msg['method'], msg)
+    self.assertTrue(devtools_connection.stop_has_been_called)
+
+  def testUnknownParent(self):
+    page_track = PageTrack(None)
+    msg = {'method': 'Page.frameAttached',
+           'params': {'frameId': '1234.12', 'parentFrameId': '1234.1'}}
+    with self.assertRaises(AssertionError):
+      page_track.Handle(msg['method'], msg)
+
+  def testStopsLoadingUnknownFrame(self):
+    page_track = PageTrack(None)
+    msg = {'method': 'Page.frameStoppedLoading',
+           'params': {'frameId': '1234.12'}}
+    with self.assertRaises(AssertionError):
+      page_track.Handle(msg['method'], msg)
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/request_track.py b/loading/request_track.py
index de3fd11..08e1588 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -104,6 +104,17 @@ class Request(object):
       setattr(result, k, v)
     return result
 
+  def GetContentType(self):
+    """Returns the content type, or None."""
+    content_type = self.response_headers.get('Content-Type', None)
+    if not content_type or ';' not in content_type:
+      return content_type
+    else:
+      return content_type[:content_type.index(';')]
+
+  def IsDataRequest(self):
+    return self.protocol == 'data'
+
   # For testing.
   def __eq__(self, o):
     return self.__dict__ == o.__dict__
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 459e2de..1e9403b 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -8,6 +8,17 @@ import unittest
 from request_track import (Request, RequestTrack, _TimingFromDict)
 
 
+class RequestTestCase(unittest.TestCase):
+  def testContentType(self):
+    r = Request()
+    r.response_headers = {}
+    self.assertEquals(None, r.GetContentType())
+    r.response_headers = {'Content-Type': 'application/javascript'}
+    self.assertEquals('application/javascript', r.GetContentType())
+    r.response_headers = {'Content-Type': 'application/javascript;bla'}
+    self.assertEquals('application/javascript', r.GetContentType())
+
+
 class RequestTrackTestCase(unittest.TestCase):
   _REQUEST_WILL_BE_SENT = {
       'method': 'Network.requestWillBeSent',
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index cb9aa7a..0b96225 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -19,46 +19,7 @@ import devil_chromium
 
 import device_setup
 import devtools_monitor
-
-
-class PageTrack(devtools_monitor.Track):
-  """Records the events from the page track."""
-  def __init__(self, connection):
-    super(PageTrack, self).__init__(connection)
-    self._connection = connection
-    self._events = []
-    self._main_frame_id = None
-    if self._connection:
-      self._connection.RegisterListener('Page.frameStartedLoading', self)
-      self._connection.RegisterListener('Page.frameStoppedLoading', self)
-
-  def Handle(self, method, msg):
-    params = msg['params']
-    frame_id = params['frameId']
-    should_stop = False
-    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
-      self._main_frame_id = params['frameId']
-    elif (method == 'Page.frameStoppedLoading'
-          and params['frameId'] == self._main_frame_id):
-      should_stop = True
-    self._events.append((method, frame_id))
-    if should_stop:
-      self._connection.StopMonitoring()
-
-  def GetEvents(self):
-    return self._events
-
-  def ToJsonDict(self):
-    return {'events': [event for event in self._events]}
-
-  @classmethod
-  def FromJsonDict(cls, json_dict):
-    assert 'events' in json_dict
-    result = PageTrack(None)
-    events = [event for event in json_dict['events']]
-    result._events = events
-    return result
-
+import page_track
 
 class AndroidTraceRecorder(object):
   """Records a loading trace."""
@@ -69,8 +30,7 @@ class AndroidTraceRecorder(object):
 
   def Go(self, connection):
     self.devtools_connection = connection
-    self.page_track = PageTrack(self.devtools_connection)
-
+    self.page_track = page_track.PageTrack(self.devtools_connection)
     self.devtools_connection.SetUpMonitoring()
     self.devtools_connection.SendAndIgnoreResponse(
         'Page.navigate', {'url': self.url})
diff --git a/loading/tracing_driver.py b/loading/tracing_driver.py
index 0996d34..c62e870 100755
--- a/loading/tracing_driver.py
+++ b/loading/tracing_driver.py
@@ -19,7 +19,7 @@ from devil.android import device_utils
 
 sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 import device_setup
-import trace_recorder
+import page_track
 import tracing
 
 
@@ -37,7 +37,7 @@ def main():
        file(args.output + '.page', 'w') as page_output, \
        device_setup.DeviceConnection(device) as connection:
     track = tracing.TracingTrack(connection, fetch_stream=False)
-    page = trace_recorder.PageTrack(connection)
+    page = page_track.PageTrack(connection)
     connection.SetUpMonitoring()
     connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
     connection.StartMonitoring()

commit cd86311f4fd2e04e16555090b0102711810f1135
Author: mattcary <mattcary@chromium.org>
Date:   Mon Jan 18 07:42:51 2016 -0800

    Tracing tack for devtools monitor.
    
    This just dumps tracing events, and doesn't do any processing of those events yet.
    
    I experimented with fetching trace data as a stream. It appears to not be
    beneficial but I have left the option in. I also refactored some of the device
    setup work to make it easier to switch between local and device debugging,
    changing devtools ports, etc. Note that I also changed the FlagChanger context
    to make it more explicit what's going on w/rt adding or replacing flags.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1589843002
    
    Cr-Original-Commit-Position: refs/heads/master@{#370028}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: fe374e019637b02c4e2540f2b43e1cf87122ccde

diff --git a/loading/device_setup.py b/loading/device_setup.py
index 24becb7..4d5d40b 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -17,17 +17,21 @@ sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 from pylib import flag_changer
 
+import devtools_monitor
+
 DEVTOOLS_PORT = 9222
 DEVTOOLS_HOSTNAME = 'localhost'
+DEFAULT_CHROME_PACKAGE = 'chrome'
 
 @contextlib.contextmanager
-def FlagChanger(device, command_line_path, new_flags):
-  """Changes the flags in a context, restores them afterwards.
+def FlagReplacer(device, command_line_path, new_flags):
+  """Replaces chrome flags in a context, restores them afterwards.
 
   Args:
-    device: Device to target, from DeviceUtils.
+    device: Device to target, from DeviceUtils. Can be None, in which case this
+      context manager is a no-op.
     command_line_path: Full path to the command-line file.
-    new_flags: Flags to add.
+    new_flags: Flags to replace.
   """
   # If we're logging requests from a local desktop chrome instance there is no
   # device.
@@ -35,7 +39,7 @@ def FlagChanger(device, command_line_path, new_flags):
     yield
     return
   changer = flag_changer.FlagChanger(device, command_line_path)
-  changer.AddFlags(new_flags)
+  changer.ReplaceFlags(new_flags)
   try:
     yield
   finally:
@@ -63,28 +67,30 @@ def _SetUpDevice(device, package_info):
   device.KillAll(package_info.package, quiet=True)
 
 
-def SetUpAndExecute(device, package, fn):
-  """Start logging process.
+@contextlib.contextmanager
+def DeviceConnection(device,
+                     package=DEFAULT_CHROME_PACKAGE,
+                     hostname=DEVTOOLS_HOSTNAME,
+                     port=DEVTOOLS_PORT):
+  """Context for starting recording on a device.
 
-  Sets up any device and tracing appropriately and then executes the core
-  logging function.
+  Sets up and restores any device and tracing appropriately
 
   Args:
-    device: Android device, or None for a local run.
+    device: Android device, or None for a local run (in which case chrome needs
+      to have been started with --remote-debugging-port=XXX).
     package: the key for chrome package info.
-    fn: the function to execute that launches chrome and performs the
-        appropriate instrumentation, see _Log*Internal().
 
   Returns:
-    As fn() returns.
+    A context manager type which evaluates to a DevToolsConnection.
   """
   package_info = constants.PACKAGE_INFO[package]
   command_line_path = '/data/local/chrome-command-line'
   new_flags = ['--enable-test-events',
-               '--remote-debugging-port=%d' % DEVTOOLS_PORT]
+               '--remote-debugging-port=%d' % port]
   if device:
     _SetUpDevice(device, package_info)
-  with FlagChanger(device, command_line_path, new_flags):
+  with FlagReplacer(device, command_line_path, new_flags):
     if device:
       start_intent = intent.Intent(
           package=package_info.package, activity=package_info.activity,
@@ -92,6 +98,25 @@ def SetUpAndExecute(device, package, fn):
       device.StartActivity(start_intent, blocking=True)
       time.sleep(2)
     # If no device, we don't care about chrome startup so skip the about page.
-    with ForwardPort(device, 'tcp:%d' % DEVTOOLS_PORT,
+    with ForwardPort(device, 'tcp:%d' % port,
                      'localabstract:chrome_devtools_remote'):
-      return fn()
+      yield devtools_monitor.DevToolsConnection(hostname, port)
+
+
+def SetUpAndExecute(device, package, fn):
+  """Start logging process.
+
+  Wrapper for DeviceConnection for those functionally inclined.
+
+  Args:
+    device: Android device, or None for a local run.
+    package: the key for chrome package info.
+    fn: the function to execute that launches chrome and performs the
+        appropriate instrumentation. The function will receive a
+        DevToolsConnection as its sole parameter.
+
+  Returns:
+    As fn() returns.
+  """
+  with DeviceConnection(device, package) as connection:
+    return fn(connection)
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index fc652d2..5e8470b 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -24,9 +24,62 @@ class DevToolsConnectionException(Exception):
     logging.warning("DevToolsConnectionException: " + message)
 
 
+# Taken from telemetry.internal.backends.chrome_inspector.tracing_backend.
+# TODO(mattcary): combine this with the above and export?
+class _StreamReader(object):
+  def __init__(self, inspector, stream_handle):
+    self._inspector_websocket = inspector
+    self._handle = stream_handle
+    self._callback = None
+    self._data = None
+
+  def Read(self, callback):
+    # Do not allow the instance of this class to be reused, as
+    # we only read data sequentially at the moment, so a stream
+    # can only be read once.
+    assert not self._callback
+    self._data = []
+    self._callback = callback
+    self._ReadChunkFromStream()
+    # Queue one extra read ahead to avoid latency.
+    self._ReadChunkFromStream()
+
+  def _ReadChunkFromStream(self):
+    # Limit max block size to avoid fragmenting memory in sock.recv(),
+    # (see https://github.com/liris/websocket-client/issues/163 for details)
+    req = {'method': 'IO.read', 'params': {
+        'handle': self._handle, 'size': 32768}}
+    self._inspector_websocket.AsyncRequest(req, self._GotChunkFromStream)
+
+  def _GotChunkFromStream(self, response):
+    # Quietly discard responses from reads queued ahead after EOF.
+    if self._data is None:
+      return
+    if 'error' in response:
+      raise DevToolsConnectionException(
+          'Reading trace failed: %s' % response['error']['message'])
+    result = response['result']
+    self._data.append(result['data'])
+    if not result.get('eof', False):
+      self._ReadChunkFromStream()
+      return
+    req = {'method': 'IO.close', 'params': {'handle': self._handle}}
+    self._inspector_websocket.SendAndIgnoreResponse(req)
+    trace_string = ''.join(self._data)
+    self._data = None
+    self._callback(trace_string)
+
+
 class DevToolsConnection(object):
   """Handles the communication with a DevTools server.
   """
+  TRACING_DOMAIN = 'Tracing'
+  TRACING_END_METHOD = 'Tracing.end'
+  TRACING_DATA_METHOD = 'Tracing.dataCollected'
+  TRACING_DONE_EVENT = 'Tracing.tracingComplete'
+  TRACING_STREAM_EVENT = 'Tracing.tracingComplete'  # Same as TRACING_DONE.
+  TRACING_TIMEOUT = 300
+
   def __init__(self, hostname, port):
     """Initializes the connection with a DevTools server.
 
@@ -35,8 +88,11 @@ class DevToolsConnection(object):
       port: port number.
     """
     self._ws = self._Connect(hostname, port)
-    self._listeners = {}
+    self._event_listeners = {}
+    self._domain_listeners = {}
     self._domains_to_enable = set()
+    self._tearing_down_tracing = False
+    self._set_up = False
     self._please_stop = False
 
   def RegisterListener(self, name, listener):
@@ -45,12 +101,16 @@ class DevToolsConnection(object):
     Also takes care of enabling the relevant domain before starting monitoring.
 
     Args:
-      name: (str) Event the listener wants to listen to, e.g.
-            Network.requestWillBeSent.
+      name: (str) Domain or event the listener wants to listen to, e.g.
+            "Network.requestWillBeSent" or "Tracing".
       listener: (Listener) listener instance.
     """
-    domain = name[:name.index('.')]
-    self._listeners[name] = listener
+    if '.' in name:
+      domain = name[:name.index('.')]
+      self._event_listeners[name] = listener
+    else:
+      domain = name
+      self._domain_listeners[domain] = listener
     self._domains_to_enable.add(domain)
 
   def UnregisterListener(self, listener):
@@ -59,10 +119,14 @@ class DevToolsConnection(object):
     Args:
       listener: (Listener) listener to unregister.
     """
-    keys = [k for (k, v) in self._listeners if v is listener]
+    keys = ([k for k, l in self._event_listeners if l is listener] +
+            [k for k, l in self._domain_listeners if l is listener])
     assert keys, "Removing non-existent listener"
     for key in keys:
-      del(self._listeners[key])
+      if key in self._event_listeners:
+        del(self._event_listeners[key])
+      if key in self._domain_listeners:
+        del(self._domain_listeners[key])
 
   def SyncRequest(self, method, params=None):
     """Issues a synchronous request to the DevTools server.
@@ -91,41 +155,103 @@ class DevToolsConnection(object):
       request['params'] = params
     self._ws.SendAndIgnoreResponse(request)
 
+  def SyncRequestNoResponse(self, method, params=None):
+    """As SyncRequest, but asserts that no meaningful response was received.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Optional parameters to the request.
+    """
+    result = self.SyncRequest(method, params)
+    if 'error' in result or ('result' in result and
+                             result['result']):
+      raise DevToolsConnectionException(
+          'Unexpected response for %s: %s' % (method, result))
+
   def SetUpMonitoring(self):
     for domain in self._domains_to_enable:
       self._ws.RegisterDomain(domain, self._OnDataReceived)
-      self.SyncRequest('%s.enable' % domain)
+      if domain != self.TRACING_DOMAIN:
+        self.SyncRequestNoResponse('%s.enable' % domain)
+        # Tracing setup must be done by the tracing track to control filtering
+        # and output.
+    self._tearing_down_tracing = False
+    self._set_up = True
 
   def StartMonitoring(self):
     """Starts monitoring.
 
     DevToolsConnection.SetUpMonitoring() has to be called first.
     """
-    while not self._please_stop:
-      try:
-        self._ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException:
-        break
-    if not self._please_stop:
-      logging.warning('Monitoring stopped on a timeout.')
+    assert self._set_up, 'DevToolsConnection.SetUpMonitoring not called.'
+    self._Dispatch()
     self._TearDownMonitoring()
 
   def StopMonitoring(self):
     """Stops the monitoring."""
     self._please_stop = True
 
+  def _Dispatch(self, kind='Monitoring', timeout=10):
+    self._please_stop = False
+    while not self._please_stop:
+      try:
+        self._ws.DispatchNotifications(timeout=timeout)
+      except websocket.WebSocketTimeoutException:
+        break
+    if not self._please_stop:
+      logging.warning('%s stopped on a timeout.' % kind)
+
   def _TearDownMonitoring(self):
+    if self.TRACING_DOMAIN in self._domains_to_enable:
+      logging.info('Fetching tracing')
+      self.SyncRequestNoResponse(self.TRACING_END_METHOD)
+      self._tearing_down_tracing = True
+      self._Dispatch(kind='Tracing', timeout=self.TRACING_TIMEOUT)
     for domain in self._domains_to_enable:
-      self.SyncRequest('%s.disable' % domain)
+      if domain != self.TRACING_DOMAIN:
+        self.SyncRequest('%s.disable' % domain)
       self._ws.UnregisterDomain(domain)
     self._domains_to_enable.clear()
-    self._listeners.clear()
+    self._domain_listeners.clear()
+    self._event_listeners.clear()
 
   def _OnDataReceived(self, msg):
-    method = msg.get('method', None)
-    if method not in self._listeners:
+    if 'method' not in msg:
+      raise DevToolsConnectionException('Malformed message: %s' % msg)
+    method = msg['method']
+    domain = method[:method.index('.')]
+
+    if self._tearing_down_tracing and method == self.TRACING_STREAM_EVENT:
+      stream_handle = msg.get('params', {}).get('stream')
+      if not stream_handle:
+        self._tearing_down_tracing = False
+        self.StopMonitoring()
+        # Fall through to regular dispatching.
+      else:
+        _StreamReader(self._ws, stream_handle).Read(self._TracingStreamDone)
+        # Skip regular dispatching.
+        return
+
+    if (method not in self._event_listeners and
+        domain not in self._domain_listeners):
       return
-    self._listeners[method].Handle(method, msg)
+    if method in self._event_listeners:
+      self._event_listeners[method].Handle(method, msg)
+    if domain in self._domain_listeners:
+      self._domain_listeners[domain].Handle(method, msg)
+    if self._tearing_down_tracing and method == self.TRACING_DONE_EVENT:
+      self._tearing_down_tracing = False
+      self.StopMonitoring()
+
+  def _TracingStreamDone(self, data):
+    tracing_events = json.loads(data)
+    for evt in tracing_events:
+      self._OnDataReceived({'method': self.TRACING_DATA_METHOD,
+                            'params': {'value': [evt]}})
+      if self._please_stop:
+        break
+    self._tearing_down_tracing = False
+    self.StopMonitoring()
 
   @classmethod
   def _GetWebSocketUrl(cls, hostname, port):
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index 6e638c4..cb9aa7a 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -24,7 +24,7 @@ import devtools_monitor
 class PageTrack(devtools_monitor.Track):
   """Records the events from the page track."""
   def __init__(self, connection):
-    super(PageTrack, self).__init__()
+    super(PageTrack, self).__init__(connection)
     self._connection = connection
     self._events = []
     self._main_frame_id = None
@@ -67,9 +67,8 @@ class AndroidTraceRecorder(object):
     self.devtools_connection = None
     self.page_track = None
 
-  def Go(self):
-    self.devtools_connection = devtools_monitor.DevToolsConnection(
-        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
+  def Go(self, connection):
+    self.devtools_connection = connection
     self.page_track = PageTrack(self.devtools_connection)
 
     self.devtools_connection.SetUpMonitoring()
diff --git a/loading/trace_to_chrome_trace.py b/loading/trace_to_chrome_trace.py
new file mode 100755
index 0000000..998614f
--- /dev/null
+++ b/loading/trace_to_chrome_trace.py
@@ -0,0 +1,23 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Convert trace output for Chrome.
+
+Take the tracing track output from tracing_driver.py to a zip'd json that can be
+loading by chrome devtools tracing.
+"""
+
+import argparse
+import gzip
+import json
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument('input')
+  parser.add_argument('output')
+  args = parser.parse_args()
+  with gzip.GzipFile(args.output, 'w') as output_f, file(args.input) as input_f:
+    events = json.load(input_f)
+    json.dump({'traceEvents': events, 'metadata': {}}, output_f)
diff --git a/loading/tracing.py b/loading/tracing.py
new file mode 100644
index 0000000..83ad38c
--- /dev/null
+++ b/loading/tracing.py
@@ -0,0 +1,38 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Monitor tracing events on chrome via chrome remote debugging."""
+
+import devtools_monitor
+
+class TracingTrack(devtools_monitor.Track):
+  def __init__(self, connection, categories=None, fetch_stream=False):
+    """Initialize this TracingTrack.
+
+    Args:
+      connection: a DevToolsConnection.
+      categories: None, or a string, or list of strings, of tracing categories
+        to filter.
+
+      fetch_stream: if true, use a websocket stream to fetch tracing data rather
+        than dataCollected events. It appears based on very limited testing that
+        a stream is slower than the default reporting as dataCollected events.
+    """
+    super(TracingTrack, self).__init__(connection)
+    connection.RegisterListener('Tracing.dataCollected', self)
+    params = {}
+    if categories:
+      params['categories'] = (categories if type(categories) is str
+                              else ','.join(categories))
+    if fetch_stream:
+      params['transferMode'] = 'ReturnAsStream'
+
+    connection.SyncRequestNoResponse('Tracing.start', params)
+    self._events = []
+
+  def Handle(self, method, event):
+    self._events.append(event)
+
+  def GetEvents(self):
+    return self._events
diff --git a/loading/tracing_driver.py b/loading/tracing_driver.py
new file mode 100755
index 0000000..0996d34
--- /dev/null
+++ b/loading/tracing_driver.py
@@ -0,0 +1,49 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Drive TracingConnection"""
+
+import argparse
+import json
+import logging
+import os.path
+import sys
+
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
+
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import device_setup
+import trace_recorder
+import tracing
+
+
+def main():
+  logging.basicConfig(level=logging.INFO)
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--url', required=True)
+  parser.add_argument('--output', required=True)
+  args = parser.parse_args()
+  url = args.url
+  if not url.startswith('http'):
+    url = 'http://' + url
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  with file(args.output, 'w') as output, \
+       file(args.output + '.page', 'w') as page_output, \
+       device_setup.DeviceConnection(device) as connection:
+    track = tracing.TracingTrack(connection, fetch_stream=False)
+    page = trace_recorder.PageTrack(connection)
+    connection.SetUpMonitoring()
+    connection.SendAndIgnoreResponse('Page.navigate', {'url': url})
+    connection.StartMonitoring()
+    json.dump(page.GetEvents(), page_output, sort_keys=True, indent=2)
+    json.dump(track.GetEvents(), output, sort_keys=True, indent=2)
+
+
+if __name__ == '__main__':
+  main()

commit 2ca96ddd2444077f79ef67b5c89c6bb50fa0bbe0
Author: lizeb <lizeb@chromium.org>
Date:   Mon Jan 18 05:36:37 2016 -0800

    tools/android/loading: Add support for Serializing tracks to JSON.
    
    Review URL: https://codereview.chromium.org/1596293004
    
    Cr-Original-Commit-Position: refs/heads/master@{#370019}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: d5a762d45595f0725056e5bee6b77c52098a97ee

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index d4847cd..fc652d2 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -173,3 +173,25 @@ class Track(Listener):
   def GetEvents(self):
     """Returns a list of collected events, finalizing the state if necessary."""
     pass
+
+  def ToJsonDict(self):
+    """Serializes to a dictionary, to be dumped as JSON.
+
+    Returns:
+      A dict that can be dumped by the json module, and loaded by
+      FromJsonDict().
+    """
+    pass
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    """Returns a Track instance constructed from data dumped by
+       Track.ToJsonDict().
+
+    Args:
+      json_data: (dict) Parsed from a JSON file using the json module.
+
+    Returns:
+      a Track instance.
+    """
+    pass
diff --git a/loading/request_track.py b/loading/request_track.py
index 995c574..de3fd11 100644
--- a/loading/request_track.py
+++ b/loading/request_track.py
@@ -94,16 +94,20 @@ class Request(object):
     request_time = self.timing.request_time
     return (timestamp - request_time) * 1000
 
-  def ToDict(self):
+  def ToJsonDict(self):
     return copy.deepcopy(self.__dict__)
 
   @classmethod
-  def FromDict(cls, data_dict):
+  def FromJsonDict(cls, data_dict):
     result = Request()
     for (k, v) in data_dict.items():
       setattr(result, k, v)
     return result
 
+  # For testing.
+  def __eq__(self, o):
+    return self.__dict__ == o.__dict__
+
 
 class RequestTrack(devtools_monitor.Track):
   """Aggregates request data."""
@@ -135,6 +139,20 @@ class RequestTrack(devtools_monitor.Track):
                       % len(self._requests_in_flight))
     return self._requests
 
+  def ToJsonDict(self):
+    if self._requests_in_flight:
+      logging.warning('Requests in flight, will be ignored in the dump')
+    return {'events': [request.ToJsonDict() for request in self._requests]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    result = RequestTrack(None)
+    requests = [Request.FromJsonDict(request)
+                for request in json_dict['events']]
+    result._requests = requests
+    return result
+
   def _RequestWillBeSent(self, request_id, params):
     # Several "requestWillBeSent" events can be dispatched in a row in the case
     # of redirects.
@@ -239,6 +257,9 @@ class RequestTrack(devtools_monitor.Track):
     self._completed_requests_by_id[request_id] = request
     self._requests.append(request)
 
+  def __eq__(self, o):
+    return self._requests == o._requests
+
 
 RequestTrack._METHOD_TO_HANDLER = {
     'Network.requestWillBeSent': RequestTrack._RequestWillBeSent,
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
index 5a25b81..459e2de 100644
--- a/loading/request_track_unittest.py
+++ b/loading/request_track_unittest.py
@@ -2,6 +2,7 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
+import json
 import unittest
 
 from request_track import (Request, RequestTrack, _TimingFromDict)
@@ -242,6 +243,17 @@ class RequestTrackTestCase(unittest.TestCase):
         RequestTrackTestCase._DATA_RECEIVED_2['params']['encodedDataLength'],
         r.data_chunks[1][1])
 
+  def testCanSerialize(self):
+    self._ValidSequence(self.request_track)
+    json_dict = self.request_track.ToJsonDict()
+    _ = json.dumps(json_dict)  # Should not raise an exception.
+
+  def testCanDeserialize(self):
+    self._ValidSequence(self.request_track)
+    json_dict = self.request_track.ToJsonDict()
+    request_track = RequestTrack.FromJsonDict(json_dict)
+    self.assertEquals(self.request_track, request_track)
+
   @classmethod
   def _ValidSequence(cls, request_track):
     request_track.Handle(
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index a3f289b..6e638c4 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -28,8 +28,9 @@ class PageTrack(devtools_monitor.Track):
     self._connection = connection
     self._events = []
     self._main_frame_id = None
-    self._connection.RegisterListener('Page.frameStartedLoading', self)
-    self._connection.RegisterListener('Page.frameStoppedLoading', self)
+    if self._connection:
+      self._connection.RegisterListener('Page.frameStartedLoading', self)
+      self._connection.RegisterListener('Page.frameStoppedLoading', self)
 
   def Handle(self, method, msg):
     params = msg['params']
@@ -47,6 +48,17 @@ class PageTrack(devtools_monitor.Track):
   def GetEvents(self):
     return self._events
 
+  def ToJsonDict(self):
+    return {'events': [event for event in self._events]}
+
+  @classmethod
+  def FromJsonDict(cls, json_dict):
+    assert 'events' in json_dict
+    result = PageTrack(None)
+    events = [event for event in json_dict['events']]
+    result._events = events
+    return result
+
 
 class AndroidTraceRecorder(object):
   """Records a loading trace."""

commit 1f62c12cc98cd98d0178be1e3a56d5897cfae7a5
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jan 15 02:36:58 2016 -0800

    tools/android/loading: Implement RequestTrack.
    
    Review URL: https://codereview.chromium.org/1582023002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369718}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 278b5aa86d9acb9ef99fc6cca755658b051dcf82

diff --git a/loading/request_track.py b/loading/request_track.py
new file mode 100644
index 0000000..995c574
--- /dev/null
+++ b/loading/request_track.py
@@ -0,0 +1,272 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""The request data track.
+
+When executed, parses a JSON dump of DevTools messages.
+"""
+
+import collections
+import copy
+import logging
+
+import devtools_monitor
+
+
+_TIMING_NAMES_MAPPING = {
+    'connectEnd': 'connect_end', 'connectStart': 'connect_start',
+    'dnsEnd': 'dns_end', 'dnsStart': 'dns_start', 'proxyEnd': 'proxy_end',
+    'proxyStart': 'proxy_start', 'receiveHeadersEnd': 'receive_headers_end',
+    'requestTime': 'request_time', 'sendEnd': 'send_end',
+    'sendStart': 'send_start', 'sslEnd': 'ssl_end', 'sslStart': 'ssl_start',
+    'workerReady': 'worker_ready', 'workerStart': 'worker_start',
+    'loadingFinished': 'loading_finished'}
+
+Timing = collections.namedtuple('Timing', _TIMING_NAMES_MAPPING.values())
+
+
+class Request(object):
+  """Represents a single request.
+
+  Generally speaking, fields here closely mirror those documented in
+  third_party/WebKit/Source/devtools/protocol.json.
+
+  Fields:
+    request_id: (str) unique request ID. Postfixed with ".redirect" for
+                redirects.
+    frame_id: (str) unique frame identifier.
+    loader_id: (str) unique frame identifier.
+    document_url: (str) URL of the document this request is loaded for.
+    url: (str) Request URL.
+    protocol: (str) protocol used for the request.
+    method: (str) HTTP method, such as POST or GET.
+    request_headers: (dict) {'header': 'value'} Request headers.
+    response_headers: (dict) {'header': 'value'} Response headers.
+    initial_priority: (str) Initial request priority, in REQUEST_PRIORITIES.
+    timestamp: (float) Request timestamp, in s.
+    wall_time: (float) Request timestamp, UTC timestamp in s.
+    initiator: (dict) Request initiator, in INITIATORS.
+    resource_type: (str) Resource type, in RESOURCE_TYPES
+    served_from_cache: (bool) Whether the request was served from cache.
+    from_disk_cache: (bool) Whether the request was served from the disk cache.
+    from_service_worker: (bool) Whether the request was served by a Service
+                         Worker.
+    timing: (Timing) Request timing, extended with loading_finished.
+    status: (int) Response status code.
+    encoded_data_length: (int) Total encoded data length.
+    data_chunks: (list) [(offset, encoded_data_length), ...] List of data
+                 chunks received, with their offset in ms relative to
+                 Timing.requestTime.
+    failed: (bool) Whether the request failed.
+  """
+  REQUEST_PRIORITIES = ('VeryLow', 'Low', 'Medium', 'High', 'VeryHigh')
+  RESOURCE_TYPES = ('Document', 'Stylesheet', 'Image', 'Media', 'Font',
+                    'Script', 'TextTrack', 'XHR', 'Fetch', 'EventSource',
+                    'WebSocket', 'Manifest', 'Other')
+  INITIATORS = ('parser', 'script', 'other')
+  def __init__(self):
+    self.request_id = None
+    self.frame_id = None
+    self.loader_id = None
+    self.document_url = None
+    self.url = None
+    self.protocol = None
+    self.method = None
+    self.request_headers = None
+    self.response_headers = None
+    self.initial_priority = None
+    self.timestamp = -1
+    self.wall_time = -1
+    self.initiator = None
+    self.resource_type = None
+    self.served_from_cache = False
+    self.from_disk_cache = False
+    self.from_service_worker = False
+    self.timing = None
+    self.status = None
+    self.encoded_data_length = 0
+    self.data_chunks = []
+    self.failed = False
+
+  def _TimestampOffsetFromStartMs(self, timestamp):
+    assert self.timing.request_time != -1
+    request_time = self.timing.request_time
+    return (timestamp - request_time) * 1000
+
+  def ToDict(self):
+    return copy.deepcopy(self.__dict__)
+
+  @classmethod
+  def FromDict(cls, data_dict):
+    result = Request()
+    for (k, v) in data_dict.items():
+      setattr(result, k, v)
+    return result
+
+
+class RequestTrack(devtools_monitor.Track):
+  """Aggregates request data."""
+  # Request status
+  _STATUS_SENT = 0
+  _STATUS_RESPONSE = 1
+  _STATUS_DATA = 2
+  _STATUS_FINISHED = 3
+  _STATUS_FAILED = 4
+  def __init__(self, connection):
+    super(RequestTrack, self).__init__(connection)
+    self._connection = connection
+    self._requests = []
+    self._requests_in_flight = {}  # requestId -> (request, status)
+    self._completed_requests_by_id = {}
+    if connection:  # Optional for testing.
+      for method in RequestTrack._METHOD_TO_HANDLER:
+        self._connection.RegisterListener(method, self)
+
+  def Handle(self, method, msg):
+    assert method in RequestTrack._METHOD_TO_HANDLER
+    params = msg['params']
+    request_id = params['requestId']
+    RequestTrack._METHOD_TO_HANDLER[method](self, request_id, params)
+
+  def GetEvents(self):
+    if self._requests_in_flight:
+      logging.warning('Number of requests still in flight: %d.'
+                      % len(self._requests_in_flight))
+    return self._requests
+
+  def _RequestWillBeSent(self, request_id, params):
+    # Several "requestWillBeSent" events can be dispatched in a row in the case
+    # of redirects.
+    if request_id in self._requests_in_flight:
+      self._HandleRedirect(request_id, params)
+    assert (request_id not in self._requests_in_flight
+            and request_id not in self._completed_requests_by_id)
+    r = Request()
+    r.request_id = request_id
+    _CopyFromDictToObject(
+        params, r, (('frameId', 'frame_id'), ('loaderId', 'loader_id'),
+                    ('documentURL', 'document_url'),
+                    ('timestamp', 'timestamp'), ('wallTime', 'wall_time'),
+                    ('initiator', 'initiator')))
+    request = params['request']
+    _CopyFromDictToObject(
+        request, r, (('url', 'url'), ('method', 'method'),
+                     ('headers', 'headers'),
+                     ('initialPriority', 'initial_priority')))
+    r.resource_type = params.get('type', 'Other')
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_SENT)
+
+  def _HandleRedirect(self, request_id, params):
+    (r, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_SENT
+    # The second request contains timing information pertaining to the first
+    # one. Finalize the first request.
+    assert 'redirectResponse' in params
+    redirect_response = params['redirectResponse']
+    _CopyFromDictToObject(redirect_response, r,
+                          (('headers', 'response_headers'),
+                           ('encodedDataLength', 'encoded_data_length'),
+                           ('fromDiskCache', 'from_disk_cache')))
+    r.timing = _TimingFromDict(redirect_response['timing'])
+    r.request_id = request_id + '.redirect'
+    self._requests_in_flight[r.request_id] = (r, RequestTrack._STATUS_FINISHED)
+    del self._requests_in_flight[request_id]
+    self._FinalizeRequest(r.request_id)
+
+  def _RequestServedFromCache(self, request_id, _):
+    assert request_id in self._requests_in_flight
+    (request, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_SENT
+    request.served_from_cache = True
+
+  def _ResponseReceived(self, request_id, params):
+    assert request_id in self._requests_in_flight
+    (r, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_SENT
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
+    assert r.frame_id == params['frameId']
+    assert r.timestamp <= params['timestamp']
+    if r.resource_type == 'Other':
+      r.resource_type = params.get('type', 'Other')
+    else:
+      assert r.resource_type == params.get('type', 'Other')
+    response = params['response']
+    _CopyFromDictToObject(
+        response, r, (('status', 'status'), ('mimeType', 'mime_type'),
+                      ('fromDiskCache', 'from_disk_cache'),
+                      ('fromServiceWorker', 'from_service_worker'),
+                      ('protocol', 'protocol'),
+                      # Actual request headers are not known before reaching the
+                      # network stack.
+                      ('requestHeaders', 'request_headers'),
+                      ('headers', 'response_headers')))
+    timing_dict = response['timing'] if r.protocol != 'data' else {}
+    r.timing = _TimingFromDict(timing_dict)
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_RESPONSE)
+
+  def _DataReceived(self, request_id, params):
+    (r, status) = self._requests_in_flight[request_id]
+    assert (status == RequestTrack._STATUS_RESPONSE
+            or status == RequestTrack._STATUS_DATA)
+    offset = r._TimestampOffsetFromStartMs(params['timestamp'])
+    r.data_chunks.append((offset, params['encodedDataLength']))
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_DATA)
+
+  def _LoadingFinished(self, request_id, params):
+    assert request_id in self._requests_in_flight
+    (r, status) = self._requests_in_flight[request_id]
+    assert (status == RequestTrack._STATUS_RESPONSE
+            or status == RequestTrack._STATUS_DATA)
+    r.encoded_data_length = params['encodedDataLength']
+    r.timing = r.timing._replace(
+        loading_finished=r._TimestampOffsetFromStartMs(params['timestamp']))
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
+    self._FinalizeRequest(request_id)
+
+  def _LoadingFailed(self, request_id, _):
+    assert request_id in self._requests_in_flight
+    (r, _) = self._requests_in_flight[request_id]
+    r.failed = True
+    self._requests_in_flight[request_id] = (r, RequestTrack._STATUS_FINISHED)
+    self._FinalizeRequest(request_id)
+
+  def _FinalizeRequest(self, request_id):
+    assert request_id in self._requests_in_flight
+    (request, status) = self._requests_in_flight[request_id]
+    assert status == RequestTrack._STATUS_FINISHED
+    del self._requests_in_flight[request_id]
+    self._completed_requests_by_id[request_id] = request
+    self._requests.append(request)
+
+
+RequestTrack._METHOD_TO_HANDLER = {
+    'Network.requestWillBeSent': RequestTrack._RequestWillBeSent,
+    'Network.requestServedFromCache': RequestTrack._RequestServedFromCache,
+    'Network.responseReceived': RequestTrack._ResponseReceived,
+    'Network.dataReceived': RequestTrack._DataReceived,
+    'Network.loadingFinished': RequestTrack._LoadingFinished,
+    'Network.loadingFailed': RequestTrack._LoadingFailed}
+
+
+def _TimingFromDict(timing_dict):
+  complete_timing_dict = {field: -1 for field in Timing._fields}
+  timing_dict_mapped = {
+      _TIMING_NAMES_MAPPING[k]: v for (k, v) in timing_dict.items()}
+  complete_timing_dict.update(timing_dict_mapped)
+  return Timing(**complete_timing_dict)
+
+
+def _CopyFromDictToObject(d, o, key_attrs):
+  for (key, attr) in key_attrs:
+    setattr(o, attr, d[key])
+
+
+if __name__ == '__main__':
+  import json
+  import sys
+  events = json.load(open(sys.argv[1], 'r'))
+  request_track = RequestTrack(None)
+  for event in events:
+    event_method = event['method']
+    request_track.Handle(event_method, event)
diff --git a/loading/request_track_unittest.py b/loading/request_track_unittest.py
new file mode 100644
index 0000000..5a25b81
--- /dev/null
+++ b/loading/request_track_unittest.py
@@ -0,0 +1,255 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import unittest
+
+from request_track import (Request, RequestTrack, _TimingFromDict)
+
+
+class RequestTrackTestCase(unittest.TestCase):
+  _REQUEST_WILL_BE_SENT = {
+      'method': 'Network.requestWillBeSent',
+      'params': {
+          'documentURL': 'http://example.com/',
+          'frameId': '32493.1',
+          'initiator': {
+              'type': 'other'
+              },
+          'loaderId': '32493.3',
+          'request': {
+              'headers': {
+                  'Accept': 'text/html',
+                  'Upgrade-Insecure-Requests': '1',
+                  'User-Agent': 'Mozilla/5.0'
+                  },
+              'initialPriority': 'VeryHigh',
+              'method': 'GET',
+              'mixedContentType': 'none',
+              'url': 'http://example.com/'
+              },
+          'requestId': '32493.1',
+          'timestamp': 5571441.535053,
+          'type': 'Document',
+          'wallTime': 1452691674.08878}}
+  _REDIRECT = {
+      'method': 'Network.requestWillBeSent',
+      'params': {
+          'documentURL': 'http://www.example.com/',
+          'frameId': '32493.1',
+          'initiator': {
+              'type': 'other'
+              },
+          'loaderId': '32493.3',
+          'redirectResponse': {
+              'connectionId': 18,
+              'connectionReused': False,
+              'encodedDataLength': 198,
+              'fromDiskCache': False,
+              'fromServiceWorker': False,
+              'headers': {},
+              'headersText': 'HTTP/1.1 301 Moved Permanently\r\n',
+              'mimeType': 'text/html',
+              'protocol': 'http/1.1',
+              'remoteIPAddress': '216.146.46.10',
+              'remotePort': 80,
+              'requestHeaders': {
+                  'Accept': 'text/html',
+                  'User-Agent': 'Mozilla/5.0'
+                  },
+              'securityState': 'neutral',
+              'status': 301,
+              'statusText': 'Moved Permanently',
+              'timing': {
+                  'connectEnd': 137.435999698937,
+                  'connectStart': 51.1459996923804,
+                  'dnsEnd': 51.1459996923804,
+                  'dnsStart': 0,
+                  'proxyEnd': -1,
+                  'proxyStart': -1,
+                  'receiveHeadersEnd': 228.187000378966,
+                  'requestTime': 5571441.55002,
+                  'sendEnd': 138.841999694705,
+                  'sendStart': 138.031999580562,
+                  'sslEnd': -1,
+                  'sslStart': -1,
+                  'workerReady': -1,
+                  'workerStart': -1
+                  },
+              'url': 'http://example.com/'
+              },
+          'request': {
+              'headers': {
+                  'Accept': 'text/html',
+                  'User-Agent': 'Mozilla/5.0'
+                  },
+              'initialPriority': 'VeryLow',
+              'method': 'GET',
+              'mixedContentType': 'none',
+              'url': 'http://www.example.com/'
+              },
+          'requestId': '32493.1',
+          'timestamp': 5571441.795948,
+          'type': 'Document',
+          'wallTime': 1452691674.34968}}
+  _RESPONSE_RECEIVED = {
+      'method': 'Network.responseReceived',
+      'params': {
+          'frameId': '32493.1',
+          'loaderId': '32493.3',
+          'requestId': '32493.1',
+          'response': {
+              'connectionId': 26,
+              'connectionReused': False,
+              'encodedDataLength': -1,
+              'fromDiskCache': False,
+              'fromServiceWorker': False,
+              'headers': {
+                  'Age': '67',
+                  'Cache-Control': 'max-age=0,must-revalidate',
+                  },
+              'headersText': 'HTTP/1.1 200 OK\r\n',
+              'mimeType': 'text/html',
+              'protocol': 'http/1.1',
+              'requestHeaders': {
+                  'Accept': 'text/html',
+                    'Host': 'www.example.com',
+                    'User-Agent': 'Mozilla/5.0'
+                },
+                'status': 200,
+                'timing': {
+                    'connectEnd': 37.9800004884601,
+                    'connectStart': 26.8250005319715,
+                    'dnsEnd': 26.8250005319715,
+                    'dnsStart': 0,
+                    'proxyEnd': -1,
+                    'proxyStart': -1,
+                    'receiveHeadersEnd': 54.9750002101064,
+                    'requestTime': 5571441.798671,
+                    'sendEnd': 38.3980004116893,
+                    'sendStart': 38.1810003891587,
+                    'sslEnd': -1,
+                    'sslStart': -1,
+                    'workerReady': -1,
+                    'workerStart': -1
+                },
+                'url': 'http://www.example.com/'
+            },
+            'timestamp': 5571441.865639,
+            'type': 'Document'}}
+  _DATA_RECEIVED_1 = {
+      "method": "Network.dataReceived",
+      "params": {
+          "dataLength": 1803,
+          "encodedDataLength": 1326,
+          "requestId": "32493.1",
+          "timestamp": 5571441.867347}}
+  _DATA_RECEIVED_2 = {
+      "method": "Network.dataReceived",
+      "params": {
+          "dataLength": 32768,
+          "encodedDataLength": 32768,
+          "requestId": "32493.1",
+          "timestamp": 5571441.893121}}
+  _LOADING_FINISHED = {'method': 'Network.loadingFinished',
+                       'params': {
+                           'encodedDataLength': 101829,
+                           'requestId': '32493.1',
+                           'timestamp': 5571441.891189}}
+
+  def setUp(self):
+    self.request_track = RequestTrack(None)
+
+  def testParseRequestWillBeSent(self):
+    msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    request_id = msg['params']['requestId']
+    self.request_track.Handle('Network.requestWillBeSent', msg)
+    self.assertTrue(request_id in self.request_track._requests_in_flight)
+    (_, status) = self.request_track._requests_in_flight[request_id]
+    self.assertEquals(RequestTrack._STATUS_SENT, status)
+
+  def testRejectsUnknownMethod(self):
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle(
+          'unknown', RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+
+  def testHandleRedirect(self):
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REQUEST_WILL_BE_SENT)
+    self.request_track.Handle('Network.requestWillBeSent',
+                              RequestTrackTestCase._REDIRECT)
+    self.assertEquals(1, len(self.request_track._requests_in_flight))
+    self.assertEquals(1, len(self.request_track.GetEvents()))
+
+  def testRejectDuplicates(self):
+    msg = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    self.request_track.Handle('Network.requestWillBeSent', msg)
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle('Network.requestWillBeSent', msg)
+
+  def testInvalidSequence(self):
+    msg1 = RequestTrackTestCase._REQUEST_WILL_BE_SENT
+    msg2 = RequestTrackTestCase._LOADING_FINISHED
+    self.request_track.Handle('Network.requestWillBeSent', msg1)
+    with self.assertRaises(AssertionError):
+      self.request_track.Handle('Network.loadingFinished', msg2)
+
+  def testValidSequence(self):
+    self._ValidSequence(self.request_track)
+    self.assertEquals(1, len(self.request_track.GetEvents()))
+    self.assertEquals(0, len(self.request_track._requests_in_flight))
+    r = self.request_track.GetEvents()[0]
+    self.assertEquals('32493.1', r.request_id)
+    self.assertEquals('32493.1', r.frame_id)
+    self.assertEquals('32493.3', r.loader_id)
+    self.assertEquals('http://example.com/', r.document_url)
+    self.assertEquals('http://example.com/', r.url)
+    self.assertEquals('http/1.1', r.protocol)
+    self.assertEquals('GET', r.method)
+    response = RequestTrackTestCase._RESPONSE_RECEIVED['params']['response']
+    self.assertEquals(response['requestHeaders'], r.request_headers)
+    self.assertEquals(response['headers'], r.response_headers)
+    self.assertEquals('VeryHigh', r.initial_priority)
+    request_will_be_sent = (
+        RequestTrackTestCase._REQUEST_WILL_BE_SENT['params'])
+    self.assertEquals(request_will_be_sent['timestamp'], r.timestamp)
+    self.assertEquals(request_will_be_sent['wallTime'], r.wall_time)
+    self.assertEquals(request_will_be_sent['initiator'], r.initiator)
+    self.assertEquals(request_will_be_sent['type'], r.resource_type)
+    self.assertEquals(False, r.served_from_cache)
+    self.assertEquals(False, r.from_disk_cache)
+    self.assertEquals(False, r.from_service_worker)
+    timing = _TimingFromDict(response['timing'])
+    loading_finished = RequestTrackTestCase._LOADING_FINISHED['params']
+    loading_finished_offset = r._TimestampOffsetFromStartMs(
+        loading_finished['timestamp'])
+    timing = timing._replace(loading_finished=loading_finished_offset)
+    self.assertEquals(timing, r.timing)
+    self.assertEquals(200, r.status)
+    self.assertEquals(
+        loading_finished['encodedDataLength'], r.encoded_data_length)
+    self.assertEquals(False, r.failed)
+
+  def testDataReceived(self):
+    self._ValidSequence(self.request_track)
+    self.assertEquals(1, len(self.request_track.GetEvents()))
+    r = self.request_track.GetEvents()[0]
+    self.assertEquals(2, len(r.data_chunks))
+    self.assertEquals(
+        RequestTrackTestCase._DATA_RECEIVED_1['params']['encodedDataLength'],
+        r.data_chunks[0][1])
+    self.assertEquals(
+        RequestTrackTestCase._DATA_RECEIVED_2['params']['encodedDataLength'],
+        r.data_chunks[1][1])
+
+  @classmethod
+  def _ValidSequence(cls, request_track):
+    request_track.Handle(
+        'Network.requestWillBeSent', cls._REQUEST_WILL_BE_SENT)
+    request_track.Handle('Network.responseReceived', cls._RESPONSE_RECEIVED)
+    request_track.Handle('Network.dataReceived', cls._DATA_RECEIVED_1)
+    request_track.Handle('Network.dataReceived', cls._DATA_RECEIVED_2)
+    request_track.Handle('Network.loadingFinished', cls._LOADING_FINISHED)
+
+if __name__ == '__main__':
+  unittest.main()

commit 5e74beb3ab48ee8701496d1febc33f8e27818ab1
Author: jbudorick <jbudorick@chromium.org>
Date:   Thu Jan 14 10:14:34 2016 -0800

    [Android] Port tools/android from pylib to catapult+devil.
    
    BUG=476719
    
    Review URL: https://codereview.chromium.org/1584963002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369467}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 636bdd225528b28b4605bc704e366a3d9ef08d22

diff --git a/appstats.py b/appstats.py
index ba53eac..8bb93a5 100755
--- a/appstats.py
+++ b/appstats.py
@@ -14,13 +14,15 @@ import time
 
 from operator import sub
 
-sys.path.append(os.path.join(os.path.dirname(__file__),
-                             os.pardir,
-                             os.pardir,
-                             'build',
-                             'android'))
-from pylib.device import device_errors
-from pylib.device import device_utils
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..'))
+
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil'))
+from devil.android import device_errors
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 
 class Utils(object):
   """A helper class to hold various utility methods."""
@@ -944,6 +946,8 @@ def main(argv):
   if not args.show_net and not args.show_mem:
     args.show_mem = True
 
+  devil_chromium.Initialize()
+
   curses.setupterm()
 
   printer = OutputBeautifier(not args.dull_output, not args.no_overwrite)
diff --git a/customtabs_benchmark/scripts/customtabs_benchmark.py b/customtabs_benchmark/scripts/customtabs_benchmark.py
index 9bf4343..56f48bc 100755
--- a/customtabs_benchmark/scripts/customtabs_benchmark.py
+++ b/customtabs_benchmark/scripts/customtabs_benchmark.py
@@ -13,13 +13,17 @@ import re
 import sys
 import time
 
-sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
-                             os.pardir, os.pardir, 'build', 'android'))
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..'))
 
-from pylib.device import device_errors
-from pylib.device import device_utils
-from pylib.device import intent
-from pylib.perf import cache_control
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil'))
+from devil.android import device_errors
+from devil.android import device_utils
+from devil.android.perf import cache_control
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 
 
 def RunOnce(device, url, warmup, no_prerendering, delay_to_may_launch_url,
@@ -150,6 +154,7 @@ def _CreateOptionParser():
 def main():
   parser = _CreateOptionParser()
   options, _ = parser.parse_args()
+  devil_chromium.Initialize()
   devices = device_utils.DeviceUtils.HealthyDevices()
   device = devices[0]
   if len(devices) != 1 and options.device is None:
diff --git a/customtabs_benchmark/scripts/run_benchmark.py b/customtabs_benchmark/scripts/run_benchmark.py
index 5df547b..8a3b88d 100755
--- a/customtabs_benchmark/scripts/run_benchmark.py
+++ b/customtabs_benchmark/scripts/run_benchmark.py
@@ -15,12 +15,16 @@ import random
 import sys
 import threading
 
-sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
-                             os.pardir, os.pardir, 'build', 'android'))
+import customtabs_benchmark
 
-from pylib.device import device_utils
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..', '..'))
 
-import customtabs_benchmark
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 
 
 _KEYS = ['url', 'warmup', 'no_prerendering', 'delay_to_may_launch_url',
@@ -132,6 +136,7 @@ def main():
   if options.config is None:
     logging.error('A configuration file must be provided.')
     sys.exit(0)
+  devil_chromium.Initialize()
   configs = _ParseConfiguration(options.config)
   _Run(options.output_file_prefix, configs)
 
diff --git a/mempressure.py b/mempressure.py
index f316790..7d67d9d 100755
--- a/mempressure.py
+++ b/mempressure.py
@@ -9,17 +9,18 @@ import optparse
 import os
 import sys
 
-BUILD_ANDROID_DIR = os.path.join(os.path.dirname(__file__),
-                                 os.pardir,
-                                 os.pardir,
-                                 'build',
-                                 'android')
-sys.path.append(BUILD_ANDROID_DIR)
+_SRC_PATH = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..'))
+
+sys.path.append(os.path.join(_SRC_PATH, 'third_party', 'catapult', 'devil')
+from devil.android import device_errors
+from devil.android import device_utils
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_PATH, 'build', 'android'))
+import devil_chromium
 from pylib import constants
 from pylib import flag_changer
-from pylib.device import device_errors
-from pylib.device import device_utils
-from pylib.device import intent
 
 # Browser Constants
 DEFAULT_BROWSER = 'chrome'
@@ -82,6 +83,8 @@ def main(argv):
   if not options.browser in constants.PACKAGE_INFO.keys():
     option_parser.error('Unknown browser option ' + options.browser)
 
+  devil_chromium.Initialize()
+
   package_info = constants.PACKAGE_INFO[options.browser]
 
   package = package_info.package

commit 74844a254d5dfefeddff54bc02c91cd54bff3fd5
Author: ruuda <ruuda@google.com>
Date:   Thu Jan 14 04:06:11 2016 -0800

    [Android] Fix sign of integer literal in test
    
    BUG=577575
    TBR=digit@chromium.org
    
    Review URL: https://codereview.chromium.org/1586033002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369394}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 373cde59b9b483db9e897871b7a4d332dce76920

diff --git a/heap_profiler/heap_profiler_unittest.cc b/heap_profiler/heap_profiler_unittest.cc
index 7b9aedd..893f214 100644
--- a/heap_profiler/heap_profiler_unittest.cc
+++ b/heap_profiler/heap_profiler_unittest.cc
@@ -434,8 +434,8 @@ TEST_F(HeapProfilerTest, Test64Bit) {
       (void*)0x7ffffffffffff000L, 4096, st2.frames, st2.depth, 0);
   heap_profiler_alloc(
       (void*)0xfffffffffffff000L, 4096, st3.frames, st3.depth, 0);
-  EXPECT_EQ(3, stats_.num_allocs);
-  EXPECT_EQ(3, stats_.num_stack_traces);
+  EXPECT_EQ(3u, stats_.num_allocs);
+  EXPECT_EQ(3u, stats_.num_stack_traces);
   EXPECT_EQ(4096u + 4096 + 4096, stats_.total_alloc_bytes);
 
   heap_profiler_free((void*)0x1000, 4096, NULL);

commit 5c7ace1a3db24742a3e9bf808692264d9d39ca02
Author: jbudorick <jbudorick@chromium.org>
Date:   Wed Jan 13 09:54:18 2016 -0800

    [Android] pylib -> devil in tools/android/loading/.
    
    BUG=476719
    
    Review URL: https://codereview.chromium.org/1582573004
    
    Cr-Original-Commit-Position: refs/heads/master@{#369211}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: c778474f46615814646078e12715116d866484c4

diff --git a/loading/analyze.py b/loading/analyze.py
index 1a49155..3bb5890 100755
--- a/loading/analyze.py
+++ b/loading/analyze.py
@@ -13,12 +13,16 @@ import sys
 import tempfile
 import time
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
 from pylib import constants
-from pylib.device import device_utils
-from pylib.device import intent
 
 import log_parser
 import log_requests
@@ -37,7 +41,7 @@ def _SetupAndGetDevice():
   """Gets an android device, set up the way we like it.
 
   Returns:
-    An AdbWrapper for the first device found.
+    An instance of DeviceUtils for the first device found.
   """
   device = device_utils.DeviceUtils.HealthyDevices()[0]
   device.EnableRoot()
@@ -354,6 +358,7 @@ def main():
   parser.add_argument('command')
   parser.add_argument('rest', nargs=argparse.REMAINDER)
   args = parser.parse_args()
+  devil_chromium.Initialize()
   COMMAND_MAP.get(args.command,
                   lambda _: InvalidCommand(args.command))(args.rest)
 
diff --git a/loading/device_setup.py b/loading/device_setup.py
index d98014c..24becb7 100644
--- a/loading/device_setup.py
+++ b/loading/device_setup.py
@@ -7,13 +7,15 @@ import os
 import sys
 import time
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android.sdk import intent
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
 from pylib import constants
 from pylib import flag_changer
-from pylib.device import device_utils
-from pylib.device import intent
 
 DEVTOOLS_PORT = 9222
 DEVTOOLS_HOSTNAME = 'localhost'
diff --git a/loading/log_requests.py b/loading/log_requests.py
index ecbb0e8..ee5f091 100755
--- a/loading/log_requests.py
+++ b/loading/log_requests.py
@@ -15,16 +15,22 @@ import optparse
 import os
 import sys
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
-sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
-sys.path.append(os.path.join(file_dir, '..', '..', 'chrome_proxy'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
-from pylib.device import device_utils
-from common import inspector_network
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
+
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'telemetry'))
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
+sys.path.append(os.path.join(_SRC_DIR, 'tools', 'chrome_proxy'))
+from common import inspector_network
+
 import device_setup
 
 
@@ -210,11 +216,15 @@ def main():
   logging.basicConfig(level=logging.WARNING)
   parser = _CreateOptionParser()
   options, _ = parser.parse_args()
+
+  devil_chromium.Initialize()
+
   if options.local:
     device = None
   else:
     devices = device_utils.DeviceUtils.HealthyDevices()
     device = devices[0]
+
   request_logger = AndroidRequestsLogger(device)
   response_data = request_logger.LogPageLoad(
       options.url, options.clear_cache, options.package)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index f5294ce..a3f289b 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -8,10 +8,14 @@
 import os
 import sys
 
-file_dir = os.path.dirname(__file__)
-sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+_SRC_DIR = os.path.abspath(os.path.join(
+    os.path.dirname(__file__), '..', '..', '..'))
 
-from pylib.device import device_utils
+sys.path.append(os.path.join(_SRC_DIR, 'third_party', 'catapult', 'devil'))
+from devil.android import device_utils
+
+sys.path.append(os.path.join(_SRC_DIR, 'build', 'android'))
+import devil_chromium
 
 import device_setup
 import devtools_monitor
@@ -64,6 +68,7 @@ class AndroidTraceRecorder(object):
 
 
 def DoIt(url):
+  devil_chromium.Initialize()
   devices = device_utils.DeviceUtils.HealthyDevices()
   device = devices[0]
   trace_recorder = AndroidTraceRecorder(url)

commit 05ca4e0b1209b4d6d5be17179348c4666ab2eab0
Author: lizeb <lizeb@chromium.org>
Date:   Wed Jan 13 03:09:25 2016 -0800

    tools/android/loading: add PRESUBMIT.py.
    
    Fix pylint errors along the way.
    
    Review URL: https://codereview.chromium.org/1581913002
    
    Cr-Original-Commit-Position: refs/heads/master@{#369142}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: b56eb89d5185ac49a7721cb5d4fcd99abeb766b3

diff --git a/loading/PRESUBMIT.py b/loading/PRESUBMIT.py
new file mode 100644
index 0000000..b9bcc19
--- /dev/null
+++ b/loading/PRESUBMIT.py
@@ -0,0 +1,34 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Top-level presubmit script for loading.
+
+See http://dev.chromium.org/developers/how-tos/depottools/presubmit-scripts
+for more details on the presubmit API built into depot_tools.
+"""
+
+
+def CommonChecks(input_api, output_api):
+  output = []
+  blacklist = []
+  output.extend(input_api.canned_checks.RunPylint(
+      input_api, output_api, black_list=blacklist))
+  output.extend(input_api.canned_checks.RunUnitTests(
+      input_api,
+      output_api,
+      [input_api.os_path.join(input_api.PresubmitLocalPath(), 'run_tests')]))
+
+  if input_api.is_committing:
+    output.extend(input_api.canned_checks.PanProjectChecks(input_api,
+                                                           output_api,
+                                                           owners_check=False))
+  return output
+
+
+def CheckChangeOnUpload(input_api, output_api):
+  return CommonChecks(input_api, output_api)
+
+
+def CheckChangeOnCommit(input_api, output_api):
+  return CommonChecks(input_api, output_api)
diff --git a/loading/dag.py b/loading/dag.py
index d9f1a47..2eacd3a 100644
--- a/loading/dag.py
+++ b/loading/dag.py
@@ -96,10 +96,7 @@ def TopologicalSort(nodes, node_filter=None):
   sorted_nodes = []
   sources = []
   remaining_in_edges = {}
-  valid_node_count = 0
   for n in nodes:
-    if node_filter(n):
-      valid_node_count += 1
     if n.Predecessors():
       remaining_in_edges[n] = len(n.Predecessors())
     elif node_filter(n):
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index f811ff1..d4847cd 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -59,7 +59,7 @@ class DevToolsConnection(object):
     Args:
       listener: (Listener) listener to unregister.
     """
-    keys = [k for (k, v) in self._listeners if k == name]
+    keys = [k for (k, v) in self._listeners if v is listener]
     assert keys, "Removing non-existent listener"
     for key in keys:
       del(self._listeners[key])
@@ -104,7 +104,7 @@ class DevToolsConnection(object):
     while not self._please_stop:
       try:
         self._ws.DispatchNotifications()
-      except websocket.WebSocketTimeoutException as e:
+      except websocket.WebSocketTimeoutException:
         break
     if not self._please_stop:
       logging.warning('Monitoring stopped on a timeout.')
diff --git a/loading/loading_model.py b/loading/loading_model.py
index 2e76384..fed48e4 100644
--- a/loading/loading_model.py
+++ b/loading/loading_model.py
@@ -131,7 +131,8 @@ class ResourceGraph(object):
     node_filter = self._node_filter if node_filter is None else node_filter
     total = 0
     for n in self._node_info:
-      if not node_filter(n.Node()): continue
+      if not node_filter(n.Node()):
+        continue
       for s in n.Node().Successors():
         if node_filter(s):
           total += self._EdgeCost(n.Node(), s)
@@ -391,7 +392,7 @@ class ResourceGraph(object):
       new_parent.Node().AddSuccessor(self.Node())
       new_parent.SetEdgeCost(self, edge_cost)
       for a in edge_annotations:
-        new_parent.AddEdgeAnnotation(self, edge_annotations)
+        new_parent.AddEdgeAnnotation(self, a)
 
     def __eq__(self, o):
       return self.Node().Index() == o.Node().Index()
@@ -481,12 +482,13 @@ class ResourceGraph(object):
                 and urlparse.urlparse(r.url).hostname == request_hostname)]
         most_recent = None
         # Linear search is bad, but this shouldn't matter here.
-        for request in sorted_script_requests_from_hostname:
-          if request.timestamp < r.timing.requestTime:
-            most_recent = request
+        for r in sorted_script_requests_from_hostname:
+          if r.timestamp < request.timing.requestTime:
+            most_recent = r
           else:
             break
         if most_recent is not None:
+          url = most_recent.url
           if url in indicies_by_url:
             predecessor_url = url
             predecessor_type = 'script_inferred'
@@ -495,7 +497,8 @@ class ResourceGraph(object):
         predecessor = self._FindBestPredecessor(
             current_node_info, indicies_by_url[predecessor_url])
         edge_cost = current_node_info.StartTime() - predecessor.EndTime()
-        if edge_cost < 0: edge_cost = 0
+        if edge_cost < 0:
+          edge_cost = 0
         if current_node_info.StartTime() < predecessor.StartTime():
           logging.error('Inverted dependency: %s->%s',
                         predecessor.ShortName(), current_node_info.ShortName())
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
index bdc3915..970f00a 100644
--- a/loading/loading_model_unittest.py
+++ b/loading/loading_model_unittest.py
@@ -144,7 +144,8 @@ class LoadingModelTestCase(unittest.TestCase):
         'http://afae61024b33032ef.profile.sfo20.cloudfront.net/tst.png'))
 
     self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
-        'http://ums.adtechus.com/mapuser?providerid=1003;userid=RUmecco4z3o===='))
+        'http://ums.adtechus.com/mapuser?providerid=1003;'
+        'userid=RUmecco4z3o===='))
     self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
         'http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'))
 
diff --git a/loading/processing.py b/loading/processing.py
index a6ddde1..1f48939 100644
--- a/loading/processing.py
+++ b/loading/processing.py
@@ -10,25 +10,26 @@ import log_parser
 import loading_model
 
 
-def SitesFromDir(dir):
-  """Extract sites from a data dir.
+def SitesFromDir(directory):
+  """Extract sites from a data directory.
 
   Based on ./analyze.py fetch file name conventions. We assume each site
   corresponds to two files, <site>.json and <site>.json.cold, and that no other
   kind of file appears in the data directory.
 
   Args:
-    dir: the directory to process.
+    directory: the directory to process.
 
   Returns:
     A list of sites as strings.
 
   """
-  files = set(os.listdir(dir))
+  files = set(os.listdir(directory))
   assert files
   sites = []
   for f in files:
-    if f.endswith('.png'): continue
+    if f.endswith('.png'):
+      continue
     assert f.endswith('.json') or f.endswith('.json.cold'), f
     if f.endswith('.json'):
       assert f + '.cold' in files
diff --git a/loading/run_tests b/loading/run_tests
new file mode 100755
index 0000000..7f182a7
--- /dev/null
+++ b/loading/run_tests
@@ -0,0 +1,26 @@
+#!/usr/bin/env python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import logging
+import os
+import sys
+import unittest
+
+
+if __name__ == '__main__':
+  logging.basicConfig(
+      level=logging.DEBUG if '-v' in sys.argv else logging.WARNING,
+      format='%(levelname)5s %(filename)15s(%(lineno)3d): %(message)s')
+
+  suite = unittest.TestSuite()
+  loader = unittest.TestLoader()
+  pattern = '*%s*_unittest.py' % ('' if len(sys.argv) < 2 else sys.argv[1])
+  root_dir = os.path.dirname(os.path.realpath(__file__))
+  suite.addTests(loader.discover(start_dir=root_dir, pattern=pattern))
+  res = unittest.TextTestRunner(verbosity=2).run(suite)
+  if res.wasSuccessful():
+    sys.exit(0)
+  else:
+    sys.exit(1)
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
index b6ff35b..f5294ce 100755
--- a/loading/trace_recorder.py
+++ b/loading/trace_recorder.py
@@ -20,6 +20,7 @@ import devtools_monitor
 class PageTrack(devtools_monitor.Track):
   """Records the events from the page track."""
   def __init__(self, connection):
+    super(PageTrack, self).__init__()
     self._connection = connection
     self._events = []
     self._main_frame_id = None
@@ -47,6 +48,8 @@ class AndroidTraceRecorder(object):
   """Records a loading trace."""
   def __init__(self, url):
     self.url = url
+    self.devtools_connection = None
+    self.page_track = None
 
   def Go(self):
     self.devtools_connection = devtools_monitor.DevToolsConnection(

commit 438cb7d8fb09c012d784d538df5948824d787fe1
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 12 10:42:06 2016 -0800

    tools/android/loading: Implement devtools_monitor.
    
     tools/android/loading: Implement devtools_monitor.
    
    Also,
    - Move device setup out of log_requests.py
    - Add trace_recorder.py as a skeleton example of the API use.
    
    Review URL: https://codereview.chromium.org/1580793002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368922}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 3e6374730d7e1a650622fe98d6a65ebb79d3612b

diff --git a/loading/OWNERS b/loading/OWNERS
index 3301555..0b168ed 100644
--- a/loading/OWNERS
+++ b/loading/OWNERS
@@ -1,2 +1,4 @@
 lizeb@chromium.org
 pasko@chromium.org
+# Not a committer yet, but OWNER nonetheless:
+# mattcary@chromium.org
diff --git a/loading/device_setup.py b/loading/device_setup.py
new file mode 100644
index 0000000..d98014c
--- /dev/null
+++ b/loading/device_setup.py
@@ -0,0 +1,95 @@
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import contextlib
+import os
+import sys
+import time
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+
+from pylib import constants
+from pylib import flag_changer
+from pylib.device import device_utils
+from pylib.device import intent
+
+DEVTOOLS_PORT = 9222
+DEVTOOLS_HOSTNAME = 'localhost'
+
+@contextlib.contextmanager
+def FlagChanger(device, command_line_path, new_flags):
+  """Changes the flags in a context, restores them afterwards.
+
+  Args:
+    device: Device to target, from DeviceUtils.
+    command_line_path: Full path to the command-line file.
+    new_flags: Flags to add.
+  """
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
+  changer = flag_changer.FlagChanger(device, command_line_path)
+  changer.AddFlags(new_flags)
+  try:
+    yield
+  finally:
+    changer.Restore()
+
+
+@contextlib.contextmanager
+def ForwardPort(device, local, remote):
+  """Forwards a local port to a remote one on a device in a context."""
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
+  device.adb.Forward(local, remote)
+  try:
+    yield
+  finally:
+    device.adb.ForwardRemove(local)
+
+
+def _SetUpDevice(device, package_info):
+  """Enables root and closes Chrome on a device."""
+  device.EnableRoot()
+  device.KillAll(package_info.package, quiet=True)
+
+
+def SetUpAndExecute(device, package, fn):
+  """Start logging process.
+
+  Sets up any device and tracing appropriately and then executes the core
+  logging function.
+
+  Args:
+    device: Android device, or None for a local run.
+    package: the key for chrome package info.
+    fn: the function to execute that launches chrome and performs the
+        appropriate instrumentation, see _Log*Internal().
+
+  Returns:
+    As fn() returns.
+  """
+  package_info = constants.PACKAGE_INFO[package]
+  command_line_path = '/data/local/chrome-command-line'
+  new_flags = ['--enable-test-events',
+               '--remote-debugging-port=%d' % DEVTOOLS_PORT]
+  if device:
+    _SetUpDevice(device, package_info)
+  with FlagChanger(device, command_line_path, new_flags):
+    if device:
+      start_intent = intent.Intent(
+          package=package_info.package, activity=package_info.activity,
+          data='about:blank')
+      device.StartActivity(start_intent, blocking=True)
+      time.sleep(2)
+    # If no device, we don't care about chrome startup so skip the about page.
+    with ForwardPort(device, 'tcp:%d' % DEVTOOLS_PORT,
+                     'localabstract:chrome_devtools_remote'):
+      return fn()
diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
index 7104e3a..f811ff1 100644
--- a/loading/devtools_monitor.py
+++ b/loading/devtools_monitor.py
@@ -5,6 +5,25 @@
 """Library handling DevTools websocket interaction.
 """
 
+import httplib
+import json
+import logging
+import os
+import sys
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
+
+from telemetry.internal.backends.chrome_inspector import inspector_websocket
+from telemetry.internal.backends.chrome_inspector import websocket
+
+
+class DevToolsConnectionException(Exception):
+  def __init__(self, message):
+    super(DevToolsConnectionException, self).__init__(message)
+    logging.warning("DevToolsConnectionException: " + message)
+
+
 class DevToolsConnection(object):
   """Handles the communication with a DevTools server.
   """
@@ -15,7 +34,10 @@ class DevToolsConnection(object):
       hostname: server hostname.
       port: port number.
     """
-    pass
+    self._ws = self._Connect(hostname, port)
+    self._listeners = {}
+    self._domains_to_enable = set()
+    self._please_stop = False
 
   def RegisterListener(self, name, listener):
     """Registers a listener for an event.
@@ -27,7 +49,9 @@ class DevToolsConnection(object):
             Network.requestWillBeSent.
       listener: (Listener) listener instance.
     """
-    pass
+    domain = name[:name.index('.')]
+    self._listeners[name] = listener
+    self._domains_to_enable.add(domain)
 
   def UnregisterListener(self, listener):
     """Unregisters a listener.
@@ -35,7 +59,10 @@ class DevToolsConnection(object):
     Args:
       listener: (Listener) listener to unregister.
     """
-    pass
+    keys = [k for (k, v) in self._listeners if k == name]
+    assert keys, "Removing non-existent listener"
+    for key in keys:
+      del(self._listeners[key])
 
   def SyncRequest(self, method, params=None):
     """Issues a synchronous request to the DevTools server.
@@ -47,7 +74,10 @@ class DevToolsConnection(object):
     Returns:
       The answer.
     """
-    pass
+    request = {'method': method}
+    if params:
+      request['params'] = params
+    return self._ws.SyncRequest(request)
 
   def SendAndIgnoreResponse(self, method, params=None):
     """Issues a request to the DevTools server, do not wait for the response.
@@ -56,15 +86,66 @@ class DevToolsConnection(object):
       method: (str) Method.
       params: (dict) Optional parameters to the request.
     """
-    pass
+    request = {'method': method}
+    if params:
+      request['params'] = params
+    self._ws.SendAndIgnoreResponse(request)
+
+  def SetUpMonitoring(self):
+    for domain in self._domains_to_enable:
+      self._ws.RegisterDomain(domain, self._OnDataReceived)
+      self.SyncRequest('%s.enable' % domain)
 
   def StartMonitoring(self):
-    """Starts monitoring, enabling the relevant domains first."""
-    pass
+    """Starts monitoring.
 
-  def Stop(self):
+    DevToolsConnection.SetUpMonitoring() has to be called first.
+    """
+    while not self._please_stop:
+      try:
+        self._ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException as e:
+        break
+    if not self._please_stop:
+      logging.warning('Monitoring stopped on a timeout.')
+    self._TearDownMonitoring()
+
+  def StopMonitoring(self):
     """Stops the monitoring."""
-    pass
+    self._please_stop = True
+
+  def _TearDownMonitoring(self):
+    for domain in self._domains_to_enable:
+      self.SyncRequest('%s.disable' % domain)
+      self._ws.UnregisterDomain(domain)
+    self._domains_to_enable.clear()
+    self._listeners.clear()
+
+  def _OnDataReceived(self, msg):
+    method = msg.get('method', None)
+    if method not in self._listeners:
+      return
+    self._listeners[method].Handle(method, msg)
+
+  @classmethod
+  def _GetWebSocketUrl(cls, hostname, port):
+    r = httplib.HTTPConnection(hostname, port)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      raise DevToolsConnectionException(
+          'Cannot connect to DevTools, reponse code %d' % response.status)
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    return websocket_url
+
+  @classmethod
+  def _Connect(cls, hostname, port):
+    websocket_url = cls._GetWebSocketUrl(hostname, port)
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    return ws
 
 
 class Listener(object):
@@ -77,7 +158,7 @@ class Listener(object):
     """
     pass
 
-  def Handle(self, event_name, event):
+  def Handle(self, method, msg):
     """Handles an event this instance listens for.
 
     Args:
diff --git a/loading/log_requests.py b/loading/log_requests.py
index 071870c..ecbb0e8 100755
--- a/loading/log_requests.py
+++ b/loading/log_requests.py
@@ -14,66 +14,18 @@ import logging
 import optparse
 import os
 import sys
-import time
 
 file_dir = os.path.dirname(__file__)
 sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
 sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
 sys.path.append(os.path.join(file_dir, '..', '..', 'chrome_proxy'))
 
-from pylib import constants
-from pylib import flag_changer
 from pylib.device import device_utils
-from pylib.device import intent
 from common import inspector_network
 from telemetry.internal.backends.chrome_inspector import inspector_websocket
 from telemetry.internal.backends.chrome_inspector import websocket
 
-
-_PORT = 9222 # DevTools port number.
-
-
-@contextlib.contextmanager
-def FlagChanger(device, command_line_path, new_flags):
-  """Changes the flags in a context, restores them afterwards.
-
-  Args:
-    device: Device to target, from DeviceUtils.
-    command_line_path: Full path to the command-line file.
-    new_flags: Flags to add.
-  """
-  # If we're logging requests from a local desktop chrome instance there is no
-  # device.
-  if not device:
-    yield
-    return
-  changer = flag_changer.FlagChanger(device, command_line_path)
-  changer.AddFlags(new_flags)
-  try:
-    yield
-  finally:
-    changer.Restore()
-
-
-@contextlib.contextmanager
-def ForwardPort(device, local, remote):
-  """Forwards a local port to a remote one on a device in a context."""
-  # If we're logging requests from a local desktop chrome instance there is no
-  # device.
-  if not device:
-    yield
-    return
-  device.adb.Forward(local, remote)
-  try:
-    yield
-  finally:
-    device.adb.ForwardRemove(local)
-
-
-def _SetUpDevice(device, package_info):
-  """Enables root and closes Chrome on a device."""
-  device.EnableRoot()
-  device.KillAll(package_info.package, quiet=True)
+import device_setup
 
 
 class AndroidRequestsLogger(object):
@@ -111,7 +63,7 @@ class AndroidRequestsLogger(object):
   def _LogPageLoadInternal(self, url, clear_cache):
     """Returns the collection of requests made to load a given URL.
 
-    Assumes that DevTools is available on http://localhost:_PORT.
+    Assumes that DevTools is available on http://localhost:DEVTOOLS_PORT.
 
     Args:
       url: URL to load.
@@ -122,7 +74,8 @@ class AndroidRequestsLogger(object):
     """
     self._main_frame_id = None
     self._please_stop = False
-    r = httplib.HTTPConnection('localhost', _PORT)
+    r = httplib.HTTPConnection(
+        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
     r.request('GET', '/json')
     response = r.getresponse()
     if response.status != 200:
@@ -155,7 +108,7 @@ class AndroidRequestsLogger(object):
   def _LogTracingInternal(self, url):
     self._main_frame_id = None
     self._please_stop = False
-    r = httplib.HTTPConnection('localhost', _PORT)
+    r = httplib.HTTPConnection('localhost', device_setup.DEVTOOLS_PORT)
     r.request('GET', '/json')
     response = r.getresponse()
     if response.status != 200:
@@ -182,36 +135,6 @@ class AndroidRequestsLogger(object):
     return {'events': self._tracing_data,
             'end': ws.SyncRequest({'method': 'Tracing.end'})}
 
-  def _DoSomeLogging(self, package, fn):
-    """Start logging process.
-
-    Sets up any device and tracing appropriately and then executes the core
-    logging function.
-
-    Args:
-      package: the key for chrome package info.
-      fn: the function to execute that launches chrome and performs the
-      appropriate instrumentation, see _Log*Internal().
-
-    Returns:
-      As fn() returns.
-    """
-    package_info = constants.PACKAGE_INFO[package]
-    command_line_path = '/data/local/chrome-command-line'
-    new_flags = ['--enable-test-events', '--remote-debugging-port=%d' % _PORT]
-    if self.device:
-      _SetUpDevice(self.device, package_info)
-    with FlagChanger(self.device, command_line_path, new_flags):
-      if self.device:
-        start_intent = intent.Intent(
-            package=package_info.package, activity=package_info.activity,
-            data='about:blank')
-        self.device.StartActivity(start_intent, blocking=True)
-        time.sleep(2)
-      # If no device, we don't care about chrome startup so skip the about page.
-      with ForwardPort(self.device, 'tcp:%d' % _PORT,
-                       'localabstract:chrome_devtools_remote'):
-        return fn()
 
   def LogPageLoad(self, url, clear_cache, package):
     """Returns the collection of requests made to load a given URL on a device.
@@ -223,8 +146,9 @@ class AndroidRequestsLogger(object):
     Returns:
       See _LogPageLoadInternal().
     """
-    return self._DoSomeLogging(
-        package, lambda: self._LogPageLoadInternal(url, clear_cache))
+    return device_setup.SetUpAndExecute(
+        self.device, package,
+        lambda: self._LogPageLoadInternal(url, clear_cache))
 
   def LogTracing(self, url):
     """Log tracing events from a load of the given URL.
@@ -233,7 +157,8 @@ class AndroidRequestsLogger(object):
     simultaneously with network requests, but as that wasn't working the tracing
     logging was broken out separately. It still doesn't work...
     """
-    return self._DoSomeLogging('chrome', lambda: self._LogTracingInternal(url))
+    return device_setup.SetUpAndExecute(
+        self.device, 'chrome', lambda: self._LogTracingInternal(url))
 
 
 def _ResponseDataToJson(data):
diff --git a/loading/trace_recorder.py b/loading/trace_recorder.py
new file mode 100755
index 0000000..b6ff35b
--- /dev/null
+++ b/loading/trace_recorder.py
@@ -0,0 +1,71 @@
+#! /usr/bin/python
+# Copyright 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loading trace recorder."""
+
+import os
+import sys
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+
+from pylib.device import device_utils
+
+import device_setup
+import devtools_monitor
+
+
+class PageTrack(devtools_monitor.Track):
+  """Records the events from the page track."""
+  def __init__(self, connection):
+    self._connection = connection
+    self._events = []
+    self._main_frame_id = None
+    self._connection.RegisterListener('Page.frameStartedLoading', self)
+    self._connection.RegisterListener('Page.frameStoppedLoading', self)
+
+  def Handle(self, method, msg):
+    params = msg['params']
+    frame_id = params['frameId']
+    should_stop = False
+    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
+      self._main_frame_id = params['frameId']
+    elif (method == 'Page.frameStoppedLoading'
+          and params['frameId'] == self._main_frame_id):
+      should_stop = True
+    self._events.append((method, frame_id))
+    if should_stop:
+      self._connection.StopMonitoring()
+
+  def GetEvents(self):
+    return self._events
+
+
+class AndroidTraceRecorder(object):
+  """Records a loading trace."""
+  def __init__(self, url):
+    self.url = url
+
+  def Go(self):
+    self.devtools_connection = devtools_monitor.DevToolsConnection(
+        device_setup.DEVTOOLS_HOSTNAME, device_setup.DEVTOOLS_PORT)
+    self.page_track = PageTrack(self.devtools_connection)
+
+    self.devtools_connection.SetUpMonitoring()
+    self.devtools_connection.SendAndIgnoreResponse(
+        'Page.navigate', {'url': self.url})
+    self.devtools_connection.StartMonitoring()
+    print self.page_track.GetEvents()
+
+
+def DoIt(url):
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  device = devices[0]
+  trace_recorder = AndroidTraceRecorder(url)
+  device_setup.SetUpAndExecute(device, 'chrome', trace_recorder.Go)
+
+
+if __name__ == '__main__':
+  DoIt(sys.argv[1])

commit 30c9ec080d6c95e986a21ef66b6dc089c02caf04
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 12 05:39:13 2016 -0800

    tools/android/loading: Skeleton of the DevTools monitor.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1574253002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368864}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 450085ec099e0c8eb2ba6f2f369f759607658c84

diff --git a/loading/devtools_monitor.py b/loading/devtools_monitor.py
new file mode 100644
index 0000000..7104e3a
--- /dev/null
+++ b/loading/devtools_monitor.py
@@ -0,0 +1,94 @@
+# Copyright (c) 2016 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Library handling DevTools websocket interaction.
+"""
+
+class DevToolsConnection(object):
+  """Handles the communication with a DevTools server.
+  """
+  def __init__(self, hostname, port):
+    """Initializes the connection with a DevTools server.
+
+    Args:
+      hostname: server hostname.
+      port: port number.
+    """
+    pass
+
+  def RegisterListener(self, name, listener):
+    """Registers a listener for an event.
+
+    Also takes care of enabling the relevant domain before starting monitoring.
+
+    Args:
+      name: (str) Event the listener wants to listen to, e.g.
+            Network.requestWillBeSent.
+      listener: (Listener) listener instance.
+    """
+    pass
+
+  def UnregisterListener(self, listener):
+    """Unregisters a listener.
+
+    Args:
+      listener: (Listener) listener to unregister.
+    """
+    pass
+
+  def SyncRequest(self, method, params=None):
+    """Issues a synchronous request to the DevTools server.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Optional parameters to the request.
+
+    Returns:
+      The answer.
+    """
+    pass
+
+  def SendAndIgnoreResponse(self, method, params=None):
+    """Issues a request to the DevTools server, do not wait for the response.
+
+    Args:
+      method: (str) Method.
+      params: (dict) Optional parameters to the request.
+    """
+    pass
+
+  def StartMonitoring(self):
+    """Starts monitoring, enabling the relevant domains first."""
+    pass
+
+  def Stop(self):
+    """Stops the monitoring."""
+    pass
+
+
+class Listener(object):
+  """Listens to events forwarded by a DevToolsConnection instance."""
+  def __init__(self, connection):
+    """Initializes a Listener instance.
+
+    Args:
+      connection: (DevToolsConnection).
+    """
+    pass
+
+  def Handle(self, event_name, event):
+    """Handles an event this instance listens for.
+
+    Args:
+      event_name: (str) Event name, as registered.
+      event: (dict) complete event.
+    """
+    pass
+
+
+class Track(Listener):
+  """Collects data from a DevTools server."""
+  def GetEvents(self):
+    """Returns a list of collected events, finalizing the state if necessary."""
+    pass

commit 01f5dc9ebfc7d83490cc723e57c66a16a9a7a8bb
Author: mattcary <mattcary@google.com>
Date:   Mon Jan 11 08:04:34 2016 -0800

    Initial loading model and analysis tools.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1562373002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368597}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 04a313090df5eb6a419becc1588449c74aa1a7ae

diff --git a/loading/analyze.py b/loading/analyze.py
new file mode 100755
index 0000000..1a49155
--- /dev/null
+++ b/loading/analyze.py
@@ -0,0 +1,362 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import cgi
+import json
+import logging
+import os
+import subprocess
+import sys
+import tempfile
+import time
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+
+from pylib import constants
+from pylib.device import device_utils
+from pylib.device import intent
+
+import log_parser
+import log_requests
+import loading_model
+
+
+# TODO(mattcary): logging.info isn't that useful; we need something finer
+# grained. For now we just do logging.warning.
+
+
+# TODO(mattcary): probably we want this piped in through a flag.
+CHROME = constants.PACKAGE_INFO['chrome']
+
+
+def _SetupAndGetDevice():
+  """Gets an android device, set up the way we like it.
+
+  Returns:
+    An AdbWrapper for the first device found.
+  """
+  device = device_utils.DeviceUtils.HealthyDevices()[0]
+  device.EnableRoot()
+  device.KillAll(CHROME.package, quiet=True)
+  return device
+
+
+def _LoadPage(device, url):
+  """Load a page on chrome on our device.
+
+  Args:
+    device: an AdbWrapper for the device on which to load the page.
+    url: url as a string to load.
+  """
+  load_intent = intent.Intent(
+      package=CHROME.package, activity=CHROME.activity, data=url)
+  logging.warning('Loading ' + url)
+  device.StartActivity(load_intent, blocking=True)
+
+
+def _WriteJson(output, json_data):
+  """Write JSON data in a nice way.
+
+  Args:
+    output: a file object
+    json_data: JSON data as a dict.
+  """
+  json.dump(json_data, output, sort_keys=True, indent=2)
+
+
+def _GetPrefetchHtml(graph, name=None):
+  """Generate prefetch page for the resources in resource graph.
+
+  Args:
+    graph: a ResourceGraph.
+    name: optional string used in the generated page.
+
+  Returns:
+    HTML as a string containing all the link rel=prefetch directives necessary
+    for prefetching the given ResourceGraph.
+  """
+  if name:
+    title = 'Prefetch for ' + cgi.escape(name)
+  else:
+    title = 'Generated prefetch page'
+  output = []
+  output.append("""<!DOCTYPE html>
+<html>
+<head>
+<title>%s</title>
+""" % title)
+  for info in graph.ResourceInfo():
+    output.append('<link rel="prefetch" href="%s">\n' % info.Url())
+  output.append("""</head>
+<body>%s</body>
+</html>
+  """ % title)
+
+  return '\n'.join(output)
+
+
+def _LogRequests(url, clear_cache=True, local=False):
+  """Log requests for a web page.
+
+  TODO(mattcary): loading.log_requests probably needs to be refactored as we're
+  using private methods, also there's ugliness like _ResponseDataToJson return a
+  json.dumps that we immediately json.loads.
+
+  Args:
+    url: url to log as string.
+    clear_cache: optional flag to clear the cache.
+    local: log from local (desktop) chrome session.
+
+  Returns:
+    JSON of logged information (ie, a dict that describes JSON).
+  """
+  device = _SetupAndGetDevice() if not local else None
+  request_logger = log_requests.AndroidRequestsLogger(device)
+  logging.warning('Logging %scached %s' % ('un' if clear_cache else '', url))
+  response_data = request_logger.LogPageLoad(
+      url, clear_cache, 'chrome')
+  return json.loads(log_requests._ResponseDataToJson(response_data))
+
+
+def _FullFetch(url, json_output, prefetch, local, prefetch_delay_seconds):
+  """Do a full fetch with optional prefetching."""
+  if not url.startswith('http'):
+    url = 'http://' + url
+  logging.warning('Cold fetch')
+  cold_data = _LogRequests(url, local=local)
+  assert cold_data, 'Cold fetch failed to produce data. Check your phone.'
+  if prefetch:
+    assert not local
+    logging.warning('Generating prefetch')
+    prefetch_html = _GetPrefetchHtml(_ProcessJson(cold_data), name=url)
+    tmp = tempfile.NamedTemporaryFile()
+    tmp.write(prefetch_html)
+    tmp.flush()
+    # We hope that the tmpfile name is unique enough for the device.
+    target = os.path.join('/sdcard/Download', os.path.basename(tmp.name))
+    device = _SetupAndGetDevice()
+    device.adb.Push(tmp.name, target)
+    logging.warning('Pushed prefetch %s to device at %s' % (tmp.name, target))
+    _LoadPage(device, 'file://' + target)
+    time.sleep(prefetch_delay_seconds)
+    logging.warning('Warm fetch')
+    warm_data = _LogRequests(url, clear_cache=False)
+    with open(json_output, 'w') as f:
+      _WriteJson(f, warm_data)
+    logging.warning('Wrote ' + json_output)
+    with open(json_output + '.cold', 'w') as f:
+      _WriteJson(f, cold_data)
+    logging.warning('Wrote ' + json_output + '.cold')
+  else:
+    with open(json_output, 'w') as f:
+      _WriteJson(f, cold_data)
+    logging.warning('Wrote ' + json_output)
+
+
+# TODO(mattcary): it would be nice to refactor so the --noads flag gets dealt
+# with here.
+def _ProcessRequests(filename):
+  requests = log_parser.FilterRequests(log_parser.ParseJsonFile(filename))
+  return loading_model.ResourceGraph(requests)
+
+
+def _ProcessJson(json_data):
+  assert json_data
+  return loading_model.ResourceGraph(log_parser.FilterRequests(
+      [log_parser.RequestData.FromDict(r) for r in json_data]))
+
+
+def InvalidCommand(cmd):
+  sys.exit('Invalid command "%s"\nChoices are: %s' %
+           (cmd, ' '.join(COMMAND_MAP.keys())))
+
+
+def DoCost(arg_str):
+  parser = argparse.ArgumentParser(usage='cost [--parameter ...] REQUEST_JSON')
+  parser.add_argument('request_json')
+  parser.add_argument('--parameter', nargs='*', default=[])
+  parser.add_argument('--path', action='store_true')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  for p in args.parameter:
+    graph.Set(**{p: True})
+  path_list = []
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  print 'Graph cost: ' + str(graph.Cost(path_list))
+  if args.path:
+    for p in path_list:
+      print '  ' + p.ShortName()
+
+
+def DoPng(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='png [--eog] [--highlight X[,...] REQUEST_JSON [PNG_OUTPUT]')
+  parser.add_argument('request_json')
+  parser.add_argument('png_output', nargs='?')
+  parser.add_argument('--eog', action='store_true')
+  parser.add_argument('--highlight')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  tmp = tempfile.NamedTemporaryFile()
+  graph.MakeGraphviz(
+      tmp,
+      highlight=args.highlight.split(',') if args.highlight else None)
+  tmp.flush()
+  png_output = args.png_output
+  if not png_output:
+    if args.request_json.endswith('.json'):
+      png_output = args.request_json[:args.request_json.rfind('.json')] + '.png'
+    else:
+      png_output = args.request_json + '.png'
+  subprocess.check_call(['dot', '-Tpng', tmp.name, '-o', png_output])
+  logging.warning('Wrote ' + png_output)
+  if args.eog:
+    subprocess.Popen(['eog', png_output])
+  tmp.close()
+
+
+def DoCompare(arg_str):
+  parser = argparse.ArgumentParser(usage='compare REQUEST_JSON REQUEST_JSON')
+  parser.add_argument('g1_json')
+  parser.add_argument('g2_json')
+  args = parser.parse_args(arg_str)
+  g1 = _ProcessRequests(args.g1_json)
+  g2 = _ProcessRequests(args.g2_json)
+  discrepancies = loading_model.ResourceGraph.CheckImageLoadConsistency(g1, g2)
+  if discrepancies:
+    print '%d discrepancies' % len(discrepancies)
+    print '\n'.join([str(r) for r in discrepancies])
+  else:
+    print 'Consistent!'
+
+
+def DoPrefetchSetup(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='prefetch_setup [--upload] REQUEST_JSON TARGET_HTML')
+  parser.add_argument('request_json')
+  parser.add_argument('target_html')
+  parser.add_argument('--upload', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  with open(args.target_html, 'w') as html:
+    html.write(_GetPrefetchHtml(
+        graph, name=os.path.basename(args.request_json)))
+  if args.upload:
+    device = _SetupAndGetDevice()
+    destination = os.path.join('/sdcard/Download',
+                               os.path.basename(args.target_html))
+    device.adb.Push(args.target_html, destination)
+
+    logging.warning(
+        'Pushed %s to device at %s' % (args.target_html, destination))
+
+
+def DoLogRequests(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='log_requests [--prefetch] --site URL --output JSON_OUTPUT')
+  parser.add_argument('--url', required=True)
+  parser.add_argument('--output', required=True)
+  parser.add_argument('--prefetch', action='store_true')
+  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
+  parser.add_argument('--local', action='store_true')
+  args = parser.parse_args(arg_str)
+  _FullFetch(url=args.url,
+             json_output=args.output,
+             prefetch=args.prefetch,
+             prefetch_delay_seconds=args.prefetch_delay_seconds,
+             local=args.local)
+
+
+def DoFetch(arg_str):
+  parser = argparse.ArgumentParser(usage='fetch --site SITE --dir DIR\n'
+                                   'Fetches SITE into DIR with standard naming '
+                                   'that can be processed by ./cost_to_csv.py. '
+                                   'Both warm and cold fetches are done. '
+                                   'SITE can be a full url but the filename '
+                                   'may be strange so better to just use a '
+                                   'site (ie, domain).')
+  # Arguments are flags as it's easy to get the wrong order of site vs dir.
+  parser.add_argument('--site', required=True)
+  parser.add_argument('--dir', required=True)
+  parser.add_argument('--prefetch_delay_seconds', type=int, default=5)
+  args = parser.parse_args(arg_str)
+  if not os.path.exists(args.dir):
+    os.makedirs(args.dir)
+  _FullFetch(url=args.site,
+             json_output=os.path.join(args.dir, args.site + '.json'),
+             prefetch=True,
+             prefetch_delay_seconds=args.prefetch_delay_seconds,
+             local=False)
+
+
+def DoTracing(arg_str):
+  parser = argparse.ArgumentParser(
+      usage='tracing URL JSON_OUTPUT')
+  parser.add_argument('url')
+  parser.add_argument('json_output')
+  args = parser.parse_args(arg_str)
+  device = _SetupAndGetDevice()
+  request_logger = log_requests.AndroidRequestsLogger(device)
+  tracing = request_logger.LogTracing(args.url)
+  with open(args.json_output, 'w') as f:
+    _WriteJson(f, tracing)
+  logging.warning('Wrote ' + args.json_output)
+
+
+def DoLongPole(arg_str):
+  parser = argparse.ArgumentParser(usage='longpole [--noads] REQUEST_JSON')
+  parser.add_argument('request_json')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  path_list = []
+  cost = graph.Cost(path_list=path_list)
+  print '%s (%s)' % (path_list[-1], cost)
+
+
+def DoNodeCost(arg_str):
+  parser = argparse.ArgumentParser(usage='nodecost [--noads] REQUEST_JSON')
+  parser.add_argument('request_json')
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args(arg_str)
+  graph = _ProcessRequests(args.request_json)
+  if args.noads:
+    graph.Set(node_filter=graph.FilterAds)
+  print sum((n.NodeCost() for n in graph.Nodes()))
+
+
+COMMAND_MAP = {
+    'cost': DoCost,
+    'png': DoPng,
+    'compare': DoCompare,
+    'prefetch_setup': DoPrefetchSetup,
+    'log_requests': DoLogRequests,
+    'tracing': DoTracing,
+    'longpole': DoLongPole,
+    'nodecost': DoNodeCost,
+    'fetch': DoFetch,
+}
+
+def main():
+  logging.basicConfig(level=logging.WARNING)
+  parser = argparse.ArgumentParser(usage=' '.join(COMMAND_MAP.keys()))
+  parser.add_argument('command')
+  parser.add_argument('rest', nargs=argparse.REMAINDER)
+  args = parser.parse_args()
+  COMMAND_MAP.get(args.command,
+                  lambda _: InvalidCommand(args.command))(args.rest)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/cost_to_csv.py b/loading/cost_to_csv.py
new file mode 100755
index 0000000..d271d06
--- /dev/null
+++ b/loading/cost_to_csv.py
@@ -0,0 +1,43 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import logging
+import os
+import sys
+
+from processing import (SitesFromDir, WarmGraph, ColdGraph)
+
+
+def main():
+  logging.basicConfig(level=logging.ERROR)
+  parser = argparse.ArgumentParser(
+      description=('Convert a directory created by ./analyze.py fetch '
+                   'to a CSV.'))
+  parser.add_argument('--datadir', required=True)
+  parser.add_argument('--csv', required=True)
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args()
+  sites = SitesFromDir(args.datadir)
+  with open(args.csv, 'w') as output:
+    output.write('site,kind,cost\n')
+    for site in sites:
+      print site
+      warm = WarmGraph(args.datadir, site)
+      if args.noads:
+        warm.Set(node_filter=warm.FilterAds)
+      cold = ColdGraph(args.datadir, site)
+      if args.noads:
+        cold.Set(node_filter=cold.FilterAds)
+      output.write('%s,%s,%s\n' % (site, 'warm', warm.Cost()))
+      warm.Set(cache_all=True)
+      output.write('%s,%s,%s\n' % (site, 'warm-cache', warm.Cost()))
+      output.write('%s,%s,%s\n' % (site, 'cold', cold.Cost()))
+      cold.Set(cache_all=True)
+      output.write('%s,%s,%s\n' % (site, 'cold-cache', cold.Cost()))
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/dag.py b/loading/dag.py
new file mode 100644
index 0000000..d9f1a47
--- /dev/null
+++ b/loading/dag.py
@@ -0,0 +1,119 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Support for directed acyclic graphs.
+
+Used in the ResourceGraph model for chrome loading.
+"""
+
+class Node(object):
+  """A node in a DAG.
+
+  We do not enforce at a node level that a graph is a DAG. Methods like
+  TopologicalSort will assume a DAG and may fail if that's not the case.
+
+  Nodes are identified with an index that must be unique for a particular graph
+  (it is used for hashing and equality). A graph is represented as a list of
+  nodes, for example in the TopologicalSort class method. By convention a node's
+  index is its position in this list, making it easy to store auxillary
+  information.
+  """
+  def __init__(self, index):
+    """Create a new node.
+
+    Args:
+      index: index of the node. We assume these indicies uniquely identify a
+        node (and so use it for hashing and equality).
+    """
+    self._predecessors = set()
+    self._successors = set()
+    self._index = index
+
+  def Predecessors(self):
+    return self._predecessors
+
+  def Successors(self):
+    return self._successors
+
+  def AddSuccessor(self, s):
+    """Add a successor.
+
+    Updates appropriate links. Any existing parents of s are unchanged; to move
+    a node you must do a combination of RemoveSuccessor and AddSuccessor.
+
+    Args:
+      s: the node to add as a successor.
+    """
+    self._successors.add(s)
+    s._predecessors.add(self)
+
+  def RemoveSuccessor(self, s):
+    """Removes a successor.
+
+    Updates appropriate links.
+
+    Args:
+      s: the node to remove as a successor. Will raise a set exception if s is
+         not an existing successor.
+    """
+    self._successors.remove(s)
+    s._predecessors.remove(self)
+
+  def SortedSuccessors(self):
+    children = [c for c in self.Successors()]
+    children.sort(key=lambda c: c.Index())
+    return children
+
+  def Index(self):
+    return self._index
+
+  def __eq__(self, o):
+    return self.Index() == o.Index()
+
+  def __hash__(self):
+    return hash(self.Index())
+
+
+def TopologicalSort(nodes, node_filter=None):
+  """Topological sort.
+
+  We use a BFS-like walk which ensures that sibling are always grouped
+  together in the output. This is more convenient for some later analyses.
+
+  Args:
+    nodes: [Node, ...] Nodes to sort.
+    node_filter: a filter Node->boolean to restrict the graph. A node passes the
+      filter on a return value of True. Only the subgraph reachable from a root
+      passing the filter is considered.
+
+  Returns:
+    A list of Nodes in topological order. Note that node indicies are
+    unchanged; the original list nodes is not modified.
+  """
+  if node_filter is None:
+    node_filter = lambda _: True
+  sorted_nodes = []
+  sources = []
+  remaining_in_edges = {}
+  valid_node_count = 0
+  for n in nodes:
+    if node_filter(n):
+      valid_node_count += 1
+    if n.Predecessors():
+      remaining_in_edges[n] = len(n.Predecessors())
+    elif node_filter(n):
+      sources.append(n)
+  while sources:
+    n = sources.pop(0)
+    assert node_filter(n)
+    sorted_nodes.append(n)
+    # We sort by index to get consistent sorts across runs/machines.
+    for c in n.SortedSuccessors():
+      assert remaining_in_edges[c] > 0
+      if not node_filter(c):
+        continue
+      remaining_in_edges[c] -= 1
+      if not remaining_in_edges[c]:
+        sources.append(c)
+  return sorted_nodes
diff --git a/loading/dag_unittest.py b/loading/dag_unittest.py
new file mode 100644
index 0000000..6701c9e
--- /dev/null
+++ b/loading/dag_unittest.py
@@ -0,0 +1,92 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import sys
+import unittest
+
+import dag
+
+class DagTestCase(unittest.TestCase):
+
+  def MakeDag(self, links):
+    """Make a graph from a description of links.
+
+    Args:
+      links: A list of (index, (successor index...)) tuples. Index must equal
+        the location of the tuple in the list and are provided to make it easier
+        to read.
+
+    Returns:
+      A list of Nodes.
+    """
+    nodes = []
+    for i in xrange(len(links)):
+      assert i == links[i][0]
+      nodes.append(dag.Node(i))
+    for l in links:
+      for s in l[1]:
+        nodes[l[0]].AddSuccessor(nodes[s])
+    return nodes
+
+  def SortedIndicies(self, graph, node_filter=None):
+    return [n.Index() for n in dag.TopologicalSort(graph, node_filter)]
+
+  def SuccessorIndicies(self, node):
+    return [c.Index() for c in node.SortedSuccessors()]
+
+  def test_SimpleSorting(self):
+    graph = self.MakeDag([(0, (1,2)),
+                          (1, (3,)),
+                          (2, ()),
+                          (3, (4,)),
+                          (4, ()),
+                          (5, (6,)),
+                          (6, ())])
+    self.assertEqual(self.SuccessorIndicies(graph[0]), [1, 2])
+    self.assertEqual(self.SuccessorIndicies(graph[1]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph[2]), [])
+    self.assertEqual(self.SuccessorIndicies(graph[3]), [4])
+    self.assertEqual(self.SuccessorIndicies(graph[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph[5]), [6])
+    self.assertEqual(self.SuccessorIndicies(graph[6]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 5, 1, 2, 6, 3, 4])
+
+  def test_SortSiblingsAreGrouped(self):
+    graph = self.MakeDag([(0, (1, 2, 3)),
+                          (1, (4,)),
+                          (2, (5, 6)),
+                          (3, (7, 8)),
+                          (4, ()),
+                          (5, ()),
+                          (6, ()),
+                          (7, ()),
+                          (8, ())])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5, 6, 7, 8])
+
+  def test_FilteredSorting(self):
+    # 0 is a filtered-out root, which means the subgraph containing 1, 2, 3 and
+    # 4 should be ignored. 5 is an unfiltered root, and the subgraph containing
+    # 6, 7, 8 and 10 should be sorted. 9 and 11 are filtered out, and should
+    # exclude the unfiltred node 12.
+    graph = self.MakeDag([(0, (1,)),
+                          (1, (2, 3)),
+                          (2, ()),
+                          (3, (4,)),
+                          (4, ()),
+                          (5, (6, 7)),
+                          (6, (11,)),
+                          (7, (8,)),
+                          (8, (9, 10)),
+                          (9, ()),
+                          (10, ()),
+                          (11, (12,)),
+                          (12, ())])
+    self.assertEqual(self.SortedIndicies(
+        graph, lambda n: n.Index() not in (0, 3, 9, 11)),
+                     [5, 6, 7, 8, 10])
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/loading_model.py b/loading/loading_model.py
new file mode 100644
index 0000000..2e76384
--- /dev/null
+++ b/loading/loading_model.py
@@ -0,0 +1,732 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Models for loading in chrome.
+
+(Redirect the following to the general model module once we have one)
+A model is an object with the following methods.
+  CostMs(): return the cost of the cost in milliseconds.
+  Set(): set model-specifical parameters.
+
+ResourceGraph
+  This creates a DAG of resource dependancies from loading.log_requests to model
+  loading time. The model may be parameterized by changing the loading time of
+  a particular or all resources.
+"""
+
+import logging
+import os
+import urlparse
+import sys
+
+import dag
+import log_parser
+
+class ResourceGraph(object):
+  """A model of loading by a DAG (tree?) of resource dependancies.
+
+  Set parameters:
+    cache_all: if true, assume zero loading time for all resources.
+  """
+
+  def __init__(self, requests):
+    """Create from a parsed request set.
+
+    Args:
+      requests: [RequestData, ...] filtered RequestData from loading.log_parser.
+    """
+    self._BuildDag(requests)
+    self._global_start = min([n.StartTime() for n in self._node_info])
+    # Sort before splitting children so that we can correctly dectect if a
+    # reparented child is actually a dependency for a child of its new parent.
+    try:
+      for n in dag.TopologicalSort(self._nodes):
+        self._SplitChildrenByTime(self._node_info[n.Index()])
+    except AssertionError as exc:
+      sys.stderr.write('Bad topological sort: %s\n'
+                       'Skipping child split\n' % str(exc))
+    self._cache_all = False
+    self._node_filter = lambda _: True
+
+  @classmethod
+  def CheckImageLoadConsistency(cls, g1, g2):
+    """Check that images have the same dependancies between ResourceGraphs.
+
+    Image resources are identified by their short names.
+
+    Args:
+      g1: a ResourceGraph instance
+      g2: a ResourceGraph instance
+
+    Returns:
+      A list of discrepancy tuples. If this list is empty, g1 and g2 are
+      consistent with respect to image load dependencies. Otherwise, each tuple
+      is of the form:
+        ( g1 resource short name or str(list of short names),
+          g2 resource short name or str(list of short names),
+          human-readable discrepancy reason )
+      Either or both of the g1 and g2 image resource short names may be None if
+      it's not applicable for the discrepancy reason.
+    """
+    discrepancies = []
+    g1_image_to_info = g1._ExtractImages()
+    g2_image_to_info = g2._ExtractImages()
+    for image in set(g1_image_to_info.keys()) - set(g2_image_to_info.keys()):
+      discrepancies.append((image, None, 'Missing in g2'))
+    for image in set(g2_image_to_info.keys()) - set(g1_image_to_info.keys()):
+      discrepancies.append((None, image, 'Missing in g1'))
+
+    for image in set(g1_image_to_info.keys()) & set(g2_image_to_info.keys()):
+      def PredecessorInfo(g, n):
+        info = [g._ShortName(p) for p in n.Node().Predecessors()]
+        info.sort()
+        return str(info)
+      g1_pred = PredecessorInfo(g1, g1_image_to_info[image])
+      g2_pred = PredecessorInfo(g2, g2_image_to_info[image])
+      if g1_pred != g2_pred:
+        discrepancies.append((g1_pred, g2_pred,
+                              'Predecessor mismatch for ' + image))
+
+    return discrepancies
+
+  def Set(self, cache_all=None, node_filter=None):
+    """Set model parameters.
+
+    TODO(mattcary): add parameters for caching certain types of resources (just
+    scripts, just cachable, etc).
+
+    Args:
+      cache_all: boolean that if true ignores emperical resource load times for
+        all resources.
+      node_filter: a Node->boolean used to restrict the graph for most
+        operations.
+    """
+    if self._cache_all is not None:
+      self._cache_all = cache_all
+    if node_filter is not None:
+      self._node_filter = node_filter
+
+  def Nodes(self):
+    """Return iterable of all nodes via their _NodeInfos.
+
+    Returns:
+      Iterable of node infos in arbitrary order.
+    """
+    for n in self._node_info:
+      if self._node_filter(n.Node()):
+        yield n
+
+  def EdgeCosts(self, node_filter=None):
+    """Edge costs.
+
+    Args:
+      node_filter: if not none, a Node->boolean filter to use instead of the
+      current one from Set.
+
+    Returns:
+      The total edge costs of our graph.
+
+    """
+    node_filter = self._node_filter if node_filter is None else node_filter
+    total = 0
+    for n in self._node_info:
+      if not node_filter(n.Node()): continue
+      for s in n.Node().Successors():
+        if node_filter(s):
+          total += self._EdgeCost(n.Node(), s)
+    return total
+
+  def Intersect(self, other_nodes):
+    """Return iterable of nodes that intersect with another graph.
+
+    Args:
+      other_nodes: iterable of the nodes of another graph, eg from Nodes().
+
+    Returns:
+      an iterable of (mine, other) pairs for all nodes for which the URL is
+      identical.
+    """
+    other_map = {n.Url(): n for n in other_nodes}
+    for n in self._node_info:
+      if self._node_filter(n.Node()) and n.Url() in other_map:
+        yield(n, other_map[n.Url()])
+
+
+  def Cost(self, path_list=None):
+    """Compute cost of current model.
+
+    Args:
+      path_list: if not None, gets a list of NodeInfo in the longest path.
+
+    Returns:
+      Cost of the longest path.
+
+    """
+    costs = [0] * len(self._nodes)
+    for n in dag.TopologicalSort(self._nodes, self._node_filter):
+      cost = 0
+      if n.Predecessors():
+        cost = max([costs[p.Index()] + self._EdgeCost(p, n)
+                    for p in n.Predecessors()])
+      if not self._cache_all:
+        cost += self._NodeCost(n)
+      costs[n.Index()] = cost
+    max_cost = max(costs)
+    assert max_cost > 0  # Otherwise probably the filter went awry.
+    if path_list is not None:
+      del path_list[:-1]
+      n = (i for i in self._nodes if costs[i.Index()] == max_cost).next()
+      path_list.append(self._node_info[n.Index()])
+      while n.Predecessors():
+        n = reduce(lambda costliest, next:
+                   next if (self._node_filter(next) and
+                            cost[next.Index()] > cost[costliest.Index()])
+                        else costliest,
+                   n.Predecessors())
+        path_list.insert(0, self._node_info[n.Index()])
+    return max_cost
+
+  def FilterAds(self, node):
+    """A filter for use in eg, Cost, to remove advertising nodes.
+
+    Args:
+      node: A dag.Node.
+
+    Returns:
+      True if the node is not ad-related.
+    """
+    return not self._IsAdUrl(self._node_info[node.Index()].Url())
+
+  def MakeGraphviz(self, output, highlight=None):
+    """Output a graphviz representation of our DAG.
+
+    Args:
+      output: a file-like output stream which recieves a graphviz dot.
+      highlight: a list of node items to emphasize. Any resource url which
+        contains any highlight text will be distinguished in the output.
+    """
+    output.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+    orphans = set()
+    try:
+      sorted_nodes = dag.TopologicalSort(self._nodes,
+                                         node_filter=self._node_filter)
+    except AssertionError as exc:
+      sys.stderr.write('Bad topological sort: %s\n'
+                       'Writing children in order\n' % str(exc))
+      sorted_nodes = self._nodes
+    for n in sorted_nodes:
+      if not n.Successors() and not n.Predecessors():
+        orphans.add(n)
+    if orphans:
+      output.write("""subgraph cluster_orphans {
+  color=black;
+  label="Orphans";
+""")
+      for n in orphans:
+        output.write(self._GraphvizNode(n.Index(), highlight))
+      output.write('}\n')
+
+    output.write("""subgraph cluster_nodes {
+  color=invis;
+""")
+    for n in sorted_nodes:
+      if not n.Successors() and not n.Predecessors():
+        continue
+      output.write(self._GraphvizNode(n.Index(), highlight))
+
+    for n in sorted_nodes:
+      for s in n.Successors():
+        style = 'color = orange'
+        annotations = self._EdgeAnnotation(n, s)
+        if 'parser' in annotations:
+          style = 'color = red'
+        elif 'stack' in annotations:
+          style = 'color = blue'
+        elif 'script_inferred' in annotations:
+          style = 'color = purple'
+        if 'timing' in annotations:
+          style += '; style=dashed'
+        arrow = '[%s; label="%s"]' % (style, self._EdgeCost(n, s))
+        output.write('%d -> %d %s;\n' % (n.Index(), s.Index(), arrow))
+    output.write('}\n}\n')
+
+  def ResourceInfo(self):
+    """Get resource info.
+
+    Returns:
+      A list of _NodeInfo objects that describe the resources fetched.
+    """
+    return self._node_info
+
+  def DebugString(self):
+    """Graph structure for debugging.
+
+    TODO(mattcary): this fails for graphs with more than one component or where
+    self._nodes[0] is not a root.
+
+    Returns:
+      A human-readable string of the graph.
+    """
+    output = []
+    queue = [self._nodes[0]]
+    visited = set()
+    while queue:
+      n = queue.pop(0)
+      assert n not in visited
+      visited.add(n)
+      children = n.SortedSuccessors()
+      output.append('%d -> [%s]' %
+                    (n.Index(), ' '.join([str(c.Index()) for c in children])))
+      for c in children:
+        assert n in c.Predecessors()  # Integrity checking
+        queue.append(c)
+    assert len(visited) == len(self._nodes)
+    return '\n'.join(output)
+
+  ##
+  ## Internal items
+  ##
+
+  _CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
+                            'json': 'purple', 'gif_image': 'grey',
+                            'image': 'orange', 'other': 'white'}
+
+  # This resource type may induce a timing dependency. See _SplitChildrenByTime
+  # for details.
+  # TODO(mattcary): are these right?
+  _CAN_BE_TIMING_PARENT = set(['script', 'magic-debug-content'])
+  _CAN_MAKE_TIMING_DEPENDENCE = set(['json', 'other', 'magic-debug-content'])
+
+  class _NodeInfo(object):
+    """Our internal class that adds cost and other information to nodes.
+
+    Costs are stored on the node as well as edges. Edge information is only
+    stored on successor edges and not predecessor, that is, you get them from
+    the parent and not the child.
+
+    We also store the request on the node, and expose request-derived
+    information like content type.
+    """
+    def __init__(self, node, request):
+      """Create a new node info.
+
+      Args:
+        node: The node to augment.
+        request: The request associated with this node.
+      """
+      self._request = request
+      self._node = node
+      self._edge_costs = {}
+      self._edge_annotations = {}
+      # All fields in timing are millis relative to requestTime, which is epoch
+      # seconds.
+      self._node_cost = max([t for f, t in request.timing._asdict().iteritems()
+                             if f != 'requestTime'])
+
+    def __str__(self):
+      return self.ShortName()
+
+    def Node(self):
+      return self._node
+
+    def Index(self):
+      return self._node.Index()
+
+    def Request(self):
+      return self._request
+
+    def NodeCost(self):
+      return self._node_cost
+
+    def EdgeCost(self, s):
+      return self._edge_costs[s]
+
+    def StartTime(self):
+      return self._request.timing.requestTime * 1000
+
+    def EndTime(self):
+      return self._request.timing.requestTime * 1000 + self._node_cost
+
+    def EdgeAnnotation(self, s):
+      assert s.Node() in self.Node().Successors()
+      return self._edge_annotations.get(s, [])
+
+    def ContentType(self):
+      return log_parser.Resource.FromRequest(self._request).GetContentType()
+
+    def ShortName(self):
+      return log_parser.Resource.FromRequest(self._request).GetShortName()
+
+    def Url(self):
+      return self._request.url
+
+    def SetEdgeCost(self, child, cost):
+      assert child.Node() in self._node.Successors()
+      self._edge_costs[child] = cost
+
+    def AddEdgeAnnotation(self, s, annotation):
+      assert s.Node() in self._node.Successors()
+      self._edge_annotations.setdefault(s, []).append(annotation)
+
+    def ReparentTo(self, old_parent, new_parent):
+      """Move costs and annotatations from old_parent to new_parent.
+
+      Also updates the underlying node connections, ie, do not call
+      old_parent.RemoveSuccessor(), etc.
+
+      Args:
+        old_parent: the _NodeInfo of a current parent of self. We assert this
+          is actually a parent.
+        new_parent: the _NodeInfo of the new parent. We assert it is not already
+          a parent.
+      """
+      assert old_parent.Node() in self.Node().Predecessors()
+      assert new_parent.Node() not in self.Node().Predecessors()
+      edge_annotations = old_parent._edge_annotations.pop(self, [])
+      edge_cost =  old_parent._edge_costs.pop(self)
+      old_parent.Node().RemoveSuccessor(self.Node())
+      new_parent.Node().AddSuccessor(self.Node())
+      new_parent.SetEdgeCost(self, edge_cost)
+      for a in edge_annotations:
+        new_parent.AddEdgeAnnotation(self, edge_annotations)
+
+    def __eq__(self, o):
+      return self.Node().Index() == o.Node().Index()
+
+    def __hash__(self):
+      return hash(self.Node().Index())
+
+  def _ShortName(self, node):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[node.Index()].ShortName()
+
+  def _Url(self, node):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[node.Index()].Url()
+
+  def _NodeCost(self, node):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[node.Index()].NodeCost()
+
+  def _EdgeCost(self, parent, child):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[parent.Index()].EdgeCost(
+        self._node_info[child.Index()])
+
+  def _EdgeAnnotation(self, parent, child):
+    """Convenience function for redirecting Nodes to _NodeInfo."""
+    return self._node_info[parent.Index()].EdgeAnnotation(
+        self._node_info[child.Index()])
+
+  def _BuildDag(self, requests):
+    """Build DAG of resources.
+
+    Build a DAG from our requests and augment with _NodeInfo (see above) in a
+    parallel array indexed by Node.Index().
+
+    Creates self._nodes and self._node_info.
+
+    Args:
+      requests: [Request, ...] Requests from loading.log_parser.
+    """
+    self._nodes = []
+    self._node_info = []
+    indicies_by_url = {}
+    requests_by_completion = log_parser.SortedByCompletion(requests)
+    for request in requests:
+      next_index = len(self._nodes)
+      indicies_by_url.setdefault(request.url, []).append(next_index)
+      node = dag.Node(next_index)
+      node_info = self._NodeInfo(node, request)
+      self._nodes.append(node)
+      self._node_info.append(node_info)
+    for url, indicies in indicies_by_url.iteritems():
+      if len(indicies) > 1:
+        logging.warning('Multiple loads (%d) for url: %s' %
+                        (len(indicies), url))
+    for i in xrange(len(requests)):
+      request = requests[i]
+      current_node_info = self._node_info[i]
+      resource = log_parser.Resource.FromRequest(current_node_info.Request())
+      initiator = request.initiator
+      initiator_type = initiator['type']
+      predecessor_url = None
+      predecessor_type = None
+      # Classify & infer the predecessor. If a candidate url we identify as the
+      # predecessor is not in index_by_url, then we haven't seen it in our
+      # requests and we will try to find a better predecessor.
+      if initiator_type == 'parser':
+        url = initiator['url']
+        if url in indicies_by_url:
+          predecessor_url = url
+          predecessor_type = 'parser'
+      elif initiator_type == 'script' and 'stackTrace' in initiator:
+        for frame in initiator['stackTrace']:
+          url = frame['url']
+          if url in indicies_by_url:
+            predecessor_url = url
+            predecessor_type = 'stack'
+            break
+      elif initiator_type == 'script':
+        # When the initiator is a script without a stackTrace, infer that it
+        # comes from the most recent script from the same hostname.  TLD+1 might
+        # be better, but finding what is a TLD requires a database.
+        request_hostname = urlparse.urlparse(request.url).hostname
+        sorted_script_requests_from_hostname = [
+            r for r in requests_by_completion
+            if (resource.GetContentType() in ('script', 'html', 'json')
+                and urlparse.urlparse(r.url).hostname == request_hostname)]
+        most_recent = None
+        # Linear search is bad, but this shouldn't matter here.
+        for request in sorted_script_requests_from_hostname:
+          if request.timestamp < r.timing.requestTime:
+            most_recent = request
+          else:
+            break
+        if most_recent is not None:
+          if url in indicies_by_url:
+            predecessor_url = url
+            predecessor_type = 'script_inferred'
+      # TODO(mattcary): we skip initiator type other, is that correct?
+      if predecessor_url is not None:
+        predecessor = self._FindBestPredecessor(
+            current_node_info, indicies_by_url[predecessor_url])
+        edge_cost = current_node_info.StartTime() - predecessor.EndTime()
+        if edge_cost < 0: edge_cost = 0
+        if current_node_info.StartTime() < predecessor.StartTime():
+          logging.error('Inverted dependency: %s->%s',
+                        predecessor.ShortName(), current_node_info.ShortName())
+          # Note that current.StartTime() < predecessor.EndTime() appears to
+          # happen a fair amount in practice.
+        predecessor.Node().AddSuccessor(current_node_info.Node())
+        predecessor.SetEdgeCost(current_node_info, edge_cost)
+        predecessor.AddEdgeAnnotation(current_node_info, predecessor_type)
+
+  def _FindBestPredecessor(self, node_info, candidate_indicies):
+    """Find best predecessor for node_info
+
+    If there is only one candidate, we use it regardless of timings. We will
+    later warn about inverted dependencies. If there are more than one, we use
+    the latest whose end time is before node_info's start time. If there is no
+    such candidate, we throw up our hands and return an arbitrary one.
+
+    Args:
+      node_info: _NodeInfo of interest.
+      candidate_indicies: indicies of candidate predecessors.
+
+    Returns:
+      _NodeInfo of best predecessor.
+    """
+    assert candidate_indicies
+    if len(candidate_indicies) == 1:
+      return self._node_info[candidate_indicies[0]]
+    candidate = self._node_info[candidate_indicies[0]]
+    for i in xrange(1, len(candidate_indicies)):
+      next_candidate = self._node_info[candidate_indicies[i]]
+      if (next_candidate.EndTime() < node_info.StartTime() and
+          next_candidate.StartTime() > candidate.StartTime()):
+        candidate = next_candidate
+    if candidate.EndTime() > node_info.StartTime():
+      logging.warning('Multiple candidates but all inverted for ' +
+                      node_info.ShortName())
+    return candidate
+
+
+  def _SplitChildrenByTime(self, parent):
+    """Split children of a node by request times.
+
+    The initiator of a request may not be the true dependency of a request. For
+    example, a script may appear to load several resources independently, but in
+    fact one of them may be a JSON data file, and the remaining resources assets
+    described in the JSON. The assets should be dependent upon the JSON data
+    file, and not the original script.
+
+    This function approximates that by rearranging the children of a node
+    according to their request times. The predecessor of each child is made to
+    be the node with the greatest finishing time, that is before the start time
+    of the child.
+
+    We do this by sorting the nodes twice, once by start time and once by end
+    time. We mark the earliest end time, and then we walk the start time list,
+    advancing the end time mark when it is less than our current start time.
+
+    This is refined by only considering assets which we believe actually create
+    a dependency. We only split if the original parent is a script, and the new
+    parent a data file. We confirm these relationships heuristically by loading
+    pages multiple times and ensuring that dependacies do not change; see
+    CheckImageLoadConsistency() for details.
+
+    We incorporate this heuristic by skipping over any non-script/json resources
+    when moving the end mark.
+
+    TODO(mattcary): More heuristics, like incorporating cachability somehow, and
+    not just picking arbitrarily if there are two nodes with the same end time
+    (does that ever really happen?)
+
+    Args:
+      parent: the NodeInfo whose children we are going to rearrange.
+
+    """
+    if parent.ContentType() not in self._CAN_BE_TIMING_PARENT:
+      return  # No dependency changes.
+    children_by_start_time = [self._node_info[s.Index()]
+                              for s in parent.Node().Successors()]
+    children_by_start_time.sort(key=lambda c: c.StartTime())
+    children_by_end_time = [self._node_info[s.Index()]
+                            for s in parent.Node().Successors()]
+    children_by_end_time.sort(key=lambda c: c.EndTime())
+    end_mark = 0
+    for current in children_by_start_time:
+      if current.StartTime() < parent.EndTime() - 1e-5:
+        logging.warning('Child loaded before parent finished: %s -> %s',
+                        parent.ShortName(), current.ShortName())
+      go_to_next_child = False
+      while end_mark < len(children_by_end_time):
+        if children_by_end_time[end_mark] == current:
+          go_to_next_child = True
+          break
+        elif (children_by_end_time[end_mark].ContentType() not in
+            self._CAN_MAKE_TIMING_DEPENDENCE):
+          end_mark += 1
+        elif (end_mark < len(children_by_end_time) - 1 and
+              children_by_end_time[end_mark + 1].EndTime() <
+                  current.StartTime()):
+          end_mark += 1
+        else:
+          break
+      if end_mark >= len(children_by_end_time):
+        break  # It's not possible to rearrange any more children.
+      if go_to_next_child:
+        continue  # We can't rearrange this child, but the next child may be
+                  # eligible.
+      if children_by_end_time[end_mark].EndTime() <= current.StartTime():
+        current.ReparentTo(parent, children_by_end_time[end_mark])
+        children_by_end_time[end_mark].AddEdgeAnnotation(current, 'timing')
+
+  def _GraphvizNode(self, index, highlight):
+    """Returns a graphviz node description for a given node.
+
+    Args:
+      index: index of the node.
+      highlight: a list of node items to emphasize. Any resource url which
+        contains any highlight text will be distinguished in the output.
+
+    Returns:
+      A string describing the resource in graphviz format.
+      The resource is color-coded according to its content type, and its shape
+      is oval if its max-age is less than 300s (or if it's not cacheable).
+    """
+    node_info = self._node_info[index]
+    color = self._CONTENT_TYPE_TO_COLOR[node_info.ContentType()]
+    max_age = log_parser.MaxAge(node_info.Request())
+    shape = 'polygon' if max_age > 300 else 'oval'
+    styles = ['filled']
+    if highlight:
+      for fragment in highlight:
+        if fragment in node_info.Url():
+          styles.append('dotted')
+          break
+    return ('%d [label = "%s\\n%.2f->%.2f (%.2f)"; style = "%s"; '
+            'fillcolor = %s; shape = %s];\n'
+            % (index, node_info.ShortName(),
+               node_info.StartTime() - self._global_start,
+               node_info.EndTime() - self._global_start,
+               node_info.EndTime() - node_info.StartTime(),
+               ','.join(styles), color, shape))
+
+  @classmethod
+  def _IsAdUrl(cls, url):
+    """Return true if the url is an ad.
+
+    We group content that doesn't seem to be specific to the website along with
+    ads, eg staticxx.facebook.com, as well as analytics like googletagmanager (?
+    is this correct?).
+
+    Args:
+      url: The full string url to examine.
+
+    Returns:
+      True iff the url appears to be an ad.
+
+    """
+    # See below for how these patterns are defined.
+    AD_PATTERNS = ['2mdn.net',
+                   'admarvel.com',
+                   'adnxs.com',
+                   'adobedtm.com',
+                   'adsrvr.org',
+                   'adsafeprotected.com',
+                   'adsymptotic.com',
+                   'adtech.de',
+                   'adtechus.com',
+                   'advertising.com',
+                   'atwola.com',  # brand protection from cscglobal.com?
+                   'bounceexchange.com',
+                   'betrad.com',
+                   'casalemedia.com',
+                   'cloudfront.net//test.png',
+                   'cloudfront.net//atrk.js',
+                   'contextweb.com',
+                   'crwdcntrl.net',
+                   'doubleclick.net',
+                   'dynamicyield.com',
+                   'krxd.net',
+                   'facebook.com//ping',
+                   'fastclick.net',
+                   'google.com//-ads.js',
+                   'cse.google.com',  # Custom search engine.
+                   'googleadservices.com',
+                   'googlesyndication.com',
+                   'googletagmanager.com',
+                   'lightboxcdn.com',
+                   'mediaplex.com',
+                   'meltdsp.com',
+                   'mobile.nytimes.com//ads-success',
+                   'mookie1.com',
+                   'newrelic.com',
+                   'nr-data.net',   # Apparently part of newrelic.
+                   'optnmnstr.com',
+                   'pubmatic.com',
+                   'quantcast.com',
+                   'quantserve.com',
+                   'rubiconproject.com',
+                   'scorecardresearch.com',
+                   'sekindo.com',
+                   'serving-sys.com',
+                   'sharethrough.com',
+                   'staticxx.facebook.com',  # ?
+                   'syndication.twimg.com',
+                   'tapad.com',
+                   'yieldmo.com',
+                ]
+    parts = urlparse.urlparse(url)
+    for pattern in AD_PATTERNS:
+      if '//' in pattern:
+        domain, path = pattern.split('//')
+      else:
+        domain, path = (pattern, None)
+      if parts.netloc.endswith(domain):
+        if not path or path in parts.path:
+          return True
+    return False
+
+  def _ExtractImages(self):
+    """Return interesting image resources.
+
+    Uninteresting image resources are things like ads that we don't expect to be
+    constant across fetches.
+
+    Returns:
+      Dict of image url + short name to NodeInfo.
+    """
+    image_to_info = {}
+    for n in self._node_info:
+      if (n.ContentType().startswith('image') and
+          not self._IsAdUrl(n.Url())):
+        key = str((n.Url(), n.ShortName(), n.StartTime()))
+        assert key not in image_to_info, n.Url()
+        image_to_info[key] = n
+    return image_to_info
diff --git a/loading/loading_model_unittest.py b/loading/loading_model_unittest.py
new file mode 100644
index 0000000..bdc3915
--- /dev/null
+++ b/loading/loading_model_unittest.py
@@ -0,0 +1,153 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import sys
+import unittest
+
+import dag
+import loading_model
+import log_parser
+
+class LoadingModelTestCase(unittest.TestCase):
+
+  def MakeParserRequest(self, url, source_url, start_time, end_time,
+                        magic_content_type=False):
+    timing_data = {f: -1 for f in log_parser.Timing._fields}
+    # We should ignore connectEnd.
+    timing_data['connectEnd'] = (end_time - start_time) / 2
+    timing_data['receiveHeadersEnd'] = end_time - start_time
+    timing_data['requestTime'] = start_time / 1000.0
+    return log_parser.RequestData(
+        None, {'Content-Type': 'null' if not magic_content_type
+                                      else 'magic-debug-content' },
+        None, start_time, timing_data, 'http://' + str(url), False,
+        {'type': 'parser', 'url': 'http://' + str(source_url)})
+
+  def SortedIndicies(self, graph):
+    return [n.Index() for n in dag.TopologicalSort(graph._nodes)]
+
+  def SuccessorIndicies(self, node):
+    return [c.Index() for c in node.SortedSuccessors()]
+
+  def test_Costing(self):
+    requests = [self.MakeParserRequest(0, 'null', 100, 110),
+                self.MakeParserRequest(1, 0, 115, 120),
+                self.MakeParserRequest(2, 0, 112, 120),
+                self.MakeParserRequest(3, 1, 122, 126),
+                self.MakeParserRequest(4, 3, 127, 128),
+                self.MakeParserRequest(5, 'null', 100, 105),
+                self.MakeParserRequest(6, 5, 105, 110)]
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [4])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [6])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 5, 1, 2, 6, 3, 4])
+    self.assertEqual(28, graph.Cost())
+    graph.Set(cache_all=True)
+    self.assertEqual(8, graph.Cost())
+
+  def test_MaxPath(self):
+    requests = [self.MakeParserRequest(0, 'null', 100, 110),
+                self.MakeParserRequest(1, 0, 115, 120),
+                self.MakeParserRequest(2, 0, 112, 120),
+                self.MakeParserRequest(3, 1, 122, 126),
+                self.MakeParserRequest(4, 3, 127, 128),
+                self.MakeParserRequest(5, 'null', 100, 105),
+                self.MakeParserRequest(6, 5, 105, 110)]
+    graph = loading_model.ResourceGraph(requests)
+    path_list = []
+    self.assertEqual(28, graph.Cost(path_list))
+    self.assertEqual([0, 1, 3, 4], [n.Index() for n in path_list])
+
+    # More interesting would be a test when a node has multiple predecessors,
+    # but it's not possible for us to construct such a graph from requests yet.
+
+  def test_TimingSplit(self):
+    # Timing adds node 1 as a parent to 2 but not 3.
+    requests = [self.MakeParserRequest(0, 'null', 100, 110,
+                                       magic_content_type=True),
+                self.MakeParserRequest(1, 0, 115, 120,
+                                       magic_content_type=True),
+                self.MakeParserRequest(2, 0, 121, 122,
+                                       magic_content_type=True),
+                self.MakeParserRequest(3, 0, 112, 119),
+                self.MakeParserRequest(4, 2, 122, 126),
+                self.MakeParserRequest(5, 2, 122, 126)]
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
+
+    # Change node 1 so it is a parent of 3, which become parent of 2.
+    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
+                                         magic_content_type=True)
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 3, 2, 4, 5])
+
+    # Add an initiator dependence to 1 that will become the parent of 3.
+    requests[1] = self.MakeParserRequest(1, 0, 110, 111)
+    requests.append(self.MakeParserRequest(6, 1, 111, 112))
+    graph = loading_model.ResourceGraph(requests)
+    # Check it doesn't change until we change the content type of 1.
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [3, 6])
+    requests[1] = self.MakeParserRequest(1, 0, 110, 111,
+                                         magic_content_type=True)
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [6])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [2])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[6]), [3])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 6, 3, 2, 4, 5])
+
+  def test_TimingSplitImage(self):
+    # If we're all image types, then we shouldn't split by timing.
+    requests = [self.MakeParserRequest(0, 'null', 100, 110),
+                self.MakeParserRequest(1, 0, 115, 120),
+                self.MakeParserRequest(2, 0, 121, 122),
+                self.MakeParserRequest(3, 0, 112, 119),
+                self.MakeParserRequest(4, 2, 122, 126),
+                self.MakeParserRequest(5, 2, 122, 126)]
+    for r in requests:
+      r.headers['Content-Type'] = 'image/gif'
+    graph = loading_model.ResourceGraph(requests)
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[0]), [1, 2, 3])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[1]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[2]), [4, 5])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[3]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[4]), [])
+    self.assertEqual(self.SuccessorIndicies(graph._nodes[5]), [])
+    self.assertEqual(self.SortedIndicies(graph), [0, 1, 2, 3, 4, 5])
+
+  def test_AdUrl(self):
+    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
+        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/test.png'))
+    self.assertFalse(loading_model.ResourceGraph._IsAdUrl(
+        'http://afae61024b33032ef.profile.sfo20.cloudfront.net/tst.png'))
+
+    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
+        'http://ums.adtechus.com/mapuser?providerid=1003;userid=RUmecco4z3o===='))
+    self.assertTrue(loading_model.ResourceGraph._IsAdUrl(
+        'http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'))
+
+
+if __name__ == '__main__':
+  unittest.main()
diff --git a/loading/log_parser.py b/loading/log_parser.py
index e77fd4d..a58bd55 100644
--- a/loading/log_parser.py
+++ b/loading/log_parser.py
@@ -9,13 +9,11 @@ import json
 import operator
 import urlparse
 
-
 Timing = collections.namedtuple(
     'Timing',
     ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
      'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
-     'serviceWorkerFetchEnd', 'serviceWorkerFetchReady',
-     'serviceWorkerFetchStart', 'sslEnd', 'sslStart'])
+     'sslEnd', 'sslStart', 'workerReady', 'workerStart', 'loadingFinished'])
 
 
 class Resource(object):
@@ -33,7 +31,7 @@ class Resource(object):
 
   def GetShortName(self):
     """Returns either the hostname of the resource, or the filename,
-    or the end of the path.
+    or the end of the path. Tries to include the domain as much as possible.
     """
     parsed = urlparse.urlparse(self.url)
     path = parsed.path
@@ -41,17 +39,22 @@ class Resource(object):
       last_path = parsed.path.split('/')[-1]
       if len(last_path) < 10:
         if len(path) < 10:
-          return path
+          return parsed.hostname + '/' + path
         else:
-          return parsed.path[-10:]
+          return parsed.hostname + '/..' + parsed.path[-10:]
+      elif len(last_path) > 10:
+        return parsed.hostname + '/..' + last_path[:5]
       else:
-        return last_path
+        return parsed.hostname + '/..' + last_path
     else:
       return parsed.hostname
 
   def GetContentType(self):
     mime = self.content_type
-    if mime == 'text/html':
+    if 'magic-debug-content' in mime:
+      # A silly hack to make the unittesting easier.
+      return 'magic-debug-content'
+    elif mime == 'text/html':
       return 'html'
     elif mime == 'text/css':
       return 'css'
diff --git a/loading/log_requests.py b/loading/log_requests.py
index 16a10d5..071870c 100755
--- a/loading/log_requests.py
+++ b/loading/log_requests.py
@@ -42,6 +42,11 @@ def FlagChanger(device, command_line_path, new_flags):
     command_line_path: Full path to the command-line file.
     new_flags: Flags to add.
   """
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
   changer = flag_changer.FlagChanger(device, command_line_path)
   changer.AddFlags(new_flags)
   try:
@@ -53,6 +58,11 @@ def FlagChanger(device, command_line_path, new_flags):
 @contextlib.contextmanager
 def ForwardPort(device, local, remote):
   """Forwards a local port to a remote one on a device in a context."""
+  # If we're logging requests from a local desktop chrome instance there is no
+  # device.
+  if not device:
+    yield
+    return
   device.adb.Forward(local, remote)
   try:
     yield
@@ -70,9 +80,11 @@ class AndroidRequestsLogger(object):
   """Logs all the requests made to load a page on a device."""
 
   def __init__(self, device):
+    """If device is None, we connect to a local chrome session."""
     self.device = device
     self._please_stop = False
     self._main_frame_id = None
+    self._tracing_data = []
 
   def _PageDataReceived(self, msg):
     """Called when a Page event is received.
@@ -93,6 +105,9 @@ class AndroidRequestsLogger(object):
           and params['frameId'] == self._main_frame_id):
       self._please_stop = True
 
+  def _TracingDataReceived(self, msg):
+    self._tracing_data.append(msg)
+
   def _LogPageLoadInternal(self, url, clear_cache):
     """Returns the collection of requests made to load a given URL.
 
@@ -132,32 +147,93 @@ class AndroidRequestsLogger(object):
       except websocket.WebSocketTimeoutException as e:
         logging.warning('Exception: ' + str(e))
         break
+    if not self._please_stop:
+      logging.warning('Finished with timeout instead of page load')
     inspector.StopMonitoringNetwork()
     return inspector.GetResponseData()
 
-  def LogPageLoad(self, url, clear_cache):
-    """Returns the collection of requests made to load a given URL on a device.
+  def _LogTracingInternal(self, url):
+    self._main_frame_id = None
+    self._please_stop = False
+    r = httplib.HTTPConnection('localhost', _PORT)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      logging.error('Cannot connect to the remote target.')
+      return None
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    ws.RegisterDomain('Tracing', self._TracingDataReceived)
+    logging.warning('Tracing.start: ' +
+                    str(ws.SyncRequest({'method': 'Tracing.start',
+                                        'options': 'zork'})))
+    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
+                              'params': {'url': url}})
+    while not self._please_stop:
+      try:
+        ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException:
+        break
+    if not self._please_stop:
+      logging.warning('Finished with timeout instead of page load')
+    return {'events': self._tracing_data,
+            'end': ws.SyncRequest({'method': 'Tracing.end'})}
+
+  def _DoSomeLogging(self, package, fn):
+    """Start logging process.
+
+    Sets up any device and tracing appropriately and then executes the core
+    logging function.
 
     Args:
-      url: (str) URL to load on the device.
-      clear_cache: (bool) Whether to clear the HTTP cache.
+      package: the key for chrome package info.
+      fn: the function to execute that launches chrome and performs the
+      appropriate instrumentation, see _Log*Internal().
 
     Returns:
-      See _LogPageLoadInternal().
+      As fn() returns.
     """
-    package_info = constants.PACKAGE_INFO['chrome']
+    package_info = constants.PACKAGE_INFO[package]
     command_line_path = '/data/local/chrome-command-line'
     new_flags = ['--enable-test-events', '--remote-debugging-port=%d' % _PORT]
-    _SetUpDevice(self.device, package_info)
+    if self.device:
+      _SetUpDevice(self.device, package_info)
     with FlagChanger(self.device, command_line_path, new_flags):
-      start_intent = intent.Intent(
-          package=package_info.package, activity=package_info.activity,
-          data='about:blank')
-      self.device.StartActivity(start_intent, blocking=True)
-      time.sleep(2)
+      if self.device:
+        start_intent = intent.Intent(
+            package=package_info.package, activity=package_info.activity,
+            data='about:blank')
+        self.device.StartActivity(start_intent, blocking=True)
+        time.sleep(2)
+      # If no device, we don't care about chrome startup so skip the about page.
       with ForwardPort(self.device, 'tcp:%d' % _PORT,
                        'localabstract:chrome_devtools_remote'):
-        return self._LogPageLoadInternal(url, clear_cache)
+        return fn()
+
+  def LogPageLoad(self, url, clear_cache, package):
+    """Returns the collection of requests made to load a given URL on a device.
+
+    Args:
+      url: (str) URL to load on the device.
+      clear_cache: (bool) Whether to clear the HTTP cache.
+
+    Returns:
+      See _LogPageLoadInternal().
+    """
+    return self._DoSomeLogging(
+        package, lambda: self._LogPageLoadInternal(url, clear_cache))
+
+  def LogTracing(self, url):
+    """Log tracing events from a load of the given URL.
+
+    TODO(mattcary): This doesn't work. It would be best to log tracing
+    simultaneously with network requests, but as that wasn't working the tracing
+    logging was broken out separately. It still doesn't work...
+    """
+    return self._DoSomeLogging('chrome', lambda: self._LogTracingInternal(url))
 
 
 def _ResponseDataToJson(data):
@@ -197,16 +273,26 @@ def _CreateOptionParser():
   parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
                                               'before loading the URL.'),
                     default=True, action='store_false', dest='clear_cache')
+  parser.add_option('--package', help='Package info for chrome build. '
+                                      'See build/android/pylib/constants.',
+                    default='chrome')
+  parser.add_option('--local', action='store_true', default=False,
+                    help='Connect to local chrome session rather than android.')
   return parser
 
 
 def main():
+  logging.basicConfig(level=logging.WARNING)
   parser = _CreateOptionParser()
   options, _ = parser.parse_args()
-  devices = device_utils.DeviceUtils.HealthyDevices()
-  device = devices[0]
+  if options.local:
+    device = None
+  else:
+    devices = device_utils.DeviceUtils.HealthyDevices()
+    device = devices[0]
   request_logger = AndroidRequestsLogger(device)
-  response_data = request_logger.LogPageLoad(options.url, options.clear_cache)
+  response_data = request_logger.LogPageLoad(
+      options.url, options.clear_cache, options.package)
   json_data = _ResponseDataToJson(response_data)
   with open(options.output, 'w') as f:
     f.write(json_data)
diff --git a/loading/node_cost_csv.py b/loading/node_cost_csv.py
new file mode 100755
index 0000000..79d3b06
--- /dev/null
+++ b/loading/node_cost_csv.py
@@ -0,0 +1,61 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import argparse
+import logging
+import os
+import sys
+
+from processing import (SitesFromDir, WarmGraph, ColdGraph)
+
+
+def main():
+  logging.basicConfig(level=logging.ERROR)
+  parser = argparse.ArgumentParser(
+      description=('Convert a directory created by ./analyze.py fetch '
+                   'to a node cost CSV which compares cold and warm total '
+                   'node costs.'))
+  parser.add_argument('--datadir', required=True)
+  parser.add_argument('--csv', required=True)
+  parser.add_argument('--noads', action='store_true')
+  args = parser.parse_args()
+  sites = SitesFromDir(args.datadir)
+  with open(args.csv, 'w') as output:
+    output.write('site,cold.total,warm.total,cold.common,warm.common,'
+                 'cold.node.count,common.cold.node.count,'
+                 'cold.all.edges,warm.all.edges,'
+                 'cold.common.edges,warm.common.edges,'
+                 'cold.edge.fraction,common.cold.edge.fraction\n')
+    for site in sites:
+      print site
+      warm = WarmGraph(args.datadir, site)
+      if args.noads:
+        warm.Set(node_filter=warm.FilterAds)
+      cold = ColdGraph(args.datadir, site)
+      if args.noads:
+        cold.Set(node_filter=cold.FilterAds)
+      common = [p for p in cold.Intersect(warm.Nodes())]
+      common_cold = set([c.Node() for c, w in common])
+      common_warm = set([w.Node() for c, w in common])
+      output.write(','.join([str(s) for s in [
+          site,
+          sum((n.NodeCost() for n in cold.Nodes())),
+          sum((n.NodeCost() for n in warm.Nodes())),
+          sum((c.NodeCost() for c, w in common)),
+          sum((w.NodeCost() for c, w in common)),
+          sum((1 for n in cold.Nodes())),
+          len(common),
+          cold.EdgeCosts(), warm.EdgeCosts(),
+          cold.EdgeCosts(lambda n: n in common_cold),
+          warm.EdgeCosts(lambda n: n in common_warm),
+          (cold.EdgeCosts() /
+           sum((n.NodeCost() for n in cold.Nodes()))),
+          (cold.EdgeCosts(lambda n: n in common_cold) /
+           sum((c.NodeCost() for c, w in common)))
+          ]]) + '\n')
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/processing.py b/loading/processing.py
new file mode 100644
index 0000000..a6ddde1
--- /dev/null
+++ b/loading/processing.py
@@ -0,0 +1,71 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import os
+import os.path
+import sys
+
+import log_parser
+import loading_model
+
+
+def SitesFromDir(dir):
+  """Extract sites from a data dir.
+
+  Based on ./analyze.py fetch file name conventions. We assume each site
+  corresponds to two files, <site>.json and <site>.json.cold, and that no other
+  kind of file appears in the data directory.
+
+  Args:
+    dir: the directory to process.
+
+  Returns:
+    A list of sites as strings.
+
+  """
+  files = set(os.listdir(dir))
+  assert files
+  sites = []
+  for f in files:
+    if f.endswith('.png'): continue
+    assert f.endswith('.json') or f.endswith('.json.cold'), f
+    if f.endswith('.json'):
+      assert f + '.cold' in files
+      sites.append(f[:f.rfind('.json')])
+    elif f.endswith('.cold'):
+      assert f[:f.rfind('.cold')] in files
+  sites.sort()
+  return sites
+
+
+def WarmGraph(datadir, site):
+  """Return a loading model graph for the warm pull of site.
+
+  Based on ./analyze.py fetch file name conventions.
+
+  Args:
+    datadir: the directory containing site JSON data.
+    site: a site string.
+
+  Returns:
+    A loading model object.
+  """
+  return loading_model.ResourceGraph(log_parser.FilterRequests(
+      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json'))))
+
+
+def ColdGraph(datadir, site):
+  """Return a loading model graph for the cold pull of site.
+
+  Based on ./analyze.py fetch file name conventions.
+
+  Args:
+    datadir: the directory containing site JSON data.
+    site: a site string.
+
+  Returns:
+    A loading model object.
+  """
+  return loading_model.ResourceGraph(log_parser.FilterRequests(
+      log_parser.ParseJsonFile(os.path.join(datadir, site + '.json.cold'))))
diff --git a/loading/util.r b/loading/util.r
new file mode 100644
index 0000000..a65c572
--- /dev/null
+++ b/loading/util.r
@@ -0,0 +1,47 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Useful R routines for analyzing output from several cost_to_csv.py
+# output and producing interesting graphs.
+
+combine.runs <- function(times, prefix, suffix)
+  do.call("rbind", lapply(times, function (t)
+    with(read.csv(paste0(prefix, t, suffix)),
+         data.frame(site, kind, cost, time=t))))
+
+get.ordered.names <- function(runs) {
+  means <- with(runs, tapply(cost, list(site, kind), mean))
+  return(names(means[,"cold"])[order(means[,"cold"])])
+}
+
+plot.warm.cold <- function(runs, main="") {
+  ordered.names <- get.ordered.names(runs)
+  n <- length(ordered.names)
+  par(mar=c(8,4,4,4), bg="white")
+  plot(NULL, xlim=c(1,25), ylim=range(runs$cost), xaxt="n",
+       ylab="ms", xlab="", main=main)
+  axis(1, 1:n, labels=ordered.names, las=2)
+  getdata <- function(k, t) sapply(
+      ordered.names, function (s) with(runs, cost[site==s & kind==k & time==t]))
+  for (t in unique(runs$time)) {
+    points(1:n, getdata("cold", t), pch=1)
+    points(1:n, getdata("warm", t), pch=3)
+  }
+  legend("topleft", pch=c(1, 3), legend=c("cold", "warm"))
+}
+
+plot.relative.sds <- function(runs, main="") {
+  sds <- with(runs, tapply(cost, list(site, kind), sd))
+  means <- with(runs, tapply(cost, list(site, kind), mean))
+  ordered.names <- get.ordered.names(runs)
+  n <- length(ordered.names)
+  par(mar=c(8,4,4,4), bg="white")
+  plot(NULL, xlim=c(1,25), ylim=c(0,.8),
+       xaxt="n", ylab="Relative SD", xlab="", main=main)
+  axis(1, 1:n, labels=ordered.names, las=2)
+  getdata <- function(k) sapply(ordered.names, function(s) (sds/means)[s, k])
+  points(1:n, getdata("cold"), pch=1)
+  points(1:n, getdata("warm"), pch=3)
+  legend("topleft", pch=c(1, 3), legend=c("cold", "warm"))
+}

commit fa2b2135fd3d1946be06e49c195b4fa9c6b90fc9
Author: wangxianzhu <wangxianzhu@chromium.org>
Date:   Fri Jan 8 15:53:24 2016 -0800

    Fix crash in device_forwarder
    
    There are many crashes in device_forwarder on bots, e.g.
    https://build.chromium.org/p/tryserver.chromium.android/builders/linux_android_rel_ng/builds/5713/steps/stack_tool_for_tombstones/logs/stdio/text
    
    The original method was incorrect because
    ServiceDelegate::DeleteControllerOnInternalThread()
    may execute after ServiceDelegate::~ServiceDelegate().
    
    Review URL: https://codereview.chromium.org/1571643003
    
    Cr-Original-Commit-Position: refs/heads/master@{#368466}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 91b8d5ad02d99b59cb17e100eb645b2fefb4eb7f

diff --git a/forwarder2/device_forwarder_main.cc b/forwarder2/device_forwarder_main.cc
index fdf5fe5..09790f9 100644
--- a/forwarder2/device_forwarder_main.cc
+++ b/forwarder2/device_forwarder_main.cc
@@ -52,19 +52,9 @@ class ServerDelegate : public Daemon::ServerDelegate {
     if (!controller_thread_.get())
       return;
     // The DeviceController instance, if any, is constructed on the controller
-    // thread. Make sure that it gets deleted on that same thread. Note that
-    // DeleteSoon() is not used here since it would imply reading |controller_|
-    // from the main thread while it's set on the internal thread.
-    controller_thread_->task_runner()->PostTask(
-        FROM_HERE,
-        base::Bind(&ServerDelegate::DeleteControllerOnInternalThread,
-                   base::Unretained(this)));
-  }
-
-  void DeleteControllerOnInternalThread() {
-    DCHECK(
-        controller_thread_->task_runner()->RunsTasksOnCurrentThread());
-    controller_.reset();
+    // thread. Make sure that it gets deleted on that same thread.
+    controller_thread_->task_runner()->DeleteSoon(
+        FROM_HERE, controller_.release());
   }
 
   // Daemon::ServerDelegate:

commit 837d21cc509b7b52049f515b523a7450be851df0
Author: thakis <thakis@chromium.org>
Date:   Fri Jan 8 11:37:43 2016 -0800

    Enable warning on reserved user-defined literals everywhere but CrOS.
    
    This was disabled because a dbus header was missing a few spaces.
    The version of the dbus header in the sysroot has them though,
    and we use the sysroot everywhere except in Chrome OS-on-Linux
    builds these days.
    
    The spaces were added here, almost 3 years ago by now:
    http://cgit.freedesktop.org/dbus/dbus/commit/?h=dbus-1.4&id=51b88b4c7919487290c0862b013cd8e7cd2de34b
    
    No behavior change.
    
    BUG=263960, 573778
    
    Review URL: https://codereview.chromium.org/1570193002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368408}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 6fc6aa0987377a5d5633103de9c153fcccbf2ed8

diff --git a/memdump/memdump.cc b/memdump/memdump.cc
index 0564f85..7f135ee 100644
--- a/memdump/memdump.cc
+++ b/memdump/memdump.cc
@@ -422,9 +422,9 @@ void DumpProcessesMemoryMapsInExtendedFormat(
       AppendAppSharedField(memory_map.app_shared_pages, &app_shared_buf);
       base::SStringPrintf(
           &buf,
-          "%"PRIx64"-%"PRIx64" %s %"PRIx64" private_unevictable=%d private=%d "
-          "shared_app=%s shared_other_unevictable=%d shared_other=%d "
-          "\"%s\" [%s]\n",
+          "%" PRIx64 "-%" PRIx64 " %s %" PRIx64 " private_unevictable=%d "
+          "private=%d shared_app=%s shared_other_unevictable=%d "
+          "shared_other=%d \"%s\" [%s]\n",
           memory_map.start_address,
           memory_map.end_address,
           memory_map.flags.c_str(),

commit 60002985a7fa9e3cd8e483abd4b052be5740227c
Author: lizeb <lizeb@chromium.org>
Date:   Fri Jan 8 07:52:58 2016 -0800

    customtabs: Add a test script for benchmarking.
    
    BUG=520967
    
    Review URL: https://codereview.chromium.org/1292363002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368337}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 76bb5b8d785094765773c926e54046effda39a9e

diff --git a/customtabs_benchmark/scripts/__init__.py b/customtabs_benchmark/scripts/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/customtabs_benchmark/scripts/customtabs_benchmark.py b/customtabs_benchmark/scripts/customtabs_benchmark.py
new file mode 100755
index 0000000..9bf4343
--- /dev/null
+++ b/customtabs_benchmark/scripts/customtabs_benchmark.py
@@ -0,0 +1,170 @@
+#!/usr/bin/python
+#
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loops Custom Tabs tests and outputs the results into a CSV file."""
+
+import logging
+import optparse
+import os
+import re
+import sys
+import time
+
+sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
+                             os.pardir, os.pardir, 'build', 'android'))
+
+from pylib.device import device_errors
+from pylib.device import device_utils
+from pylib.device import intent
+from pylib.perf import cache_control
+
+
+def RunOnce(device, url, warmup, no_prerendering, delay_to_may_launch_url,
+            delay_to_launch_url, cold):
+  """Runs a test on a device once.
+
+  Args:
+    device: (DeviceUtils) device to run the tests on.
+    warmup: (bool) Whether to call warmup.
+    no_prerendering: (bool) Whether to disable prerendering.
+    delay_to_may_launch_url: (int) Delay to mayLaunchUrl() in ms.
+    delay_to_launch_url: (int) Delay to launchUrl() in ms.
+    cold: (bool) Whether the page cache should be dropped.
+
+  Returns:
+    The output line (str), like this (one line only):
+    <warmup>,<no_prerendering>,<delay_to_may_launch_url>,<intent_sent_ms>,
+      <page_load_started_ms>,<page_load_finished_ms>
+    or None on error.
+  """
+  launch_intent = intent.Intent(
+      action='android.intent.action.MAIN',
+      package='org.chromium.customtabsclient.test',
+      activity='org.chromium.customtabs.test.MainActivity',
+      extras={'url': url, 'warmup': warmup, 'no_prerendering': no_prerendering,
+              'delay_to_may_launch_url': delay_to_may_launch_url,
+              'delay_to_launch_url': delay_to_launch_url})
+  result_line_re = re.compile(r'W/CUSTOMTABSBENCH.*: (.*)')
+  logcat_monitor = device.GetLogcatMonitor(clear=True)
+  logcat_monitor.Start()
+  device.ForceStop('com.google.android.apps.chrome')
+  device.ForceStop('org.chromium.customtabsclient.test')
+  if cold:
+    if not device.HasRoot():
+      device.EnableRoot()
+    cache_control.CacheControl(device).DropRamCaches()
+  device.StartActivity(launch_intent, blocking=True)
+  match = None
+  try:
+    match = logcat_monitor.WaitFor(result_line_re, timeout=10)
+  except device_errors.CommandTimeoutError as e:
+    logging.warning('Timeout waiting for the result line')
+  return match.group(1) if match is not None else None
+
+
+def LoopOnDevice(device, url, warmup, no_prerendering, delay_to_may_launch_url,
+                 delay_to_launch_url, cold, output_filename, once=False):
+  """Loops the tests on a device.
+
+  Args:
+    device: (DeviceUtils) device to run the tests on.
+    url: (str) URL to navigate to.
+    warmup: (bool) Whether to call warmup.
+    no_prerendering: (bool) Whether to disable prerendering.
+    delay_to_may_launch_url: (int) Delay to mayLaunchUrl() in ms.
+    delay_to_launch_url: (int) Delay to launchUrl() in ms.
+    cold: (bool) Whether the page cache should be dropped.
+    output_filename: (str) Output filename. '-' for stdout.
+    once: (bool) Run only once.
+  """
+  while True:
+    out = sys.stdout if output_filename == '-' else open(output_filename, 'a')
+    try:
+      result = RunOnce(device, url, warmup, no_prerendering,
+                       delay_to_may_launch_url, delay_to_launch_url, cold)
+      if result is not None:
+        out.write(result + '\n')
+        out.flush()
+      if once:
+        return
+      time.sleep(10)
+    finally:
+      if output_filename != '-':
+        out.close()
+
+
+def ProcessOutput(filename):
+  """Reads an output file, and returns a processed numpy array.
+
+  Args:
+    filename: (str) file to process.
+
+  Returns:
+    A numpy structured array.
+  """
+  import numpy as np
+  data = np.genfromtxt(filename, delimiter=',')
+  result = np.array(np.zeros(len(data)),
+                    dtype=[('warmup', bool), ('no_prerendering', bool),
+                           ('delay_to_may_launch_url', np.int32),
+                           ('delay_to_launch_url', np.int32),
+                           ('commit', np.int32), ('plt', np.int32)])
+  result['warmup'] = data[:, 0]
+  result['no_prerendering'] = data[:, 1]
+  result['delay_to_may_launch_url'] = data[:, 2]
+  result['delay_to_launch_url'] = data[:, 3]
+  result['commit'] = data[:, 5] - data[:, 4]
+  result['plt'] = data[:, 6] - data[:, 4]
+  return result
+
+
+def _CreateOptionParser():
+  parser = optparse.OptionParser(description='Loops Custom Tabs tests on a '
+                                 'device, and outputs the navigation timings '
+                                 'in a CSV file.')
+  parser.add_option('--device', help='Device ID')
+  parser.add_option('--url', help='URL to navigate to.',
+                    default='https://www.android.com')
+  parser.add_option('--warmup', help='Call warmup.', default=False,
+                    action='store_true')
+  parser.add_option('--no_prerendering', help='Disable prerendering.',
+                    default=False, action='store_true')
+  parser.add_option('--delay_to_may_launch_url',
+                    help='Delay before calling mayLaunchUrl() in ms.',
+                    type='int')
+  parser.add_option('--delay_to_launch_url',
+                    help='Delay before calling launchUrl() in ms.',
+                    type='int')
+  parser.add_option('--cold', help='Purge the page cache before each run.',
+                    default=False, action='store_true')
+  parser.add_option('--output_file', help='Output file (append). "-" for '
+                    'stdout')
+  parser.add_option('--once', help='Run only one iteration.',
+                    action='store_true', default=False)
+  return parser
+
+
+def main():
+  parser = _CreateOptionParser()
+  options, _ = parser.parse_args()
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  device = devices[0]
+  if len(devices) != 1 and options.device is None:
+    logging.error('Several devices attached, must specify one with --device.')
+    sys.exit(0)
+  if options.device is not None:
+    matching_devices = [d for d in devices if str(d) == options.device]
+    if len(matching_devices) == 0:
+      logging.error('Device not found.')
+      sys.exit(0)
+    device = matching_devices[0]
+  LoopOnDevice(device, options.url, options.warmup, options.no_prerendering,
+               options.delay_to_may_launch_url, options.delay_to_launch_url,
+               options.cold, options.output_file, options.once)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/customtabs_benchmark/scripts/run_benchmark.py b/customtabs_benchmark/scripts/run_benchmark.py
new file mode 100755
index 0000000..5df547b
--- /dev/null
+++ b/customtabs_benchmark/scripts/run_benchmark.py
@@ -0,0 +1,140 @@
+#!/usr/bin/env python
+#
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loops Custom Tabs tests and outputs the results into a CSV file."""
+
+import copy
+import json
+import logging
+import optparse
+import os
+import random
+import sys
+import threading
+
+sys.path.append(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir,
+                             os.pardir, os.pardir, 'build', 'android'))
+
+from pylib.device import device_utils
+
+import customtabs_benchmark
+
+
+_KEYS = ['url', 'warmup', 'no_prerendering', 'delay_to_may_launch_url',
+         'delay_to_launch_url', 'cold']
+
+
+def _ParseConfiguration(filename):
+  """Reads a JSON file and returns a list of configurations.
+
+  Each valid value in the JSON file can be either a scalar or a list of
+  values. This function expands the scalar values to be lists. All list must
+  have the same length.
+
+  Sample configuration:
+  {
+    "url": "https://www.android.com",
+    "warmup": [false, true],
+    "no_prerendering": false,
+    "delay_to_may_launch_url": [-1, 1000],
+    "delay_to_launch_url": [-1, 1000],
+    "cold": true
+  }
+
+  Args:
+    filename: (str) Point to a file containins a JSON dictionnary of config
+              values.
+
+  Returns:
+    A list of configurations, where each value is specified.
+  """
+  config = json.load(open(filename, 'r'))
+  has_all_values = all(k in config for k in _KEYS)
+  assert has_all_values
+  config['url'] = str(config['url']) # Intents don't like unicode.
+  has_list = any(isinstance(config[k], list) for k in _KEYS)
+  if not has_list:
+    return [config]
+  list_keys = [k for k in _KEYS if isinstance(config[k], list)]
+  list_length = len(config[list_keys[0]])
+  assert all(len(config[k]) == list_length for k in list_keys)
+  result = []
+  for i in range(list_length):
+    result.append(copy.deepcopy(config))
+    for k in list_keys:
+      result[-1][k] = result[-1][k][i]
+  return result
+
+
+def _CreateOptionParser():
+  parser = optparse.OptionParser(description='Loops tests on all attached '
+                                 'devices, with randomly selected '
+                                 'configurations, and outputs the results in '
+                                 'CSV files.')
+  parser.add_option('--config', help='JSON configuration file. Required.')
+  parser.add_option('--output_file_prefix', help='Output file prefix. Actual '
+                    'output file is prefix_<device ID>.csv', default='result')
+  return parser
+
+
+def _RunOnDevice(device, output_filename, configs, should_stop):
+  """Loops the tests described by configs on a device.
+
+  Args:
+    device: (DeviceUtils) device to run the tests on.
+    output_filename: (str) Output file name.
+    configs: (list of dict) List of configurations.
+    should_stop: (Event) When set, this function should return.
+  """
+  with open(output_filename, 'a') as f:
+    while not should_stop.is_set():
+      config = configs[random.randint(0, len(configs) - 1)]
+      result = customtabs_benchmark.RunOnce(
+          device, config['url'], config['warmup'], config['no_prerendering'],
+          config['delay_to_may_launch_url'], config['delay_to_launch_url'],
+          config['cold'])
+      if result is not None:
+        f.write(result + '\n')
+        f.flush()
+      should_stop.wait(10.)
+
+
+def _Run(output_file_prefix, configs):
+  """Loops the tests described by the configs on connected devices.
+
+  Args:
+    output_file_prefix: (str) Prefix for the output file name.
+    configs: (list of dict) List of configurations.
+  """
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  should_stop = threading.Event()
+  threads = []
+  for device in devices:
+    output_filename = '%s_%s.csv' % (output_file_prefix, str(device))
+    thread = threading.Thread(
+        target=_RunOnDevice,
+        args=(device, output_filename, configs, should_stop))
+    thread.start()
+    threads.append(thread)
+  for thread in threads:
+    try:
+      thread.join()
+    except KeyboardInterrupt as e:
+      should_stop.set()
+
+
+def main():
+  parser = _CreateOptionParser()
+  options, _ = parser.parse_args()
+  if options.config is None:
+    logging.error('A configuration file must be provided.')
+    sys.exit(0)
+  configs = _ParseConfiguration(options.config)
+  _Run(options.output_file_prefix, configs)
+
+
+if __name__ == '__main__':
+  main()

commit daf76dc482f89d42dcbacddb0198a0c5be71f894
Author: pasko <pasko@chromium.org>
Date:   Fri Jan 8 07:16:56 2016 -0800

    Add OWNERS for tools/android/customtabs_benchmark
    
    BUG=none
    
    Review URL: https://codereview.chromium.org/1573463002
    
    Cr-Original-Commit-Position: refs/heads/master@{#368331}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 4caac96fa8857a46100feeac45c74cef264c0cb4

diff --git a/customtabs_benchmark/OWNERS b/customtabs_benchmark/OWNERS
new file mode 100644
index 0000000..c319c21
--- /dev/null
+++ b/customtabs_benchmark/OWNERS
@@ -0,0 +1,3 @@
+lizeb@chromium.org
+pasko@chromium.org
+yusufo@chromium.org

commit 05879a9bdf44908492f2984c29aa6b10f635719b
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 5 11:00:14 2016 -0800

    tools/android/loading: Add service worker timings.
    
    Names have changed, making the current version crash.
    
    Review URL: https://codereview.chromium.org/1555343002
    
    Cr-Original-Commit-Position: refs/heads/master@{#367597}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 9a43baf66057981abc88abaed50523ce6f4b5e00

diff --git a/loading/log_parser.py b/loading/log_parser.py
index bb2f953..e77fd4d 100644
--- a/loading/log_parser.py
+++ b/loading/log_parser.py
@@ -14,7 +14,8 @@ Timing = collections.namedtuple(
     'Timing',
     ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
      'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
-     'sslEnd', 'sslStart', 'workerReady', 'workerStart'])
+     'serviceWorkerFetchEnd', 'serviceWorkerFetchReady',
+     'serviceWorkerFetchStart', 'sslEnd', 'sslStart'])
 
 
 class Resource(object):

commit 42172ce9ef5ffc04ac7d8a5ab13227e5d3bb729d
Author: zqzhang <zqzhang@chromium.org>
Date:   Tue Jan 5 09:58:56 2016 -0800

    A simple app for simulating audio focus actions for testing MediaSession
    
    We introduce a simple app, called "AudioFocusGrabber", for testing audio
    focus handling in apps, especially MediaSession in Chrome. It can
    simulate a short sound when receiving an SMS or email, requesting the
    current playback to pause or lower its volume for a short while. It can
    also simulate the situation that another media player app gains the
    audio focus permanently, where the app under test need to pause the
    playback until the user explicitly resumes it.
    
    BUG=
    
    Review URL: https://codereview.chromium.org/1513743002
    
    Cr-Original-Commit-Position: refs/heads/master@{#367571}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 0dfd8454bbdc16a1e3a12d8c8889de9ca1508407

diff --git a/BUILD.gn b/BUILD.gn
index 102072f..012dc35 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -68,3 +68,11 @@ group("customtabs_benchmark") {
     "//tools/android/customtabs_benchmark:customtabs_benchmark_apk",
   ]
 }
+
+# GYP: //tools/android/android_tools.gyp:audio_focus_grabber
+group("audio_focus_grabber") {
+  testonly = true
+  deps = [
+    "//tools/android/audio_focus_grabber:audio_focus_grabber_apk",
+  ]
+}
diff --git a/android_tools.gyp b/android_tools.gyp
index 152636a..b963b3f 100644
--- a/android_tools.gyp
+++ b/android_tools.gyp
@@ -77,5 +77,13 @@
         'customtabs_benchmark/customtabs_benchmark.gyp:customtabs_benchmark_apk',
       ],
     },
+    {
+      # GN: //tools/android:audio_focus_grabber
+      'target_name': 'audio_focus_grabber',
+      'type': 'none',
+      'dependencies': [
+        'audio_focus_grabber/audio_focus_grabber.gyp:audio_focus_grabber_apk',
+      ],
+    },
   ],
 }
diff --git a/audio_focus_grabber/BUILD.gn b/audio_focus_grabber/BUILD.gn
new file mode 100644
index 0000000..94ea751
--- /dev/null
+++ b/audio_focus_grabber/BUILD.gn
@@ -0,0 +1,29 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+import("//build/config/android/rules.gni")
+
+# GYP: //tools/android/audio_focus_grabber/audio_focus_grabber.gyp:audio_focus_grabber_apk
+android_apk("audio_focus_grabber_apk") {
+  testonly = true
+  android_manifest = "java/AndroidManifest.xml"
+  apk_name = "AudioFocusGrabber"
+
+  deps = [
+    ":audio_focus_grabber_apk_resources",
+    "//base:base_java",
+    "//third_party/android_tools:android_support_v13_java",
+  ]
+
+  java_files = [
+    "java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java",
+    "java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java",
+  ]
+}
+
+android_resources("audio_focus_grabber_apk_resources") {
+  testonly = true
+  resource_dirs = [ "java/res" ]
+  android_manifest = "java/AndroidManifest.xml"
+}
diff --git a/audio_focus_grabber/OWNERS b/audio_focus_grabber/OWNERS
new file mode 100644
index 0000000..5ad525b
--- /dev/null
+++ b/audio_focus_grabber/OWNERS
@@ -0,0 +1 @@
+zqzhang@chromium.org
diff --git a/audio_focus_grabber/README.md b/audio_focus_grabber/README.md
new file mode 100644
index 0000000..9640898
--- /dev/null
+++ b/audio_focus_grabber/README.md
@@ -0,0 +1,50 @@
+## AudioFocusGrabber: a Tool for Testing Audio Focus Handling in Apps
+
+A simple app used to test audio focus handling in apps, especially
+MediaSession in Chrome. You can perform audio gain/abandon actions, in
+order to simulate a short ping from an SMS or email, or permanent
+audio focus gain from other media player apps.
+
+### Setup
+
+#### 1: Build and install the AudioFocusGrabber app
+
+	ninja -C out/Debug audio_focus_grabber_apk
+	adb install -r out/Debug/apks/AudioFocusGrabber.apk
+
+#### 2: Simulate audio focus actions
+
+You can simulate audio focus actions using the UI, the notification
+bar or through `adb` shell. There are three kinds of audio focus
+actions, corresponding to:
+
+* `AudioManager.AUDIOFOCUS_GAIN`
+* `AudioManager.AUDIOFOCUS_TRANSIENT`
+* `AudioManager.AUDIOFOCUS_TRANSIENT_MAY_DUCK`
+
+##### 2.1: Controlling from the UI
+
+From the UI, there are three buttons for the three actions. Just click
+it, and AudioFocusGrabber will perform the audio focus action, and play a ping
+sound.
+
+However in this way, the app must be in background.
+
+##### 2.2: Controlling from the notification
+
+You can also start a notification from the UI, and then you can make
+controls from the notification.
+
+In this way, the app must be in background or losed window focus.
+
+##### 2.3 Controlling from the `adb` shell
+
+From the `adb` shell, which you can do it even if the AudioFocusGrabber is not
+in foreground. You may use the following three commands:
+
+	adb shell am startservice -a AUDIO_FOCUS_GRABBER_GAIN
+	adb shell am startservice -a AUDIO_FOCUS_GRABBER_TRANSIENT_PAUSE
+	adb shell am startservice -a AUDIO_FOCUS_GRABBER_TRANSIENT_DUCK
+
+In this way, the app may be in the foreground, in the background or
+losed window focus.
diff --git a/audio_focus_grabber/audio_focus_grabber.gyp b/audio_focus_grabber/audio_focus_grabber.gyp
new file mode 100644
index 0000000..bad0432
--- /dev/null
+++ b/audio_focus_grabber/audio_focus_grabber.gyp
@@ -0,0 +1,22 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+{
+  'targets': [
+    {
+      # GN: //tools/android/audio_focus_grabber:audio_focus_grabber_apk
+      'target_name': 'audio_focus_grabber_apk',
+      'type': 'none',
+      'variables': {
+        'apk_name': 'AudioFocusGrabber',
+        'android_manifest_path': 'java/AndroidManifest.xml',
+        'java_in_dir': 'java',
+        'resource_dir': 'java/res',
+      },
+      'dependencies': [
+        '../../../base/base.gyp:base_java',
+        '../../../third_party/android_tools/android_tools.gyp:android_support_v13_javalib'
+      ],
+      'includes': [ '../../../build/java_apk.gypi' ],
+  }]
+}
diff --git a/audio_focus_grabber/java/AndroidManifest.xml b/audio_focus_grabber/java/AndroidManifest.xml
new file mode 100644
index 0000000..e83b915
--- /dev/null
+++ b/audio_focus_grabber/java/AndroidManifest.xml
@@ -0,0 +1,37 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2016 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<manifest xmlns:android="http://schemas.android.com/apk/res/android"
+    package="org.chromium.tools.audio_focus_grabber" >
+
+    <uses-sdk android:minSdkVersion="16" android:targetSdkVersion="23" />
+
+    <application
+        android:label="@string/app_name" >
+
+        <activity
+            android:name="org.chromium.tools.audio_focus_grabber.AudioFocusGrabberActivity"
+            android:label="@string/app_name" >
+            <intent-filter>
+                <action android:name="android.intent.action.MAIN" />
+                <category android:name="android.intent.category.LAUNCHER" />
+            </intent-filter>
+        </activity>
+
+        <service
+            android:name="org.chromium.tools.audio_focus_grabber.AudioFocusGrabberListenerService"
+            android:exported="true" >
+            <intent-filter>
+                <action android:name="AUDIO_FOCUS_GRABBER_GAIN" />
+                <action android:name="AUDIO_FOCUS_GRABBER_TRANSIENT_PAUSE" />
+                <action android:name="AUDIO_FOCUS_GRABBER_TRANSIENT_DUCK" />
+            </intent-filter>
+        </service>
+
+    </application>
+
+</manifest>
diff --git a/audio_focus_grabber/java/res/drawable-hdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-hdpi/notification_icon.png
new file mode 100644
index 0000000..ca6ce5f
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-hdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-mdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-mdpi/notification_icon.png
new file mode 100644
index 0000000..82d17aa
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-mdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-xhdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-xhdpi/notification_icon.png
new file mode 100644
index 0000000..833485e
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-xhdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-xxhdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-xxhdpi/notification_icon.png
new file mode 100644
index 0000000..9eaf533
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-xxhdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/drawable-xxxhdpi/notification_icon.png b/audio_focus_grabber/java/res/drawable-xxxhdpi/notification_icon.png
new file mode 100644
index 0000000..5c0f943
Binary files /dev/null and b/audio_focus_grabber/java/res/drawable-xxxhdpi/notification_icon.png differ
diff --git a/audio_focus_grabber/java/res/layout/audio_focus_grabber_activity.xml b/audio_focus_grabber/java/res/layout/audio_focus_grabber_activity.xml
new file mode 100644
index 0000000..101b2e6
--- /dev/null
+++ b/audio_focus_grabber/java/res/layout/audio_focus_grabber_activity.xml
@@ -0,0 +1,80 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2015 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+    android:orientation="vertical"
+    android:layout_width="match_parent"
+    android:layout_height="match_parent"
+    android:layout_margin="20dp" >
+
+    <TextView
+        android:id="@+id/message"
+        android:layout_gravity="center_horizontal|top"
+        android:layout_width="match_parent"
+        android:layout_height="wrap_content"
+        android:layout_weight="1"
+        android:text="" />
+
+    <LinearLayout
+        android:orientation="horizontal"
+        android:layout_gravity="center_horizontal|bottom"
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:layout_weight="0"
+        style="?android:attr/buttonBarStyle" >
+
+        <Button
+            android:id="@+id/button_gain"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_gain_name"
+            android:onClick="onButtonClicked"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/button_transient_pause"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_pause_name"
+            android:onClick="onButtonClicked"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/button_transient_duck"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_duck_name"
+            android:onClick="onButtonClicked"
+            style="?android:attr/buttonBarButtonStyle" />
+
+    </LinearLayout>
+
+    <LinearLayout
+        android:orientation="horizontal"
+        android:layout_gravity="center_horizontal|bottom"
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:layout_weight="0" >
+
+        <TextView
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/text_notification_prompt" />
+        <Button
+            android:id="@+id/button_show_notification"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_show_notification_name"
+            android:onClick="onButtonClicked" />
+        <Button
+            android:id="@+id/button_hide_notification"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_hide_notification_name"
+            android:onClick="onButtonClicked" />
+
+    </LinearLayout>
+
+</LinearLayout>
diff --git a/audio_focus_grabber/java/res/layout/audio_focus_grabber_notification_bar.xml b/audio_focus_grabber/java/res/layout/audio_focus_grabber_notification_bar.xml
new file mode 100644
index 0000000..ec7ffd8
--- /dev/null
+++ b/audio_focus_grabber/java/res/layout/audio_focus_grabber_notification_bar.xml
@@ -0,0 +1,54 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2015 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
+    android:layout_width="match_parent"
+    android:layout_height="wrap_content"
+    android:gravity="center_vertical"
+    android:orientation="vertical"
+    style="?android:attr/buttonBarStyle">
+
+    <TextView
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:text="@string/app_name" />
+
+    <LinearLayout
+        android:layout_width="fill_parent"
+        android:layout_height="wrap_content"
+        android:gravity="center_vertical"
+        android:orientation="horizontal"
+        style="?android:attr/buttonBarStyle">
+
+        <Button
+            android:id="@+id/notification_button_gain"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_gain_name"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/notification_button_transient_pause"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_pause_name"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/notification_button_transient_duck"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_transient_duck_name"
+            style="?android:attr/buttonBarButtonStyle" />
+        <Button
+            android:id="@+id/notification_button_hide"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:text="@string/button_hide_notification_name"
+            style="?android:attr/buttonBarButtonStyle" />
+
+    </LinearLayout>
+
+</LinearLayout>
diff --git a/audio_focus_grabber/java/res/raw/ping.mp3 b/audio_focus_grabber/java/res/raw/ping.mp3
new file mode 100644
index 0000000..75b83ba
Binary files /dev/null and b/audio_focus_grabber/java/res/raw/ping.mp3 differ
diff --git a/audio_focus_grabber/java/res/values/strings.xml b/audio_focus_grabber/java/res/values/strings.xml
new file mode 100644
index 0000000..222879c
--- /dev/null
+++ b/audio_focus_grabber/java/res/values/strings.xml
@@ -0,0 +1,16 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Copyright (c) 2016 The Chromium Authors. All rights reserved.  Use of this
+  source code is governed by a BSD-style license that can be found in the
+  LICENSE file.
+-->
+
+<resources>
+    <string name="app_name">AudioFocusGrabber</string>
+    <string name="button_gain_name">Gain</string>
+    <string name="button_transient_pause_name">Pause</string>
+    <string name="button_transient_duck_name">Duck</string>
+    <string name="button_hide_notification_name">Hide</string>
+    <string name="button_show_notification_name">Show</string>
+    <string name="text_notification_prompt">Notification Controller:</string>
+</resources>
diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
new file mode 100644
index 0000000..7030b8f
--- /dev/null
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberActivity.java
@@ -0,0 +1,45 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.audio_focus_grabber;
+
+import android.app.Activity;
+import android.content.Intent;
+import android.os.Bundle;
+import android.view.View;
+
+/**
+ * The main activity of AudioFocusGrabber. It starts the background service,
+ * and responds to UI button controls.
+ */
+public class AudioFocusGrabberActivity extends Activity {
+    @Override
+    protected void onCreate(Bundle savedInstanceState) {
+        super.onCreate(savedInstanceState);
+        setContentView(R.layout.audio_focus_grabber_activity);
+    }
+
+    public void onButtonClicked(View view) {
+        Intent intent = new Intent(this, AudioFocusGrabberListenerService.class);
+        switch (view.getId()) {
+            case R.id.button_gain:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_GAIN);
+                break;
+            case R.id.button_transient_pause:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_TRANSIENT_PAUSE);
+                break;
+            case R.id.button_transient_duck:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_TRANSIENT_DUCK);
+                break;
+            case R.id.button_show_notification:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_SHOW_NOTIFICATION);
+                break;
+            case R.id.button_hide_notification:
+                intent.setAction(AudioFocusGrabberListenerService.ACTION_HIDE_NOTIFICATION);
+                break;
+        }
+        startService(intent);
+    }
+
+}
diff --git a/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
new file mode 100644
index 0000000..9981fd8
--- /dev/null
+++ b/audio_focus_grabber/java/src/org/chromium/tools/audio_focus_grabber/AudioFocusGrabberListenerService.java
@@ -0,0 +1,180 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.tools.audio_focus_grabber;
+
+import android.app.PendingIntent;
+import android.app.Service;
+import android.content.Context;
+import android.content.Intent;
+import android.media.AudioManager;
+import android.media.MediaPlayer;
+import android.os.IBinder;
+import android.support.v4.app.NotificationCompat;
+import android.support.v4.app.NotificationManagerCompat;
+import android.widget.RemoteViews;
+
+import org.chromium.base.Log;
+
+/**
+ * The listener service, which listens to intents and perform audio focus actions.
+ */
+public class AudioFocusGrabberListenerService extends Service {
+    private static final String TAG = "AudioFocusGrabber";
+
+    public static final String ACTION_GAIN = "AUDIO_FOCUS_GRABBER_GAIN";
+    public static final String ACTION_TRANSIENT_PAUSE = "AUDIO_FOCUS_GRABBER_TRANSIENT_PAUSE";
+    public static final String ACTION_TRANSIENT_DUCK = "AUDIO_FOCUS_GRABBER_TRANSIENT_DUCK";
+    public static final String ACTION_SHOW_NOTIFICATION = "AUDIO_FOCUS_GRABBER_SHOW_NOTIFICATION";
+    public static final String ACTION_HIDE_NOTIFICATION = "AUDIO_FOCUS_GRABBER_HIDE_NOTIFICATION";
+
+    private static final int NOTIFICATION_ID = 1;
+
+    AudioManager mAudioManager = null;
+    MediaPlayer mMediaPlayer = null;
+    boolean mIsDucking = false;
+
+    @Override
+    public void onCreate() {
+        super.onCreate();
+        mAudioManager = (AudioManager) getApplicationContext()
+                .getSystemService(Context.AUDIO_SERVICE);
+    }
+
+    @Override
+    public void onDestroy() {
+        hideNotification();
+    }
+
+    @Override
+    public IBinder onBind(Intent intent) {
+        return null;
+    }
+
+    @Override
+    public int onStartCommand(Intent intent, int flags, int startId) {
+        if (intent != null) {
+            Log.i(TAG, "received intent: " + intent.getAction());
+        } else {
+            Log.i(TAG, "received null intent");
+            return START_NOT_STICKY;
+        }
+        processIntent(intent);
+        return START_NOT_STICKY;
+    }
+
+    void processIntent(Intent intent) {
+        if (mMediaPlayer != null) {
+            Log.i(TAG, "There's already a MediaPlayer playing,"
+                    + " stopping the existing player and abandon focus");
+            releaseAndAbandonAudioFocus();
+        }
+        String action = intent.getAction();
+        if (ACTION_SHOW_NOTIFICATION.equals(action)) {
+            showNotification();
+        } else if (ACTION_HIDE_NOTIFICATION.equals(action)) {
+            hideNotification();
+        } else if (ACTION_GAIN.equals(action)) {
+            gainFocusAndPlay(AudioManager.AUDIOFOCUS_GAIN);
+        } else if (ACTION_TRANSIENT_PAUSE.equals(action)) {
+            gainFocusAndPlay(AudioManager.AUDIOFOCUS_GAIN_TRANSIENT);
+        } else if (ACTION_TRANSIENT_DUCK.equals(action)) {
+            gainFocusAndPlay(AudioManager.AUDIOFOCUS_GAIN_TRANSIENT_MAY_DUCK);
+        } else {
+            assert false;
+        }
+    }
+
+
+    void gainFocusAndPlay(int focusType) {
+        int result = mAudioManager.requestAudioFocus(
+                mOnAudioFocusChangeListener,
+                AudioManager.STREAM_MUSIC,
+                focusType);
+        if (result == AudioManager.AUDIOFOCUS_REQUEST_GRANTED) {
+            playSound();
+        } else {
+            Log.i(TAG, "cannot request audio focus");
+        }
+    }
+
+    void playSound() {
+        mMediaPlayer = MediaPlayer.create(getApplicationContext(), R.raw.ping);
+        mMediaPlayer.setOnCompletionListener(mOnCompletionListener);
+        mMediaPlayer.start();
+    }
+
+    void releaseAndAbandonAudioFocus() {
+        mMediaPlayer.release();
+        mMediaPlayer = null;
+        mAudioManager.abandonAudioFocus(mOnAudioFocusChangeListener);
+    }
+
+    MediaPlayer.OnCompletionListener mOnCompletionListener =
+            new MediaPlayer.OnCompletionListener() {
+                @Override
+                public void onCompletion(MediaPlayer mp) {
+                    releaseAndAbandonAudioFocus();
+                }
+            };
+
+    AudioManager.OnAudioFocusChangeListener mOnAudioFocusChangeListener =
+            new AudioManager.OnAudioFocusChangeListener() {
+                @Override
+                public void onAudioFocusChange(int focusChange) {
+                    switch (focusChange) {
+                        case AudioManager.AUDIOFOCUS_GAIN:
+                            if (mIsDucking) {
+                                mMediaPlayer.setVolume(1.0f, 1.0f);
+                                mIsDucking = false;
+                            } else {
+                                mMediaPlayer.start();
+                            }
+                            break;
+                        case AudioManager.AUDIOFOCUS_LOSS:
+                            mMediaPlayer.stop();
+                            mMediaPlayer.release();
+                            mMediaPlayer = null;
+                            break;
+                        case AudioManager.AUDIOFOCUS_LOSS_TRANSIENT:
+                            mMediaPlayer.pause();
+                            break;
+                        case AudioManager.AUDIOFOCUS_LOSS_TRANSIENT_CAN_DUCK:
+                            mMediaPlayer.setVolume(0.1f, 0.1f);
+                            mIsDucking = true;
+                            break;
+                    }
+                }
+            };
+
+    private void showNotification() {
+        RemoteViews view = new RemoteViews(this.getPackageName(),
+                                           R.layout.audio_focus_grabber_notification_bar);
+        view.setOnClickPendingIntent(R.id.notification_button_gain,
+                createPendingIntent(ACTION_GAIN));
+        view.setOnClickPendingIntent(R.id.notification_button_transient_pause,
+                createPendingIntent(ACTION_TRANSIENT_PAUSE));
+        view.setOnClickPendingIntent(R.id.notification_button_transient_duck,
+                createPendingIntent(ACTION_TRANSIENT_DUCK));
+        view.setOnClickPendingIntent(R.id.notification_button_hide,
+                createPendingIntent(ACTION_HIDE_NOTIFICATION));
+
+        NotificationManagerCompat manager = NotificationManagerCompat.from(this);
+        NotificationCompat.Builder builder = new NotificationCompat.Builder(this)
+                .setContent(view)
+                .setSmallIcon(R.drawable.notification_icon);
+        manager.notify(NOTIFICATION_ID, builder.build());
+    }
+
+    private void hideNotification() {
+        NotificationManagerCompat manager = NotificationManagerCompat.from(this);
+        manager.cancel(NOTIFICATION_ID);
+    }
+
+    private PendingIntent createPendingIntent(String action) {
+        Intent i = new Intent(this, AudioFocusGrabberListenerService.class);
+        i.setAction(action);
+        return PendingIntent.getService(this, 0, i, PendingIntent.FLAG_CANCEL_CURRENT);
+    }
+}

commit 07560f59ae10d4dc40c0fefeb96df4569c569cb8
Author: lizeb <lizeb@chromium.org>
Date:   Tue Jan 5 05:46:52 2016 -0800

    Tools to log all requests made to load a web page on Android.
    
    The requests are output to a JSON file, and are logged by connecting to
    DevTools. Also another tool to output a graphviz representation of the
    resources dependency graph.
    
    TBR=yfriedman
    
    Review URL: https://codereview.chromium.org/1400463003
    
    Cr-Original-Commit-Position: refs/heads/master@{#367516}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 20aaba1ed5c0b33f84ef5057e39874158379552a

diff --git a/loading/OWNERS b/loading/OWNERS
new file mode 100644
index 0000000..3301555
--- /dev/null
+++ b/loading/OWNERS
@@ -0,0 +1,2 @@
+lizeb@chromium.org
+pasko@chromium.org
diff --git a/loading/log_parser.py b/loading/log_parser.py
new file mode 100644
index 0000000..bb2f953
--- /dev/null
+++ b/loading/log_parser.py
@@ -0,0 +1,214 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Parses a JSON request log created by log_requests.py."""
+
+import collections
+import json
+import operator
+import urlparse
+
+
+Timing = collections.namedtuple(
+    'Timing',
+    ['connectEnd', 'connectStart', 'dnsEnd', 'dnsStart', 'proxyEnd',
+     'proxyStart', 'receiveHeadersEnd', 'requestTime', 'sendEnd', 'sendStart',
+     'sslEnd', 'sslStart', 'workerReady', 'workerStart'])
+
+
+class Resource(object):
+  """Describes a resource."""
+
+  def __init__(self, url, content_type):
+    """Creates an instance of Resource.
+
+    Args:
+      url: URL of the resource
+      content_type: Content-Type of the resources.
+    """
+    self.url = url
+    self.content_type = content_type
+
+  def GetShortName(self):
+    """Returns either the hostname of the resource, or the filename,
+    or the end of the path.
+    """
+    parsed = urlparse.urlparse(self.url)
+    path = parsed.path
+    if path != '' and path != '/':
+      last_path = parsed.path.split('/')[-1]
+      if len(last_path) < 10:
+        if len(path) < 10:
+          return path
+        else:
+          return parsed.path[-10:]
+      else:
+        return last_path
+    else:
+      return parsed.hostname
+
+  def GetContentType(self):
+    mime = self.content_type
+    if mime == 'text/html':
+      return 'html'
+    elif mime == 'text/css':
+      return 'css'
+    elif mime in ('application/x-javascript', 'text/javascript',
+                  'application/javascript'):
+      return 'script'
+    elif mime == 'application/json':
+      return 'json'
+    elif mime == 'image/gif':
+      return 'gif_image'
+    elif mime.startswith('image/'):
+      return 'image'
+    else:
+      return 'other'
+
+  @classmethod
+  def FromRequest(cls, request):
+    """Creates a Resource from an instance of RequestData."""
+    return Resource(request.url, request.GetContentType())
+
+  def __Fields(self):
+    return (self.url, self.content_type)
+
+  def __eq__(self, o):
+    return  self.__Fields() == o.__Fields()
+
+  def __hash__(self):
+    return hash(self.__Fields())
+
+
+class RequestData(object):
+  """Represents a request, as dumped by log_requests.py."""
+
+  def __init__(self, status, headers, request_headers, timestamp, timing, url,
+               served_from_cache, initiator):
+    self.status = status
+    self.headers = headers
+    self.request_headers = request_headers
+    self.timestamp = timestamp
+    self.timing = Timing(**timing) if timing else None
+    self.url = url
+    self.served_from_cache = served_from_cache
+    self.initiator = initiator
+
+  def IsDataUrl(self):
+    return self.url.startswith('data:')
+
+  def GetContentType(self):
+    content_type = self.headers['Content-Type']
+    if ';' in content_type:
+      return content_type[:content_type.index(';')]
+    else:
+      return content_type
+
+  @classmethod
+  def FromDict(cls, r):
+    """Creates a RequestData object from a dict."""
+    return RequestData(r['status'], r['headers'], r['request_headers'],
+                       r['timestamp'], r['timing'], r['url'],
+                       r['served_from_cache'], r['initiator'])
+
+
+def ParseJsonFile(filename):
+  """Converts a JSON file to a sequence of RequestData."""
+  with open(filename) as f:
+    json_data = json.load(f)
+    return [RequestData.FromDict(r) for r in json_data]
+
+
+def FilterRequests(requests):
+  """Filters a list of requests.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    A list of requests that are not data URL, have a Content-Type, and are
+    not served from the cache.
+  """
+  return [r for r in requests if not r.IsDataUrl()
+          and 'Content-Type' in r.headers and not r.served_from_cache]
+
+
+def ResourceToRequestMap(requests):
+  """Returns a Resource -> Request map.
+
+  A resource can be requested several times in a single page load. Keeps the
+  first request in this case.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    [Resource, ...]
+  """
+  # reversed(requests) because we want the first one to win.
+  return dict([(Resource.FromRequest(r), r) for r in reversed(requests)])
+
+
+def GetResources(requests):
+  """Returns an ordered list of resources from a list of requests.
+
+  The same resource can be requested several time for a single page load. This
+  keeps only the first request.
+
+  Args:
+    requests: [RequestData]
+
+  Returns:
+    [Resource]
+  """
+  resources = []
+  known_resources = set()
+  for r in requests:
+    resource = Resource.FromRequest(r)
+    if r in known_resources:
+      continue
+    known_resources.add(resource)
+    resources.append(resource)
+  return resources
+
+
+def ParseCacheControl(headers):
+  """Parses the "Cache-Control" header and returns a dict representing it.
+
+  Args:
+    headers: (dict) Response headers.
+
+  Returns:
+    {Directive: Value, ...}
+  """
+  # TODO(lizeb): Handle the "Expires" header as well.
+  result = {}
+  cache_control = headers.get('Cache-Control', None)
+  if cache_control is None:
+    return result
+  directives = [s.strip() for s in cache_control.split(',')]
+  for directive in directives:
+    parts = [s.strip() for s in directive.split('=')]
+    if len(parts) == 1:
+      result[parts[0]] = True
+    else:
+      result[parts[0]] = parts[1]
+  return result
+
+
+def MaxAge(request):
+  """Returns the max-age of a resource, or -1."""
+  cache_control = ParseCacheControl(request.headers)
+  if (u'no-store' in cache_control
+      or u'no-cache' in cache_control
+      or len(cache_control) == 0):
+    return -1
+  if 'max-age' in cache_control:
+    return int(cache_control['max-age'])
+  return -1
+
+
+def SortedByCompletion(requests):
+  """Returns the requests, sorted by completion time."""
+  return sorted(requests, key=operator.attrgetter('timestamp'))
diff --git a/loading/log_requests.py b/loading/log_requests.py
new file mode 100755
index 0000000..16a10d5
--- /dev/null
+++ b/loading/log_requests.py
@@ -0,0 +1,216 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Loads a URL on an Android device, logging all the requests made to do it
+to a JSON file using DevTools.
+"""
+
+import contextlib
+import httplib
+import json
+import logging
+import optparse
+import os
+import sys
+import time
+
+file_dir = os.path.dirname(__file__)
+sys.path.append(os.path.join(file_dir, '..', '..', '..', 'build', 'android'))
+sys.path.append(os.path.join(file_dir, '..', '..', 'telemetry'))
+sys.path.append(os.path.join(file_dir, '..', '..', 'chrome_proxy'))
+
+from pylib import constants
+from pylib import flag_changer
+from pylib.device import device_utils
+from pylib.device import intent
+from common import inspector_network
+from telemetry.internal.backends.chrome_inspector import inspector_websocket
+from telemetry.internal.backends.chrome_inspector import websocket
+
+
+_PORT = 9222 # DevTools port number.
+
+
+@contextlib.contextmanager
+def FlagChanger(device, command_line_path, new_flags):
+  """Changes the flags in a context, restores them afterwards.
+
+  Args:
+    device: Device to target, from DeviceUtils.
+    command_line_path: Full path to the command-line file.
+    new_flags: Flags to add.
+  """
+  changer = flag_changer.FlagChanger(device, command_line_path)
+  changer.AddFlags(new_flags)
+  try:
+    yield
+  finally:
+    changer.Restore()
+
+
+@contextlib.contextmanager
+def ForwardPort(device, local, remote):
+  """Forwards a local port to a remote one on a device in a context."""
+  device.adb.Forward(local, remote)
+  try:
+    yield
+  finally:
+    device.adb.ForwardRemove(local)
+
+
+def _SetUpDevice(device, package_info):
+  """Enables root and closes Chrome on a device."""
+  device.EnableRoot()
+  device.KillAll(package_info.package, quiet=True)
+
+
+class AndroidRequestsLogger(object):
+  """Logs all the requests made to load a page on a device."""
+
+  def __init__(self, device):
+    self.device = device
+    self._please_stop = False
+    self._main_frame_id = None
+
+  def _PageDataReceived(self, msg):
+    """Called when a Page event is received.
+
+    Records the main frame, and stops the recording once it has finished
+    loading.
+
+    Args:
+      msg: (dict) Message sent by DevTools.
+    """
+    if 'params' not in msg:
+      return
+    params = msg['params']
+    method = msg.get('method', None)
+    if method == 'Page.frameStartedLoading' and self._main_frame_id is None:
+      self._main_frame_id = params['frameId']
+    elif (method == 'Page.frameStoppedLoading'
+          and params['frameId'] == self._main_frame_id):
+      self._please_stop = True
+
+  def _LogPageLoadInternal(self, url, clear_cache):
+    """Returns the collection of requests made to load a given URL.
+
+    Assumes that DevTools is available on http://localhost:_PORT.
+
+    Args:
+      url: URL to load.
+      clear_cache: Whether to clear the HTTP cache.
+
+    Returns:
+      [inspector_network.InspectorNetworkResponseData, ...]
+    """
+    self._main_frame_id = None
+    self._please_stop = False
+    r = httplib.HTTPConnection('localhost', _PORT)
+    r.request('GET', '/json')
+    response = r.getresponse()
+    if response.status != 200:
+      logging.error('Cannot connect to the remote target.')
+      return None
+    json_response = json.loads(response.read())
+    r.close()
+    websocket_url = json_response[0]['webSocketDebuggerUrl']
+    ws = inspector_websocket.InspectorWebsocket()
+    ws.Connect(websocket_url)
+    inspector = inspector_network.InspectorNetwork(ws)
+    if clear_cache:
+      inspector.ClearCache()
+    ws.SyncRequest({'method': 'Page.enable'})
+    ws.RegisterDomain('Page', self._PageDataReceived)
+    inspector.StartMonitoringNetwork()
+    ws.SendAndIgnoreResponse({'method': 'Page.navigate',
+                              'params': {'url': url}})
+    while not self._please_stop:
+      try:
+        ws.DispatchNotifications()
+      except websocket.WebSocketTimeoutException as e:
+        logging.warning('Exception: ' + str(e))
+        break
+    inspector.StopMonitoringNetwork()
+    return inspector.GetResponseData()
+
+  def LogPageLoad(self, url, clear_cache):
+    """Returns the collection of requests made to load a given URL on a device.
+
+    Args:
+      url: (str) URL to load on the device.
+      clear_cache: (bool) Whether to clear the HTTP cache.
+
+    Returns:
+      See _LogPageLoadInternal().
+    """
+    package_info = constants.PACKAGE_INFO['chrome']
+    command_line_path = '/data/local/chrome-command-line'
+    new_flags = ['--enable-test-events', '--remote-debugging-port=%d' % _PORT]
+    _SetUpDevice(self.device, package_info)
+    with FlagChanger(self.device, command_line_path, new_flags):
+      start_intent = intent.Intent(
+          package=package_info.package, activity=package_info.activity,
+          data='about:blank')
+      self.device.StartActivity(start_intent, blocking=True)
+      time.sleep(2)
+      with ForwardPort(self.device, 'tcp:%d' % _PORT,
+                       'localabstract:chrome_devtools_remote'):
+        return self._LogPageLoadInternal(url, clear_cache)
+
+
+def _ResponseDataToJson(data):
+  """Converts a list of inspector_network.InspectorNetworkResponseData to JSON.
+
+  Args:
+    data: as returned by _LogPageLoad()
+
+  Returns:
+    A JSON file with the following format:
+    [request1, request2, ...], and a request is:
+    {'status': str, 'headers': dict, 'request_headers': dict,
+     'timestamp': double, 'timing': dict, 'url': str,
+      'served_from_cache': bool, 'initiator': str})
+  """
+  result = []
+  for r in data:
+    result.append({'status': r.status,
+                   'headers': r.headers,
+                   'request_headers': r.request_headers,
+                   'timestamp': r.timestamp,
+                   'timing': r.timing,
+                   'url': r.url,
+                   'served_from_cache': r.served_from_cache,
+                   'initiator': r.initiator})
+  return json.dumps(result)
+
+
+def _CreateOptionParser():
+  """Returns the option parser for this tool."""
+  parser = optparse.OptionParser(description='Starts a browser on an Android '
+                                 'device, gathers the requests made to load a '
+                                 'page and dumps it to a JSON file.')
+  parser.add_option('--url', help='URL to load.',
+                    default='https://www.google.com', metavar='URL')
+  parser.add_option('--output', help='Output file.', default='result.json')
+  parser.add_option('--no-clear-cache', help=('Do not clear the HTTP cache '
+                                              'before loading the URL.'),
+                    default=True, action='store_false', dest='clear_cache')
+  return parser
+
+
+def main():
+  parser = _CreateOptionParser()
+  options, _ = parser.parse_args()
+  devices = device_utils.DeviceUtils.HealthyDevices()
+  device = devices[0]
+  request_logger = AndroidRequestsLogger(device)
+  response_data = request_logger.LogPageLoad(options.url, options.clear_cache)
+  json_data = _ResponseDataToJson(response_data)
+  with open(options.output, 'w') as f:
+    f.write(json_data)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/loading/process_request_log.py b/loading/process_request_log.py
new file mode 100755
index 0000000..bbec0a8
--- /dev/null
+++ b/loading/process_request_log.py
@@ -0,0 +1,189 @@
+#! /usr/bin/python
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Creates a Graphviz file visualizing the resource dependencies from a JSON
+file dumped by log_requests.py.
+"""
+
+import collections
+import sys
+import urlparse
+
+import log_parser
+from log_parser import Resource
+
+
+def _BuildResourceDependencyGraph(requests):
+  """Builds the graph of resource dependencies.
+
+  Args:
+    requests: [RequestData, ...]
+
+  Returns:
+    A tuple ([Resource], [(resource1, resource2, reason), ...])
+  """
+  resources = log_parser.GetResources(requests)
+  resources_from_url = {resource.url: resource for resource in resources}
+  requests_by_completion = log_parser.SortedByCompletion(requests)
+  deps = []
+  for r in requests:
+    resource = Resource.FromRequest(r)
+    initiator = r.initiator
+    initiator_type = initiator['type']
+    dep = None
+    if initiator_type == 'parser':
+      url = initiator['url']
+      blocking_resource = resources_from_url.get(url, None)
+      if blocking_resource is None:
+        continue
+      dep = (blocking_resource, resource, 'parser')
+    elif initiator_type == 'script' and 'stackTrace' in initiator:
+      for frame in initiator['stackTrace']:
+        url = frame['url']
+        blocking_resource = resources_from_url.get(url, None)
+        if blocking_resource is None:
+          continue
+        dep = (blocking_resource, resource, 'stack')
+        break
+    else:
+      # When the initiator is a script without a stackTrace, infer that it comes
+      # from the most recent script from the same hostname.
+      # TLD+1 might be better, but finding what is a TLD requires a database.
+      request_hostname = urlparse.urlparse(r.url).hostname
+      sorted_script_requests_from_hostname = [
+          r for r in requests_by_completion
+          if (resource.GetContentType() in ('script', 'html', 'json')
+              and urlparse.urlparse(r.url).hostname == request_hostname)]
+      most_recent = None
+      # Linear search is bad, but this shouldn't matter here.
+      for request in sorted_script_requests_from_hostname:
+        if request.timestamp < r.timing.requestTime:
+          most_recent = request
+        else:
+          break
+      if most_recent is not None:
+        blocking = resources_from_url.get(most_recent.url, None)
+        if blocking is not None:
+          dep = (blocking, resource, 'script_inferred')
+    if dep is not None:
+      deps.append(dep)
+  return (resources, deps)
+
+
+def PrefetchableResources(requests):
+  """Returns a list of resources that are discoverable without JS.
+
+  Args:
+    requests: List of requests.
+
+  Returns:
+    List of discoverable resources, with their initial request.
+  """
+  resource_to_request = log_parser.ResourceToRequestMap(requests)
+  (_, all_deps) = _BuildResourceDependencyGraph(requests)
+  # Only keep "parser" arcs
+  deps = [(first, second) for (first, second, reason) in all_deps
+          if reason == 'parser']
+  deps_per_resource = collections.defaultdict(list)
+  for (first, second) in deps:
+    deps_per_resource[first].append(second)
+  result = []
+  visited = set()
+  to_visit = [deps[0][0]]
+  while len(to_visit) != 0:
+    r = to_visit.pop()
+    visited.add(r)
+    to_visit += deps_per_resource[r]
+    result.append(resource_to_request[r])
+  return result
+
+
+_CONTENT_TYPE_TO_COLOR = {'html': 'red', 'css': 'green', 'script': 'blue',
+                          'json': 'purple', 'gif_image': 'grey',
+                          'image': 'orange', 'other': 'white'}
+
+
+def _ResourceGraphvizNode(resource, request, resource_to_index):
+  """Returns the node description for a given resource.
+
+  Args:
+    resource: Resource.
+    request: RequestData associated with the resource.
+    resource_to_index: {Resource: int}.
+
+  Returns:
+    A string describing the resource in graphviz format.
+    The resource is color-coded according to its content type, and its shape is
+    oval if its max-age is less than 300s (or if it's not cacheable).
+  """
+  color = _CONTENT_TYPE_TO_COLOR[resource.GetContentType()]
+  max_age = log_parser.MaxAge(request)
+  shape = 'polygon' if max_age > 300 else 'oval'
+  return ('%d [label = "%s"; style = "filled"; fillcolor = %s; shape = %s];\n'
+          % (resource_to_index[resource], resource.GetShortName(), color,
+             shape))
+
+
+def _GraphvizFileFromDeps(resources, requests, deps, output_filename):
+  """Writes a graphviz file from a set of resource dependencies.
+
+  Args:
+    resources: [Resource, ...]
+    requests: list of requests
+    deps: [(resource1, resource2, reason), ...]
+    output_filename: file to write the graph to.
+  """
+  with open(output_filename, 'w') as f:
+    f.write("""digraph dependencies {
+    rankdir = LR;
+    """)
+    resource_to_request = log_parser.ResourceToRequestMap(requests)
+    resource_to_index = {r: i for (i, r) in enumerate(resources)}
+    resources_with_edges = set()
+    for (first, second, reason) in deps:
+      resources_with_edges.add(first)
+      resources_with_edges.add(second)
+    if len(resources_with_edges) != len(resources):
+      f.write("""subgraph cluster_orphans {
+  color=black;
+  label="Orphans";
+""")
+      for resource in resources:
+        if resource not in resources_with_edges:
+          request = resource_to_request[resource]
+          f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
+      f.write('}\n')
+
+    f.write("""subgraph cluster_nodes {
+  color=invis;
+""")
+    for resource in resources:
+      request = resource_to_request[resource]
+      print resource.url
+      if resource in resources_with_edges:
+        f.write(_ResourceGraphvizNode(resource, request, resource_to_index))
+    for (first, second, reason) in deps:
+      arrow = ''
+      if reason == 'parser':
+        arrow = '[color = red]'
+      elif reason == 'stack':
+        arrow = '[color = blue]'
+      elif reason == 'script_inferred':
+        arrow = '[color = blue; style=dotted]'
+      f.write('%d -> %d %s;\n' % (
+          resource_to_index[first], resource_to_index[second], arrow))
+    f.write('}\n}\n')
+
+
+def main():
+  filename = sys.argv[1]
+  requests = log_parser.ParseJsonFile(filename)
+  requests = log_parser.FilterRequests(requests)
+  (resources, deps) = _BuildResourceDependencyGraph(requests)
+  _GraphvizFileFromDeps(resources, requests, deps, filename + '.dot')
+
+
+if __name__ == '__main__':
+  main()

commit 1f10edc48758361ad944c8aab3d845e4f67ea300
Author: thakis <thakis@chromium.org>
Date:   Mon Jan 4 12:07:23 2016 -0800

    Enable -Wformat in linux, android, chromeos, cast builds.
    
    No behavior change.
    
    BUG=573780
    
    Review URL: https://codereview.chromium.org/1551313002
    
    Cr-Original-Commit-Position: refs/heads/master@{#367353}
    Cr-Mirrored-From: https://chromium.googlesource.com/chromium/src
    Cr-Mirrored-Commit: 932e1e5939963b5b67019f428e8f209246069a44

diff --git a/file_poller/file_poller.cc b/file_poller/file_poller.cc
index f7b6d58..448e229 100644
--- a/file_poller/file_poller.cc
+++ b/file_poller/file_poller.cc
@@ -88,7 +88,7 @@ void acquire_sample(int fd, const Context& context) {
   struct timeval tv;
   gettimeofday(&tv, NULL);
   char buffer[1024];
-  int n = snprintf(buffer, sizeof(buffer), "%d.%06d ", tv.tv_sec, tv.tv_usec);
+  int n = snprintf(buffer, sizeof(buffer), "%ld.%06ld ", tv.tv_sec, tv.tv_usec);
   safe_write(fd, buffer, n);
 
   for (int i = 0; i < context.nb_files; ++i)
diff --git a/memconsumer/memconsumer_hook.cc b/memconsumer/memconsumer_hook.cc
index 9ae0bc1..78a98d9 100644
--- a/memconsumer/memconsumer_hook.cc
+++ b/memconsumer/memconsumer_hook.cc
@@ -42,7 +42,7 @@ JNIEXPORT void JNICALL
   if (!g_memory) {
     __android_log_print(ANDROID_LOG_WARN,
                         "MemConsumer",
-                        "Unable to allocate %ld bytes",
+                        "Unable to allocate %lld bytes",
                         memory);
   }
   // If memory allocation failed, try to allocate as much as possible.
